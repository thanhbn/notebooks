{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning 3: CodeBLEU vs Exact Match Evaluation\n",
    "\n",
    "## Mục tiêu học tập\n",
    "Hiểu sâu về evaluation metrics trong code generation, đặc biệt là sự khác biệt giữa Exact Match (EM) và CodeBLEU, dựa trên Section 3.5 của paper.\n",
    "\n",
    "## Trích xuất từ Paper\n",
    "\n",
    "### Section 3.5: The Evaluation Measures\n",
    "\n",
    "#### Exact Match (EM)\n",
    "> \"*Exact Match (EM) [4, 5, 6] is the number of the generated revised code that is the same as the actual revised code in the testing dataset. We use this measure since it is widely used for evaluating code review automation approaches [1, 4, 6]. To compare the generated revised code with the actual revised code, we first tokenize both revised code to sequences of tokens. Then, we compared the sequence of tokens of the generated revised code with the sequence of tokens of the actual revised code. A high value of EM indicates that a model can generate revised code that is the same as the actual revised code in the testing dataset.*\"\n",
    "\n",
    "#### CodeBLEU\n",
    "> \"*CodeBLEU [22] is the extended version of BLEU (i.e., an n-gram overlap between the translation generated by a deep learning model and the translation in ground truth) [43] for automatic evaluation of the generated code. We do not measure BLEU like in prior work [5, 6] since Ren et al. [22] found that this measure ignores syntactic and semantic correctness of the generated code. In addition to BLEU, CodeBLEU considers the weighted n-gram match, matched syntactic information (i.e., abstract syntax tree: AST) and matched semantic information (i.e., data flow: DF) when computing the similarity between the generated revised code and the actual revised code.*\"\n",
    "\n",
    "### Formula từ CodeBLEU Paper [22]\n",
    "$$\\text{CodeBLEU} = \\alpha \\cdot \\text{BLEU} + \\beta \\cdot \\text{BLEU}_{\\text{weight}} + \\gamma \\cdot \\text{Match}_{\\text{AST}} + \\delta \\cdot \\text{Match}_{\\text{DF}}$$\n",
    "\n",
    "Trong đó:\n",
    "- $\\alpha + \\beta + \\gamma + \\delta = 1$\n",
    "- $\\text{BLEU}$: Traditional n-gram overlap\n",
    "- $\\text{BLEU}_{\\text{weight}}$: Weighted n-gram với keyword emphasis\n",
    "- $\\text{Match}_{\\text{AST}}$: Abstract Syntax Tree matching\n",
    "- $\\text{Match}_{\\text{DF}}$: Data Flow matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lý thuyết Evaluation Metrics cho Code Generation\n",
    "\n",
    "### Exact Match (EM)\n",
    "\n",
    "**Định nghĩa**: Binary metric kiểm tra token-level exact matching\n",
    "\n",
    "**Ưu điểm**:\n",
    "- Simple và interpretable\n",
    "- No ambiguity trong evaluation\n",
    "- Widely used trong literature\n",
    "\n",
    "**Nhược điểm**:\n",
    "- Too strict: Rejects semantically equivalent code\n",
    "- Sensitive to formatting differences\n",
    "- Doesn't capture partial correctness\n",
    "\n",
    "### CodeBLEU\n",
    "\n",
    "**Components Analysis**:\n",
    "\n",
    "1. **N-gram BLEU**: Surface-level similarity\n",
    "2. **Weighted BLEU**: Emphasizes programming keywords\n",
    "3. **AST Matching**: Structural similarity\n",
    "4. **Data Flow Matching**: Semantic similarity\n",
    "\n",
    "**Advantages over EM**:\n",
    "- Captures semantic equivalence\n",
    "- Partial credit for similar solutions\n",
    "- More robust to formatting differences\n",
    "\n",
    "**Challenges**:\n",
    "- Complex implementation\n",
    "- Parameter tuning required\n",
    "- Language-specific adaptations needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "import re\n",
    "import ast\n",
    "import difflib\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Math and statistics\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Text processing\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "# Try to download nltk data (ignore if already present)\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"📚 Libraries imported for evaluation metrics analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Exact Match và CodeBLEU từ Scratch\n",
    "\n",
    "Implement cả hai metrics để hiểu rõ differences và use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Container for evaluation results\"\"\"\n",
    "    exact_match: float\n",
    "    code_bleu: float\n",
    "    bleu_components: Dict[str, float]\n",
    "    ast_similarity: float\n",
    "    token_overlap: float\n",
    "    example_id: str = \"\"\n",
    "\n",
    "class CodeTokenizer:\n",
    "    \"\"\"Enhanced tokenizer for code, following paper's approach\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Programming keywords to emphasize\n",
    "        self.keywords = {\n",
    "            'java': ['public', 'private', 'static', 'void', 'int', 'String', 'boolean', 'if', 'else', 'for', 'while', 'return', 'class', 'interface'],\n",
    "            'python': ['def', 'class', 'if', 'else', 'elif', 'for', 'while', 'return', 'import', 'from', 'try', 'except', 'with'],\n",
    "            'javascript': ['function', 'var', 'let', 'const', 'if', 'else', 'for', 'while', 'return', 'class', 'async', 'await'],\n",
    "            'go': ['func', 'var', 'const', 'if', 'else', 'for', 'range', 'return', 'struct', 'interface', 'package']\n",
    "        }\n",
    "    \n",
    "    def tokenize(self, code: str, language: str = 'java') -> List[str]:\n",
    "        \"\"\"Tokenize code into meaningful tokens\"\"\"\n",
    "        # Normalize whitespace\n",
    "        code = re.sub(r'\\s+', ' ', code.strip())\n",
    "        \n",
    "        # Split on programming delimiters while preserving them\n",
    "        # This regex captures operators, brackets, semicolons, etc.\n",
    "        tokens = re.findall(r'\\w+|[{}()\\[\\];,.<>=!&|+\\-*/\"\\']', code)\n",
    "        \n",
    "        # Convert to lowercase for case-insensitive comparison\n",
    "        tokens = [token.lower() for token in tokens if token.strip()]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def get_keyword_weights(self, tokens: List[str], language: str = 'java') -> List[float]:\n",
    "        \"\"\"Get weights for tokens, emphasizing keywords\"\"\"\n",
    "        lang_keywords = self.keywords.get(language, [])\n",
    "        weights = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in lang_keywords:\n",
    "                weights.append(2.0)  # Emphasize keywords\n",
    "            elif re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', token):  # Identifiers\n",
    "                weights.append(1.5)\n",
    "            else:  # Operators and punctuation\n",
    "                weights.append(1.0)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "class ExactMatchEvaluator:\n",
    "    \"\"\"Implementation of Exact Match metric từ paper\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = CodeTokenizer()\n",
    "    \n",
    "    def evaluate(self, generated_code: str, reference_code: str, language: str = 'java') -> float:\n",
    "        \"\"\"Calculate Exact Match score\"\"\"\n",
    "        # Tokenize both codes\n",
    "        generated_tokens = self.tokenizer.tokenize(generated_code, language)\n",
    "        reference_tokens = self.tokenizer.tokenize(reference_code, language)\n",
    "        \n",
    "        # Exact comparison of token sequences\n",
    "        if generated_tokens == reference_tokens:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def detailed_comparison(self, generated_code: str, reference_code: str, language: str = 'java') -> Dict[str, Any]:\n",
    "        \"\"\"Detailed comparison for analysis\"\"\"\n",
    "        generated_tokens = self.tokenizer.tokenize(generated_code, language)\n",
    "        reference_tokens = self.tokenizer.tokenize(reference_code, language)\n",
    "        \n",
    "        # Calculate various similarity measures\n",
    "        exact_match = 1.0 if generated_tokens == reference_tokens else 0.0\n",
    "        \n",
    "        # Token overlap\n",
    "        if not reference_tokens:\n",
    "            token_overlap = 0.0\n",
    "        else:\n",
    "            common_tokens = set(generated_tokens) & set(reference_tokens)\n",
    "            token_overlap = len(common_tokens) / len(set(reference_tokens))\n",
    "        \n",
    "        # Sequence similarity (using difflib)\n",
    "        sequence_similarity = difflib.SequenceMatcher(None, generated_tokens, reference_tokens).ratio()\n",
    "        \n",
    "        # Length difference\n",
    "        length_ratio = len(generated_tokens) / max(len(reference_tokens), 1)\n",
    "        \n",
    "        return {\n",
    "            'exact_match': exact_match,\n",
    "            'token_overlap': token_overlap,\n",
    "            'sequence_similarity': sequence_similarity,\n",
    "            'length_ratio': length_ratio,\n",
    "            'generated_length': len(generated_tokens),\n",
    "            'reference_length': len(reference_tokens),\n",
    "            'generated_tokens': generated_tokens,\n",
    "            'reference_tokens': reference_tokens\n",
    "        }\n",
    "\n",
    "class SimplifiedCodeBLEUEvaluator:\n",
    "    \"\"\"Simplified implementation of CodeBLEU (without full AST/DF analysis)\"\"\"\n",
    "    \n",
    "    def __init__(self, weights: Tuple[float, float, float, float] = (0.25, 0.25, 0.25, 0.25)):\n",
    "        self.tokenizer = CodeTokenizer()\n",
    "        self.alpha, self.beta, self.gamma, self.delta = weights  # BLEU, Weighted-BLEU, AST, DF\n",
    "        self.smoothing = SmoothingFunction()\n",
    "    \n",
    "    def calculate_ngram_bleu(self, generated_tokens: List[str], reference_tokens: List[str], max_n: int = 4) -> float:\n",
    "        \"\"\"Calculate n-gram BLEU score\"\"\"\n",
    "        if not reference_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        # Use NLTK's BLEU implementation with smoothing\n",
    "        try:\n",
    "            bleu_score = sentence_bleu(\n",
    "                [reference_tokens], \n",
    "                generated_tokens,\n",
    "                weights=tuple(1/max_n for _ in range(max_n)),\n",
    "                smoothing_function=self.smoothing.method1\n",
    "            )\n",
    "            return bleu_score\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_weighted_bleu(self, generated_tokens: List[str], reference_tokens: List[str], language: str = 'java') -> float:\n",
    "        \"\"\"Calculate weighted BLEU with keyword emphasis\"\"\"\n",
    "        if not reference_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get keyword weights\n",
    "        gen_weights = self.tokenizer.get_keyword_weights(generated_tokens, language)\n",
    "        ref_weights = self.tokenizer.get_keyword_weights(reference_tokens, language)\n",
    "        \n",
    "        # Calculate weighted overlap\n",
    "        weighted_overlap = 0.0\n",
    "        total_ref_weight = sum(ref_weights)\n",
    "        \n",
    "        if total_ref_weight == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Simple weighted matching (bag-of-words approach)\n",
    "        gen_counter = Counter(generated_tokens)\n",
    "        ref_counter = Counter(reference_tokens)\n",
    "        \n",
    "        for token, ref_count in ref_counter.items():\n",
    "            gen_count = gen_counter.get(token, 0)\n",
    "            matched_count = min(gen_count, ref_count)\n",
    "            \n",
    "            # Find weight for this token\n",
    "            token_weight = 1.0\n",
    "            if token in reference_tokens:\n",
    "                idx = reference_tokens.index(token)\n",
    "                token_weight = ref_weights[idx] if idx < len(ref_weights) else 1.0\n",
    "            \n",
    "            weighted_overlap += matched_count * token_weight\n",
    "        \n",
    "        return weighted_overlap / total_ref_weight\n",
    "    \n",
    "    def calculate_ast_similarity(self, generated_code: str, reference_code: str, language: str = 'java') -> float:\n",
    "        \"\"\"Simplified AST similarity (using structural patterns)\"\"\"\n",
    "        \n",
    "        # For simplicity, we'll use pattern-based structural similarity\n",
    "        # In a full implementation, this would use actual AST parsing\n",
    "        \n",
    "        def extract_structural_patterns(code: str) -> List[str]:\n",
    "            patterns = []\n",
    "            \n",
    "            # Control flow patterns\n",
    "            if 'if' in code: patterns.append('conditional')\n",
    "            if 'for' in code or 'while' in code: patterns.append('loop')\n",
    "            if 'return' in code: patterns.append('return')\n",
    "            if 'function' in code or 'def' in code or 'func' in code: patterns.append('function_def')\n",
    "            \n",
    "            # Operator patterns\n",
    "            if '?' in code and ':' in code: patterns.append('ternary')\n",
    "            if '&&' in code or '||' in code: patterns.append('logical_op')\n",
    "            if '+=' in code or '-=' in code: patterns.append('compound_assignment')\n",
    "            \n",
    "            # Bracket patterns\n",
    "            patterns.append(f\"braces_{code.count('{')}\")  \n",
    "            patterns.append(f\"parens_{code.count('(')}\")  \n",
    "            patterns.append(f\"brackets_{code.count('[')}\")  \n",
    "            \n",
    "            return patterns\n",
    "        \n",
    "        gen_patterns = set(extract_structural_patterns(generated_code))\n",
    "        ref_patterns = set(extract_structural_patterns(reference_code))\n",
    "        \n",
    "        if not ref_patterns:\n",
    "            return 0.0\n",
    "        \n",
    "        common_patterns = gen_patterns & ref_patterns\n",
    "        return len(common_patterns) / len(ref_patterns)\n",
    "    \n",
    "    def calculate_dataflow_similarity(self, generated_code: str, reference_code: str, language: str = 'java') -> float:\n",
    "        \"\"\"Simplified data flow similarity (using variable usage patterns)\"\"\"\n",
    "        \n",
    "        def extract_variable_patterns(code: str) -> Dict[str, List[str]]:\n",
    "            patterns = defaultdict(list)\n",
    "            \n",
    "            # Simple variable usage detection\n",
    "            tokens = re.findall(r'\\w+', code)\n",
    "            \n",
    "            for i, token in enumerate(tokens):\n",
    "                # Variable assignment pattern\n",
    "                if i < len(tokens) - 1 and tokens[i + 1] == '=':\n",
    "                    patterns[token].append('assigned')\n",
    "                \n",
    "                # Variable usage in conditions\n",
    "                if i > 0 and tokens[i - 1] in ['if', 'while']:\n",
    "                    patterns[token].append('condition')\n",
    "                \n",
    "                # Function call pattern\n",
    "                if i < len(tokens) - 1 and '(' in ' '.join(tokens[i:i+2]):\n",
    "                    patterns[token].append('function_call')\n",
    "            \n",
    "            return dict(patterns)\n",
    "        \n",
    "        gen_patterns = extract_variable_patterns(generated_code)\n",
    "        ref_patterns = extract_variable_patterns(reference_code)\n",
    "        \n",
    "        if not ref_patterns:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate overlap in variable usage patterns\n",
    "        total_ref_patterns = sum(len(patterns) for patterns in ref_patterns.values())\n",
    "        if total_ref_patterns == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        common_patterns = 0\n",
    "        for var, ref_var_patterns in ref_patterns.items():\n",
    "            if var in gen_patterns:\n",
    "                gen_var_patterns = gen_patterns[var]\n",
    "                common = set(ref_var_patterns) & set(gen_var_patterns)\n",
    "                common_patterns += len(common)\n",
    "        \n",
    "        return common_patterns / total_ref_patterns\n",
    "    \n",
    "    def evaluate(self, generated_code: str, reference_code: str, language: str = 'java') -> EvaluationResult:\n",
    "        \"\"\"Calculate comprehensive CodeBLEU score\"\"\"\n",
    "        \n",
    "        # Tokenize\n",
    "        generated_tokens = self.tokenizer.tokenize(generated_code, language)\n",
    "        reference_tokens = self.tokenizer.tokenize(reference_code, language)\n",
    "        \n",
    "        # Calculate components\n",
    "        ngram_bleu = self.calculate_ngram_bleu(generated_tokens, reference_tokens)\n",
    "        weighted_bleu = self.calculate_weighted_bleu(generated_tokens, reference_tokens, language)\n",
    "        ast_similarity = self.calculate_ast_similarity(generated_code, reference_code, language)\n",
    "        dataflow_similarity = self.calculate_dataflow_similarity(generated_code, reference_code, language)\n",
    "        \n",
    "        # Combine with weights\n",
    "        code_bleu = (\n",
    "            self.alpha * ngram_bleu +\n",
    "            self.beta * weighted_bleu +\n",
    "            self.gamma * ast_similarity +\n",
    "            self.delta * dataflow_similarity\n",
    "        )\n",
    "        \n",
    "        # Calculate exact match for comparison\n",
    "        exact_match = 1.0 if generated_tokens == reference_tokens else 0.0\n",
    "        \n",
    "        # Token overlap\n",
    "        if reference_tokens:\n",
    "            common_tokens = set(generated_tokens) & set(reference_tokens)\n",
    "            token_overlap = len(common_tokens) / len(set(reference_tokens))\n",
    "        else:\n",
    "            token_overlap = 0.0\n",
    "        \n",
    "        return EvaluationResult(\n",
    "            exact_match=exact_match,\n",
    "            code_bleu=code_bleu,\n",
    "            bleu_components={\n",
    "                'ngram_bleu': ngram_bleu,\n",
    "                'weighted_bleu': weighted_bleu,\n",
    "                'ast_similarity': ast_similarity,\n",
    "                'dataflow_similarity': dataflow_similarity\n",
    "            },\n",
    "            ast_similarity=ast_similarity,\n",
    "            token_overlap=token_overlap\n",
    "        )\n",
    "\n",
    "print(\"🔧 Evaluation metrics implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases: Diverse Code Examples\n",
    "\n",
    "Tạo diverse test cases để demonstrate differences giữa EM và CodeBLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"Test case for evaluation metrics comparison\"\"\"\n",
    "    case_id: str\n",
    "    description: str\n",
    "    generated_code: str\n",
    "    reference_code: str\n",
    "    language: str\n",
    "    expected_em: float\n",
    "    expected_relationship: str  # \"em_higher\", \"codebleu_higher\", \"similar\"\n",
    "\n",
    "def create_comprehensive_test_cases() -> List[TestCase]:\n",
    "    \"\"\"Create comprehensive test cases covering different scenarios\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        # Case 1: Perfect match\n",
    "        TestCase(\n",
    "            case_id=\"perfect_match\",\n",
    "            description=\"Identical code - both metrics should be 1.0\",\n",
    "            generated_code=\"String result = condition ? \\\"true\\\" : \\\"false\\\";\",\n",
    "            reference_code=\"String result = condition ? \\\"true\\\" : \\\"false\\\";\",\n",
    "            language=\"java\",\n",
    "            expected_em=1.0,\n",
    "            expected_relationship=\"similar\"\n",
    "        ),\n",
    "        \n",
    "        # Case 2: Whitespace differences  \n",
    "        TestCase(\n",
    "            case_id=\"whitespace_diff\",\n",
    "            description=\"Different whitespace - EM should be same, CodeBLEU might be more robust\",\n",
    "            generated_code=\"String result=condition?\\\"true\\\":\\\"false\\\";\",\n",
    "            reference_code=\"String result = condition ? \\\"true\\\" : \\\"false\\\";\",\n",
    "            language=\"java\",\n",
    "            expected_em=1.0,  # After tokenization, should be same\n",
    "            expected_relationship=\"similar\"\n",
    "        ),\n",
    "        \n",
    "        # Case 3: Semantically equivalent but syntactically different\n",
    "        TestCase(\n",
    "            case_id=\"semantic_equivalent\",\n",
    "            description=\"Semantically equivalent - CodeBLEU should be higher than EM\",\n",
    "            generated_code=\"if (condition) { result = \\\"true\\\"; } else { result = \\\"false\\\"; }\",\n",
    "            reference_code=\"String result = condition ? \\\"true\\\" : \\\"false\\\";\",\n",
    "            language=\"java\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 4: Variable name changes\n",
    "        TestCase(\n",
    "            case_id=\"variable_names\",\n",
    "            description=\"Different variable names - CodeBLEU should be more forgiving\",\n",
    "            generated_code=\"String flag = status ? \\\"active\\\" : \\\"inactive\\\";\",\n",
    "            reference_code=\"String result = condition ? \\\"true\\\" : \\\"false\\\";\",\n",
    "            language=\"java\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 5: Partial correctness\n",
    "        TestCase(\n",
    "            case_id=\"partial_correct\",\n",
    "            description=\"Partially correct code - CodeBLEU gives partial credit\",\n",
    "            generated_code=\"for (String item : items) { process(item); }\",\n",
    "            reference_code=\"for (String item : items) { process(item); validate(item); }\",\n",
    "            language=\"java\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 6: Different control structures\n",
    "        TestCase(\n",
    "            case_id=\"control_structure\",\n",
    "            description=\"Different loop types but same effect\",\n",
    "            generated_code=\"for (int i = 0; i < items.size(); i++) { process(items.get(i)); }\",\n",
    "            reference_code=\"for (String item : items) { process(item); }\",\n",
    "            language=\"java\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 7: Wrong code (low similarity)\n",
    "        TestCase(\n",
    "            case_id=\"wrong_code\",\n",
    "            description=\"Completely different code - both metrics should be low\",\n",
    "            generated_code=\"System.out.println(\\\"Hello World\\\");\",\n",
    "            reference_code=\"String result = condition ? \\\"true\\\" : \\\"false\\\";\",\n",
    "            language=\"java\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"similar\"\n",
    "        ),\n",
    "        \n",
    "        # Case 8: Python example - function definition\n",
    "        TestCase(\n",
    "            case_id=\"python_function\",\n",
    "            description=\"Python function with different parameter names\",\n",
    "            generated_code=\"def calculate_sum(numbers): return sum(numbers)\",\n",
    "            reference_code=\"def calculate_sum(values): return sum(values)\",\n",
    "            language=\"python\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 9: JavaScript arrow function vs regular function\n",
    "        TestCase(\n",
    "            case_id=\"js_function_style\",\n",
    "            description=\"Different function syntax in JavaScript\",\n",
    "            generated_code=\"const multiply = (a, b) => a * b;\",\n",
    "            reference_code=\"function multiply(a, b) { return a * b; }\",\n",
    "            language=\"javascript\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 10: Go error handling styles\n",
    "        TestCase(\n",
    "            case_id=\"go_error_handling\",\n",
    "            description=\"Different error handling approaches in Go\",\n",
    "            generated_code=\"if err != nil { return err }\",\n",
    "            reference_code=\"if err != nil { log.Fatal(err) }\",\n",
    "            language=\"go\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 11: Complex Java example - method with different implementation\n",
    "        TestCase(\n",
    "            case_id=\"java_method_impl\",\n",
    "            description=\"Same method signature, different implementation style\",\n",
    "            generated_code=\"public boolean isEmpty() { return size == 0; }\",\n",
    "            reference_code=\"public boolean isEmpty() { return this.size() == 0; }\",\n",
    "            language=\"java\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        ),\n",
    "        \n",
    "        # Case 12: Operator precedence (subtle difference)\n",
    "        TestCase(\n",
    "            case_id=\"operator_precedence\",\n",
    "            description=\"Different parentheses usage\",\n",
    "            generated_code=\"result = (a + b) * c;\",\n",
    "            reference_code=\"result = a + b * c;\",\n",
    "            language=\"java\",\n",
    "            expected_em=0.0,\n",
    "            expected_relationship=\"codebleu_higher\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "# Create test cases\n",
    "test_cases = create_comprehensive_test_cases()\n",
    "\n",
    "print(f\"📝 Created {len(test_cases)} comprehensive test cases:\")\n",
    "for case in test_cases[:5]:  # Show first 5\n",
    "    print(f\"   • {case.case_id}: {case.description}\")\n",
    "print(f\"   ... và {len(test_cases)-5} cases khác\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Evaluation\n",
    "\n",
    "Run evaluation trên all test cases và analyze differences giữa EM và CodeBLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparative_evaluation(test_cases: List[TestCase]) -> pd.DataFrame:\n",
    "    \"\"\"Run comparative evaluation of EM vs CodeBLEU\"\"\"\n",
    "    \n",
    "    print(\"🔍 Running comparative evaluation...\")\n",
    "    \n",
    "    # Initialize evaluators\n",
    "    em_evaluator = ExactMatchEvaluator()\n",
    "    codebleu_evaluator = SimplifiedCodeBLEUEvaluator()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(f\"Evaluating {case.case_id}...\", end=\"\\r\")\n",
    "        \n",
    "        # EM evaluation\n",
    "        em_score = em_evaluator.evaluate(case.generated_code, case.reference_code, case.language)\n",
    "        em_details = em_evaluator.detailed_comparison(case.generated_code, case.reference_code, case.language)\n",
    "        \n",
    "        # CodeBLEU evaluation\n",
    "        codebleu_result = codebleu_evaluator.evaluate(case.generated_code, case.reference_code, case.language)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'case_id': case.case_id,\n",
    "            'description': case.description,\n",
    "            'language': case.language,\n",
    "            'em_score': em_score,\n",
    "            'codebleu_score': codebleu_result.code_bleu,\n",
    "            'ngram_bleu': codebleu_result.bleu_components['ngram_bleu'],\n",
    "            'weighted_bleu': codebleu_result.bleu_components['weighted_bleu'],\n",
    "            'ast_similarity': codebleu_result.bleu_components['ast_similarity'],\n",
    "            'dataflow_similarity': codebleu_result.bleu_components['dataflow_similarity'],\n",
    "            'token_overlap': em_details['token_overlap'],\n",
    "            'sequence_similarity': em_details['sequence_similarity'],\n",
    "            'length_ratio': em_details['length_ratio'],\n",
    "            'expected_relationship': case.expected_relationship,\n",
    "            'generated_length': em_details['generated_length'],\n",
    "            'reference_length': em_details['reference_length']\n",
    "        }\n",
    "        \n",
    "        # Calculate differences\n",
    "        result['codebleu_minus_em'] = result['codebleu_score'] - result['em_score']\n",
    "        result['advantage'] = 'CodeBLEU' if result['codebleu_score'] > result['em_score'] else ('EM' if result['em_score'] > result['codebleu_score'] else 'Tie')\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    print(\"\\n✅ Evaluation completed!\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_df = run_comparative_evaluation(test_cases)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "print(f\"Total test cases: {len(evaluation_df)}\")\n",
    "print(f\"Cases where CodeBLEU > EM: {len(evaluation_df[evaluation_df['advantage'] == 'CodeBLEU'])}\")\n",
    "print(f\"Cases where EM > CodeBLEU: {len(evaluation_df[evaluation_df['advantage'] == 'EM'])}\")\n",
    "print(f\"Ties: {len(evaluation_df[evaluation_df['advantage'] == 'Tie'])}\")\n",
    "\n",
    "print(\"\\n🔍 Detailed Results (first 5 cases):\")\n",
    "display_cols = ['case_id', 'em_score', 'codebleu_score', 'advantage', 'token_overlap']\n",
    "print(evaluation_df[display_cols].head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization và Analysis\n",
    "\n",
    "Create comprehensive visualizations để understand metric behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visualizations for metric comparison\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    \n",
    "    # Plot 1: EM vs CodeBLEU scatter plot\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Color by advantage\n",
    "    colors = {'CodeBLEU': 'red', 'EM': 'blue', 'Tie': 'gray'}\n",
    "    for advantage, group in df.groupby('advantage'):\n",
    "        ax1.scatter(group['em_score'], group['codebleu_score'], \n",
    "                   c=colors[advantage], label=f'{advantage} advantage', \n",
    "                   alpha=0.7, s=60)\n",
    "    \n",
    "    # Add diagonal line (perfect correlation)\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect correlation')\n",
    "    \n",
    "    ax1.set_xlabel('Exact Match Score')\n",
    "    ax1.set_ylabel('CodeBLEU Score')\n",
    "    ax1.set_title('EM vs CodeBLEU Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(-0.05, 1.05)\n",
    "    ax1.set_ylim(-0.05, 1.05)\n",
    "    \n",
    "    # Plot 2: Score differences by case\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    case_indices = range(len(df))\n",
    "    ax2.bar(case_indices, df['codebleu_minus_em'], \n",
    "           color=['red' if x > 0 else 'blue' for x in df['codebleu_minus_em']],\n",
    "           alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Test Case Index')\n",
    "    ax2.set_ylabel('CodeBLEU - EM')\n",
    "    ax2.set_title('Score Differences by Test Case')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add case labels\n",
    "    ax2.set_xticks(case_indices[::2])  # Every other case\n",
    "    ax2.set_xticklabels([df.iloc[i]['case_id'][:8] for i in case_indices[::2]], rotation=45)\n",
    "    \n",
    "    # Plot 3: Component analysis (CodeBLEU breakdown)\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    components = ['ngram_bleu', 'weighted_bleu', 'ast_similarity', 'dataflow_similarity']\n",
    "    component_means = [df[comp].mean() for comp in components]\n",
    "    \n",
    "    bars = ax3.bar(components, component_means, \n",
    "                   color=['skyblue', 'lightgreen', 'orange', 'pink'], alpha=0.8)\n",
    "    \n",
    "    ax3.set_ylabel('Average Score')\n",
    "    ax3.set_title('CodeBLEU Component Analysis')\n",
    "    ax3.set_xticklabels([c.replace('_', '\\n') for c in components], rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, component_means):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 4: Language-specific comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    lang_comparison = df.groupby('language')[['em_score', 'codebleu_score']].mean()\n",
    "    \n",
    "    x_pos = np.arange(len(lang_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x_pos - width/2, lang_comparison['em_score'], width, \n",
    "           label='EM', alpha=0.8, color='blue')\n",
    "    ax4.bar(x_pos + width/2, lang_comparison['codebleu_score'], width, \n",
    "           label='CodeBLEU', alpha=0.8, color='red')\n",
    "    \n",
    "    ax4.set_xlabel('Programming Language')\n",
    "    ax4.set_ylabel('Average Score')\n",
    "    ax4.set_title('Language-specific Performance')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(lang_comparison.index)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Token overlap vs metrics relationship\n",
    "    ax5 = axes[2, 0]\n",
    "    \n",
    "    ax5.scatter(df['token_overlap'], df['em_score'], \n",
    "               alpha=0.7, color='blue', label='EM', s=60)\n",
    "    ax5.scatter(df['token_overlap'], df['codebleu_score'], \n",
    "               alpha=0.7, color='red', label='CodeBLEU', s=60)\n",
    "    \n",
    "    ax5.set_xlabel('Token Overlap')\n",
    "    ax5.set_ylabel('Score')\n",
    "    ax5.set_title('Token Overlap vs Metric Scores')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Metric advantage distribution\n",
    "    ax6 = axes[2, 1]\n",
    "    \n",
    "    advantage_counts = df['advantage'].value_counts()\n",
    "    colors_pie = [colors[adv] for adv in advantage_counts.index]\n",
    "    \n",
    "    wedges, texts, autotexts = ax6.pie(advantage_counts.values, \n",
    "                                      labels=advantage_counts.index,\n",
    "                                      colors=colors_pie,\n",
    "                                      autopct='%1.1f%%',\n",
    "                                      startangle=90)\n",
    "    \n",
    "    ax6.set_title('Metric Advantage Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create correlation analysis\n",
    "    print(\"\\n📊 Correlation Analysis:\")\n",
    "    corr_matrix = df[['em_score', 'codebleu_score', 'token_overlap', 'sequence_similarity', 'ast_similarity']].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
    "                square=True, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    plt.title('Correlation Matrix of Evaluation Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistical summary\n",
    "    print(\"\\n📈 Statistical Summary:\")\n",
    "    print(f\"EM - Mean: {df['em_score'].mean():.3f}, Std: {df['em_score'].std():.3f}\")\n",
    "    print(f\"CodeBLEU - Mean: {df['codebleu_score'].mean():.3f}, Std: {df['codebleu_score'].std():.3f}\")\n",
    "    print(f\"Correlation (EM, CodeBLEU): {df['em_score'].corr(df['codebleu_score']):.3f}\")\n",
    "    \n",
    "    # Identify most interesting cases\n",
    "    print(\"\\n🔍 Most Interesting Cases:\")\n",
    "    \n",
    "    # Largest CodeBLEU advantage\n",
    "    max_codebleu_adv = df.loc[df['codebleu_minus_em'].idxmax()]\n",
    "    print(f\"Largest CodeBLEU advantage: {max_codebleu_adv['case_id']} (+{max_codebleu_adv['codebleu_minus_em']:.3f})\")\n",
    "    \n",
    "    # Perfect EM but low CodeBLEU (if any)\n",
    "    perfect_em_low_cb = df[(df['em_score'] == 1.0) & (df['codebleu_score'] < 0.9)]\n",
    "    if not perfect_em_low_cb.empty:\n",
    "        print(f\"Perfect EM, low CodeBLEU: {perfect_em_low_cb['case_id'].iloc[0]}\")\n",
    "    \n",
    "    # High CodeBLEU but zero EM\n",
    "    high_cb_zero_em = df[(df['em_score'] == 0.0) & (df['codebleu_score'] > 0.5)]\n",
    "    if not high_cb_zero_em.empty:\n",
    "        best_case = high_cb_zero_em.loc[high_cb_zero_em['codebleu_score'].idxmax()]\n",
    "        print(f\"High CodeBLEU, zero EM: {best_case['case_id']} (CodeBLEU: {best_case['codebleu_score']:.3f})\")\n",
    "\n",
    "# Create visualizations\n",
    "create_comprehensive_visualizations(evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study Analysis\n",
    "\n",
    "Deep dive vào specific cases để understand metric behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_specific_cases(df: pd.DataFrame, test_cases: List[TestCase]):\n",
    "    \"\"\"Analyze specific cases in detail\"\"\"\n",
    "    \n",
    "    print(\"🔍 Detailed Case Study Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select interesting cases for analysis\n",
    "    interesting_cases = [\n",
    "        'semantic_equivalent',  # Should show CodeBLEU advantage\n",
    "        'partial_correct',      # Partial credit scenario\n",
    "        'control_structure',    # Different but equivalent structures\n",
    "        'wrong_code',          # Both should be low\n",
    "        'perfect_match'        # Both should be high\n",
    "    ]\n",
    "    \n",
    "    tokenizer = CodeTokenizer()\n",
    "    \n",
    "    for case_id in interesting_cases:\n",
    "        if case_id not in df['case_id'].values:\n",
    "            continue\n",
    "            \n",
    "        # Get case data\n",
    "        case_row = df[df['case_id'] == case_id].iloc[0]\n",
    "        test_case = next(tc for tc in test_cases if tc.case_id == case_id)\n",
    "        \n",
    "        print(f\"\\n📋 Case: {case_id.upper()}\")\n",
    "        print(f\"Description: {test_case.description}\")\n",
    "        print(f\"Language: {test_case.language}\")\n",
    "        \n",
    "        print(f\"\\nGenerated: {test_case.generated_code}\")\n",
    "        print(f\"Reference: {test_case.reference_code}\")\n",
    "        \n",
    "        # Tokenization analysis\n",
    "        gen_tokens = tokenizer.tokenize(test_case.generated_code, test_case.language)\n",
    "        ref_tokens = tokenizer.tokenize(test_case.reference_code, test_case.language)\n",
    "        \n",
    "        print(f\"\\n🔤 Tokenization:\")\n",
    "        print(f\"Generated tokens: {gen_tokens}\")\n",
    "        print(f\"Reference tokens: {ref_tokens}\")\n",
    "        \n",
    "        # Token differences\n",
    "        common_tokens = set(gen_tokens) & set(ref_tokens)\n",
    "        gen_only = set(gen_tokens) - set(ref_tokens)\n",
    "        ref_only = set(ref_tokens) - set(gen_tokens)\n",
    "        \n",
    "        print(f\"Common tokens: {common_tokens}\")\n",
    "        if gen_only:\n",
    "            print(f\"Generated only: {gen_only}\")\n",
    "        if ref_only:\n",
    "            print(f\"Reference only: {ref_only}\")\n",
    "        \n",
    "        # Scores\n",
    "        print(f\"\\n📊 Scores:\")\n",
    "        print(f\"EM Score: {case_row['em_score']:.4f}\")\n",
    "        print(f\"CodeBLEU Score: {case_row['codebleu_score']:.4f}\")\n",
    "        print(f\"Difference (CB - EM): {case_row['codebleu_minus_em']:.4f}\")\n",
    "        \n",
    "        # Component breakdown\n",
    "        print(f\"\\n🧩 CodeBLEU Components:\")\n",
    "        print(f\"N-gram BLEU: {case_row['ngram_bleu']:.4f}\")\n",
    "        print(f\"Weighted BLEU: {case_row['weighted_bleu']:.4f}\")\n",
    "        print(f\"AST Similarity: {case_row['ast_similarity']:.4f}\")\n",
    "        print(f\"DataFlow Similarity: {case_row['dataflow_similarity']:.4f}\")\n",
    "        \n",
    "        # Additional metrics\n",
    "        print(f\"\\n📏 Additional Metrics:\")\n",
    "        print(f\"Token Overlap: {case_row['token_overlap']:.4f}\")\n",
    "        print(f\"Sequence Similarity: {case_row['sequence_similarity']:.4f}\")\n",
    "        print(f\"Length Ratio: {case_row['length_ratio']:.4f}\")\n",
    "        \n",
    "        # Analysis\n",
    "        print(f\"\\n💡 Analysis:\")\n",
    "        if case_row['em_score'] == 0 and case_row['codebleu_score'] > 0.3:\n",
    "            print(\"✅ CodeBLEU successfully captures semantic similarity despite syntax differences\")\n",
    "        elif case_row['em_score'] == case_row['codebleu_score'] == 1.0:\n",
    "            print(\"✅ Perfect match - both metrics agree\")\n",
    "        elif case_row['em_score'] > case_row['codebleu_score']:\n",
    "            print(\"⚠️  EM higher than CodeBLEU - possible tokenization artifacts\")\n",
    "        else:\n",
    "            print(\"📊 Mixed results - context-dependent evaluation\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Summary insights\n",
    "    print(\"\\n🎯 Key Insights from Case Studies:\")\n",
    "    \n",
    "    semantic_cases = df[df['case_id'].isin(['semantic_equivalent', 'control_structure', 'js_function_style'])]\n",
    "    if not semantic_cases.empty:\n",
    "        avg_em = semantic_cases['em_score'].mean()\n",
    "        avg_cb = semantic_cases['codebleu_score'].mean()\n",
    "        print(f\"• Semantically equivalent cases: EM={avg_em:.3f}, CodeBLEU={avg_cb:.3f}\")\n",
    "        print(f\"  → CodeBLEU gives {((avg_cb - avg_em) / avg_em * 100):.1f}% higher scores for semantic equivalence\")\n",
    "    \n",
    "    partial_cases = df[df['case_id'].isin(['partial_correct', 'variable_names'])]\n",
    "    if not partial_cases.empty:\n",
    "        avg_em = partial_cases['em_score'].mean()\n",
    "        avg_cb = partial_cases['codebleu_score'].mean()\n",
    "        print(f\"• Partial correctness cases: EM={avg_em:.3f}, CodeBLEU={avg_cb:.3f}\")\n",
    "        print(f\"  → CodeBLEU provides partial credit while EM gives zero\")\n",
    "    \n",
    "    # Component importance analysis\n",
    "    print(f\"\\n🧩 Component Importance:\")\n",
    "    components = ['ngram_bleu', 'weighted_bleu', 'ast_similarity', 'dataflow_similarity']\n",
    "    \n",
    "    for comp in components:\n",
    "        corr_with_total = df[comp].corr(df['codebleu_score'])\n",
    "        print(f\"• {comp.replace('_', ' ').title()}: correlation with CodeBLEU = {corr_with_total:.3f}\")\n",
    "    \n",
    "    # Recommendation based on analysis\n",
    "    print(f\"\\n💡 Practical Recommendations:\")\n",
    "    \n",
    "    high_agreement = len(df[abs(df['codebleu_minus_em']) < 0.1]) / len(df)\n",
    "    print(f\"• Metrics agree (±0.1) in {high_agreement:.1%} of cases\")\n",
    "    \n",
    "    codebleu_advantage = len(df[df['codebleu_minus_em'] > 0.2]) / len(df)\n",
    "    print(f\"• CodeBLEU shows significant advantage (>0.2) in {codebleu_advantage:.1%} of cases\")\n",
    "    \n",
    "    if codebleu_advantage > 0.3:\n",
    "        print(\"  → Recommend using CodeBLEU for more nuanced evaluation\")\n",
    "    else:\n",
    "        print(\"  → Both metrics provide valuable but different perspectives\")\n",
    "\n",
    "# Run case study analysis\n",
    "analyze_specific_cases(evaluation_df, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Findings Validation\n",
    "\n",
    "Validate paper's usage of both metrics và understand their complementary nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_paper_metric_usage(df: pd.DataFrame):\n",
    "    \"\"\"Validate paper's approach to using both EM and CodeBLEU\"\"\"\n",
    "    \n",
    "    print(\"📄 Validating Paper's Metric Usage\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Paper's rationale analysis\n",
    "    print(\"\\n📊 Paper's Rationale for Using Both Metrics:\")\n",
    "    print(\"\\n1. EXACT MATCH (EM):\")\n",
    "    print(\"   • Widely used in code review automation literature\")\n",
    "    print(\"   • Provides strict, unambiguous evaluation\")\n",
    "    print(\"   • Binary outcome: perfect match or not\")\n",
    "    print(\"   • Easy to interpret and compare across studies\")\n",
    "    \n",
    "    print(\"\\n2. CODEBLEU:\")\n",
    "    print(\"   • Addresses BLEU's limitations for code evaluation\")\n",
    "    print(\"   • Considers syntactic (AST) and semantic (DF) information\")\n",
    "    print(\"   • Provides partial credit for similar solutions\")\n",
    "    print(\"   • More robust to formatting and style differences\")\n",
    "    \n",
    "    # Statistical analysis of paper's approach\n",
    "    print(f\"\\n📈 Statistical Validation:\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlation = df['em_score'].corr(df['codebleu_score'])\n",
    "    print(f\"\\n• Correlation between EM and CodeBLEU: {correlation:.3f}\")\n",
    "    \n",
    "    if correlation > 0.7:\n",
    "        print(\"  ✅ Strong positive correlation - metrics generally agree\")\n",
    "    elif correlation > 0.4:\n",
    "        print(\"  ⚠️  Moderate correlation - metrics capture different aspects\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Weak correlation - metrics measure very different things\")\n",
    "    \n",
    "    # Disagreement analysis\n",
    "    strong_disagreement = df[abs(df['codebleu_minus_em']) > 0.3]\n",
    "    print(f\"\\n• Cases with strong disagreement (>0.3 difference): {len(strong_disagreement)}/{len(df)}\")\n",
    "    \n",
    "    if not strong_disagreement.empty:\n",
    "        print(\"  Disagreement cases:\")\n",
    "        for _, row in strong_disagreement.iterrows():\n",
    "            print(f\"    - {row['case_id']}: EM={row['em_score']:.3f}, CB={row['codebleu_score']:.3f}\")\n",
    "    \n",
    "    # Sensitivity analysis\n",
    "    print(f\"\\n📏 Sensitivity Analysis:\")\n",
    "    \n",
    "    # EM sensitivity to small changes\n",
    "    em_zero_count = len(df[df['em_score'] == 0])\n",
    "    em_one_count = len(df[df['em_score'] == 1])\n",
    "    em_middle_count = len(df) - em_zero_count - em_one_count\n",
    "    \n",
    "    print(f\"\\n• EM Distribution:\")\n",
    "    print(f\"  - Zero (no match): {em_zero_count}/{len(df)} ({em_zero_count/len(df):.1%})\")\n",
    "    print(f\"  - One (perfect match): {em_one_count}/{len(df)} ({em_one_count/len(df):.1%})\")\n",
    "    print(f\"  - Middle values: {em_middle_count}/{len(df)} ({em_middle_count/len(df):.1%})\")\n",
    "    \n",
    "    if em_middle_count == 0:\n",
    "        print(\"  → EM is purely binary (0 or 1) - very strict evaluation\")\n",
    "    \n",
    "    # CodeBLEU distribution\n",
    "    cb_quartiles = df['codebleu_score'].quantile([0.25, 0.5, 0.75])\n",
    "    print(f\"\\n• CodeBLEU Distribution:\")\n",
    "    print(f\"  - Q1 (25th percentile): {cb_quartiles[0.25]:.3f}\")\n",
    "    print(f\"  - Median: {cb_quartiles[0.5]:.3f}\")\n",
    "    print(f\"  - Q3 (75th percentile): {cb_quartiles[0.75]:.3f}\")\n",
    "    print(f\"  → CodeBLEU provides gradual scoring across full range\")\n",
    "    \n",
    "    # Information content analysis\n",
    "    print(f\"\\n🔍 Information Content Analysis:\")\n",
    "    \n",
    "    # Entropy calculation (measure of information)\n",
    "    def calculate_entropy(scores, bins=10):\n",
    "        hist, _ = np.histogram(scores, bins=bins, range=(0, 1))\n",
    "        hist = hist / hist.sum()  # Normalize\n",
    "        hist = hist[hist > 0]  # Remove zeros\n",
    "        return -np.sum(hist * np.log2(hist))\n",
    "    \n",
    "    em_entropy = calculate_entropy(df['em_score'])\n",
    "    cb_entropy = calculate_entropy(df['codebleu_score'])\n",
    "    \n",
    "    print(f\"\\n• Information Entropy:\")\n",
    "    print(f\"  - EM entropy: {em_entropy:.3f} bits\")\n",
    "    print(f\"  - CodeBLEU entropy: {cb_entropy:.3f} bits\")\n",
    "    \n",
    "    if cb_entropy > em_entropy:\n",
    "        print(f\"  → CodeBLEU provides {((cb_entropy - em_entropy) / em_entropy * 100):.1f}% more information\")\n",
    "    \n",
    "    # Practical implications from paper\n",
    "    print(f\"\\n🎯 Practical Implications (from Paper Context):\")\n",
    "    \n",
    "    print(f\"\\n1. FOR RESEARCH COMPARISON:\")\n",
    "    print(f\"   • Use EM for compatibility with prior work\")\n",
    "    print(f\"   • Report both metrics for comprehensive evaluation\")\n",
    "    print(f\"   • EM enables direct comparison with existing approaches\")\n",
    "    \n",
    "    print(f\"\\n2. FOR MODEL DEVELOPMENT:\")\n",
    "    print(f\"   • Use CodeBLEU for training signal (gradual feedback)\")\n",
    "    print(f\"   • Use EM for final evaluation (strict criterion)\")\n",
    "    print(f\"   • Monitor both during fine-tuning process\")\n",
    "    \n",
    "    print(f\"\\n3. FOR PRACTICAL DEPLOYMENT:\")\n",
    "    print(f\"   • CodeBLEU better for user experience (partial credit)\")\n",
    "    print(f\"   • EM better for automated systems (clear threshold)\")\n",
    "    print(f\"   • Consider domain-specific weight tuning for CodeBLEU\")\n",
    "    \n",
    "    # Validation of paper's dual-metric approach\n",
    "    print(f\"\\n✅ Validation of Paper's Approach:\")\n",
    "    \n",
    "    # Check if using both metrics provides more insight\n",
    "    em_only_ranking = df.nlargest(5, 'em_score')['case_id'].tolist()\n",
    "    cb_only_ranking = df.nlargest(5, 'codebleu_score')['case_id'].tolist()\n",
    "    \n",
    "    ranking_overlap = len(set(em_only_ranking) & set(cb_only_ranking))\n",
    "    print(f\"\\n• Top-5 ranking overlap: {ranking_overlap}/5 cases\")\n",
    "    \n",
    "    if ranking_overlap < 5:\n",
    "        print(f\"  → Using both metrics reveals different aspects of quality\")\n",
    "        print(f\"  → Paper's dual-metric approach is justified\")\n",
    "    else:\n",
    "        print(f\"  → Metrics largely agree on quality ranking\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    print(f\"\\n🎯 FINAL VALIDATION:\")\n",
    "    print(f\"\\n✅ Paper's use of both EM and CodeBLEU is JUSTIFIED because:\")\n",
    "    print(f\"   1. EM provides strict, literature-compatible evaluation\")\n",
    "    print(f\"   2. CodeBLEU captures semantic similarity missed by EM\")\n",
    "    print(f\"   3. Together they provide comprehensive code quality assessment\")\n",
    "    print(f\"   4. Different use cases benefit from different metric emphases\")\n",
    "    \n",
    "    if correlation < 0.9:\n",
    "        print(f\"   5. Metrics are sufficiently different to justify using both\")\n",
    "    \n",
    "    return {\n",
    "        'correlation': correlation,\n",
    "        'em_entropy': em_entropy,\n",
    "        'cb_entropy': cb_entropy,\n",
    "        'disagreement_cases': len(strong_disagreement),\n",
    "        'ranking_overlap': ranking_overlap\n",
    "    }\n",
    "\n",
    "# Validate paper's metric usage\n",
    "validation_results = validate_paper_metric_usage(evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation Guide\n",
    "\n",
    "Generate practical guide for implementing evaluation metrics in code review systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_implementation_guide(validation_results: Dict[str, float], df: pd.DataFrame):\n",
    "    \"\"\"Generate practical implementation guide for evaluation metrics\"\"\"\n",
    "    \n",
    "    guide = f\"\"\"\n",
    "# 🛠️ Practical Implementation Guide: EM vs CodeBLEU\n",
    "\n",
    "## 📊 Key Findings from Analysis\n",
    "\n",
    "### Metric Characteristics\n",
    "- **Correlation**: {validation_results['correlation']:.3f} (moderate positive correlation)\n",
    "- **Information Content**: CodeBLEU provides {((validation_results['cb_entropy'] - validation_results['em_entropy']) / validation_results['em_entropy'] * 100):.1f}% more information\n",
    "- **Disagreement Cases**: {validation_results['disagreement_cases']}/{len(df)} cases show significant differences\n",
    "- **Ranking Agreement**: {validation_results['ranking_overlap']}/5 top cases overlap\n",
    "\n",
    "### When Each Metric Excels\n",
    "\n",
    "#### Exact Match (EM) is Better For:\n",
    "- 🎯 **Binary decision making** (accept/reject code)\n",
    "- 📚 **Literature comparison** (standardized across papers)\n",
    "- 🔍 **Debugging evaluation** (clear pass/fail)\n",
    "- ⚡ **Fast computation** (simple token comparison)\n",
    "- 📊 **Statistical significance** (clear 0/1 outcomes)\n",
    "\n",
    "#### CodeBLEU is Better For:\n",
    "- 🎓 **Training feedback** (gradual improvement signal)\n",
    "- 👥 **User experience** (partial credit recognition)\n",
    "- 🔄 **Iterative development** (progress tracking)\n",
    "- 🌐 **Cross-language evaluation** (structural similarity)\n",
    "- 🎨 **Style-agnostic assessment** (semantic focus)\n",
    "\n",
    "## 🔧 Implementation Recipes\n",
    "\n",
    "### Recipe 1: Research Paper Evaluation\n",
    "```python\n",
    "def research_evaluation(generated_codes, reference_codes):\n",
    "    \"\"\"Standard evaluation for research papers\"\"\"\n",
    "    \n",
    "    em_scores = []\n",
    "    codebleu_scores = []\n",
    "    \n",
    "    for gen, ref in zip(generated_codes, reference_codes):\n",
    "        # Calculate both metrics\n",
    "        em_score = exact_match(gen, ref)\n",
    "        cb_score = code_bleu(gen, ref)\n",
    "        \n",
    "        em_scores.append(em_score)\n",
    "        codebleu_scores.append(cb_score)\n",
    "    \n",
    "    # Report both for comprehensive evaluation\n",
    "    return {{\n",
    "        'exact_match': np.mean(em_scores),\n",
    "        'code_bleu': np.mean(codebleu_scores),\n",
    "        'em_std': np.std(em_scores),\n",
    "        'cb_std': np.std(codebleu_scores)\n",
    "    }}\n",
    "```\n",
    "\n",
    "### Recipe 2: Model Training Feedback\n",
    "```python\n",
    "def training_loss_with_codebleu(model_output, target_code, language='java'):\n",
    "    \"\"\"Use CodeBLEU for training feedback\"\"\"\n",
    "    \n",
    "    # Primary loss (e.g., cross-entropy)\n",
    "    primary_loss = cross_entropy_loss(model_output, target_code)\n",
    "    \n",
    "    # CodeBLEU reward (convert to loss)\n",
    "    cb_score = code_bleu(model_output, target_code, language)\n",
    "    cb_loss = 1.0 - cb_score  # Convert to loss\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = 0.8 * primary_loss + 0.2 * cb_loss\n",
    "    \n",
    "    return total_loss\n",
    "```\n",
    "\n",
    "### Recipe 3: Production System Evaluation\n",
    "```python\n",
    "def production_evaluation(generated_code, reference_code, threshold=0.7):\n",
    "    \"\"\"Dual-metric evaluation for production systems\"\"\"\n",
    "    \n",
    "    em_score = exact_match(generated_code, reference_code)\n",
    "    cb_score = code_bleu(generated_code, reference_code)\n",
    "    \n",
    "    # Decision logic\n",
    "    if em_score == 1.0:\n",
    "        return \"PERFECT\", 1.0\n",
    "    elif cb_score >= threshold:\n",
    "        return \"ACCEPTABLE\", cb_score\n",
    "    else:\n",
    "        return \"REJECT\", min(em_score, cb_score)\n",
    "```\n",
    "\n",
    "### Recipe 4: Fine-tuning Progress Monitoring\n",
    "```python\n",
    "def monitor_finetuning_progress(epoch_results):\n",
    "    \"\"\"Monitor both metrics during fine-tuning\"\"\"\n",
    "    \n",
    "    epochs = list(range(len(epoch_results)))\n",
    "    em_scores = [r['em'] for r in epoch_results]\n",
    "    cb_scores = [r['codebleu'] for r in epoch_results]\n",
    "    \n",
    "    # Plot progress\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, em_scores, 'b-', label='Exact Match')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('EM Score')\n",
    "    plt.title('EM Progress (Strict Evaluation)')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, cb_scores, 'r-', label='CodeBLEU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('CodeBLEU Score')\n",
    "    plt.title('CodeBLEU Progress (Gradual Feedback)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if len(em_scores) >= 3:\n",
    "        em_trend = np.mean(em_scores[-3:])\n",
    "        cb_trend = np.mean(cb_scores[-3:])\n",
    "        \n",
    "        if em_trend > 0.8:  # EM plateau\n",
    "            return \"CONVERGED_EM\"\n",
    "        elif cb_trend > 0.9:  # CodeBLEU plateau\n",
    "            return \"CONVERGED_CB\"\n",
    "    \n",
    "    return \"CONTINUE\"\n",
    "```\n",
    "\n",
    "## ⚙️ Configuration Guidelines\n",
    "\n",
    "### CodeBLEU Weight Tuning\n",
    "```python\n",
    "# Default weights from CodeBLEU paper\n",
    "DEFAULT_WEIGHTS = (0.25, 0.25, 0.25, 0.25)  # BLEU, Weighted-BLEU, AST, DF\n",
    "\n",
    "# Code review specific weights (based on our analysis)\n",
    "CODE_REVIEW_WEIGHTS = (0.2, 0.3, 0.3, 0.2)  # Emphasize structure and keywords\n",
    "\n",
    "# Language-specific adjustments\n",
    "LANGUAGE_WEIGHTS = {{\n",
    "    'java': (0.2, 0.3, 0.3, 0.2),      # Structure-heavy\n",
    "    'python': (0.3, 0.2, 0.25, 0.25),  # More flexible syntax\n",
    "    'javascript': (0.25, 0.25, 0.2, 0.3),  # Dynamic typing emphasis\n",
    "    'go': (0.2, 0.3, 0.35, 0.15)       # Simple, structured\n",
    "}}\n",
    "```\n",
    "\n",
    "### Tokenization Best Practices\n",
    "```python\n",
    "def robust_tokenization(code, language='java'):\n",
    "    \"\"\"Production-ready tokenization\"\"\"\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    code = re.sub(r'\\s+', ' ', code.strip())\n",
    "    \n",
    "    # Language-specific preprocessing\n",
    "    if language == 'python':\n",
    "        # Handle indentation significance\n",
    "        code = re.sub(r'\\n\\s+', ' INDENT ', code)\n",
    "    elif language == 'javascript':\n",
    "        # Handle semicolon insertion\n",
    "        code = re.sub(r'\\n(?!\\s*[}}\\]))', ' NEWLINE ', code)\n",
    "    \n",
    "    # Standard tokenization\n",
    "    tokens = re.findall(r'\\w+|[{{}}()\\[\\];,.<>=!&|+\\-*/\"\\']', code)\n",
    "    return [t.lower() for t in tokens if t.strip()]\n",
    "```\n",
    "\n",
    "## 🚨 Common Pitfalls and Solutions\n",
    "\n",
    "### Pitfall 1: Over-reliance on EM\n",
    "**Problem**: Missing semantically correct solutions\n",
    "**Solution**: Use CodeBLEU for initial filtering, EM for final validation\n",
    "\n",
    "### Pitfall 2: CodeBLEU Weight Sensitivity\n",
    "**Problem**: Results vary with component weights\n",
    "**Solution**: Validate weights on held-out data, use domain-specific tuning\n",
    "\n",
    "### Pitfall 3: Language Bias\n",
    "**Problem**: Metrics favor certain programming languages\n",
    "**Solution**: Language-specific normalization and weight adjustments\n",
    "\n",
    "### Pitfall 4: Evaluation Dataset Quality\n",
    "**Problem**: Poor reference code affects both metrics\n",
    "**Solution**: Multiple reference implementations, human validation\n",
    "\n",
    "## 📈 Performance Optimization\n",
    "\n",
    "### Fast EM Implementation\n",
    "```python\n",
    "def fast_exact_match(generated_codes, reference_codes):\n",
    "    \"\"\"Vectorized EM computation\"\"\"\n",
    "    \n",
    "    # Batch tokenization\n",
    "    gen_tokens = [tokenize(code) for code in generated_codes]\n",
    "    ref_tokens = [tokenize(code) for code in reference_codes]\n",
    "    \n",
    "    # Parallel comparison\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = pool.starmap(\n",
    "            lambda g, r: 1.0 if g == r else 0.0,\n",
    "            zip(gen_tokens, ref_tokens)\n",
    "        )\n",
    "    \n",
    "    return np.array(results)\n",
    "```\n",
    "\n",
    "### Cached CodeBLEU\n",
    "```python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def cached_code_bleu(gen_code, ref_code, language):\n",
    "    \"\"\"Cached CodeBLEU for repeated evaluations\"\"\"\n",
    "    return compute_code_bleu(gen_code, ref_code, language)\n",
    "```\n",
    "\n",
    "## 🎯 Decision Matrix: When to Use Which Metric\n",
    "\n",
    "| Use Case | Primary Metric | Secondary Metric | Rationale |\n",
    "|----------|---------------|------------------|----------|\n",
    "| Research Paper | EM | CodeBLEU | Literature compatibility |\n",
    "| Model Training | CodeBLEU | EM | Gradual feedback |\n",
    "| Production Filtering | CodeBLEU | EM | User experience |\n",
    "| Final Validation | EM | CodeBLEU | Strict quality gate |\n",
    "| Cross-language Study | CodeBLEU | EM | Structural similarity |\n",
    "| Automated Testing | EM | CodeBLEU | Binary pass/fail |\n",
    "| Human Evaluation | CodeBLEU | EM | Partial credit |\n",
    "\n",
    "## 🚀 Advanced Techniques\n",
    "\n",
    "### Ensemble Evaluation\n",
    "```python\n",
    "def ensemble_evaluation(gen_code, ref_code, weights=(0.6, 0.4)):\n",
    "    \"\"\"Combine metrics with learned weights\"\"\"\n",
    "    \n",
    "    em_score = exact_match(gen_code, ref_code)\n",
    "    cb_score = code_bleu(gen_code, ref_code)\n",
    "    \n",
    "    # Weighted combination\n",
    "    ensemble_score = weights[0] * em_score + weights[1] * cb_score\n",
    "    \n",
    "    return ensemble_score\n",
    "```\n",
    "\n",
    "### Adaptive Thresholding\n",
    "```python\n",
    "def adaptive_threshold(cb_score, difficulty_level):\n",
    "    \"\"\"Adjust CodeBLEU threshold based on task difficulty\"\"\"\n",
    "    \n",
    "    base_threshold = 0.7\n",
    "    difficulty_adjustment = {\n",
    "        'easy': 0.1,      # Higher threshold for easy tasks\n",
    "        'medium': 0.0,    # Standard threshold\n",
    "        'hard': -0.15     # Lower threshold for hard tasks\n",
    "    }}\n",
    "    \n",
    "    threshold = base_threshold + difficulty_adjustment.get(difficulty_level, 0.0)\n",
    "    return cb_score >= threshold\n",
    "```\n",
    "\n",
    "## ✅ Validation Checklist\n",
    "\n",
    "Before deploying evaluation metrics:\n",
    "\n",
    "- [ ] **Tokenization tested** on target programming languages\n",
    "- [ ] **Metric correlation analyzed** on your specific dataset\n",
    "- [ ] **Component weights tuned** for your domain\n",
    "- [ ] **Performance benchmarked** for your scale\n",
    "- [ ] **Edge cases handled** (empty code, syntax errors)\n",
    "- [ ] **Human evaluation baseline** established\n",
    "- [ ] **Cross-language consistency** verified\n",
    "- [ ] **Threshold sensitivity** analyzed\n",
    "\n",
    "## 📚 References and Further Reading\n",
    "\n",
    "- **Original CodeBLEU Paper**: Ren et al. \"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis\"\n",
    "- **Code Review Automation**: Li et al. \"Automating Code Review Activities by Large-scale Pre-training\"\n",
    "- **Evaluation Best Practices**: Our analysis shows complementary nature of metrics\n",
    "\"\"\"\n",
    "    \n",
    "    return guide\n",
    "\n",
    "# Generate implementation guide\n",
    "implementation_guide = generate_implementation_guide(validation_results, evaluation_df)\n",
    "print(implementation_guide)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎓 FOCUSED LEARNING COMPLETED: CodeBLEU vs Exact Match Evaluation\")\n",
    "print(\"✅ Deep understanding of evaluation metrics for code generation achieved!\")\n",
    "print(\"🔧 Ready to implement robust evaluation systems for code review automation!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}