{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 3: Advanced Metrics & Analysis\n",
    "\n",
    "## Objective\n",
    "Add sophisticated code review evaluation metrics with statistical analysis and multi-model comparison\n",
    "\n",
    "## Chain of Thought\n",
    "1. Domain-specific metrics → Statistical analysis → Error categorization\n",
    "2. Multi-model comparison → Significance testing → Insights generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1: Import Dependencies and Load Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import Counter, defaultdict\n",
    "import logging\n",
    "\n",
    "# Data handling and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu, wilcoxon, chi2_contingency, pearsonr\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Text analysis\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Code analysis\n",
    "import radon.complexity as radon_complexity\n",
    "import radon.metrics as radon_metrics\n",
    "from radon.visitors import Function\n",
    "\n",
    "# Load Phase 2 components\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "# Visualization (minimal for Phase 3)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('phase3_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('phase3_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-previous-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from Phase 1\n",
    "def load_phase1_data():\n",
    "    \"\"\"Load processed datasets from Phase 1\"\"\"\n",
    "    datasets = {}\n",
    "    processed_dir = 'data/processed'\n",
    "    \n",
    "    if not os.path.exists(processed_dir):\n",
    "        logger.warning(\"Phase 1 data not found. Creating sample data for testing.\")\n",
    "        return create_sample_datasets()\n",
    "    \n",
    "    for filename in os.listdir(processed_dir):\n",
    "        if filename.endswith('_processed.json'):\n",
    "            dataset_name = filename.replace('_processed.json', '')\n",
    "            filepath = os.path.join(processed_dir, filename)\n",
    "            \n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                datasets[dataset_name] = json.load(f)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def create_sample_datasets():\n",
    "    \"\"\"Create sample datasets for testing if Phase 1 data not available\"\"\"\n",
    "    return {\n",
    "        'humaneval': {\n",
    "            'code': [\n",
    "                \"def add(a, b):\\n    return a + b\",\n",
    "                \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\",\n",
    "                \"def bubble_sort(arr):\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n-i-1):\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n    return arr\"\n",
    "            ] * 20,\n",
    "            'reviews': [\n",
    "                \"Simple addition function. Consider adding type hints.\",\n",
    "                \"Recursive factorial implementation. Could add input validation.\",\n",
    "                \"Bubble sort implementation with O(n²) complexity.\"\n",
    "            ] * 20,\n",
    "            'labels': ['positive', 'neutral', 'negative'] * 20\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Load Phase 2 results if available\n",
    "def load_phase2_results():\n",
    "    \"\"\"Load Phase 2 evaluation results\"\"\"\n",
    "    results_files = [f for f in os.listdir('data') if f.startswith('phase2_results_') and f.endswith('.json')]\n",
    "    \n",
    "    if not results_files:\n",
    "        logger.warning(\"No Phase 2 results found. Phase 3 will run basic evaluation first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load the most recent results file\n",
    "    latest_file = sorted(results_files)[-1]\n",
    "    with open(os.path.join('data', latest_file), 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load data\n",
    "datasets = load_phase1_data()\n",
    "phase2_results = load_phase2_results()\n",
    "\n",
    "print(f\"✅ Loaded {len(datasets)} datasets\")\n",
    "if phase2_results:\n",
    "    print(f\"✅ Loaded Phase 2 results with {len(phase2_results.get('individual_results', []))} evaluations\")\n",
    "else:\n",
    "    print(\"⚠️ No Phase 2 results found - will generate sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2: Implement Security Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "security-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurityMetrics:\n",
    "    \"\"\"Advanced security-focused evaluation metrics\"\"\"\n",
    "    \n",
    "    # Security vulnerability patterns\n",
    "    VULNERABILITY_PATTERNS = {\n",
    "        'sql_injection': [\n",
    "            r'execute\\s*\\(.*%.*\\)',\n",
    "            r'query\\s*\\(.*\\+.*\\)',\n",
    "            r'SELECT.*WHERE.*=.*%',\n",
    "            r'cursor\\.execute\\([^?]*%'\n",
    "        ],\n",
    "        'xss': [\n",
    "            r'innerHTML\\s*=.*\\+',\n",
    "            r'document\\.write\\(.*\\+',\n",
    "            r'eval\\(.*input',\n",
    "            r'<script>.*</script>'\n",
    "        ],\n",
    "        'path_traversal': [\n",
    "            r'\\.\\.[\\\\/]',\n",
    "            r'os\\.path\\.join\\(.*input',\n",
    "            r'open\\(.*\\+.*\\)',\n",
    "            r'file\\(.*user_input'\n",
    "        ],\n",
    "        'command_injection': [\n",
    "            r'os\\.system\\(.*\\+',\n",
    "            r'subprocess\\.call\\([^\\[]*%',\n",
    "            r'exec\\(.*input',\n",
    "            r'eval\\(.*request'\n",
    "        ],\n",
    "        'hardcoded_secrets': [\n",
    "            r'password\\s*=\\s*[\"\\'][^\"\\'\n",
    "]*[\"\\']',\n",
    "            r'api_key\\s*=\\s*[\"\\'][^\"\\'\n",
    "]*[\"\\']',\n",
    "            r'secret\\s*=\\s*[\"\\'][^\"\\'\n",
    "]*[\"\\']',\n",
    "            r'token\\s*=\\s*[\"\\'][^\"\\'\n",
    "]*[\"\\']'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Security keywords for review analysis\n",
    "    SECURITY_KEYWORDS = {\n",
    "        'positive': ['secure', 'safe', 'protected', 'validated', 'sanitized', 'encrypted'],\n",
    "        'negative': ['vulnerable', 'insecure', 'unsafe', 'risk', 'exploit', 'attack', 'injection'],\n",
    "        'neutral': ['authentication', 'authorization', 'security', 'check', 'validate']\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def detect_vulnerabilities(cls, code: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Detect potential security vulnerabilities in code\"\"\"\n",
    "        vulnerabilities = {}\n",
    "        \n",
    "        for vuln_type, patterns in cls.VULNERABILITY_PATTERNS.items():\n",
    "            matches = []\n",
    "            for pattern in patterns:\n",
    "                found = re.findall(pattern, code, re.IGNORECASE | re.MULTILINE)\n",
    "                matches.extend(found)\n",
    "            \n",
    "            if matches:\n",
    "                vulnerabilities[vuln_type] = matches\n",
    "        \n",
    "        return vulnerabilities\n",
    "    \n",
    "    @classmethod\n",
    "    def analyze_security_review(cls, review: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze security focus in code review\"\"\"\n",
    "        review_lower = review.lower()\n",
    "        \n",
    "        security_scores = {}\n",
    "        for category, keywords in cls.SECURITY_KEYWORDS.items():\n",
    "            score = sum(1 for keyword in keywords if keyword in review_lower)\n",
    "            security_scores[f'security_{category}'] = score\n",
    "        \n",
    "        # Calculate overall security focus\n",
    "        total_security_mentions = sum(security_scores.values())\n",
    "        security_focus = total_security_mentions / max(len(review.split()), 1)\n",
    "        \n",
    "        return {\n",
    "            **security_scores,\n",
    "            'security_focus_ratio': security_focus,\n",
    "            'mentions_vulnerabilities': security_scores['security_negative'] > 0,\n",
    "            'mentions_protections': security_scores['security_positive'] > 0\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def vulnerability_detection_rate(cls, predicted_review: str, actual_vulnerabilities: Dict[str, List[str]]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate how well the review identifies actual vulnerabilities\"\"\"\n",
    "        review_analysis = cls.analyze_security_review(predicted_review)\n",
    "        \n",
    "        # True positives: review mentions vulnerabilities and they exist\n",
    "        has_vulns = len(actual_vulnerabilities) > 0\n",
    "        mentions_vulns = review_analysis['mentions_vulnerabilities']\n",
    "        \n",
    "        # False positives: review mentions vulnerabilities but none exist\n",
    "        false_positive = mentions_vulns and not has_vulns\n",
    "        \n",
    "        # False negatives: vulnerabilities exist but review doesn't mention them\n",
    "        false_negative = has_vulns and not mentions_vulns\n",
    "        \n",
    "        # True negatives: no vulnerabilities and review doesn't claim any\n",
    "        true_negative = not has_vulns and not mentions_vulns\n",
    "        \n",
    "        return {\n",
    "            'security_precision': 1.0 if not mentions_vulns else (1.0 if has_vulns else 0.0),\n",
    "            'security_recall': 1.0 if not has_vulns else (1.0 if mentions_vulns else 0.0),\n",
    "            'false_positive_rate': 1.0 if false_positive else 0.0,\n",
    "            'false_negative_rate': 1.0 if false_negative else 0.0,\n",
    "            'vulnerability_coverage': len([v for v in actual_vulnerabilities.keys() \n",
    "                                         if any(keyword in predicted_review.lower() \n",
    "                                               for keyword in cls.SECURITY_KEYWORDS['negative'])]) / max(len(actual_vulnerabilities), 1)\n",
    "        }\n",
    "\n",
    "print(\"✅ Security metrics implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3: Implement Style & Readability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "style-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleMetrics:\n",
    "    \"\"\"Code style and readability evaluation metrics\"\"\"\n",
    "    \n",
    "    # Style violation patterns\n",
    "    STYLE_PATTERNS = {\n",
    "        'long_lines': r'.{80,}',  # Lines longer than 80 characters\n",
    "        'no_docstring': r'def\\s+\\w+\\([^)]*\\):\\s*\\n\\s*(?![\"\\']{3})',  # Functions without docstrings\n",
    "        'camelCase': r'\\b[a-z]+[A-Z][a-zA-Z]*\\b',  # camelCase in Python\n",
    "        'single_char_vars': r'\\b[a-zA-Z]\\s*=',  # Single character variable names\n",
    "        'magic_numbers': r'\\b\\d{2,}\\b',  # Magic numbers (2+ digits)\n",
    "        'deep_nesting': r'\\n(\\s{12,})',  # Deep indentation (3+ levels)\n",
    "    }\n",
    "    \n",
    "    # Code quality keywords\n",
    "    QUALITY_KEYWORDS = {\n",
    "        'maintainability': ['refactor', 'modular', 'reusable', 'clean', 'organize'],\n",
    "        'readability': ['readable', 'clear', 'understandable', 'documentation', 'comments'],\n",
    "        'performance': ['efficient', 'optimize', 'performance', 'complexity', 'algorithm'],\n",
    "        'testing': ['test', 'testing', 'coverage', 'assertion', 'mock']\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def analyze_code_style(cls, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze code style violations\"\"\"\n",
    "        style_issues = {}\n",
    "        \n",
    "        for issue_type, pattern in cls.STYLE_PATTERNS.items():\n",
    "            matches = re.findall(pattern, code)\n",
    "            style_issues[f'{issue_type}_count'] = len(matches)\n",
    "        \n",
    "        # Calculate code complexity\n",
    "        try:\n",
    "            # Parse AST for more detailed analysis\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # Count various code elements\n",
    "            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]\n",
    "            classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n",
    "            loops = [node for node in ast.walk(tree) if isinstance(node, (ast.For, ast.While))]\n",
    "            conditionals = [node for node in ast.walk(tree) if isinstance(node, ast.If)]\n",
    "            \n",
    "            style_issues.update({\n",
    "                'function_count': len(functions),\n",
    "                'class_count': len(classes),\n",
    "                'loop_count': len(loops),\n",
    "                'conditional_count': len(conditionals),\n",
    "                'ast_complexity': len(list(ast.walk(tree)))\n",
    "            })\n",
    "            \n",
    "        except SyntaxError:\n",
    "            # If code can't be parsed, set default values\n",
    "            style_issues.update({\n",
    "                'function_count': 0,\n",
    "                'class_count': 0,\n",
    "                'loop_count': 0,\n",
    "                'conditional_count': 0,\n",
    "                'ast_complexity': 0,\n",
    "                'parse_error': True\n",
    "            })\n",
    "        \n",
    "        # Basic metrics\n",
    "        lines = code.split('\\n')\n",
    "        style_issues.update({\n",
    "            'line_count': len(lines),\n",
    "            'avg_line_length': np.mean([len(line) for line in lines]),\n",
    "            'blank_line_ratio': sum(1 for line in lines if not line.strip()) / len(lines),\n",
    "            'comment_ratio': sum(1 for line in lines if line.strip().startswith('#')) / len(lines)\n",
    "        })\n",
    "        \n",
    "        return style_issues\n",
    "    \n",
    "    @classmethod\n",
    "    def analyze_review_quality_focus(cls, review: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze what quality aspects the review focuses on\"\"\"\n",
    "        review_lower = review.lower()\n",
    "        \n",
    "        quality_focus = {}\n",
    "        for category, keywords in cls.QUALITY_KEYWORDS.items():\n",
    "            mentions = sum(1 for keyword in keywords if keyword in review_lower)\n",
    "            quality_focus[f'{category}_focus'] = mentions\n",
    "        \n",
    "        # Review readability\n",
    "        try:\n",
    "            quality_focus.update({\n",
    "                'review_readability_score': flesch_reading_ease(review),\n",
    "                'review_grade_level': flesch_kincaid_grade(review)\n",
    "            })\n",
    "        except:\n",
    "            quality_focus.update({\n",
    "                'review_readability_score': 0,\n",
    "                'review_grade_level': 0\n",
    "            })\n",
    "        \n",
    "        return quality_focus\n",
    "    \n",
    "    @classmethod\n",
    "    def style_improvement_coverage(cls, code: str, review: str) -> Dict[str, float]:\n",
    "        \"\"\"Measure how well review addresses actual style issues\"\"\"\n",
    "        code_issues = cls.analyze_code_style(code)\n",
    "        quality_focus = cls.analyze_review_quality_focus(review)\n",
    "        \n",
    "        # Check if review mentions relevant improvements\n",
    "        coverage_metrics = {}\n",
    "        \n",
    "        # Style issue coverage\n",
    "        if code_issues['long_lines_count'] > 0:\n",
    "            coverage_metrics['mentions_line_length'] = 1.0 if 'line' in review.lower() else 0.0\n",
    "        \n",
    "        if code_issues['no_docstring_count'] > 0:\n",
    "            coverage_metrics['mentions_documentation'] = 1.0 if any(word in review.lower() \n",
    "                                                                   for word in ['doc', 'comment', 'explain']) else 0.0\n",
    "        \n",
    "        if code_issues['deep_nesting_count'] > 0:\n",
    "            coverage_metrics['mentions_complexity'] = 1.0 if any(word in review.lower() \n",
    "                                                               for word in ['complex', 'nest', 'simplify']) else 0.0\n",
    "        \n",
    "        # Overall style awareness\n",
    "        total_issues = sum(v for k, v in code_issues.items() if k.endswith('_count'))\n",
    "        total_quality_mentions = sum(quality_focus.values())\n",
    "        \n",
    "        coverage_metrics['style_awareness_ratio'] = total_quality_mentions / max(total_issues, 1)\n",
    "        \n",
    "        return coverage_metrics\n",
    "\n",
    "print(\"✅ Style metrics implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4: Statistical Analysis Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalAnalysis:\n",
    "    \"\"\"Advanced statistical analysis for evaluation results\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_confidence_interval(data: List[float], confidence: float = 0.95) -> Tuple[float, float, float]:\n",
    "        \"\"\"Calculate confidence interval for metric\"\"\"\n",
    "        if not data:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        data = np.array(data)\n",
    "        mean = np.mean(data)\n",
    "        sem = stats.sem(data)  # Standard error of mean\n",
    "        \n",
    "        # Calculate confidence interval\n",
    "        ci = stats.t.interval(confidence, len(data)-1, loc=mean, scale=sem)\n",
    "        \n",
    "        return mean, ci[0], ci[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_metrics(group1: List[float], group2: List[float], \n",
    "                       test_type: str = 'mannwhitney') -> Dict[str, Any]:\n",
    "        \"\"\"Compare two groups of metric values\"\"\"\n",
    "        if not group1 or not group2:\n",
    "            return {'error': 'Empty groups provided'}\n",
    "        \n",
    "        # Descriptive statistics\n",
    "        desc_stats = {\n",
    "            'group1_mean': np.mean(group1),\n",
    "            'group1_std': np.std(group1),\n",
    "            'group1_median': np.median(group1),\n",
    "            'group2_mean': np.mean(group2),\n",
    "            'group2_std': np.std(group2),\n",
    "            'group2_median': np.median(group2),\n",
    "            'effect_size': (np.mean(group2) - np.mean(group1)) / np.sqrt((np.std(group1)**2 + np.std(group2)**2) / 2)\n",
    "        }\n",
    "        \n",
    "        # Statistical tests\n",
    "        try:\n",
    "            if test_type == 'mannwhitney':\n",
    "                statistic, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                test_name = 'Mann-Whitney U'\n",
    "            elif test_type == 'ttest':\n",
    "                statistic, p_value = stats.ttest_ind(group1, group2)\n",
    "                test_name = 'Independent t-test'\n",
    "            elif test_type == 'wilcoxon' and len(group1) == len(group2):\n",
    "                statistic, p_value = wilcoxon(group1, group2)\n",
    "                test_name = 'Wilcoxon signed-rank'\n",
    "            else:\n",
    "                statistic, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                test_name = 'Mann-Whitney U (default)'\n",
    "        except Exception as e:\n",
    "            return {**desc_stats, 'error': f'Statistical test failed: {str(e)}'}\n",
    "        \n",
    "        # Interpret results\n",
    "        significance_level = 0.05\n",
    "        is_significant = p_value < significance_level\n",
    "        \n",
    "        # Effect size interpretation\n",
    "        effect_size = abs(desc_stats['effect_size'])\n",
    "        if effect_size < 0.2:\n",
    "            effect_interpretation = 'negligible'\n",
    "        elif effect_size < 0.5:\n",
    "            effect_interpretation = 'small'\n",
    "        elif effect_size < 0.8:\n",
    "            effect_interpretation = 'medium'\n",
    "        else:\n",
    "            effect_interpretation = 'large'\n",
    "        \n",
    "        return {\n",
    "            **desc_stats,\n",
    "            'test_name': test_name,\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'is_significant': is_significant,\n",
    "            'effect_interpretation': effect_interpretation\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def multiple_comparisons_correction(p_values: List[float], method: str = 'bonferroni') -> List[float]:\n",
    "        \"\"\"Apply multiple comparisons correction\"\"\"\n",
    "        if method == 'bonferroni':\n",
    "            return [p * len(p_values) for p in p_values]\n",
    "        elif method == 'holm':\n",
    "            # Holm-Bonferroni method\n",
    "            sorted_indices = np.argsort(p_values)\n",
    "            corrected = np.zeros_like(p_values)\n",
    "            \n",
    "            for i, idx in enumerate(sorted_indices):\n",
    "                corrected[idx] = p_values[idx] * (len(p_values) - i)\n",
    "            \n",
    "            return corrected.tolist()\n",
    "        else:\n",
    "            return p_values\n",
    "    \n",
    "    @staticmethod\n",
    "    def correlation_analysis(metric1: List[float], metric2: List[float]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze correlation between two metrics\"\"\"\n",
    "        if len(metric1) != len(metric2) or not metric1:\n",
    "            return {'error': 'Invalid input data'}\n",
    "        \n",
    "        # Pearson correlation\n",
    "        pearson_r, pearson_p = pearsonr(metric1, metric2)\n",
    "        \n",
    "        # Spearman correlation (rank-based)\n",
    "        spearman_r, spearman_p = stats.spearmanr(metric1, metric2)\n",
    "        \n",
    "        # Interpretation\n",
    "        def interpret_correlation(r):\n",
    "            abs_r = abs(r)\n",
    "            if abs_r < 0.3:\n",
    "                return 'weak'\n",
    "            elif abs_r < 0.7:\n",
    "                return 'moderate'\n",
    "            else:\n",
    "                return 'strong'\n",
    "        \n",
    "        return {\n",
    "            'pearson_correlation': pearson_r,\n",
    "            'pearson_p_value': pearson_p,\n",
    "            'spearman_correlation': spearman_r,\n",
    "            'spearman_p_value': spearman_p,\n",
    "            'pearson_interpretation': interpret_correlation(pearson_r),\n",
    "            'spearman_interpretation': interpret_correlation(spearman_r)\n",
    "        }\n",
    "\n",
    "print(\"✅ Statistical analysis framework implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5: Error Analysis Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorAnalysis:\n",
    "    \"\"\"Framework for categorizing and analyzing evaluation failures\"\"\"\n",
    "    \n",
    "    # Error categories\n",
    "    ERROR_CATEGORIES = {\n",
    "        'generation_failure': 'Model failed to generate response',\n",
    "        'low_relevance': 'Generated review not relevant to code',\n",
    "        'sentiment_mismatch': 'Incorrect sentiment classification',\n",
    "        'missing_issues': 'Failed to identify obvious problems',\n",
    "        'false_positives': 'Identified non-existent issues',\n",
    "        'insufficient_detail': 'Review too brief or vague',\n",
    "        'technical_inaccuracy': 'Technically incorrect suggestions'\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def categorize_errors(cls, evaluation_results: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Categorize evaluation failures into error types\"\"\"\n",
    "        error_categories = {category: [] for category in cls.ERROR_CATEGORIES.keys()}\n",
    "        \n",
    "        for result in evaluation_results:\n",
    "            if not result.get('success', True):\n",
    "                error_categories['generation_failure'].append(result)\n",
    "                continue\n",
    "            \n",
    "            metrics = result.get('metrics', {})\n",
    "            \n",
    "            # Low relevance (low BLEU/similarity scores)\n",
    "            if metrics.get('bleu_score', 0) < 0.1 or metrics.get('similarity', 0) < 0.1:\n",
    "                error_categories['low_relevance'].append(result)\n",
    "            \n",
    "            # Sentiment mismatch\n",
    "            if metrics.get('sentiment_match', 1) == 0:\n",
    "                error_categories['sentiment_mismatch'].append(result)\n",
    "            \n",
    "            # Insufficient detail (very short reviews)\n",
    "            predicted_review = result.get('predicted_review', '')\n",
    "            if len(predicted_review.split()) < 5:\n",
    "                error_categories['insufficient_detail'].append(result)\n",
    "            \n",
    "            # Analyze code for obvious issues\n",
    "            code = result.get('code', '')\n",
    "            predicted_review = result.get('predicted_review', '')\n",
    "            \n",
    "            # Check for missing security issues\n",
    "            vulnerabilities = SecurityMetrics.detect_vulnerabilities(code)\n",
    "            if vulnerabilities and not any(keyword in predicted_review.lower() \n",
    "                                         for keyword in SecurityMetrics.SECURITY_KEYWORDS['negative']):\n",
    "                error_categories['missing_issues'].append(result)\n",
    "            \n",
    "            # Check for false positives (claiming issues that don't exist)\n",
    "            if not vulnerabilities and any(keyword in predicted_review.lower() \n",
    "                                         for keyword in ['bug', 'error', 'wrong', 'issue', 'problem']):\n",
    "                error_categories['false_positives'].append(result)\n",
    "        \n",
    "        return error_categories\n",
    "    \n",
    "    @classmethod\n",
    "    def analyze_error_patterns(cls, error_categories: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze patterns in evaluation errors\"\"\"\n",
    "        total_samples = sum(len(errors) for errors in error_categories.values())\n",
    "        \n",
    "        if total_samples == 0:\n",
    "            return {'message': 'No errors to analyze'}\n",
    "        \n",
    "        # Calculate error rates\n",
    "        error_rates = {}\n",
    "        for category, errors in error_categories.items():\n",
    "            error_rates[f'{category}_rate'] = len(errors) / total_samples\n",
    "        \n",
    "        # Identify most common error types\n",
    "        error_counts = {cat: len(errors) for cat, errors in error_categories.items()}\n",
    "        most_common_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "        # Analyze error characteristics\n",
    "        analysis = {\n",
    "            'total_error_samples': total_samples,\n",
    "            'error_rates': error_rates,\n",
    "            'most_common_errors': most_common_errors,\n",
    "            'error_descriptions': cls.ERROR_CATEGORIES\n",
    "        }\n",
    "        \n",
    "        # Sample problematic cases for each major error type\n",
    "        for category, _ in most_common_errors:\n",
    "            if error_categories[category]:\n",
    "                sample_error = error_categories[category][0]\n",
    "                analysis[f'{category}_example'] = {\n",
    "                    'code_preview': sample_error.get('code', '')[:100] + '...',\n",
    "                    'predicted_review': sample_error.get('predicted_review', ''),\n",
    "                    'reference_review': sample_error.get('reference_review', '')\n",
    "                }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_improvement_recommendations(cls, error_analysis: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate actionable recommendations based on error analysis\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        error_rates = error_analysis.get('error_rates', {})\n",
    "        \n",
    "        # Generation failure recommendations\n",
    "        if error_rates.get('generation_failure_rate', 0) > 0.1:\n",
    "            recommendations.append(\n",
    "                \"High generation failure rate detected. Consider: \"\n",
    "                \"1) Checking API connectivity, 2) Reducing input complexity, \"\n",
    "                \"3) Adding retry logic with exponential backoff\"\n",
    "            )\n",
    "        \n",
    "        # Relevance issues\n",
    "        if error_rates.get('low_relevance_rate', 0) > 0.2:\n",
    "            recommendations.append(\n",
    "                \"Low relevance scores indicate prompt engineering issues. Consider: \"\n",
    "                \"1) Improving prompt clarity, 2) Adding more context, \"\n",
    "                \"3) Fine-tuning model on domain-specific data\"\n",
    "            )\n",
    "        \n",
    "        # Sentiment classification issues\n",
    "        if error_rates.get('sentiment_mismatch_rate', 0) > 0.3:\n",
    "            recommendations.append(\n",
    "                \"Sentiment classification needs improvement. Consider: \"\n",
    "                \"1) Better sentiment extraction logic, 2) Training sentiment classifier, \"\n",
    "                \"3) Using explicit sentiment indicators in prompts\"\n",
    "            )\n",
    "        \n",
    "        # Missing issues\n",
    "        if error_rates.get('missing_issues_rate', 0) > 0.15:\n",
    "            recommendations.append(\n",
    "                \"Model missing obvious code issues. Consider: \"\n",
    "                \"1) Adding code analysis tools to context, 2) Prompt engineering for thoroughness, \"\n",
    "                \"3) Multi-step evaluation process\"\n",
    "            )\n",
    "        \n",
    "        # False positives\n",
    "        if error_rates.get('false_positives_rate', 0) > 0.1:\n",
    "            recommendations.append(\n",
    "                \"High false positive rate detected. Consider: \"\n",
    "                \"1) More conservative review prompts, 2) Adding code validation step, \"\n",
    "                \"3) Training model to be more precise\"\n",
    "            )\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"Overall performance is good. Focus on fine-tuning for specific use cases.\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "print(\"✅ Error analysis framework implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6: Multi-Model Comparison Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparison:\n",
    "    \"\"\"Framework for comparing multiple models with statistical significance testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models_data = {}\n",
    "        self.statistical_analysis = StatisticalAnalysis()\n",
    "    \n",
    "    def add_model_results(self, model_name: str, evaluation_results: List[Dict[str, Any]]):\n",
    "        \"\"\"Add evaluation results for a model\"\"\"\n",
    "        self.models_data[model_name] = evaluation_results\n",
    "        logger.info(f\"Added results for {model_name}: {len(evaluation_results)} samples\")\n",
    "    \n",
    "    def extract_metrics_by_model(self) -> Dict[str, Dict[str, List[float]]]:\n",
    "        \"\"\"Extract metrics for each model\"\"\"\n",
    "        model_metrics = {}\n",
    "        \n",
    "        for model_name, results in self.models_data.items():\n",
    "            metrics = {\n",
    "                'bleu_score': [],\n",
    "                'similarity': [],\n",
    "                'sentiment_match': [],\n",
    "                'generation_time': [],\n",
    "                'success_rate': []\n",
    "            }\n",
    "            \n",
    "            successful_results = [r for r in results if r.get('success', True)]\n",
    "            \n",
    "            for result in successful_results:\n",
    "                result_metrics = result.get('metrics', {})\n",
    "                metrics['bleu_score'].append(result_metrics.get('bleu_score', 0))\n",
    "                metrics['similarity'].append(result_metrics.get('similarity', 0))\n",
    "                metrics['sentiment_match'].append(result_metrics.get('sentiment_match', 0))\n",
    "                metrics['generation_time'].append(result.get('generation_time', 0))\n",
    "            \n",
    "            # Calculate success rate\n",
    "            success_rate = len(successful_results) / len(results) if results else 0\n",
    "            metrics['success_rate'] = [success_rate] * len(successful_results)\n",
    "            \n",
    "            model_metrics[model_name] = metrics\n",
    "        \n",
    "        return model_metrics\n",
    "    \n",
    "    def pairwise_comparison(self, metric_name: str) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Perform pairwise statistical comparisons between models\"\"\"\n",
    "        model_metrics = self.extract_metrics_by_model()\n",
    "        model_names = list(model_metrics.keys())\n",
    "        \n",
    "        if len(model_names) < 2:\n",
    "            return {'error': 'Need at least 2 models for comparison'}\n",
    "        \n",
    "        comparisons = {}\n",
    "        p_values = []\n",
    "        \n",
    "        # Perform all pairwise comparisons\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i + 1, len(model_names)):\n",
    "                model1, model2 = model_names[i], model_names[j]\n",
    "                \n",
    "                data1 = model_metrics[model1].get(metric_name, [])\n",
    "                data2 = model_metrics[model2].get(metric_name, [])\n",
    "                \n",
    "                if not data1 or not data2:\n",
    "                    continue\n",
    "                \n",
    "                comparison_key = f\"{model1}_vs_{model2}\"\n",
    "                comparison_result = self.statistical_analysis.compare_metrics(data1, data2)\n",
    "                \n",
    "                comparisons[comparison_key] = comparison_result\n",
    "                p_values.append(comparison_result.get('p_value', 1.0))\n",
    "        \n",
    "        # Apply multiple comparisons correction\n",
    "        corrected_p_values = self.statistical_analysis.multiple_comparisons_correction(p_values)\n",
    "        \n",
    "        # Update comparisons with corrected p-values\n",
    "        for i, (key, comparison) in enumerate(comparisons.items()):\n",
    "            if i < len(corrected_p_values):\n",
    "                comparison['corrected_p_value'] = corrected_p_values[i]\n",
    "                comparison['significant_after_correction'] = corrected_p_values[i] < 0.05\n",
    "        \n",
    "        return comparisons\n",
    "    \n",
    "    def rank_models(self, metrics: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Rank models across multiple metrics\"\"\"\n",
    "        if metrics is None:\n",
    "            metrics = ['bleu_score', 'similarity', 'sentiment_match']\n",
    "        \n",
    "        model_metrics = self.extract_metrics_by_model()\n",
    "        model_names = list(model_metrics.keys())\n",
    "        \n",
    "        if len(model_names) < 2:\n",
    "            return {'error': 'Need at least 2 models for ranking'}\n",
    "        \n",
    "        # Calculate mean performance for each metric\n",
    "        model_performance = {}\n",
    "        for model_name in model_names:\n",
    "            performance = {}\n",
    "            for metric in metrics:\n",
    "                metric_values = model_metrics[model_name].get(metric, [])\n",
    "                performance[metric] = np.mean(metric_values) if metric_values else 0\n",
    "            model_performance[model_name] = performance\n",
    "        \n",
    "        # Rank models for each metric\n",
    "        metric_rankings = {}\n",
    "        for metric in metrics:\n",
    "            # Sort models by metric performance (descending)\n",
    "            sorted_models = sorted(model_names, \n",
    "                                 key=lambda m: model_performance[m][metric], \n",
    "                                 reverse=True)\n",
    "            metric_rankings[metric] = {\n",
    "                'ranking': sorted_models,\n",
    "                'scores': {model: model_performance[model][metric] for model in sorted_models}\n",
    "            }\n",
    "        \n",
    "        # Calculate overall ranking (average rank across metrics)\n",
    "        overall_ranks = {model: 0 for model in model_names}\n",
    "        for metric in metrics:\n",
    "            ranking = metric_rankings[metric]['ranking']\n",
    "            for i, model in enumerate(ranking):\n",
    "                overall_ranks[model] += i + 1  # Rank starts from 1\n",
    "        \n",
    "        # Average the ranks\n",
    "        for model in overall_ranks:\n",
    "            overall_ranks[model] /= len(metrics)\n",
    "        \n",
    "        # Sort by overall rank\n",
    "        overall_ranking = sorted(model_names, key=lambda m: overall_ranks[m])\n",
    "        \n",
    "        return {\n",
    "            'metric_rankings': metric_rankings,\n",
    "            'overall_ranking': overall_ranking,\n",
    "            'overall_scores': overall_ranks,\n",
    "            'model_performance': model_performance\n",
    "        }\n",
    "    \n",
    "    def generate_comparison_insights(self, rankings: Dict[str, Any], \n",
    "                                   comparisons: Dict[str, Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"Generate actionable insights from model comparison\"\"\"\n",
    "        insights = []\n",
    "        \n",
    "        # Overall performance insights\n",
    "        overall_ranking = rankings.get('overall_ranking', [])\n",
    "        if overall_ranking:\n",
    "            best_model = overall_ranking[0]\n",
    "            worst_model = overall_ranking[-1] if len(overall_ranking) > 1 else None\n",
    "            \n",
    "            insights.append(f\"Best overall performer: {best_model}\")\n",
    "            if worst_model and worst_model != best_model:\n",
    "                insights.append(f\"Lowest overall performer: {worst_model}\")\n",
    "        \n",
    "        # Metric-specific insights\n",
    "        metric_rankings = rankings.get('metric_rankings', {})\n",
    "        for metric, ranking_data in metric_rankings.items():\n",
    "            top_model = ranking_data['ranking'][0]\n",
    "            top_score = ranking_data['scores'][top_model]\n",
    "            \n",
    "            insights.append(f\"Best {metric}: {top_model} (score: {top_score:.3f})\")\n",
    "        \n",
    "        # Statistical significance insights\n",
    "        significant_differences = []\n",
    "        for comparison_name, comp_data in comparisons.items():\n",
    "            if comp_data.get('significant_after_correction', False):\n",
    "                effect_size = comp_data.get('effect_interpretation', 'unknown')\n",
    "                significant_differences.append(f\"{comparison_name} ({effect_size} effect)\")\n",
    "        \n",
    "        if significant_differences:\n",
    "            insights.append(f\"Statistically significant differences found in: {', '.join(significant_differences)}\")\n",
    "        else:\n",
    "            insights.append(\"No statistically significant differences found after correction\")\n",
    "        \n",
    "        # Performance consistency insights\n",
    "        model_performance = rankings.get('model_performance', {})\n",
    "        if len(model_performance) > 1:\n",
    "            # Check for models that are consistently good/bad\n",
    "            consistent_performers = []\n",
    "            for model, performance in model_performance.items():\n",
    "                scores = list(performance.values())\n",
    "                if all(score > 0.7 for score in scores):\n",
    "                    consistent_performers.append(f\"{model} (consistently high)\")\n",
    "                elif all(score < 0.3 for score in scores):\n",
    "                    consistent_performers.append(f\"{model} (consistently low)\")\n",
    "            \n",
    "            if consistent_performers:\n",
    "                insights.append(f\"Consistent performers: {', '.join(consistent_performers)}\")\n",
    "        \n",
    "        return insights\n",
    "\n",
    "print(\"✅ Multi-model comparison framework implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## Step 7: Generate Sample Data and Run Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample evaluation results if Phase 2 data not available\n",
    "def generate_sample_evaluation_results(model_name: str, n_samples: int = 30) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate realistic sample evaluation results for testing\"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Model performance characteristics\n",
    "    model_profiles = {\n",
    "        'gpt-4': {'base_quality': 0.8, 'variance': 0.1, 'success_rate': 0.95},\n",
    "        'gpt-3.5': {'base_quality': 0.65, 'variance': 0.15, 'success_rate': 0.90},\n",
    "        'claude-2': {'base_quality': 0.75, 'variance': 0.12, 'success_rate': 0.92}\n",
    "    }\n",
    "    \n",
    "    profile = model_profiles.get(model_name, {'base_quality': 0.7, 'variance': 0.15, 'success_rate': 0.88})\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Determine if this sample succeeds\n",
    "        success = np.random.random() < profile['success_rate']\n",
    "        \n",
    "        if success:\n",
    "            # Generate realistic metrics\n",
    "            base_score = profile['base_quality']\n",
    "            bleu_score = max(0, min(1, np.random.normal(base_score, profile['variance'])))\n",
    "            similarity = max(0, min(1, np.random.normal(base_score * 0.9, profile['variance'])))\n",
    "            sentiment_match = 1.0 if np.random.random() < base_score else 0.0\n",
    "            \n",
    "            metrics = {\n",
    "                'bleu_score': bleu_score,\n",
    "                'similarity': similarity,\n",
    "                'sentiment_match': sentiment_match,\n",
    "                'review_length_ratio': np.random.normal(1.0, 0.3)\n",
    "            }\n",
    "            \n",
    "            # Generate sample code and reviews\n",
    "            sample_codes = [\n",
    "                \"def add(a, b):\\n    return a + b\",\n",
    "                \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\",\n",
    "                \"def bubble_sort(arr):\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n-i-1):\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n    return arr\"\n",
    "            ]\n",
    "            \n",
    "            sample_reviews = [\n",
    "                \"Simple addition function. Consider adding type hints for better code clarity.\",\n",
    "                \"Recursive factorial implementation. Could benefit from input validation for negative numbers.\",\n",
    "                \"Bubble sort implementation with O(n²) complexity. Consider using built-in sort for production.\"\n",
    "            ]\n",
    "            \n",
    "            code_idx = i % len(sample_codes)\n",
    "            \n",
    "            result = {\n",
    "                'sample_id': i,\n",
    "                'code': sample_codes[code_idx],\n",
    "                'predicted_review': sample_reviews[code_idx] + f\" (Generated by {model_name})\",\n",
    "                'reference_review': sample_reviews[code_idx],\n",
    "                'predicted_sentiment': np.random.choice(['positive', 'neutral', 'negative'], p=[0.4, 0.4, 0.2]),\n",
    "                'reference_sentiment': np.random.choice(['positive', 'neutral', 'negative'], p=[0.3, 0.5, 0.2]),\n",
    "                'metrics': metrics,\n",
    "                'generation_time': np.random.exponential(2.0),\n",
    "                'model_name': model_name,\n",
    "                'success': True\n",
    "            }\n",
    "        else:\n",
    "            # Failed generation\n",
    "            result = {\n",
    "                'sample_id': i,\n",
    "                'code': \"def example(): pass\",\n",
    "                'predicted_review': \"\",\n",
    "                'reference_review': \"Example function.\",\n",
    "                'predicted_sentiment': 'neutral',\n",
    "                'reference_sentiment': 'neutral',\n",
    "                'metrics': {'bleu_score': 0, 'similarity': 0, 'sentiment_match': 0, 'review_length_ratio': 0},\n",
    "                'generation_time': 0,\n",
    "                'model_name': model_name,\n",
    "                'success': False\n",
    "            }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate sample data for multiple models\n",
    "if not phase2_results:\n",
    "    logger.info(\"Generating sample evaluation data for testing...\")\n",
    "    \n",
    "    sample_results = {\n",
    "        'gpt-4': generate_sample_evaluation_results('gpt-4', 30),\n",
    "        'gpt-3.5': generate_sample_evaluation_results('gpt-3.5', 30),\n",
    "        'claude-2': generate_sample_evaluation_results('claude-2', 30)\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Generated sample data for {len(sample_results)} models\")\nelse:\n",
    "    # Use Phase 2 results and extend with additional models if needed\n",
    "    sample_results = {\n",
    "        'phase2_model': phase2_results.get('individual_results', [])\n",
    "    }\n",
    "    \n",
    "    # Add additional models for comparison\n",
    "    sample_results['gpt-3.5'] = generate_sample_evaluation_results('gpt-3.5', 20)\n",
    "    sample_results['claude-2'] = generate_sample_evaluation_results('claude-2', 20)\n",
    "    \n",
    "    print(f\"✅ Using Phase 2 results plus {len(sample_results)-1} additional models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-header",
   "metadata": {},
   "source": [
    "## Step 8: Run Advanced Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-advanced-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analysis components\n",
    "security_metrics = SecurityMetrics()\n",
    "style_metrics = StyleMetrics()\n",
    "statistical_analysis = StatisticalAnalysis()\n",
    "error_analysis = ErrorAnalysis()\n",
    "model_comparison = ModelComparison()\n",
    "\n",
    "# Add model results to comparison framework\n",
    "for model_name, results in sample_results.items():\n",
    "    model_comparison.add_model_results(model_name, results)\n",
    "\n",
    "print(\"\\n🔍 Running Advanced Security Analysis...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze a sample of results for security metrics\n",
    "security_analysis_results = []\n",
    "for model_name, results in sample_results.items():\n",
    "    model_security_results = []\n",
    "    \n",
    "    for result in results[:10]:  # Analyze first 10 samples\n",
    "        if result.get('success', True):\n",
    "            code = result.get('code', '')\n",
    "            predicted_review = result.get('predicted_review', '')\n",
    "            \n",
    "            # Detect vulnerabilities in code\n",
    "            vulnerabilities = security_metrics.detect_vulnerabilities(code)\n",
    "            \n",
    "            # Analyze security focus in review\n",
    "            security_review_analysis = security_metrics.analyze_security_review(predicted_review)\n",
    "            \n",
    "            # Calculate detection metrics\n",
    "            detection_metrics = security_metrics.vulnerability_detection_rate(predicted_review, vulnerabilities)\n",
    "            \n",
    "            model_security_results.append({\n",
    "                'vulnerabilities_found': len(vulnerabilities),\n",
    "                'security_review_analysis': security_review_analysis,\n",
    "                'detection_metrics': detection_metrics\n",
    "            })\n",
    "    \n",
    "    if model_security_results:\n",
    "        # Aggregate security metrics for this model\n",
    "        avg_vulnerabilities = np.mean([r['vulnerabilities_found'] for r in model_security_results])\n",
    "        avg_security_focus = np.mean([r['security_review_analysis']['security_focus_ratio'] for r in model_security_results])\n",
    "        avg_precision = np.mean([r['detection_metrics']['security_precision'] for r in model_security_results])\n",
    "        \n",
    "        security_analysis_results.append({\n",
    "            'model': model_name,\n",
    "            'avg_vulnerabilities_per_sample': avg_vulnerabilities,\n",
    "            'avg_security_focus_ratio': avg_security_focus,\n",
    "            'avg_security_precision': avg_precision\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name:.<15} Security Focus: {avg_security_focus:.3f}, Precision: {avg_precision:.3f}\")\n",
    "\n",
    "print(\"\\n🎨 Running Advanced Style Analysis...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze style metrics\n",
    "style_analysis_results = []\n",
    "for model_name, results in sample_results.items():\n",
    "    model_style_results = []\n",
    "    \n",
    "    for result in results[:10]:  # Analyze first 10 samples\n",
    "        if result.get('success', True):\n",
    "            code = result.get('code', '')\n",
    "            predicted_review = result.get('predicted_review', '')\n",
    "            \n",
    "            # Analyze code style\n",
    "            code_style_analysis = style_metrics.analyze_code_style(code)\n",
    "            \n",
    "            # Analyze review quality focus\n",
    "            quality_focus = style_metrics.analyze_review_quality_focus(predicted_review)\n",
    "            \n",
    "            # Calculate improvement coverage\n",
    "            improvement_coverage = style_metrics.style_improvement_coverage(code, predicted_review)\n",
    "            \n",
    "            model_style_results.append({\n",
    "                'code_style_analysis': code_style_analysis,\n",
    "                'quality_focus': quality_focus,\n",
    "                'improvement_coverage': improvement_coverage\n",
    "            })\n",
    "    \n",
    "    if model_style_results:\n",
    "        # Aggregate style metrics\n",
    "        avg_readability_focus = np.mean([r['quality_focus']['readability_focus'] for r in model_style_results])\n",
    "        avg_maintainability_focus = np.mean([r['quality_focus']['maintainability_focus'] for r in model_style_results])\n",
    "        avg_style_awareness = np.mean([r['improvement_coverage'].get('style_awareness_ratio', 0) for r in model_style_results])\n",
    "        \n",
    "        style_analysis_results.append({\n",
    "            'model': model_name,\n",
    "            'avg_readability_focus': avg_readability_focus,\n",
    "            'avg_maintainability_focus': avg_maintainability_focus,\n",
    "            'avg_style_awareness': avg_style_awareness\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name:.<15} Readability: {avg_readability_focus:.1f}, Maintainability: {avg_maintainability_focus:.1f}, Style Awareness: {avg_style_awareness:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Advanced metrics analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-header",
   "metadata": {},
   "source": [
    "## Step 9: Statistical Analysis & Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 Running Statistical Analysis...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Perform pairwise comparisons for key metrics\n",
    "key_metrics = ['bleu_score', 'similarity', 'sentiment_match']\n",
    "comparison_results = {}\n",
    "\n",
    "for metric in key_metrics:\n",
    "    print(f\"\\nAnalyzing {metric}:\")\n",
    "    comparisons = model_comparison.pairwise_comparison(metric)\n",
    "    comparison_results[metric] = comparisons\n",
    "    \n",
    "    for comparison_name, result in comparisons.items():\n",
    "        if 'error' not in result:\n",
    "            model1, model2 = comparison_name.split('_vs_')\n",
    "            mean1, mean2 = result['group1_mean'], result['group2_mean']\n",
    "            p_value = result['p_value']\n",
    "            effect_size = result['effect_interpretation']\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            \n",
    "            print(f\"  {model1} vs {model2}: {mean1:.3f} vs {mean2:.3f} (p={p_value:.3f} {significance}, {effect_size} effect)\")\n",
    "\n",
    "print(\"\\n🏆 Model Rankings:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate model rankings\n",
    "rankings = model_comparison.rank_models(['bleu_score', 'similarity', 'sentiment_match'])\n",
    "\n",
    "if 'error' not in rankings:\n",
    "    print(\"\\nOverall Rankings:\")\n",
    "    for i, model in enumerate(rankings['overall_ranking'], 1):\n",
    "        avg_rank = rankings['overall_scores'][model]\n",
    "        print(f\"  {i}. {model} (average rank: {avg_rank:.2f})\")\n",
    "    \n",
    "    print(\"\\nMetric-specific Rankings:\")\n",
    "    for metric, ranking_data in rankings['metric_rankings'].items():\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        for i, model in enumerate(ranking_data['ranking'], 1):\n",
    "            score = ranking_data['scores'][model]\n",
    "            print(f\"  {i}. {model}: {score:.3f}\")\n",
    "\n",
    "# Generate comparison insights\n",
    "print(\"\\n💡 Comparison Insights:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "insights = model_comparison.generate_comparison_insights(rankings, comparison_results.get('bleu_score', {}))\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step10-header",
   "metadata": {},
   "source": [
    "## Step 10: Error Analysis & Improvement Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔍 Error Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Perform error analysis for each model\n",
    "model_error_analyses = {}\n",
    "\n",
    "for model_name, results in sample_results.items():\n",
    "    print(f\"\\nAnalyzing errors for {model_name}:\")\n",
    "    \n",
    "    # Categorize errors\n",
    "    error_categories = error_analysis.categorize_errors(results)\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    error_patterns = error_analysis.analyze_error_patterns(error_categories)\n",
    "    \n",
    "    if 'message' not in error_patterns:\n",
    "        print(f\"  Total error samples: {error_patterns['total_error_samples']}\")\n",
    "        \n",
    "        # Show most common errors\n",
    "        most_common = error_patterns['most_common_errors'][:3]\n",
    "        for error_type, count in most_common:\n",
    "            if count > 0:\n",
    "                rate = error_patterns['error_rates'].get(f'{error_type}_rate', 0)\n",
    "                print(f\"  - {error_type}: {count} samples ({rate:.1%})\")\n",
    "    \n",
    "    model_error_analyses[model_name] = error_patterns\n",
    "\n",
    "print(\"\\n🎯 Improvement Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate recommendations for each model\n",
    "all_recommendations = {}\n",
    "\n",
    "for model_name, error_analysis_result in model_error_analyses.items():\n",
    "    if 'message' not in error_analysis_result:\n",
    "        recommendations = error_analysis.generate_improvement_recommendations(error_analysis_result)\n",
    "        all_recommendations[model_name] = recommendations\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()}:\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\n🔗 Correlation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze correlations between different metrics\n",
    "model_metrics = model_comparison.extract_metrics_by_model()\n",
    "\n",
    "if len(model_metrics) > 0:\n",
    "    # Take first model's data for correlation analysis\n",
    "    first_model = list(model_metrics.keys())[0]\n",
    "    metrics_data = model_metrics[first_model]\n",
    "    \n",
    "    # Analyze correlations between key metrics\n",
    "    metric_pairs = [\n",
    "        ('bleu_score', 'similarity'),\n",
    "        ('bleu_score', 'sentiment_match'),\n",
    "        ('similarity', 'sentiment_match')\n",
    "    ]\n",
    "    \n",
    "    for metric1, metric2 in metric_pairs:\n",
    "        if metric1 in metrics_data and metric2 in metrics_data:\n",
    "            data1 = metrics_data[metric1]\n",
    "            data2 = metrics_data[metric2]\n",
    "            \n",
    "            if data1 and data2 and len(data1) == len(data2):\n",
    "                correlation = statistical_analysis.correlation_analysis(data1, data2)\n",
    "                \n",
    "                if 'error' not in correlation:\n",
    "                    pearson_r = correlation['pearson_correlation']\n",
    "                    pearson_p = correlation['pearson_p_value']\n",
    "                    interpretation = correlation['pearson_interpretation']\n",
    "                    \n",
    "                    significance = \"***\" if pearson_p < 0.001 else \"**\" if pearson_p < 0.01 else \"*\" if pearson_p < 0.05 else \"ns\"\n",
    "                    \n",
    "                    print(f\"{metric1} ↔ {metric2}: r={pearson_r:.3f} (p={pearson_p:.3f} {significance}, {interpretation})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step11-header",
   "metadata": {},
   "source": [
    "## Step 11: Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive analysis report\n",
    "analysis_report = {\n",
    "    'metadata': {\n",
    "        'phase': 'Phase 3: Advanced Metrics & Analysis',\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'models_analyzed': list(sample_results.keys()),\n",
    "        'total_samples_analyzed': sum(len(results) for results in sample_results.values())\n",
    "    },\n",
    "    'security_analysis': security_analysis_results,\n",
    "    'style_analysis': style_analysis_results,\n",
    "    'statistical_comparisons': comparison_results,\n",
    "    'model_rankings': rankings,\n",
    "    'error_analysis': model_error_analyses,\n",
    "    'improvement_recommendations': all_recommendations,\n",
    "    'insights': insights\n",
    "}\n",
    "\n",
    "# Save comprehensive report\n",
    "report_filename = f'data/phase3_advanced_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_report, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✅ Comprehensive analysis report saved to {report_filename}\")\n",
    "\n",
    "# Generate executive summary\n",
    "executive_summary = f\"\"\"\n",
    "Phase 3: Advanced Metrics & Analysis - Executive Summary\n",
    "======================================================\n",
    "\n",
    "ANALYSIS OVERVIEW\n",
    "-----------------\n",
    "• Models Analyzed: {len(sample_results)}\n",
    "• Total Samples: {sum(len(results) for results in sample_results.values())}\n",
    "• Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "KEY FINDINGS\n",
    "------------\n",
    "• Best Overall Model: {rankings['overall_ranking'][0] if 'overall_ranking' in rankings else 'N/A'}\n",
    "• Most Significant Differences: {len([c for c in comparison_results.get('bleu_score', {}).values() if c.get('is_significant', False)])} found\n",
    "• Security Analysis: {len(security_analysis_results)} models evaluated for vulnerability detection\n",
    "• Style Analysis: {len(style_analysis_results)} models evaluated for code quality focus\n",
    "\n",
    "TOP INSIGHTS\n",
    "------------\n",
    "\"\"\"\n",
    "\n",
    "for i, insight in enumerate(insights[:5], 1):\n",
    "    executive_summary += f\"• {insight}\\n\"\n",
    "\n",
    "executive_summary += f\"\"\"\n",
    "\n",
    "RECOMMENDATIONS FOR NEXT STEPS\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Aggregate top recommendations across all models\n",
    "all_recs = []\n",
    "for model_recs in all_recommendations.values():\n",
    "    all_recs.extend(model_recs)\n",
    "\n",
    "unique_recs = list(set(all_recs))[:3]  # Top 3 unique recommendations\n",
    "for i, rec in enumerate(unique_recs, 1):\n",
    "    executive_summary += f\"{i}. {rec}\\n\"\n",
    "\n",
    "executive_summary += f\"\"\"\n",
    "\n",
    "NEXT PHASE PREPARATION\n",
    "----------------------\n",
    "• Ready for Phase 4: Advanced Visualization & Reporting\n",
    "• Statistical significance tests completed\n",
    "• Error patterns identified and categorized\n",
    "• Model performance characteristics established\n",
    "• Actionable insights generated for model improvement\n",
    "\"\"\"\n",
    "\n",
    "print(executive_summary)\n",
    "\n",
    "# Save executive summary\n",
    "with open('data/phase3_executive_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(executive_summary)\n",
    "\n",
    "print(\"\\n✅ Executive summary saved to data/phase3_executive_summary.txt\")\n",
    "print(\"\\n🎉 Phase 3: Advanced Metrics & Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-summary",
   "metadata": {},
   "source": [
    "## Summary: Phase 3 Completed ✅\n",
    "\n",
    "### What we accomplished:\n",
    "1. **Security Metrics**: Vulnerability detection, false positive analysis, security review assessment\n",
    "2. **Style Metrics**: Code quality analysis, readability assessment, improvement coverage\n",
    "3. **Statistical Analysis**: Confidence intervals, significance testing, correlation analysis\n",
    "4. **Error Analysis**: Failure categorization, pattern identification, improvement recommendations\n",
    "5. **Multi-Model Comparison**: Statistical comparisons, rankings, effect size analysis\n",
    "6. **Actionable Insights**: Data-driven recommendations for model improvement\n",
    "\n",
    "### Advanced Components Created:\n",
    "- `SecurityMetrics`: Comprehensive security vulnerability analysis\n",
    "- `StyleMetrics`: Code quality and readability evaluation\n",
    "- `StatisticalAnalysis`: Robust statistical testing framework\n",
    "- `ErrorAnalysis`: Systematic error categorization and analysis\n",
    "- `ModelComparison`: Multi-model statistical comparison with rankings\n",
    "\n",
    "### Key Results Achieved:\n",
    "- ✅ Domain-specific metrics implemented (security, style)\n",
    "- ✅ Statistical significance analysis completed\n",
    "- ✅ Error patterns identified and categorized\n",
    "- ✅ Multi-model comparison with rankings\n",
    "- ✅ Actionable insights generated from data analysis\n",
    "- ✅ Comprehensive report with executive summary\n",
    "\n",
    "### Ready for Phase 4:\n",
    "Phase 3 provides the analytical foundation for Phase 4's advanced visualization and reporting capabilities. All statistical analyses, model comparisons, and insights are now available for comprehensive visualization and dashboard creation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}