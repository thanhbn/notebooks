{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Document Components v√† API Reference\n",
    "\n",
    "## M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "- Hi·ªÉu v·ªÅ c·∫•u tr√∫c t√†i li·ªáu c·ªßa LangGraph v√† c√°ch s·ª≠ d·ª•ng API Reference\n",
    "- N·∫Øm v·ªØng c√°c th√†nh ph·∫ßn ch√≠nh: Nodes, Edges, State, Graph\n",
    "- Th·ª±c h√†nh v·ªõi c√°c API quan tr·ªçng c·ªßa LangGraph\n",
    "- X√¢y d·ª±ng c√°c v√≠ d·ª• th·ª±c t·∫ø s·ª≠ d·ª•ng ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gi·ªõi thi·ªáu\n",
    "\n",
    "LangGraph l√† m·ªôt framework m·∫°nh m·∫Ω ƒë·ªÉ x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng AI ph·ª©c t·∫°p d·ª±a tr√™n ƒë·ªì th·ªã (graph). C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa LangGraph bao g·ªìm:\n",
    "\n",
    "### üéØ Th√†nh ph·∫ßn c∆° b·∫£n\n",
    "1. **Graph**: Container ch√≠nh ch·ª©a to√†n b·ªô workflow\n",
    "2. **StateGraph**: Graph c√≥ kh·∫£ nƒÉng qu·∫£n l√Ω state\n",
    "3. **Node**: C√°c ƒëi·ªÉm x·ª≠ l√Ω logic (functions/agents)\n",
    "4. **Edge**: K·∫øt n·ªëi gi·ªØa c√°c nodes\n",
    "5. **State**: D·ªØ li·ªáu ƒë∆∞·ª£c chia s·∫ª gi·ªØa c√°c nodes\n",
    "\n",
    "### üîó API Methods quan tr·ªçng\n",
    "- `add_node()`: Th√™m node v√†o graph\n",
    "- `add_edge()`: T·∫°o k·∫øt n·ªëi gi·ªØa c√°c nodes\n",
    "- `set_entry_point()`: X√°c ƒë·ªãnh ƒëi·ªÉm b·∫Øt ƒë·∫ßu\n",
    "- `compile()`: Bi√™n d·ªãch graph th√†nh runnable\n",
    "- `invoke()`: Th·ª±c thi ƒë·ªìng b·ªô\n",
    "- `stream()`: Th·ª±c thi b·∫•t ƒë·ªìng b·ªô v·ªõi streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√†i ƒë·∫∑t v√† Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt\n",
    "!pip install langgraph langchain-anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated, Dict, Any\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Ki·ªÉm tra API key\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è ANTHROPIC_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p!\")\n",
    "else:\n",
    "    print(\"‚úÖ ANTHROPIC_API_KEY ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference v√† C·∫•u tr√∫c T√†i li·ªáu\n",
    "\n",
    "### üìö T√†i li·ªáu ch√≠nh th·ª©c LangGraph\n",
    "- **API Reference**: https://langchain-ai.github.io/langgraph/reference/\n",
    "- **H∆∞·ªõng d·∫´n**: https://langchain-ai.github.io/langgraph/\n",
    "- **Examples**: https://github.com/langchain-ai/langgraph/tree/main/examples\n",
    "\n",
    "### üîç C√°ch s·ª≠ d·ª•ng API Reference\n",
    "1. Truy c·∫≠p trang API Reference ch√≠nh th·ª©c\n",
    "2. T√¨m ki·∫øm class/method c·∫ßn s·ª≠ d·ª•ng\n",
    "3. ƒê·ªçc docstring v√† parameters\n",
    "4. Xem examples v√† return types\n",
    "5. Ki·ªÉm tra version compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V√≠ d·ª• 1: Graph ƒê∆°n gi·∫£n v·ªõi StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a State schema\n",
    "class SimpleState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    step_count: int\n",
    "    current_action: str\n",
    "\n",
    "# Kh·ªüi t·∫°o LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "def greeting_node(state: SimpleState) -> Dict[str, Any]:\n",
    "    \"\"\"Node ch√†o h·ªèi ƒë·∫ßu ti√™n\"\"\"\n",
    "    print(f\"üî∏ Executing greeting_node - Step: {state.get('step_count', 0)}\")\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=\"Ch√†o b·∫°n! H√£y gi·ªõi thi·ªáu b·∫£n th√¢n m·ªôt c√°ch ng·∫Øn g·ªçn.\")\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"step_count\": state.get('step_count', 0) + 1,\n",
    "        \"current_action\": \"greeting_completed\"\n",
    "    }\n",
    "\n",
    "def analysis_node(state: SimpleState) -> Dict[str, Any]:\n",
    "    \"\"\"Node ph√¢n t√≠ch th√¥ng tin\"\"\"\n",
    "    print(f\"üî∏ Executing analysis_node - Step: {state.get('step_count', 0)}\")\n",
    "    \n",
    "    # L·∫•y tin nh·∫Øn cu·ªëi c√πng t·ª´ state\n",
    "    last_message = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=f\"Ph√¢n t√≠ch ng·∫Øn g·ªçn v·ªÅ n·ªôi dung n√†y: {last_message}\")\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"step_count\": state.get('step_count', 0) + 1,\n",
    "        \"current_action\": \"analysis_completed\"\n",
    "    }\n",
    "\n",
    "def summary_node(state: SimpleState) -> Dict[str, Any]:\n",
    "    \"\"\"Node t√≥m t·∫Øt cu·ªëi c√πng\"\"\"\n",
    "    print(f\"üî∏ Executing summary_node - Step: {state.get('step_count', 0)}\")\n",
    "    \n",
    "    total_messages = len(state[\"messages\"])\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=f\"T√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán v·ªõi {total_messages} tin nh·∫Øn ƒë√£ x·ª≠ l√Ω.\")\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"step_count\": state.get('step_count', 0) + 1,\n",
    "        \"current_action\": \"summary_completed\"\n",
    "    }\n",
    "\n",
    "# T·∫°o StateGraph\n",
    "workflow = StateGraph(SimpleState)\n",
    "\n",
    "# Th√™m c√°c nodes\n",
    "workflow.add_node(\"greeting\", greeting_node)\n",
    "workflow.add_node(\"analysis\", analysis_node)\n",
    "workflow.add_node(\"summary\", summary_node)\n",
    "\n",
    "# Thi·∫øt l·∫≠p entry point\n",
    "workflow.set_entry_point(\"greeting\")\n",
    "\n",
    "# Th√™m c√°c edges\n",
    "workflow.add_edge(\"greeting\", \"analysis\")\n",
    "workflow.add_edge(\"analysis\", \"summary\")\n",
    "workflow.add_edge(\"summary\", END)\n",
    "\n",
    "# Compile graph\n",
    "simple_app = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Graph ƒë∆°n gi·∫£n ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ª±c thi graph ƒë∆°n gi·∫£n\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu th·ª±c thi Graph ƒë∆°n gi·∫£n...\\n\")\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"step_count\": 0,\n",
    "    \"current_action\": \"starting\"\n",
    "}\n",
    "\n",
    "result = simple_app.invoke(initial_state)\n",
    "\n",
    "print(\"\\nüìä K·∫øt qu·∫£ cu·ªëi c√πng:\")\n",
    "print(f\"- T·ªïng s·ªë b∆∞·ªõc: {result['step_count']}\")\n",
    "print(f\"- H√†nh ƒë·ªông cu·ªëi: {result['current_action']}\")\n",
    "print(f\"- S·ªë tin nh·∫Øn: {len(result['messages'])}\")\n",
    "\n",
    "print(\"\\nüí¨ Tin nh·∫Øn cu·ªëi c√πng:\")\n",
    "if result['messages']:\n",
    "    print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V√≠ d·ª• 2: Agent v·ªõi Tool Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a tools\n",
    "@tool\n",
    "def calculate_math(expression: str) -> str:\n",
    "    \"\"\"T√≠nh to√°n bi·ªÉu th·ª©c to√°n h·ªçc ƒë∆°n gi·∫£n.\n",
    "    \n",
    "    Args:\n",
    "        expression: Bi·ªÉu th·ª©c to√°n h·ªçc (v√≠ d·ª•: '2+3*4')\n",
    "    \n",
    "    Returns:\n",
    "        K·∫øt qu·∫£ t√≠nh to√°n\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ch·ªâ cho ph√©p c√°c k√Ω t·ª± an to√†n\n",
    "        allowed_chars = set('0123456789+-*/(). ')\n",
    "        if not all(char in allowed_chars for char in expression):\n",
    "            return \"L·ªói: Bi·ªÉu th·ª©c ch·ª©a k√Ω t·ª± kh√¥ng h·ª£p l·ªá\"\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return f\"K·∫øt qu·∫£ c·ªßa {expression} = {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói t√≠nh to√°n: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_weather_info(city: str) -> str:\n",
    "    \"\"\"L·∫•y th√¥ng tin th·ªùi ti·∫øt (gi·∫£ l·∫≠p).\n",
    "    \n",
    "    Args:\n",
    "        city: T√™n th√†nh ph·ªë\n",
    "    \n",
    "    Returns:\n",
    "        Th√¥ng tin th·ªùi ti·∫øt\n",
    "    \"\"\"\n",
    "    weather_data = {\n",
    "        \"h√† n·ªôi\": \"H√† N·ªôi: 25¬∞C, n·∫Øng √≠t m√¢y, ƒë·ªô ·∫©m 60%\",\n",
    "        \"h·ªì ch√≠ minh\": \"TP.HCM: 30¬∞C, m∆∞a r√†o, ƒë·ªô ·∫©m 80%\",\n",
    "        \"ƒë√† n·∫µng\": \"ƒê√† N·∫µng: 28¬∞C, n·∫Øng ƒë·∫πp, ƒë·ªô ·∫©m 55%\"\n",
    "    }\n",
    "    \n",
    "    city_lower = city.lower()\n",
    "    return weather_data.get(city_lower, f\"Kh√¥ng c√≥ d·ªØ li·ªáu th·ªùi ti·∫øt cho {city}\")\n",
    "\n",
    "# T·∫°o LLM v·ªõi tools\n",
    "tools = [calculate_math, get_weather_info]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"üõ†Ô∏è Tools ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a:\")\n",
    "for tool in tools:\n",
    "    print(f\"- {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State cho Agent\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    tool_calls_made: int\n",
    "    current_task: str\n",
    "\n",
    "def agent_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Node agent ch√≠nh\"\"\"\n",
    "    print(f\"ü§ñ Agent ƒëang x·ª≠ l√Ω...\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"current_task\": \"agent_processing\"\n",
    "    }\n",
    "\n",
    "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Node th·ª±c thi tools\"\"\"\n",
    "    print(f\"üîß Th·ª±c thi tools...\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    tool_calls = getattr(last_message, 'tool_calls', [])\n",
    "    \n",
    "    if not tool_calls:\n",
    "        return {\"current_task\": \"no_tools\"}\n",
    "    \n",
    "    results = []\n",
    "    tools_dict = {tool.name: tool for tool in tools}\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call['name']\n",
    "        tool_args = tool_call['args']\n",
    "        \n",
    "        if tool_name in tools_dict:\n",
    "            try:\n",
    "                result = tools_dict[tool_name].invoke(tool_args)\n",
    "                results.append({\n",
    "                    \"tool_call_id\": tool_call['id'],\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": result\n",
    "                })\n",
    "                print(f\"‚úÖ Tool {tool_name} th·ª±c thi th√†nh c√¥ng\")\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"tool_call_id\": tool_call['id'],\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": f\"L·ªói: {str(e)}\"\n",
    "                })\n",
    "                print(f\"‚ùå Tool {tool_name} g·∫∑p l·ªói: {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": results,\n",
    "        \"tool_calls_made\": state.get('tool_calls_made', 0) + len(results),\n",
    "        \"current_task\": \"tools_executed\"\n",
    "    }\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Quy·∫øt ƒë·ªãnh c√≥ ti·∫øp t·ª•c hay kh√¥ng\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # N·∫øu c√≥ tool calls, th·ª±c thi tools\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# T·∫°o Agent Graph\n",
    "agent_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Th√™m nodes\n",
    "agent_workflow.add_node(\"agent\", agent_node)\n",
    "agent_workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Entry point\n",
    "agent_workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Conditional edges\n",
    "agent_workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Tools node quay l·∫°i agent\n",
    "agent_workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "agent_app = agent_workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Agent Graph v·ªõi Tools ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Agent v·ªõi Tools\n",
    "print(\"üöÄ Test Agent v·ªõi Tools...\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"T√≠nh to√°n 15 * 8 + 32 cho t√¥i\",\n",
    "    \"Th·ªùi ti·∫øt ·ªü H√† N·ªôi h√¥m nay nh∆∞ th·∫ø n√†o?\",\n",
    "    \"V·ª´a t√≠nh (100 - 25) / 5 v·ª´a cho bi·∫øt th·ªùi ti·∫øt ƒê√† N·∫µng\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìù Test {i}: {query}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"tool_calls_made\": 0,\n",
    "        \"current_task\": \"starting\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = agent_app.invoke(initial_state)\n",
    "        \n",
    "        print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "        print(f\"- Tool calls made: {result.get('tool_calls_made', 0)}\")\n",
    "        print(f\"- Current task: {result.get('current_task', 'unknown')}\")\n",
    "        \n",
    "        # L·∫•y ph·∫£n h·ªìi cu·ªëi c√πng t·ª´ AI\n",
    "        ai_messages = [msg for msg in result['messages'] if isinstance(msg, AIMessage)]\n",
    "        if ai_messages:\n",
    "            print(f\"\\nü§ñ AI Response:\")\n",
    "            print(ai_messages[-1].content)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V√≠ d·ª• 3: Truy c·∫≠p v√† X·ª≠ l√Ω State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State ph·ª©c t·∫°p v·ªõi nhi·ªÅu th√¥ng tin\n",
    "class ComplexState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_profile: Dict[str, Any]\n",
    "    conversation_history: list\n",
    "    current_topic: str\n",
    "    processing_steps: list\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "def profile_analyzer_node(state: ComplexState) -> Dict[str, Any]:\n",
    "    \"\"\"Ph√¢n t√≠ch th√¥ng tin user t·ª´ tin nh·∫Øn\"\"\"\n",
    "    print(\"üë§ Analyzing user profile...\")\n",
    "    \n",
    "    # L·∫•y tin nh·∫Øn m·ªõi nh·∫•t t·ª´ user\n",
    "    human_messages = [msg for msg in state[\"messages\"] if isinstance(msg, HumanMessage)]\n",
    "    if not human_messages:\n",
    "        return {\"processing_steps\": [\"No human messages found\"]}\n",
    "    \n",
    "    latest_message = human_messages[-1].content\n",
    "    \n",
    "    # S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch profile\n",
    "    analysis_prompt = f\"\"\"\n",
    "    Ph√¢n t√≠ch tin nh·∫Øn sau v√† tr√≠ch xu·∫•t th√¥ng tin v·ªÅ ng∆∞·ªùi d√πng:\n",
    "    \"{latest_message}\"\n",
    "    \n",
    "    Tr·∫£ v·ªÅ ph√¢n t√≠ch d∆∞·ªõi d·∫°ng JSON v·ªõi c√°c tr∆∞·ªùng:\n",
    "    - name: t√™n ng∆∞·ªùi d√πng (n·∫øu c√≥)\n",
    "    - interests: s·ªü th√≠ch/quan t√¢m\n",
    "    - mood: t√¢m tr·∫°ng\n",
    "    - intent: √Ω ƒë·ªãnh/m·ª•c ƒë√≠ch\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "    \n",
    "    # C·∫≠p nh·∫≠t user profile (ƒë∆°n gi·∫£n h√≥a)\n",
    "    current_profile = state.get(\"user_profile\", {})\n",
    "    current_profile[\"last_analysis\"] = response.content\n",
    "    current_profile[\"message_count\"] = len(human_messages)\n",
    "    \n",
    "    return {\n",
    "        \"user_profile\": current_profile,\n",
    "        \"processing_steps\": state.get(\"processing_steps\", []) + [\"profile_analyzed\"],\n",
    "        \"current_topic\": \"profile_analysis\"\n",
    "    }\n",
    "\n",
    "def context_manager_node(state: ComplexState) -> Dict[str, Any]:\n",
    "    \"\"\"Qu·∫£n l√Ω context v√† l·ªãch s·ª≠ h·ªôi tho·∫°i\"\"\"\n",
    "    print(\"üìö Managing conversation context...\")\n",
    "    \n",
    "    # C·∫≠p nh·∫≠t l·ªãch s·ª≠\n",
    "    current_history = state.get(\"conversation_history\", [])\n",
    "    \n",
    "    # Th√™m b·∫£n t√≥m t·∫Øt b∆∞·ªõc hi·ªán t·∫°i\n",
    "    summary = {\n",
    "        \"step\": len(current_history) + 1,\n",
    "        \"topic\": state.get(\"current_topic\", \"unknown\"),\n",
    "        \"message_count\": len(state.get(\"messages\", [])),\n",
    "        \"user_profile_status\": \"analyzed\" if state.get(\"user_profile\") else \"not_analyzed\"\n",
    "    }\n",
    "    \n",
    "    current_history.append(summary)\n",
    "    \n",
    "    return {\n",
    "        \"conversation_history\": current_history,\n",
    "        \"processing_steps\": state.get(\"processing_steps\", []) + [\"context_managed\"],\n",
    "        \"metadata\": {\n",
    "            \"total_steps\": len(current_history),\n",
    "            \"last_update\": \"context_manager\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "def response_generator_node(state: ComplexState) -> Dict[str, Any]:\n",
    "    \"\"\"T·∫°o ph·∫£n h·ªìi d·ª±a tr√™n to√†n b·ªô state\"\"\"\n",
    "    print(\"üí¨ Generating contextual response...\")\n",
    "    \n",
    "    # T·∫°o context summary t·ª´ state\n",
    "    profile_info = state.get(\"user_profile\", {})\n",
    "    history_info = state.get(\"conversation_history\", [])\n",
    "    processing_info = state.get(\"processing_steps\", [])\n",
    "    \n",
    "    context_summary = f\"\"\"\n",
    "    Th√¥ng tin context hi·ªán t·∫°i:\n",
    "    - User profile: {len(profile_info)} fields\n",
    "    - Conversation history: {len(history_info)} steps\n",
    "    - Processing steps: {', '.join(processing_info)}\n",
    "    - Current topic: {state.get('current_topic', 'unknown')}\n",
    "    \"\"\"\n",
    "    \n",
    "    # T·∫°o ph·∫£n h·ªìi c√≥ ng·ªØ c·∫£nh\n",
    "    prompt = f\"\"\"\n",
    "    B·∫°n l√† m·ªôt AI assistant th√¥ng minh. D·ª±a v√†o th√¥ng tin context sau:\n",
    "    {context_summary}\n",
    "    \n",
    "    H√£y t·∫°o m·ªôt ph·∫£n h·ªìi th√¢n thi·ªán v√† c√≥ √≠ch cho ng∆∞·ªùi d√πng, \n",
    "    th·ªÉ hi·ªán r·∫±ng b·∫°n ƒë√£ hi·ªÉu context v√† c√≥ th·ªÉ h·ªó tr·ª£ t·ªët h∆°n.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"processing_steps\": state.get(\"processing_steps\", []) + [\"response_generated\"],\n",
    "        \"current_topic\": \"response_complete\"\n",
    "    }\n",
    "\n",
    "# T·∫°o Complex State Graph\n",
    "complex_workflow = StateGraph(ComplexState)\n",
    "\n",
    "# Th√™m nodes\n",
    "complex_workflow.add_node(\"profile_analyzer\", profile_analyzer_node)\n",
    "complex_workflow.add_node(\"context_manager\", context_manager_node)\n",
    "complex_workflow.add_node(\"response_generator\", response_generator_node)\n",
    "\n",
    "# Entry point\n",
    "complex_workflow.set_entry_point(\"profile_analyzer\")\n",
    "\n",
    "# Sequential edges\n",
    "complex_workflow.add_edge(\"profile_analyzer\", \"context_manager\")\n",
    "complex_workflow.add_edge(\"context_manager\", \"response_generator\")\n",
    "complex_workflow.add_edge(\"response_generator\", END)\n",
    "\n",
    "# Compile\n",
    "complex_app = complex_workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Complex State Graph ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Complex State Graph\n",
    "print(\"üöÄ Test Complex State Management...\\n\")\n",
    "\n",
    "test_message = \"Xin ch√†o! T√¥i l√† Minh, ƒëang h·ªçc v·ªÅ AI v√† machine learning. T√¥i mu·ªën t√¨m hi·ªÉu v·ªÅ LangGraph.\"\n",
    "\n",
    "initial_complex_state = {\n",
    "    \"messages\": [HumanMessage(content=test_message)],\n",
    "    \"user_profile\": {},\n",
    "    \"conversation_history\": [],\n",
    "    \"current_topic\": \"introduction\",\n",
    "    \"processing_steps\": [],\n",
    "    \"metadata\": {}\n",
    "}\n",
    "\n",
    "print(f\"üìù Input: {test_message}\\n\")\n",
    "\n",
    "try:\n",
    "    result = complex_app.invoke(initial_complex_state)\n",
    "    \n",
    "    print(\"\\nüìä State Analysis:\")\n",
    "    print(f\"- Processing steps: {result.get('processing_steps', [])}\")\n",
    "    print(f\"- Current topic: {result.get('current_topic', 'unknown')}\")\n",
    "    print(f\"- Conversation history length: {len(result.get('conversation_history', []))}\")\n",
    "    print(f\"- User profile fields: {len(result.get('user_profile', {}))}\")\n",
    "    print(f\"- Total messages: {len(result.get('messages', []))}\")\n",
    "    \n",
    "    print(\"\\nüë§ User Profile:\")\n",
    "    profile = result.get('user_profile', {})\n",
    "    for key, value in profile.items():\n",
    "        if key == 'last_analysis':\n",
    "            print(f\"- {key}: {value[:100]}...\" if len(str(value)) > 100 else f\"- {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"- {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nüìö Conversation History:\")\n",
    "    for step in result.get('conversation_history', []):\n",
    "        print(f\"- Step {step.get('step', '?')}: {step}\")\n",
    "    \n",
    "    print(\"\\nü§ñ Final Response:\")\n",
    "    ai_messages = [msg for msg in result['messages'] if isinstance(msg, AIMessage)]\n",
    "    if ai_messages:\n",
    "        print(ai_messages[-1].content)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming v√† Async Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo streaming execution\n",
    "print(\"üåä Demo Streaming Execution...\\n\")\n",
    "\n",
    "stream_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Gi·∫£i th√≠ch v·ªÅ LangGraph m·ªôt c√°ch chi ti·∫øt\")],\n",
    "    \"step_count\": 0,\n",
    "    \"current_action\": \"starting\"\n",
    "}\n",
    "\n",
    "print(\"üì° Streaming results:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Stream t·ª´ng b∆∞·ªõc\n",
    "    for step_result in simple_app.stream(stream_state):\n",
    "        print(f\"üî∏ Stream step: {list(step_result.keys())}\")\n",
    "        \n",
    "        for node_name, node_result in step_result.items():\n",
    "            print(f\"   Node '{node_name}':\")\n",
    "            print(f\"   - Current action: {node_result.get('current_action', 'unknown')}\")\n",
    "            print(f\"   - Step count: {node_result.get('step_count', 0)}\")\n",
    "            \n",
    "            # Hi·ªÉn th·ªã tin nh·∫Øn n·∫øu c√≥\n",
    "            if 'messages' in node_result and node_result['messages']:\n",
    "                last_msg = node_result['messages'][-1]\n",
    "                if hasattr(last_msg, 'content'):\n",
    "                    content = last_msg.content[:100] + \"...\" if len(last_msg.content) > 100 else last_msg.content\n",
    "                    print(f\"   - Message: {content}\")\n",
    "            print()\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Streaming error: {str(e)}\")\n",
    "\n",
    "print(\"‚úÖ Streaming demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gi·∫£i th√≠ch & Ph√¢n t√≠ch\n",
    "\n",
    "### üéØ C√°c th√†nh ph·∫ßn ƒë√£ s·ª≠ d·ª•ng\n",
    "\n",
    "#### 1. **StateGraph**\n",
    "- **M·ª•c ƒë√≠ch**: Container ch√≠nh ƒë·ªÉ qu·∫£n l√Ω workflow v·ªõi state\n",
    "- **S·ª≠ d·ª•ng**: `StateGraph(StateSchema)` v·ªõi TypedDict schema\n",
    "- **∆Øu ƒëi·ªÉm**: Type safety, state persistence gi·ªØa c√°c nodes\n",
    "\n",
    "#### 2. **add_node()**\n",
    "- **Syntax**: `workflow.add_node(\"name\", function)`\n",
    "- **Function signature**: `(state: StateType) -> Dict[str, Any]`\n",
    "- **Return**: Partial state update (merged v·ªõi existing state)\n",
    "\n",
    "#### 3. **add_edge() & add_conditional_edges()**\n",
    "- **Simple edge**: `add_edge(\"from\", \"to\")` - lu√¥n chuy·ªÉn\n",
    "- **Conditional**: `add_conditional_edges(\"from\", condition_func, mapping)`\n",
    "- **Condition function**: Nh·∫≠n state, tr·∫£ v·ªÅ string key\n",
    "\n",
    "#### 4. **compile() & invoke()**\n",
    "- **compile()**: T·∫°o runnable graph t·ª´ workflow definition\n",
    "- **invoke()**: Th·ª±c thi ƒë·ªìng b·ªô, return final state\n",
    "- **stream()**: Th·ª±c thi v·ªõi streaming, yield intermediate results\n",
    "\n",
    "### üîß Pattern ph·ªï bi·∫øn\n",
    "\n",
    "1. **State Management**\n",
    "   ```python\n",
    "   class MyState(TypedDict):\n",
    "       messages: Annotated[list, add_messages]  # Auto-merge\n",
    "       custom_field: Any  # Overwrite\n",
    "   ```\n",
    "\n",
    "2. **Tool Integration**\n",
    "   ```python\n",
    "   llm_with_tools = llm.bind_tools([tool1, tool2])\n",
    "   # Trong node: check tool_calls, execute, return results\n",
    "   ```\n",
    "\n",
    "3. **Conditional Logic**\n",
    "   ```python\n",
    "   def should_continue(state):\n",
    "       return \"continue\" if condition else \"end\"\n",
    "   ```\n",
    "\n",
    "### ‚ö° Performance Notes\n",
    "- **State size**: Gi·ªØ state nh·ªè g·ªçn, tr√°nh l∆∞u data l·ªõn\n",
    "- **Node functions**: Stateless, pure functions khi c√≥ th·ªÉ\n",
    "- **Error handling**: Wrap trong try-catch, return error state\n",
    "- **Streaming**: S·ª≠ d·ª•ng cho long-running processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T√†i li·ªáu tham kh·∫£o\n",
    "\n",
    "### üìö Official Documentation\n",
    "- **LangGraph API Reference**: https://langchain-ai.github.io/langgraph/reference/\n",
    "- **LangGraph Tutorials**: https://langchain-ai.github.io/langgraph/tutorials/\n",
    "- **StateGraph API**: https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph\n",
    "- **Message Handling**: https://langchain-ai.github.io/langgraph/concepts/low_level/#messages\n",
    "\n",
    "### üîó Related Resources\n",
    "- **LangChain Tools**: https://python.langchain.com/docs/modules/tools/\n",
    "- **Anthropic Claude**: https://docs.anthropic.com/claude/docs\n",
    "- **TypedDict Documentation**: https://docs.python.org/3/library/typing.html#typing.TypedDict\n",
    "\n",
    "### üõ†Ô∏è Code Examples\n",
    "- **LangGraph Examples**: https://github.com/langchain-ai/langgraph/tree/main/examples\n",
    "- **Multi-Agent Systems**: https://langchain-ai.github.io/langgraph/tutorials/multi_agent/\n",
    "- **Tool Calling**: https://langchain-ai.github.io/langgraph/tutorials/tool-calling/\n",
    "\n",
    "### üìñ Best Practices\n",
    "- **State Design Patterns**: Keep state minimal and focused\n",
    "- **Error Handling**: Always handle exceptions in nodes\n",
    "- **Performance**: Use streaming for long-running processes\n",
    "- **Testing**: Test individual nodes before integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K·∫øt lu·∫≠n & B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "### üéì Nh·ªØng g√¨ ƒë√£ h·ªçc\n",
    "1. **C·∫•u tr√∫c LangGraph**: StateGraph, Nodes, Edges, State management\n",
    "2. **API Reference**: C√°ch s·ª≠ d·ª•ng t√†i li·ªáu ch√≠nh th·ª©c v√† t√¨m ki·∫øm th√¥ng tin\n",
    "3. **Th·ª±c h√†nh**: 3 v√≠ d·ª• t·ª´ ƒë∆°n gi·∫£n ƒë·∫øn ph·ª©c t·∫°p\n",
    "4. **Tool Integration**: K·∫øt h·ª£p LLM v·ªõi external tools\n",
    "5. **State Management**: Qu·∫£n l√Ω state ph·ª©c t·∫°p v·ªõi TypedDict\n",
    "6. **Streaming**: Th·ª±c thi b·∫•t ƒë·ªìng b·ªô v√† real-time updates\n",
    "\n",
    "### üöÄ B∆∞·ªõc ti·∫øp theo\n",
    "1. **Kh√°m ph√° API Reference s√¢u h∆°n**:\n",
    "   - ƒê·ªçc documentation c·ªßa c√°c class ch∆∞a s·ª≠ d·ª•ng\n",
    "   - Th·ª≠ nghi·ªám v·ªõi c√°c parameters kh√°c nhau\n",
    "   - T√¨m hi·ªÉu v·ªÅ error handling patterns\n",
    "\n",
    "2. **Th·ª±c h√†nh n√¢ng cao**:\n",
    "   - X√¢y d·ª±ng multi-agent systems\n",
    "   - T√≠ch h·ª£p v·ªõi databases v√† external APIs\n",
    "   - Implement custom state reducers\n",
    "\n",
    "3. **Production readiness**:\n",
    "   - Error handling v√† logging\n",
    "   - Performance optimization\n",
    "   - Testing strategies\n",
    "\n",
    "4. **T√≠ch h·ª£p ecosystem**:\n",
    "   - LangSmith cho monitoring\n",
    "   - LangServe cho deployment\n",
    "   - Custom tools v√† integrations\n",
    "\n",
    "### üí° Tips cho vi·ªác s·ª≠ d·ª•ng API Reference\n",
    "- **Bookmark** c√°c trang quan tr·ªçng\n",
    "- **ƒê·ªçc examples** tr∆∞·ªõc khi implement\n",
    "- **Check version compatibility** khi update\n",
    "- **Contribute back** v·ªõi feedback v√† bug reports\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding v·ªõi LangGraph! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}