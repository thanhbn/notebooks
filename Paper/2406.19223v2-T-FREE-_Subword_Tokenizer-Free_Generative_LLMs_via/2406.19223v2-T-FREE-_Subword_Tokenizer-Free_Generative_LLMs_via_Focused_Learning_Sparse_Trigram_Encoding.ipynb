{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Sparse Trigram Encoding in T-FREE\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the mathematical foundation of sparse trigram encoding\n",
    "- Implement the robust hash function for trigram-to-vocabulary mapping\n",
    "- Explore how sparse representations enable parameter reduction\n",
    "- Analyze the collision resistance and uniqueness properties\n",
    "\n",
    "## Paper Reference\n",
    "This notebook focuses on the core encoding mechanism described in Section 3.1 Step 2 of the T-FREE paper:\n",
    "\n",
    "> \"Next, we define a robust hash function that uniformly encodes a token into n descriptors, where n usually equals the word-length. Specifically, we apply convolutions of size three and byte-wise stride to each word. This operation yields a set of character triplets, which we refer to as 'trigrams'.\" (Section 3.1, Step 2)\n",
    "\n",
    "The sparse encoding is fundamental to T-FREE's ability to achieve >85% parameter reduction while maintaining performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Trigram Extraction\n",
    "\n",
    "Let's start by understanding how T-FREE extracts trigrams from words using \"convolutions of size three\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Set, Tuple, Dict\n",
    "import xxhash\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trigram_extraction(word: str):\n",
    "    \"\"\"Visualize the trigram extraction process.\n",
    "    \n",
    "    From the paper: \"Consequently, 'Hello' is decomposed into \n",
    "    {_He, Hel, ell, llo, lo_}\" (Section 3.1)\n",
    "    \"\"\"\n",
    "    # Add whitespace padding\n",
    "    padded_word = f\"_{word}_\"\n",
    "    \n",
    "    # Extract trigrams with sliding window\n",
    "    trigrams = []\n",
    "    positions = []\n",
    "    \n",
    "    for i in range(len(padded_word) - 2):\n",
    "        trigram = padded_word[i:i+3]\n",
    "        trigrams.append(trigram)\n",
    "        positions.append(i)\n",
    "    \n",
    "    # Visualize the process\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Draw the padded word\n",
    "    for i, char in enumerate(padded_word):\n",
    "        ax.text(i, 0, char, fontsize=20, ha='center', va='center',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue'))\n",
    "    \n",
    "    # Draw trigram windows\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(trigrams)))\n",
    "    for idx, (trigram, pos) in enumerate(zip(trigrams, positions)):\n",
    "        y_offset = -1.5 - (idx % 3) * 0.8\n",
    "        \n",
    "        # Draw bracket\n",
    "        ax.plot([pos-0.3, pos+2.3], [y_offset+0.2, y_offset+0.2], \n",
    "                color=colors[idx], linewidth=2)\n",
    "        ax.plot([pos-0.3, pos-0.3], [y_offset+0.2, y_offset+0.1], \n",
    "                color=colors[idx], linewidth=2)\n",
    "        ax.plot([pos+2.3, pos+2.3], [y_offset+0.2, y_offset+0.1], \n",
    "                color=colors[idx], linewidth=2)\n",
    "        \n",
    "        # Label trigram\n",
    "        ax.text(pos+1, y_offset-0.3, f\"'{trigram}'\", \n",
    "                fontsize=14, ha='center', color=colors[idx], weight='bold')\n",
    "    \n",
    "    ax.set_xlim(-1, len(padded_word))\n",
    "    ax.set_ylim(-5, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Trigram Extraction for '{word}'\", fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return trigrams\n",
    "\n",
    "# Example from the paper\n",
    "trigrams = visualize_trigram_extraction(\"Hello\")\n",
    "print(f\"\\nExtracted trigrams: {trigrams}\")\n",
    "print(f\"Number of trigrams: {len(trigrams)}\")\n",
    "print(f\"\\nNote: This matches the paper's example exactly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sparse Hash Mapping\n",
    "\n",
    "The key innovation is mapping trigrams to vocabulary indices using multiple hash functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseTrigramEncoder:\n",
    "    \"\"\"Implementation of T-FREE's sparse trigram encoding.\n",
    "    \n",
    "    From Section 3.1: \"T-FREE calculates m numerical hashes of each trigram, \n",
    "    which can be considered as identifiers. We map these into the LLMs \n",
    "    embedding matrix by calculating each hash value modulo v.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 8192, num_hashes: int = 8, \n",
    "                 lowercase_ratio: float = 0.5):\n",
    "        self.vocab_size = vocab_size  # v in the paper\n",
    "        self.num_hashes = num_hashes  # m in the paper\n",
    "        self.lowercase_ratio = lowercase_ratio  # k/m ratio\n",
    "        self.num_lowercase = int(num_hashes * lowercase_ratio)\n",
    "        \n",
    "        # Hash seeds for different hash functions\n",
    "        self.hash_seeds = [i * 1337 for i in range(num_hashes)]\n",
    "        \n",
    "    def hash_trigram(self, trigram: str, hash_idx: int, use_lowercase: bool) -> int:\n",
    "        \"\"\"Hash a trigram to vocabulary index.\n",
    "        \n",
    "        From the paper: \"To further exploit word similarities and bootstrap \n",
    "        training, we calculate k∈[0, m) out of these hash calculations \n",
    "        with the lowercased trigram.\" (Section 3.1)\n",
    "        \"\"\"\n",
    "        # Apply lowercase if specified\n",
    "        trigram_to_hash = trigram.lower() if use_lowercase else trigram\n",
    "        \n",
    "        # Create hash with seed\n",
    "        hash_obj = xxhash.xxh32(seed=self.hash_seeds[hash_idx])\n",
    "        hash_obj.update(trigram_to_hash.encode('utf-8'))\n",
    "        hash_value = hash_obj.intdigest()\n",
    "        \n",
    "        # Map to vocabulary using modulo\n",
    "        return hash_value % self.vocab_size\n",
    "    \n",
    "    def encode_trigram(self, trigram: str) -> List[int]:\n",
    "        \"\"\"Encode a single trigram into m vocabulary indices.\"\"\"\n",
    "        indices = []\n",
    "        \n",
    "        for i in range(self.num_hashes):\n",
    "            # First k hashes use lowercase\n",
    "            use_lowercase = i < self.num_lowercase\n",
    "            idx = self.hash_trigram(trigram, i, use_lowercase)\n",
    "            indices.append(idx)\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def visualize_hash_distribution(self, trigrams: List[str]):\n",
    "        \"\"\"Visualize how trigrams are distributed across vocabulary.\"\"\"\n",
    "        all_indices = []\n",
    "        \n",
    "        for trigram in trigrams:\n",
    "            indices = self.encode_trigram(trigram)\n",
    "            all_indices.extend(indices)\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(all_indices, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "        plt.xlabel('Vocabulary Index')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Hash Indices')\n",
    "        \n",
    "        # Plot sparsity pattern\n",
    "        plt.subplot(1, 2, 2)\n",
    "        vocab_usage = np.zeros(self.vocab_size)\n",
    "        for idx in all_indices:\n",
    "            vocab_usage[idx] += 1\n",
    "        \n",
    "        # Show only first 1000 indices for visibility\n",
    "        plt.imshow(vocab_usage[:1000].reshape(40, 25), cmap='hot', aspect='auto')\n",
    "        plt.colorbar(label='Activation Count')\n",
    "        plt.title('Vocabulary Usage Pattern (first 1000 indices)')\n",
    "        plt.xlabel('Index (mod 25)')\n",
    "        plt.ylabel('Index (div 25)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        active_indices = np.sum(vocab_usage > 0)\n",
    "        sparsity = 1 - (active_indices / self.vocab_size)\n",
    "        print(f\"\\nSparsity: {sparsity:.2%} of vocabulary unused\")\n",
    "        print(f\"Active indices: {active_indices} out of {self.vocab_size}\")\n",
    "\n",
    "# Test the sparse encoder\n",
    "encoder = SparseTrigramEncoder(vocab_size=8192, num_hashes=4)\n",
    "\n",
    "# Test on example trigrams\n",
    "test_trigrams = ['_He', 'Hel', 'ell', 'llo', 'lo_']\n",
    "print(\"Trigram encoding examples:\")\n",
    "for trigram in test_trigrams:\n",
    "    indices = encoder.encode_trigram(trigram)\n",
    "    print(f\"  '{trigram}' -> {indices}\")\n",
    "\n",
    "# Visualize distribution\n",
    "encoder.visualize_hash_distribution(test_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word-Level Sparse Activation Patterns\n",
    "\n",
    "Now let's see how complete words are encoded as sparse activation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordSparseEncoder:\n",
    "    \"\"\"Encode complete words using sparse trigram patterns.\n",
    "    \n",
    "    From the paper: \"Overall, we obtain n·m total activations for any \n",
    "    single word.\" (Section 3.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 8192, num_hashes: int = 4):\n",
    "        self.trigram_encoder = SparseTrigramEncoder(vocab_size, num_hashes)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def extract_trigrams(self, word: str) -> List[str]:\n",
    "        \"\"\"Extract trigrams from a word with padding.\"\"\"\n",
    "        padded_word = f\"_{word}_\"\n",
    "        trigrams = []\n",
    "        for i in range(len(padded_word) - 2):\n",
    "            trigrams.append(padded_word[i:i+3])\n",
    "        return trigrams\n",
    "    \n",
    "    def encode_word(self, word: str) -> Tuple[List[int], np.ndarray]:\n",
    "        \"\"\"Encode a word into sparse activation pattern.\n",
    "        \n",
    "        Returns:\n",
    "            indices: List of active vocabulary indices\n",
    "            pattern: Dense binary pattern for visualization\n",
    "        \"\"\"\n",
    "        trigrams = self.extract_trigrams(word)\n",
    "        all_indices = []\n",
    "        \n",
    "        # Collect all indices from all trigrams\n",
    "        for trigram in trigrams:\n",
    "            indices = self.trigram_encoder.encode_trigram(trigram)\n",
    "            all_indices.extend(indices)\n",
    "        \n",
    "        # Create binary pattern\n",
    "        pattern = np.zeros(self.vocab_size)\n",
    "        for idx in all_indices:\n",
    "            pattern[idx] = 1.0\n",
    "            \n",
    "        return all_indices, pattern\n",
    "    \n",
    "    def visualize_word_patterns(self, words: List[str]):\n",
    "        \"\"\"Visualize sparse activation patterns for multiple words.\"\"\"\n",
    "        patterns = []\n",
    "        indices_lists = []\n",
    "        \n",
    "        for word in words:\n",
    "            indices, pattern = self.encode_word(word)\n",
    "            patterns.append(pattern)\n",
    "            indices_lists.append(indices)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # 1. Show individual patterns\n",
    "        ax1 = axes[0]\n",
    "        for i, (word, indices) in enumerate(zip(words, indices_lists)):\n",
    "            y_positions = np.full(len(indices), i)\n",
    "            ax1.scatter(indices, y_positions, alpha=0.6, s=50, label=word)\n",
    "        \n",
    "        ax1.set_yticks(range(len(words)))\n",
    "        ax1.set_yticklabels(words)\n",
    "        ax1.set_xlabel('Vocabulary Index')\n",
    "        ax1.set_title('Sparse Activation Patterns for Different Words')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlim(0, self.vocab_size)\n",
    "        \n",
    "        # 2. Show pattern overlap heatmap\n",
    "        ax2 = axes[1]\n",
    "        n_words = len(words)\n",
    "        overlap_matrix = np.zeros((n_words, n_words))\n",
    "        \n",
    "        for i in range(n_words):\n",
    "            for j in range(n_words):\n",
    "                set_i = set(indices_lists[i])\n",
    "                set_j = set(indices_lists[j])\n",
    "                if i == j:\n",
    "                    overlap_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    intersection = len(set_i & set_j)\n",
    "                    union = len(set_i | set_j)\n",
    "                    overlap_matrix[i, j] = intersection / union if union > 0 else 0\n",
    "        \n",
    "        im = ax2.imshow(overlap_matrix, cmap='YlOrRd', aspect='auto')\n",
    "        ax2.set_xticks(range(n_words))\n",
    "        ax2.set_yticks(range(n_words))\n",
    "        ax2.set_xticklabels(words, rotation=45)\n",
    "        ax2.set_yticklabels(words)\n",
    "        ax2.set_title('Pattern Overlap Matrix (Jaccard Similarity)')\n",
    "        \n",
    "        # Add values to heatmap\n",
    "        for i in range(n_words):\n",
    "            for j in range(n_words):\n",
    "                text = ax2.text(j, i, f'{overlap_matrix[i, j]:.2f}',\n",
    "                               ha='center', va='center', color='black' if overlap_matrix[i, j] < 0.5 else 'white')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax2)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nActivation Statistics:\")\n",
    "        for word, indices in zip(words, indices_lists):\n",
    "            n_trigrams = len(self.extract_trigrams(word))\n",
    "            print(f\"  '{word}': {len(indices)} activations ({n_trigrams} trigrams × {self.trigram_encoder.num_hashes} hashes)\")\n",
    "\n",
    "# Test word encoding\n",
    "word_encoder = WordSparseEncoder(vocab_size=2048, num_hashes=4)\n",
    "\n",
    "# Example words showing morphological relationships\n",
    "test_words = ['hello', 'Hello', 'hell', 'help', 'world', 'word']\n",
    "word_encoder.visualize_word_patterns(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collision Analysis and Uniqueness Guarantees\n",
    "\n",
    "Let's analyze the collision properties of the sparse encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_collision_properties(vocab_size: int, num_hashes: int, n_words: int = 10000):\n",
    "    \"\"\"Analyze collision properties of the sparse encoding.\n",
    "    \n",
    "    From the paper: \"As we ensure that trigram encodings do not collide, \n",
    "    neither will the word encodings.\" (Section 3.1)\n",
    "    \"\"\"\n",
    "    encoder = WordSparseEncoder(vocab_size, num_hashes)\n",
    "    \n",
    "    # Generate random words of varying lengths\n",
    "    import string\n",
    "    import random\n",
    "    \n",
    "    random.seed(42)\n",
    "    words = []\n",
    "    for _ in range(n_words):\n",
    "        length = random.randint(3, 12)\n",
    "        word = ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "        words.append(word)\n",
    "    \n",
    "    # Encode all words\n",
    "    patterns = {}\n",
    "    for word in words:\n",
    "        indices, _ = encoder.encode_word(word)\n",
    "        patterns[word] = tuple(sorted(indices))  # Use tuple for hashing\n",
    "    \n",
    "    # Check for exact collisions\n",
    "    pattern_to_words = defaultdict(list)\n",
    "    for word, pattern in patterns.items():\n",
    "        pattern_to_words[pattern].append(word)\n",
    "    \n",
    "    # Find collisions\n",
    "    collisions = [(pattern, words) for pattern, words in pattern_to_words.items() if len(words) > 1]\n",
    "    \n",
    "    print(f\"\\nCollision Analysis:\")\n",
    "    print(f\"Total words: {n_words}\")\n",
    "    print(f\"Unique patterns: {len(pattern_to_words)}\")\n",
    "    print(f\"Exact collisions: {len(collisions)}\")\n",
    "    print(f\"Collision rate: {len(collisions) / n_words * 100:.4f}%\")\n",
    "    \n",
    "    if collisions:\n",
    "        print(f\"\\nFirst 5 collisions:\")\n",
    "        for pattern, words in collisions[:5]:\n",
    "            print(f\"  Words: {words}\")\n",
    "    \n",
    "    # Analyze near-collisions (high overlap)\n",
    "    high_overlap_threshold = 0.8\n",
    "    near_collisions = 0\n",
    "    \n",
    "    # Sample pairs to check (checking all would be too expensive)\n",
    "    sample_size = min(1000, n_words)\n",
    "    sampled_words = random.sample(words, sample_size)\n",
    "    \n",
    "    for i in range(len(sampled_words)):\n",
    "        for j in range(i+1, len(sampled_words)):\n",
    "            set_i = set(patterns[sampled_words[i]])\n",
    "            set_j = set(patterns[sampled_words[j]])\n",
    "            \n",
    "            if set_i and set_j:\n",
    "                overlap = len(set_i & set_j) / len(set_i | set_j)\n",
    "                if overlap > high_overlap_threshold:\n",
    "                    near_collisions += 1\n",
    "    \n",
    "    total_pairs = sample_size * (sample_size - 1) // 2\n",
    "    print(f\"\\nNear-collision Analysis (>{high_overlap_threshold*100}% overlap):\")\n",
    "    print(f\"Sampled pairs: {total_pairs}\")\n",
    "    print(f\"Near-collisions: {near_collisions}\")\n",
    "    print(f\"Near-collision rate: {near_collisions / total_pairs * 100:.4f}%\")\n",
    "    \n",
    "    return pattern_to_words\n",
    "\n",
    "# Analyze with different vocabulary sizes\n",
    "vocab_sizes = [1024, 2048, 4096, 8192]\n",
    "collision_rates = []\n",
    "\n",
    "for v_size in vocab_sizes:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Vocabulary size: {v_size}\")\n",
    "    pattern_dict = analyze_collision_properties(v_size, num_hashes=4, n_words=5000)\n",
    "    collision_rate = (5000 - len(pattern_dict)) / 5000 * 100\n",
    "    collision_rates.append(collision_rate)\n",
    "\n",
    "# Plot collision rates vs vocabulary size\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(vocab_sizes, collision_rates, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Vocabulary Size')\n",
    "plt.ylabel('Collision Rate (%)')\n",
    "plt.title('Collision Rate vs Vocabulary Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log', base=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Impact of Hash Parameters\n",
    "\n",
    "Let's explore how the number of hash functions (m) and vocabulary size (v) affect the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hash_parameters():\n",
    "    \"\"\"Analyze the impact of hash parameters on encoding quality.\"\"\"\n",
    "    \n",
    "    # Test words with known relationships\n",
    "    word_groups = [\n",
    "        ['run', 'runs', 'running'],\n",
    "        ['happy', 'happiness', 'unhappy'],\n",
    "        ['code', 'coder', 'coding']\n",
    "    ]\n",
    "    \n",
    "    # Parameters to test\n",
    "    num_hashes_list = [2, 4, 8, 16]\n",
    "    vocab_size = 4096\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, m in enumerate(num_hashes_list):\n",
    "        encoder = WordSparseEncoder(vocab_size, num_hashes=m)\n",
    "        \n",
    "        # Calculate average overlap within and between groups\n",
    "        within_group_overlaps = []\n",
    "        between_group_overlaps = []\n",
    "        \n",
    "        all_words = [word for group in word_groups for word in group]\n",
    "        word_patterns = {}\n",
    "        \n",
    "        for word in all_words:\n",
    "            indices, _ = encoder.encode_word(word)\n",
    "            word_patterns[word] = set(indices)\n",
    "        \n",
    "        # Within group overlaps\n",
    "        for group in word_groups:\n",
    "            for i in range(len(group)):\n",
    "                for j in range(i+1, len(group)):\n",
    "                    set_i = word_patterns[group[i]]\n",
    "                    set_j = word_patterns[group[j]]\n",
    "                    if set_i and set_j:\n",
    "                        overlap = len(set_i & set_j) / len(set_i | set_j)\n",
    "                        within_group_overlaps.append(overlap)\n",
    "        \n",
    "        # Between group overlaps\n",
    "        for g1_idx in range(len(word_groups)):\n",
    "            for g2_idx in range(g1_idx+1, len(word_groups)):\n",
    "                for w1 in word_groups[g1_idx]:\n",
    "                    for w2 in word_groups[g2_idx]:\n",
    "                        set_1 = word_patterns[w1]\n",
    "                        set_2 = word_patterns[w2]\n",
    "                        if set_1 and set_2:\n",
    "                            overlap = len(set_1 & set_2) / len(set_1 | set_2)\n",
    "                            between_group_overlaps.append(overlap)\n",
    "        \n",
    "        # Plot results\n",
    "        ax = axes[idx]\n",
    "        data = [within_group_overlaps, between_group_overlaps]\n",
    "        positions = [1, 2]\n",
    "        \n",
    "        bp = ax.boxplot(data, positions=positions, widths=0.6)\n",
    "        ax.set_xticklabels(['Within Group', 'Between Groups'])\n",
    "        ax.set_ylabel('Overlap (Jaccard Similarity)')\n",
    "        ax.set_title(f'm = {m} hashes')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add mean lines\n",
    "        for i, d in enumerate(data):\n",
    "            if d:\n",
    "                mean_val = np.mean(d)\n",
    "                ax.hlines(mean_val, positions[i]-0.3, positions[i]+0.3, \n",
    "                         colors='red', linestyles='dashed', linewidth=2)\n",
    "                ax.text(positions[i], mean_val+0.05, f'{mean_val:.3f}', \n",
    "                       ha='center', va='bottom', color='red')\n",
    "    \n",
    "    plt.suptitle('Impact of Number of Hashes on Word Similarity Preservation', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze sparsity vs number of hashes\n",
    "    print(\"\\nSparsity Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_word = \"example\"\n",
    "    for m in num_hashes_list:\n",
    "        encoder = WordSparseEncoder(vocab_size, num_hashes=m)\n",
    "        indices, pattern = encoder.encode_word(test_word)\n",
    "        n_trigrams = len(encoder.extract_trigrams(test_word))\n",
    "        total_activations = len(indices)\n",
    "        sparsity = 1 - (total_activations / vocab_size)\n",
    "        \n",
    "        print(f\"\\nm = {m}:\")\n",
    "        print(f\"  Word: '{test_word}' ({n_trigrams} trigrams)\")\n",
    "        print(f\"  Total activations: {total_activations} (= {n_trigrams} × {m})\")\n",
    "        print(f\"  Sparsity: {sparsity:.2%}\")\n",
    "        print(f\"  Activation density: {total_activations/vocab_size:.4f}\")\n",
    "\n",
    "analyze_hash_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Theoretical Analysis of Sparse Encoding\n",
    "\n",
    "Let's analyze the theoretical properties that make sparse trigram encoding effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_analysis():\n",
    "    \"\"\"Analyze theoretical properties of sparse trigram encoding.\"\"\"\n",
    "    \n",
    "    print(\"Theoretical Analysis of T-FREE Sparse Encoding\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Information capacity\n",
    "    print(\"\\n1. Information Capacity:\")\n",
    "    \n",
    "    # Number of possible trigrams\n",
    "    alphabet_size = 26 + 1  # letters + underscore\n",
    "    n_possible_trigrams = alphabet_size ** 3\n",
    "    print(f\"   Possible trigrams (lowercase only): {n_possible_trigrams:,}\")\n",
    "    print(f\"   With case sensitivity: ~{2 * n_possible_trigrams:,}\")\n",
    "    \n",
    "    # Vocabulary sizes in practice\n",
    "    vocab_sizes = [4096, 8192, 16384, 32768]\n",
    "    for v in vocab_sizes:\n",
    "        ratio = n_possible_trigrams / v\n",
    "        print(f\"   Vocab size {v}: {ratio:.1f} trigrams per index (average)\")\n",
    "    \n",
    "    # 2. Collision probability\n",
    "    print(\"\\n2. Collision Probability (Birthday Paradox):\")\n",
    "    \n",
    "    def collision_probability(n_items, n_buckets):\n",
    "        \"\"\"Calculate collision probability using birthday paradox.\"\"\"\n",
    "        if n_items > n_buckets:\n",
    "            return 1.0\n",
    "        \n",
    "        # P(no collision) = (n/n) * ((n-1)/n) * ((n-2)/n) * ...\n",
    "        p_no_collision = 1.0\n",
    "        for i in range(n_items):\n",
    "            p_no_collision *= (n_buckets - i) / n_buckets\n",
    "        \n",
    "        return 1 - p_no_collision\n",
    "    \n",
    "    # For different numbers of words\n",
    "    n_words_list = [100, 1000, 10000, 50000]\n",
    "    vocab_size = 8192\n",
    "    num_hashes = 4\n",
    "    avg_word_length = 6  # Average English word length\n",
    "    \n",
    "    for n_words in n_words_list:\n",
    "        # Estimate total activations\n",
    "        total_activations = n_words * avg_word_length * num_hashes\n",
    "        \n",
    "        # This is an approximation - actual collision rate depends on pattern overlap\n",
    "        p_collision = collision_probability(total_activations // 10, vocab_size)\n",
    "        \n",
    "        print(f\"   {n_words:,} words: ~{p_collision:.4%} collision probability\")\n",
    "    \n",
    "    # 3. Embedding efficiency\n",
    "    print(\"\\n3. Embedding Efficiency:\")\n",
    "    \n",
    "    # Traditional vs T-FREE\n",
    "    traditional_vocab_sizes = [32000, 64000, 128000, 256000]\n",
    "    tfree_vocab_size = 8192\n",
    "    hidden_size = 768  # BERT-base size\n",
    "    \n",
    "    print(f\"\\n   Hidden size: {hidden_size}\")\n",
    "    print(f\"   T-FREE vocab size: {tfree_vocab_size:,}\")\n",
    "    print(\"\\n   Embedding layer parameters:\")\n",
    "    \n",
    "    tfree_params = tfree_vocab_size * hidden_size\n",
    "    \n",
    "    for trad_size in traditional_vocab_sizes:\n",
    "        trad_params = trad_size * hidden_size\n",
    "        reduction = (1 - tfree_params / trad_params) * 100\n",
    "        print(f\"     Traditional ({trad_size:,}): {trad_params/1e6:.1f}M params\")\n",
    "        print(f\"     T-FREE reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    # 4. Morphological similarity preservation\n",
    "    print(\"\\n4. Morphological Similarity Preservation:\")\n",
    "    \n",
    "    def trigram_overlap(word1, word2):\n",
    "        \"\"\"Calculate trigram overlap between two words.\"\"\"\n",
    "        pad1 = f\"_{word1}_\"\n",
    "        pad2 = f\"_{word2}_\"\n",
    "        \n",
    "        trigrams1 = set(pad1[i:i+3] for i in range(len(pad1)-2))\n",
    "        trigrams2 = set(pad2[i:i+3] for i in range(len(pad2)-2))\n",
    "        \n",
    "        if not trigrams1 or not trigrams2:\n",
    "            return 0\n",
    "        \n",
    "        return len(trigrams1 & trigrams2) / len(trigrams1 | trigrams2)\n",
    "    \n",
    "    # Test morphological variants\n",
    "    word_pairs = [\n",
    "        ('run', 'runs'),\n",
    "        ('run', 'running'),\n",
    "        ('happy', 'unhappy'),\n",
    "        ('compute', 'computer'),\n",
    "        ('run', 'walk'),  # Unrelated\n",
    "        ('cat', 'dog')    # Unrelated\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n   Trigram overlap for word pairs:\")\n",
    "    for w1, w2 in word_pairs:\n",
    "        overlap = trigram_overlap(w1, w2)\n",
    "        print(f\"     '{w1}' - '{w2}': {overlap:.2%}\")\n",
    "\n",
    "theoretical_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Implementation Considerations\n",
    "\n",
    "Let's explore practical considerations for implementing sparse trigram encoding in production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSparseEncoder:\n",
    "    \"\"\"Optimized implementation with caching and batch processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 8192, num_hashes: int = 4):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hashes = num_hashes\n",
    "        self.encoder = SparseTrigramEncoder(vocab_size, num_hashes)\n",
    "        \n",
    "        # Cache for trigram encodings\n",
    "        self.trigram_cache = {}\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "    def encode_trigram_cached(self, trigram: str) -> List[int]:\n",
    "        \"\"\"Encode trigram with caching.\"\"\"\n",
    "        if trigram in self.trigram_cache:\n",
    "            self.cache_hits += 1\n",
    "            return self.trigram_cache[trigram]\n",
    "        \n",
    "        self.cache_misses += 1\n",
    "        indices = self.encoder.encode_trigram(trigram)\n",
    "        self.trigram_cache[trigram] = indices\n",
    "        return indices\n",
    "    \n",
    "    def batch_encode_words(self, words: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Efficiently encode a batch of words into sparse tensors.\"\"\"\n",
    "        batch_size = len(words)\n",
    "        \n",
    "        # Collect all unique trigrams first\n",
    "        all_trigrams = set()\n",
    "        word_trigrams = []\n",
    "        \n",
    "        for word in words:\n",
    "            padded = f\"_{word}_\"\n",
    "            trigrams = [padded[i:i+3] for i in range(len(padded)-2)]\n",
    "            word_trigrams.append(trigrams)\n",
    "            all_trigrams.update(trigrams)\n",
    "        \n",
    "        # Encode all unique trigrams\n",
    "        for trigram in all_trigrams:\n",
    "            self.encode_trigram_cached(trigram)\n",
    "        \n",
    "        # Create sparse tensor representation\n",
    "        # Using COO format for efficiency\n",
    "        indices = []\n",
    "        values = []\n",
    "        \n",
    "        for batch_idx, trigrams in enumerate(word_trigrams):\n",
    "            for trigram in trigrams:\n",
    "                vocab_indices = self.trigram_cache[trigram]\n",
    "                for vocab_idx in vocab_indices:\n",
    "                    indices.append([batch_idx, vocab_idx])\n",
    "                    values.append(1.0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        if indices:\n",
    "            indices_tensor = torch.tensor(indices).t()\n",
    "            values_tensor = torch.tensor(values)\n",
    "            sparse_tensor = torch.sparse_coo_tensor(\n",
    "                indices_tensor, \n",
    "                values_tensor,\n",
    "                (batch_size, self.vocab_size)\n",
    "            )\n",
    "        else:\n",
    "            sparse_tensor = torch.sparse_coo_tensor(\n",
    "                torch.zeros(2, 0, dtype=torch.long),\n",
    "                torch.zeros(0),\n",
    "                (batch_size, self.vocab_size)\n",
    "            )\n",
    "        \n",
    "        return sparse_tensor\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get cache performance statistics.\"\"\"\n",
    "        total_accesses = self.cache_hits + self.cache_misses\n",
    "        hit_rate = self.cache_hits / total_accesses if total_accesses > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'cache_size': len(self.trigram_cache),\n",
    "            'cache_hits': self.cache_hits,\n",
    "            'cache_misses': self.cache_misses,\n",
    "            'hit_rate': hit_rate\n",
    "        }\n",
    "\n",
    "# Performance comparison\n",
    "import time\n",
    "\n",
    "# Generate test data\n",
    "test_words = ['hello', 'world', 'this', 'is', 'a', 'test', 'of', 'sparse', 'encoding'] * 100\n",
    "encoder = OptimizedSparseEncoder()\n",
    "\n",
    "# Time batch encoding\n",
    "start_time = time.time()\n",
    "sparse_batch = encoder.batch_encode_words(test_words)\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"Batch Encoding Performance:\")\n",
    "print(f\"  Words encoded: {len(test_words)}\")\n",
    "print(f\"  Time taken: {batch_time:.4f} seconds\")\n",
    "print(f\"  Words per second: {len(test_words)/batch_time:.0f}\")\n",
    "print(f\"\\nCache Statistics:\")\n",
    "for key, value in encoder.get_cache_stats().items():\n",
    "    if key == 'hit_rate':\n",
    "        print(f\"  {key}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Memory usage analysis\n",
    "print(f\"\\nMemory Usage:\")\n",
    "dense_size = len(test_words) * encoder.vocab_size * 4  # 4 bytes per float32\n",
    "sparse_nnz = sparse_batch._nnz()\n",
    "sparse_size = sparse_nnz * (8 + 4)  # 8 bytes for indices, 4 for values\n",
    "\n",
    "print(f\"  Dense representation: {dense_size / 1024:.1f} KB\")\n",
    "print(f\"  Sparse representation: {sparse_size / 1024:.1f} KB\")\n",
    "print(f\"  Compression ratio: {dense_size / sparse_size:.1f}x\")\n",
    "print(f\"  Sparsity: {1 - sparse_nnz / (len(test_words) * encoder.vocab_size):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of Sparse Patterns\n",
    "\n",
    "Let's create a comprehensive visualization of how sparse trigram encoding works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_visualization():\n",
    "    \"\"\"Create a comprehensive visualization of sparse trigram encoding.\"\"\"\n",
    "    \n",
    "    # Example sentence\n",
    "    sentence = \"T-FREE reduces parameters\"\n",
    "    words = sentence.split()\n",
    "    \n",
    "    encoder = WordSparseEncoder(vocab_size=512, num_hashes=3)  # Small for visualization\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1.5])\n",
    "    \n",
    "    # 1. Word to trigrams\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.text(0.5, 0.8, f'Input: \"{sentence}\"', fontsize=16, ha='center', transform=ax1.transAxes)\n",
    "    \n",
    "    y_pos = 0.5\n",
    "    x_positions = np.linspace(0.1, 0.9, len(words))\n",
    "    \n",
    "    for i, (word, x_pos) in enumerate(zip(words, x_positions)):\n",
    "        # Word\n",
    "        ax1.text(x_pos, y_pos, word, fontsize=14, ha='center', \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue'))\n",
    "        \n",
    "        # Trigrams\n",
    "        trigrams = encoder.extract_trigrams(word)\n",
    "        trigram_y = y_pos - 0.3\n",
    "        \n",
    "        for j, trigram in enumerate(trigrams):\n",
    "            offset = (j - len(trigrams)/2 + 0.5) * 0.08\n",
    "            ax1.text(x_pos + offset, trigram_y, f\"'{trigram}'\", \n",
    "                    fontsize=10, ha='center', style='italic')\n",
    "            ax1.arrow(x_pos, y_pos - 0.05, offset, -0.15, \n",
    "                     head_width=0.01, head_length=0.02, fc='gray', ec='gray', alpha=0.5)\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Step 1: Word → Trigram Decomposition', fontsize=14, pad=20)\n",
    "    \n",
    "    # 2. Trigram to indices\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    # Show hashing for one word\n",
    "    example_word = words[0]\n",
    "    trigrams = encoder.extract_trigrams(example_word)\n",
    "    \n",
    "    ax2.text(0.5, 0.9, f'Hashing trigrams from \"{example_word}\":', \n",
    "            fontsize=12, ha='center', transform=ax2.transAxes)\n",
    "    \n",
    "    y_start = 0.7\n",
    "    for i, trigram in enumerate(trigrams[:3]):  # Show first 3\n",
    "        y = y_start - i * 0.2\n",
    "        ax2.text(0.1, y, f\"'{trigram}' →\", fontsize=10, transform=ax2.transAxes)\n",
    "        \n",
    "        indices = encoder.trigram_encoder.encode_trigram(trigram)\n",
    "        indices_str = ', '.join(map(str, indices[:3]))\n",
    "        ax2.text(0.4, y, f\"[{indices_str}, ...]\"", fontsize=10, \n",
    "                transform=ax2.transAxes, color='red')\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Step 2: Trigram → Vocabulary Indices', fontsize=12)\n",
    "    \n",
    "    # Show sparsity pattern\n",
    "    _, pattern = encoder.encode_word(example_word)\n",
    "    pattern_2d = pattern.reshape(32, 16)  # Reshape for visualization\n",
    "    \n",
    "    im = ax3.imshow(pattern_2d, cmap='Blues', aspect='auto')\n",
    "    ax3.set_title(f'Sparse Pattern for \"{example_word}\"', fontsize=12)\n",
    "    ax3.set_xlabel('Vocabulary Index (mod 16)')\n",
    "    ax3.set_ylabel('Vocabulary Index (div 16)')\n",
    "    \n",
    "    # 3. Full sentence encoding\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    \n",
    "    # Encode all words\n",
    "    all_patterns = []\n",
    "    for word in words:\n",
    "        _, pattern = encoder.encode_word(word)\n",
    "        all_patterns.append(pattern)\n",
    "    \n",
    "    # Stack patterns\n",
    "    pattern_matrix = np.array(all_patterns)\n",
    "    \n",
    "    # Visualize\n",
    "    im = ax4.imshow(pattern_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    ax4.set_yticks(range(len(words)))\n",
    "    ax4.set_yticklabels(words)\n",
    "    ax4.set_xlabel('Vocabulary Index')\n",
    "    ax4.set_title('Step 3: Complete Sparse Encoding Matrix', fontsize=14)\n",
    "    \n",
    "    # Add sparsity info\n",
    "    total_activations = np.sum(pattern_matrix > 0)\n",
    "    total_elements = pattern_matrix.size\n",
    "    sparsity = 1 - (total_activations / total_elements)\n",
    "    \n",
    "    ax4.text(0.02, 0.95, f'Sparsity: {sparsity:.1%}', \n",
    "            transform=ax4.transAxes, fontsize=12,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.colorbar(im, ax=ax4, label='Activation')\n",
    "    \n",
    "    plt.suptitle('T-FREE Sparse Trigram Encoding Process', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "comprehensive_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Sparse Representation**: Each word is represented by n×m active indices out of v total vocabulary slots, where n is the number of trigrams and m is the number of hash functions.\n",
    "\n",
    "2. **Collision Resistance**: By using multiple hash functions and ensuring unique patterns through their combination, T-FREE achieves very low collision rates even with small vocabulary sizes.\n",
    "\n",
    "3. **Morphological Preservation**: Words with similar spellings naturally share trigrams, leading to overlapping activation patterns that preserve morphological relationships.\n",
    "\n",
    "4. **Parameter Efficiency**: The sparse encoding enables dramatic reduction in embedding layer size (>85%) while maintaining expressiveness.\n",
    "\n",
    "5. **Implementation Considerations**: Caching trigram encodings and using sparse tensor representations are crucial for efficient implementation.\n",
    "\n",
    "This sparse trigram encoding forms the foundation of T-FREE's ability to eliminate traditional tokenizers while achieving superior cross-lingual performance and parameter efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}