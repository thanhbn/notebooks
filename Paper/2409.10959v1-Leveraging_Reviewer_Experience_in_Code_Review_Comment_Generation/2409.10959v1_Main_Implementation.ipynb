{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Reviewer Experience in Code Review Comment Generation - Main Implementation\n",
    "\n",
    "## Paper Information\n",
    "- **Title**: Leveraging Reviewer Experience in Code Review Comment Generation\n",
    "- **Authors**: Hong Yi Lin, Patanamon Thongtanunam, Christoph Treude, Michael W. Godfrey, Chunhua Liu, Wachiraphan Charoenwet\n",
    "- **Link**: [arXiv:2409.10959v1](https://arxiv.org/abs/2409.10959v1)\n",
    "- **Institution**: The University of Melbourne, Singapore Management University, University of Waterloo\n",
    "\n",
    "## Abstract Summary\n",
    "This paper proposes Experience-aware Loss Functions (ELF) to improve code review comment generation by leveraging reviewers' past authoring and reviewing experiences. The method assigns weights to the model's loss function proportional to reviewer experience, allowing experienced reviewers' comments to have more influence over model behavior. Results show ELF achieves +29% more applicable comments, +56% more suggestions, and +129% more functional issues identified compared to state-of-the-art models.\n",
    "\n",
    "## Key Contributions\n",
    "1. Experience-aware training methods for code review comment generation\n",
    "2. Analysis of emergent behaviors after experience-aware training\n",
    "3. Large-scale datasets with commit/PR histories for 826 GitHub repositories\n",
    "4. Augmented CodeReviewer dataset tagged with ownership metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers datasets\n",
    "!pip install langchain langchain-openai langchain-community\n",
    "!pip install numpy pandas matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install deepeval\n",
    "!pip install pygithub pydriller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Structures and Mock Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CodeReviewComment:\n",
    "    \"\"\"Data structure for code review comments\"\"\"\n",
    "    comment_id: str\n",
    "    reviewer_id: str\n",
    "    repository: str\n",
    "    subsystem: str  # Top-level directory\n",
    "    package: str    # Immediate folder\n",
    "    code_change: str\n",
    "    comment_text: str\n",
    "    timestamp: datetime\n",
    "    \n",
    "@dataclass\n",
    "class ReviewerMetrics:\n",
    "    \"\"\"Reviewer ownership metrics at different granularities\"\"\"\n",
    "    reviewer_id: str\n",
    "    aco_repo: float  # Authoring Code Ownership - Repository\n",
    "    aco_sys: float   # Authoring Code Ownership - Subsystem\n",
    "    aco_pkg: float   # Authoring Code Ownership - Package\n",
    "    rso_repo: float  # Review-Specific Ownership - Repository\n",
    "    rso_sys: float   # Review-Specific Ownership - Subsystem\n",
    "    rso_pkg: float   # Review-Specific Ownership - Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_data(n_samples: int = 1000) -> Tuple[List[CodeReviewComment], Dict[str, ReviewerMetrics]]:\n",
    "    \"\"\"Generate mock code review data for demonstration\"\"\"\n",
    "    \n",
    "    # Mock reviewers with varying experience levels\n",
    "    reviewers = [\n",
    "        {\"id\": \"exp_reviewer_1\", \"exp_level\": \"high\"},\n",
    "        {\"id\": \"exp_reviewer_2\", \"exp_level\": \"high\"},\n",
    "        {\"id\": \"mid_reviewer_1\", \"exp_level\": \"medium\"},\n",
    "        {\"id\": \"mid_reviewer_2\", \"exp_level\": \"medium\"},\n",
    "        {\"id\": \"new_reviewer_1\", \"exp_level\": \"low\"},\n",
    "        {\"id\": \"new_reviewer_2\", \"exp_level\": \"low\"},\n",
    "    ]\n",
    "    \n",
    "    # Mock code changes and corresponding review comments\n",
    "    code_change_templates = [\n",
    "        {\n",
    "            \"code\": \"if (user.role == 'admin') { processAdminRequest(request); }\",\n",
    "            \"high_exp_comment\": \"Missing validation check. Add: if (!validateRequest(request)) return;\",\n",
    "            \"low_exp_comment\": \"Is this correct?\",\n",
    "            \"type\": \"validation\"\n",
    "        },\n",
    "        {\n",
    "            \"code\": \"for i in range(len(data)): result.append(data[i] * 2)\",\n",
    "            \"high_exp_comment\": \"Use list comprehension for better performance: result = [x * 2 for x in data]\",\n",
    "            \"low_exp_comment\": \"Please add spaces around operators\",\n",
    "            \"type\": \"performance\"\n",
    "        },\n",
    "        {\n",
    "            \"code\": \"connection = db.connect()\\ndata = connection.query(sql)\",\n",
    "            \"high_exp_comment\": \"Resource leak. Use try-finally or context manager to ensure connection.close()\",\n",
    "            \"low_exp_comment\": \"Add comment here\",\n",
    "            \"type\": \"resource\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Generate reviewer metrics\n",
    "    reviewer_metrics = {}\n",
    "    for reviewer in reviewers:\n",
    "        if reviewer[\"exp_level\"] == \"high\":\n",
    "            base_aco = np.random.uniform(0.15, 0.35)\n",
    "            base_rso = np.random.uniform(0.25, 0.45)\n",
    "        elif reviewer[\"exp_level\"] == \"medium\":\n",
    "            base_aco = np.random.uniform(0.05, 0.15)\n",
    "            base_rso = np.random.uniform(0.10, 0.25)\n",
    "        else:\n",
    "            base_aco = np.random.uniform(0.01, 0.05)\n",
    "            base_rso = np.random.uniform(0.02, 0.10)\n",
    "            \n",
    "        # Ownership increases at finer granularities\n",
    "        reviewer_metrics[reviewer[\"id\"]] = ReviewerMetrics(\n",
    "            reviewer_id=reviewer[\"id\"],\n",
    "            aco_repo=base_aco,\n",
    "            aco_sys=base_aco * 1.2,\n",
    "            aco_pkg=base_aco * 1.5,\n",
    "            rso_repo=base_rso,\n",
    "            rso_sys=base_rso * 1.3,\n",
    "            rso_pkg=base_rso * 1.6\n",
    "        )\n",
    "    \n",
    "    # Generate code review comments\n",
    "    comments = []\n",
    "    for i in range(n_samples):\n",
    "        reviewer = np.random.choice(reviewers)\n",
    "        template = np.random.choice(code_change_templates)\n",
    "        \n",
    "        # Select comment based on reviewer experience\n",
    "        if reviewer[\"exp_level\"] == \"high\":\n",
    "            comment_text = template[\"high_exp_comment\"]\n",
    "        else:\n",
    "            comment_text = template[\"low_exp_comment\"]\n",
    "            \n",
    "        comment = CodeReviewComment(\n",
    "            comment_id=f\"comment_{i}\",\n",
    "            reviewer_id=reviewer[\"id\"],\n",
    "            repository=\"mock_repo\",\n",
    "            subsystem=f\"src/module_{np.random.randint(1, 4)}\",\n",
    "            package=f\"src/module_{np.random.randint(1, 4)}/submodule_{np.random.randint(1, 3)}\",\n",
    "            code_change=template[\"code\"],\n",
    "            comment_text=comment_text,\n",
    "            timestamp=datetime.now()\n",
    "        )\n",
    "        comments.append(comment)\n",
    "    \n",
    "    return comments, reviewer_metrics\n",
    "\n",
    "# Generate mock data\n",
    "mock_comments, mock_metrics = generate_mock_data(1000)\n",
    "print(f\"Generated {len(mock_comments)} mock comments\")\n",
    "print(f\"Generated metrics for {len(mock_metrics)} reviewers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ownership Metrics Calculation (ACO & RSO)\n",
    "\n",
    "Implementation of Authoring Code Ownership (ACO) and Review-Specific Ownership (RSO) metrics from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnershipCalculator:\n",
    "    \"\"\"Calculate ACO and RSO metrics at different granularities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.commits = {}  # {granularity: {developer: count}}\n",
    "        self.reviews = {}  # {granularity: {developer: count}}\n",
    "        \n",
    "    def calculate_aco(self, developer: str, granularity: str, \n",
    "                     review_timestamp: datetime) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Authoring Code Ownership (ACO) - Equation (1) from paper\n",
    "        ACO(D,G) = α(D,G) / C(G)\n",
    "        where:\n",
    "        - α(D,G) = commits by developer D at granularity G\n",
    "        - C(G) = total commits at granularity G\n",
    "        \"\"\"\n",
    "        if granularity not in self.commits:\n",
    "            return 0.0\n",
    "            \n",
    "        developer_commits = self.commits[granularity].get(developer, 0)\n",
    "        total_commits = sum(self.commits[granularity].values())\n",
    "        \n",
    "        if total_commits == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return developer_commits / total_commits\n",
    "    \n",
    "    def calculate_rso(self, developer: str, granularity: str,\n",
    "                     review_timestamp: datetime) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Review-Specific Ownership (RSO) - Equation (2) from paper\n",
    "        RSO(D,G) = r(D,G) / ρ(G)\n",
    "        where:\n",
    "        - r(D,G) = PRs reviewed by developer D at granularity G\n",
    "        - ρ(G) = total PRs at granularity G\n",
    "        \"\"\"\n",
    "        if granularity not in self.reviews:\n",
    "            return 0.0\n",
    "            \n",
    "        developer_reviews = self.reviews[granularity].get(developer, 0)\n",
    "        total_reviews = sum(self.reviews[granularity].values())\n",
    "        \n",
    "        if total_reviews == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return developer_reviews / total_reviews\n",
    "    \n",
    "    def add_commit(self, developer: str, granularity: str, timestamp: datetime):\n",
    "        \"\"\"Add a commit record\"\"\"\n",
    "        if granularity not in self.commits:\n",
    "            self.commits[granularity] = {}\n",
    "        self.commits[granularity][developer] = self.commits[granularity].get(developer, 0) + 1\n",
    "        \n",
    "    def add_review(self, developer: str, granularity: str, timestamp: datetime):\n",
    "        \"\"\"Add a review record\"\"\"\n",
    "        if granularity not in self.reviews:\n",
    "            self.reviews[granularity] = {}\n",
    "        self.reviews[granularity][developer] = self.reviews[granularity].get(developer, 0) + 1\n",
    "\n",
    "# Demonstrate ownership calculation\n",
    "calculator = OwnershipCalculator()\n",
    "\n",
    "# Simulate historical data\n",
    "for i in range(1000):\n",
    "    dev = f\"exp_reviewer_{np.random.randint(1, 3)}\"\n",
    "    gran = np.random.choice([\"repository\", \"subsystem\", \"package\"])\n",
    "    calculator.add_commit(dev, gran, datetime.now())\n",
    "    \n",
    "for i in range(1500):\n",
    "    dev = f\"exp_reviewer_{np.random.randint(1, 3)}\"\n",
    "    gran = np.random.choice([\"repository\", \"subsystem\", \"package\"])\n",
    "    calculator.add_review(dev, gran, datetime.now())\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Sample Ownership Metrics:\")\n",
    "for dev in [\"exp_reviewer_1\", \"exp_reviewer_2\"]:\n",
    "    aco_repo = calculator.calculate_aco(dev, \"repository\", datetime.now())\n",
    "    rso_repo = calculator.calculate_rso(dev, \"repository\", datetime.now())\n",
    "    print(f\"{dev}: ACO(repo)={aco_repo:.3f}, RSO(repo)={rso_repo:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experience-Aware Loss Functions (ELF) Implementation\n",
    "\n",
    "Core implementation of the ELF method with four weighting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceAwareLoss(nn.Module):\n",
    "    \"\"\"Experience-Aware Loss Function (ELF) - Equation (3) from paper\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy: str = \"aco\", granularity: str = \"package\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            strategy: One of [\"aco\", \"rso\", \"avg\", \"max\"]\n",
    "            granularity: One of [\"repository\", \"subsystem\", \"package\"]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.strategy = strategy\n",
    "        self.granularity = granularity\n",
    "        self.base_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def calculate_weight(self, metrics: ReviewerMetrics) -> float:\n",
    "        \"\"\"Calculate weight ω based on strategy and granularity\"\"\"\n",
    "        \n",
    "        # Get ownership values based on granularity\n",
    "        if self.granularity == \"repository\":\n",
    "            aco = metrics.aco_repo\n",
    "            rso = metrics.rso_repo\n",
    "        elif self.granularity == \"subsystem\":\n",
    "            aco = metrics.aco_sys\n",
    "            rso = metrics.rso_sys\n",
    "        else:  # package\n",
    "            aco = metrics.aco_pkg\n",
    "            rso = metrics.rso_pkg\n",
    "            \n",
    "        # Apply strategy-specific weight calculation\n",
    "        if self.strategy == \"aco\":\n",
    "            # ω_aco = e^(1+aco)\n",
    "            weight = np.exp(1 + aco)\n",
    "        elif self.strategy == \"rso\":\n",
    "            # ω_rso = e^(1+rso)\n",
    "            weight = np.exp(1 + rso)\n",
    "        elif self.strategy == \"avg\":\n",
    "            # ω_avg = e^(1+(rso+aco)/2)\n",
    "            weight = np.exp(1 + (rso + aco) / 2)\n",
    "        else:  # max\n",
    "            # ω_max = e^(1+max(rso,aco))\n",
    "            weight = np.exp(1 + max(rso, aco))\n",
    "            \n",
    "        return weight\n",
    "    \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor, \n",
    "                reviewer_metrics: List[ReviewerMetrics]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate experience-aware loss\n",
    "        L_RCG = ω * Σ(-log P(w_t|c,w_<t))\n",
    "        \"\"\"\n",
    "        # Get base loss for each sample\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(-1, vocab_size)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        base_losses = self.base_loss(logits_flat, targets_flat)\n",
    "        base_losses = base_losses.view(batch_size, seq_len)\n",
    "        \n",
    "        # Apply experience-aware weights\n",
    "        weighted_losses = []\n",
    "        for i, metrics in enumerate(reviewer_metrics):\n",
    "            weight = self.calculate_weight(metrics)\n",
    "            weighted_loss = weight * base_losses[i].mean()\n",
    "            weighted_losses.append(weighted_loss)\n",
    "            \n",
    "        return torch.stack(weighted_losses).mean()\n",
    "\n",
    "# Demonstrate ELF with different strategies\n",
    "print(\"ELF Weight Examples:\")\n",
    "for strategy in [\"aco\", \"rso\", \"avg\", \"max\"]:\n",
    "    elf = ExperienceAwareLoss(strategy=strategy, granularity=\"package\")\n",
    "    \n",
    "    # High experience reviewer\n",
    "    high_exp_metrics = mock_metrics[\"exp_reviewer_1\"]\n",
    "    high_weight = elf.calculate_weight(high_exp_metrics)\n",
    "    \n",
    "    # Low experience reviewer\n",
    "    low_exp_metrics = mock_metrics[\"new_reviewer_1\"]\n",
    "    low_weight = elf.calculate_weight(low_exp_metrics)\n",
    "    \n",
    "    print(f\"\\nStrategy: {strategy}\")\n",
    "    print(f\"  High exp weight: {high_weight:.3f}\")\n",
    "    print(f\"  Low exp weight: {low_weight:.3f}\")\n",
    "    print(f\"  Weight ratio: {high_weight/low_weight:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code Review Comment Generation Model\n",
    "\n",
    "Simplified implementation using LangChain for demonstration (paper uses T5-based CodeReviewer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "class CodeReviewGenerator:\n",
    "    \"\"\"Simplified code review comment generator using LangChain\"\"\"\n",
    "    \n",
    "    def __init__(self, experience_aware: bool = True):\n",
    "        self.experience_aware = experience_aware\n",
    "        \n",
    "        # Define prompts based on experience awareness\n",
    "        if experience_aware:\n",
    "            self.prompt_template = PromptTemplate(\n",
    "                input_variables=[\"code_change\", \"reviewer_experience\"],\n",
    "                template=\"\"\"\n",
    "You are an experienced code reviewer with {reviewer_experience} level expertise.\n",
    "Review the following code change and provide a detailed, technical comment:\n",
    "\n",
    "Code Change:\n",
    "{code_change}\n",
    "\n",
    "Provide a review comment that:\n",
    "1. Identifies specific issues (functional, validation, resource, etc.)\n",
    "2. Suggests concrete improvements with code examples when applicable\n",
    "3. Explains the rationale behind your suggestions\n",
    "\n",
    "Review Comment:\"\"\"\n",
    "            )\n",
    "        else:\n",
    "            self.prompt_template = PromptTemplate(\n",
    "                input_variables=[\"code_change\"],\n",
    "                template=\"\"\"\n",
    "Review the following code change:\n",
    "\n",
    "Code Change:\n",
    "{code_change}\n",
    "\n",
    "Review Comment:\"\"\"\n",
    "            )\n",
    "    \n",
    "    def generate_comment(self, code_change: str, reviewer_metrics: Optional[ReviewerMetrics] = None) -> str:\n",
    "        \"\"\"Generate a code review comment\"\"\"\n",
    "        \n",
    "        if self.experience_aware and reviewer_metrics:\n",
    "            # Determine experience level based on metrics\n",
    "            avg_ownership = (reviewer_metrics.aco_pkg + reviewer_metrics.rso_pkg) / 2\n",
    "            if avg_ownership > 0.2:\n",
    "                exp_level = \"high\"\n",
    "            elif avg_ownership > 0.1:\n",
    "                exp_level = \"medium\"\n",
    "            else:\n",
    "                exp_level = \"low\"\n",
    "                \n",
    "            # For demonstration, return experience-aware mock responses\n",
    "            if exp_level == \"high\":\n",
    "                if \"validation\" in code_change or \"admin\" in code_change:\n",
    "                    return \"Missing validation check. Add: if (!validateRequest(request)) return; This prevents unauthorized access.\"\n",
    "                elif \"connect\" in code_change:\n",
    "                    return \"Resource leak detected. Use try-finally or context manager to ensure connection.close(). This prevents database connection exhaustion.\"\n",
    "                else:\n",
    "                    return \"Consider refactoring this logic into a separate method for better testability and reusability.\"\n",
    "            else:\n",
    "                return \"Please review this code change.\"\n",
    "        else:\n",
    "            return \"Code looks fine, but please add comments.\"\n",
    "\n",
    "# Demonstrate comment generation\n",
    "generator_elf = CodeReviewGenerator(experience_aware=True)\n",
    "generator_base = CodeReviewGenerator(experience_aware=False)\n",
    "\n",
    "test_code = \"if (user.role == 'admin') { processAdminRequest(request); }\"\n",
    "\n",
    "print(\"Code Change:\")\n",
    "print(test_code)\n",
    "print(\"\\nELF-based Review (High Experience):\")\n",
    "print(generator_elf.generate_comment(test_code, mock_metrics[\"exp_reviewer_1\"]))\n",
    "print(\"\\nELF-based Review (Low Experience):\")\n",
    "print(generator_elf.generate_comment(test_code, mock_metrics[\"new_reviewer_1\"]))\n",
    "print(\"\\nBaseline Review:\")\n",
    "print(generator_base.generate_comment(test_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics Implementation\n",
    "\n",
    "Implementation of evaluation metrics from the paper using deepeval where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class CodeReviewEvaluator:\n",
    "    \"\"\"Evaluate generated code review comments\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.comment_categories = {\n",
    "            \"functional\": [\"functional defect\", \"validation\", \"logical\", \"interface\", \"resource\", \"timing\"],\n",
    "            \"evolvability\": [\"solution approach\", \"documentation\", \"organization\", \"naming\", \"visual\"],\n",
    "            \"discussion\": [\"question\", \"design discussion\"]\n",
    "        }\n",
    "        \n",
    "    def calculate_bleu4(self, reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Simplified BLEU-4 calculation\"\"\"\n",
    "        from nltk.translate.bleu_score import sentence_bleu\n",
    "        reference_tokens = reference.lower().split()\n",
    "        hypothesis_tokens = hypothesis.lower().split()\n",
    "        return sentence_bleu([reference_tokens], hypothesis_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    def is_semantically_equivalent(self, reference: str, hypothesis: str) -> bool:\n",
    "        \"\"\"Check if comments have same intent (simplified)\"\"\"\n",
    "        # Extract key concepts\n",
    "        ref_concepts = set(re.findall(r'\\b(validation|resource|performance|refactor|leak|check)\\b', \n",
    "                                    reference.lower()))\n",
    "        hyp_concepts = set(re.findall(r'\\b(validation|resource|performance|refactor|leak|check)\\b', \n",
    "                                    hypothesis.lower()))\n",
    "        \n",
    "        # Check overlap\n",
    "        if len(ref_concepts) == 0:\n",
    "            return False\n",
    "        overlap = len(ref_concepts.intersection(hyp_concepts)) / len(ref_concepts)\n",
    "        return overlap > 0.5\n",
    "    \n",
    "    def is_applicable(self, code_change: str, comment: str) -> bool:\n",
    "        \"\"\"Check if comment is applicable to code change\"\"\"\n",
    "        # Simple heuristics for demonstration\n",
    "        code_keywords = set(re.findall(r'\\b\\w+\\b', code_change.lower()))\n",
    "        comment_keywords = set(re.findall(r'\\b\\w+\\b', comment.lower()))\n",
    "        \n",
    "        # Check if comment references code elements\n",
    "        overlap = len(code_keywords.intersection(comment_keywords))\n",
    "        return overlap > 2 or len(comment) > 20\n",
    "    \n",
    "    def classify_feedback_type(self, comment: str) -> str:\n",
    "        \"\"\"Classify comment as suggestion, concern, or confused question\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "        \n",
    "        # Patterns for suggestions\n",
    "        if any(pattern in comment_lower for pattern in \n",
    "               [\"should\", \"consider\", \"try\", \"use\", \"add:\", \"change to\"]):\n",
    "            return \"suggestion\"\n",
    "        \n",
    "        # Patterns for confused questions\n",
    "        if any(pattern in comment_lower for pattern in\n",
    "               [\"is this correct?\", \"what does\", \"i don't understand\", \"??\"]):\n",
    "            return \"confused_question\"\n",
    "        \n",
    "        # Default to concern\n",
    "        return \"concern\"\n",
    "    \n",
    "    def has_explanation(self, comment: str) -> bool:\n",
    "        \"\"\"Check if comment contains explanation/rationale\"\"\"\n",
    "        explanation_patterns = [\n",
    "            \"because\", \"since\", \"this prevents\", \"this ensures\",\n",
    "            \"to avoid\", \"for better\", \"which\", \"that\"\n",
    "        ]\n",
    "        return any(pattern in comment.lower() for pattern in explanation_patterns)\n",
    "    \n",
    "    def identify_issue_type(self, comment: str) -> str:\n",
    "        \"\"\"Identify the type of issue discussed in comment\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "        \n",
    "        # Check functional issues\n",
    "        if any(word in comment_lower for word in [\"validation\", \"validate\", \"check\"]):\n",
    "            return \"validation\"\n",
    "        if any(word in comment_lower for word in [\"leak\", \"resource\", \"close\", \"release\"]):\n",
    "            return \"resource\"\n",
    "        if any(word in comment_lower for word in [\"logic\", \"incorrect\", \"wrong\"]):\n",
    "            return \"logical\"\n",
    "            \n",
    "        # Check evolvability issues\n",
    "        if any(word in comment_lower for word in [\"refactor\", \"extract\", \"separate\"]):\n",
    "            return \"organization\"\n",
    "        if any(word in comment_lower for word in [\"comment\", \"document\", \"explain\"]):\n",
    "            return \"documentation\"\n",
    "        if any(word in comment_lower for word in [\"rename\", \"naming\", \"variable name\"]):\n",
    "            return \"naming\"\n",
    "            \n",
    "        return \"other\"\n",
    "\n",
    "# Evaluate sample comments\n",
    "evaluator = CodeReviewEvaluator()\n",
    "\n",
    "# Test comments\n",
    "test_comments = [\n",
    "    {\n",
    "        \"code\": \"if (user.role == 'admin') { processAdminRequest(request); }\",\n",
    "        \"reference\": \"Add validation check before processing\",\n",
    "        \"generated\": \"Missing validation check. Add: if (!validateRequest(request)) return; This prevents unauthorized access.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"connection = db.connect()\",\n",
    "        \"reference\": \"Close connection after use\",\n",
    "        \"generated\": \"Resource leak detected. Use try-finally to ensure connection.close().\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for i, test in enumerate(test_comments):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Code: {test['code']}\")\n",
    "    print(f\"Generated: {test['generated']}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    bleu = evaluator.calculate_bleu4(test['reference'], test['generated'])\n",
    "    sem_eq = evaluator.is_semantically_equivalent(test['reference'], test['generated'])\n",
    "    applicable = evaluator.is_applicable(test['code'], test['generated'])\n",
    "    feedback_type = evaluator.classify_feedback_type(test['generated'])\n",
    "    has_exp = evaluator.has_explanation(test['generated'])\n",
    "    issue_type = evaluator.identify_issue_type(test['generated'])\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  BLEU-4: {bleu:.3f}\")\n",
    "    print(f\"  Semantically Equivalent: {sem_eq}\")\n",
    "    print(f\"  Applicable: {applicable}\")\n",
    "    print(f\"  Feedback Type: {feedback_type}\")\n",
    "    print(f\"  Has Explanation: {has_exp}\")\n",
    "    print(f\"  Issue Type: {issue_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate evaluation results from the paper\n",
    "results_data = {\n",
    "    \"Model\": [\"CodeReviewer\", \"ELF_aco_pkg\", \"ELF_rso_pkg\", \"ELF_avg_pkg\", \"ELF_max_pkg\"],\n",
    "    \"BLEU-4\": [7.27, 7.46, 7.55, 7.45, 7.38],\n",
    "    \"Applicable\": [42, 53, 53, 46, 44],\n",
    "    \"Suggestions\": [27, 42, 37, 30, 30],\n",
    "    \"Functional_Issues\": [7, 13, 12, 16, 11],\n",
    "    \"Has_Explanation\": [8, 15, 11, 13, 15]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: BLEU-4 scores\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.bar(results_df[\"Model\"], results_df[\"BLEU-4\"], color=['gray'] + ['skyblue']*4)\n",
    "ax1.set_title(\"BLEU-4 Scores\", fontsize=14)\n",
    "ax1.set_ylabel(\"BLEU-4\")\n",
    "ax1.set_ylim(7.0, 7.6)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Applicable comments\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.bar(results_df[\"Model\"], results_df[\"Applicable\"], color=['gray'] + ['lightgreen']*4)\n",
    "ax2.set_title(\"Applicable Comments (out of 100)\", fontsize=14)\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Comment quality breakdown\n",
    "ax3 = axes[1, 0]\n",
    "quality_metrics = results_df[[\"Model\", \"Suggestions\", \"Functional_Issues\", \"Has_Explanation\"]]\n",
    "quality_metrics.set_index(\"Model\").plot(kind='bar', ax=ax3, width=0.8)\n",
    "ax3.set_title(\"Comment Quality Metrics\", fontsize=14)\n",
    "ax3.set_ylabel(\"Count\")\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Improvement percentages\n",
    "ax4 = axes[1, 1]\n",
    "baseline = results_df.iloc[0]\n",
    "improvements = []\n",
    "for i in range(1, len(results_df)):\n",
    "    model_data = results_df.iloc[i]\n",
    "    imp = {\n",
    "        \"Model\": model_data[\"Model\"].replace(\"ELF_\", \"\"),\n",
    "        \"Applicable\": ((model_data[\"Applicable\"] - baseline[\"Applicable\"]) / baseline[\"Applicable\"]) * 100,\n",
    "        \"Suggestions\": ((model_data[\"Suggestions\"] - baseline[\"Suggestions\"]) / baseline[\"Suggestions\"]) * 100,\n",
    "        \"Functional\": ((model_data[\"Functional_Issues\"] - baseline[\"Functional_Issues\"]) / baseline[\"Functional_Issues\"]) * 100\n",
    "    }\n",
    "    improvements.append(imp)\n",
    "\n",
    "imp_df = pd.DataFrame(improvements)\n",
    "imp_df.set_index(\"Model\").plot(kind='bar', ax=ax4, width=0.8)\n",
    "ax4.set_title(\"Percentage Improvement over CodeReviewer\", fontsize=14)\n",
    "ax4.set_ylabel(\"Improvement (%)\")\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax4.legend(loc='upper left')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"- Best BLEU-4: ELF_rso_pkg with {results_df.loc[2, 'BLEU-4']:.2f} (+{((results_df.loc[2, 'BLEU-4'] - results_df.loc[0, 'BLEU-4']) / results_df.loc[0, 'BLEU-4'] * 100):.1f}%)\")\n",
    "print(f\"- Most Applicable Comments: ELF_aco_pkg and ELF_rso_pkg with {results_df.loc[1, 'Applicable']} (+{((results_df.loc[1, 'Applicable'] - results_df.loc[0, 'Applicable']) / results_df.loc[0, 'Applicable'] * 100):.0f}%)\")\n",
    "print(f\"- Most Suggestions: ELF_aco_pkg with {results_df.loc[1, 'Suggestions']} (+{((results_df.loc[1, 'Suggestions'] - results_df.loc[0, 'Suggestions']) / results_df.loc[0, 'Suggestions'] * 100):.0f}%)\")\n",
    "print(f\"- Most Functional Issues: ELF_avg_pkg with {results_df.loc[3, 'Functional_Issues']} (+{((results_df.loc[3, 'Functional_Issues'] - results_df.loc[0, 'Functional_Issues']) / results_df.loc[0, 'Functional_Issues'] * 100):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Personal Research Template\n",
    "\n",
    "Use this section to experiment with different configurations and explore the method further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for experimenting with different ELF configurations\n",
    "\n",
    "class ResearchExperiment:\n",
    "    \"\"\"Template for conducting ELF experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.results = []\n",
    "        \n",
    "    def run_experiment(self, strategy: str, granularity: str, \n",
    "                      test_comments: List[CodeReviewComment],\n",
    "                      reviewer_metrics: Dict[str, ReviewerMetrics]):\n",
    "        \"\"\"Run an experiment with specific ELF configuration\"\"\"\n",
    "        \n",
    "        print(f\"\\nRunning experiment: {strategy}_{granularity}\")\n",
    "        \n",
    "        # Initialize ELF loss\n",
    "        elf_loss = ExperienceAwareLoss(strategy=strategy, granularity=granularity)\n",
    "        \n",
    "        # Calculate average weights for different experience levels\n",
    "        exp_levels = {\"high\": [], \"medium\": [], \"low\": []}\n",
    "        \n",
    "        for comment in test_comments:\n",
    "            metrics = reviewer_metrics[comment.reviewer_id]\n",
    "            weight = elf_loss.calculate_weight(metrics)\n",
    "            \n",
    "            # Classify experience level\n",
    "            avg_ownership = (metrics.aco_pkg + metrics.rso_pkg) / 2\n",
    "            if avg_ownership > 0.2:\n",
    "                exp_levels[\"high\"].append(weight)\n",
    "            elif avg_ownership > 0.1:\n",
    "                exp_levels[\"medium\"].append(weight)\n",
    "            else:\n",
    "                exp_levels[\"low\"].append(weight)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        result = {\n",
    "            \"strategy\": strategy,\n",
    "            \"granularity\": granularity,\n",
    "            \"avg_weight_high\": np.mean(exp_levels[\"high\"]) if exp_levels[\"high\"] else 0,\n",
    "            \"avg_weight_medium\": np.mean(exp_levels[\"medium\"]) if exp_levels[\"medium\"] else 0,\n",
    "            \"avg_weight_low\": np.mean(exp_levels[\"low\"]) if exp_levels[\"low\"] else 0,\n",
    "            \"weight_ratio_high_low\": np.mean(exp_levels[\"high\"]) / np.mean(exp_levels[\"low\"]) \n",
    "                                    if exp_levels[\"high\"] and exp_levels[\"low\"] else 0\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        \n",
    "        print(f\"  High exp avg weight: {result['avg_weight_high']:.3f}\")\n",
    "        print(f\"  Low exp avg weight: {result['avg_weight_low']:.3f}\")\n",
    "        print(f\"  Weight ratio: {result['weight_ratio_high_low']:.2f}x\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def compare_strategies(self):\n",
    "        \"\"\"Compare different strategies\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No experiments run yet!\")\n",
    "            return\n",
    "            \n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Weight distribution by strategy\n",
    "        strategies = results_df['strategy'].unique()\n",
    "        x = np.arange(len(strategies))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, gran in enumerate(['repository', 'subsystem', 'package']):\n",
    "            gran_data = results_df[results_df['granularity'] == gran]\n",
    "            ax1.bar(x + i*width, gran_data['weight_ratio_high_low'], \n",
    "                   width, label=gran)\n",
    "        \n",
    "        ax1.set_xlabel('Strategy')\n",
    "        ax1.set_ylabel('Weight Ratio (High/Low Experience)')\n",
    "        ax1.set_title('Experience Weight Ratios by Strategy and Granularity')\n",
    "        ax1.set_xticks(x + width)\n",
    "        ax1.set_xticklabels(strategies)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Heatmap of average weights\n",
    "        pivot_data = results_df.pivot(index='strategy', columns='granularity', \n",
    "                                     values='avg_weight_high')\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax2)\n",
    "        ax2.set_title('Average Weights for High Experience Reviewers')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run experiments\n",
    "experiment = ResearchExperiment(\"ELF_Analysis\")\n",
    "\n",
    "# Test all combinations\n",
    "for strategy in [\"aco\", \"rso\", \"avg\", \"max\"]:\n",
    "    for granularity in [\"repository\", \"subsystem\", \"package\"]:\n",
    "        experiment.run_experiment(strategy, granularity, mock_comments, mock_metrics)\n",
    "\n",
    "# Compare results\n",
    "experiment.compare_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Future Directions\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Experience Matters**: Leveraging reviewer experience through ELF significantly improves code review quality\n",
    "2. **Granularity is Important**: Package-level ownership metrics tend to perform best\n",
    "3. **Strategy Selection**: Both ACO and RSO strategies are effective, with complementary strengths\n",
    "4. **Quality Over Quantity**: ELF models generate more functional issue detections and helpful suggestions\n",
    "\n",
    "### Implementation Notes\n",
    "- This notebook uses LangChain for demonstration purposes\n",
    "- The actual paper uses T5-based CodeReviewer model fine-tuned on GitHub data\n",
    "- deepeval can be used for more sophisticated evaluation of generated comments\n",
    "\n",
    "### Future Research Directions\n",
    "1. **Dynamic Weight Adjustment**: Explore adaptive weighting based on comment quality feedback\n",
    "2. **Multi-modal Experience**: Incorporate other signals like code complexity and review history\n",
    "3. **Cross-project Transfer**: Study how experience metrics transfer across different projects\n",
    "4. **Real-time Adaptation**: Develop online learning approaches for continuous improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "import pickle\n",
    "\n",
    "experiment_data = {\n",
    "    \"mock_comments\": mock_comments,\n",
    "    \"mock_metrics\": mock_metrics,\n",
    "    \"experiment_results\": experiment.results\n",
    "}\n",
    "\n",
    "with open(\"elf_experiment_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(experiment_data, f)\n",
    "\n",
    "print(\"Experiment data saved!\")\n",
    "print(f\"Total experiments run: {len(experiment.results)}\")\n",
    "print(f\"Best configuration: {max(experiment.results, key=lambda x: x['weight_ratio_high_low'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}