{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) với Claude\n",
    "\n",
    "## 🎯 Mục tiêu\n",
    "\n",
    "Trong notebook này, chúng ta sẽ xây dựng một **RAG system hoàn chỉnh** từ đầu đến cuối:\n",
    "\n",
    "1. **📁 Document Loading**: Tải và xử lý documents\n",
    "2. **✂️ Text Splitting**: Chia documents thành chunks\n",
    "3. **🧮 Embeddings**: Chuyển đổi text thành vectors\n",
    "4. **🗄️ Vector Store**: Lưu trữ và index embeddings\n",
    "5. **🔍 Retrieval**: Tìm kiếm relevant documents\n",
    "6. **🤖 Generation**: Sử dụng Claude để generate answers\n",
    "7. **🔗 End-to-end Pipeline**: Kết hợp tất cả thành RAG system\n",
    "\n",
    "## 🏗️ RAG Architecture Overview\n",
    "\n",
    "```\n",
    "📄 Documents → Text Splitter → 🧮 Embeddings → 🗄️ Vector Store\n",
    "                                                      ↓\n",
    "❓ User Query → 🧮 Query Embedding → 🔍 Similarity Search\n",
    "                                                      ↓\n",
    "📚 Retrieved Context → 🤖 Claude (LLM) → ✨ Final Answer\n",
    "```\n",
    "\n",
    "## 💡 Why RAG?\n",
    "\n",
    "RAG giải quyết những limitations của LLMs:\n",
    "- **📚 Knowledge Cutoff**: LLMs trained với data cũ\n",
    "- **🔍 Domain Specificity**: Cần knowledge về specific domains\n",
    "- **📊 Factual Accuracy**: Giảm hallucination bằng grounded information\n",
    "- **🔄 Dynamic Updates**: Update knowledge mà không cần retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup và Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# Suppress warnings cho cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    DirectoryLoader\n",
    ")\n",
    "\n",
    "# Vector Stores và Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# LangChain Anthropic\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo models\n",
    "print(\"🚀 Initializing models...\")\n",
    "\n",
    "# Claude LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0,  # Deterministic for factual responses\n",
    "    anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "# HuggingFace Embeddings (consistent với previous notebooks)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"✅ Claude LLM initialized\")\n",
    "print(\"✅ HuggingFace embeddings initialized\")\n",
    "\n",
    "# Test both models\n",
    "test_llm = llm.invoke(\"Hello Claude!\")\n",
    "test_embedding = embeddings.embed_query(\"test sentence\")\n",
    "\n",
    "print(f\"🧪 LLM test: {test_llm.content[:50]}...\")\n",
    "print(f\"🧪 Embedding test: dimension={len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chuẩn bị Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo sample documents về LangChain và AI\n",
    "sample_documents = {\n",
    "    \"langchain_overview.txt\": \"\"\"\n",
    "LangChain Framework Overview\n",
    "\n",
    "LangChain là một framework mạnh mẽ được thiết kế để phát triển các ứng dụng sử dụng Large Language Models (LLMs). \n",
    "Framework này cung cấp một bộ công cụ toàn diện để xây dựng, triển khai và quản lý các ứng dụng AI phức tạp.\n",
    "\n",
    "Các thành phần chính của LangChain:\n",
    "\n",
    "1. LLMs và Prompts: LangChain hỗ trợ nhiều LLM providers như OpenAI, Anthropic, Cohere, và Hugging Face. \n",
    "Framework cung cấp prompt templates và prompt management để tối ưu hóa interactions với LLMs.\n",
    "\n",
    "2. Chains: Chains cho phép kết nối nhiều components lại với nhau để tạo ra workflows phức tạp. \n",
    "Ví dụ: LLMChain, SequentialChain, và custom chains cho specific use cases.\n",
    "\n",
    "3. Data Connection: LangChain cung cấp các document loaders, text splitters, retrievers, và vector stores \n",
    "để tích hợp với external data sources một cách hiệu quả.\n",
    "\n",
    "4. Memory: Memory components giúp maintain context giữa các interactions, bao gồm ConversationBufferMemory, \n",
    "ConversationSummaryMemory, và các loại memory khác.\n",
    "\n",
    "5. Agents: Agents sử dụng LLMs để quyết định actions và có thể access tools để thực hiện complex tasks.\n",
    "\n",
    "LangChain được thiết kế với tính modularity cao, cho phép developers mix và match các components \n",
    "để tạo ra solutions phù hợp với requirements cụ thể.\n",
    "\"\"\",\n",
    "    \n",
    "    \"rag_systems.txt\": \"\"\"\n",
    "Retrieval Augmented Generation (RAG) Systems\n",
    "\n",
    "RAG là một kỹ thuật quan trọng trong AI hiện đại, kết hợp khả năng truy xuất thông tin (retrieval) \n",
    "với khả năng sinh văn bản (generation) của LLMs để tạo ra responses chính xác và có căn cứ.\n",
    "\n",
    "Kiến trúc RAG System:\n",
    "\n",
    "1. Document Processing: Documents được load từ various sources (PDFs, websites, databases) \n",
    "và được preprocessing để extract clean text content.\n",
    "\n",
    "2. Text Chunking: Large documents được chia thành smaller chunks để optimize retrieval performance \n",
    "và fit trong context windows của LLMs. Chunk size và overlap phải được tune carefully.\n",
    "\n",
    "3. Embedding Generation: Text chunks được convert thành dense vector representations \n",
    "sử dụng embedding models như OpenAI embeddings hoặc sentence transformers.\n",
    "\n",
    "4. Vector Storage: Embeddings được store trong specialized vector databases như Pinecone, \n",
    "Chroma, FAISS, hoặc Weaviate để enable fast similarity search.\n",
    "\n",
    "5. Query Processing: User queries được embed sử dụng same embedding model \n",
    "để ensure consistency trong vector space.\n",
    "\n",
    "6. Retrieval: System thực hiện similarity search để find most relevant chunks \n",
    "based on cosine similarity hoặc other distance metrics.\n",
    "\n",
    "7. Context Construction: Retrieved chunks được combine thành coherent context \n",
    "với appropriate formatting và metadata.\n",
    "\n",
    "8. Response Generation: LLM generates final response based on retrieved context và original query, \n",
    "ensuring answers are grounded in actual information.\n",
    "\n",
    "RAG Benefits:\n",
    "- Improved factual accuracy\n",
    "- Access to up-to-date information\n",
    "- Domain-specific knowledge integration\n",
    "- Reduced hallucination\n",
    "- Transparency và traceability\n",
    "\"\"\",\n",
    "    \n",
    "    \"vector_databases.txt\": \"\"\"\n",
    "Vector Databases trong AI Applications\n",
    "\n",
    "Vector databases là specialized storage systems được thiết kế để efficiently store, index, \n",
    "và search high-dimensional vector data. Chúng đóng vai trò quan trọng trong modern AI applications.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "1. High-dimensional Storage: Vector databases optimize storage cho vectors với hundreds hoặc \n",
    "thousands of dimensions, thường generated từ machine learning models.\n",
    "\n",
    "2. Similarity Search: Core functionality là tìm vectors similar nhất với query vector \n",
    "sử dụng metrics như cosine similarity, euclidean distance, hoặc dot product.\n",
    "\n",
    "3. Approximate Nearest Neighbor (ANN): Để achieve fast search trên large datasets, \n",
    "vector databases sử dụng ANN algorithms như HNSW, IVF, hoặc LSH.\n",
    "\n",
    "4. Metadata Filtering: Ngoài vector similarity, systems support filtering based on metadata \n",
    "để narrow down search results.\n",
    "\n",
    "Popular Vector Databases:\n",
    "\n",
    "- Pinecone: Fully managed cloud service với excellent performance và scalability\n",
    "- Chroma: Open-source, lightweight option tốt cho development và prototyping\n",
    "- FAISS: Facebook's library, excellent performance nhưng requires more setup\n",
    "- Weaviate: Open-source với good GraphQL interface\n",
    "- Qdrant: Rust-based với high performance\n",
    "\n",
    "Use Cases:\n",
    "- Semantic search\n",
    "- Recommendation systems\n",
    "- RAG systems\n",
    "- Image và audio similarity search\n",
    "- Anomaly detection\n",
    "\n",
    "Vector databases enable applications to understand semantic meaning rather than \n",
    "just keyword matching, leading to more intelligent và contextual results.\n",
    "\"\"\",\n",
    "    \n",
    "    \"embedding_models.txt\": \"\"\"\n",
    "Embedding Models cho Text Representation\n",
    "\n",
    "Embedding models convert text thành dense vector representations mà capture semantic meaning. \n",
    "Quality của embeddings directly impacts performance của downstream tasks như search và RAG.\n",
    "\n",
    "Types of Embedding Models:\n",
    "\n",
    "1. Sentence Transformers: Based on BERT architecture, được fine-tuned specifically \n",
    "cho sentence-level embeddings. Popular models include all-MiniLM-L6-v2, all-mpnet-base-v2.\n",
    "\n",
    "2. OpenAI Embeddings: Commercial service providing high-quality embeddings \n",
    "như text-embedding-ada-002. Excellent performance nhưng requires API calls.\n",
    "\n",
    "3. Instructor Models: Specialized embeddings mà can be instructed cho specific tasks \n",
    "bằng cách provide task descriptions.\n",
    "\n",
    "4. Domain-specific Models: Models được fine-tuned cho specific domains \n",
    "như legal, medical, hoặc scientific text.\n",
    "\n",
    "Key Considerations:\n",
    "\n",
    "Model Size vs Performance: Larger models generally provide better embeddings \n",
    "nhưng slower inference times và higher memory requirements.\n",
    "\n",
    "Language Support: Some models work better với specific languages. \n",
    "Multilingual models can handle multiple languages nhưng might sacrifice some performance.\n",
    "\n",
    "Maximum Input Length: Models have limits on input text length, \n",
    "typically 512 tokens cho BERT-based models.\n",
    "\n",
    "Embedding Dimension: Higher dimensions can capture more information \n",
    "nhưng increase storage requirements và computation costs.\n",
    "\n",
    "Best Practices:\n",
    "- Use same embedding model cho both indexing và querying\n",
    "- Consider domain-specific models cho specialized content\n",
    "- Normalize embeddings cho consistent similarity calculations\n",
    "- Evaluate multiple models với your specific use case\n",
    "- Monitor embedding quality với human evaluation\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"📚 Created {len(sample_documents)} sample documents\")\n",
    "for filename in sample_documents.keys():\n",
    "    print(f\"   📄 {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo sample data directory và save documents\n",
    "data_dir = Path(\"rag_sample_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save documents to files\n",
    "for filename, content in sample_documents.items():\n",
    "    file_path = data_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content.strip())\n",
    "\n",
    "print(f\"✅ Documents saved to {data_dir}\")\n",
    "print(f\"📊 Total files: {len(list(data_dir.glob('*.txt')))}\")\n",
    "\n",
    "# Verify files\n",
    "for file_path in data_dir.glob('*.txt'):\n",
    "    size = file_path.stat().st_size\n",
    "    print(f\"   📄 {file_path.name}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading và Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents sử dụng DirectoryLoader\n",
    "print(\"📁 Loading documents...\")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    str(data_dir),\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "# Load all documents\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"✅ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Inspect documents\n",
    "for i, doc in enumerate(documents):\n",
    "    filename = Path(doc.metadata['source']).name\n",
    "    content_length = len(doc.page_content)\n",
    "    word_count = len(doc.page_content.split())\n",
    "    \n",
    "    print(f\"📄 Document {i+1}: {filename}\")\n",
    "    print(f\"   📏 Length: {content_length:,} characters, {word_count:,} words\")\n",
    "    print(f\"   📝 Preview: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document characteristics để determine optimal splitting strategy\n",
    "def analyze_documents(docs):\n",
    "    \"\"\"Analyze documents để recommend splitting parameters\"\"\"\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    total_words = sum(len(doc.page_content.split()) for doc in docs)\n",
    "    avg_chars_per_doc = total_chars / len(docs)\n",
    "    avg_words_per_doc = total_words / len(docs)\n",
    "    \n",
    "    # Analyze paragraph structure\n",
    "    paragraphs = []\n",
    "    for doc in docs:\n",
    "        doc_paragraphs = [p.strip() for p in doc.page_content.split('\\n\\n') if p.strip()]\n",
    "        paragraphs.extend(doc_paragraphs)\n",
    "    \n",
    "    avg_paragraph_length = sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0\n",
    "    \n",
    "    print(\"📊 Document Analysis:\")\n",
    "    print(f\"   📚 Total documents: {len(docs)}\")\n",
    "    print(f\"   📏 Total characters: {total_chars:,}\")\n",
    "    print(f\"   📝 Total words: {total_words:,}\")\n",
    "    print(f\"   📄 Average chars per document: {avg_chars_per_doc:.0f}\")\n",
    "    print(f\"   📖 Average words per document: {avg_words_per_doc:.0f}\")\n",
    "    print(f\"   📋 Total paragraphs: {len(paragraphs)}\")\n",
    "    print(f\"   📐 Average paragraph length: {avg_paragraph_length:.0f} characters\")\n",
    "    \n",
    "    # Recommend chunk size\n",
    "    if avg_paragraph_length > 800:\n",
    "        recommended_chunk_size = 1000\n",
    "        recommended_overlap = 100\n",
    "    elif avg_paragraph_length > 400:\n",
    "        recommended_chunk_size = 600\n",
    "        recommended_overlap = 60\n",
    "    else:\n",
    "        recommended_chunk_size = 400\n",
    "        recommended_overlap = 40\n",
    "    \n",
    "    print(f\"\\n💡 Recommendations:\")\n",
    "    print(f\"   🎯 Chunk size: {recommended_chunk_size}\")\n",
    "    print(f\"   🔄 Overlap: {recommended_overlap}\")\n",
    "    \n",
    "    return recommended_chunk_size, recommended_overlap\n",
    "\n",
    "chunk_size, overlap = analyze_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitting với optimized parameters\n",
    "print(f\"\\n✂️ Splitting documents với chunk_size={chunk_size}, overlap={overlap}\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Created {len(splits)} chunks from {len(documents)} documents\")\n",
    "\n",
    "# Analyze splits\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in splits]\n",
    "print(f\"\\n📊 Chunk Statistics:\")\n",
    "print(f\"   📏 Average chunk size: {sum(chunk_sizes)/len(chunk_sizes):.0f} characters\")\n",
    "print(f\"   📐 Min chunk size: {min(chunk_sizes)} characters\")\n",
    "print(f\"   📏 Max chunk size: {max(chunk_sizes)} characters\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\n📋 Sample Chunks:\")\n",
    "for i, chunk in enumerate(splits[:3]):\n",
    "    source_file = Path(chunk.metadata['source']).name\n",
    "    print(f\"\\nChunk {i+1} (from {source_file}):\")\n",
    "    print(f\"Length: {len(chunk.page_content)} chars\")\n",
    "    print(f\"Content: {chunk.page_content[:150]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings và Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings và vector store\n",
    "print(\"🧮 Creating embeddings và building vector store...\")\n",
    "print(f\"📊 Processing {len(splits)} chunks...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"✅ Vector store created in {embedding_time:.2f} seconds\")\n",
    "print(f\"📊 Vector store info:\")\n",
    "print(f\"   🔢 Total vectors: {vectorstore.index.ntotal}\")\n",
    "print(f\"   📐 Vector dimension: {vectorstore.index.d}\")\n",
    "print(f\"   ⚡ Average embedding time: {embedding_time/len(splits):.3f}s per chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector store với similarity search\n",
    "print(\"\\n🔍 Testing vector store với similarity search:\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"Types of vector databases\",\n",
    "    \"Embedding model selection\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n❓ Query: {query}\")\n",
    "    \n",
    "    # Similarity search\n",
    "    similar_docs = vectorstore.similarity_search(query, k=2)\n",
    "    \n",
    "    print(f\"📚 Found {len(similar_docs)} relevant chunks:\")\n",
    "    for i, doc in enumerate(similar_docs):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        content_preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"   {i+1}. {source}: {content_preview}...\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced search với scores\n",
    "print(\"\\n🎯 Advanced similarity search với scores:\")\n",
    "\n",
    "query = \"What are the benefits of RAG systems?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Search với similarity scores\n",
    "docs_with_scores = vectorstore.similarity_search_with_score(query, k=4)\n",
    "\n",
    "print(f\"\\n📊 Results with similarity scores:\")\n",
    "for i, (doc, score) in enumerate(docs_with_scores):\n",
    "    source = Path(doc.metadata['source']).name\n",
    "    content_preview = doc.page_content[:120].replace('\\n', ' ')\n",
    "    \n",
    "    print(f\"\\n{i+1}. Score: {score:.4f} | Source: {source}\")\n",
    "    print(f\"   Content: {content_preview}...\")\n",
    "\n",
    "print(f\"\\n💡 Lower scores = higher similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retriever Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"🔍 Retriever configured:\")\n",
    "print(f\"   Search type: similarity\")\n",
    "print(f\"   Retrieve top K: 4 chunks\")\n",
    "\n",
    "# Test retriever\n",
    "test_query = \"Explain vector databases and their use cases\"\n",
    "retrieved_docs = retriever.get_relevant_documents(test_query)\n",
    "\n",
    "print(f\"\\n🧪 Retriever test with query: '{test_query}'\")\n",
    "print(f\"📚 Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    source = Path(doc.metadata['source']).name\n",
    "    content_length = len(doc.page_content)\n",
    "    first_sentence = doc.page_content.split('.')[0] + '.'\n",
    "    \n",
    "    print(f\"\\n📄 Document {i+1}:\")\n",
    "    print(f\"   📁 Source: {source}\")\n",
    "    print(f\"   📏 Length: {content_length} characters\")\n",
    "    print(f\"   📝 First sentence: {first_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Chain with RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RetrievalQA chain\n",
    "print(\"🔗 Creating RetrievalQA chain...\")\n",
    "\n",
    "# Custom prompt template for RAG\n",
    "rag_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Always provide your answer in Vietnamese and be specific about what information you're basing your answer on.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in Vietnamese:\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" all retrieved docs into prompt\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True,  # Return source docs for transparency\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"✅ RetrievalQA chain created successfully\")\n",
    "print(f\"   🧠 LLM: Claude 3.5 Sonnet\")\n",
    "print(f\"   🔍 Retriever: FAISS similarity search (k=4)\")\n",
    "print(f\"   📝 Chain type: stuff\")\n",
    "print(f\"   📄 Return sources: Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RetrievalQA chain\n",
    "def test_qa_chain(question: str, chain):\n",
    "    \"\"\"Test QA chain và display results\"\"\"\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = chain({\"query\": question})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n🤖 Answer:\")\n",
    "    print(result['result'])\n",
    "    \n",
    "    print(f\"\\n📚 Source Documents ({len(result['source_documents'])}):");
    "    for i, doc in enumerate(result['source_documents']):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        content_preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"   {i+1}. {source}: {content_preview}...\")\n",
    "    \n",
    "    print(f\"\\n⏱️ Response time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"LangChain là gì và nó có những thành phần chính nào?\",\n",
    "    \"RAG hoạt động như thế nào?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = test_qa_chain(question, qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom RAG Chain với LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RAG chain sử dụng LCEL cho more control\n",
    "print(\"🔧 Building custom RAG chain với LCEL...\")\n",
    "\n",
    "# Custom prompt template với better formatting\n",
    "custom_rag_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Bạn là một AI assistant chuyên về LangChain và RAG systems. \n",
    "Sử dụng thông tin từ context dưới đây để trả lời câu hỏi một cách chính xác và chi tiết.\n",
    "Nếu thông tin không có trong context, hãy nói rõ là bạn không tìm thấy thông tin đó.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER (in Vietnamese):\"\"\"\n",
    ")\n",
    "\n",
    "# Function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into context string\"\"\"\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        content = doc.page_content.strip()\n",
    "        formatted_docs.append(f\"Document {i+1} (from {source}):\\n{content}\")\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Custom RAG chain using LCEL\n",
    "custom_rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ Custom LCEL RAG chain created\")\n",
    "print(\"   🔍 Retriever → Format Docs → Prompt → LLM → Parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom RAG chain\n",
    "def test_custom_rag(question: str, chain, retriever):\n",
    "    \"\"\"Test custom RAG chain với detailed output\"\"\"\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show retrieved documents first\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"\\n🔍 Retrieved {len(retrieved_docs)} relevant documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        print(f\"   {i+1}. {source} ({len(doc.page_content)} chars)\")\n",
    "    \n",
    "    # Generate answer\n",
    "    print(f\"\\n🤖 Generating answer...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    answer = chain.invoke(question)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n💬 Answer:\")\n",
    "    print(answer)\n",
    "    \n",
    "    print(f\"\\n⏱️ Generation time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test custom chain\n",
    "custom_questions = [\n",
    "    \"So sánh các loại vector databases khác nhau\",\n",
    "    \"Những yếu tố nào cần consider khi chọn embedding model?\",\n",
    "    \"Agents trong LangChain hoạt động như thế nào?\"\n",
    "]\n",
    "\n",
    "for question in custom_questions[:2]:  # Test first 2 questions\n",
    "    answer = test_custom_rag(question, custom_rag_chain, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Enhanced RAG với Multiple Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple retrievers với different strategies\n",
    "print(\"🔍 Creating multiple retrieval strategies...\")\n",
    "\n",
    "# 1. Similarity retriever (already created)\n",
    "similarity_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# 2. MMR (Maximum Marginal Relevance) retriever for diversity\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"fetch_k\": 6,  # Fetch more candidates for MMR\n",
    "        \"lambda_mult\": 0.7  # Balance relevance vs diversity\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Similarity with threshold\n",
    "threshold_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.8,  # Only return docs above threshold\n",
    "        \"k\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "retrievers = {\n",
    "    \"similarity\": similarity_retriever,\n",
    "    \"mmr\": mmr_retriever,\n",
    "    \"threshold\": threshold_retriever\n",
    "}\n",
    "\n",
    "print(\"✅ Created multiple retrieval strategies:\")\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"   🔍 {name}: {retriever.search_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval strategies\n",
    "def compare_retrievers(question: str, retrievers_dict):\n",
    "    \"\"\"Compare different retrieval strategies\"\"\"\n",
    "    print(f\"\\n🔍 Comparing retrievers for: '{question}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, retriever in retrievers_dict.items():\n",
    "        try:\n",
    "            docs = retriever.get_relevant_documents(question)\n",
    "            results[name] = docs\n",
    "            \n",
    "            print(f\"\\n📚 {name.upper()} Retriever ({len(docs)} docs):\")\n",
    "            for i, doc in enumerate(docs[:2]):  # Show first 2\n",
    "                source = Path(doc.metadata['source']).name\n",
    "                preview = doc.page_content[:80].replace('\\n', ' ')\n",
    "                print(f\"   {i+1}. {source}: {preview}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ {name.upper()} Retriever failed: {e}\")\n",
    "            results[name] = []\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test retriever comparison\n",
    "comparison_query = \"What are the main components of LangChain?\"\n",
    "retrieval_results = compare_retrievers(comparison_query, retrievers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RAG Evaluation và Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG evaluation framework\n",
    "def evaluate_rag_response(question: str, answer: str, source_docs: List[Document]):\n",
    "    \"\"\"Evaluate RAG response quality\"\"\"\n",
    "    evaluation = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"source_count\": len(source_docs),\n",
    "        \"answer_length\": len(answer),\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    # 1. Check if answer acknowledges sources\n",
    "    source_files = [Path(doc.metadata['source']).stem for doc in source_docs]\n",
    "    mentions_sources = any(file.lower() in answer.lower() for file in source_files)\n",
    "    evaluation[\"metrics\"][\"mentions_sources\"] = mentions_sources\n",
    "    \n",
    "    # 2. Check for uncertainty expressions\n",
    "    uncertainty_phrases = [\"không biết\", \"không rõ\", \"không có thông tin\", \"không chắc chắn\"]\n",
    "    expresses_uncertainty = any(phrase in answer.lower() for phrase in uncertainty_phrases)\n",
    "    evaluation[\"metrics\"][\"expresses_uncertainty_when_needed\"] = expresses_uncertainty\n",
    "    \n",
    "    # 3. Answer completeness (rough heuristic)\n",
    "    answer_completeness = \"complete\" if len(answer) > 200 else \"brief\" if len(answer) > 50 else \"very_brief\"\n",
    "    evaluation[\"metrics\"][\"answer_completeness\"] = answer_completeness\n",
    "    \n",
    "    # 4. Source diversity\n",
    "    unique_sources = len(set(Path(doc.metadata['source']).name for doc in source_docs))\n",
    "    source_diversity = unique_sources / len(source_docs) if source_docs else 0\n",
    "    evaluation[\"metrics\"][\"source_diversity\"] = source_diversity\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Evaluation test set\n",
    "eval_questions = [\n",
    "    \"LangChain có những thành phần chính nào?\",\n",
    "    \"Ưu điểm của RAG systems là gì?\",\n",
    "    \"Làm thế nào để choose embedding model?\",\n",
    "    \"TensorFlow có phải là một vector database không?\"  # Question not in docs\n",
    "]\n",
    "\n",
    "print(\"📊 Evaluating RAG system performance...\")\n",
    "evaluations = []\n",
    "\n",
    "for question in eval_questions:\n",
    "    print(f\"\\n🧪 Testing: {question}\")\n",
    "    \n",
    "    # Get answer using RetrievalQA\n",
    "    result = qa_chain({\"query\": question})\n",
    "    answer = result['result']\n",
    "    sources = result['source_documents']\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_result = evaluate_rag_response(question, answer, sources)\n",
    "    evaluations.append(eval_result)\n",
    "    \n",
    "    # Print evaluation\n",
    "    print(f\"   📚 Sources used: {eval_result['source_count']}\")\n",
    "    print(f\"   📏 Answer length: {eval_result['answer_length']} chars\")\n",
    "    print(f\"   🎯 Completeness: {eval_result['metrics']['answer_completeness']}\")\n",
    "    print(f\"   📖 Source diversity: {eval_result['metrics']['source_diversity']:.2f}\")\n",
    "    print(f\"   ❓ Expresses uncertainty: {eval_result['metrics']['expresses_uncertainty_when_needed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate evaluation results\n",
    "print(\"\\n📊 RAG System Evaluation Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_questions = len(evaluations)\n",
    "avg_sources = sum(e['source_count'] for e in evaluations) / total_questions\n",
    "avg_answer_length = sum(e['answer_length'] for e in evaluations) / total_questions\n",
    "avg_source_diversity = sum(e['metrics']['source_diversity'] for e in evaluations) / total_questions\n",
    "\n",
    "completeness_dist = {}\n",
    "for eval_result in evaluations:\n",
    "    completeness = eval_result['metrics']['answer_completeness']\n",
    "    completeness_dist[completeness] = completeness_dist.get(completeness, 0) + 1\n",
    "\n",
    "uncertainty_count = sum(1 for e in evaluations if e['metrics']['expresses_uncertainty_when_needed'])\n",
    "\n",
    "print(f\"📈 Performance Metrics:\")\n",
    "print(f\"   📚 Average sources per answer: {avg_sources:.1f}\")\n",
    "print(f\"   📏 Average answer length: {avg_answer_length:.0f} characters\")\n",
    "print(f\"   🎯 Source diversity: {avg_source_diversity:.2f}\")\n",
    "print(f\"   ❓ Questions với uncertainty: {uncertainty_count}/{total_questions}\")\n",
    "\n",
    "print(f\"\\n📊 Answer Completeness Distribution:\")\n",
    "for completeness, count in completeness_dist.items():\n",
    "    percentage = (count / total_questions) * 100\n",
    "    print(f\"   {completeness}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n💡 System Assessment:\")\nif avg_sources >= 3 and avg_source_diversity > 0.7:\n",
    "    print(f\"   ✅ Good source utilization\")\nelse:\n",
    "    print(f\"   ⚠️ Could improve source utilization\")\n\nif uncertainty_count > 0:\n",
    "    print(f\"   ✅ Appropriately expresses uncertainty\")\nelse:\n",
    "    print(f\"   ⚠️ May not handle unknown questions well\")\n\nif avg_answer_length > 150:\n",
    "    print(f\"   ✅ Provides detailed answers\")\nelse:\n",
    "    print(f\"   ⚠️ Answers may be too brief\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production-Ready RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production RAG system with optimizations\n",
    "class ProductionRAGSystem:\n",
    "    \"\"\"Production-ready RAG system với caching và error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, embeddings):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.cache = {}  # Simple in-memory cache\n",
    "        self.retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Production prompt\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Bạn là một AI assistant chuyên về LangChain, RAG systems, và AI technologies.\n",
    "            \n",
    "            INSTRUCTIONS:\n",
    "            1. Sử dụng ONLY thông tin từ context để trả lời\n",
    "            2. Nếu không tìm thấy thông tin trong context, nói rõ điều đó\n",
    "            3. Trả lời bằng tiếng Việt một cách chính xác và chi tiết\n",
    "            4. Cite sources khi có thể\n",
    "            \n",
    "            CONTEXT:\n",
    "            {context}\n",
    "            \n",
    "            QUESTION: {question}\n",
    "            \n",
    "            ANSWER:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Create chain\n",
    "        self.chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | self._format_docs,\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def _format_docs(self, docs):\n",
    "        \"\"\"Format retrieved documents\"\"\"\n",
    "        if not docs:\n",
    "            return \"No relevant documents found.\"\n",
    "        \n",
    "        formatted = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            source = Path(doc.metadata['source']).name\n",
    "            content = doc.page_content.strip()\n",
    "            formatted.append(f\"[Source {i+1}: {source}]\\n{content}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted)\n",
    "    \n",
    "    def query(self, question: str, use_cache: bool = True):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        # Check cache\n",
    "        if use_cache and question in self.cache:\n",
    "            print(\"🚀 Cache hit!\")\n",
    "            return self.cache[question]\n",
    "        \n",
    "        try:\n",
    "            # Validate input\n",
    "            if not question or len(question.strip()) == 0:\n",
    "                return {\n",
    "                    \"answer\": \"Please provide a valid question.\",\n",
    "                    \"sources\": [],\n",
    "                    \"error\": \"Empty question\"\n",
    "                }\n",
    "            \n",
    "            # Retrieve relevant documents\n",
    "            start_time = time.time()\n",
    "            relevant_docs = self.retriever.get_relevant_documents(question)\n",
    "            retrieval_time = time.time() - start_time\n",
    "            \n",
    "            # Generate answer\n",
    "            generation_start = time.time()\n",
    "            answer = self.chain.invoke(question)\n",
    "            generation_time = time.time() - generation_start\n",
    "            \n",
    "            result = {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": [\n",
    "                    {\n",
    "                        \"filename\": Path(doc.metadata['source']).name,\n",
    "                        \"content_preview\": doc.page_content[:200] + \"...\"\n",
    "                    }\n",
    "                    for doc in relevant_docs\n",
    "                ],\n",
    "                \"metrics\": {\n",
    "                    \"retrieval_time\": retrieval_time,\n",
    "                    \"generation_time\": generation_time,\n",
    "                    \"total_time\": retrieval_time + generation_time,\n",
    "                    \"sources_count\": len(relevant_docs)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Cache result\n",
    "            if use_cache:\n",
    "                self.cache[question] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": \"Xin lỗi, đã có lỗi xảy ra khi xử lý câu hỏi của bạn.\",\n",
    "                \"sources\": [],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def add_documents(self, new_docs: List[Document]):\n",
    "        \"\"\"Add new documents to the knowledge base\"\"\"\n",
    "        try:\n",
    "            # Clear cache when adding new documents\n",
    "            self.cache.clear()\n",
    "            \n",
    "            # Add to vectorstore\n",
    "            self.vectorstore.add_documents(new_docs)\n",
    "            \n",
    "            return f\"Successfully added {len(new_docs)} documents\"\n",
    "        except Exception as e:\n",
    "            return f\"Error adding documents: {str(e)}\"\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get system statistics\"\"\"\n",
    "        return {\n",
    "            \"total_vectors\": self.vectorstore.index.ntotal,\n",
    "            \"vector_dimension\": self.vectorstore.index.d,\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"model_info\": {\n",
    "                \"llm\": \"Claude 3.5 Sonnet\",\n",
    "                \"embeddings\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize production system\n",
    "rag_system = ProductionRAGSystem(vectorstore, llm, embeddings)\n",
    "\n",
    "print(\"🏭 Production RAG System initialized\")\n",
    "stats = rag_system.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production system\n",
    "def demo_production_system(system, questions):\n",
    "    \"\"\"Demo production RAG system\"\"\"\n",
    "    print(\"🚀 Production RAG System Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n📝 Question {i}: {question}\")\n",
    "        \n",
    "        result = system.query(question)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"❌ Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"\\n💬 Answer:\")\n",
    "            print(result['answer'])\n",
    "            \n",
    "            print(f\"\\n📊 Metrics:\")\n",
    "            metrics = result['metrics']\n",
    "            print(f\"   ⏱️ Total time: {metrics['total_time']:.2f}s\")\n",
    "            print(f\"   🔍 Retrieval: {metrics['retrieval_time']:.2f}s\")\n",
    "            print(f\"   🤖 Generation: {metrics['generation_time']:.2f}s\")\n",
    "            print(f\"   📚 Sources: {metrics['sources_count']}\")\n",
    "            \n",
    "            print(f\"\\n📁 Sources used:\")\n",
    "            for j, source in enumerate(result['sources']):\n",
    "                print(f\"   {j+1}. {source['filename']}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Demo questions\n",
    "demo_questions = [\n",
    "    \"LangChain agents khác với chains như thế nào?\",\n",
    "    \"Làm thế nào để optimize RAG system performance?\",\n",
    "    \"\"  # Test empty question\n",
    "]\n",
    "\n",
    "demo_production_system(rag_system, demo_questions[:2])  # Skip empty question for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup và Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore for future use\n",
    "vectorstore_path = \"./rag_vectorstore\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"💾 Vectorstore saved to {vectorstore_path}\")\n",
    "\n",
    "# Cleanup sample data\n",
    "import shutil\n",
    "try:\n",
    "    if data_dir.exists():\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"🧹 Cleaned up {data_dir}\")\nexcept Exception as e:\n",
    "    print(f\"Note: Could not cleanup {data_dir}: {e}\")\n",
    "\n",
    "print(\"\\n✅ RAG system setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "def rag_system_summary():\n",
    "    print(\"🎉 RAG with Claude - Complete Tutorial Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary = {\n",
    "        \"🏗️ Components Built\": [\n",
    "            \"Document loading và preprocessing system\",\n",
    "            \"Intelligent text splitting với analysis\",\n",
    "            \"FAISS vector store với HuggingFace embeddings\",\n",
    "            \"Multiple retrieval strategies (similarity, MMR, threshold)\",\n",
    "            \"RetrievalQA chain với Claude 3.5 Sonnet\",\n",
    "            \"Custom LCEL RAG pipeline\",\n",
    "            \"Production-ready system với caching\"\n",
    "        ],\n",
    "        \"📊 Key Features\": [\n",
    "            \"Source attribution và transparency\",\n",
    "            \"Error handling và graceful degradation\",\n",
    "            \"Performance monitoring và metrics\",\n",
    "            \"Caching for improved response times\",\n",
    "            \"Evaluation framework for quality assessment\",\n",
    "            \"Extensible architecture for new documents\"\n",
    "        ],\n",
    "        \"💡 Best Practices Implemented\": [\n",
    "            \"Chunk size optimization based on content analysis\",\n",
    "            \"Multiple retrieval strategies for robustness\",\n",
    "            \"Comprehensive prompt engineering\",\n",
    "            \"Production-ready error handling\",\n",
    "            \"Performance benchmarking và optimization\",\n",
    "            \"Quality evaluation framework\"\n",
    "        ],\n",
    "        \"🚀 Production Considerations\": [\n",
    "            \"Vectorstore persistence for deployment\",\n",
    "            \"Caching strategy for performance\",\n",
    "            \"Error handling for reliability\",\n",
    "            \"Metrics collection for monitoring\",\n",
    "            \"Extensible design for new content\",\n",
    "            \"Source tracking for accountability\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for section, items in summary.items():\n",
    "        print(f\"\\n{section}:\")\n",
    "        for item in items:\n",
    "            print(f\"   ✅ {item}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Next Steps:\")\n",
    "    next_steps = [\n",
    "        \"Deploy RAG system to production environment\",\n",
    "        \"Integrate với web interface or API\",\n",
    "        \"Add more sophisticated evaluation metrics\",\n",
    "        \"Implement user feedback collection\",\n",
    "        \"Scale với larger document collections\",\n",
    "        \"Add real-time document updates\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"   🚀 {step}\")\n",
    "\n",
    "rag_system_summary()\n",
    "\n",
    "print(f\"\\n🏆 You've successfully built a complete RAG system với Claude!\")\n",
    "print(f\"💪 Ready for production deployment và real-world applications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}