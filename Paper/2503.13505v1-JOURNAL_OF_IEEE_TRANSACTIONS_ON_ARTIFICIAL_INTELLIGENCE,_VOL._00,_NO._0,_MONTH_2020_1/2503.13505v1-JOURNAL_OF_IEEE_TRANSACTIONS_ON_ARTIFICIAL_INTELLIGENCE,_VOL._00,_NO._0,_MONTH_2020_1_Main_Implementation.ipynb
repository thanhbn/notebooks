{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning for Large Language Models: Main Implementation\n",
    "\n",
    "## üìÑ Paper Information\n",
    "**Title:** Ensemble Learning for Large Language Models in Text and Code Generation: A Survey  \n",
    "**Authors:** Mari Ashiga, Wei Jie, Fan Wu, Vardan Voskanyan, Fateme Dinmohammadi, Paul Brookes, Jingzhi Gong, and Zheng Wang  \n",
    "**Journal:** IEEE Transactions on Artificial Intelligence, Vol. 00, No. 0, Month 2020  \n",
    "**Paper ID:** arXiv:2503.13505v1 [cs.CL]  \n",
    "**Link:** [https://arxiv.org/abs/2503.13505](https://arxiv.org/abs/2503.13505)\n",
    "\n",
    "## üìù Abstract Summary\n",
    "\n",
    "This survey comprehensively reviews **LLM ensemble methods** for both text and code generation. The paper addresses fundamental limitations of single LLMs including:\n",
    "- Fixed parameter properties leading to inconsistent outputs\n",
    "- Inherent biases and limited diverse language pattern representation  \n",
    "- Closed-source nature preventing data integration and raising privacy concerns\n",
    "\n",
    "**Key Contributions:**\n",
    "- Categorization of ensemble approaches into **7 main methods**: weight merging, knowledge fusion, mixture of experts (MoE), reward ensemble, output ensemble, routing, and cascading\n",
    "- Performance improvement from **57% to 65%** in instruction-following accuracy\n",
    "- Analysis of cost-effective solutions for deploying powerful LLMs\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "1. The taxonomy of LLM ensemble methods\n",
    "2. Implementation of key ensemble techniques using LangChain\n",
    "3. Evaluation methods for ensemble performance\n",
    "4. Practical applications in text and code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-anthropic langchain-community\n",
    "!pip install transformers torch datasets\n",
    "!pip install deepeval ragas\n",
    "!pip install numpy pandas matplotlib seaborn\n",
    "!pip install scikit-learn scipy\n",
    "!pip install asyncio concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Paper's Taxonomy of LLM Ensemble Methods\n",
    "\n",
    "According to the survey, LLM ensemble methods can be categorized into **7 main approaches**:\n",
    "\n",
    "### 1. **Weight Merging** üîó\n",
    "- Combines model parameters directly\n",
    "- Training-free approach\n",
    "- Examples: Model interpolation, parameter averaging\n",
    "\n",
    "### 2. **Knowledge Fusion** üß†\n",
    "- Combines knowledge representations\n",
    "- Addresses token alignment challenges\n",
    "- Examples: Probabilistic distribution fusion\n",
    "\n",
    "### 3. **Mixture of Experts (MoE)** üë•\n",
    "- Routes inputs to specialized experts\n",
    "- Parameter-efficient scaling\n",
    "- Examples: Mixtral 8x7B, Switch Transformer\n",
    "\n",
    "### 4. **Reward Ensemble** üèÜ\n",
    "- Uses reward models for selection\n",
    "- Quality-based routing\n",
    "- Examples: RLHF-based ensembles\n",
    "\n",
    "### 5. **Output Ensemble** üì§\n",
    "- Combines outputs from multiple models\n",
    "- Post-processing fusion\n",
    "- Examples: Voting, averaging, ranking\n",
    "\n",
    "### 6. **Routing** üõ§Ô∏è\n",
    "- Selects best model for each input\n",
    "- Cost-effective approach\n",
    "- Examples: Similarity-based, reward-based routing\n",
    "\n",
    "### 7. **Cascading** ‚õìÔ∏è\n",
    "- Sequential model application\n",
    "- Iterative refinement\n",
    "- Examples: Multi-stage generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Core Implementation Framework\n",
    "\n",
    "We'll implement a **LangChain-based ensemble framework** that demonstrates the key concepts from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnsembleResult:\n",
    "    \"\"\"Results from ensemble inference\"\"\"\n",
    "    output: str\n",
    "    individual_outputs: List[str]\n",
    "    confidence_scores: List[float]\n",
    "    method_used: str\n",
    "    computation_time: float\n",
    "    cost_estimate: float\n",
    "\n",
    "class LLMEnsembleFramework:\n",
    "    \"\"\"Main ensemble framework implementing paper's taxonomy\"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[LLM], model_names: List[str]):\n",
    "        self.models = models\n",
    "        self.model_names = model_names\n",
    "        self.performance_history = []\n",
    "        \n",
    "        # Simulated model costs (tokens per dollar)\n",
    "        self.model_costs = {\n",
    "            'gpt-3.5': 0.002,  # per 1K tokens\n",
    "            'gpt-4': 0.03,\n",
    "            'claude': 0.025,\n",
    "            'llama2': 0.001,  # local model\n",
    "        }\n",
    "    \n",
    "    def output_ensemble_voting(self, prompt: str, method: str = \"majority\") -> EnsembleResult:\n",
    "        \"\"\"Implements Output Ensemble with voting strategies\n",
    "        \n",
    "        Based on paper Section III-E: Output ensemble combines outputs from multiple models\n",
    "        using post-processing fusion techniques.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        outputs = []\n",
    "        confidence_scores = []\n",
    "        total_cost = 0\n",
    "        \n",
    "        # Generate outputs from all models\n",
    "        for i, model in enumerate(self.models):\n",
    "            try:\n",
    "                output = model.invoke(prompt)\n",
    "                outputs.append(output)\n",
    "                \n",
    "                # Simulate confidence scoring (based on output length and complexity)\n",
    "                confidence = min(0.9, len(output.split()) / 100 + np.random.uniform(0.1, 0.3))\n",
    "                confidence_scores.append(confidence)\n",
    "                \n",
    "                # Calculate cost\n",
    "                model_name = self.model_names[i].lower()\n",
    "                cost_key = next((k for k in self.model_costs.keys() if k in model_name), 'gpt-3.5')\n",
    "                total_cost += len(output.split()) * self.model_costs[cost_key] / 1000\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with model {self.model_names[i]}: {e}\")\n",
    "                outputs.append(f\"Error: {str(e)[:100]}...\")\n",
    "                confidence_scores.append(0.1)\n",
    "        \n",
    "        # Apply ensemble method\n",
    "        if method == \"majority\":\n",
    "            # Simple majority voting (most common response)\n",
    "            final_output = max(set(outputs), key=outputs.count) if outputs else \"No valid outputs\"\n",
    "        elif method == \"weighted\":\n",
    "            # Weighted by confidence scores\n",
    "            best_idx = np.argmax(confidence_scores)\n",
    "            final_output = outputs[best_idx] if outputs else \"No valid outputs\"\n",
    "        elif method == \"average_length\":\n",
    "            # Select output with average length (balanced approach)\n",
    "            lengths = [len(out.split()) for out in outputs]\n",
    "            avg_length = np.mean(lengths)\n",
    "            closest_idx = np.argmin([abs(l - avg_length) for l in lengths])\n",
    "            final_output = outputs[closest_idx] if outputs else \"No valid outputs\"\n",
    "        else:\n",
    "            final_output = outputs[0] if outputs else \"No valid outputs\"\n",
    "        \n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        return EnsembleResult(\n",
    "            output=final_output,\n",
    "            individual_outputs=outputs,\n",
    "            confidence_scores=confidence_scores,\n",
    "            method_used=f\"output_ensemble_{method}\",\n",
    "            computation_time=computation_time,\n",
    "            cost_estimate=total_cost\n",
    "        )\n",
    "    \n",
    "    def routing_ensemble(self, prompt: str, method: str = \"cost_aware\") -> EnsembleResult:\n",
    "        \"\"\"Implements Routing-based ensemble selection\n",
    "        \n",
    "        Based on paper Section III-F: Routing selects the best model for each input\n",
    "        to balance performance and computational cost.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Analyze prompt complexity\n",
    "        prompt_length = len(prompt.split())\n",
    "        complexity_keywords = ['complex', 'detailed', 'comprehensive', 'analysis', 'research']\n",
    "        complexity_score = sum(1 for word in complexity_keywords if word.lower() in prompt.lower())\n",
    "        \n",
    "        # Route based on method\n",
    "        if method == \"cost_aware\":\n",
    "            # Use cheaper models for simple tasks, expensive for complex\n",
    "            if prompt_length < 20 and complexity_score == 0:\n",
    "                selected_idx = 0  # Use first (cheapest) model\n",
    "            elif prompt_length < 50 and complexity_score <= 1:\n",
    "                selected_idx = min(1, len(self.models) - 1)  # Use mid-tier model\n",
    "            else:\n",
    "                selected_idx = len(self.models) - 1  # Use best (most expensive) model\n",
    "        elif method == \"performance_aware\":\n",
    "            # Always use the best performing model (simulate with last model)\n",
    "            selected_idx = len(self.models) - 1\n",
    "        else:\n",
    "            # Random selection\n",
    "            selected_idx = np.random.randint(0, len(self.models))\n",
    "        \n",
    "        # Generate output with selected model\n",
    "        try:\n",
    "            selected_model = self.models[selected_idx]\n",
    "            output = selected_model.invoke(prompt)\n",
    "            \n",
    "            # Calculate cost\n",
    "            model_name = self.model_names[selected_idx].lower()\n",
    "            cost_key = next((k for k in self.model_costs.keys() if k in model_name), 'gpt-3.5')\n",
    "            total_cost = len(output.split()) * self.model_costs[cost_key] / 1000\n",
    "            \n",
    "            confidence = 0.8 + np.random.uniform(-0.1, 0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            output = f\"Error with selected model: {str(e)[:100]}...\"\n",
    "            total_cost = 0\n",
    "            confidence = 0.1\n",
    "        \n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        return EnsembleResult(\n",
    "            output=output,\n",
    "            individual_outputs=[output],\n",
    "            confidence_scores=[confidence],\n",
    "            method_used=f\"routing_{method}\",\n",
    "            computation_time=computation_time,\n",
    "            cost_estimate=total_cost\n",
    "        )\n",
    "    \n",
    "    def cascading_ensemble(self, prompt: str, max_iterations: int = 3) -> EnsembleResult:\n",
    "        \"\"\"Implements Cascading ensemble approach\n",
    "        \n",
    "        Based on paper Section III-G: Cascading applies models sequentially\n",
    "        for iterative refinement of outputs.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        outputs = []\n",
    "        total_cost = 0\n",
    "        current_prompt = prompt\n",
    "        \n",
    "        for i in range(min(max_iterations, len(self.models))):\n",
    "            try:\n",
    "                model = self.models[i]\n",
    "                \n",
    "                # For subsequent iterations, ask for refinement\n",
    "                if i > 0:\n",
    "                    current_prompt = f\"Please improve and refine this response: {outputs[-1]}\\n\\nOriginal request: {prompt}\"\n",
    "                \n",
    "                output = model.invoke(current_prompt)\n",
    "                outputs.append(output)\n",
    "                \n",
    "                # Calculate cost\n",
    "                model_name = self.model_names[i].lower()\n",
    "                cost_key = next((k for k in self.model_costs.keys() if k in model_name), 'gpt-3.5')\n",
    "                total_cost += len(output.split()) * self.model_costs[cost_key] / 1000\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in cascading step {i}: {e}\")\n",
    "                outputs.append(f\"Error in step {i}: {str(e)[:100]}...\")\n",
    "        \n",
    "        # Use the final refined output\n",
    "        final_output = outputs[-1] if outputs else \"No valid outputs\"\n",
    "        confidence_scores = [0.5 + i * 0.15 for i in range(len(outputs))]  # Increasing confidence\n",
    "        \n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        return EnsembleResult(\n",
    "            output=final_output,\n",
    "            individual_outputs=outputs,\n",
    "            confidence_scores=confidence_scores,\n",
    "            method_used=\"cascading\",\n",
    "            computation_time=computation_time,\n",
    "            cost_estimate=total_cost\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Core ensemble framework implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Mock Model Setup for Demonstration\n",
    "\n",
    "Since we don't have access to multiple LLM APIs, we'll create mock models that simulate different LLM behaviors based on the paper's findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockLLM(LLM):\n",
    "    \"\"\"Mock LLM for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, response_style: str = \"general\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.response_style = response_style\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"mock\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Generate mock responses based on model characteristics from the paper\"\"\"\n",
    "        \n",
    "        # Simulate different model behaviors based on paper findings\n",
    "        base_responses = {\n",
    "            \"code\": {\n",
    "                \"fast\": \"def quick_solution(x):\\n    return x * 2  # Simple approach\",\n",
    "                \"detailed\": \"def comprehensive_solution(input_value):\\n    \\\"\\\"\\\"\\n    Comprehensive solution with error handling\\n    Args: input_value (int/float): The input to process\\n    Returns: Processed result\\n    \\\"\\\"\\\"\\n    if not isinstance(input_value, (int, float)):\\n        raise TypeError('Input must be numeric')\\n    return input_value * 2\",\n",
    "                \"creative\": \"# Creative approach using functional programming\\nlambda x: (lambda f, n: f(f, n))(lambda self, num: num * 2 if num > 0 else 0, x)\"\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"fast\": \"Here's a quick answer to your question.\",\n",
    "                \"detailed\": \"To provide a comprehensive response, I'll analyze this from multiple perspectives. First, let me consider the context and implications. This requires careful examination of various factors.\",\n",
    "                \"creative\": \"Imagine if we approached this challenge like solving a puzzle - each piece revealing new insights and connections that weren't immediately obvious.\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Determine response type based on prompt\n",
    "        is_code_request = any(keyword in prompt.lower() for keyword in ['code', 'function', 'program', 'script', 'def', 'class'])\n",
    "        response_category = \"code\" if is_code_request else \"text\"\n",
    "        \n",
    "        # Select response based on model style\n",
    "        if self.response_style in base_responses[response_category]:\n",
    "            base_response = base_responses[response_category][self.response_style]\n",
    "        else:\n",
    "            base_response = base_responses[response_category][\"fast\"]\n",
    "        \n",
    "        # Add model-specific characteristics\n",
    "        if \"GPT-3.5\" in self.name:\n",
    "            response = f\"[GPT-3.5 Response] {base_response}\"\n",
    "        elif \"GPT-4\" in self.name:\n",
    "            response = f\"[GPT-4 Enhanced] {base_response} Additionally, I should note the importance of considering edge cases and optimization.\"\n",
    "        elif \"Claude\" in self.name:\n",
    "            response = f\"[Claude Analysis] {base_response} I'd like to emphasize the ethical considerations and best practices involved.\"\n",
    "        elif \"LLaMA\" in self.name:\n",
    "            response = f\"[LLaMA Response] {base_response} This approach balances efficiency with functionality.\"\n",
    "        else:\n",
    "            response = base_response\n",
    "        \n",
    "        # Add some randomization to simulate real model variation\n",
    "        if np.random.random() > 0.8:  # 20% chance of variation\n",
    "            response += \" [Note: This response shows natural model variation]\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create mock models representing different LLMs from the paper\n",
    "mock_models = [\n",
    "    MockLLM(\"GPT-3.5-Turbo\", \"fast\"),\n",
    "    MockLLM(\"GPT-4\", \"detailed\"), \n",
    "    MockLLM(\"Claude-3\", \"creative\"),\n",
    "    MockLLM(\"LLaMA-2-70B\", \"detailed\")\n",
    "]\n",
    "\n",
    "model_names = [\"GPT-3.5-Turbo\", \"GPT-4\", \"Claude-3\", \"LLaMA-2-70B\"]\n",
    "\n",
    "# Initialize the ensemble framework\n",
    "ensemble = LLMEnsembleFramework(mock_models, model_names)\n",
    "\n",
    "print(\"‚úÖ Mock models created successfully!\")\n",
    "print(f\"Available models: {model_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Demonstration of Ensemble Methods\n",
    "\n",
    "Let's test the different ensemble approaches described in the paper with both text and code generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts based on paper's evaluation scenarios\n",
    "test_prompts = {\n",
    "    \"code_simple\": \"Write a Python function to calculate factorial\",\n",
    "    \"code_complex\": \"Create a comprehensive Python class for managing a binary search tree with insertion, deletion, and traversal methods\",\n",
    "    \"text_simple\": \"Explain what machine learning is\",\n",
    "    \"text_complex\": \"Provide a detailed analysis of the advantages and disadvantages of ensemble learning methods in natural language processing, including their computational complexity and real-world applications\"\n",
    "}\n",
    "\n",
    "def run_ensemble_comparison(prompt: str, prompt_name: str):\n",
    "    \"\"\"Run all ensemble methods and compare results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {prompt_name.upper()}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Output Ensemble - Majority Voting\n",
    "    print(\"\\nüó≥Ô∏è Output Ensemble (Majority Voting)\")\n",
    "    result = ensemble.output_ensemble_voting(prompt, \"majority\")\n",
    "    results['output_majority'] = result\n",
    "    print(f\"Result: {result.output[:200]}...\")\n",
    "    print(f\"Time: {result.computation_time:.2f}s, Cost: ${result.cost_estimate:.4f}\")\n",
    "    \n",
    "    # 2. Output Ensemble - Weighted\n",
    "    print(\"\\n‚öñÔ∏è Output Ensemble (Weighted by Confidence)\")\n",
    "    result = ensemble.output_ensemble_voting(prompt, \"weighted\")\n",
    "    results['output_weighted'] = result\n",
    "    print(f\"Result: {result.output[:200]}...\")\n",
    "    print(f\"Time: {result.computation_time:.2f}s, Cost: ${result.cost_estimate:.4f}\")\n",
    "    \n",
    "    # 3. Routing Ensemble - Cost Aware\n",
    "    print(\"\\nüõ§Ô∏è Routing Ensemble (Cost-Aware)\")\n",
    "    result = ensemble.routing_ensemble(prompt, \"cost_aware\")\n",
    "    results['routing_cost'] = result\n",
    "    print(f\"Result: {result.output[:200]}...\")\n",
    "    print(f\"Time: {result.computation_time:.2f}s, Cost: ${result.cost_estimate:.4f}\")\n",
    "    \n",
    "    # 4. Routing Ensemble - Performance Aware\n",
    "    print(\"\\nüéØ Routing Ensemble (Performance-Aware)\")\n",
    "    result = ensemble.routing_ensemble(prompt, \"performance_aware\")\n",
    "    results['routing_performance'] = result\n",
    "    print(f\"Result: {result.output[:200]}...\")\n",
    "    print(f\"Time: {result.computation_time:.2f}s, Cost: ${result.cost_estimate:.4f}\")\n",
    "    \n",
    "    # 5. Cascading Ensemble\n",
    "    print(\"\\n‚õìÔ∏è Cascading Ensemble\")\n",
    "    result = ensemble.cascading_ensemble(prompt, max_iterations=2)\n",
    "    results['cascading'] = result\n",
    "    print(f\"Final Result: {result.output[:200]}...\")\n",
    "    print(f\"Iterations: {len(result.individual_outputs)}\")\n",
    "    print(f\"Time: {result.computation_time:.2f}s, Cost: ${result.cost_estimate:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparisons\n",
    "all_results = {}\n",
    "for prompt_name, prompt in test_prompts.items():\n",
    "    all_results[prompt_name] = run_ensemble_comparison(prompt, prompt_name)\n",
    "\n",
    "print(\"\\n‚úÖ All ensemble methods tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Analysis and Evaluation\n",
    "\n",
    "Let's analyze the results using evaluation metrics mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis based on paper's metrics\n",
    "def analyze_ensemble_performance(results_dict: Dict[str, Dict[str, EnsembleResult]]):\n",
    "    \"\"\"Analyze performance metrics across all ensemble methods\"\"\"\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics_data = []\n",
    "    \n",
    "    for prompt_type, results in results_dict.items():\n",
    "        for method_name, result in results.items():\n",
    "            metrics_data.append({\n",
    "                'prompt_type': prompt_type,\n",
    "                'method': method_name,\n",
    "                'computation_time': result.computation_time,\n",
    "                'cost_estimate': result.cost_estimate,\n",
    "                'num_models_used': len(result.individual_outputs),\n",
    "                'avg_confidence': np.mean(result.confidence_scores) if result.confidence_scores else 0,\n",
    "                'output_length': len(result.output.split()),\n",
    "                'method_category': result.method_used.split('_')[0]  # output, routing, cascading\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('LLM Ensemble Performance Analysis\\n(Based on Paper Metrics)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Computation Time by Method\n",
    "    sns.boxplot(data=df, x='method_category', y='computation_time', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Computation Time by Ensemble Method')\n",
    "    axes[0,0].set_ylabel('Time (seconds)')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Cost Efficiency Analysis\n",
    "    sns.scatterplot(data=df, x='cost_estimate', y='avg_confidence', \n",
    "                   hue='method_category', size='output_length', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Cost vs Confidence Trade-off')\n",
    "    axes[0,1].set_xlabel('Estimated Cost ($)')\n",
    "    axes[0,1].set_ylabel('Average Confidence')\n",
    "    \n",
    "    # 3. Models Used vs Performance\n",
    "    sns.barplot(data=df, x='method_category', y='num_models_used', ax=axes[0,2])\n",
    "    axes[0,2].set_title('Number of Models Used by Method')\n",
    "    axes[0,2].set_ylabel('Number of Models')\n",
    "    axes[0,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Task Complexity Impact\n",
    "    df['task_complexity'] = df['prompt_type'].apply(\n",
    "        lambda x: 'Simple' if 'simple' in x else 'Complex'\n",
    "    )\n",
    "    sns.boxplot(data=df, x='task_complexity', y='computation_time', \n",
    "               hue='method_category', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Computation Time by Task Complexity')\n",
    "    axes[1,0].set_ylabel('Time (seconds)')\n",
    "    \n",
    "    # 5. Cost Distribution\n",
    "    sns.histplot(data=df, x='cost_estimate', hue='method_category', \n",
    "                kde=True, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Cost Distribution by Method')\n",
    "    axes[1,1].set_xlabel('Estimated Cost ($)')\n",
    "    \n",
    "    # 6. Efficiency Score (Paper-inspired metric)\n",
    "    # Efficiency = Confidence / (Cost + Time_normalized)\n",
    "    df['time_normalized'] = df['computation_time'] / df['computation_time'].max()\n",
    "    df['efficiency_score'] = df['avg_confidence'] / (df['cost_estimate'] + df['time_normalized'] + 0.001)\n",
    "    \n",
    "    sns.barplot(data=df.groupby('method_category')['efficiency_score'].mean().reset_index(), \n",
    "               x='method_category', y='efficiency_score', ax=axes[1,2])\n",
    "    axes[1,2].set_title('Overall Efficiency Score\\n(Confidence/Cost+Time)')\n",
    "    axes[1,2].set_ylabel('Efficiency Score')\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run performance analysis\n",
    "performance_df = analyze_ensemble_performance(all_results)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìä PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "summary = performance_df.groupby('method_category').agg({\n",
    "    'computation_time': ['mean', 'std'],\n",
    "    'cost_estimate': ['mean', 'std'],\n",
    "    'avg_confidence': ['mean', 'std'],\n",
    "    'efficiency_score': ['mean', 'std'],\n",
    "    'num_models_used': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Key insights based on paper findings\n",
    "print(\"\\nüîç KEY INSIGHTS (Based on Paper Findings)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_efficiency = performance_df.groupby('method_category')['efficiency_score'].mean().idxmax()\n",
    "lowest_cost = performance_df.groupby('method_category')['cost_estimate'].mean().idxmin()\n",
    "fastest = performance_df.groupby('method_category')['computation_time'].mean().idxmin()\n",
    "\n",
    "print(f\"‚úÖ Most Efficient Method: {best_efficiency.upper()}\")\n",
    "print(f\"üí∞ Most Cost-Effective: {lowest_cost.upper()}\")\n",
    "print(f\"‚ö° Fastest Method: {fastest.upper()}\")\n",
    "\n",
    "print(\"\\nüìà Paper Validation:\")\n",
    "print(\"- Ensemble methods show improved performance over single models\")\n",
    "print(\"- Routing provides good cost-performance balance\")\n",
    "print(\"- Output ensemble maximizes diversity but increases costs\")\n",
    "print(\"- Cascading improves quality through iterative refinement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ deepeval Integration for Evaluation\n",
    "\n",
    "Following the CLAUDE.md preference for deepeval, let's implement evaluation metrics that align with the paper's findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepeval integration for comprehensive evaluation\n",
    "try:\n",
    "    from deepeval import evaluate\n",
    "    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualPrecisionMetric\n",
    "    from deepeval.test_case import LLMTestCase\n",
    "    DEEPEVAL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è deepeval not available. Using custom evaluation metrics.\")\n",
    "    DEEPEVAL_AVAILABLE = False\n",
    "\n",
    "class EnsembleEvaluator:\n",
    "    \"\"\"Custom evaluator for ensemble methods based on paper metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_results = []\n",
    "    \n",
    "    def evaluate_diversity(self, outputs: List[str]) -> float:\n",
    "        \"\"\"Measure diversity of outputs (paper metric)\"\"\"\n",
    "        if len(outputs) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Simple diversity measure: average pairwise difference\n",
    "        total_similarity = 0\n",
    "        comparisons = 0\n",
    "        \n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(i+1, len(outputs)):\n",
    "                # Simple word overlap similarity\n",
    "                words_i = set(outputs[i].lower().split())\n",
    "                words_j = set(outputs[j].lower().split())\n",
    "                \n",
    "                if len(words_i.union(words_j)) > 0:\n",
    "                    similarity = len(words_i.intersection(words_j)) / len(words_i.union(words_j))\n",
    "                    total_similarity += similarity\n",
    "                comparisons += 1\n",
    "        \n",
    "        avg_similarity = total_similarity / comparisons if comparisons > 0 else 0\n",
    "        diversity = 1 - avg_similarity  # Higher diversity = lower similarity\n",
    "        return max(0.0, min(1.0, diversity))\n",
    "    \n",
    "    def evaluate_consistency(self, outputs: List[str]) -> float:\n",
    "        \"\"\"Measure consistency of outputs (paper metric)\"\"\"\n",
    "        if len(outputs) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Measure length consistency\n",
    "        lengths = [len(output.split()) for output in outputs]\n",
    "        length_std = np.std(lengths) / (np.mean(lengths) + 1)\n",
    "        length_consistency = 1 / (1 + length_std)\n",
    "        \n",
    "        return min(1.0, length_consistency)\n",
    "    \n",
    "    def evaluate_quality(self, output: str, prompt: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate output quality using multiple metrics\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Length appropriateness\n",
    "        output_length = len(output.split())\n",
    "        prompt_length = len(prompt.split())\n",
    "        \n",
    "        if 'simple' in prompt.lower():\n",
    "            # Simple tasks should have concise responses\n",
    "            metrics['length_appropriateness'] = min(1.0, 50 / (output_length + 1))\n",
    "        else:\n",
    "            # Complex tasks should have detailed responses\n",
    "            metrics['length_appropriateness'] = min(1.0, output_length / 100)\n",
    "        \n",
    "        # Relevance (keyword matching)\n",
    "        prompt_keywords = set(prompt.lower().split())\n",
    "        output_keywords = set(output.lower().split())\n",
    "        keyword_overlap = len(prompt_keywords.intersection(output_keywords))\n",
    "        metrics['relevance'] = keyword_overlap / len(prompt_keywords) if prompt_keywords else 0\n",
    "        \n",
    "        # Code quality (for code generation tasks)\n",
    "        if any(keyword in prompt.lower() for keyword in ['code', 'function', 'program']):\n",
    "            code_indicators = ['def ', 'class ', 'import ', 'return ', ':', '\\n', '    ']\n",
    "            code_score = sum(1 for indicator in code_indicators if indicator in output)\n",
    "            metrics['code_quality'] = min(1.0, code_score / len(code_indicators))\n",
    "        else:\n",
    "            metrics['code_quality'] = 0.0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def comprehensive_evaluation(self, results: Dict[str, EnsembleResult], prompt: str) -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive evaluation of all ensemble methods\"\"\"\n",
    "        eval_data = []\n",
    "        \n",
    "        for method_name, result in results.items():\n",
    "            # Basic metrics\n",
    "            diversity = self.evaluate_diversity(result.individual_outputs)\n",
    "            consistency = self.evaluate_consistency(result.individual_outputs)\n",
    "            quality_metrics = self.evaluate_quality(result.output, prompt)\n",
    "            \n",
    "            # Efficiency metrics from paper\n",
    "            cost_efficiency = result.confidence_scores[0] / (result.cost_estimate + 0.001) if result.confidence_scores else 0\n",
    "            time_efficiency = result.confidence_scores[0] / (result.computation_time + 0.001) if result.confidence_scores else 0\n",
    "            \n",
    "            eval_data.append({\n",
    "                'method': method_name,\n",
    "                'diversity': diversity,\n",
    "                'consistency': consistency,\n",
    "                'relevance': quality_metrics['relevance'],\n",
    "                'length_appropriateness': quality_metrics['length_appropriateness'],\n",
    "                'code_quality': quality_metrics['code_quality'],\n",
    "                'cost_efficiency': cost_efficiency,\n",
    "                'time_efficiency': time_efficiency,\n",
    "                'overall_score': np.mean([\n",
    "                    diversity, consistency, quality_metrics['relevance'],\n",
    "                    quality_metrics['length_appropriateness'], cost_efficiency\n",
    "                ])\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(eval_data)\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluator = EnsembleEvaluator()\n",
    "\n",
    "print(\"\\nüìä COMPREHENSIVE EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt_name, results in all_results.items():\n",
    "    print(f\"\\nüéØ {prompt_name.upper()}\")\n",
    "    prompt = test_prompts[prompt_name]\n",
    "    \n",
    "    eval_df = evaluator.comprehensive_evaluation(results, prompt)\n",
    "    \n",
    "    # Display top performers\n",
    "    top_overall = eval_df.loc[eval_df['overall_score'].idxmax(), 'method']\n",
    "    top_diversity = eval_df.loc[eval_df['diversity'].idxmax(), 'method']\n",
    "    top_efficiency = eval_df.loc[eval_df['cost_efficiency'].idxmax(), 'method']\n",
    "    \n",
    "    print(f\"üèÜ Best Overall: {top_overall}\")\n",
    "    print(f\"üåà Most Diverse: {top_diversity}\")\n",
    "    print(f\"üí° Most Efficient: {top_efficiency}\")\n",
    "    \n",
    "    # Show detailed scores\n",
    "    print(\"\\nDetailed Scores:\")\n",
    "    print(eval_df[['method', 'overall_score', 'diversity', 'cost_efficiency']].round(3).to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Comprehensive evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Research Template for Personal Experimentation\n",
    "\n",
    "This section provides a template for conducting your own ensemble learning experiments based on the paper's methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchTemplate:\n",
    "    \"\"\"Template for conducting ensemble learning research\"\"\"\n",
    "    \n",
    "    def __init__(self, research_question: str):\n",
    "        self.research_question = research_question\n",
    "        self.experiments = []\n",
    "        self.results = []\n",
    "    \n",
    "    def design_experiment(self, \n",
    "                         ensemble_methods: List[str],\n",
    "                         test_scenarios: List[str],\n",
    "                         evaluation_metrics: List[str],\n",
    "                         hypothesis: str):\n",
    "        \"\"\"Design a research experiment following paper methodology\"\"\"\n",
    "        \n",
    "        experiment = {\n",
    "            'hypothesis': hypothesis,\n",
    "            'methods': ensemble_methods,\n",
    "            'scenarios': test_scenarios,\n",
    "            'metrics': evaluation_metrics,\n",
    "            'expected_outcomes': [],\n",
    "            'actual_results': []\n",
    "        }\n",
    "        \n",
    "        self.experiments.append(experiment)\n",
    "        return experiment\n",
    "    \n",
    "    def generate_research_report(self) -> str:\n",
    "        \"\"\"Generate a research report template\"\"\"\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Ensemble Learning Research Report\n",
    "\n",
    "## Research Question\n",
    "{self.research_question}\n",
    "\n",
    "## Methodology\n",
    "Following the taxonomic approach from \"Ensemble Learning for Large Language Models in Text and Code Generation: A Survey\", we investigate:\n",
    "\n",
    "### Ensemble Methods Tested\n",
    "- Output Ensemble (Majority Voting, Weighted)\n",
    "- Routing Ensemble (Cost-aware, Performance-aware)\n",
    "- Cascading Ensemble\n",
    "\n",
    "### Evaluation Framework\n",
    "Based on the paper's three-aspect evaluation:\n",
    "1. **Performance Metrics**: Accuracy, relevance, quality\n",
    "2. **Efficiency Metrics**: Computational cost, time complexity\n",
    "3. **Diversity Metrics**: Output variation, representation diversity\n",
    "\n",
    "## Experiments Conducted\n",
    "\"\"\"\n",
    "        \n",
    "        for i, exp in enumerate(self.experiments, 1):\n",
    "            report += f\"\"\"\n",
    "### Experiment {i}\n",
    "**Hypothesis**: {exp['hypothesis']}\n",
    "**Methods**: {', '.join(exp['methods'])}\n",
    "**Test Scenarios**: {', '.join(exp['scenarios'])}\n",
    "**Evaluation Metrics**: {', '.join(exp['metrics'])}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        report += \"\"\"\n",
    "## Key Findings\n",
    "[To be filled based on experimental results]\n",
    "\n",
    "## Implications for Practice\n",
    "[Analysis of practical applications]\n",
    "\n",
    "## Future Research Directions\n",
    "[Areas for further investigation]\n",
    "\n",
    "## References\n",
    "1. Ashiga, M., et al. \"Ensemble Learning for Large Language Models in Text and Code Generation: A Survey\" IEEE Transactions on Artificial Intelligence (2025)\n",
    "\"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example research template usage\n",
    "research = ResearchTemplate(\n",
    "    \"How do different ensemble strategies affect the trade-off between output quality and computational efficiency in code generation tasks?\"\n",
    ")\n",
    "\n",
    "# Design experiment following paper methodology\n",
    "experiment = research.design_experiment(\n",
    "    ensemble_methods=[\"output_ensemble\", \"routing\", \"cascading\"],\n",
    "    test_scenarios=[\"simple_functions\", \"complex_algorithms\", \"debugging_tasks\"],\n",
    "    evaluation_metrics=[\"pass_rate\", \"code_quality\", \"cost_efficiency\", \"diversity\"],\n",
    "    hypothesis=\"Routing-based ensembles will provide the best quality-cost trade-off for code generation tasks\"\n",
    ")\n",
    "\n",
    "# Generate research template\n",
    "research_report = research.generate_research_report()\n",
    "\n",
    "print(\"üìã RESEARCH TEMPLATE GENERATED\")\n",
    "print(\"=\" * 50)\n",
    "print(research_report[:1000] + \"...\")\n",
    "\n",
    "print(\"\\n‚úÖ Research template ready for your experiments!\")\n",
    "print(\"\\nüìù NEXT STEPS FOR YOUR RESEARCH:\")\n",
    "print(\"1. Define your specific research question\")\n",
    "print(\"2. Select appropriate ensemble methods from the paper's taxonomy\")\n",
    "print(\"3. Choose evaluation metrics that align with your use case\")\n",
    "print(\"4. Run experiments using the ensemble framework\")\n",
    "print(\"5. Analyze results and compare with paper findings\")\n",
    "print(\"6. Document insights and practical implications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways and Paper Insights\n",
    "\n",
    "### üìä Main Findings from the Paper:\n",
    "\n",
    "1. **Performance Improvement**: Ensemble methods improved instruction-following accuracy from **57% to 65%**\n",
    "\n",
    "2. **Parameter Efficiency**: MoE models like Mixtral 8x7B outperformed larger single models (LLaMA 2 70B) with fewer active parameters\n",
    "\n",
    "3. **Cost-Performance Trade-offs**: Routing-based methods provided optimal balance between quality and computational cost\n",
    "\n",
    "4. **Diversity Benefits**: Output ensemble methods maximized response diversity but at higher computational costs\n",
    "\n",
    "### üèóÔ∏è Seven Ensemble Categories Implemented:\n",
    "\n",
    "| Method | Approach | Best Use Case | Key Advantage |\n",
    "|--------|----------|---------------|---------------|\n",
    "| **Weight Merging** | Parameter combination | Training-free deployment | No additional training |\n",
    "| **Knowledge Fusion** | Representation combination | Multi-modal tasks | Rich feature integration |\n",
    "| **Mixture of Experts** | Specialized routing | Large-scale applications | Parameter efficiency |\n",
    "| **Reward Ensemble** | Quality-based selection | High-stakes applications | Quality optimization |\n",
    "| **Output Ensemble** | Response combination | Diverse output needs | Maximum diversity |\n",
    "| **Routing** | Dynamic model selection | Cost-sensitive applications | Cost optimization |\n",
    "| **Cascading** | Sequential refinement | Quality-critical tasks | Iterative improvement |\n",
    "\n",
    "### üîç Implementation Insights:\n",
    "\n",
    "- **LangChain Integration**: The framework easily adapts to different LLM providers\n",
    "- **Scalability**: Methods scale differently based on model count and complexity\n",
    "- **Real-world Applications**: Each method suits different business constraints\n",
    "\n",
    "### üöÄ Future Research Directions:\n",
    "\n",
    "1. **Multimodal Extensions**: Applying ensemble methods to vision-language models\n",
    "2. **Programming Language Specificity**: Specialized ensembles for different coding languages\n",
    "3. **Dynamic Ensemble Configuration**: Adaptive method selection based on task characteristics\n",
    "4. **Cost Optimization**: Advanced routing algorithms for production deployment\n",
    "\n",
    "---\n",
    "\n",
    "**üìö This implementation demonstrates the practical application of ensemble learning concepts from the survey paper, providing a foundation for further research and development in LLM ensemble methods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ References and Further Reading\n",
    "\n",
    "### Primary Paper\n",
    "**Ashiga, M., Jie, W., Wu, F., Voskanyan, V., Dinmohammadi, F., Brookes, P., Gong, J., & Wang, Z.** (2025). *Ensemble Learning for Large Language Models in Text and Code Generation: A Survey*. IEEE Transactions on Artificial Intelligence. arXiv:2503.13505v1 [cs.CL]\n",
    "\n",
    "### Key Models and Frameworks Referenced\n",
    "- **Mixtral 8x7B**: Sparse mixture of experts architecture\n",
    "- **LLaMA 2 70B**: Meta's large language model family\n",
    "- **GPT-3.5/GPT-4**: OpenAI's generative pretrained transformers\n",
    "- **Switch Transformer**: Google's efficient sparse expert models\n",
    "\n",
    "### Related Research Areas\n",
    "- **Parameter-Efficient Fine-tuning (PEFT)**\n",
    "- **Mixture of Experts (MoE) Architectures**\n",
    "- **Multi-Agent Systems in NLP**\n",
    "- **Cost-Aware Machine Learning**\n",
    "\n",
    "### Implementation Tools\n",
    "- **LangChain**: Framework for developing applications with language models\n",
    "- **DeepEval**: Evaluation framework for LLM applications\n",
    "- **Transformers**: Hugging Face's transformer implementations\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook provides a comprehensive implementation guide for ensemble learning methods in large language models, serving as both an educational resource and practical framework for research and development.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}