{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COTTON Setup and Requirements\n",
    "\n",
    "## Installation Guide and Environment Configuration\n",
    "\n",
    "This notebook provides comprehensive setup instructions for the COTTON framework implementation.\n",
    "\n",
    "### What You'll Learn:\n",
    "1. Dependency installation and management\n",
    "2. Environment validation and configuration\n",
    "3. API setup for external services\n",
    "4. Hardware requirements and optimization\n",
    "5. Quick start guide and troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements.txt - Core Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Dependencies for COTTON Implementation\n",
    "requirements_txt = \"\"\"\n",
    "# Core Dependencies\n",
    "torch>=2.0.0\n",
    "transformers>=4.35.0\n",
    "accelerate>=0.24.0\n",
    "peft>=0.6.0\n",
    "bitsandbytes>=0.41.0\n",
    "datasets>=2.14.0\n",
    "\n",
    "# Data Processing\n",
    "pandas>=1.5.0\n",
    "numpy>=1.24.0\n",
    "scikit-learn>=1.3.0\n",
    "\n",
    "# Evaluation Metrics\n",
    "rouge-score>=0.1.2\n",
    "nltk>=3.8.0\n",
    "sacrebleu>=2.3.0\n",
    "\n",
    "# LangChain & LangGraph\n",
    "langchain>=0.1.0\n",
    "langgraph>=0.0.30\n",
    "langchain-openai>=0.0.5\n",
    "langchain-anthropic>=0.1.0\n",
    "\n",
    "# API Clients\n",
    "openai>=1.0.0\n",
    "anthropic>=0.8.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib>=3.7.0\n",
    "seaborn>=0.12.0\n",
    "plotly>=5.15.0\n",
    "\n",
    "# Utilities\n",
    "tqdm>=4.65.0\n",
    "python-dotenv>=1.0.0\n",
    "requests>=2.31.0\n",
    "jupyter>=1.0.0\n",
    "ipywidgets>=8.0.0\n",
    "\n",
    "# Optional: For enhanced features\n",
    "faiss-cpu>=1.7.4\n",
    "sentence-transformers>=2.2.0\n",
    "\"\"\"\n",
    "\n",
    "# Save requirements.txt\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements_txt.strip())\n",
    "\n",
    "print(\"‚úÖ Requirements.txt created successfully!\")\n",
    "print(\"\\nüì¶ To install all dependencies, run:\")\n",
    "print(\"   pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automated Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ Successfully installed {package}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup the complete environment for COTTON\"\"\"\n",
    "    print(\"üöÄ Setting up COTTON environment...\")\n",
    "    \n",
    "    # Core packages\n",
    "    core_packages = [\n",
    "        \"torch>=2.0.0\",\n",
    "        \"transformers>=4.35.0\", \n",
    "        \"accelerate>=0.24.0\",\n",
    "        \"peft>=0.6.0\",\n",
    "        \"bitsandbytes>=0.41.0\",\n",
    "        \"datasets>=2.14.0\"\n",
    "    ]\n",
    "    \n",
    "    # LangChain packages\n",
    "    langchain_packages = [\n",
    "        \"langchain>=0.1.0\",\n",
    "        \"langgraph>=0.0.30\", \n",
    "        \"langchain-openai>=0.0.5\",\n",
    "        \"langchain-anthropic>=0.1.0\"\n",
    "    ]\n",
    "    \n",
    "    # Data science packages\n",
    "    data_packages = [\n",
    "        \"pandas>=1.5.0\",\n",
    "        \"numpy>=1.24.0\",\n",
    "        \"scikit-learn>=1.3.0\",\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"seaborn>=0.12.0\"\n",
    "    ]\n",
    "    \n",
    "    # Evaluation packages\n",
    "    eval_packages = [\n",
    "        \"rouge-score>=0.1.2\",\n",
    "        \"nltk>=3.8.0\",\n",
    "        \"sacrebleu>=2.3.0\"\n",
    "    ]\n",
    "    \n",
    "    # API packages\n",
    "    api_packages = [\n",
    "        \"openai>=1.0.0\",\n",
    "        \"anthropic>=0.8.0\"\n",
    "    ]\n",
    "    \n",
    "    # Utility packages\n",
    "    util_packages = [\n",
    "        \"tqdm>=4.65.0\",\n",
    "        \"python-dotenv>=1.0.0\",\n",
    "        \"requests>=2.31.0\",\n",
    "        \"jupyter>=1.0.0\"\n",
    "    ]\n",
    "    \n",
    "    all_packages = (\n",
    "        core_packages + langchain_packages + data_packages + \n",
    "        eval_packages + api_packages + util_packages\n",
    "    )\n",
    "    \n",
    "    failed_packages = []\n",
    "    \n",
    "    for package in all_packages:\n",
    "        if not install_package(package):\n",
    "            failed_packages.append(package)\n",
    "    \n",
    "    print(f\"\\nüìä Installation Summary:\")\n",
    "    print(f\"   ‚úÖ Successfully installed: {len(all_packages) - len(failed_packages)}/{len(all_packages)}\")\n",
    "    \n",
    "    if failed_packages:\n",
    "        print(f\"   ‚ùå Failed packages: {failed_packages}\")\n",
    "        print(\"   üí° Try installing failed packages manually\")\n",
    "    \n",
    "    # Download NLTK data\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"‚úÖ NLTK data downloaded\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  Could not download NLTK data\")\n",
    "    \n",
    "    print(\"\\nüéâ Environment setup complete!\")\n",
    "    return len(failed_packages) == 0\n",
    "\n",
    "# Uncomment the line below to run automatic installation\n",
    "# setup_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class APIConfig:\n",
    "    \"\"\"API configuration for external services\"\"\"\n",
    "    \n",
    "    # OpenAI API (for multi-agent cleaning and evaluation)\n",
    "    OPENAI_API_KEY: Optional[str] = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    # Anthropic API (for Claude integration)\n",
    "    ANTHROPIC_API_KEY: Optional[str] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    \n",
    "    # Hugging Face API (for model downloads)\n",
    "    HF_TOKEN: Optional[str] = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate API configuration\"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            warnings.append(\"‚ö†Ô∏è  OpenAI API key not found - multi-agent cleaning will use fallback methods\")\n",
    "        \n",
    "        if not self.ANTHROPIC_API_KEY:\n",
    "            warnings.append(\"‚ö†Ô∏è  Anthropic API key not found - Claude enhancement will be simulated\")\n",
    "        \n",
    "        if not self.HF_TOKEN:\n",
    "            warnings.append(\"‚ö†Ô∏è  Hugging Face token not found - may have issues downloading gated models\")\n",
    "        \n",
    "        for warning in warnings:\n",
    "            print(warning)\n",
    "        \n",
    "        return len(warnings) == 0\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration for COTTON training\"\"\"\n",
    "    \n",
    "    # Base model selection\n",
    "    BASE_MODEL: str = \"codellama/CodeLlama-7b-hf\"\n",
    "    \n",
    "    # Alternative models (uncomment to use)\n",
    "    # BASE_MODEL: str = \"microsoft/CodeGPT-small-py\"\n",
    "    # BASE_MODEL: str = \"Salesforce/codet5p-770m\"\n",
    "    \n",
    "    # Hardware configuration\n",
    "    DEVICE: str = \"cuda\" if os.system(\"nvidia-smi\") == 0 else \"cpu\"\n",
    "    MIXED_PRECISION: bool = True\n",
    "    USE_QUANTIZATION: bool = True\n",
    "    \n",
    "    # Memory optimization\n",
    "    GRADIENT_CHECKPOINTING: bool = True\n",
    "    DATALOADER_NUM_WORKERS: int = 4\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate model configuration\"\"\"\n",
    "        if self.DEVICE == \"cpu\":\n",
    "            print(\"‚ö†Ô∏è  Using CPU - training will be very slow\")\n",
    "            self.USE_QUANTIZATION = False\n",
    "        \n",
    "        if not self.USE_QUANTIZATION and self.DEVICE == \"cuda\":\n",
    "            print(\"üí° Quantization disabled - ensure sufficient GPU memory\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Test configuration\n",
    "api_config = APIConfig()\n",
    "model_config = ModelConfig()\n",
    "\n",
    "print(\"üîß Configuration Validation:\")\n",
    "print(\"=\" * 40)\n",
    "api_config.validate()\n",
    "model_config.validate()\n",
    "print(\"‚úÖ Configuration validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Variables Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env template file\n",
    "env_template = \"\"\"\n",
    "# COTTON Environment Variables\n",
    "# Copy this to .env and fill in your values\n",
    "\n",
    "# API Keys\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "ANTHROPIC_API_KEY=your_anthropic_api_key_here\n",
    "HF_TOKEN=your_huggingface_token_here\n",
    "\n",
    "# Model Configuration  \n",
    "BASE_MODEL=codellama/CodeLlama-7b-hf\n",
    "DEVICE=cuda\n",
    "USE_QUANTIZATION=true\n",
    "\n",
    "# Data Paths\n",
    "DATA_PATH=./data/codecot-9k\n",
    "OUTPUT_PATH=./outputs\n",
    "CACHE_DIR=./cache\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE=1\n",
    "LEARNING_RATE=1e-4\n",
    "MAX_EPOCHS=20\n",
    "EARLY_STOPPING=5\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVAL_BATCH_SIZE=4\n",
    "TEMPERATURE=0.1\n",
    "MAX_NEW_TOKENS=256\n",
    "\"\"\"\n",
    "\n",
    "# Save .env template\n",
    "with open('.env.template', 'w') as f:\n",
    "    f.write(env_template.strip())\n",
    "\n",
    "print(\"‚úÖ .env template created successfully!\")\n",
    "print(\"\\nüîë API Setup Instructions:\")\n",
    "print(\"   1. Copy .env.template to .env\")\n",
    "print(\"   2. Replace placeholder values with your actual API keys\")\n",
    "print(\"   3. Ensure .env is in your .gitignore to keep keys secure\")\n",
    "\n",
    "# Load environment variables if .env exists\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    if os.path.exists('.env'):\n",
    "        load_dotenv()\n",
    "        print(\"\\n‚úÖ Environment variables loaded from .env\")\n",
    "    else:\n",
    "        print(\"\\nüí° Create .env file from template to use API keys\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  python-dotenv not installed. Install with: pip install python-dotenv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. System Validation and Hardware Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "def validate_setup():\n",
    "    \"\"\"Validate that all components are properly set up\"\"\"\n",
    "    \n",
    "    print(\"üîç Validating COTTON setup...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # System Information\n",
    "    print(f\"üñ•Ô∏è  System: {platform.system()} {platform.release()}\")\n",
    "    print(f\"üêç Python: {sys.version}\")\n",
    "    \n",
    "    # Check Python version\n",
    "    if sys.version_info < (3, 8):\n",
    "        issues.append(\"‚ùå Python 3.8+ required\")\n",
    "    else:\n",
    "        print(\"‚úÖ Python version OK\")\n",
    "    \n",
    "    # Check memory\n",
    "    memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    print(f\"üß† RAM: {memory_gb:.1f} GB\")\n",
    "    if memory_gb < 8:\n",
    "        issues.append(\"‚ö†Ô∏è  Low RAM - recommend 16GB+ for training\")\n",
    "    \n",
    "    # Check core dependencies\n",
    "    required_packages = [\n",
    "        \"torch\", \"transformers\", \"peft\", \"datasets\",\n",
    "        \"langchain\", \"langgraph\", \"pandas\", \"numpy\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüì¶ Package Availability:\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"‚úÖ {package} available\")\n",
    "        except ImportError:\n",
    "            issues.append(f\"‚ùå {package} not installed\")\n",
    "            print(f\"‚ùå {package} not installed\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    print(\"\\nüéÆ GPU Information:\")\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            for i in range(gpu_count):\n",
    "                gpu_name = torch.cuda.get_device_name(i)\n",
    "                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "                print(f\"‚úÖ GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "            \n",
    "            if gpu_memory < 8:\n",
    "                issues.append(\"‚ö†Ô∏è  GPU memory < 8GB - may need quantization\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No GPU available - using CPU (training will be slow)\")\n",
    "            issues.append(\"‚ö†Ô∏è  No GPU detected\")\n",
    "    except Exception as e:\n",
    "        issues.append(f\"‚ùå Could not check GPU status: {e}\")\n",
    "    \n",
    "    # Check disk space\n",
    "    print(\"\\nüíæ Storage Information:\")\n",
    "    try:\n",
    "        free_space = shutil.disk_usage(\".\").free // (1024**3)  # GB\n",
    "        print(f\"üíæ Free space: {free_space} GB\")\n",
    "        if free_space < 20:\n",
    "            issues.append(\"‚ö†Ô∏è  Low disk space - recommend 50GB+ for models and data\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not check disk space: {e}\")\n",
    "    \n",
    "    # Check API keys (optional)\n",
    "    print(\"\\nüîë API Configuration:\")\n",
    "    api_config = APIConfig()\n",
    "    api_config.validate()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if issues:\n",
    "        print(f\"‚ùå Setup Issues Found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   {issue}\")\n",
    "        print(\"\\nüí° Please resolve these issues before proceeding\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"üéâ Setup validation complete - ready to run COTTON!\")\n",
    "        return True\n",
    "\n",
    "# Run validation\n",
    "validation_result = validate_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Start Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quick_start_guide():\n",
    "    \"\"\"Print comprehensive quick start guide\"\"\"\n",
    "    \n",
    "    guide = \"\"\"\n",
    "    üöÄ COTTON QUICK START GUIDE\n",
    "    ============================\n",
    "    \n",
    "    1. üì¶ INSTALLATION\n",
    "       pip install -r requirements.txt\n",
    "       # or run: python install_dependencies.py\n",
    "    \n",
    "    2. üîë API SETUP (Optional but recommended)\n",
    "       - Copy .env.template to .env\n",
    "       - Add your API keys:\n",
    "         * OpenAI API key (for multi-agent cleaning)\n",
    "         * Anthropic API key (for Claude integration)\n",
    "         * Hugging Face token (for model access)\n",
    "    \n",
    "    3. üìä DATA PREPARATION\n",
    "       - Use provided sample data, or\n",
    "       - Clone COTTON repository: https://github.com/NTDXYG/COTTON\n",
    "       - Place CodeCoT-9k dataset in ./data/codecot-9k/\n",
    "    \n",
    "    4. üèÉ RUNNING THE NOTEBOOK\n",
    "       jupyter notebook 1.6.2.1_COTTON_Implementation.ipynb\n",
    "       \n",
    "       OR run sections individually:\n",
    "       \n",
    "       # Load and explore data\n",
    "       dataset = CodeCoTDataset()\n",
    "       data = dataset.load_from_github()\n",
    "       \n",
    "       # Apply multi-agent cleaning\n",
    "       cleaner = MultiAgentCleaner(api_key=\"your_openai_key\")\n",
    "       cleaned_data = cleaner.clean_dataset(data)\n",
    "       \n",
    "       # Setup and train COTTON model\n",
    "       trainer = COTTONTrainer(config)\n",
    "       model, tokenizer = trainer.setup_model()\n",
    "       # trainer.train(train_data, val_data)  # Uncomment for actual training\n",
    "       \n",
    "       # Generate CoTs\n",
    "       inference = COTTONInference(model, tokenizer, config)\n",
    "       cots = inference.batch_generate_cots(problems)\n",
    "       \n",
    "       # Evaluate with LangChain\n",
    "       evaluator = COTTONEvaluator(api_key=\"your_openai_key\")\n",
    "       results = evaluator.batch_evaluate(cots, references)\n",
    "    \n",
    "    5. üîß CUSTOMIZATION\n",
    "       - Modify COTTONConfig for different models/settings\n",
    "       - Adjust multi-agent prompts for your domain\n",
    "       - Extend evaluation metrics\n",
    "       - Add custom CoT templates\n",
    "    \n",
    "    6. üöÄ DEPLOYMENT\n",
    "       - Export trained model: trainer.save_model(\"./cotton_final\")\n",
    "       - Use COTTONInference for production inference\n",
    "       - Integrate with your development workflow\n",
    "    \n",
    "    üìù EXAMPLE USAGE:\n",
    "    \n",
    "    ```python\n",
    "    # Simple CoT generation\n",
    "    problem = \"def find_max(numbers): ...\"\n",
    "    cot = inference.generate_cot(problem)\n",
    "    print(f\"Generated CoT: {cot}\")\n",
    "    \n",
    "    # LangChain evaluation\n",
    "    evaluation = evaluator.evaluate_cot_quality(cot)\n",
    "    print(f\"Quality scores: {evaluation}\")\n",
    "    \n",
    "    # Claude enhancement\n",
    "    enhanced = claude_enhancer.enhance_cot_with_claude(problem, cot)\n",
    "    print(f\"Enhanced CoT: {enhanced}\")\n",
    "    ```\n",
    "    \n",
    "    üÜò TROUBLESHOOTING:\n",
    "    \n",
    "    - CUDA out of memory? Reduce batch_size or enable quantization\n",
    "    - API errors? Check your API keys and quotas\n",
    "    - Model download issues? Verify HF_TOKEN and internet connection\n",
    "    - Evaluation errors? Ensure all dependencies are installed\n",
    "    \n",
    "    üìö RESOURCES:\n",
    "    \n",
    "    - Original paper: https://arxiv.org/abs/2312.05562\n",
    "    - GitHub repository: https://github.com/NTDXYG/COTTON\n",
    "    - LangChain docs: https://python.langchain.com/\n",
    "    - LangGraph docs: https://langchain-ai.github.io/langgraph/\n",
    "    \n",
    "    üéØ NEXT STEPS:\n",
    "    \n",
    "    1. Run the complete notebook end-to-end\n",
    "    2. Experiment with different base models\n",
    "    3. Try your own code generation problems\n",
    "    4. Integrate with your existing workflow\n",
    "    5. Contribute improvements back to the community!\n",
    "    \"\"\"\n",
    "    \n",
    "    print(guide)\n",
    "    return guide\n",
    "\n",
    "# Display quick start guide\n",
    "quick_start_guide = print_quick_start_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hardware Recommendations and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hardware_recommendations():\n",
    "    \"\"\"Print hardware recommendations for different use cases\"\"\"\n",
    "    \n",
    "    recommendations = \"\"\"\n",
    "    üñ•Ô∏è  HARDWARE RECOMMENDATIONS\n",
    "    ===============================\n",
    "    \n",
    "    üìä MINIMUM REQUIREMENTS:\n",
    "    - CPU: 4+ cores, 2.5GHz+\n",
    "    - RAM: 8GB (16GB recommended)\n",
    "    - GPU: Optional, any CUDA-capable GPU\n",
    "    - Storage: 20GB free space\n",
    "    - Internet: For model downloads and API calls\n",
    "    \n",
    "    üéØ RECOMMENDED SETUPS:\n",
    "    \n",
    "    1. DEVELOPMENT & EXPERIMENTATION:\n",
    "       - CPU: Intel i5/AMD Ryzen 5 or better\n",
    "       - RAM: 16GB DDR4\n",
    "       - GPU: RTX 3060/4060 (8GB VRAM) or RTX 3070/4070\n",
    "       - Storage: 50GB SSD space\n",
    "       \n",
    "    2. TRAINING & PRODUCTION:\n",
    "       - CPU: Intel i7/AMD Ryzen 7 or better\n",
    "       - RAM: 32GB DDR4\n",
    "       - GPU: RTX 3090/4090 (24GB VRAM) or A100/H100\n",
    "       - Storage: 100GB+ NVMe SSD\n",
    "       \n",
    "    3. CLOUD ALTERNATIVES:\n",
    "       - Google Colab Pro/Pro+ (T4/A100 access)\n",
    "       - AWS EC2 g4dn/p3 instances\n",
    "       - Azure NC series VMs\n",
    "       - Vast.ai for cost-effective GPU rental\n",
    "    \n",
    "    ‚ö° PERFORMANCE OPTIMIZATION TIPS:\n",
    "    \n",
    "    1. MEMORY OPTIMIZATION:\n",
    "       - Enable 4-bit quantization (bitsandbytes)\n",
    "       - Use gradient checkpointing\n",
    "       - Reduce batch size if OOM errors\n",
    "       - Clear GPU cache between runs\n",
    "       \n",
    "    2. TRAINING OPTIMIZATION:\n",
    "       - Use LoRA for parameter-efficient fine-tuning\n",
    "       - Enable mixed precision training\n",
    "       - Use data parallelism for multi-GPU setups\n",
    "       - Optimize dataloader workers\n",
    "       \n",
    "    3. INFERENCE OPTIMIZATION:\n",
    "       - Use greedy decoding for deterministic results\n",
    "       - Batch multiple requests when possible\n",
    "       - Cache frequently used prompts\n",
    "       - Consider model quantization for deployment\n",
    "    \n",
    "    üí∞ COST CONSIDERATIONS:\n",
    "    \n",
    "    1. LOCAL DEVELOPMENT:\n",
    "       - One-time hardware investment\n",
    "       - No ongoing API costs\n",
    "       - Full control over data privacy\n",
    "       \n",
    "    2. CLOUD USAGE:\n",
    "       - Pay-per-use model\n",
    "       - Access to latest hardware\n",
    "       - Ongoing operational costs\n",
    "       \n",
    "    3. HYBRID APPROACH:\n",
    "       - Local development + cloud training\n",
    "       - Best of both worlds\n",
    "       - Cost-effective for most users\n",
    "    \"\"\"\n",
    "    \n",
    "    print(recommendations)\n",
    "    \n",
    "    # Current system assessment\n",
    "    print(\"\\nüîç YOUR CURRENT SYSTEM ASSESSMENT:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        import psutil\n",
    "        \n",
    "        # Memory\n",
    "        ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        print(f\"üíæ RAM: {ram_gb:.1f}GB\")\n",
    "        \n",
    "        if ram_gb >= 32:\n",
    "            print(\"   ‚úÖ Excellent for training\")\n",
    "        elif ram_gb >= 16:\n",
    "            print(\"   ‚úÖ Good for development\")\n",
    "        elif ram_gb >= 8:\n",
    "            print(\"   ‚ö†Ô∏è  Minimum - consider upgrade\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Insufficient for training\")\n",
    "        \n",
    "        # GPU\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"üéÆ GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "            \n",
    "            if gpu_memory >= 20:\n",
    "                print(\"   ‚úÖ Excellent for training large models\")\n",
    "            elif gpu_memory >= 12:\n",
    "                print(\"   ‚úÖ Good for training with quantization\")\n",
    "            elif gpu_memory >= 8:\n",
    "                print(\"   ‚ö†Ô∏è  Sufficient for inference, limited training\")\n",
    "            else:\n",
    "                print(\"   ‚ùå May need significant optimization\")\n",
    "        else:\n",
    "            print(\"üéÆ GPU: None detected\")\n",
    "            print(\"   ‚ùå CPU-only training will be very slow\")\n",
    "        \n",
    "        # CPU\n",
    "        cpu_count = psutil.cpu_count()\n",
    "        print(f\"üîß CPU: {cpu_count} cores\")\n",
    "        \n",
    "        if cpu_count >= 8:\n",
    "            print(\"   ‚úÖ Excellent for data processing\")\n",
    "        elif cpu_count >= 4:\n",
    "            print(\"   ‚úÖ Good for most tasks\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  May bottleneck data loading\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not assess system: {e}\")\n",
    "\n",
    "# Display hardware recommendations\n",
    "print_hardware_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Troubleshooting Common Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_troubleshooting_guide():\n",
    "    \"\"\"Print comprehensive troubleshooting guide\"\"\"\n",
    "    \n",
    "    troubleshooting = \"\"\"\n",
    "    üîß TROUBLESHOOTING GUIDE\n",
    "    =========================\n",
    "    \n",
    "    üö® COMMON ISSUES AND SOLUTIONS:\n",
    "    \n",
    "    1. CUDA OUT OF MEMORY (OOM)\n",
    "       Symptoms: RuntimeError: CUDA out of memory\n",
    "       Solutions:\n",
    "       ‚úÖ Reduce batch size: config.BATCH_SIZE = 1\n",
    "       ‚úÖ Enable quantization: USE_QUANTIZATION = True\n",
    "       ‚úÖ Use gradient checkpointing: GRADIENT_CHECKPOINTING = True\n",
    "       ‚úÖ Clear GPU cache: torch.cuda.empty_cache()\n",
    "       ‚úÖ Use CPU offloading: device_map=\"auto\"\n",
    "    \n",
    "    2. MODEL DOWNLOAD FAILURES\n",
    "       Symptoms: HTTPError, Connection timeout\n",
    "       Solutions:\n",
    "       ‚úÖ Check internet connection\n",
    "       ‚úÖ Verify HuggingFace token: HF_TOKEN in .env\n",
    "       ‚úÖ Try different mirror: export HF_ENDPOINT=https://hf-mirror.com\n",
    "       ‚úÖ Download manually and specify local path\n",
    "       ‚úÖ Use smaller model variant first\n",
    "    \n",
    "    3. API AUTHENTICATION ERRORS\n",
    "       Symptoms: 401 Unauthorized, Invalid API key\n",
    "       Solutions:\n",
    "       ‚úÖ Verify API keys in .env file\n",
    "       ‚úÖ Check API key permissions and quotas\n",
    "       ‚úÖ Ensure .env is loaded: load_dotenv()\n",
    "       ‚úÖ Test API keys independently\n",
    "       ‚úÖ Use fallback methods when API unavailable\n",
    "    \n",
    "    4. DEPENDENCY CONFLICTS\n",
    "       Symptoms: ImportError, Version conflicts\n",
    "       Solutions:\n",
    "       ‚úÖ Create fresh virtual environment\n",
    "       ‚úÖ Use exact versions from requirements.txt\n",
    "       ‚úÖ Update pip: pip install --upgrade pip\n",
    "       ‚úÖ Install packages individually to isolate issues\n",
    "       ‚úÖ Check Python version compatibility\n",
    "    \n",
    "    5. SLOW TRAINING/INFERENCE\n",
    "       Symptoms: Very slow progress, high CPU usage\n",
    "       Solutions:\n",
    "       ‚úÖ Verify GPU is being used: torch.cuda.is_available()\n",
    "       ‚úÖ Enable mixed precision: use_amp=True\n",
    "       ‚úÖ Optimize dataloader: num_workers=4, pin_memory=True\n",
    "       ‚úÖ Use compiled models: torch.compile() (PyTorch 2.0+)\n",
    "       ‚úÖ Profile code to identify bottlenecks\n",
    "    \n",
    "    6. EVALUATION ERRORS\n",
    "       Symptoms: ROUGE/BLEU calculation failures\n",
    "       Solutions:\n",
    "       ‚úÖ Download NLTK data: nltk.download('punkt')\n",
    "       ‚úÖ Handle empty strings in metrics\n",
    "       ‚úÖ Use fallback evaluation when metrics fail\n",
    "       ‚úÖ Check text encoding issues\n",
    "       ‚úÖ Validate input format for evaluation functions\n",
    "    \n",
    "    7. LANGCHAIN/LANGGRAPH ISSUES\n",
    "       Symptoms: Import errors, workflow failures\n",
    "       Solutions:\n",
    "       ‚úÖ Update to latest versions: pip install --upgrade langchain langgraph\n",
    "       ‚úÖ Check compatibility matrix\n",
    "       ‚úÖ Use mock LLMs for testing without API keys\n",
    "       ‚úÖ Simplify workflows to isolate issues\n",
    "       ‚úÖ Check langchain community examples\n",
    "    \n",
    "    üîç DEBUGGING TECHNIQUES:\n",
    "    \n",
    "    1. ENABLE VERBOSE LOGGING:\n",
    "       ```python\n",
    "       import logging\n",
    "       logging.basicConfig(level=logging.DEBUG)\n",
    "       ```\n",
    "    \n",
    "    2. MEMORY MONITORING:\n",
    "       ```python\n",
    "       import torch\n",
    "       print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "       ```\n",
    "    \n",
    "    3. GRADUAL TESTING:\n",
    "       - Start with minimal examples\n",
    "       - Add complexity step by step\n",
    "       - Test each component independently\n",
    "    \n",
    "    4. ENVIRONMENT ISOLATION:\n",
    "       - Use virtual environments\n",
    "       - Document working configurations\n",
    "       - Keep backup of working setups\n",
    "    \n",
    "    üìû GETTING HELP:\n",
    "    \n",
    "    1. Check GitHub Issues: https://github.com/NTDXYG/COTTON/issues\n",
    "    2. LangChain Community: https://github.com/langchain-ai/langchain/discussions\n",
    "    3. HuggingFace Forums: https://discuss.huggingface.co/\n",
    "    4. Stack Overflow: Use tags [pytorch], [langchain], [transformers]\n",
    "    5. Discord Communities: HuggingFace, LangChain\n",
    "    \n",
    "    üéØ PREVENTION TIPS:\n",
    "    \n",
    "    - Always use virtual environments\n",
    "    - Pin dependency versions in requirements.txt\n",
    "    - Test with small datasets first\n",
    "    - Monitor system resources during execution\n",
    "    - Keep backups of working configurations\n",
    "    - Document successful setups for team sharing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(troubleshooting)\n",
    "\n",
    "# Display troubleshooting guide\n",
    "print_troubleshooting_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Setup Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_setup_check():\n",
    "    \"\"\"Perform final comprehensive setup verification\"\"\"\n",
    "    \n",
    "    print(\"üéØ FINAL SETUP VERIFICATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    checks = {\n",
    "        \"Python Version\": False,\n",
    "        \"Core Dependencies\": False,\n",
    "        \"GPU Access\": False,\n",
    "        \"API Configuration\": False,\n",
    "        \"Disk Space\": False,\n",
    "        \"Memory\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Python version check\n",
    "        if sys.version_info >= (3, 8):\n",
    "            checks[\"Python Version\"] = True\n",
    "            print(\"‚úÖ Python version compatible\")\n",
    "        else:\n",
    "            print(\"‚ùå Python version too old\")\n",
    "        \n",
    "        # Core dependencies check\n",
    "        required = [\"torch\", \"transformers\", \"langchain\", \"pandas\", \"numpy\"]\n",
    "        missing = []\n",
    "        for pkg in required:\n",
    "            try:\n",
    "                __import__(pkg)\n",
    "            except ImportError:\n",
    "                missing.append(pkg)\n",
    "        \n",
    "        if not missing:\n",
    "            checks[\"Core Dependencies\"] = True\n",
    "            print(\"‚úÖ All core dependencies available\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing dependencies: {missing}\")\n",
    "        \n",
    "        # GPU check\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                checks[\"GPU Access\"] = True\n",
    "                print(\"‚úÖ GPU access confirmed\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No GPU access (CPU fallback available)\")\n",
    "                checks[\"GPU Access\"] = \"partial\"\n",
    "        except:\n",
    "            print(\"‚ùå Cannot check GPU status\")\n",
    "        \n",
    "        # API configuration check\n",
    "        api_keys_found = 0\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            api_keys_found += 1\n",
    "        if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "            api_keys_found += 1\n",
    "        if os.getenv(\"HF_TOKEN\"):\n",
    "            api_keys_found += 1\n",
    "        \n",
    "        if api_keys_found >= 1:\n",
    "            checks[\"API Configuration\"] = True\n",
    "            print(f\"‚úÖ {api_keys_found}/3 API keys configured\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No API keys found (fallback methods available)\")\n",
    "            checks[\"API Configuration\"] = \"partial\"\n",
    "        \n",
    "        # Disk space check\n",
    "        try:\n",
    "            free_space = shutil.disk_usage(\".\").free // (1024**3)\n",
    "            if free_space >= 20:\n",
    "                checks[\"Disk Space\"] = True\n",
    "                print(f\"‚úÖ Sufficient disk space ({free_space}GB)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Low disk space ({free_space}GB)\")\n",
    "        except:\n",
    "            print(\"‚ùå Cannot check disk space\")\n",
    "        \n",
    "        # Memory check\n",
    "        try:\n",
    "            memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "            if memory_gb >= 8:\n",
    "                checks[\"Memory\"] = True\n",
    "                print(f\"‚úÖ Sufficient RAM ({memory_gb:.1f}GB)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Low RAM ({memory_gb:.1f}GB)\")\n",
    "        except:\n",
    "            print(\"‚ùå Cannot check memory\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during setup check: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    passed_checks = sum(1 for v in checks.values() if v is True)\n",
    "    partial_checks = sum(1 for v in checks.values() if v == \"partial\")\n",
    "    total_checks = len(checks)\n",
    "    \n",
    "    print(f\"üìä Setup Status: {passed_checks}/{total_checks} checks passed\")\n",
    "    if partial_checks > 0:\n",
    "        print(f\"‚ö†Ô∏è  {partial_checks} checks partially satisfied\")\n",
    "    \n",
    "    if passed_checks >= 4:\n",
    "        print(\"\\nüéâ READY TO RUN COTTON!\")\n",
    "        print(\"   You can proceed with the main implementation notebook.\")\n",
    "        print(\"   Start with: 1.6.2.1_COTTON_Implementation.ipynb\")\n",
    "    elif passed_checks + partial_checks >= 4:\n",
    "        print(\"\\n‚ö†Ô∏è  PARTIALLY READY\")\n",
    "        print(\"   You can run COTTON with limitations.\")\n",
    "        print(\"   Consider addressing warnings for full functionality.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå SETUP INCOMPLETE\")\n",
    "        print(\"   Please resolve the failed checks before proceeding.\")\n",
    "        print(\"   Refer to the troubleshooting guide above.\")\n",
    "    \n",
    "    return checks\n",
    "\n",
    "# Run final verification\n",
    "setup_status = final_setup_check()\n",
    "\n",
    "# Save setup report\n",
    "import json\n",
    "with open('setup_report.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'timestamp': '2024-01-01 12:00:00',  # Would use actual timestamp\n",
    "        'checks': setup_status,\n",
    "        'python_version': f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\",\n",
    "        'platform': platform.system()\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\nüìÅ Setup report saved to 'setup_report.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain_env)",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
