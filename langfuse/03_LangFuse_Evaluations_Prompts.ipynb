{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - LangFuse Evaluations & Prompt Management\n",
    "\n",
    "## 📖 Mục tiêu học tập\n",
    "\n",
    "Notebook này sẽ hướng dẫn bạn:\n",
    "- Hiểu cách thức đánh giá (evaluation) hệ thống LLM với LangFuse\n",
    "- Thực hiện đánh giá thủ công (human feedback) và tự động\n",
    "- Quản lý prompt versions thông qua LangFuse Prompt Management\n",
    "- Theo dõi hiệu suất và cải thiện chất lượng model qua thời gian\n",
    "- Tích hợp evaluation workflow vào quy trình phát triển LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 Giới thiệu\n",
    "\n",
    "### Tầm quan trọng của Evaluation trong LLM Development\n",
    "\n",
    "Đánh giá là một phần thiết yếu trong phát triển ứng dụng LLM vì:\n",
    "\n",
    "1. **Đảm bảo chất lượng**: Kiểm tra độ chính xác và phù hợp của output\n",
    "2. **Theo dõi hiệu suất**: Giám sát sự thay đổi chất lượng theo thời gian\n",
    "3. **A/B Testing**: So sánh hiệu quả của các prompt/model khác nhau\n",
    "4. **Compliance & Safety**: Đảm bảo output tuân thủ quy định và an toàn\n",
    "\n",
    "### LangFuse Evaluation Features\n",
    "\n",
    "- **Human Feedback**: Thu thập đánh giá từ người dùng thực tế\n",
    "- **Automated Scoring**: Đánh giá tự động dựa trên rule-based hoặc model-based\n",
    "- **Prompt Management**: Version control cho prompts với A/B testing\n",
    "- **Analytics Dashboard**: Visualize evaluation metrics và trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Cài đặt & Cấu hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt dependencies cần thiết\n",
    "!pip install langfuse langchain-anthropic python-dotenv uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langfuse import Langfuse\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ Đã import các thư viện cần thiết\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo LangFuse client\n",
    "langfuse = Langfuse(\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    ")\n",
    "\n",
    "# Khởi tạo ChatAnthropic\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"🚀 Đã khởi tạo LangFuse và ChatAnthropic\")\n",
    "print(f\"📊 LangFuse Host: {os.getenv('LANGFUSE_HOST', 'http://localhost:3000')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Ví dụ 1: Ghi nhận đánh giá thủ công (Human Feedback)\n",
    "\n",
    "Human feedback là cách quan trọng nhất để đánh giá chất lượng output của LLM vì chỉ con người mới có thể đánh giá được context, tone, và appropriateness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def generate_customer_response(customer_query: str, context: str = \"\") -> str:\n",
    "    \"\"\"Tạo phản hồi cho khách hàng dựa trên query và context\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"Bạn là một chuyên viên hỗ trợ khách hàng chuyên nghiệp. \n",
    "    Hãy trả lời câu hỏi của khách hàng một cách thân thiện, chính xác và hữu ích.\n",
    "    Nếu có thông tin context, hãy sử dụng để đưa ra câu trả lời phù hợp nhất.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=f\"Context: {context}\\n\\nCâu hỏi khách hàng: {customer_query}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test customer service responses\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"Tôi muốn đổi trả sản phẩm đã mua 3 ngày trước\",\n",
    "        \"context\": \"Chính sách đổi trả: 7 ngày, sản phẩm còn nguyên seal\",\n",
    "        \"trace_id\": str(uuid.uuid4())\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Làm sao để theo dõi đơn hàng của tôi?\",\n",
    "        \"context\": \"Hệ thống tracking có sẵn trên website, cần mã đơn hàng\",\n",
    "        \"trace_id\": str(uuid.uuid4())\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Sản phẩm này có bảo hành bao lâu?\",\n",
    "        \"context\": \"Điện thoại iPhone: bảo hành 12 tháng chính hãng\",\n",
    "        \"trace_id\": str(uuid.uuid4())\n",
    "    }\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for test_case in test_queries:\n",
    "    # Set trace context\n",
    "    langfuse_context.configure(\n",
    "        trace_id=test_case[\"trace_id\"],\n",
    "        session_id=\"customer_support_evaluation\",\n",
    "        user_id=\"evaluator_001\"\n",
    "    )\n",
    "    \n",
    "    response = generate_customer_response(\n",
    "        customer_query=test_case[\"query\"],\n",
    "        context=test_case[\"context\"]\n",
    "    )\n",
    "    \n",
    "    responses.append({\n",
    "        \"query\": test_case[\"query\"],\n",
    "        \"response\": response,\n",
    "        \"trace_id\": test_case[\"trace_id\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"❓ Query: {test_case['query']}\")\n",
    "    print(f\"🤖 Response: {response}\")\n",
    "    print(f\"🔗 Trace ID: {test_case['trace_id']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate human feedback collection\n",
    "def collect_human_feedback(trace_id: str, response_text: str, query: str):\n",
    "    \"\"\"Thu thập feedback từ người đánh giá\"\"\"\n",
    "    \n",
    "    print(f\"\\n📋 Đánh giá phản hồi cho: {query}\")\n",
    "    print(f\"🤖 Phản hồi: {response_text}\")\n",
    "    print(\"\\n📊 Tiêu chí đánh giá:\")\n",
    "    print(\"1. Helpfulness (1-5): Mức độ hữu ích\")\n",
    "    print(\"2. Accuracy (1-5): Độ chính xác\")\n",
    "    print(\"3. Friendliness (1-5): Độ thân thiện\")\n",
    "    print(\"4. Overall (1-5): Đánh giá tổng thể\")\n",
    "    \n",
    "    # Simulate evaluator scores (trong thực tế sẽ input từ UI)\n",
    "    import random\n",
    "    scores = {\n",
    "        \"helpfulness\": random.randint(3, 5),\n",
    "        \"accuracy\": random.randint(3, 5),\n",
    "        \"friendliness\": random.randint(3, 5),\n",
    "        \"overall\": random.randint(3, 5)\n",
    "    }\n",
    "    \n",
    "    # Tạo comment mẫu\n",
    "    comments = [\n",
    "        \"Phản hồi rất chuyên nghiệp và đầy đủ thông tin\",\n",
    "        \"Cần cải thiện thêm về độ chi tiết\",\n",
    "        \"Tone rất phù hợp với khách hàng\",\n",
    "        \"Chính xác và hữu ích\"\n",
    "    ]\n",
    "    \n",
    "    feedback_comment = random.choice(comments)\n",
    "    \n",
    "    return scores, feedback_comment\n",
    "\n",
    "# Thu thập feedback cho tất cả responses\n",
    "for response_data in responses:\n",
    "    scores, comment = collect_human_feedback(\n",
    "        response_data[\"trace_id\"],\n",
    "        response_data[\"response\"],\n",
    "        response_data[\"query\"]\n",
    "    )\n",
    "    \n",
    "    # Ghi feedback vào LangFuse\n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"helpfulness\",\n",
    "        value=scores[\"helpfulness\"],\n",
    "        comment=f\"Human feedback: {comment}\"\n",
    "    )\n",
    "    \n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"accuracy\",\n",
    "        value=scores[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"friendliness\",\n",
    "        value=scores[\"friendliness\"]\n",
    "    )\n",
    "    \n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"overall_rating\",\n",
    "        value=scores[\"overall\"],\n",
    "        comment=comment\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Đã ghi feedback cho trace: {response_data['trace_id'][:8]}...\")\n",
    "    print(f\"📊 Scores: {scores}\")\n",
    "    print(f\"💬 Comment: {comment}\\n\")\n",
    "\n",
    "print(\"🎉 Hoàn thành thu thập human feedback!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Ví dụ 2: Đánh giá tự động (Automated Evaluation)\n",
    "\n",
    "Automated evaluation cho phép scale việc đánh giá và đảm bảo consistency. Chúng ta sẽ implement các evaluator cho JSON format, safety, và content quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Format Evaluator\n",
    "def evaluate_json_format(response_text: str) -> Dict:\n",
    "    \"\"\"Kiểm tra xem response có đúng format JSON không\"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(response_text)\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"passed\": True,\n",
    "            \"reason\": \"Valid JSON format\",\n",
    "            \"details\": {\"keys\": list(parsed.keys()) if isinstance(parsed, dict) else \"array\"}\n",
    "        }\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Invalid JSON: {str(e)}\",\n",
    "            \"details\": {\"error\": str(e)}\n",
    "        }\n",
    "\n",
    "# Safety Evaluator\n",
    "def evaluate_safety(response_text: str) -> Dict:\n",
    "    \"\"\"Kiểm tra content safety (simplified version)\"\"\"\n",
    "    unsafe_keywords = [\n",
    "        \"hack\", \"phishing\", \"scam\", \"illegal\", \"virus\",\n",
    "        \"password\", \"credit card\", \"sensitive\"\n",
    "    ]\n",
    "    \n",
    "    found_unsafe = []\n",
    "    for keyword in unsafe_keywords:\n",
    "        if keyword.lower() in response_text.lower():\n",
    "            found_unsafe.append(keyword)\n",
    "    \n",
    "    if found_unsafe:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Unsafe keywords detected: {found_unsafe}\",\n",
    "            \"details\": {\"unsafe_keywords\": found_unsafe}\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"passed\": True,\n",
    "            \"reason\": \"No unsafe content detected\",\n",
    "            \"details\": {\"checked_keywords\": len(unsafe_keywords)}\n",
    "        }\n",
    "\n",
    "# Length Evaluator\n",
    "def evaluate_response_length(response_text: str, min_length: int = 50, max_length: int = 500) -> Dict:\n",
    "    \"\"\"Kiểm tra độ dài phản hồi có phù hợp không\"\"\"\n",
    "    length = len(response_text)\n",
    "    \n",
    "    if length < min_length:\n",
    "        return {\n",
    "            \"score\": 0.3,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Response too short: {length} chars (min: {min_length})\",\n",
    "            \"details\": {\"length\": length, \"min_required\": min_length}\n",
    "        }\n",
    "    elif length > max_length:\n",
    "        return {\n",
    "            \"score\": 0.7,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Response too long: {length} chars (max: {max_length})\",\n",
    "            \"details\": {\"length\": length, \"max_allowed\": max_length}\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"passed\": True,\n",
    "            \"reason\": f\"Appropriate length: {length} chars\",\n",
    "            \"details\": {\"length\": length, \"min\": min_length, \"max\": max_length}\n",
    "        }\n",
    "\n",
    "print(\"✅ Đã định nghĩa các automated evaluators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def generate_product_info(product_name: str, format_type: str = \"json\") -> str:\n",
    "    \"\"\"Tạo thông tin sản phẩm theo format được yêu cầu\"\"\"\n",
    "    \n",
    "    if format_type == \"json\":\n",
    "        system_prompt = \"\"\"Bạn là một AI assistant chuyên cung cấp thông tin sản phẩm.\n",
    "        Hãy trả về thông tin sản phẩm dưới dạng JSON với các field: name, price, description, features, availability.\n",
    "        Chỉ trả về JSON, không có text khác.\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"\"\"Bạn là một AI assistant chuyên cung cấp thông tin sản phẩm.\n",
    "        Hãy mô tả sản phẩm một cách chi tiết và hấp dẫn.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=f\"Cung cấp thông tin về sản phẩm: {product_name}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# Test với các sản phẩm khác nhau\n",
    "test_products = [\n",
    "    {\"name\": \"iPhone 15 Pro\", \"format\": \"json\", \"trace_id\": str(uuid.uuid4())},\n",
    "    {\"name\": \"MacBook Air M2\", \"format\": \"json\", \"trace_id\": str(uuid.uuid4())},\n",
    "    {\"name\": \"AirPods Pro\", \"format\": \"text\", \"trace_id\": str(uuid.uuid4())}\n",
    "]\n",
    "\n",
    "product_responses = []\n",
    "\n",
    "for product in test_products:\n",
    "    langfuse_context.configure(\n",
    "        trace_id=product[\"trace_id\"],\n",
    "        session_id=\"product_info_evaluation\",\n",
    "        user_id=\"system_test\"\n",
    "    )\n",
    "    \n",
    "    response = generate_product_info(\n",
    "        product_name=product[\"name\"],\n",
    "        format_type=product[\"format\"]\n",
    "    )\n",
    "    \n",
    "    product_responses.append({\n",
    "        \"product\": product[\"name\"],\n",
    "        \"format\": product[\"format\"],\n",
    "        \"response\": response,\n",
    "        \"trace_id\": product[\"trace_id\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"📱 Product: {product['name']} ({product['format']})\")\n",
    "    print(f\"🔗 Trace: {product['trace_id']}\")\n",
    "    print(f\"📝 Response: {response[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Ví dụ 3: Prompt Management với LangFuse\n",
    "\n",
    "Prompt Management cho phép version control prompts, A/B test different versions, và theo dõi performance của từng prompt version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define các prompt versions cho customer support\n",
    "customer_support_prompts = {\n",
    "    \"v1_basic\": {\n",
    "        \"content\": \"\"\"Bạn là một nhân viên hỗ trợ khách hàng. \n",
    "Hãy trả lời câu hỏi của khách hàng một cách lịch sự và hữu ích.\n",
    "\n",
    "Câu hỏi: {question}\n",
    "Context: {context}\"\"\",\n",
    "        \"labels\": [\"customer-support\", \"basic\"]\n",
    "    },\n",
    "    \n",
    "    \"v2_detailed\": {\n",
    "        \"content\": \"\"\"Bạn là một chuyên viên hỗ trợ khách hàng chuyên nghiệp của công ty.\n",
    "Khi trả lời khách hàng, hãy:\n",
    "1. Thể hiện sự đồng cảm và hiểu biết\n",
    "2. Cung cấp thông tin chính xác và chi tiết\n",
    "3. Đưa ra các bước hành động cụ thể nếu cần\n",
    "4. Kết thúc bằng việc hỏi thêm nếu khách hàng cần hỗ trợ gì khác\n",
    "\n",
    "Context có sẵn: {context}\n",
    "Câu hỏi của khách hàng: {question}\n",
    "\n",
    "Phản hồi của bạn:\"\"\",\n",
    "        \"labels\": [\"customer-support\", \"detailed\", \"structured\"]\n",
    "    },\n",
    "    \n",
    "    \"v3_empathetic\": {\n",
    "        \"content\": \"\"\"Bạn là một chuyên viên hỗ trợ khách hàng giàu kinh nghiệm và rất quan tâm đến trải nghiệm khách hàng.\n",
    "\n",
    "Nguyên tắc phản hồi:\n",
    "- Luôn bắt đầu bằng việc thừa nhận và đồng cảm với tình huống của khách hàng\n",
    "- Sử dụng ngôn ngữ ấm áp, thân thiện nhưng vẫn chuyên nghiệp\n",
    "- Giải thích rõ ràng các policy và quy trình\n",
    "- Đưa ra các lựa chọn và giải pháp thay thế nếu có thể\n",
    "- Đảm bảo khách hàng cảm thấy được lắng nghe và quan tâm\n",
    "\n",
    "Thông tin tham khảo: {context}\n",
    "Câu hỏi/Vấn đề của khách hàng: {question}\n",
    "\n",
    "Hãy trả lời một cách chu đáo và hỗ trợ tốt nhất:\"\"\",\n",
    "        \"labels\": [\"customer-support\", \"empathetic\", \"advanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"📝 Defined {len(customer_support_prompts)} prompt versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def respond_with_prompt_version(question: str, context: str, prompt_version: str):\n",
    "    \"\"\"Sử dụng specific prompt version để trả lời\"\"\"\n",
    "    \n",
    "    # Get prompt content\n",
    "    prompt_content = customer_support_prompts[prompt_version][\"content\"]\n",
    "    \n",
    "    # Format prompt với variables\n",
    "    formatted_prompt = prompt_content.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    # Gọi LLM\n",
    "    messages = [HumanMessage(content=formatted_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Log prompt version được sử dụng\n",
    "    langfuse_context.update_current_trace(\n",
    "        metadata={\n",
    "            \"prompt_version\": prompt_version,\n",
    "            \"prompt_name\": \"customer_support_response\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "print(\"✅ Đã setup prompt management functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Test các prompt versions\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"question\": \"Tôi đặt hàng 1 tuần rồi mà chưa nhận được, rất thất vọng!\",\n",
    "        \"context\": \"Đơn hàng #12345, ship date dự kiến: 3-5 ngày, hiện tại đã 7 ngày\",\n",
    "        \"scenario_id\": \"delayed_delivery\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Sản phẩm tôi nhận được bị lỗi, muốn đổi trả\",\n",
    "        \"context\": \"Chính sách: đổi trả trong 7 ngày, cần hóa đơn và sản phẩm nguyên vẹn\",\n",
    "        \"scenario_id\": \"defective_product\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test từng prompt version với mỗi scenario\n",
    "comparison_results = []\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\n🧪 Testing scenario: {scenario['scenario_id']}\")\n",
    "    print(f\"❓ Question: {scenario['question']}\")\n",
    "    \n",
    "    scenario_results = {\n",
    "        \"scenario_id\": scenario[\"scenario_id\"],\n",
    "        \"question\": scenario[\"question\"],\n",
    "        \"responses\": {}\n",
    "    }\n",
    "    \n",
    "    for version in [\"v1_basic\", \"v2_detailed\", \"v3_empathetic\"]:\n",
    "        trace_id = str(uuid.uuid4())\n",
    "        \n",
    "        langfuse_context.configure(\n",
    "            trace_id=trace_id,\n",
    "            session_id=f\"prompt_ab_test_{scenario['scenario_id']}\",\n",
    "            user_id=\"prompt_tester\",\n",
    "            tags=[\"ab_test\", f\"prompt_{version}\", scenario[\"scenario_id\"]]\n",
    "        )\n",
    "        \n",
    "        response = respond_with_prompt_version(\n",
    "            question=scenario[\"question\"],\n",
    "            context=scenario[\"context\"],\n",
    "            prompt_version=version\n",
    "        )\n",
    "        \n",
    "        scenario_results[\"responses\"][version] = {\n",
    "            \"response\": response,\n",
    "            \"trace_id\": trace_id,\n",
    "            \"length\": len(response)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📝 {version.upper()}:\")\n",
    "        print(f\"Response ({len(response)} chars): {response[:150]}...\")\n",
    "        print(f\"Trace: {trace_id}\")\n",
    "    \n",
    "    comparison_results.append(scenario_results)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n🎯 Hoàn thành A/B testing các prompt versions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Giải thích & Phân tích\n",
    "\n",
    "### Truy cập LangFuse Dashboard\n",
    "\n",
    "Để xem và phân tích kết quả evaluation:\n",
    "\n",
    "1. **Mở LangFuse UI**: Truy cập `http://localhost:3000` (hoặc URL của LangFuse instance)\n",
    "\n",
    "2. **Xem Traces**: \n",
    "   - Navigate đến \"Traces\" tab\n",
    "   - Filter theo session_id để xem specific evaluation runs\n",
    "   - Click vào individual traces để xem chi tiết\n",
    "\n",
    "3. **Phân tích Scores**:\n",
    "   - Trong \"Scores\" tab, xem distribution của các evaluation metrics\n",
    "   - So sánh performance giữa các prompt versions\n",
    "   - Track trends theo thời gian\n",
    "\n",
    "4. **Prompt Management**:\n",
    "   - Trong \"Prompts\" tab, xem các versions đã tạo\n",
    "   - Compare performance của different versions\n",
    "   - Deploy prompts tốt nhất vào production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush all data to LangFuse\n",
    "langfuse.flush()\n",
    "print(\"✅ All evaluation data has been sent to LangFuse!\")\n",
    "print(\"📊 Check LangFuse Dashboard at http://localhost:3000 for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Kết luận & Bước tiếp theo\n",
    "\n",
    "### Những gì đã học được\n",
    "\n",
    "1. **Human Feedback Collection**: Cách thu thập và ghi nhận feedback từ người dùng thực tế\n",
    "2. **Automated Evaluation**: Implement các evaluator tự động cho format, safety, và quality\n",
    "3. **Prompt Management**: Version control và A/B testing cho prompts\n",
    "4. **Performance Analysis**: Phân tích và so sánh hiệu suất của các approaches khác nhau\n",
    "5. **LangFuse Integration**: Sử dụng LangFuse để tracking và visualization\n",
    "\n",
    "### Best Practices đã áp dụng\n",
    "\n",
    "- **Structured Evaluation**: Định nghĩa rõ các tiêu chí đánh giá\n",
    "- **Automated + Human**: Kết hợp automation với human judgment\n",
    "- **Version Control**: Theo dõi changes và performance của prompts\n",
    "- **Continuous Monitoring**: Setup pipeline để evaluation liên tục\n",
    "\n",
    "### Bước tiếp theo\n",
    "\n",
    "1. **Production Deployment**: \n",
    "   - Deploy prompt version tốt nhất vào production\n",
    "   - Implement gradual rollout strategy\n",
    "\n",
    "2. **Automated Pipeline**:\n",
    "   - Setup CI/CD cho prompt testing\n",
    "   - Automated regression testing\n",
    "\n",
    "3. **Advanced Evaluation**:\n",
    "   - Model-based evaluation (using LLM as judge)\n",
    "   - Multi-dimensional scoring\n",
    "   - Real-time feedback collection\n",
    "\n",
    "4. **Scaling**:\n",
    "   - Evaluation cho multiple use cases\n",
    "   - Cross-model comparison\n",
    "   - Performance optimization\n",
    "\n",
    "### Resources cho học thêm\n",
    "\n",
    "- [LangFuse Documentation](https://langfuse.com/docs)\n",
    "- [LLM Evaluation Best Practices](https://langfuse.com/guides/evaluation)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [A/B Testing for AI Systems](https://langfuse.com/guides/ab-testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}