{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning 1: Evaluation Metrics for Semantic Search\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Master the mathematical foundations of nDCG, MRR, and mAP metrics\n",
    "- Understand why these metrics are crucial for semantic search evaluation\n",
    "- Implement metrics from scratch with step-by-step explanations\n",
    "- Analyze how different ranking scenarios affect metric scores\n",
    "\n",
    "## üìö Paper Context\n",
    "**Section 3.2**: \"Assessing the proficiency and effectiveness of various semantic search methodologies necessitates the use of specific evaluation metrics. Our attention centers on the pivotal metrics: Normalized Discounted Cumulative Gain (nDCG), Mean Reciprocal Rank (MRR), and Mean Average Precision (mAP).\"\n",
    "\n",
    "## üî¨ Why These Metrics Matter\n",
    "Unlike simple accuracy metrics, these evaluation measures account for:\n",
    "- **Ranking Quality**: Position matters in search results\n",
    "- **Graded Relevance**: Documents can be highly relevant, somewhat relevant, or irrelevant\n",
    "- **User Experience**: Earlier results are more valuable to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä Evaluation Metrics Learning Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalized Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "### Mathematical Foundation\n",
    "From the paper (Equation 1-2):\n",
    "\n",
    "$$nDCG@k = \\frac{DCG@k}{IDCG@k}$$\n",
    "\n",
    "$$DCG@k = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}$$\n",
    "\n",
    "Where:\n",
    "- `DCG@k`: Discounted Cumulative Gain at rank k\n",
    "- `IDCG@k`: Ideal DCG (maximum possible DCG)\n",
    "- `rel_i`: Graded relevance of result at position i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCGCalculator:\n",
    "    \"\"\"Step-by-step nDCG calculation with detailed explanations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.calculation_steps = []\n",
    "    \n",
    "    def calculate_dcg(self, relevance_scores: List[int], k: int = None, verbose: bool = True) -> float:\n",
    "        \"\"\"Calculate Discounted Cumulative Gain with detailed steps\"\"\"\n",
    "        if k is None:\n",
    "            k = len(relevance_scores)\n",
    "        \n",
    "        k = min(k, len(relevance_scores))\n",
    "        dcg = 0.0\n",
    "        calculation_details = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüî¢ Calculating DCG@{k}:\")\n",
    "            print(\"Position | Relevance | 2^rel - 1 | log‚ÇÇ(pos+1) | Contribution\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        for i in range(k):\n",
    "            position = i + 1\n",
    "            relevance = relevance_scores[i]\n",
    "            \n",
    "            # DCG formula components\n",
    "            numerator = (2 ** relevance) - 1\n",
    "            denominator = np.log2(position + 1)\n",
    "            contribution = numerator / denominator\n",
    "            \n",
    "            dcg += contribution\n",
    "            \n",
    "            calculation_details.append({\n",
    "                'position': position,\n",
    "                'relevance': relevance,\n",
    "                'numerator': numerator,\n",
    "                'denominator': denominator,\n",
    "                'contribution': contribution\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{position:8d} | {relevance:9d} | {numerator:9.0f} | {denominator:10.3f} | {contribution:12.4f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüìä Total DCG@{k} = {dcg:.4f}\")\n",
    "        \n",
    "        return dcg, calculation_details\n",
    "    \n",
    "    def calculate_ideal_dcg(self, relevance_scores: List[int], k: int = None, verbose: bool = True) -> float:\n",
    "        \"\"\"Calculate Ideal DCG (best possible ranking)\"\"\"\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n‚≠ê Ideal ranking: {ideal_scores}\")\n",
    "        \n",
    "        ideal_dcg, _ = self.calculate_dcg(ideal_scores, k, verbose)\n",
    "        return ideal_dcg\n",
    "    \n",
    "    def calculate_ndcg(self, relevance_scores: List[int], k: int = None, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate nDCG with complete breakdown\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\nüéØ nDCG Calculation for relevance scores: {relevance_scores}\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        # Calculate actual DCG\n",
    "        dcg, dcg_details = self.calculate_dcg(relevance_scores, k, verbose)\n",
    "        \n",
    "        # Calculate ideal DCG\n",
    "        ideal_dcg = self.calculate_ideal_dcg(relevance_scores, k, verbose)\n",
    "        \n",
    "        # Calculate nDCG\n",
    "        if ideal_dcg == 0:\n",
    "            ndcg = 0.0\n",
    "        else:\n",
    "            ndcg = dcg / ideal_dcg\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüèÜ Final nDCG@{k or len(relevance_scores)} = {dcg:.4f} / {ideal_dcg:.4f} = {ndcg:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'ndcg': ndcg,\n",
    "            'dcg': dcg,\n",
    "            'ideal_dcg': ideal_dcg,\n",
    "            'dcg_details': dcg_details\n",
    "        }\n",
    "\n",
    "# Demonstrate nDCG calculation\n",
    "ndcg_calc = NDCGCalculator()\n",
    "\n",
    "# Example from paper: relevance scores (0=irrelevant, 1=somewhat relevant, 2=very relevant)\n",
    "example_relevance = [2, 1, 0, 1, 2]  # Ranking: very, somewhat, irrelevant, somewhat, very\n",
    "result = ndcg_calc.calculate_ndcg(example_relevance, k=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean Reciprocal Rank (MRR)\n",
    "\n",
    "### Mathematical Foundation\n",
    "From the paper (Equation 3):\n",
    "\n",
    "$$MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$$\n",
    "\n",
    "Where:\n",
    "- `|Q|`: Total number of queries\n",
    "- `rank_i`: Rank position of first very relevant document for query i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRRCalculator:\n",
    "    \"\"\"Mean Reciprocal Rank calculation with detailed analysis\"\"\"\n",
    "    \n",
    "    def find_first_relevant_rank(self, relevance_scores: List[int], \n",
    "                                predictions: List[float], \n",
    "                                min_relevance: int = 2) -> int:\n",
    "        \"\"\"Find rank of first highly relevant document\"\"\"\n",
    "        # Sort by predictions (highest first)\n",
    "        sorted_indices = np.argsort(predictions)[::-1]\n",
    "        \n",
    "        for rank, idx in enumerate(sorted_indices, 1):\n",
    "            if relevance_scores[idx] >= min_relevance:\n",
    "                return rank\n",
    "        \n",
    "        return float('inf')  # No relevant document found\n",
    "    \n",
    "    def calculate_mrr_single_query(self, relevance_scores: List[int], \n",
    "                                  predictions: List[float], \n",
    "                                  verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate MRR for a single query with detailed breakdown\"\"\"\n",
    "        \n",
    "        # Find first relevant rank\n",
    "        first_relevant_rank = self.find_first_relevant_rank(relevance_scores, predictions)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        if first_relevant_rank == float('inf'):\n",
    "            rr = 0.0\n",
    "        else:\n",
    "            rr = 1.0 / first_relevant_rank\n",
    "        \n",
    "        if verbose:\n",
    "            # Show ranking process\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            print(f\"\\nüîç MRR Calculation:\")\n",
    "            print(\"Rank | Doc Index | Prediction | Relevance | Very Relevant?\")\n",
    "            print(\"-\" * 55)\n",
    "            \n",
    "            for rank, idx in enumerate(sorted_indices, 1):\n",
    "                is_very_relevant = \"‚úÖ YES\" if relevance_scores[idx] >= 2 else \"‚ùå No\"\n",
    "                print(f\"{rank:4d} | {idx:9d} | {predictions[idx]:10.3f} | {relevance_scores[idx]:9d} | {is_very_relevant}\")\n",
    "                \n",
    "                if relevance_scores[idx] >= 2 and rank == first_relevant_rank:\n",
    "                    print(f\"     ‚¨ÜÔ∏è First very relevant document found at rank {rank}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\nüìä Reciprocal Rank = 1/{first_relevant_rank} = {rr:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'reciprocal_rank': rr,\n",
    "            'first_relevant_rank': first_relevant_rank,\n",
    "            'sorted_ranking': list(np.argsort(predictions)[::-1])\n",
    "        }\n",
    "    \n",
    "    def calculate_mrr_multiple_queries(self, queries_data: List[Dict], verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate MRR across multiple queries\"\"\"\n",
    "        reciprocal_ranks = []\n",
    "        query_details = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüìã MRR Calculation for {len(queries_data)} Queries\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        for i, query_data in enumerate(queries_data):\n",
    "            if verbose:\n",
    "                print(f\"\\nüîç Query {i+1}: {query_data.get('query', f'Query {i+1}')}\")\n",
    "            \n",
    "            result = self.calculate_mrr_single_query(\n",
    "                query_data['relevance'], \n",
    "                query_data['predictions'], \n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            reciprocal_ranks.append(result['reciprocal_rank'])\n",
    "            query_details.append(result)\n",
    "        \n",
    "        mean_rr = np.mean(reciprocal_ranks)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüèÜ Mean Reciprocal Rank = {mean_rr:.4f}\")\n",
    "            print(f\"üìä Individual RRs: {[f'{rr:.3f}' for rr in reciprocal_ranks]}\")\n",
    "        \n",
    "        return {\n",
    "            'mrr': mean_rr,\n",
    "            'reciprocal_ranks': reciprocal_ranks,\n",
    "            'query_details': query_details\n",
    "        }\n",
    "\n",
    "# Demonstrate MRR calculation\n",
    "mrr_calc = MRRCalculator()\n",
    "\n",
    "# Example single query\n",
    "example_relevance = [0, 2, 1, 0, 1]  # Second document is very relevant\n",
    "example_predictions = [0.9, 0.3, 0.7, 0.1, 0.5]  # Our model's confidence scores\n",
    "\n",
    "single_result = mrr_calc.calculate_mrr_single_query(example_relevance, example_predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mean Average Precision (mAP)\n",
    "\n",
    "### Mathematical Foundation\n",
    "From the paper (Equations 4-5):\n",
    "\n",
    "$$AP = \\frac{\\sum_{k=1}^{n} (P(k) \\times rel_k)}{\\text{# relevant documents}}$$\n",
    "\n",
    "$$mAP = \\frac{\\sum_{q=1}^{Q} AP_q}{Q}$$\n",
    "\n",
    "Where:\n",
    "- `P(k)`: Precision at cutoff k\n",
    "- `rel_k`: Relevancy score at rank k\n",
    "- `AP_q`: Average Precision for query q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPCalculator:\n",
    "    \"\"\"Mean Average Precision calculation with comprehensive analysis\"\"\"\n",
    "    \n",
    "    def calculate_precision_at_k(self, relevance_scores: List[int], \n",
    "                                sorted_indices: List[int], \n",
    "                                k: int) -> float:\n",
    "        \"\"\"Calculate precision at cutoff k\"\"\"\n",
    "        relevant_count = 0\n",
    "        \n",
    "        for i in range(min(k, len(sorted_indices))):\n",
    "            idx = sorted_indices[i]\n",
    "            if relevance_scores[idx] > 0:  # Any relevance > 0\n",
    "                relevant_count += 1\n",
    "        \n",
    "        return relevant_count / k if k > 0 else 0.0\n",
    "    \n",
    "    def calculate_ap_single_query(self, relevance_scores: List[int], \n",
    "                                 predictions: List[float], \n",
    "                                 k: int = None, \n",
    "                                 verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate Average Precision for single query\"\"\"\n",
    "        \n",
    "        if k is None:\n",
    "            k = len(relevance_scores)\n",
    "        \n",
    "        # Sort by predictions (highest first)\n",
    "        sorted_indices = list(np.argsort(predictions)[::-1])\n",
    "        total_relevant = sum(1 for rel in relevance_scores if rel > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            return {'ap': 0.0, 'precision_at_k': [], 'calculation_steps': []}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüìä Average Precision Calculation (k={k}):\")\n",
    "            print(f\"Total relevant documents: {total_relevant}\")\n",
    "            print(\"\\nRank | Doc | Pred | Rel | P@k | Rel? | Contribution\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        precision_sum = 0.0\n",
    "        precision_at_k_values = []\n",
    "        calculation_steps = []\n",
    "        \n",
    "        for rank in range(1, min(k + 1, len(sorted_indices) + 1)):\n",
    "            idx = sorted_indices[rank - 1]\n",
    "            relevance = relevance_scores[idx]\n",
    "            prediction = predictions[idx]\n",
    "            \n",
    "            # Calculate precision at this rank\n",
    "            precision_at_rank = self.calculate_precision_at_k(relevance_scores, sorted_indices, rank)\n",
    "            precision_at_k_values.append(precision_at_rank)\n",
    "            \n",
    "            # Add to precision sum if document is relevant\n",
    "            is_relevant = relevance > 0\n",
    "            contribution = precision_at_rank if is_relevant else 0.0\n",
    "            precision_sum += contribution\n",
    "            \n",
    "            calculation_steps.append({\n",
    "                'rank': rank,\n",
    "                'doc_idx': idx,\n",
    "                'prediction': prediction,\n",
    "                'relevance': relevance,\n",
    "                'precision_at_k': precision_at_rank,\n",
    "                'is_relevant': is_relevant,\n",
    "                'contribution': contribution\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                relevant_symbol = \"‚úÖ\" if is_relevant else \"‚ùå\"\n",
    "                print(f\"{rank:4d} | {idx:3d} | {prediction:.2f} | {relevance:3d} | {precision_at_rank:.3f} | {relevant_symbol:2s} | {contribution:.4f}\")\n",
    "        \n",
    "        # Calculate final Average Precision\n",
    "        ap = precision_sum / total_relevant\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüéØ Average Precision = {precision_sum:.4f} / {total_relevant} = {ap:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'ap': ap,\n",
    "            'precision_at_k': precision_at_k_values,\n",
    "            'calculation_steps': calculation_steps,\n",
    "            'precision_sum': precision_sum,\n",
    "            'total_relevant': total_relevant\n",
    "        }\n",
    "    \n",
    "    def calculate_map_multiple_queries(self, queries_data: List[Dict], \n",
    "                                      k: int = None, \n",
    "                                      verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate Mean Average Precision across multiple queries\"\"\"\n",
    "        \n",
    "        ap_scores = []\n",
    "        query_details = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüìã mAP Calculation for {len(queries_data)} Queries\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        for i, query_data in enumerate(queries_data):\n",
    "            if verbose:\n",
    "                print(f\"\\nüîç Query {i+1}: {query_data.get('query', f'Query {i+1}')}\")\n",
    "            \n",
    "            result = self.calculate_ap_single_query(\n",
    "                query_data['relevance'], \n",
    "                query_data['predictions'], \n",
    "                k=k, \n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            ap_scores.append(result['ap'])\n",
    "            query_details.append(result)\n",
    "        \n",
    "        mean_ap = np.mean(ap_scores)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüèÜ Mean Average Precision (mAP@{k or 'all'}) = {mean_ap:.4f}\")\n",
    "            print(f\"üìä Individual APs: {[f'{ap:.3f}' for ap in ap_scores]}\")\n",
    "        \n",
    "        return {\n",
    "            'map': mean_ap,\n",
    "            'ap_scores': ap_scores,\n",
    "            'query_details': query_details\n",
    "        }\n",
    "\n",
    "# Demonstrate mAP calculation\n",
    "map_calc = MAPCalculator()\n",
    "\n",
    "# Example with more complex relevance pattern\n",
    "example_relevance = [1, 0, 2, 1, 0]  # Mixed relevance levels\n",
    "example_predictions = [0.8, 0.2, 0.6, 0.9, 0.1]  # Our model's predictions\n",
    "\n",
    "ap_result = map_calc.calculate_ap_single_query(example_relevance, example_predictions, k=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Analysis: Understanding Metric Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ranking_scenarios():\n",
    "    \"\"\"Compare how different ranking scenarios affect all three metrics\"\"\"\n",
    "    \n",
    "    # Define different ranking scenarios\n",
    "    scenarios = {\n",
    "        'Perfect_Ranking': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.9, 0.8, 0.7, 0.6, 0.1],\n",
    "            'description': 'Perfect ranking: highly relevant docs first'\n",
    "        },\n",
    "        'Worst_Ranking': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.1, 0.2, 0.3, 0.4, 0.9],\n",
    "            'description': 'Worst ranking: irrelevant docs first'\n",
    "        },\n",
    "        'Mixed_Ranking': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.7, 0.3, 0.9, 0.1, 0.5],\n",
    "            'description': 'Mixed ranking: some relevant docs early'\n",
    "        },\n",
    "        'Late_Success': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.1, 0.2, 0.3, 0.9, 0.8],\n",
    "            'description': 'Late success: relevant docs at end'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"üîÑ Comparative Analysis of Ranking Scenarios\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for scenario_name, scenario_data in scenarios.items():\n",
    "        print(f\"\\nüìä Scenario: {scenario_name}\")\n",
    "        print(f\"Description: {scenario_data['description']}\")\n",
    "        print(f\"Relevance: {scenario_data['relevance']}\")\n",
    "        print(f\"Predictions: {scenario_data['predictions']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        ndcg_result = ndcg_calc.calculate_ndcg(scenario_data['relevance'], k=3, verbose=False)\n",
    "        mrr_result = mrr_calc.calculate_mrr_single_query(scenario_data['relevance'], scenario_data['predictions'], verbose=False)\n",
    "        map_result = map_calc.calculate_ap_single_query(scenario_data['relevance'], scenario_data['predictions'], k=3, verbose=False)\n",
    "        \n",
    "        results.append({\n",
    "            'Scenario': scenario_name,\n",
    "            'nDCG@3': ndcg_result['ndcg'],\n",
    "            'MRR': mrr_result['reciprocal_rank'],\n",
    "            'mAP@3': map_result['ap'],\n",
    "            'Description': scenario_data['description']\n",
    "        })\n",
    "        \n",
    "        print(f\"nDCG@3: {ndcg_result['ndcg']:.4f}\")\n",
    "        print(f\"MRR:    {mrr_result['reciprocal_rank']:.4f}\")\n",
    "        print(f\"mAP@3:  {map_result['ap']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run comparative analysis\n",
    "comparison_df = compare_ranking_scenarios()\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metrics = ['nDCG@3', 'MRR', 'mAP@3']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    bars = ax.bar(comparison_df['Scenario'], comparison_df[metric], color=colors[i], alpha=0.8)\n",
    "    ax.set_title(f'{metric} Across Scenarios', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, comparison_df[metric]):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_comparison_scenarios.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Summary Table:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Metric Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_metric_calculator():\n",
    "    \"\"\"Interactive tool to understand how metrics respond to different inputs\"\"\"\n",
    "    \n",
    "    print(\"üéÆ Interactive Metric Calculator\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Experiment with different relevance and prediction patterns!\")\n",
    "    print(\"\\nRelevance scale: 0=Irrelevant, 1=Somewhat Relevant, 2=Very Relevant\")\n",
    "    \n",
    "    # Pre-defined experiments\n",
    "    experiments = [\n",
    "        {\n",
    "            'name': 'Early Success',\n",
    "            'relevance': [2, 1, 0, 0, 1],\n",
    "            'predictions': [0.9, 0.8, 0.3, 0.2, 0.7]\n",
    "        },\n",
    "        {\n",
    "            'name': 'Late Discovery', \n",
    "            'relevance': [0, 0, 1, 2, 1],\n",
    "            'predictions': [0.2, 0.3, 0.9, 0.8, 0.7]\n",
    "        },\n",
    "        {\n",
    "            'name': 'Mixed Results',\n",
    "            'relevance': [1, 0, 2, 1, 0],\n",
    "            'predictions': [0.6, 0.8, 0.4, 0.7, 0.5]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for exp in experiments:\n",
    "        print(f\"\\nüß™ Experiment: {exp['name']}\")\n",
    "        print(f\"Relevance:   {exp['relevance']}\")\n",
    "        print(f\"Predictions: {exp['predictions']}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ndcg_res = ndcg_calc.calculate_ndcg(exp['relevance'], k=3, verbose=False)\n",
    "        mrr_res = mrr_calc.calculate_mrr_single_query(exp['relevance'], exp['predictions'], verbose=False)\n",
    "        map_res = map_calc.calculate_ap_single_query(exp['relevance'], exp['predictions'], k=3, verbose=False)\n",
    "        \n",
    "        results_summary.append({\n",
    "            'Experiment': exp['name'],\n",
    "            'nDCG@3': ndcg_res['ndcg'],\n",
    "            'MRR': mrr_res['reciprocal_rank'],\n",
    "            'mAP@3': map_res['ap']\n",
    "        })\n",
    "        \n",
    "        print(f\"üìä Results: nDCG@3={ndcg_res['ndcg']:.3f}, MRR={mrr_res['reciprocal_rank']:.3f}, mAP@3={map_res['ap']:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(results_summary)\n",
    "\n",
    "interactive_results = interactive_metric_calculator()\n",
    "\n",
    "# Create radar chart for metric comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "metrics = ['nDCG@3', 'MRR', 'mAP@3']\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, (_, row) in enumerate(interactive_results.iterrows()):\n",
    "    values = [row['nDCG@3'], row['MRR'], row['mAP@3']]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Experiment'], color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Metric Performance Comparison\\n(Radar Chart)', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interactive Results Summary:\")\n",
    "print(interactive_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Insights and Learning Summary\n",
    "\n",
    "### üîç What We've Learned\n",
    "\n",
    "1. **nDCG (Normalized Discounted Cumulative Gain)**:\n",
    "   - Emphasizes **graded relevance** and **position importance**\n",
    "   - Higher scores for relevant documents at top positions\n",
    "   - Normalizes by ideal ranking for fair comparison\n",
    "   - Best for scenarios with multiple relevance levels\n",
    "\n",
    "2. **MRR (Mean Reciprocal Rank)**:\n",
    "   - Focuses on **first highly relevant result**\n",
    "   - Critical for applications where users need quick answers\n",
    "   - Sensitive to top-ranked results\n",
    "   - Ideal for question-answering systems\n",
    "\n",
    "3. **mAP (Mean Average Precision)**:\n",
    "   - Considers **all relevant documents**\n",
    "   - Balances precision across different cutoff points\n",
    "   - Good for comprehensive retrieval evaluation\n",
    "   - Useful when recall matters as much as precision\n",
    "\n",
    "### üéØ Why These Metrics Matter for Arabic Semantic Search\n",
    "\n",
    "As highlighted in the paper, Arabic language presents unique challenges:\n",
    "- **Complex morphology**: Same root can have many forms\n",
    "- **Dialectal variations**: Different regions use different expressions\n",
    "- **Limited resources**: Fewer labeled datasets available\n",
    "\n",
    "These metrics help evaluate how well semantic search systems handle these challenges by providing nuanced performance measures that go beyond simple accuracy.\n",
    "\n",
    "### üöÄ Practical Applications\n",
    "\n",
    "- **Customer Support**: MRR critical for quick problem resolution\n",
    "- **Document Search**: mAP important for comprehensive retrieval\n",
    "- **Question Answering**: nDCG valuable for ranked answer lists\n",
    "- **RAG Systems**: All three metrics crucial for retrieval quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Metric sensitivity analysis\n",
    "position_effect = []\n",
    "for pos in range(1, 6):\n",
    "    # Move a very relevant document to different positions\n",
    "    relevance = [0] * 5\n",
    "    relevance[pos-1] = 2\n",
    "    predictions = [0.1] * 5\n",
    "    predictions[pos-1] = 0.9\n",
    "    \n",
    "    ndcg_res = ndcg_calc.calculate_ndcg(relevance, k=5, verbose=False)\n",
    "    mrr_res = mrr_calc.calculate_mrr_single_query(relevance, predictions, verbose=False)\n",
    "    map_res = map_calc.calculate_ap_single_query(relevance, predictions, k=5, verbose=False)\n",
    "    \n",
    "    position_effect.append({\n",
    "        'Position': pos,\n",
    "        'nDCG@5': ndcg_res['ndcg'],\n",
    "        'MRR': mrr_res['reciprocal_rank'],\n",
    "        'mAP@5': map_res['ap']\n",
    "    })\n",
    "\n",
    "pos_df = pd.DataFrame(position_effect)\n",
    "\n",
    "ax1.plot(pos_df['Position'], pos_df['nDCG@5'], 'o-', label='nDCG@5', linewidth=2, markersize=8)\n",
    "ax1.plot(pos_df['Position'], pos_df['MRR'], 's-', label='MRR', linewidth=2, markersize=8)\n",
    "ax1.plot(pos_df['Position'], pos_df['mAP@5'], '^-', label='mAP@5', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Position of Relevant Document')\n",
    "ax1.set_ylabel('Metric Score')\n",
    "ax1.set_title('Position Sensitivity Analysis')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Relevance level impact\n",
    "relevance_levels = [0, 1, 2]\n",
    "relevance_impact = []\n",
    "\n",
    "for rel_level in relevance_levels:\n",
    "    relevance = [rel_level, 0, 0, 0, 0]\n",
    "    predictions = [0.9, 0.1, 0.1, 0.1, 0.1]\n",
    "    \n",
    "    ndcg_res = ndcg_calc.calculate_ndcg(relevance, k=3, verbose=False)\n",
    "    mrr_res = mrr_calc.calculate_mrr_single_query(relevance, predictions, verbose=False)\n",
    "    map_res = map_calc.calculate_ap_single_query(relevance, predictions, k=3, verbose=False)\n",
    "    \n",
    "    relevance_impact.append({\n",
    "        'Relevance Level': rel_level,\n",
    "        'nDCG@3': ndcg_res['ndcg'],\n",
    "        'MRR': mrr_res['reciprocal_rank'],\n",
    "        'mAP@3': map_res['ap']\n",
    "    })\n",
    "\n",
    "rel_df = pd.DataFrame(relevance_impact)\n",
    "\n",
    "width = 0.25\n",
    "x = np.arange(len(relevance_levels))\n",
    "ax2.bar(x - width, rel_df['nDCG@3'], width, label='nDCG@3', alpha=0.8)\n",
    "ax2.bar(x, rel_df['MRR'], width, label='MRR', alpha=0.8)\n",
    "ax2.bar(x + width, rel_df['mAP@3'], width, label='mAP@3', alpha=0.8)\n",
    "ax2.set_xlabel('Relevance Level')\n",
    "ax2.set_ylabel('Metric Score')\n",
    "ax2.set_title('Impact of Relevance Levels')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['Irrelevant (0)', 'Somewhat (1)', 'Very (2)'])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cutoff sensitivity (k parameter)\n",
    "k_values = range(1, 6)\n",
    "relevance = [2, 1, 0, 1, 2]\n",
    "predictions = [0.9, 0.7, 0.3, 0.6, 0.8]\n",
    "\n",
    "k_sensitivity = []\n",
    "for k in k_values:\n",
    "    ndcg_res = ndcg_calc.calculate_ndcg(relevance, k=k, verbose=False)\n",
    "    map_res = map_calc.calculate_ap_single_query(relevance, predictions, k=k, verbose=False)\n",
    "    \n",
    "    k_sensitivity.append({\n",
    "        'k': k,\n",
    "        'nDCG@k': ndcg_res['ndcg'],\n",
    "        'mAP@k': map_res['ap']\n",
    "    })\n",
    "\n",
    "k_df = pd.DataFrame(k_sensitivity)\n",
    "\n",
    "ax3.plot(k_df['k'], k_df['nDCG@k'], 'o-', label='nDCG@k', linewidth=2, markersize=8)\n",
    "ax3.plot(k_df['k'], k_df['mAP@k'], 's-', label='mAP@k', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Cutoff k')\n",
    "ax3.set_ylabel('Metric Score')\n",
    "ax3.set_title('Cutoff Sensitivity Analysis')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Summary insights\n",
    "ax4.axis('off')\n",
    "insights_text = \"\"\"\n",
    "üìä KEY INSIGHTS FROM ANALYSIS\n",
    "\n",
    "üéØ Position Matters:\n",
    "‚Ä¢ MRR most sensitive to top positions\n",
    "‚Ä¢ nDCG gradually decreases with position\n",
    "‚Ä¢ mAP considers all relevant documents\n",
    "\n",
    "üìà Relevance Levels:\n",
    "‚Ä¢ nDCG best captures graded relevance\n",
    "‚Ä¢ MRR binary: relevant vs irrelevant\n",
    "‚Ä¢ mAP treats all relevant docs equally\n",
    "\n",
    "‚öôÔ∏è Cutoff Effects:\n",
    "‚Ä¢ Lower k values favor precision\n",
    "‚Ä¢ Higher k values include more context\n",
    "‚Ä¢ Choice depends on application needs\n",
    "\n",
    "üîç For Arabic Semantic Search:\n",
    "‚Ä¢ Use all three metrics together\n",
    "‚Ä¢ nDCG for ranking quality\n",
    "‚Ä¢ MRR for user experience\n",
    "‚Ä¢ mAP for comprehensive evaluation\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, insights_text, transform=ax4.transAxes, fontsize=11, \n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comprehensive_metrics_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì EVALUATION METRICS MASTERY COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "‚úÖ What you've mastered:\n",
    "‚Ä¢ Deep understanding of nDCG, MRR, and mAP calculations\n",
    "‚Ä¢ Implementation of metrics from mathematical foundations\n",
    "‚Ä¢ Analysis of metric behaviors in different scenarios\n",
    "‚Ä¢ Practical insights for Arabic semantic search evaluation\n",
    "‚Ä¢ Interactive exploration of metric sensitivities\n",
    "\n",
    "üöÄ Ready to apply these metrics to real-world semantic search systems!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Papers (PDF Utils)",
   "language": "python",
   "name": "ai-papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}