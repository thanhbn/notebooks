{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kotlin ML Pack: Technical Report - Main Implementation\n",
    "\n",
    "## Paper Information\n",
    "- **Title**: Kotlin ML Pack: Technical Report\n",
    "- **Authors**: Sergey Titov, Mikhail Evtikhiev, Anton Shapkin, et al. (JetBrains Research)\n",
    "- **Link**: [arXiv:2405.19250v1](https://arxiv.org/abs/2405.19250)\n",
    "- **Date**: May 29, 2024\n",
    "\n",
    "## Abstract Summary\n",
    "This paper presents three novel datasets for Kotlin code generation:\n",
    "1. **KStack**: The largest collection of permissively licensed Kotlin files (4M files, 3.1B tokens)\n",
    "2. **KStack-clean**: A highly filtered version with 25,000 high-quality examples\n",
    "3. **KExercises**: Translated and improved version of Python exercises dataset\n",
    "\n",
    "The authors fine-tuned CodeLlama and DeepSeek models achieving up to 16-point increase in pass rate on HumanEval benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "### Why LangChain/LangGraph?\n",
    "- **LangChain** provides excellent abstractions for working with LLMs, embeddings, and document processing\n",
    "- **LangGraph** can orchestrate the complex multi-stage pipeline of dataset filtering and model evaluation\n",
    "- The paper's dataset creation involves LLM-based classification which maps perfectly to LangChain's capabilities\n",
    "\n",
    "### Why DeepEval?\n",
    "- The paper evaluates models on HumanEval benchmark - DeepEval provides built-in support for code evaluation metrics\n",
    "- Metrics like pass rate, syntax error rate map directly to DeepEval's correctness and syntax metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-community langgraph\n",
    "!pip install deepeval transformers datasets torch\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "# DeepEval imports  \n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Set up visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Implementation\n",
    "\n",
    "### 2.1 KStack Dataset Structure\n",
    "Based on Section III.A of the paper, KStack is collected from:\n",
    "- GitHub repositories where main language is Kotlin\n",
    "- Repositories with 10+ stars containing Kotlin files\n",
    "- Stack v1.2 repositories with Kotlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KotlinFile:\n",
    "    \"\"\"Represents a Kotlin file from the dataset\"\"\"\n",
    "    content: str\n",
    "    repo_stars: int\n",
    "    file_path: str\n",
    "    extension: str  # .kt, .kts, .gradle.kts\n",
    "    license: str\n",
    "    \n",
    "@dataclass\n",
    "class DatasetStats:\n",
    "    \"\"\"Statistics from Table I in the paper\"\"\"\n",
    "    name: str\n",
    "    files: int\n",
    "    repositories: int\n",
    "    lines: int\n",
    "    tokens: int\n",
    "\n",
    "# Dataset statistics from the paper\n",
    "dataset_stats = {\n",
    "    \"The Stack v2\": DatasetStats(\"The Stack v2\", 2_000_000, 109_547, 162_000_000, 1_700_000_000),\n",
    "    \"KStack\": DatasetStats(\"KStack\", 4_000_000, 163_310, 293_000_000, 3_100_000_000),\n",
    "    \"KStack-clean\": DatasetStats(\"KStack-clean\", 25_000, 3_366, 2_000_000, 22_000_000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 KStack-clean Quality Filtering with LangChain\n",
    "\n",
    "Section III.B describes using LLM classifiers for quality filtering. We'll implement this using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KotlinQualityClassifier:\n",
    "    \"\"\"Implements the pairwise quality classification from Section III.B\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\"):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "        \n",
    "        # Prompt for pairwise comparison as described in the paper\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"You are evaluating Kotlin code quality.\"),\n",
    "            HumanMessage(content=\"\"\"Compare these two Kotlin code files and determine which has \n",
    "greater educational value for learning algorithms in Kotlin.\n",
    "\n",
    "File A:\n",
    "{file_a}\n",
    "\n",
    "File B:\n",
    "{file_b}\n",
    "\n",
    "Which file (A or B) has higher educational value? Respond with only 'A' or 'B'.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "    \n",
    "    def compare_files(self, file_a: str, file_b: str) -> str:\n",
    "        \"\"\"Compare two files and return which is better\"\"\"\n",
    "        result = self.chain.run(file_a=file_a, file_b=file_b)\n",
    "        return result.strip()\n",
    "    \n",
    "    def calculate_score(self, file: str, comparison_file: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate quality score using the formula from the paper:\n",
    "        s(f) = (s(f,c)_A - s(f,c)_B + s(c,f)_B - s(c,f)_A) / 2\n",
    "        \"\"\"\n",
    "        # First comparison: file vs comparison_file\n",
    "        result1 = self.compare_files(file, comparison_file)\n",
    "        score1 = 1.0 if result1 == 'A' else 0.0\n",
    "        \n",
    "        # Second comparison: comparison_file vs file (reversed)\n",
    "        result2 = self.compare_files(comparison_file, file)\n",
    "        score2 = 1.0 if result2 == 'B' else 0.0\n",
    "        \n",
    "        # Average as per the paper's formula\n",
    "        return (score1 + score2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 KExercises Dataset Translation Pipeline\n",
    "\n",
    "Section III.D describes translating Python exercises to Kotlin using GPT-3.5-turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonToKotlinTranslator:\n",
    "    \"\"\"Translates Python code exercises to Kotlin as per Section III.D\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        \n",
    "        # Translation prompt from Figure 2 in the paper\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "            HumanMessage(content=\"\"\"Rewrite to Kotlin (do not forget about docstring):\n",
    "\n",
    "{python_code}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "    \n",
    "    def translate(self, python_code: str) -> str:\n",
    "        \"\"\"Translate Python code to Kotlin\"\"\"\n",
    "        return self.chain.run(python_code=python_code)\n",
    "\n",
    "# Example translation\n",
    "translator = PythonToKotlinTranslator()\n",
    "python_example = \"\"\"\n",
    "def fibonacci(n: int) -> int:\n",
    "    '''Calculate the nth Fibonacci number'''\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "# Note: In real implementation, you would translate the actual dataset\n",
    "# kotlin_translation = translator.translate(python_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Fine-tuning Implementation\n",
    "\n",
    "### 3.1 Training Configuration\n",
    "Based on Section V.C, the paper uses specific training techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "\n",
    "class ZLoss(nn.Module):\n",
    "    \"\"\"Z-loss implementation from Section V.C\n",
    "    \n",
    "    The z-loss term is log^2(Z) where Z = sum(exp(y_j))\n",
    "    This prevents logit divergence during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, logits):\n",
    "        # Calculate Z = sum(exp(logits))\n",
    "        # Use logsumexp for numerical stability\n",
    "        log_z = torch.logsumexp(logits, dim=-1)\n",
    "        # Z-loss is log^2(Z)\n",
    "        z_loss = log_z ** 2\n",
    "        return z_loss.mean()\n",
    "\n",
    "class KotlinModelTrainer:\n",
    "    \"\"\"Implements training setup from Section V.C\"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=1e-4, weight_decay=0.1):\n",
    "        self.model = model\n",
    "        self.z_loss = ZLoss()\n",
    "        \n",
    "        # AdamW with decreased epsilon as per paper\n",
    "        self.optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            eps=1e-16  # Decreased from default 1e-8\n",
    "        )\n",
    "        \n",
    "        # Gradient clipping value\n",
    "        self.max_grad_norm = 1.0\n",
    "        \n",
    "    def compute_loss(self, logits, labels, z_loss_weight=0.01):\n",
    "        \"\"\"Compute combined cross-entropy and z-loss\"\"\"\n",
    "        # Standard cross-entropy loss\n",
    "        ce_loss = nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        # Add z-loss for stability\n",
    "        z_loss = self.z_loss(logits)\n",
    "        \n",
    "        total_loss = ce_loss + z_loss_weight * z_loss\n",
    "        return total_loss, ce_loss, z_loss\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        \"\"\"Single training step with gradient clipping\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(**batch)\n",
    "        loss, ce_loss, z_loss = self.compute_loss(outputs.logits, batch['labels'])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'ce_loss': ce_loss.item(),\n",
    "            'z_loss': z_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Configuration\n",
    "The paper uses CodeLlama-7B and DeepSeek-coder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations from the paper\n",
    "MODEL_CONFIGS = {\n",
    "    \"CodeLlama-7B\": {\n",
    "        \"model_name\": \"codellama/CodeLlama-7b-hf\",\n",
    "        \"context_length\": 16384,\n",
    "        \"supports_fim\": True  # Fill-in-the-middle\n",
    "    },\n",
    "    \"Deepseek-coder-6.7B\": {\n",
    "        \"model_name\": \"deepseek-ai/deepseek-coder-6.7b-base\",\n",
    "        \"context_length\": 16384,\n",
    "        \"supports_fim\": True\n",
    "    },\n",
    "    \"Deepseek-coder-1.3B\": {\n",
    "        \"model_name\": \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
    "        \"context_length\": 16384,\n",
    "        \"supports_fim\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_config: dict):\n",
    "    \"\"\"Load model and tokenizer with proper configuration\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_config[\"model_name\"],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_config[\"model_name\"],\n",
    "        torch_dtype=torch.bfloat16,  # BF16 precision as per paper\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation with DeepEval\n",
    "\n",
    "### 4.1 HumanEval for Kotlin\n",
    "Section IV describes the improved HumanEval benchmark for Kotlin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class KotlinCodeMetric(BaseMetric):\n",
    "    \"\"\"Custom metric for evaluating Kotlin code generation\n",
    "    \n",
    "    Implements the metrics from Section IV.B:\n",
    "    - Pass@1\n",
    "    - Compilation Error Rate\n",
    "    - Test Error Rate\n",
    "    - Runtime Error Rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.threshold = 1.0  # Binary pass/fail\n",
    "        \n",
    "    def measure(self, test_case: LLMTestCase):\n",
    "        # Extract generated code\n",
    "        generated_code = test_case.actual_output\n",
    "        expected_tests = test_case.expected_output\n",
    "        \n",
    "        # Create temporary Kotlin file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.kt', delete=False) as f:\n",
    "            # Combine generated code with test harness\n",
    "            full_code = f\"\"\"\n",
    "{generated_code}\n",
    "\n",
    "fun main() {{\n",
    "    // Test harness\n",
    "    {expected_tests}\n",
    "}}\n",
    "\"\"\"\n",
    "            f.write(full_code)\n",
    "            kotlin_file = f.name\n",
    "        \n",
    "        try:\n",
    "            # Compile Kotlin code\n",
    "            compile_result = subprocess.run(\n",
    "                ['kotlinc', kotlin_file, '-d', tempfile.gettempdir()],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=15\n",
    "            )\n",
    "            \n",
    "            if compile_result.returncode != 0:\n",
    "                self.score = 0\n",
    "                self.reason = f\"Compilation error: {compile_result.stderr}\"\n",
    "                self.error_type = \"compilation_error\"\n",
    "                return\n",
    "            \n",
    "            # Run compiled code\n",
    "            run_result = subprocess.run(\n",
    "                ['kotlin', '-cp', tempfile.gettempdir(), 'MainKt'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=15\n",
    "            )\n",
    "            \n",
    "            if run_result.returncode != 0:\n",
    "                self.score = 0\n",
    "                self.reason = f\"Runtime error: {run_result.stderr}\"\n",
    "                self.error_type = \"runtime_error\"\n",
    "            else:\n",
    "                self.score = 1\n",
    "                self.reason = \"All tests passed\"\n",
    "                self.error_type = None\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            self.score = 0\n",
    "            self.reason = \"Execution timeout (>15 seconds)\"\n",
    "            self.error_type = \"timeout_error\"\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if os.path.exists(kotlin_file):\n",
    "                os.remove(kotlin_file)\n",
    "    \n",
    "    def is_successful(self):\n",
    "        return self.score >= self.threshold\n",
    "    \n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return \"Kotlin Code Evaluation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation Pipeline with LangGraph\n",
    "\n",
    "We'll use LangGraph to orchestrate the evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class EvaluationState(TypedDict):\n",
    "    \"\"\"State for the evaluation pipeline\"\"\"\n",
    "    model_name: str\n",
    "    prompts: List[str]\n",
    "    generated_codes: List[str]\n",
    "    test_results: List[dict]\n",
    "    metrics: dict\n",
    "\n",
    "def generate_code(state: EvaluationState) -> EvaluationState:\n",
    "    \"\"\"Generate Kotlin code from prompts\"\"\"\n",
    "    # Load model based on state.model_name\n",
    "    # Generate code for each prompt\n",
    "    # This is a placeholder - in real implementation, use actual model\n",
    "    state['generated_codes'] = [\n",
    "        f\"fun solution() {{ /* Generated code for {prompt} */ }}\" \n",
    "        for prompt in state['prompts']\n",
    "    ]\n",
    "    return state\n",
    "\n",
    "def evaluate_code(state: EvaluationState) -> EvaluationState:\n",
    "    \"\"\"Evaluate generated code using DeepEval\"\"\"\n",
    "    metric = KotlinCodeMetric()\n",
    "    results = []\n",
    "    \n",
    "    for code in state['generated_codes']:\n",
    "        test_case = LLMTestCase(\n",
    "            input=\"Generate Kotlin function\",\n",
    "            actual_output=code,\n",
    "            expected_output=\"assert(solution() == expected)\"\n",
    "        )\n",
    "        metric.measure(test_case)\n",
    "        results.append({\n",
    "            'passed': metric.is_successful(),\n",
    "            'error_type': getattr(metric, 'error_type', None),\n",
    "            'reason': metric.reason\n",
    "        })\n",
    "    \n",
    "    state['test_results'] = results\n",
    "    return state\n",
    "\n",
    "def calculate_metrics(state: EvaluationState) -> EvaluationState:\n",
    "    \"\"\"Calculate final metrics as per Section IV.B\"\"\"\n",
    "    results = state['test_results']\n",
    "    total = len(results)\n",
    "    \n",
    "    metrics = {\n",
    "        'pass_rate': sum(r['passed'] for r in results) / total * 100,\n",
    "        'compilation_error_rate': sum(1 for r in results if r['error_type'] == 'compilation_error') / total * 100,\n",
    "        'runtime_error_rate': sum(1 for r in results if r['error_type'] == 'runtime_error') / total * 100,\n",
    "        'timeout_error_rate': sum(1 for r in results if r['error_type'] == 'timeout_error') / total * 100\n",
    "    }\n",
    "    \n",
    "    # Syntax error rate = compilation + runtime errors\n",
    "    metrics['syntax_error_rate'] = metrics['compilation_error_rate'] + metrics['runtime_error_rate']\n",
    "    \n",
    "    state['metrics'] = metrics\n",
    "    return state\n",
    "\n",
    "# Build the evaluation graph\n",
    "evaluation_graph = StateGraph(EvaluationState)\n",
    "\n",
    "# Add nodes\n",
    "evaluation_graph.add_node(\"generate\", generate_code)\n",
    "evaluation_graph.add_node(\"evaluate\", evaluate_code)\n",
    "evaluation_graph.add_node(\"calculate_metrics\", calculate_metrics)\n",
    "\n",
    "# Add edges\n",
    "evaluation_graph.add_edge(\"generate\", \"evaluate\")\n",
    "evaluation_graph.add_edge(\"evaluate\", \"calculate_metrics\")\n",
    "evaluation_graph.add_edge(\"calculate_metrics\", END)\n",
    "\n",
    "# Set entry point\n",
    "evaluation_graph.set_entry_point(\"generate\")\n",
    "\n",
    "# Compile the graph\n",
    "evaluation_pipeline = evaluation_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Visualization\n",
    "\n",
    "### 5.1 Reproducing Figure 1: Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from Table II in the paper\n",
    "results_data = {\n",
    "    'Model': ['CodeLlama-7B Base', 'CodeLlama-7B KStack', 'CodeLlama-7B KStack-clean', 'CodeLlama-7B KExercises',\n",
    "              'Deepseek-7B Base', 'Deepseek-7B KExercises',\n",
    "              'Deepseek-1.3B Base', 'Deepseek-1.3B KStack', 'Deepseek-1.3B KExercises'],\n",
    "    'Pass Rate': [26.09, 29.19, 37.89, 42.24, 40.99, 55.28, 26.71, 27.95, 36.65],\n",
    "    'Syntax Error Rate': [22.98, 22.98, 18.64, 19.25, 21.12, 15.53, 19.26, 19.88, 18.63],\n",
    "    'Completion Rate': [0.388, 0.396, 0.403, 0.344, 0.403, 0.411, 0.403, 0.404, 0.388]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Pass Rate\n",
    "ax1.bar(range(len(results_df)), results_df['Pass Rate'], color='green', alpha=0.7)\n",
    "ax1.set_xlabel('Model Configuration')\n",
    "ax1.set_ylabel('Pass Rate (%)')\n",
    "ax1.set_title('Pass Rate on Kotlin HumanEval')\n",
    "ax1.set_xticks(range(len(results_df)))\n",
    "ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "\n",
    "# Syntax Error Rate\n",
    "ax2.bar(range(len(results_df)), results_df['Syntax Error Rate'], color='red', alpha=0.7)\n",
    "ax2.set_xlabel('Model Configuration')\n",
    "ax2.set_ylabel('Syntax Error Rate (%)')\n",
    "ax2.set_title('Syntax Error Rate')\n",
    "ax2.set_xticks(range(len(results_df)))\n",
    "ax2.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "\n",
    "# Completion Rate\n",
    "ax3.bar(range(len(results_df)), results_df['Completion Rate'], color='blue', alpha=0.7)\n",
    "ax3.set_xlabel('Model Configuration')\n",
    "ax3.set_ylabel('Completion Rate')\n",
    "ax3.set_title('Code Completion (Exact Match)')\n",
    "ax3.set_xticks(range(len(results_df)))\n",
    "ax3.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Progress Visualization (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating training progress data from Figure 3\n",
    "optimization_steps = np.linspace(0, 1400, 50)\n",
    "\n",
    "# Approximate curves from Figure 3\n",
    "openai_classifier = 26 + 10 * (1 - np.exp(-optimization_steps / 400)) + np.random.normal(0, 0.5, 50)\n",
    "mistral_classifier = 26 + 14 * (1 - np.exp(-optimization_steps / 300)) + np.random.normal(0, 0.3, 50)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(optimization_steps, openai_classifier, label='OpenAI GPT-3.5-based classifier', color='blue')\n",
    "plt.plot(optimization_steps, mistral_classifier, label='Mistral-based classifier', color='orange')\n",
    "plt.xlabel('Optimization step')\n",
    "plt.ylabel('Pass rate')\n",
    "plt.title('Pass rate on HumanEval for Kotlin - Different Filtration Strategies')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Template for Personal Research\n",
    "\n",
    "This section provides a template for extending the Kotlin ML Pack research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchTemplate:\n",
    "    \"\"\"Template for extending Kotlin ML Pack research\"\"\"\n",
    "    \n",
    "    def __init__(self, research_area: str):\n",
    "        self.research_area = research_area\n",
    "        self.results = []\n",
    "        \n",
    "    def experiment_1_custom_filtering(self):\n",
    "        \"\"\"Experiment with different quality filtering approaches\"\"\"\n",
    "        # TODO: Implement custom filtering logic\n",
    "        # Ideas from Section VII:\n",
    "        # - Use compiler feedback for filtering\n",
    "        # - Use IDE inspections for quality assessment\n",
    "        pass\n",
    "    \n",
    "    def experiment_2_synthetic_data(self):\n",
    "        \"\"\"Generate synthetic Kotlin data for specific domains\"\"\"\n",
    "        # TODO: Implement synthetic data generation\n",
    "        # Focus on production-oriented tasks as mentioned in Section VII\n",
    "        pass\n",
    "    \n",
    "    def experiment_3_new_benchmarks(self):\n",
    "        \"\"\"Create issue-based benchmarks for Kotlin\"\"\"\n",
    "        # TODO: Create SWE-Bench style dataset for Kotlin\n",
    "        # Use real-world Kotlin projects and issues\n",
    "        pass\n",
    "    \n",
    "    def run_experiments(self):\n",
    "        \"\"\"Run all experiments and collect results\"\"\"\n",
    "        print(f\"Running experiments for: {self.research_area}\")\n",
    "        # Run your experiments here\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "my_research = ResearchTemplate(\"Kotlin Android Development\")\n",
    "# my_research.run_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Key Insights\n",
    "\n",
    "### Key Findings from the Paper:\n",
    "1. **Quality over Quantity**: KStack-clean (25K examples) outperforms KStack (4M files) - showing that careful curation matters\n",
    "2. **Synthetic Data Works**: KExercises achieves best results (55.28% pass rate) despite being synthetic\n",
    "3. **Training Stability Matters**: Z-loss and other techniques prevent training instabilities\n",
    "4. **Evaluation is Complex**: Pass rate alone isn't sufficient - syntax error rate and completion metrics provide fuller picture\n",
    "\n",
    "### Implementation with LangChain/DeepEval:\n",
    "- **LangChain** excellently handles the LLM-based classification pipeline for dataset curation\n",
    "- **LangGraph** provides clean orchestration for complex evaluation pipelines\n",
    "- **DeepEval** metrics map directly to the paper's evaluation criteria\n",
    "\n",
    "### Future Research Directions (Section VII):\n",
    "1. **Learning from Tools**: Integrate Kotlin compiler and IDE inspections\n",
    "2. **Synthetic Data**: Generate production-oriented Kotlin tasks\n",
    "3. **Better Benchmarks**: Create SWE-Bench style evaluations for Kotlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Kotlin ML Pack Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for name, stats in dataset_stats.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Files: {stats.files:,}\")\n",
    "    print(f\"  Repositories: {stats.repositories:,}\")\n",
    "    print(f\"  Lines: {stats.lines:,}\")\n",
    "    print(f\"  Tokens: {stats.tokens:,}\")\n",
    "\n",
    "print(\"\\nBest Results:\")\n",
    "print(f\"  Model: Deepseek-7B + KExercises\")\n",
    "print(f\"  Pass Rate: 55.28% (+14.29% improvement)\")\n",
    "print(f\"  Syntax Error Rate: 15.53% (-5.59% improvement)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}