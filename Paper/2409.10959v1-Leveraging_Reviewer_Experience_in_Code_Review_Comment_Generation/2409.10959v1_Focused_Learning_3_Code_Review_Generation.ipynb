{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Code Review Comment Generation with Transformers\n",
    "\n",
    "## Learning Objective\n",
    "Master the technical implementation of neural code review comment generation, understanding how T5-based models are adapted for the code review domain and how to evaluate generated comments across multiple dimensions.\n",
    "\n",
    "## Paper Reference\n",
    "- **Section 2.2**: Review Comment Generation (Pages 4-5)\n",
    "- **Section 4.4-4.5**: Evaluation Metrics and Manual Evaluation Process (Pages 12-16)\n",
    "- **Table 2**: Code Review Comment Categories\n",
    "- **Figure 3-4**: Examples of comment evaluation\n",
    "\n",
    "## Why This Topic is Complex\n",
    "1. **Multimodal Input**: Code changes + context require specialized encoding\n",
    "2. **Diverse Output Space**: Comments range from syntax issues to design discussions\n",
    "3. **Evaluation Challenges**: No single metric captures comment quality\n",
    "4. **Domain Adaptation**: General LLMs struggle without code review specific training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Code Review Generation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Problem Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CodeReviewExample:\n",
    "    \"\"\"Represents a single code review training example\"\"\"\n",
    "    # Input\n",
    "    code_change_before: str  # Code before modification (H_pre)\n",
    "    code_change_after: str   # Code after modification (H_post)\n",
    "    \n",
    "    # Output\n",
    "    review_comment: str      # Natural language review (R_nl)\n",
    "    \n",
    "    # Metadata\n",
    "    reviewer_id: str\n",
    "    repository: str\n",
    "    file_path: str\n",
    "    programming_language: str\n",
    "    \n",
    "    def to_model_input(self, include_context: bool = True) -> str:\n",
    "        \"\"\"Format for model input following CodeReviewer approach\"\"\"\n",
    "        if include_context:\n",
    "            # Include file context\n",
    "            return f\"Review the change in {self.file_path}:\\n<OLD>\\n{self.code_change_before}\\n</OLD>\\n<NEW>\\n{self.code_change_after}\\n</NEW>\"\n",
    "        else:\n",
    "            # Simple diff format\n",
    "            return f\"<OLD>\\n{self.code_change_before}\\n</OLD>\\n<NEW>\\n{self.code_change_after}\\n</NEW>\"\n",
    "\n",
    "# Generate example code review scenarios\n",
    "example_reviews = [\n",
    "    CodeReviewExample(\n",
    "        code_change_before=\"if (user.role == 'admin') {\\n    processRequest(request);\\n}\",\n",
    "        code_change_after=\"if (user.role == 'admin') {\\n    processAdminRequest(request);\\n}\",\n",
    "        review_comment=\"Add validation check: if (!validateRequest(request)) return; before processing.\",\n",
    "        reviewer_id=\"exp_reviewer_1\",\n",
    "        repository=\"web-app\",\n",
    "        file_path=\"src/auth/handler.js\",\n",
    "        programming_language=\"javascript\"\n",
    "    ),\n",
    "    CodeReviewExample(\n",
    "        code_change_before=\"conn = database.connect()\\ndata = conn.query(sql)\",\n",
    "        code_change_after=\"conn = database.connect()\\ndata = conn.query(sql)\\nresults = process(data)\",\n",
    "        review_comment=\"Resource leak: connection is never closed. Use try-finally or context manager.\",\n",
    "        reviewer_id=\"exp_reviewer_2\",\n",
    "        repository=\"data-service\",\n",
    "        file_path=\"src/db/query.py\",\n",
    "        programming_language=\"python\"\n",
    "    ),\n",
    "    CodeReviewExample(\n",
    "        code_change_before=\"for i in range(len(items)):\\n    total += items[i].price\",\n",
    "        code_change_after=\"for item in items:\\n    total += item.price\",\n",
    "        review_comment=\"Good refactoring! This is more Pythonic and readable.\",\n",
    "        reviewer_id=\"mid_reviewer_1\",\n",
    "        repository=\"e-commerce\",\n",
    "        file_path=\"src/cart/calculator.py\",\n",
    "        programming_language=\"python\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Visualize the task\n",
    "print(\"Code Review Comment Generation Task:\")\n",
    "print(\"====================================\\n\")\n",
    "for i, example in enumerate(example_reviews[:2]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"File: {example.file_path}\")\n",
    "    print(f\"\\nBefore:\\n{example.code_change_before}\")\n",
    "    print(f\"\\nAfter:\\n{example.code_change_after}\")\n",
    "    print(f\"\\nGenerated Review: {example.review_comment}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Code Change Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeChangeProcessor:\n",
    "    \"\"\"Process code changes for model input\"\"\"\n",
    "    \n",
    "    def __init__(self, max_length: int = 512):\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def create_diff_representation(self, before: str, after: str) -> str:\n",
    "        \"\"\"Create a unified diff representation\"\"\"\n",
    "        before_lines = before.strip().split('\\n')\n",
    "        after_lines = after.strip().split('\\n')\n",
    "        \n",
    "        diff_lines = []\n",
    "        \n",
    "        # Simple line-by-line diff\n",
    "        max_lines = max(len(before_lines), len(after_lines))\n",
    "        \n",
    "        for i in range(max_lines):\n",
    "            if i < len(before_lines) and i < len(after_lines):\n",
    "                if before_lines[i] != after_lines[i]:\n",
    "                    diff_lines.append(f\"- {before_lines[i]}\")\n",
    "                    diff_lines.append(f\"+ {after_lines[i]}\")\n",
    "                else:\n",
    "                    diff_lines.append(f\"  {before_lines[i]}\")\n",
    "            elif i < len(before_lines):\n",
    "                diff_lines.append(f\"- {before_lines[i]}\")\n",
    "            else:\n",
    "                diff_lines.append(f\"+ {after_lines[i]}\")\n",
    "        \n",
    "        return \"\\n\".join(diff_lines)\n",
    "    \n",
    "    def create_ast_aware_representation(self, code: str, language: str) -> Dict:\n",
    "        \"\"\"Create AST-aware representation (simplified)\"\"\"\n",
    "        # In real implementation, use tree-sitter or similar\n",
    "        ast_features = {\n",
    "            'has_function_call': bool(re.search(r'\\w+\\(', code)),\n",
    "            'has_condition': bool(re.search(r'if\\s*\\(', code)),\n",
    "            'has_loop': bool(re.search(r'(for|while)\\s*\\(', code)),\n",
    "            'has_try_catch': bool(re.search(r'try\\s*{', code)),\n",
    "            'variable_count': len(re.findall(r'\\b(var|let|const|\\w+\\s*=)\\b', code))\n",
    "        }\n",
    "        return ast_features\n",
    "    \n",
    "    def create_semantic_tokens(self, code: str) -> List[str]:\n",
    "        \"\"\"Extract semantic tokens from code\"\"\"\n",
    "        # Extract identifiers, keywords, operators\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[(){}\\[\\];,.]|[+\\-*/=<>!]+', code)\n",
    "        \n",
    "        # Classify tokens\n",
    "        keywords = {'if', 'else', 'for', 'while', 'try', 'catch', 'return', \n",
    "                   'function', 'class', 'const', 'let', 'var'}\n",
    "        \n",
    "        semantic_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in keywords:\n",
    "                semantic_tokens.append(f\"<KEYWORD:{token}>\")\n",
    "            elif token.isdigit():\n",
    "                semantic_tokens.append(\"<NUMBER>\")\n",
    "            elif re.match(r'^[A-Z_]+$', token):\n",
    "                semantic_tokens.append(\"<CONSTANT>\")\n",
    "            elif re.match(r'^[a-z_][a-zA-Z0-9_]*$', token):\n",
    "                semantic_tokens.append(f\"<ID:{token}>\")\n",
    "            else:\n",
    "                semantic_tokens.append(token)\n",
    "        \n",
    "        return semantic_tokens\n",
    "\n",
    "# Demonstrate different representations\n",
    "processor = CodeChangeProcessor()\n",
    "example = example_reviews[0]\n",
    "\n",
    "print(\"Different Code Change Representations:\")\n",
    "print(\"====================================\\n\")\n",
    "\n",
    "# 1. Raw representation\n",
    "print(\"1. Raw Input:\")\n",
    "print(example.to_model_input(include_context=True))\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# 2. Diff representation\n",
    "print(\"2. Diff Representation:\")\n",
    "diff = processor.create_diff_representation(example.code_change_before, example.code_change_after)\n",
    "print(diff)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# 3. AST features\n",
    "print(\"3. AST Features:\")\n",
    "ast_features = processor.create_ast_aware_representation(example.code_change_after, example.programming_language)\n",
    "for feature, value in ast_features.items():\n",
    "    print(f\"  {feature}: {value}\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# 4. Semantic tokens\n",
    "print(\"4. Semantic Tokens:\")\n",
    "tokens = processor.create_semantic_tokens(example.code_change_after)\n",
    "print(\" \".join(tokens[:20]) + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer Architecture for Code Review Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewT5Model:\n",
    "    \"\"\"Simplified T5-based model for code review generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"t5-small\", max_length: int = 512):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Add code-specific tokens\n",
    "        self._add_code_tokens()\n",
    "        \n",
    "    def _add_code_tokens(self):\n",
    "        \"\"\"Add code-specific special tokens\"\"\"\n",
    "        special_tokens = [\n",
    "            '<OLD>', '</OLD>', '<NEW>', '</NEW>',\n",
    "            '<FUNC>', '<VAR>', '<CLASS>', '<COMMENT>',\n",
    "            '<ADDED>', '<REMOVED>', '<MODIFIED>'\n",
    "        ]\n",
    "        \n",
    "        self.tokenizer.add_special_tokens({\n",
    "            'additional_special_tokens': special_tokens\n",
    "        })\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "    def prepare_input(self, code_before: str, code_after: str, \n",
    "                     task_prefix: str = \"review: \") -> Dict:\n",
    "        \"\"\"Prepare input for T5 model\"\"\"\n",
    "        # Format input\n",
    "        input_text = f\"{task_prefix}<OLD>{code_before}</OLD><NEW>{code_after}</NEW>\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def generate_review(self, code_before: str, code_after: str,\n",
    "                       num_beams: int = 4,\n",
    "                       max_new_tokens: int = 100,\n",
    "                       temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate code review comment\"\"\"\n",
    "        inputs = self.prepare_input(code_before, code_after)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=num_beams,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.95\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        review = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return review\n",
    "    \n",
    "    def visualize_attention(self, code_before: str, code_after: str):\n",
    "        \"\"\"Visualize attention patterns (simplified)\"\"\"\n",
    "        inputs = self.prepare_input(code_before, code_after)\n",
    "        \n",
    "        # Get model outputs with attention\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                **inputs,\n",
    "                decoder_input_ids=inputs['input_ids'][:, :10],  # First 10 tokens\n",
    "                output_attentions=True\n",
    "            )\n",
    "        \n",
    "        # Extract cross-attention from last layer\n",
    "        cross_attention = outputs.cross_attentions[-1][0, 0, :, :].numpy()\n",
    "        \n",
    "        # Plot attention heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cross_attention[:10, :20], cmap='Blues', cbar=True)\n",
    "        plt.xlabel('Input Tokens')\n",
    "        plt.ylabel('Output Tokens')\n",
    "        plt.title('Cross-Attention Visualization')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Note: Due to model size, we'll simulate outputs instead of loading actual model\n",
    "print(\"Code Review T5 Model Architecture:\")\n",
    "print(\"=================================\")\n",
    "print(\"\\nModel Components:\")\n",
    "print(\"1. Encoder: Processes code changes (before/after)\")\n",
    "print(\"2. Decoder: Generates natural language review\")\n",
    "print(\"3. Cross-Attention: Links code elements to review content\")\n",
    "print(\"\\nSpecial Tokens for Code:\")\n",
    "print(\"- <OLD>, </OLD>: Mark original code\")\n",
    "print(\"- <NEW>, </NEW>: Mark modified code\")\n",
    "print(\"- <ADDED>, <REMOVED>: Mark diff operations\")\n",
    "\n",
    "# Simulate model outputs for demonstration\n",
    "simulated_reviews = {\n",
    "    \"example_1\": \"Missing validation check. Add: if (!validateRequest(request)) return;\",\n",
    "    \"example_2\": \"Resource leak detected. Use try-finally to ensure connection.close().\",\n",
    "    \"example_3\": \"Good refactoring! More idiomatic Python.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Process with Experience Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceAwareTrainer:\n",
    "    \"\"\"Training pipeline with ELF integration\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.training_history = []\n",
    "        \n",
    "    def create_training_batch(self, examples: List[CodeReviewExample], \n",
    "                            reviewer_metrics: Dict) -> Dict:\n",
    "        \"\"\"Create batch with experience weights\"\"\"\n",
    "        batch_inputs = []\n",
    "        batch_targets = []\n",
    "        batch_weights = []\n",
    "        \n",
    "        for example in examples:\n",
    "            # Prepare input\n",
    "            input_text = f\"review: <OLD>{example.code_change_before}</OLD><NEW>{example.code_change_after}</NEW>\"\n",
    "            batch_inputs.append(input_text)\n",
    "            \n",
    "            # Prepare target\n",
    "            batch_targets.append(example.review_comment)\n",
    "            \n",
    "            # Calculate experience weight\n",
    "            metrics = reviewer_metrics.get(example.reviewer_id, {'aco': 0.1, 'rso': 0.1})\n",
    "            weight = np.exp(1 + (metrics['aco'] + metrics['rso']) / 2)\n",
    "            batch_weights.append(weight)\n",
    "        \n",
    "        return {\n",
    "            'inputs': batch_inputs,\n",
    "            'targets': batch_targets,\n",
    "            'weights': torch.tensor(batch_weights)\n",
    "        }\n",
    "    \n",
    "    def visualize_training_dynamics(self, n_epochs: int = 10):\n",
    "        \"\"\"Visualize how ELF affects training\"\"\"\n",
    "        # Simulate training dynamics\n",
    "        epochs = range(n_epochs)\n",
    "        \n",
    "        # Different reviewer types\n",
    "        high_exp_loss = [4.5 - 0.3*e - 0.05*e*np.random.random() for e in epochs]\n",
    "        mid_exp_loss = [4.5 - 0.25*e - 0.05*e*np.random.random() for e in epochs]\n",
    "        low_exp_loss = [4.5 - 0.2*e - 0.05*e*np.random.random() for e in epochs]\n",
    "        \n",
    "        # ELF-weighted average\n",
    "        weights = [7.39, 4.48, 2.72]  # High, mid, low experience weights\n",
    "        weighted_loss = [\n",
    "            (weights[0]*h + weights[1]*m + weights[2]*l) / sum(weights)\n",
    "            for h, m, l in zip(high_exp_loss, mid_exp_loss, low_exp_loss)\n",
    "        ]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Loss by experience level\n",
    "        ax1.plot(epochs, high_exp_loss, 'r-', label='High Experience', linewidth=2)\n",
    "        ax1.plot(epochs, mid_exp_loss, 'g--', label='Mid Experience', linewidth=2)\n",
    "        ax1.plot(epochs, low_exp_loss, 'b:', label='Low Experience', linewidth=2)\n",
    "        ax1.plot(epochs, weighted_loss, 'k-', label='ELF Weighted', linewidth=3)\n",
    "        \n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss by Reviewer Experience')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Gradient influence\n",
    "        gradient_influence = [\n",
    "            weights[0] / sum(weights) * 100,\n",
    "            weights[1] / sum(weights) * 100,\n",
    "            weights[2] / sum(weights) * 100\n",
    "        ]\n",
    "        \n",
    "        ax2.bar(['High Exp', 'Mid Exp', 'Low Exp'], gradient_influence, \n",
    "                color=['red', 'green', 'blue'], alpha=0.7)\n",
    "        ax2.set_ylabel('Gradient Influence (%)')\n",
    "        ax2.set_title('Relative Influence on Model Updates')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(gradient_influence):\n",
    "            ax2.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTraining Insights:\")\n",
    "        print(f\"- High experience reviewers have {weights[0]/weights[2]:.1f}x more influence\")\n",
    "        print(f\"- Weighted loss converges faster than uniform weighting\")\n",
    "        print(f\"- Model learns to prioritize patterns from experienced reviewers\")\n",
    "\n",
    "# Create mock reviewer metrics\n",
    "mock_reviewer_metrics = {\n",
    "    'exp_reviewer_1': {'aco': 0.35, 'rso': 0.45},\n",
    "    'exp_reviewer_2': {'aco': 0.30, 'rso': 0.40},\n",
    "    'mid_reviewer_1': {'aco': 0.10, 'rso': 0.20},\n",
    "    'low_reviewer_1': {'aco': 0.02, 'rso': 0.05}\n",
    "}\n",
    "\n",
    "# Demonstrate training\n",
    "trainer = ExperienceAwareTrainer(None, None)  # Mock trainer\n",
    "batch = trainer.create_training_batch(example_reviews[:3], mock_reviewer_metrics)\n",
    "\n",
    "print(\"Training Batch with Experience Weights:\")\n",
    "print(\"======================================\")\n",
    "for i, (target, weight) in enumerate(zip(batch['targets'], batch['weights'])):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Target: {target[:50]}...\")\n",
    "    print(f\"Experience Weight: {weight:.3f}\")\n",
    "\n",
    "# Visualize training dynamics\n",
    "trainer.visualize_training_dynamics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewEvaluator:\n",
    "    \"\"\"Comprehensive evaluation following paper's methodology\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define comment categories from Table 2\n",
    "        self.functional_categories = [\n",
    "            'functional_defect', 'validation', 'logical', \n",
    "            'interface', 'resource', 'support', 'timing'\n",
    "        ]\n",
    "        \n",
    "        self.evolvability_categories = [\n",
    "            'solution_approach', 'documentation', 'organization',\n",
    "            'alternate_output', 'naming_convention', 'visual_representation'\n",
    "        ]\n",
    "        \n",
    "        self.discussion_categories = ['question', 'design_discussion']\n",
    "        \n",
    "    def evaluate_semantic_equivalence(self, generated: str, reference: str) -> bool:\n",
    "        \"\"\"Check if generated comment has same intent as reference\"\"\"\n",
    "        # Extract key concepts\n",
    "        keywords = ['validation', 'check', 'leak', 'resource', 'close', \n",
    "                   'error', 'missing', 'add', 'use', 'refactor']\n",
    "        \n",
    "        gen_concepts = set([kw for kw in keywords if kw in generated.lower()])\n",
    "        ref_concepts = set([kw for kw in keywords if kw in reference.lower()])\n",
    "        \n",
    "        # Check overlap\n",
    "        if len(ref_concepts) == 0:\n",
    "            return len(gen_concepts) == 0\n",
    "        \n",
    "        overlap = len(gen_concepts.intersection(ref_concepts)) / len(ref_concepts)\n",
    "        return overlap > 0.5\n",
    "    \n",
    "    def evaluate_applicability(self, generated: str, code_change: str) -> bool:\n",
    "        \"\"\"Check if comment is applicable to the code change\"\"\"\n",
    "        # Extract code elements\n",
    "        code_tokens = set(re.findall(r'\\b\\w+\\b', code_change.lower()))\n",
    "        comment_tokens = set(re.findall(r'\\b\\w+\\b', generated.lower()))\n",
    "        \n",
    "        # Check if comment references code elements\n",
    "        overlap = len(code_tokens.intersection(comment_tokens))\n",
    "        \n",
    "        # Also check for general applicability patterns\n",
    "        applicable_patterns = [\n",
    "            r'should\\s+\\w+',\n",
    "            r'consider\\s+\\w+',\n",
    "            r'missing\\s+\\w+',\n",
    "            r'add\\s+\\w+',\n",
    "            r'use\\s+\\w+',\n",
    "            r'\\w+\\s+leak',\n",
    "            r'refactor'\n",
    "        ]\n",
    "        \n",
    "        has_pattern = any(re.search(pattern, generated.lower()) \n",
    "                         for pattern in applicable_patterns)\n",
    "        \n",
    "        return overlap > 2 or has_pattern\n",
    "    \n",
    "    def classify_feedback_type(self, comment: str) -> str:\n",
    "        \"\"\"Classify as suggestion, concern, or confused question\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "        \n",
    "        # Suggestion patterns\n",
    "        suggestion_patterns = [\n",
    "            r'should\\s+', r'consider\\s+', r'try\\s+', r'use\\s+',\n",
    "            r'add:', r'change\\s+to', r'instead\\s+of'\n",
    "        ]\n",
    "        \n",
    "        # Confused question patterns\n",
    "        confused_patterns = [\n",
    "            r'is\\s+this\\s+correct\\?',\n",
    "            r'what\\s+does',\n",
    "            r\"i\\s+don't\\s+understand\",\n",
    "            r'\\?\\?',\n",
    "            r'why\\s+.*\\?$'\n",
    "        ]\n",
    "        \n",
    "        if any(re.search(p, comment_lower) for p in suggestion_patterns):\n",
    "            return 'suggestion'\n",
    "        elif any(re.search(p, comment_lower) for p in confused_patterns):\n",
    "            return 'confused_question'\n",
    "        else:\n",
    "            return 'concern'\n",
    "    \n",
    "    def has_explanation(self, comment: str) -> bool:\n",
    "        \"\"\"Check if comment contains rationale\"\"\"\n",
    "        explanation_patterns = [\n",
    "            r'because\\s+', r'since\\s+', r'this\\s+prevents',\n",
    "            r'this\\s+ensures', r'to\\s+avoid', r'for\\s+better',\n",
    "            r'which\\s+', r'that\\s+', r'prevents\\s+', r'improves\\s+'\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(p, comment.lower()) for p in explanation_patterns)\n",
    "    \n",
    "    def identify_issue_category(self, comment: str) -> str:\n",
    "        \"\"\"Identify the type of issue discussed\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "        \n",
    "        # Functional issue patterns\n",
    "        if any(word in comment_lower for word in ['validation', 'validate', 'check']):\n",
    "            return 'validation'\n",
    "        elif any(word in comment_lower for word in ['leak', 'resource', 'close', 'release']):\n",
    "            return 'resource'\n",
    "        elif any(word in comment_lower for word in ['logic', 'incorrect', 'wrong', 'error']):\n",
    "            return 'logical'\n",
    "        \n",
    "        # Evolvability issue patterns\n",
    "        elif any(word in comment_lower for word in ['refactor', 'extract', 'separate']):\n",
    "            return 'organization'\n",
    "        elif any(word in comment_lower for word in ['comment', 'document', 'explain']):\n",
    "            return 'documentation'\n",
    "        elif any(word in comment_lower for word in ['rename', 'naming', 'name']):\n",
    "            return 'naming_convention'\n",
    "        elif any(word in comment_lower for word in ['format', 'indent', 'space', 'style']):\n",
    "            return 'visual_representation'\n",
    "        \n",
    "        return 'other'\n",
    "    \n",
    "    def comprehensive_evaluate(self, generated: str, reference: str, \n",
    "                             code_change: str) -> Dict:\n",
    "        \"\"\"Perform comprehensive evaluation\"\"\"\n",
    "        return {\n",
    "            'semantic_equivalence': self.evaluate_semantic_equivalence(generated, reference),\n",
    "            'applicability': self.evaluate_applicability(generated, code_change),\n",
    "            'feedback_type': self.classify_feedback_type(generated),\n",
    "            'has_explanation': self.has_explanation(generated),\n",
    "            'issue_category': self.identify_issue_category(generated)\n",
    "        }\n",
    "\n",
    "# Evaluate example comments\n",
    "evaluator = CodeReviewEvaluator()\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        'generated': \"Missing validation check. Add: if (!validateRequest(request)) return; This prevents unauthorized access.\",\n",
    "        'reference': \"Add validation before processing request\",\n",
    "        'code': \"if (user.role == 'admin') { processAdminRequest(request); }\"\n",
    "    },\n",
    "    {\n",
    "        'generated': \"Resource leak detected. Use try-finally to ensure connection.close().\",\n",
    "        'reference': \"Connection should be closed after use\",\n",
    "        'code': \"conn = database.connect()\\ndata = conn.query(sql)\"\n",
    "    },\n",
    "    {\n",
    "        'generated': \"Is this correct?\",\n",
    "        'reference': \"Check if validation is needed\",\n",
    "        'code': \"processRequest(request)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"==================\")\n",
    "\n",
    "for i, test in enumerate(test_cases):\n",
    "    print(f\"\\nTest Case {i+1}:\")\n",
    "    print(f\"Generated: {test['generated']}\")\n",
    "    \n",
    "    results = evaluator.comprehensive_evaluate(\n",
    "        test['generated'], \n",
    "        test['reference'],\n",
    "        test['code']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluation:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Model Behavior and Comment Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_comment_quality_distribution():\n",
    "    \"\"\"Analyze distribution of comment quality metrics\"\"\"\n",
    "    \n",
    "    # Simulate evaluation results for different models\n",
    "    models = ['CodeReviewer', 'ELF_aco_pkg', 'ELF_rso_pkg', 'ELF_avg_pkg']\n",
    "    \n",
    "    # Simulated metrics (based on paper results)\n",
    "    metrics_data = {\n",
    "        'CodeReviewer': {\n",
    "            'applicable': 42,\n",
    "            'suggestions': 27,\n",
    "            'concerns': 8,\n",
    "            'confused': 7,\n",
    "            'has_explanation': 8,\n",
    "            'functional_issues': 7,\n",
    "            'evolvability_issues': 24\n",
    "        },\n",
    "        'ELF_aco_pkg': {\n",
    "            'applicable': 53,\n",
    "            'suggestions': 42,\n",
    "            'concerns': 9,\n",
    "            'confused': 2,\n",
    "            'has_explanation': 15,\n",
    "            'functional_issues': 13,\n",
    "            'evolvability_issues': 29\n",
    "        },\n",
    "        'ELF_rso_pkg': {\n",
    "            'applicable': 53,\n",
    "            'suggestions': 37,\n",
    "            'concerns': 14,\n",
    "            'confused': 2,\n",
    "            'has_explanation': 11,\n",
    "            'functional_issues': 12,\n",
    "            'evolvability_issues': 27\n",
    "        },\n",
    "        'ELF_avg_pkg': {\n",
    "            'applicable': 46,\n",
    "            'suggestions': 30,\n",
    "            'concerns': 12,\n",
    "            'confused': 3,\n",
    "            'has_explanation': 13,\n",
    "            'functional_issues': 16,\n",
    "            'evolvability_issues': 19\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Overall applicability\n",
    "    ax1 = axes[0, 0]\n",
    "    applicability = [metrics_data[m]['applicable'] for m in models]\n",
    "    bars1 = ax1.bar(models, applicability, color=['gray', 'lightblue', 'lightgreen', 'lightcoral'])\n",
    "    ax1.set_ylabel('Count (out of 100)')\n",
    "    ax1.set_title('Applicable Comments Generated')\n",
    "    ax1.set_ylim(0, 60)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, applicability):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: Feedback type distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    feedback_types = ['suggestions', 'concerns', 'confused']\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, ftype in enumerate(feedback_types):\n",
    "        values = [metrics_data[m][ftype] for m in models]\n",
    "        ax2.bar(x + i*width, values, width, label=ftype.capitalize())\n",
    "    \n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Feedback Type Distribution')\n",
    "    ax2.set_xticks(x + width)\n",
    "    ax2.set_xticklabels(models, rotation=45)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Plot 3: Issue type comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    functional = [metrics_data[m]['functional_issues'] for m in models]\n",
    "    evolvability = [metrics_data[m]['evolvability_issues'] for m in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, functional, width, label='Functional', color='darkred')\n",
    "    ax3.bar(x + width/2, evolvability, width, label='Evolvability', color='darkblue')\n",
    "    \n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Issue Types Identified')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(models, rotation=45)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Plot 4: Quality improvement radar chart\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    baseline = metrics_data['CodeReviewer']\n",
    "    categories = ['Applicable', 'Suggestions', 'Explanations', 'Functional', 'Less Confused']\n",
    "    \n",
    "    improvements = {}\n",
    "    for model in models[1:]:\n",
    "        m = metrics_data[model]\n",
    "        improvements[model] = [\n",
    "            (m['applicable'] - baseline['applicable']) / baseline['applicable'] * 100,\n",
    "            (m['suggestions'] - baseline['suggestions']) / baseline['suggestions'] * 100,\n",
    "            (m['has_explanation'] - baseline['has_explanation']) / baseline['has_explanation'] * 100,\n",
    "            (m['functional_issues'] - baseline['functional_issues']) / baseline['functional_issues'] * 100,\n",
    "            (baseline['confused'] - m['confused']) / baseline['confused'] * 100\n",
    "        ]\n",
    "    \n",
    "    # Bar plot of improvements\n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (model, values) in enumerate(improvements.items()):\n",
    "        ax4.bar(x + i*width, values, width, label=model.replace('ELF_', ''))\n",
    "    \n",
    "    ax4.set_xlabel('Metric')\n",
    "    ax4.set_ylabel('Improvement (%)')\n",
    "    ax4.set_title('Percentage Improvement over CodeReviewer')\n",
    "    ax4.set_xticks(x + width)\n",
    "    ax4.set_xticklabels(categories, rotation=45)\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key insights\n",
    "    print(\"\\nKey Insights from Analysis:\")\n",
    "    print(\"===========================\")\n",
    "    print(\"1. ELF models generate 26-29% more applicable comments\")\n",
    "    print(\"2. Suggestion rate improves by up to 56% with ELF_aco_pkg\")\n",
    "    print(\"3. Confused questions reduced by 71% with experience-aware training\")\n",
    "    print(\"4. Functional issue detection improves by 86-129%\")\n",
    "    print(\"5. Comments with explanations increase by 38-88%\")\n",
    "\n",
    "# Analyze comment quality\n",
    "analyze_comment_quality_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Techniques and Future Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCodeReviewTechniques:\n",
    "    \"\"\"Advanced techniques for code review generation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def multi_task_learning():\n",
    "        \"\"\"Multi-task learning approach\"\"\"\n",
    "        print(\"Multi-Task Learning for Code Review:\")\n",
    "        print(\"====================================\")\n",
    "        print(\"\\nTasks:\")\n",
    "        print(\"1. Code Change Quality Estimation (Binary)\")\n",
    "        print(\"   - Input: Code change\")\n",
    "        print(\"   - Output: Needs review? (Yes/No)\")\n",
    "        print(\"\\n2. Review Comment Generation (Seq2Seq)\")\n",
    "        print(\"   - Input: Code change\")\n",
    "        print(\"   - Output: Natural language comment\")\n",
    "        print(\"\\n3. Code Refinement (Seq2Seq)\")\n",
    "        print(\"   - Input: Code + Review comment\")\n",
    "        print(\"   - Output: Refined code\")\n",
    "        print(\"\\nShared encoder learns unified code representations\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def retrieval_augmented_generation():\n",
    "        \"\"\"RAG approach for code reviews\"\"\"\n",
    "        print(\"\\nRetrieval-Augmented Generation:\")\n",
    "        print(\"===============================\")\n",
    "        print(\"\\nComponents:\")\n",
    "        print(\"1. Code Change Encoder\")\n",
    "        print(\"2. Review Database (Vector Store)\")\n",
    "        print(\"3. Similarity Search\")\n",
    "        print(\"4. Context-aware Generator\")\n",
    "        print(\"\\nProcess:\")\n",
    "        print(\"- Encode code change\")\n",
    "        print(\"- Retrieve similar past reviews\")\n",
    "        print(\"- Generate review using retrieved context\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def visualize_future_directions():\n",
    "        \"\"\"Visualize future research directions\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Future direction 1: Multi-modal understanding\n",
    "        ax1.text(0.5, 0.9, 'Multi-Modal Code Understanding', \n",
    "                ha='center', fontsize=16, weight='bold')\n",
    "        \n",
    "        components = [\n",
    "            (0.2, 0.7, 'Code\\nText'),\n",
    "            (0.5, 0.7, 'AST\\nStructure'),\n",
    "            (0.8, 0.7, 'Data\\nFlow'),\n",
    "            (0.35, 0.4, 'Control\\nFlow'),\n",
    "            (0.65, 0.4, 'Type\\nInfo'),\n",
    "            (0.5, 0.1, 'Unified\\nRepresentation')\n",
    "        ]\n",
    "        \n",
    "        for x, y, text in components:\n",
    "            if y > 0.2:\n",
    "                ax1.add_patch(plt.Rectangle((x-0.08, y-0.05), 0.16, 0.1, \n",
    "                                          fill=True, color='lightblue', alpha=0.7))\n",
    "                ax1.text(x, y, text, ha='center', va='center')\n",
    "                ax1.arrow(x, y-0.05, 0, -0.15, head_width=0.02, head_length=0.02, \n",
    "                         fc='gray', ec='gray')\n",
    "            else:\n",
    "                ax1.add_patch(plt.Rectangle((x-0.1, y-0.05), 0.2, 0.1, \n",
    "                                          fill=True, color='lightgreen', alpha=0.7))\n",
    "                ax1.text(x, y, text, ha='center', va='center', weight='bold')\n",
    "        \n",
    "        ax1.set_xlim(0, 1)\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Future direction 2: Personalized review generation\n",
    "        ax2.text(0.5, 0.9, 'Personalized Review Generation', \n",
    "                ha='center', fontsize=16, weight='bold')\n",
    "        \n",
    "        # Create flow diagram\n",
    "        nodes = [\n",
    "            (0.2, 0.7, 'Developer\\nProfile'),\n",
    "            (0.5, 0.7, 'Project\\nContext'),\n",
    "            (0.8, 0.7, 'Team\\nStandards'),\n",
    "            (0.5, 0.4, 'Adaptive\\nModel'),\n",
    "            (0.5, 0.1, 'Personalized\\nReview')\n",
    "        ]\n",
    "        \n",
    "        for i, (x, y, text) in enumerate(nodes):\n",
    "            if i < 3:\n",
    "                color = 'lightcoral'\n",
    "            elif i == 3:\n",
    "                color = 'lightyellow'\n",
    "            else:\n",
    "                color = 'lightgreen'\n",
    "            \n",
    "            ax2.add_patch(plt.Circle((x, y), 0.08, fill=True, color=color, alpha=0.7))\n",
    "            ax2.text(x, y, text, ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        # Add arrows\n",
    "        for i in range(3):\n",
    "            x, y, _ = nodes[i]\n",
    "            ax2.arrow(x, y-0.08, 0.5-x, 0.4-y+0.08, \n",
    "                     head_width=0.02, head_length=0.02, fc='gray', ec='gray', alpha=0.5)\n",
    "        \n",
    "        ax2.arrow(0.5, 0.32, 0, -0.14, head_width=0.03, head_length=0.03, \n",
    "                 fc='darkgreen', ec='darkgreen')\n",
    "        \n",
    "        ax2.set_xlim(0, 1)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate advanced techniques\n",
    "advanced = AdvancedCodeReviewTechniques()\n",
    "advanced.multi_task_learning()\n",
    "advanced.retrieval_augmented_generation()\n",
    "advanced.visualize_future_directions()\n",
    "\n",
    "print(\"\\nFuture Research Opportunities:\")\n",
    "print(\"==============================\")\n",
    "print(\"1. **Cross-lingual Review Generation**: Support multiple programming languages\")\n",
    "print(\"2. **Interactive Review Systems**: Allow back-and-forth clarification\")\n",
    "print(\"3. **Review Quality Estimation**: Predict review helpfulness before posting\")\n",
    "print(\"4. **Team-aware Models**: Adapt to team coding standards and preferences\")\n",
    "print(\"5. **Incremental Learning**: Continuously improve from accepted/rejected reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Implementation Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PracticalImplementationGuide:\n",
    "    \"\"\"Guide for implementing code review generation in practice\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def implementation_checklist():\n",
    "        \"\"\"Checklist for implementing the system\"\"\"\n",
    "        checklist = [\n",
    "            \"Data Collection\",\n",
    "            \"- [ ] Extract code review data from repositories\",\n",
    "            \"- [ ] Filter bot accounts and non-NL comments\",\n",
    "            \"- [ ] Calculate reviewer ownership metrics\",\n",
    "            \"- [ ] Split data (train/val/test)\",\n",
    "            \"\",\n",
    "            \"Model Setup\",\n",
    "            \"- [ ] Initialize T5/CodeT5 base model\",\n",
    "            \"- [ ] Add code-specific tokens\",\n",
    "            \"- [ ] Implement ELF loss function\",\n",
    "            \"- [ ] Setup distributed training\",\n",
    "            \"\",\n",
    "            \"Training\",\n",
    "            \"- [ ] Implement experience-aware batching\",\n",
    "            \"- [ ] Monitor weight distributions\",\n",
    "            \"- [ ] Track per-experience-level metrics\",\n",
    "            \"- [ ] Implement gradient clipping\",\n",
    "            \"\",\n",
    "            \"Evaluation\",\n",
    "            \"- [ ] Automatic metrics (BLEU-4)\",\n",
    "            \"- [ ] Manual evaluation setup\",\n",
    "            \"- [ ] Inter-rater agreement\",\n",
    "            \"- [ ] Category-wise analysis\",\n",
    "            \"\",\n",
    "            \"Deployment\",\n",
    "            \"- [ ] API endpoint setup\",\n",
    "            \"- [ ] Caching for common patterns\",\n",
    "            \"- [ ] Monitoring and logging\",\n",
    "            \"- [ ] A/B testing framework\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Implementation Checklist:\")\n",
    "        print(\"========================\")\n",
    "        for item in checklist:\n",
    "            print(item)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample_config():\n",
    "        \"\"\"Sample configuration for the system\"\"\"\n",
    "        config = {\n",
    "            \"model\": {\n",
    "                \"base_model\": \"Salesforce/codet5-base\",\n",
    "                \"max_input_length\": 512,\n",
    "                \"max_output_length\": 128,\n",
    "                \"num_beams\": 4,\n",
    "                \"temperature\": 0.7\n",
    "            },\n",
    "            \"training\": {\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate\": 3e-4,\n",
    "                \"num_epochs\": 30,\n",
    "                \"warmup_steps\": 1000,\n",
    "                \"gradient_clip\": 1.0\n",
    "            },\n",
    "            \"elf\": {\n",
    "                \"strategy\": \"aco\",\n",
    "                \"granularity\": \"package\",\n",
    "                \"weight_clip\": [0.5, 10.0],\n",
    "                \"adaptive_scaling\": True\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"min_comment_length\": 10,\n",
    "                \"max_comment_length\": 200,\n",
    "                \"languages\": [\"python\", \"java\", \"javascript\"],\n",
    "                \"remove_bots\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\nSample Configuration:\")\n",
    "        print(\"====================\")\n",
    "        import json\n",
    "        print(json.dumps(config, indent=2))\n",
    "        \n",
    "        return config\n",
    "\n",
    "# Show implementation guide\n",
    "guide = PracticalImplementationGuide()\n",
    "guide.implementation_checklist()\n",
    "config = guide.sample_config()\n",
    "\n",
    "print(\"\\nKey Implementation Tips:\")\n",
    "print(\"=======================\")\n",
    "print(\"1. Start with a smaller model (T5-small) for experimentation\")\n",
    "print(\"2. Use mixed precision training for efficiency\")\n",
    "print(\"3. Implement caching for ownership metric calculations\")\n",
    "print(\"4. Monitor ELF weight distributions during training\")\n",
    "print(\"5. Use stratified sampling for evaluation sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### Core Concepts Mastered\n",
    "1. **Task Formulation**: Code change → Natural language review (Seq2Seq)\n",
    "2. **Model Architecture**: T5-based with code-specific adaptations\n",
    "3. **Experience Integration**: ELF weights influence training dynamics\n",
    "4. **Evaluation Framework**: Multi-dimensional assessment beyond BLEU\n",
    "\n",
    "### Technical Insights\n",
    "1. **Input Representation**: Combine before/after code with context\n",
    "2. **Special Tokens**: <OLD>, <NEW>, <ADDED>, <REMOVED> improve understanding\n",
    "3. **Beam Search**: Balance diversity and quality in generation\n",
    "4. **Manual Evaluation**: Essential for capturing semantic aspects\n",
    "\n",
    "### Key Results\n",
    "1. **+29% Applicable Comments**: ELF models generate more relevant reviews\n",
    "2. **+56% Suggestions**: More actionable feedback vs concerns\n",
    "3. **-71% Confused Questions**: Reduced uncertainty in comments\n",
    "4. **+129% Functional Issues**: Better detection of critical problems\n",
    "\n",
    "### Future Directions\n",
    "1. **Multi-modal Input**: Integrate AST, data flow, type information\n",
    "2. **Personalization**: Adapt to team standards and preferences\n",
    "3. **Interactive Systems**: Enable clarification dialogues\n",
    "4. **Cross-lingual Support**: Handle multiple programming languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final research template\n",
    "print(\"Code Review Generation Research Template\")\n",
    "print(\"=======================================\")\n",
    "print(\"\\nclass CodeReviewResearch:\")\n",
    "print(\"    def __init__(self):\")\n",
    "print(\"        self.model = self.load_model()\")\n",
    "print(\"        self.evaluator = CodeReviewEvaluator()\")\n",
    "print(\"        self.ownership_calculator = OwnershipCalculator()\")\n",
    "print(\"        \")\n",
    "print(\"    def train_with_elf(self, dataset, strategy='aco', granularity='package'):\")\n",
    "print(\"        # Your implementation\")\n",
    "print(\"        pass\")\n",
    "print(\"        \")\n",
    "print(\"    def evaluate_comprehensive(self, test_set):\")\n",
    "print(\"        # Your evaluation\")\n",
    "print(\"        pass\")\n",
    "print(\"        \")\n",
    "print(\"    def analyze_results(self):\")\n",
    "print(\"        # Your analysis\")\n",
    "print(\"        pass\")\n",
    "print(\"\\nReady to implement code review generation with experience awareness!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}