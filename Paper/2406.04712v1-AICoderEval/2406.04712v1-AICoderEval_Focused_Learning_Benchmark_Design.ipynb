{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: AICoderEval Benchmark Design & Data Curation\n",
    "\n",
    "## Mục tiêu học tập\n",
    "\n",
    "Notebook này tập trung vào việc hiểu sâu về cách thiết kế và xây dựng benchmark AICoderEval - một trong những đóng góp quan trọng nhất của paper. Chúng ta sẽ:\n",
    "\n",
    "1. Hiểu cách thu thập và xử lý dữ liệu từ HuggingFace Hub và PyTorch Hub\n",
    "2. Học cách thiết kế cấu trúc benchmark cho AI code generation\n",
    "3. Implement quy trình data curation với filtering và validation\n",
    "4. Xây dựng test cases theo mẫu HumanEval\n",
    "\n",
    "## Trích xuất từ paper\n",
    "\n",
    "**Section 2: Benchmark Construction** (trang 3-5)\n",
    "- \"We leverage the power of GPT-4 to process data collected from the web and format it into a structured form\"\n",
    "- \"Each generated file is meticulously structured to encompass a comprehensive suite of components necessary for robust testing\"\n",
    "- Figure 2 mô tả kiến trúc CoderGen với data generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lý thuyết: Thiết kế Benchmark cho AI Code Generation\n",
    "\n",
    "### 1.1. Challenges trong việc đánh giá AI code generation\n",
    "\n",
    "$$\\text{Evaluation Challenge} = \\text{Correctness} + \\text{Library Usage} + \\text{Real-world Applicability}$$\n",
    "\n",
    "Paper giải quyết các thách thức:\n",
    "- **Domain Specificity**: Code phải sử dụng đúng libraries (HuggingFace, PyTorch, TensorFlow)\n",
    "- **Executable Validation**: Code phải chạy được và pass test cases\n",
    "- **Diversity**: Cover nhiều domains (NLP, CV, Audio, etc.)\n",
    "\n",
    "### 1.2. Cấu trúc của một benchmark entry\n",
    "\n",
    "Mỗi entry trong AICoderEval bao gồm:\n",
    "1. **Task Description**: Mô tả clear về yêu cầu\n",
    "2. **Complete Code File**: Bao gồm imports, implementation, tests\n",
    "3. **Test Cases**: 3 levels - normal, edge case, correctness\n",
    "4. **Metadata**: Domain, model info, performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import ast\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation: Benchmark Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDomain(Enum):\n",
    "    \"\"\"Domains covered in AICoderEval\"\"\"\n",
    "    NLP = \"Natural Language Processing\"\n",
    "    CV = \"Computer Vision\"\n",
    "    TABULAR = \"Tabular Data\"\n",
    "    AUDIO = \"Audio and Speech\"\n",
    "    MULTIMODAL = \"Multimodal\"\n",
    "    RL = \"Reinforcement Learning\"\n",
    "    CLASSIFICATION = \"Classification\"\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"Structure for a single test case\"\"\"\n",
    "    name: str\n",
    "    test_type: str  # \"normal\", \"edge_case\", \"correctness\"\n",
    "    input_data: Any\n",
    "    expected_output: Any\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkEntry:\n",
    "    \"\"\"Complete structure for a benchmark entry following paper design\"\"\"\n",
    "    # Basic information\n",
    "    task_id: str\n",
    "    domain: TaskDomain\n",
    "    task_description: str\n",
    "    \n",
    "    # Model information\n",
    "    model_name: str\n",
    "    model_description: str\n",
    "    libraries_used: List[str]\n",
    "    \n",
    "    # Code components\n",
    "    imports: List[str]\n",
    "    function_signature: str\n",
    "    function_docstring: str\n",
    "    implementation: str\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases: List[TestCase]\n",
    "    \n",
    "    # Metadata\n",
    "    created_date: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    validation_status: str = \"pending\"\n",
    "    execution_time: Optional[float] = None\n",
    "    \n",
    "    def to_code_file(self) -> str:\n",
    "        \"\"\"Generate complete Python file following paper's format\"\"\"\n",
    "        code_parts = []\n",
    "        \n",
    "        # Package installation comments\n",
    "        code_parts.append(\"# Package installation\")\n",
    "        for lib in self.libraries_used:\n",
    "            code_parts.append(f\"# pip install {lib}\")\n",
    "        code_parts.append(\"\")\n",
    "        \n",
    "        # Imports\n",
    "        code_parts.append(\"# Imports\")\n",
    "        code_parts.extend(self.imports)\n",
    "        code_parts.append(\"\")\n",
    "        \n",
    "        # Main function\n",
    "        code_parts.append(\"# Main function\")\n",
    "        code_parts.append(self.function_signature)\n",
    "        code_parts.append(f'    \"\"\"{self.function_docstring}\"\"\"')\n",
    "        code_parts.append(self.implementation)\n",
    "        code_parts.append(\"\")\n",
    "        \n",
    "        # Test functions\n",
    "        code_parts.append(\"# Test functions\")\n",
    "        code_parts.append(self._generate_test_functions())\n",
    "        \n",
    "        # Main execution\n",
    "        code_parts.append(\"\")\n",
    "        code_parts.append(\"if __name__ == '__main__':\")\n",
    "        code_parts.append(\"    test_main()\")\n",
    "        \n",
    "        return \"\\n\".join(code_parts)\n",
    "    \n",
    "    def _generate_test_functions(self) -> str:\n",
    "        \"\"\"Generate test functions following paper's format\"\"\"\n",
    "        test_code = []\n",
    "        \n",
    "        test_code.append(\"def test_main():\")\n",
    "        test_code.append('    print(\"Testing started.\")')\n",
    "        test_code.append(\"    \")\n",
    "        \n",
    "        for i, test_case in enumerate(self.test_cases, 1):\n",
    "            test_code.append(f'    # Test case {i}: {test_case.test_type}')\n",
    "            test_code.append(f'    print(\"Testing case [{i}/{len(self.test_cases)}] started\")')\n",
    "            test_code.append(\"    try:\")\n",
    "            test_code.append(f\"        result = {self._generate_test_call(test_case)}\")\n",
    "            test_code.append(f\"        assert result == {repr(test_case.expected_output)}\")\n",
    "            test_code.append(f'        print(f\"Test case [{i}/{len(self.test_cases)}] succeeded\")')\n",
    "            test_code.append(\"    except Exception as e:\")\n",
    "            test_code.append(f'        print(f\"Test case [{i}/{len(self.test_cases)}] failed: {{e}}\")')\n",
    "            test_code.append(\"    \")\n",
    "        \n",
    "        test_code.append('    print(\"Testing finished.\")')\n",
    "        \n",
    "        return \"\\n\".join(test_code)\n",
    "    \n",
    "    def _generate_test_call(self, test_case: TestCase) -> str:\n",
    "        \"\"\"Generate function call for test\"\"\"\n",
    "        func_name = self.function_signature.split(\"(\")[0].replace(\"def \", \"\")\n",
    "        return f\"{func_name}({repr(test_case.input_data)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector:\n",
    "    \"\"\"Simulates data collection from HuggingFace Hub and PyTorch Hub\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mock data representing scraped model information\n",
    "        self.mock_model_data = [\n",
    "            {\n",
    "                \"source\": \"huggingface\",\n",
    "                \"model_name\": \"bert-base-uncased\",\n",
    "                \"domain\": \"NLP\",\n",
    "                \"task\": \"text-classification\",\n",
    "                \"description\": \"BERT model for text classification\",\n",
    "                \"example_code\": \"from transformers import pipeline\\nclassifier = pipeline('text-classification')\"\n",
    "            },\n",
    "            {\n",
    "                \"source\": \"pytorch\",\n",
    "                \"model_name\": \"resnet50\",\n",
    "                \"domain\": \"CV\",\n",
    "                \"task\": \"image-classification\",\n",
    "                \"description\": \"ResNet50 for image classification\",\n",
    "                \"example_code\": \"import torchvision.models as models\\nmodel = models.resnet50(pretrained=True)\"\n",
    "            },\n",
    "            {\n",
    "                \"source\": \"huggingface\",\n",
    "                \"model_name\": \"wav2vec2-base\",\n",
    "                \"domain\": \"Audio\",\n",
    "                \"task\": \"speech-recognition\",\n",
    "                \"description\": \"Wav2Vec2 for automatic speech recognition\",\n",
    "                \"example_code\": \"from transformers import pipeline\\nasr = pipeline('automatic-speech-recognition')\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def collect_raw_data(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Simulate collecting raw data from web sources\"\"\"\n",
    "        print(\"Collecting data from HuggingFace Hub and PyTorch Hub...\")\n",
    "        return self.mock_model_data\n",
    "    \n",
    "    def preprocess_data(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Preprocess and structure the collected data\"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        for item in raw_data:\n",
    "            processed_item = {\n",
    "                \"domain\": self._map_domain(item[\"domain\"]),\n",
    "                \"model_name\": item[\"model_name\"],\n",
    "                \"model_description\": item[\"description\"],\n",
    "                \"task_type\": item[\"task\"],\n",
    "                \"source_library\": item[\"source\"],\n",
    "                \"example_code\": item[\"example_code\"],\n",
    "                \"metadata\": {\n",
    "                    \"collected_date\": datetime.now().isoformat(),\n",
    "                    \"source\": item[\"source\"]\n",
    "                }\n",
    "            }\n",
    "            processed_data.append(processed_item)\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def _map_domain(self, domain_str: str) -> TaskDomain:\n",
    "        \"\"\"Map string domain to TaskDomain enum\"\"\"\n",
    "        mapping = {\n",
    "            \"NLP\": TaskDomain.NLP,\n",
    "            \"CV\": TaskDomain.CV,\n",
    "            \"Audio\": TaskDomain.AUDIO,\n",
    "            \"Tabular\": TaskDomain.TABULAR,\n",
    "            \"Multimodal\": TaskDomain.MULTIMODAL,\n",
    "            \"RL\": TaskDomain.RL\n",
    "        }\n",
    "        return mapping.get(domain_str, TaskDomain.CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark Generation với GPT-4 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkGenerator:\n",
    "    \"\"\"Generate benchmark entries following paper's approach\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entry_counter = 0\n",
    "    \n",
    "    def generate_from_model_info(self, model_info: Dict[str, Any]) -> BenchmarkEntry:\n",
    "        \"\"\"Generate a complete benchmark entry from model information\"\"\"\n",
    "        self.entry_counter += 1\n",
    "        \n",
    "        # Generate task description\n",
    "        task_description = self._generate_task_description(model_info)\n",
    "        \n",
    "        # Generate function components\n",
    "        func_components = self._generate_function_components(model_info)\n",
    "        \n",
    "        # Generate test cases following paper's 3-level approach\n",
    "        test_cases = self._generate_test_cases(model_info)\n",
    "        \n",
    "        # Create benchmark entry\n",
    "        entry = BenchmarkEntry(\n",
    "            task_id=f\"task_{self.entry_counter:04d}\",\n",
    "            domain=model_info[\"domain\"],\n",
    "            task_description=task_description,\n",
    "            model_name=model_info[\"model_name\"],\n",
    "            model_description=model_info[\"model_description\"],\n",
    "            libraries_used=self._extract_libraries(model_info),\n",
    "            imports=func_components[\"imports\"],\n",
    "            function_signature=func_components[\"signature\"],\n",
    "            function_docstring=func_components[\"docstring\"],\n",
    "            implementation=func_components[\"implementation\"],\n",
    "            test_cases=test_cases\n",
    "        )\n",
    "        \n",
    "        return entry\n",
    "    \n",
    "    def _generate_task_description(self, model_info: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate one-sentence task description\"\"\"\n",
    "        templates = {\n",
    "            \"text-classification\": \"Create a function that classifies text into categories using {model_name}.\",\n",
    "            \"image-classification\": \"Implement image classification using {model_name} to identify objects in images.\",\n",
    "            \"speech-recognition\": \"Build a speech recognition function using {model_name} to transcribe audio.\"\n",
    "        }\n",
    "        \n",
    "        template = templates.get(model_info[\"task_type\"], \n",
    "                                \"Implement a function using {model_name} for {task_type}.\")\n",
    "        return template.format(\n",
    "            model_name=model_info[\"model_name\"],\n",
    "            task_type=model_info[\"task_type\"]\n",
    "        )\n",
    "    \n",
    "    def _generate_function_components(self, model_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate function signature, docstring, and implementation\"\"\"\n",
    "        \n",
    "        # Map task type to function details\n",
    "        function_templates = {\n",
    "            \"text-classification\": {\n",
    "                \"signature\": \"def classify_text(text: str, model_name: str = '{model_name}') -> Dict[str, float]\",\n",
    "                \"docstring\": \"\"\"Classify text using {model_name}.\n",
    "                \n",
    "                Args:\n",
    "                    text: Input text to classify\n",
    "                    model_name: Name of the model to use\n",
    "                    \n",
    "                Returns:\n",
    "                    Dict containing label and confidence score\n",
    "                    \n",
    "                Raises:\n",
    "                    ValueError: If text is empty\n",
    "                    RuntimeError: If model loading fails\n",
    "                \"\"\",\n",
    "                \"imports\": [\n",
    "                    \"from transformers import pipeline\",\n",
    "                    \"from typing import Dict\",\n",
    "                    \"import warnings\",\n",
    "                    \"warnings.filterwarnings('ignore')\"\n",
    "                ],\n",
    "                \"implementation\": \"\"\"    if not text or not text.strip():\n",
    "        raise ValueError(\"Text cannot be empty\")\n",
    "    \n",
    "    try:\n",
    "        classifier = pipeline('text-classification', model=model_name)\n",
    "        results = classifier(text)\n",
    "        return {results[0]['label']: results[0]['score']}\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to classify text: {e}\")\"\"\"\n",
    "            },\n",
    "            \"image-classification\": {\n",
    "                \"signature\": \"def classify_image(image_path: str) -> Dict[str, float]\",\n",
    "                \"docstring\": \"\"\"Classify image using ResNet50.\n",
    "                \n",
    "                Args:\n",
    "                    image_path: Path to the image file\n",
    "                    \n",
    "                Returns:\n",
    "                    Dict containing top prediction and confidence\n",
    "                    \n",
    "                Raises:\n",
    "                    FileNotFoundError: If image file not found\n",
    "                    ValueError: If image format is invalid\n",
    "                \"\"\",\n",
    "                \"imports\": [\n",
    "                    \"import torch\",\n",
    "                    \"import torchvision.models as models\",\n",
    "                    \"import torchvision.transforms as transforms\",\n",
    "                    \"from PIL import Image\",\n",
    "                    \"from typing import Dict\",\n",
    "                    \"import os\"\n",
    "                ],\n",
    "                \"implementation\": \"\"\"    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Load model and classify\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model.eval()\n",
    "        \n",
    "        input_tensor = preprocess(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            \n",
    "        # Get top prediction\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        top_prob, top_class = torch.topk(probabilities, 1)\n",
    "        \n",
    "        return {f\"class_{top_class.item()}\": top_prob.item()}\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to process image: {e}\")\"\"\"\n",
    "            },\n",
    "            \"speech-recognition\": {\n",
    "                \"signature\": \"def transcribe_audio(audio_path: str) -> str\",\n",
    "                \"docstring\": \"\"\"Transcribe audio using Wav2Vec2.\n",
    "                \n",
    "                Args:\n",
    "                    audio_path: Path to audio file\n",
    "                    \n",
    "                Returns:\n",
    "                    Transcribed text\n",
    "                    \n",
    "                Raises:\n",
    "                    FileNotFoundError: If audio file not found\n",
    "                    RuntimeError: If transcription fails\n",
    "                \"\"\",\n",
    "                \"imports\": [\n",
    "                    \"from transformers import pipeline\",\n",
    "                    \"import os\"\n",
    "                ],\n",
    "                \"implementation\": \"\"\"    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "    \n",
    "    try:\n",
    "        asr = pipeline('automatic-speech-recognition', model='facebook/wav2vec2-base-960h')\n",
    "        result = asr(audio_path)\n",
    "        return result['text']\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Transcription failed: {e}\")\"\"\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        template = function_templates.get(model_info[\"task_type\"], function_templates[\"text-classification\"])\n",
    "        \n",
    "        return {\n",
    "            \"signature\": template[\"signature\"].format(model_name=model_info[\"model_name\"]),\n",
    "            \"docstring\": template[\"docstring\"].format(model_name=model_info[\"model_name\"]),\n",
    "            \"imports\": template[\"imports\"],\n",
    "            \"implementation\": template[\"implementation\"]\n",
    "        }\n",
    "    \n",
    "    def _generate_test_cases(self, model_info: Dict[str, Any]) -> List[TestCase]:\n",
    "        \"\"\"Generate 3 test cases: normal, edge case, correctness\"\"\"\n",
    "        \n",
    "        test_templates = {\n",
    "            \"text-classification\": [\n",
    "                TestCase(\n",
    "                    name=\"test_normal_execution\",\n",
    "                    test_type=\"normal\",\n",
    "                    input_data=\"This is a great product!\",\n",
    "                    expected_output={\"POSITIVE\": 0.95},\n",
    "                    description=\"Test normal text classification\"\n",
    "                ),\n",
    "                TestCase(\n",
    "                    name=\"test_edge_case\",\n",
    "                    test_type=\"edge_case\",\n",
    "                    input_data=\"\",\n",
    "                    expected_output=\"ValueError\",\n",
    "                    description=\"Test empty input handling\"\n",
    "                ),\n",
    "                TestCase(\n",
    "                    name=\"test_correctness\",\n",
    "                    test_type=\"correctness\",\n",
    "                    input_data=\"The movie was terrible and boring.\",\n",
    "                    expected_output={\"NEGATIVE\": 0.92},\n",
    "                    description=\"Test correct sentiment detection\"\n",
    "                )\n",
    "            ],\n",
    "            \"image-classification\": [\n",
    "                TestCase(\n",
    "                    name=\"test_normal_execution\",\n",
    "                    test_type=\"normal\",\n",
    "                    input_data=\"test_image.jpg\",\n",
    "                    expected_output={\"class_281\": 0.89},\n",
    "                    description=\"Test normal image classification\"\n",
    "                ),\n",
    "                TestCase(\n",
    "                    name=\"test_edge_case\",\n",
    "                    test_type=\"edge_case\",\n",
    "                    input_data=\"nonexistent.jpg\",\n",
    "                    expected_output=\"FileNotFoundError\",\n",
    "                    description=\"Test missing file handling\"\n",
    "                ),\n",
    "                TestCase(\n",
    "                    name=\"test_correctness\",\n",
    "                    test_type=\"correctness\",\n",
    "                    input_data=\"cat_image.jpg\",\n",
    "                    expected_output={\"class_285\": 0.94},\n",
    "                    description=\"Test correct cat detection\"\n",
    "                )\n",
    "            ],\n",
    "            \"speech-recognition\": [\n",
    "                TestCase(\n",
    "                    name=\"test_normal_execution\",\n",
    "                    test_type=\"normal\",\n",
    "                    input_data=\"sample_audio.wav\",\n",
    "                    expected_output=\"HELLO WORLD\",\n",
    "                    description=\"Test normal transcription\"\n",
    "                ),\n",
    "                TestCase(\n",
    "                    name=\"test_edge_case\",\n",
    "                    test_type=\"edge_case\",\n",
    "                    input_data=\"missing_audio.wav\",\n",
    "                    expected_output=\"FileNotFoundError\",\n",
    "                    description=\"Test missing audio file\"\n",
    "                ),\n",
    "                TestCase(\n",
    "                    name=\"test_correctness\",\n",
    "                    test_type=\"correctness\",\n",
    "                    input_data=\"speech_sample.wav\",\n",
    "                    expected_output=\"THE QUICK BROWN FOX\",\n",
    "                    description=\"Test accurate transcription\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return test_templates.get(model_info[\"task_type\"], test_templates[\"text-classification\"])\n",
    "    \n",
    "    def _extract_libraries(self, model_info: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Extract required libraries from model info\"\"\"\n",
    "        library_mapping = {\n",
    "            \"huggingface\": [\"transformers\", \"torch\"],\n",
    "            \"pytorch\": [\"torch\", \"torchvision\", \"pillow\"]\n",
    "        }\n",
    "        return library_mapping.get(model_info[\"source_library\"], [\"transformers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Curation và Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkCurator:\n",
    "    \"\"\"Curate and validate benchmark entries following paper's approach\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = []\n",
    "    \n",
    "    def validate_entry(self, entry: BenchmarkEntry) -> Tuple[bool, Dict[str, Any]]:\n",
    "        \"\"\"Validate a benchmark entry through execution\"\"\"\n",
    "        validation_result = {\n",
    "            \"task_id\": entry.task_id,\n",
    "            \"syntax_valid\": False,\n",
    "            \"imports_valid\": False,\n",
    "            \"tests_passed\": 0,\n",
    "            \"total_tests\": len(entry.test_cases),\n",
    "            \"execution_time\": None,\n",
    "            \"errors\": []\n",
    "        }\n",
    "        \n",
    "        # Step 1: Validate syntax\n",
    "        code = entry.to_code_file()\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            validation_result[\"syntax_valid\"] = True\n",
    "        except SyntaxError as e:\n",
    "            validation_result[\"errors\"].append(f\"Syntax error: {e}\")\n",
    "            return False, validation_result\n",
    "        \n",
    "        # Step 2: Check imports (mock validation)\n",
    "        validation_result[\"imports_valid\"] = self._validate_imports(entry.imports)\n",
    "        \n",
    "        # Step 3: Execute tests (mock execution)\n",
    "        import time\n",
    "        import random\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate test execution\n",
    "        for test_case in entry.test_cases:\n",
    "            if random.random() > 0.2:  # 80% success rate\n",
    "                validation_result[\"tests_passed\"] += 1\n",
    "            else:\n",
    "                validation_result[\"errors\"].append(\n",
    "                    f\"Test {test_case.name} failed\"\n",
    "                )\n",
    "        \n",
    "        validation_result[\"execution_time\"] = time.time() - start_time\n",
    "        \n",
    "        # Entry is valid if all tests pass\n",
    "        is_valid = validation_result[\"tests_passed\"] == validation_result[\"total_tests\"]\n",
    "        \n",
    "        return is_valid, validation_result\n",
    "    \n",
    "    def _validate_imports(self, imports: List[str]) -> bool:\n",
    "        \"\"\"Check if imports are valid\"\"\"\n",
    "        # Mock validation - in real implementation would check actual imports\n",
    "        forbidden_imports = [\"os.system\", \"subprocess.call\", \"eval\", \"exec\"]\n",
    "        \n",
    "        for imp in imports:\n",
    "            if any(forbidden in imp for forbidden in forbidden_imports):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def curate_dataset(self, entries: List[BenchmarkEntry], \n",
    "                      min_success_rate: float = 1.0) -> List[BenchmarkEntry]:\n",
    "        \"\"\"Curate dataset by filtering based on validation results\"\"\"\n",
    "        print(f\"\\nCurating {len(entries)} benchmark entries...\")\n",
    "        curated_entries = []\n",
    "        \n",
    "        for entry in entries:\n",
    "            is_valid, result = self.validate_entry(entry)\n",
    "            self.validation_results.append(result)\n",
    "            \n",
    "            success_rate = result[\"tests_passed\"] / result[\"total_tests\"]\n",
    "            \n",
    "            if success_rate >= min_success_rate:\n",
    "                entry.validation_status = \"validated\"\n",
    "                entry.execution_time = result[\"execution_time\"]\n",
    "                curated_entries.append(entry)\n",
    "                print(f\"✓ {entry.task_id}: All tests passed\")\n",
    "            else:\n",
    "                print(f\"✗ {entry.task_id}: {result['tests_passed']}/{result['total_tests']} tests passed\")\n",
    "        \n",
    "        print(f\"\\nCuration complete: {len(curated_entries)}/{len(entries)} entries validated\")\n",
    "        return curated_entries\n",
    "    \n",
    "    def generate_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate statistics about validation results\"\"\"\n",
    "        if not self.validation_results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        stats_df = pd.DataFrame(self.validation_results)\n",
    "        stats_df['success_rate'] = stats_df['tests_passed'] / stats_df['total_tests']\n",
    "        \n",
    "        return stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together: Complete Benchmark Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "collector = DataCollector()\n",
    "generator = BenchmarkGenerator()\n",
    "curator = BenchmarkCurator()\n",
    "\n",
    "# Step 1: Collect raw data\n",
    "print(\"=== Phase 1: Data Collection ===\")\n",
    "raw_data = collector.collect_raw_data()\n",
    "processed_data = collector.preprocess_data(raw_data)\n",
    "print(f\"Collected {len(processed_data)} model entries\")\n",
    "\n",
    "# Step 2: Generate benchmark entries\n",
    "print(\"\\n=== Phase 2: Benchmark Generation ===\")\n",
    "benchmark_entries = []\n",
    "for data in processed_data:\n",
    "    entry = generator.generate_from_model_info(data)\n",
    "    benchmark_entries.append(entry)\n",
    "    print(f\"Generated: {entry.task_id} - {entry.task_description[:50]}...\")\n",
    "\n",
    "# Step 3: Curate and validate\n",
    "print(\"\\n=== Phase 3: Curation & Validation ===\")\n",
    "curated_entries = curator.curate_dataset(benchmark_entries)\n",
    "\n",
    "# Step 4: Generate statistics\n",
    "print(\"\\n=== Phase 4: Statistics ===\")\n",
    "stats_df = curator.generate_statistics()\n",
    "print(\"\\nValidation Statistics:\")\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization và Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark composition\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Domain distribution\n",
    "ax1 = axes[0, 0]\n",
    "domain_counts = pd.Series([e.domain.value for e in benchmark_entries]).value_counts()\n",
    "ax1.pie(domain_counts.values, labels=domain_counts.index, autopct='%1.1f%%')\n",
    "ax1.set_title('Domain Distribution in Generated Benchmark')\n",
    "\n",
    "# 2. Validation success rates\n",
    "ax2 = axes[0, 1]\n",
    "if not stats_df.empty:\n",
    "    ax2.hist(stats_df['success_rate'], bins=5, edgecolor='black', alpha=0.7)\n",
    "    ax2.set_xlabel('Success Rate')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Distribution of Test Success Rates')\n",
    "\n",
    "# 3. Test complexity (lines of code)\n",
    "ax3 = axes[1, 0]\n",
    "code_lengths = [len(e.implementation.split('\\n')) for e in benchmark_entries]\n",
    "ax3.bar(range(len(code_lengths)), code_lengths)\n",
    "ax3.set_xlabel('Benchmark Entry')\n",
    "ax3.set_ylabel('Lines of Code')\n",
    "ax3.set_title('Code Complexity by Entry')\n",
    "\n",
    "# 4. Library usage\n",
    "ax4 = axes[1, 1]\n",
    "all_libs = []\n",
    "for e in benchmark_entries:\n",
    "    all_libs.extend(e.libraries_used)\n",
    "lib_counts = pd.Series(all_libs).value_counts()\n",
    "ax4.bar(lib_counts.index, lib_counts.values)\n",
    "ax4.set_xlabel('Library')\n",
    "ax4.set_ylabel('Usage Count')\n",
    "ax4.set_title('Library Usage Frequency')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Benchmark to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Export one entry as executable Python file\n",
    "if curated_entries:\n",
    "    example_entry = curated_entries[0]\n",
    "    \n",
    "    # Generate complete code file\n",
    "    code_content = example_entry.to_code_file()\n",
    "    \n",
    "    print(\"=== Generated Benchmark Entry ===\")\n",
    "    print(f\"Task ID: {example_entry.task_id}\")\n",
    "    print(f\"Description: {example_entry.task_description}\")\n",
    "    print(f\"Domain: {example_entry.domain.value}\")\n",
    "    print(\"\\n--- Generated Code ---\")\n",
    "    print(code_content[:1000] + \"...\" if len(code_content) > 1000 else code_content)\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = f\"benchmark_{example_entry.task_id}.py\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(code_content)\n",
    "    print(f\"\\nSaved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Benchmark Design Philosophy**:\n",
    "   - Focus on real-world AI tasks với specific libraries\n",
    "   - Structured format inspired by HumanEval\n",
    "   - Comprehensive testing với 3 levels\n",
    "\n",
    "2. **Data Curation Process**:\n",
    "   - Initial generation: ~9,000 files\n",
    "   - After filtering: ~2,000 files (ít nhất 1 test pass)\n",
    "   - Final benchmark: ~500 files (all tests pass)\n",
    "\n",
    "3. **Quality Assurance**:\n",
    "   - Syntax validation\n",
    "   - Import safety checks\n",
    "   - Execution in sandboxed environment\n",
    "   - Multiple test cases per task\n",
    "\n",
    "4. **Practical Applications**:\n",
    "   - Benchmark có thể dùng để train và evaluate LLMs\n",
    "   - Framework này có thể adapt cho domains khác\n",
    "   - Data format standardized để dễ dàng mở rộng"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}