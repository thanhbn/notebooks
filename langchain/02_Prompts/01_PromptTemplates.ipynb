{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. PromptTemplate trong LangChain\n",
    "\n",
    "## Mục tiêu\n",
    "- Hiểu về PromptTemplate và tại sao cần sử dụng\n",
    "- Tạo prompt động với các biến\n",
    "- Sử dụng partial variables\n",
    "- Format và validate prompts\n",
    "- Best practices khi làm việc với prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Giới thiệu PromptTemplate\n",
    "\n",
    "### Tại sao cần PromptTemplate?\n",
    "\n",
    "Thay vì hardcode prompts:\n",
    "```python\n",
    "prompt = f\"Translate '{text}' to {language}\"\n",
    "```\n",
    "\n",
    "PromptTemplate giúp:\n",
    "- **Tái sử dụng**: Dùng lại template cho nhiều inputs\n",
    "- **Quản lý**: Tổ chức prompts một cách có hệ thống\n",
    "- **Validation**: Kiểm tra input variables\n",
    "- **Flexibility**: Dễ dàng thay đổi format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt packages cần thiết\n",
    "!pip install langchain langchain-anthropic langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Cách 1: Tạo từ string template\n",
    "template = \"\"\"You are a helpful assistant. \n",
    "Please answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "# Format prompt với giá trị cụ thể\n",
    "formatted_prompt = prompt.format(question=\"What is LangChain?\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cách 2: Sử dụng from_template (tự động detect variables)\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "    \"Translate the following {source_lang} text to {target_lang}: {text}\"\n",
    ")\n",
    "\n",
    "print(f\"Input variables detected: {prompt2.input_variables}\")\n",
    "\n",
    "# Format với multiple variables\n",
    "result = prompt2.format(\n",
    "    source_lang=\"English\",\n",
    "    target_lang=\"Vietnamese\",\n",
    "    text=\"Hello, how are you?\"\n",
    ")\n",
    "print(f\"\\nFormatted prompt:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Template Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template với multiple lines và complex formatting\n",
    "analysis_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert {role} analyzing {topic}.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Please provide:\n",
    "1. A brief summary\n",
    "2. Key insights\n",
    "3. Recommendations\n",
    "\n",
    "Focus on: {focus_areas}\n",
    "\n",
    "Your analysis:\n",
    "\"\"\")\n",
    "\n",
    "# Format với detailed inputs\n",
    "analysis = analysis_template.format(\n",
    "    role=\"data scientist\",\n",
    "    topic=\"customer churn patterns\",\n",
    "    context=\"We have observed a 15% increase in churn rate last quarter\",\n",
    "    focus_areas=\"retention strategies and early warning signals\"\n",
    ")\n",
    "\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template với conditional logic (using Jinja2 syntax)\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "jinja_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a {{ role }} assistant.\n",
    "{% if detailed %}\n",
    "Please provide a detailed analysis including:\n",
    "- Background information\n",
    "- Step-by-step reasoning\n",
    "- Examples and references\n",
    "{% else %}\n",
    "Please provide a concise answer.\n",
    "{% endif %}\n",
    "\n",
    "Question: {{ question }}\n",
    "\"\"\",\n",
    "    input_variables=[\"role\", \"question\", \"detailed\"],\n",
    "    template_format=\"jinja2\"\n",
    ")\n",
    "\n",
    "# Test với detailed=True\n",
    "print(\"Detailed version:\")\n",
    "print(jinja_template.format(\n",
    "    role=\"technical\",\n",
    "    question=\"Explain REST API\",\n",
    "    detailed=True\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test với detailed=False\n",
    "print(\"Concise version:\")\n",
    "print(jinja_template.format(\n",
    "    role=\"technical\",\n",
    "    question=\"Explain REST API\",\n",
    "    detailed=False\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partial Variables\n",
    "\n",
    "Partial variables cho phép bạn \"điền sẵn\" một số biến trong template, tạo ra template mới với ít biến hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template với nhiều variables\n",
    "base_template = PromptTemplate.from_template(\"\"\"\n",
    "You are a {language} programming expert.\n",
    "Current date: {date}\n",
    "User level: {level}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# Partial với một số biến cố định\n",
    "python_expert = base_template.partial(\n",
    "    language=\"Python\",\n",
    "    date=\"2024-03-15\"\n",
    ")\n",
    "\n",
    "# Bây giờ chỉ cần provide 'level' và 'question'\n",
    "print(\"Partial template input variables:\", python_expert.input_variables)\n",
    "\n",
    "result = python_expert.format(\n",
    "    level=\"intermediate\",\n",
    "    question=\"How do decorators work?\"\n",
    ")\n",
    "print(\"\\nFormatted result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial với functions (dynamic values)\n",
    "from datetime import datetime\n",
    "\n",
    "def get_current_time():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def get_system_info():\n",
    "    return \"LangChain Assistant v2.0\"\n",
    "\n",
    "# Template với dynamic partial values\n",
    "dynamic_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "System: {system_info}\n",
    "Timestamp: {current_time}\n",
    "User: {user_name}\n",
    "\n",
    "Query: {query}\n",
    "\"\"\",\n",
    "    input_variables=[\"user_name\", \"query\"],\n",
    "    partial_variables={\n",
    "        \"system_info\": get_system_info,\n",
    "        \"current_time\": get_current_time\n",
    "    }\n",
    ")\n",
    "\n",
    "# Functions sẽ được gọi khi format\n",
    "print(dynamic_template.format(\n",
    "    user_name=\"Alice\",\n",
    "    query=\"What's the weather today?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Template Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết hợp nhiều templates\n",
    "system_template = PromptTemplate.from_template(\n",
    "    \"You are a {role} assistant specializing in {domain}.\"\n",
    ")\n",
    "\n",
    "instruction_template = PromptTemplate.from_template(\n",
    "    \"Please {action} the following {content_type}: {content}\"\n",
    ")\n",
    "\n",
    "# Tạo template phức tạp từ các phần nhỏ\n",
    "def create_full_prompt(role, domain, action, content_type, content):\n",
    "    system_msg = system_template.format(role=role, domain=domain)\n",
    "    instruction = instruction_template.format(\n",
    "        action=action,\n",
    "        content_type=content_type,\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    return f\"{system_msg}\\n\\n{instruction}\"\n",
    "\n",
    "# Test composition\n",
    "full_prompt = create_full_prompt(\n",
    "    role=\"technical\",\n",
    "    domain=\"software architecture\",\n",
    "    action=\"review and improve\",\n",
    "    content_type=\"system design\",\n",
    "    content=\"Microservices architecture for e-commerce platform\"\n",
    ")\n",
    "\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation và Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template với validation\n",
    "validated_template = PromptTemplate(\n",
    "    template=\"Analyze {metric} for {period} in {region}\",\n",
    "    input_variables=[\"metric\", \"period\", \"region\"],\n",
    "    validate_template=True  # Default là True\n",
    ")\n",
    "\n",
    "# Test missing variable - sẽ raise error\n",
    "try:\n",
    "    result = validated_template.format(\n",
    "        metric=\"sales\",\n",
    "        period=\"Q1 2024\"\n",
    "        # missing 'region'\n",
    "    )\n",
    "except KeyError as e:\n",
    "    print(f\"Validation error: {e}\")\n",
    "\n",
    "# Custom validation function\n",
    "def validate_inputs(**kwargs):\n",
    "    \"\"\"Custom validation logic\"\"\"\n",
    "    if \"temperature\" in kwargs:\n",
    "        temp = kwargs[\"temperature\"]\n",
    "        if not isinstance(temp, (int, float)) or temp < 0 or temp > 2:\n",
    "            raise ValueError(f\"Temperature must be between 0 and 2, got {temp}\")\n",
    "    \n",
    "    if \"max_tokens\" in kwargs:\n",
    "        tokens = kwargs[\"max_tokens\"]\n",
    "        if not isinstance(tokens, int) or tokens < 1:\n",
    "            raise ValueError(f\"max_tokens must be positive integer, got {tokens}\")\n",
    "    \n",
    "    return kwargs\n",
    "\n",
    "# Template với custom validation\n",
    "llm_template = PromptTemplate(\n",
    "    template=\"Generate text with temperature={temperature} and max_tokens={max_tokens}: {prompt}\",\n",
    "    input_variables=[\"temperature\", \"max_tokens\", \"prompt\"]\n",
    ")\n",
    "\n",
    "# Wrap formatting với validation\n",
    "def safe_format(template, **kwargs):\n",
    "    validated_kwargs = validate_inputs(**kwargs)\n",
    "    return template.format(**validated_kwargs)\n",
    "\n",
    "# Test valid inputs\n",
    "try:\n",
    "    result = safe_format(\n",
    "        llm_template,\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "        prompt=\"Write a haiku\"\n",
    "    )\n",
    "    print(\"Valid input - Success!\")\n",
    "    print(result)\n",
    "except ValueError as e:\n",
    "    print(f\"Validation failed: {e}\")\n",
    "\n",
    "# Test invalid inputs\n",
    "try:\n",
    "    result = safe_format(\n",
    "        llm_template,\n",
    "        temperature=3.0,  # Invalid!\n",
    "        max_tokens=100,\n",
    "        prompt=\"Write a haiku\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"\\nValidation failed as expected: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Chain of Thought Prompt\n",
    "cot_template = PromptTemplate.from_template(\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Let's think step by step:\n",
    "1. First, I need to understand what is being asked\n",
    "2. Then, I'll identify the key information needed\n",
    "3. Next, I'll work through the problem systematically\n",
    "4. Finally, I'll provide the answer\n",
    "\n",
    "Step-by-step solution:\n",
    "\"\"\")\n",
    "\n",
    "print(cot_template.format(\n",
    "    question=\"If a train travels 120km in 2 hours, what is its average speed?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Few-shot Learning Template\n",
    "few_shot_template = PromptTemplate.from_template(\"\"\"\n",
    "Task: {task_description}\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Now, {task_action}: {input}\n",
    "Output:\n",
    "\"\"\")\n",
    "\n",
    "# Prepare examples\n",
    "sentiment_examples = \"\"\"\n",
    "Input: \"This movie was fantastic! Best I've seen all year.\"\n",
    "Output: Positive\n",
    "\n",
    "Input: \"Terrible service, would not recommend.\"\n",
    "Output: Negative\n",
    "\n",
    "Input: \"The food was okay, nothing special.\"\n",
    "Output: Neutral\n",
    "\"\"\"\n",
    "\n",
    "# Create sentiment analyzer\n",
    "sentiment_prompt = few_shot_template.partial(\n",
    "    task_description=\"Classify the sentiment of text as Positive, Negative, or Neutral\",\n",
    "    task_action=\"classify the sentiment of\",\n",
    "    examples=sentiment_examples\n",
    ")\n",
    "\n",
    "# Use for new input\n",
    "print(sentiment_prompt.format(\n",
    "    input=\"The product works as expected, good value for money.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Multi-language Support Template\n",
    "class MultilingualPromptTemplate:\n",
    "    def __init__(self):\n",
    "        self.templates = {\n",
    "            \"en\": PromptTemplate.from_template(\n",
    "                \"You are a helpful assistant. Please {action}: {content}\"\n",
    "            ),\n",
    "            \"vi\": PromptTemplate.from_template(\n",
    "                \"Bạn là trợ lý hữu ích. Vui lòng {action}: {content}\"\n",
    "            ),\n",
    "            \"fr\": PromptTemplate.from_template(\n",
    "                \"Vous êtes un assistant utile. Veuillez {action}: {content}\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.action_translations = {\n",
    "            \"en\": {\"summarize\": \"summarize\", \"translate\": \"translate\", \"explain\": \"explain\"},\n",
    "            \"vi\": {\"summarize\": \"tóm tắt\", \"translate\": \"dịch\", \"explain\": \"giải thích\"},\n",
    "            \"fr\": {\"summarize\": \"résumer\", \"translate\": \"traduire\", \"explain\": \"expliquer\"}\n",
    "        }\n",
    "    \n",
    "    def format(self, language, action, content):\n",
    "        if language not in self.templates:\n",
    "            language = \"en\"  # fallback to English\n",
    "        \n",
    "        template = self.templates[language]\n",
    "        localized_action = self.action_translations[language].get(action, action)\n",
    "        \n",
    "        return template.format(action=localized_action, content=content)\n",
    "\n",
    "# Test multilingual templates\n",
    "ml_prompt = MultilingualPromptTemplate()\n",
    "\n",
    "for lang in [\"en\", \"vi\", \"fr\"]:\n",
    "    print(f\"\\n{lang.upper()}:\")\n",
    "    print(ml_prompt.format(\n",
    "        language=lang,\n",
    "        action=\"summarize\",\n",
    "        content=\"The latest AI research paper\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration với LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tích hợp PromptTemplate với Claude\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# Khởi tạo model (giả sử có API key)\n",
    "# llm = ChatAnthropic(\n",
    "#     model=\"claude-3-opus-20240229\",\n",
    "#     anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "# )\n",
    "\n",
    "# Create a code review template\n",
    "code_review_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert {language} developer performing a code review.\n",
    "\n",
    "Please review the following code for:\n",
    "- Code quality and best practices\n",
    "- Potential bugs or issues\n",
    "- Performance considerations\n",
    "- Security concerns\n",
    "\n",
    "Code to review:\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Provide your review in the following format:\n",
    "1. Summary\n",
    "2. Issues found\n",
    "3. Suggestions for improvement\n",
    "4. Security considerations\n",
    "\"\"\")\n",
    "\n",
    "# Example usage\n",
    "python_code = \"\"\"\n",
    "def get_user_data(user_id):\n",
    "    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n",
    "    result = database.execute(query)\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "# Format prompt\n",
    "review_prompt = code_review_template.format(\n",
    "    language=\"Python\",\n",
    "    code=python_code\n",
    ")\n",
    "\n",
    "print(\"Generated prompt for LLM:\")\n",
    "print(review_prompt)\n",
    "\n",
    "# Would send to LLM like this:\n",
    "# response = llm.invoke([HumanMessage(content=review_prompt)])\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### 1. Organize Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tổ chức templates theo chức năng\n",
    "class PromptLibrary:\n",
    "    \"\"\"Centralized prompt management\"\"\"\n",
    "    \n",
    "    # Analysis prompts\n",
    "    ANALYZE_DATA = PromptTemplate.from_template(\n",
    "        \"Analyze the following {data_type} data and provide insights: {data}\"\n",
    "    )\n",
    "    \n",
    "    # Generation prompts\n",
    "    GENERATE_CODE = PromptTemplate.from_template(\n",
    "        \"Generate {language} code that {functionality}. Requirements: {requirements}\"\n",
    "    )\n",
    "    \n",
    "    # Translation prompts\n",
    "    TRANSLATE = PromptTemplate.from_template(\n",
    "        \"Translate from {source_lang} to {target_lang}: {text}\"\n",
    "    )\n",
    "    \n",
    "    @classmethod\n",
    "    def get_prompt(cls, prompt_name: str) -> PromptTemplate:\n",
    "        \"\"\"Get prompt by name\"\"\"\n",
    "        return getattr(cls, prompt_name, None)\n",
    "\n",
    "# Usage\n",
    "prompt = PromptLibrary.GENERATE_CODE\n",
    "result = prompt.format(\n",
    "    language=\"Python\",\n",
    "    functionality=\"sorts a list of dictionaries by a specific key\",\n",
    "    requirements=\"Handle missing keys gracefully, support reverse sorting\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Version Control cho Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class VersionedPromptTemplate:\n",
    "    \"\"\"Prompt template with version control\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.versions: Dict[str, PromptTemplate] = {}\n",
    "        self.current_version: Optional[str] = None\n",
    "    \n",
    "    def add_version(self, version: str, template: PromptTemplate, set_current: bool = True):\n",
    "        \"\"\"Add a new version of the prompt\"\"\"\n",
    "        self.versions[version] = template\n",
    "        if set_current:\n",
    "            self.current_version = version\n",
    "    \n",
    "    def get_version(self, version: Optional[str] = None) -> PromptTemplate:\n",
    "        \"\"\"Get specific version or current version\"\"\"\n",
    "        if version is None:\n",
    "            version = self.current_version\n",
    "        return self.versions.get(version)\n",
    "    \n",
    "    def format(self, version: Optional[str] = None, **kwargs) -> str:\n",
    "        \"\"\"Format the prompt with given version\"\"\"\n",
    "        template = self.get_version(version)\n",
    "        if template is None:\n",
    "            raise ValueError(f\"Version {version} not found\")\n",
    "        return template.format(**kwargs)\n",
    "\n",
    "# Example usage\n",
    "summarizer = VersionedPromptTemplate(\"summarizer\")\n",
    "\n",
    "# Version 1: Simple\n",
    "summarizer.add_version(\n",
    "    \"v1.0\",\n",
    "    PromptTemplate.from_template(\"Summarize: {text}\")\n",
    ")\n",
    "\n",
    "# Version 2: With length control\n",
    "summarizer.add_version(\n",
    "    \"v2.0\",\n",
    "    PromptTemplate.from_template(\n",
    "        \"Summarize the following text in {max_words} words or less: {text}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Version 3: With style\n",
    "summarizer.add_version(\n",
    "    \"v3.0\",\n",
    "    PromptTemplate.from_template(\"\"\"\n",
    "    Summarize the following text in {style} style.\n",
    "    Maximum length: {max_words} words\n",
    "    Focus on: {focus}\n",
    "    \n",
    "    Text: {text}\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Test different versions\n",
    "text = \"LangChain is a framework for developing applications powered by language models...\"\n",
    "\n",
    "print(\"Version 1.0:\")\n",
    "print(summarizer.format(version=\"v1.0\", text=text))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Current version (v3.0):\")\n",
    "print(summarizer.format(\n",
    "    text=text,\n",
    "    style=\"technical\",\n",
    "    max_words=50,\n",
    "    focus=\"key features and benefits\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching formatted prompts\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class CachedPromptTemplate:\n",
    "    \"\"\"PromptTemplate with caching for expensive operations\"\"\"\n",
    "    \n",
    "    def __init__(self, template: PromptTemplate):\n",
    "        self.template = template\n",
    "        self._cache = {}\n",
    "    \n",
    "    def _get_cache_key(self, **kwargs) -> str:\n",
    "        \"\"\"Generate cache key from kwargs\"\"\"\n",
    "        # Sort keys for consistent hashing\n",
    "        sorted_items = sorted(kwargs.items())\n",
    "        key_str = str(sorted_items)\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    def format(self, use_cache: bool = True, **kwargs) -> str:\n",
    "        \"\"\"Format with optional caching\"\"\"\n",
    "        if not use_cache:\n",
    "            return self.template.format(**kwargs)\n",
    "        \n",
    "        cache_key = self._get_cache_key(**kwargs)\n",
    "        \n",
    "        if cache_key in self._cache:\n",
    "            print(f\"Cache hit for key: {cache_key[:8]}...\")\n",
    "            return self._cache[cache_key]\n",
    "        \n",
    "        print(f\"Cache miss for key: {cache_key[:8]}...\")\n",
    "        result = self.template.format(**kwargs)\n",
    "        self._cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the cache\"\"\"\n",
    "        self._cache.clear()\n",
    "\n",
    "# Test caching\n",
    "expensive_template = PromptTemplate.from_template(\n",
    "    \"Process {data} with method {method} and parameters {params}\"\n",
    ")\n",
    "\n",
    "cached_template = CachedPromptTemplate(expensive_template)\n",
    "\n",
    "# First call - cache miss\n",
    "result1 = cached_template.format(\n",
    "    data=\"customer_data\",\n",
    "    method=\"analysis\",\n",
    "    params=\"detailed\"\n",
    ")\n",
    "\n",
    "# Second call with same params - cache hit\n",
    "result2 = cached_template.format(\n",
    "    data=\"customer_data\",\n",
    "    method=\"analysis\",\n",
    "    params=\"detailed\"\n",
    ")\n",
    "\n",
    "# Different params - cache miss\n",
    "result3 = cached_template.format(\n",
    "    data=\"product_data\",\n",
    "    method=\"analysis\",\n",
    "    params=\"summary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng kết\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **PromptTemplate** giúp quản lý prompts một cách có hệ thống\n",
    "2. **Variables** cho phép tái sử dụng templates với different inputs\n",
    "3. **Partial** variables giúp tạo specialized templates\n",
    "4. **Validation** đảm bảo input đúng format\n",
    "5. **Composition** cho phép xây dựng complex prompts từ simple parts\n",
    "\n",
    "### Best Practices:\n",
    "- Organize prompts trong classes hoặc modules\n",
    "- Version control cho prompts quan trọng\n",
    "- Cache khi cần performance\n",
    "- Validate inputs trước khi format\n",
    "- Use meaningful variable names\n",
    "\n",
    "### Next Steps:\n",
    "- Explore ChatPromptTemplate cho conversations\n",
    "- Learn about FewShotPromptTemplate\n",
    "- Integrate với LangChain chains và agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}