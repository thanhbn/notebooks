# 2308.10462v3.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT  
# ÄÆ°á»ng dáº«n nguá»“n: D:\llm\notebooks\AI-Papers\2308.10462v3.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1063509 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho
Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n
MARTIN WEYSSOWâˆ—, DIRO, Äáº¡i há»c Montreal, Canada
XIN ZHOU, Äáº¡i há»c Quáº£n lÃ½ Singapore, Singapore
KISUB KIM, Äáº¡i há»c Quáº£n lÃ½ Singapore, Singapore
DAVID LO, Äáº¡i há»c Quáº£n lÃ½ Singapore, Singapore
HOUARI SAHRAOUI, DIRO, Äáº¡i há»c Montreal, Canada

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) thá»ƒ hiá»‡n kháº£ nÄƒng áº¥n tÆ°á»£ng trong viá»‡c sinh ra cÃ¡c Ä‘oáº¡n mÃ£ chÃ­nh xÃ¡c khi Ä‘Æ°á»£c cung cáº¥p Ã½ Ä‘á»‹nh báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn theo phÆ°Æ¡ng thá»©c zero-shot, tá»©c lÃ  khÃ´ng cáº§n tinh chá»‰nh cá»¥ thá»ƒ. Máº·c dÃ¹ cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ ná»•i báº­t nhá»¯ng Æ°u Ä‘iá»ƒm cá»§a viá»‡c tinh chá»‰nh LLMs, quÃ¡ trÃ¬nh nÃ y phÃ¡t sinh chi phÃ­ tÃ­nh toÃ¡n cao, khiáº¿n nÃ³ trá»Ÿ nÃªn khÃ´ng thá»±c táº¿ trong mÃ´i trÆ°á»ng khan hiáº¿m tÃ i nguyÃªn, Ä‘áº·c biá»‡t lÃ  Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ³ hÃ ng tá»· tham sá»‘. Äá»ƒ giáº£i quyáº¿t nhá»¯ng thÃ¡ch thá»©c nÃ y, nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ khÃ¡m phÃ¡ há»c trong ngá»¯ cáº£nh (ICL) vÃ  sinh tÄƒng cÆ°á»ng truy xuáº¥t (RAG) nhÆ° cÃ¡c chiáº¿n lÆ°á»£c Ä‘á»ƒ hÆ°á»›ng dáº«n quÃ¡ trÃ¬nh sinh cá»§a LLM vá»›i cÃ¡c vÃ­ dá»¥ prompt cá»¥ thá»ƒ cho tÃ¡c vá»¥. Tuy nhiÃªn, ICL vÃ  RAG gÃ¢y ra nhá»¯ng báº¥t tiá»‡n, cháº³ng háº¡n nhÆ° nhu cáº§u thiáº¿t káº¿ cÃ¡c prompt phÃ¹ há»£p vá»›i ngá»¯ cáº£nh vÃ  viá»‡c thiáº¿u há»c cÃ¡c tham sá»‘ cá»¥ thá»ƒ cho tÃ¡c vá»¥, do Ä‘Ã³ háº¡n cháº¿ hiá»‡u suáº¥t tÃ¡c vá»¥ downstream. Trong bá»‘i cáº£nh nÃ y, chÃºng tÃ´i dá»± bÃ¡o tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ (PEFT) nhÆ° má»™t phÆ°Æ¡ng phÃ¡p tiá»m nÄƒng Ä‘á»ƒ chuyÃªn biá»‡t hÃ³a hiá»‡u quáº£ cÃ¡c LLMs cho dá»¯ liá»‡u cá»¥ thá»ƒ tÃ¡c vá»¥ trong khi duy trÃ¬ má»©c tiÃªu thá»¥ tÃ i nguyÃªn há»£p lÃ½. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i thá»±c hiá»‡n má»™t nghiÃªn cá»©u toÃ n diá»‡n vá» cÃ¡c ká»¹ thuáº­t PEFT cho LLMs trong bá»‘i cáº£nh sinh mÃ£ tá»± Ä‘á»™ng. Cuá»™c Ä‘iá»u tra toÃ n diá»‡n cá»§a chÃºng tÃ´i vá» cÃ¡c ká»¹ thuáº­t PEFT cho LLMs tiáº¿t lá»™ tÃ­nh Æ°u viá»‡t vÃ  tiá»m nÄƒng cá»§a chÃºng so vá»›i ICL vÃ  RAG trÃªn má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c LLMs vÃ  ba bá»™ dá»¯ liá»‡u sinh mÃ£ Python Ä‘áº¡i diá»‡n: Conala, CodeAlpacaPy, vÃ  APPS. HÆ¡n ná»¯a, nghiÃªn cá»©u cá»§a chÃºng tÃ´i ná»•i báº­t tiá»m nÄƒng cho viá»‡c tinh chá»‰nh cÃ¡c LLMs lá»›n hÆ¡n vÃ  giáº£m Ä‘Ã¡ng ká»ƒ viá»‡c sá»­ dá»¥ng bá»™ nhá»› báº±ng cÃ¡ch káº¿t há»£p PEFT vá»›i lÆ°á»£ng tá»­ hÃ³a. Do Ä‘Ã³, nghiÃªn cá»©u nÃ y má»Ÿ ra cÆ¡ há»™i cho cÃ¡c á»©ng dá»¥ng rá»™ng hÆ¡n cá»§a PEFT trong cÃ¡c ká»‹ch báº£n ká»¹ thuáº­t pháº§n má»m.

CCS Concepts: â€¢Software and its engineering â†’Software creation and management ;Software development techniques .

Additional Key Words and Phrases: sinh mÃ£, mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, tinh chá»‰nh hiá»‡u quáº£ tham sá»‘, lÆ°á»£ng tá»­ hÃ³a, nghiÃªn cá»©u thá»±c nghiá»‡m

âˆ—TÃ¡c giáº£ liÃªn há»‡.
Äá»‹a chá»‰ tÃ¡c giáº£: Martin Weyssow, martin.weyssow@umontreal.ca, DIRO, Äáº¡i há»c Montreal, Canada; Xin Zhou, xinzhou.2020@phdcs.smu.edu.sg, Äáº¡i há»c Quáº£n lÃ½ Singapore, Singapore; Kisub Kim, falconlk00@gmail.com, Äáº¡i há»c Quáº£n lÃ½ Singapore, Singapore; David Lo, davidlo@smu.edu.sg, Äáº¡i há»c Quáº£n lÃ½ Singapore, Singapore; Houari Sahraoui, sahraouh@iro.umontreal.ca, DIRO, Äáº¡i há»c Montreal, Canada.

Quyá»n táº¡o báº£n sao ká»¹ thuáº­t sá»‘ hoáº·c báº£n cá»©ng cá»§a toÃ n bá»™ hoáº·c má»™t pháº§n cÃ´ng trÃ¬nh nÃ y cho viá»‡c sá»­ dá»¥ng cÃ¡ nhÃ¢n hoáº·c lá»›p há»c Ä‘Æ°á»£c cáº¥p mÃ  khÃ´ng máº¥t phÃ­ vá»›i Ä‘iá»u kiá»‡n cÃ¡c báº£n sao khÃ´ng Ä‘Æ°á»£c táº¡o hoáº·c phÃ¢n phá»‘i vÃ¬ lá»£i nhuáº­n hoáº·c lá»£i tháº¿ thÆ°Æ¡ng máº¡i vÃ  cÃ¡c báº£n sao mang thÃ´ng bÃ¡o nÃ y vÃ  trÃ­ch dáº«n Ä‘áº§y Ä‘á»§ trÃªn trang Ä‘áº§u tiÃªn. Báº£n quyá»n cho cÃ¡c thÃ nh pháº§n cá»§a cÃ´ng trÃ¬nh nÃ y thuá»™c sá»Ÿ há»¯u cá»§a nhá»¯ng ngÆ°á»i khÃ¡c ngoÃ i (cÃ¡c) tÃ¡c giáº£ pháº£i Ä‘Æ°á»£c tÃ´n trá»ng. TÃ³m táº¯t cÃ³ ghi cÃ´ng nguá»“n Ä‘Æ°á»£c cho phÃ©p. Äá»ƒ sao chÃ©p khÃ¡c, hoáº·c tÃ¡i xuáº¥t báº£n, Ä‘á»ƒ Ä‘Äƒng trÃªn mÃ¡y chá»§ hoáº·c Ä‘á»ƒ phÃ¢n phá»‘i láº¡i cho danh sÃ¡ch, yÃªu cáº§u quyá»n cá»¥ thá»ƒ trÆ°á»›c vÃ /hoáº·c phÃ­. YÃªu cáº§u quyá»n tá»« permissions@acm.org.

Â©2024 Báº£n quyá»n thuá»™c vá» (cÃ¡c) chá»§ sá»Ÿ há»¯u/tÃ¡c giáº£. Quyá»n xuáº¥t báº£n Ä‘Æ°á»£c cáº¥p phÃ©p cho ACM.
ACM XXXX-XXXX/2024/12-ART
https://doi.org/10.1145/nnnnnnn.nnnnnnn
, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.arXiv:2308.10462v3  [cs.SE]  27 Dec 2024

--- TRANG 2 ---
2â€¢M. Weyssow et al.
ACM Reference Format:
Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. 2024. Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models. 1, 1 (December 2024), 27 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 GIá»šI THIá»†U
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs) dá»±a trÃªn kiáº¿n trÃºc Transformer [67], thá»ƒ hiá»‡n tiá»m nÄƒng Ä‘Ã¡ng ká»ƒ trong cÃ¡c lÄ©nh vá»±c Ä‘a dáº¡ng, bao gá»“m xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) [29,44,76], thá»‹ giÃ¡c mÃ¡y tÃ­nh [7,59,85], vÃ  ká»¹ thuáº­t pháº§n má»m [9,66,82]. CÃ¡c mÃ´ hÃ¬nh nÃ y xuáº¥t sáº¯c trong viá»‡c sinh ná»™i dung cháº¥t lÆ°á»£ng cao khi Ä‘Æ°á»£c cung cáº¥p Ã½ Ä‘á»‹nh báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn theo phÆ°Æ¡ng thá»©c zero-shot, tá»©c lÃ  khÃ´ng cáº§n tinh chá»‰nh. Kháº£ nÄƒng nÃ y Ä‘Ã£ khÆ¡i dáº­y sá»± quan tÃ¢m Ä‘Ã¡ng ká»ƒ trong lÄ©nh vá»±c ká»¹ thuáº­t pháº§n má»m Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n mÃ£ nhÆ° sá»­a chá»¯a chÆ°Æ¡ng trÃ¬nh [27, 80, 81] vÃ  sinh mÃ£ [3, 9, 48].

Máº·c dÃ¹ kháº£ nÄƒng zero-shot cá»§a LLMs ráº¥t áº¥n tÆ°á»£ng, tiá»m nÄƒng Ä‘áº§y Ä‘á»§ cá»§a chÃºng thÆ°á»ng xuáº¥t hiá»‡n thÃ´ng qua tinh chá»‰nh [54,75]. Cá»¥ thá»ƒ, viá»‡c tinh chá»‰nh má»™t LLM cho dá»¯ liá»‡u cá»¥ thá»ƒ tÃ¡c vá»¥ cho phÃ©p nÃ³ há»c vÃ  mÃ£ hÃ³a kiáº¿n thá»©c vá» dá»¯ liá»‡u cÃ³ kháº£ nÄƒng cÃ³ ngá»¯ cáº£nh cao vÃ  do Ä‘Ã³ sinh ra ná»™i dung cÃ³ Ã½ nghÄ©a hÆ¡n. Tuy nhiÃªn, quÃ¡ trÃ¬nh nÃ y Ä‘i kÃ¨m vá»›i chi phÃ­ tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ. Tinh chá»‰nh Ä‘áº§y Ä‘á»§, nÆ¡i táº¥t cáº£ cÃ¡c tham sá»‘ cá»§a LLMs Ä‘Æ°á»£c cáº­p nháº­t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, Ä‘Ã²i há»i tÃ i nguyÃªn tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ, Ä‘áº·c biá»‡t khi LLM chá»©a hÃ ng tá»· tham sá»‘ [62]. Äá»ƒ giáº£m thiá»ƒu gÃ¡nh náº·ng tÃ­nh toÃ¡n nÃ y, cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y trong ká»¹ thuáº­t pháº§n má»m [53,80,91] Ä‘Ã£ Ä‘iá»u tra cÃ¡c ká»¹ thuáº­t ká»¹ thuáº­t prompt nhÆ° Há»c trong Ngá»¯ cáº£nh (ICL) [5,54] vÃ  Sinh TÄƒng cÆ°á»ng Truy xuáº¥t (RAG) [31]. ICL bao gá»“m viá»‡c cung cáº¥p cÃ¡c vÃ­ dá»¥ prompt cá»§a tÃ¡c vá»¥ cho LLM, hÆ°á»›ng dáº«n nÃ³ sinh ná»™i dung phÃ¹ há»£p vá»›i ngá»¯ cáº£nh mÃ  khÃ´ng cÃ³ báº¥t ká»³ tinh chá»‰nh nÃ o liÃªn quan. CÃ¡c vÃ­ dá»¥ nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o thá»§ cÃ´ng hoáº·c chá»n ngáº«u nhiÃªn tá»« má»™t bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n cÃ³ liÃªn quan. Ká»¹ thuáº­t nÃ y Ä‘Ã£ cho tháº¥y káº¿t quáº£ Ä‘áº§y há»©a háº¹n cho cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n mÃ£, bao gá»“m sá»­a chá»¯a chÆ°Æ¡ng trÃ¬nh tá»± Ä‘á»™ng [80], sá»­a lá»—i [53], vÃ  sinh mÃ£ [61,73,91].

Má»Ÿ rá»™ng tá»« ICL, RAG cung cáº¥p má»™t lá»±a chá»n thay tháº¿ máº¡nh máº½ vÃ  hiá»‡u quáº£ hÆ¡n báº±ng cÃ¡ch káº¿t há»£p má»™t há»‡ thá»‘ng truy xuáº¥t kiáº¿n thá»©c táº¡i thá»i Ä‘iá»ƒm suy luáº­n. Sá»­ dá»¥ng RAG, má»™t mÃ´ hÃ¬nh truy xuáº¥t tÃ¬m kiáº¿m thÃ´ng tin cÃ³ liÃªn quan tá»« má»™t kho tÃ i liá»‡u Ä‘Æ°á»£c láº­p chá»‰ má»¥c, cháº³ng háº¡n nhÆ° tÃ i liá»‡u mÃ£ hoáº·c cÃ¡c Ä‘oáº¡n mÃ£ tÆ°Æ¡ng tá»± vá»›i váº¥n Ä‘á» Ä‘áº§u vÃ o. ThÃ´ng tin Ä‘Æ°á»£c truy xuáº¥t sau Ä‘Ã³ Ä‘Æ°á»£c thÃªm vÃ o prompt Ä‘áº§u vÃ o Ä‘á»ƒ hÆ°á»›ng dáº«n sinh. KhÃ´ng giá»‘ng nhÆ° ICL, dá»±a vÃ o cÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c chá»n trÆ°á»›c cÃ³ thá»ƒ khÃ´ng phÃ¹ há»£p vá»›i Ä‘áº§u vÃ o cá»¥ thá»ƒ, RAG thÃ­ch á»©ng Ä‘á»™ng vá»›i tá»«ng váº¥n Ä‘á» Ä‘áº§u vÃ o riÃªng láº», cung cáº¥p ngá»¯ cáº£nh liÃªn quan hÆ¡n. Ká»¹ thuáº­t nÃ y Ä‘Ã£ thá»ƒ hiá»‡n nhá»¯ng cáº£i tiáº¿n Ä‘Ã¡ng ká»ƒ trong cÃ¡c tÃ¡c vá»¥ ká»¹ thuáº­t pháº§n má»m nhÆ° sinh mÃ£ vÃ  tÃ³m táº¯t [39,52,91], hoÃ n thiá»‡n mÃ£ [41], vÃ  sá»­a chá»¯a chÆ°Æ¡ng trÃ¬nh [70].

Máº·c dÃ¹ ICL vÃ  RAG cung cáº¥p má»™t lá»±a chá»n thay tháº¿ kháº£ thi cho tinh chá»‰nh Ä‘áº§y Ä‘á»§, nÃ³ hoáº¡t Ä‘á»™ng táº¡i thá»i Ä‘iá»ƒm suy luáº­n vÃ  khÃ´ng liÃªn quan Ä‘áº¿n viá»‡c há»c cÃ¡c tham sá»‘ cá»¥ thá»ƒ tÃ¡c vá»¥, Ä‘iá»u nÃ y cÃ³ thá»ƒ ngÄƒn cáº£n LLM khá»i viá»‡c náº¯m báº¯t thÃ´ng tin chi tiáº¿t vá» tÃ¡c vá»¥ vÃ  dáº«n Ä‘áº¿n máº¥t hiá»‡u quáº£. Trong bá»‘i cáº£nh nÃ y, cÃ¡c ká»¹ thuáº­t Tinh chá»‰nh Hiá»‡u quáº£ Tham sá»‘ (PEFT) Ä‘Ã£ xuáº¥t hiá»‡n nhÆ° cÃ¡c giáº£i phÃ¡p Ä‘áº§y há»©a háº¹n Ä‘á»ƒ lÃ m cho chi phÃ­ tinh chá»‰nh á»Ÿ má»©c tháº¥p nháº¥t trong khi cho phÃ©p mÃ´ hÃ¬nh há»c cÃ¡c tham sá»‘ cá»¥ thá»ƒ tÃ¡c vá»¥. CÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y [10,57,68,69] trong trÃ­ tuá»‡ mÃ£ Ä‘Ã£ chá»©ng minh kháº£ nÄƒng cá»§a cÃ¡c ká»¹ thuáº­t PEFT, vÃ  thÆ°á»ng cho tháº¥y tÃ­nh Æ°u viá»‡t cá»§a chÃºng so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ trÃªn má»™t loáº¡t rá»™ng cÃ¡c tÃ¡c vá»¥. Tuy nhiÃªn, cÃ¡c nghiÃªn cá»©u nÃ y táº­p trung vÃ o cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhá» (SLMs) (<0.25B tham sá»‘) nhÆ° CodeBERT [16] vÃ  CodeT5 [72] vÃ  bá» qua kháº£ nÄƒng Ã¡p dá»¥ng cá»§a cÃ¡c ká»¹ thuáº­t PEFT cho LLMs (â‰¥1B tham sá»‘), Ä‘á»ƒ láº¡i má»™t khoáº£ng trá»‘ng nghiÃªn cá»©u quan trá»ng. Vá»›i sá»± phá»• biáº¿n ngÃ y cÃ ng tÄƒng cá»§a LLMs, chÃºng tÃ´i tin ráº±ng viá»‡c giáº£i quyáº¿t khoáº£ng trá»‘ng nÃ y lÃ  tá»‘i quan trá»ng trong viá»‡c thÃºc Ä‘áº©y lÄ©nh vá»±c trÃ­ tuá»‡ mÃ£ vÃ  khai thÃ¡c Ä‘áº§y Ä‘á»§ tiá»m nÄƒng cá»§a LLMs. HÆ¡n ná»¯a, chÃºng tÃ´i xÃ¡c Ä‘á»‹nh má»™t cÆ¡ há»™i nghiÃªn cá»©u bá»• sung trong viá»‡c khÃ¡m phÃ¡ viá»‡c sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT trong cÃ¡c ká»‹ch báº£n tÃ i nguyÃªn háº¡n cháº¿, nháº±m chá»©ng minh viá»‡c dÃ¢n chá»§ hÃ³a viá»‡c tinh chá»‰nh LLMs thÃ´ng qua PEFT. Viá»‡c giáº£i quyáº¿t nhá»¯ng khoáº£ng trá»‘ng nÃ y sáº½ khÃ´ng chá»‰ cho tháº¥y cÃ¡ch cÃ¡c ká»¹ thuáº­t PEFT cÃ³ thá»ƒ nÃ¢ng cao hiá»‡u quáº£ cá»§a LLMs mÃ  cÃ²n cÃ¡ch chÃºng má»Ÿ rá»™ng kháº£ nÄƒng tiáº¿p cáº­n vÃ  tiá»‡n Ã­ch cá»§a LLMs trong cÃ¡c cÃ i Ä‘áº·t tÃ­nh toÃ¡n khan hiáº¿m vÃ  giáº£m bá»›t sá»± phá»¥ thuá»™c cá»§a cÃ¡c nhÃ  thá»±c hÃ nh vÃ o cÆ¡ sá»Ÿ háº¡ táº§ng tÃ­nh toÃ¡n lá»›n.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 3 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢3

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y má»™t nghiÃªn cá»©u thá»±c nghiá»‡m vá» viá»‡c sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT hiá»‡n cÃ³ vá»›i LLMs. ChÃºng tÃ´i táº­p trung nghiÃªn cá»©u vÃ o sinh mÃ£, Ä‘Ã¢y lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u then chá»‘t do tÃ¡c Ä‘á»™ng biáº¿n Ä‘á»•i cá»§a nÃ³ trong viá»‡c tá»± Ä‘á»™ng hÃ³a phÃ¡t triá»ƒn pháº§n má»m [9,48,50]. Má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  hai máº·t. Thá»© nháº¥t, chÃºng tÃ´i nháº±m Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng sinh mÃ£ cá»§a LLMs sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT hiá»‡n cÃ³ nhÆ° LoRA [24] vÃ  QLoRA [13] trÃªn cÃ¡c bá»™ dá»¯ liá»‡u khÃ´ng cÃ³ test cases, bao gá»“m Conala [91] vÃ  CodeAlpacaPy [8], cÅ©ng nhÆ° bá»™ dá»¯ liá»‡u APPS [22] cÃ³ test cases. Thá»© hai, chÃºng tÃ´i tÃ¬m cÃ¡ch so sÃ¡nh hiá»‡u quáº£ cá»§a LLMs Ä‘Æ°á»£c tinh chá»‰nh vá»›i cÃ¡c ká»¹ thuáº­t PEFT nÃ y so vá»›i SLMs, ICL, vÃ  RAG. NgoÃ i ra, chÃºng tÃ´i tiáº¿n hÃ nh nghiÃªn cá»©u so sÃ¡nh vá»›i tÃ¬nh tráº¡ng háº¡n cháº¿ vá» tÃ i nguyÃªn tÃ­nh toÃ¡n Ä‘á»ƒ Ä‘iá»u tra tÃ­nh thá»±c táº¿ rá»™ng rÃ£i cá»§a viá»‡c sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT cho LLMs. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng má»¥c tiÃªu nÃ y, chÃºng tÃ´i xÃ¢y dá»±ng bá»‘n cÃ¢u há»i nghiÃªn cá»©u hÆ°á»›ng dáº«n nghiÃªn cá»©u cá»§a chÃºng tÃ´i:

â€“ RQ1: LLMs vÃ  SLMs hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o khi sá»­ dá»¥ng ICL trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy?
â€“RQ2: LLMs vÃ  SLMs hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o khi sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy?
â€“ RQ3: LoRA so sÃ¡nh nhÆ° tháº¿ nÃ o vá»›i ICL vÃ  RAG trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy?
â€“RQ4: ChÃºng ta cÃ³ thá»ƒ nÃ¢ng cao hiá»‡u quáº£ cá»§a LLMs cho sinh mÃ£ trong bá»™ dá»¯ liá»‡u APPS báº±ng cÃ¡ch sá»­ dá»¥ng LoRA vÃ  QLoRA khÃ´ng?

Tá»•ng há»£p láº¡i, viá»‡c tráº£ lá»i bá»‘n cÃ¢u há»i nghiÃªn cá»©u nÃ y hoÃ n thÃ nh cáº£ hai má»¥c tiÃªu cá»§a nghiÃªn cá»©u thá»±c nghiá»‡m nÃ y. Ba RQ Ä‘áº§u tiÃªn cá»§a chÃºng tÃ´i táº­p trung vÃ o viá»‡c Ä‘Ã¡nh giÃ¡ SLMs vÃ  LLMs cho sinh mÃ£ trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpaca. Trong RQ1, chÃºng tÃ´i minh há»a hiá»‡u quáº£ cÆ¡ sá»Ÿ cá»§a SLMs vÃ  LLMs sá»­ dá»¥ng ICL, Ä‘iá»u nÃ y truy xuáº¥t cÃ¡c vÃ­ dá»¥ ngáº«u nhiÃªn tá»« táº­p huáº¥n luyá»‡n Ä‘á»ƒ hÆ°á»›ng dáº«n mÃ´ hÃ¬nh trong viá»‡c sinh mÃ£. Báº±ng cÃ¡ch giáº£i quyáº¿t RQ2, chÃºng tÃ´i cÃ³ Ä‘Æ°á»£c sá»± hiá»ƒu biáº¿t toÃ n diá»‡n vá» hiá»‡u quáº£ cá»§a SLMs vÃ  LLMs khi sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT khÃ¡c nhau. Trong RQ3, chÃºng tÃ´i tiáº¿n hÃ nh má»™t nghiÃªn cá»©u so sÃ¡nh vá» hiá»‡u quáº£ cá»§a LoRA vá»›i ICL vÃ  RAG, má»™t baseline máº¡nh máº½ truy xuáº¥t Ä‘á»™ng cÃ¡c vÃ­ dá»¥ cÃ³ liÃªn quan báº±ng cÃ¡ch chá»n nhá»¯ng vÃ­ dá»¥ gáº§n nháº¥t vá»›i cÃ¡c hÆ°á»›ng dáº«n kiá»ƒm tra tá»« táº­p huáº¥n luyá»‡n. Cuá»‘i cÃ¹ng, Ä‘á»ƒ thá»ƒ hiá»‡n tÃ¡c Ä‘á»™ng rá»™ng hÆ¡n tiá»m nÄƒng cá»§a PEFT, chÃºng tÃ´i nghiÃªn cá»©u trong RQ4 liá»‡u viá»‡c tinh chá»‰nh LLMs báº±ng LoRA vÃ  QLoRA cÃ³ thá»ƒ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a chÃºng trÃªn APPS, má»™t benchmark thÃ¡ch thá»©c vá»›i test cases.

Äá»ƒ giáº£i quyáº¿t nhá»¯ng RQ nÃ y, chÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m trÃªn ba bá»™ dá»¯ liá»‡u, APPS [22], Conala [86], vÃ  CodeAlpacaPy Ä‘Æ°á»£c tuyá»ƒn chá»n cá»¥ thá»ƒ tá»« CodeAlpaca [8] cho sinh mÃ£ Python. NgÆ°á»£c láº¡i vá»›i cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ nhÆ° HumanEval [9], cÃ¡c bá»™ dá»¯ liá»‡u APPS, Conala vÃ  CodeAlpaca, Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c nghiÃªn cá»©u sinh mÃ£ trÆ°á»›c Ä‘Ã¢y [49,71,73,73,88,91], bao gá»“m Ä‘á»§ vÃ­ dá»¥ huáº¥n luyá»‡n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng cho tinh chá»‰nh. Äá»ƒ phÃ¢n tÃ­ch so sÃ¡nh toÃ n diá»‡n, chÃºng tÃ´i chá»n bá»‘n há» mÃ´ hÃ¬nh khÃ¡c biá»‡t: CodeT5+ [71], CodeGen [48], CodeGen2 [47], vÃ  CodeLlama [56], bao gá»“m tÃ¡m biáº¿n thá»ƒ lá»›n vÃ  ba biáº¿n thá»ƒ nhá». LÆ°u Ã½ ráº±ng chÃºng tÃ´i bá» qua cÃ¡c LLMs nguá»“n Ä‘Ã³ng nhÆ° Codex do khÃ´ng thá»ƒ tiáº¿p cáº­n cÃ¡c tham sá»‘ cá»§a chÃºng, Ä‘iá»u nÃ y lÃ m cho viá»‡c nghiÃªn cá»©u báº¥t ká»³ ká»¹ thuáº­t tinh chá»‰nh nÃ o trá»Ÿ nÃªn khÃ´ng kháº£ thi. HÆ¡n ná»¯a, nghiÃªn cá»©u cá»§a chÃºng tÃ´i káº¿t há»£p sÃ¡u ká»¹ thuáº­t PEFT: LoRA [24], IA3 [37], Prompt tuning [30], vÃ  Prefix tuning [33]. NgoÃ i ra, chÃºng tÃ´i khÃ¡m phÃ¡ QLoRA [13] vá»›i lÆ°á»£ng tá»­ hÃ³a 8-bit vÃ  4-bit, káº¿t há»£p LoRA vÃ  lÆ°á»£ng tá»­ hÃ³a mÃ´ hÃ¬nh. KhÃ´ng giá»‘ng nhÆ° ICL vÃ  RAG, cÃ¡c ká»¹ thuáº­t nÃ y Ä‘Ã²i há»i viá»‡c há»c cÃ¡c tham sá»‘ má»›i Ä‘á»ƒ tinh chá»‰nh LLMs cho tÃ¡c vá»¥ downstream cá»¥ thá»ƒ. CÃ¡c phÃ¡t hiá»‡n chÃ­nh cá»§a chÃºng tÃ´i nhÆ° sau:

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 4 ---
4â€¢M. Weyssow et al.

â€“ICL cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh so vá»›i prompt zero-shot cho sinh mÃ£ trÃªn Conala vÃ  CodeAlpacaPy.
â€“TÄƒng sá»‘ lÆ°á»£ng vÃ­ dá»¥ ICL khÃ´ng pháº£i lÃºc nÃ o cÅ©ng dáº«n Ä‘áº¿n cáº£i thiá»‡n hiá»‡u quáº£. CÃ¡c mÃ´ hÃ¬nh Ä‘áº¡t hiá»‡u quáº£ Ä‘á»‰nh vá»›i tÃ¡m vÃ  bá»‘n vÃ­ dá»¥ cho Conala vÃ  CodeAlpacaPy, tÆ°Æ¡ng á»©ng.
â€“LLMs Ä‘Æ°á»£c tinh chá»‰nh vá»›i LoRA, IA3, vÃ  Prompt tuning, tá»©c lÃ  vÃ i triá»‡u tham sá»‘, luÃ´n vÆ°á»£t trá»™i hÆ¡n SLMs Ä‘Æ°á»£c tinh chá»‰nh Ä‘áº§y Ä‘á»§ vá»›i hÃ ng trÄƒm triá»‡u tham sá»‘.
â€“ Trong sá»‘ cÃ¡c ká»¹ thuáº­t PEFT, LoRA Ä‘áº¡t hiá»‡u quáº£ cao nháº¥t cho LLMs vÃ  SLMs.
â€“QLoRA giáº£m Ä‘Ã¡ng ká»ƒ viá»‡c sá»­ dá»¥ng bá»™ nhá»›, Ä‘áº¡t Ä‘Æ°á»£c giáº£m lÃªn Ä‘áº¿n 2 láº§n so vá»›i LoRA trong khi cáº£i thiá»‡n hoáº·c báº£o toÃ n hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh. HÆ¡n ná»¯a, QLoRA cho phÃ©p tinh chá»‰nh LLMs lÃªn Ä‘áº¿n 34B tham sá»‘ vá»›i Ã­t hÆ¡n 24GB bá»™ nhá»› GPU.
â€“LoRA nÃ¢ng cao Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh so vá»›i ICL vÃ  RAG cho sinh mÃ£ trÃªn Conala vÃ  CodeAlpacaPy.
â€“ LoRA vÃ  QLoRA cáº£i thiá»‡n hiá»‡u quáº£ cá»§a CodeLlama-7B-Instruct cho sinh mÃ£ trÃªn bá»™ dá»¯ liá»‡u APPS.

NghiÃªn cá»©u cá»§a chÃºng tÃ´i lÃ m sÃ¡ng tá» nhá»¯ng cÆ¡ há»™i Ä‘áº§y há»©a háº¹n mÃ  cÃ¡c ká»¹ thuáº­t PEFT náº¯m giá»¯, Ä‘áº£m báº£o khÃ¡m phÃ¡ thÃªm cho á»©ng dá»¥ng cá»§a chÃºng trong cÃ¡c tÃ¡c vá»¥ vÃ  ká»‹ch báº£n liÃªn quan Ä‘áº¿n mÃ£ khÃ¡c.

Äá»ƒ tÃ³m táº¯t, Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i nhÆ° sau:
â€“ChÃºng tÃ´i tiáº¿n hÃ nh má»™t nghiÃªn cá»©u thá»±c nghiá»‡m toÃ n diá»‡n vá» sÃ¡u ká»¹ thuáº­t PEFT, tá»©c lÃ  LoRA, IA3, Prompt tuning, Prefix tuning, QLoRA-8bit, vÃ  QLoRA-4bit, cho sinh mÃ£ Python trÃªn má»™t loáº¡t rá»™ng SLMs vÃ  LLMs.
â€“Má»™t so sÃ¡nh vÃ  phÃ¢n tÃ­ch toÃ n diá»‡n vá» cÃ¡c ká»¹ thuáº­t PEFT so vá»›i ICL vÃ  RAG cho LLMs trÃªn sinh mÃ£.
â€“ChÃºng tÃ´i chá»©ng minh tÃ­nh thá»±c táº¿ cá»§a viá»‡c táº­n dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT Ä‘á»ƒ tinh chá»‰nh hiá»‡u quáº£ LLMs vá» mÃ£ vÃ  giáº£m gÃ¡nh náº·ng tÃ­nh toÃ¡n liÃªn quan Ä‘áº¿n tinh chá»‰nh Ä‘áº§y Ä‘á»§, thá»ƒ hiá»‡n cÃ¡c á»©ng dá»¥ng rá»™ng hÆ¡n tiá»m nÄƒng cá»§a chÃºng trong ká»¹ thuáº­t pháº§n má»m.

2 KIáº¾N THá»¨C Ná»€N Táº¢NG
2.1 Há»c trong Ngá»¯ cáº£nh (ICL) vÃ  Sinh TÄƒng cÆ°á»ng Truy xuáº¥t (RAG)

NhÆ° má»™t trong nhá»¯ng loáº¡i ká»¹ thuáº­t cá»¥ thá»ƒ liÃªn quan Ä‘áº¿n LLM, ICL Ä‘Ã£ xuáº¥t hiá»‡n nhÆ° má»™t ká»¹ thuáº­t hiá»‡u quáº£ [5, 11,35,45,51]. ICL tÃ¬m cÃ¡ch cáº£i thiá»‡n kháº£ nÄƒng cá»§a LLMs báº±ng cÃ¡ch tÃ­ch há»£p thÃ´ng tin cá»¥ thá»ƒ ngá»¯ cáº£nh, dÆ°á»›i dáº¡ng prompt Ä‘áº§u vÃ o hoáº·c máº«u hÆ°á»›ng dáº«n, trong quÃ¡ trÃ¬nh suy luáº­n vÃ  do Ä‘Ã³ khÃ´ng cáº§n thá»±c hiá»‡n huáº¥n luyá»‡n dá»±a trÃªn gradient. Do Ä‘Ã³, báº±ng cÃ¡ch xem xÃ©t ngá»¯ cáº£nh, mÃ´ hÃ¬nh trá»Ÿ nÃªn cÃ³ kháº£ nÄƒng sinh ra cÃ¡c Ä‘áº§u ra máº¡ch láº¡c vÃ  phÃ¹ há»£p vá»›i ngá»¯ cáº£nh hÆ¡n. TÃ­nh máº¡ch láº¡c ngá»¯ cáº£nh nÃ y cá»§a LLM vÃ  khÃ´ng pháº£i thá»±c hiá»‡n huáº¥n luyá»‡n dá»±a trÃªn gradient tá»‘n kÃ©m táº¡o thÃ nh nhá»¯ng Æ°u Ä‘iá»ƒm chÃ­nh cá»§a viá»‡c sá»­ dá»¥ng ICL Ä‘á»ƒ chuyÃªn biá»‡t hÃ³a LLMs cho má»™t tÃ¡c vá»¥ hoáº·c bá»™ dá»¯ liá»‡u cá»¥ thá»ƒ. Tuy nhiÃªn, ICL cÅ©ng cÃ³ má»™t sá»‘ báº¥t tiá»‡n, bao gá»“m nhu cáº§u thiáº¿t káº¿ cÃ¡c prompt Ä‘áº¡i diá»‡n [37, 74, 90].

RAG lÃ  má»™t phÆ°Æ¡ng phÃ¡p tinh vi hÆ¡n Ä‘á»ƒ Ä‘Æ°a cÃ¡c vÃ­ dá»¥ vÃ o prompt Ä‘áº§u vÃ o táº¡i thá»i Ä‘iá»ƒm suy luáº­n. KhÃ´ng giá»‘ng nhÆ° ICL chá»n cÃ¡c vÃ­ dá»¥ ngáº«u nhiÃªn, RAG dá»±a vÃ o má»™t mÃ´ hÃ¬nh truy xuáº¥t truy xuáº¥t Ä‘á»™ng cÃ¡c vÃ­ dá»¥ tá»« má»™t bá»™ dá»¯ liá»‡u gáº§n vá»›i má»™t truy váº¥n. Trong thá»±c táº¿, truy váº¥n cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng sá»­ dá»¥ng thÃ´ng tin tá»« vÃ­ dá»¥ kiá»ƒm tra táº¡i thá»i Ä‘iá»ƒm kiá»ƒm tra, cháº³ng háº¡n nhÆ° váº¥n Ä‘á» mÃ£ hÃ³a cho trÆ°á»ng há»£p sinh mÃ£. Tá»•ng há»£p láº¡i,

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 5 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢5

RAG cho phÃ©p Ä‘Æ°a thÃ´ng tin liÃªn quan hÆ¡n vÃ o prompt Ä‘áº§u vÃ o so vá»›i ICL vÃ  Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ nh cÃ´ng cho cÃ¡c tÃ¡c vá»¥ ká»¹ thuáº­t pháº§n má»m, nhÆ° sinh mÃ£ [52,91], tÃ³m táº¯t mÃ£ [39,52], vÃ  hoÃ n thiá»‡n mÃ£ [41]. Tuy nhiÃªn, cáº£ ICL vÃ  RAG Ä‘á»u cÃ³ má»™t sá»‘ háº¡n cháº¿. Má»™t váº¥n Ä‘á» liÃªn quan Ä‘áº¿n viá»‡c Ä‘Æ°a thÃªm token Ä‘áº§u vÃ o trong prompt, Ä‘iá»u nÃ y cÃ³ thá»ƒ khÃ´ng kháº£ thi khi thÃ´ng tin ngá»¯ cáº£nh quÃ¡ lá»›n. Má»™t háº¡n cháº¿ khÃ¡c lÃ  sá»± phá»¥ thuá»™c vÃ o cháº¥t lÆ°á»£ng vÃ  sá»± liÃªn quan cá»§a cÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c truy xuáº¥t. Trong RAG, mÃ´ hÃ¬nh truy xuáº¥t pháº£i tÃ¬m chÃ­nh xÃ¡c cÃ¡c vÃ­ dá»¥ thá»±c sá»± tÆ°Æ¡ng tá»± hoáº·c há»¯u Ã­ch cho truy váº¥n kiá»ƒm tra. Náº¿u cÆ¡ cháº¿ truy xuáº¥t khÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c vÃ­ dá»¥ phÃ¹ há»£p, nÃ³ cÃ³ thá»ƒ Ä‘Æ°a thÃ´ng tin khÃ´ng liÃªn quan hoáº·c gÃ¢y hiá»ƒu láº§m vÃ o prompt, cuá»‘i cÃ¹ng lÃ m giáº£m hiá»‡u suáº¥t.

2.2 Tinh chá»‰nh Hiá»‡u quáº£ Tham sá»‘ (PEFT)

PEFT Ä‘á» cáº­p Ä‘áº¿n viá»‡c sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a quÃ¡ trÃ¬nh tinh chá»‰nh cá»§a LLMs báº±ng cÃ¡ch cáº­p nháº­t cÃ³ chá»n lá»c má»™t táº­p con tham sá»‘ thay vÃ¬ cáº­p nháº­t toÃ n bá»™ tham sá»‘ cá»§a mÃ´ hÃ¬nh [14]. Vá» máº·t ká»¹ thuáº­t, cÃ¡c ká»¹ thuáº­t PEFT táº­p trung vÃ o viá»‡c há»c má»™t sá»‘ lÆ°á»£ng nhá» tham sá»‘ cho tÃ¡c vá»¥ hiá»‡n táº¡i báº±ng cÃ¡ch thiáº¿t káº¿ cÃ¡c lá»›p bá»• sung [23], thÃªm cÃ¡c token bá»• sung Ä‘á»©ng trÆ°á»›c [30,33], phÃ¢n tÃ¡ch gradient trá»ng sá»‘ thÃ nh cÃ¡c ma tráº­n cá»¥ thá»ƒ [24]. Má»™t trong nhá»¯ng ká»¹ thuáº­t PEFT tiÃªn tiáº¿n Ä‘áº¡i diá»‡n lÃ  ThÃ­ch á»©ng Thá»© háº¡ng Tháº¥p cá»§a LLMs (LoRA) [24]. Ká»¹ thuáº­t nÃ y bao gá»“m viá»‡c Ä‘Ã³ng bÄƒng trá»ng sá»‘ mÃ´ hÃ¬nh vÃ  Ä‘Æ°a cÃ¡c ma tráº­n cÃ³ thá»ƒ huáº¥n luyá»‡n thá»© háº¡ng tháº¥p vÃ o cÃ¡c lá»›p attention cá»§a kiáº¿n trÃºc Transformer [67], do Ä‘Ã³ giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n. ChÃºng tÃ´i sá»­ dá»¥ng LoRA nhÆ° má»™t trong nhá»¯ng ká»¹ thuáº­t PEFT cá»§a chÃºng tÃ´i vÃ¬ nÃ³ Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong NLP [14,37,65] vÃ  cho tháº¥y hiá»‡u suáº¥t Ä‘áº§y há»©a háº¹n. ChÃºng tÃ´i cÅ©ng sá»­ dá»¥ng IA3 nháº±m cáº£i thiá»‡n LoRA vÃ  giáº£m thÃªm lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n [37]. NgoÃ i LoRA vÃ  IA3, chÃºng tÃ´i cÅ©ng bao gá»“m Prompt tuning [30] vÃ  Prefix tuning [30] trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i. Prompt tuning bao gá»“m quÃ¡ trÃ¬nh thÃªm vÃ o trÆ°á»›c cÃ¡c token áº£o vÃ o token Ä‘áº§u vÃ o cá»§a LLM, trong khi Prefix tuning chÃ¨n cÃ¡c token áº£o trong táº¥t cáº£ cÃ¡c lá»›p cá»§a mÃ´ hÃ¬nh má»¥c tiÃªu vÃ  do Ä‘Ã³ yÃªu cáº§u há»c nhiá»u tham sá»‘ hÆ¡n. CÃ¡c token áº£o nÃ y cÃ³ thá»ƒ phÃ¢n biá»‡t, cho phÃ©p chÃºng Ä‘Æ°á»£c há»c thÃ´ng qua lan truyá»n ngÆ°á»£c trong quÃ¡ trÃ¬nh tinh chá»‰nh, trong khi pháº§n cÃ²n láº¡i cá»§a LLM váº«n Ä‘Æ°á»£c Ä‘Ã³ng bÄƒng. HÆ¡n ná»¯a, QLoRA [13] káº¿t há»£p LoRA vá»›i lÆ°á»£ng tá»­ hÃ³a mÃ´ hÃ¬nh, cho phÃ©p tinh chá»‰nh LLMs vá»›i Ã­t bá»™ nhá»› GPU hÆ¡n báº±ng cÃ¡ch giáº£m Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c kiá»ƒu dá»¯ liá»‡u Ä‘iá»ƒm ná»•i trong mÃ´ hÃ¬nh.

3 ÃP Dá»¤NG LLMS Vá»šI TÃ€I NGUYÃŠN Háº N CHáº¾

Trong ká»· nguyÃªn cá»§a LLMs, tÃ­nh kháº£ dá»¥ng cá»§a tÃ i nguyÃªn tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c khai thÃ¡c kháº£ nÄƒng cao cá»§a chÃºng. Tháº­t khÃ´ng may, nhiá»u nhÃ  nghiÃªn cá»©u vÃ  thá»±c hÃ nh thÆ°á»ng tháº¥y mÃ¬nh bá»‹ háº¡n cháº¿ bá»Ÿi tÃ­nh kháº£ dá»¥ng háº¡n cháº¿ cá»§a cÆ¡ sá»Ÿ háº¡ táº§ng tÃ­nh toÃ¡n cao cáº¥p.

VÃ­ dá»¥, má»™t ká»¹ sÆ° pháº§n má»m chá»‰ cÃ³ quyá»n truy cáº­p vÃ o má»™t GPU tiÃªu dÃ¹ng duy nháº¥t (vÃ­ dá»¥: 24GB VRAM) cÃ³ thá»ƒ tháº¥y tinh chá»‰nh Ä‘áº§y Ä‘á»§ khÃ´ng thá»±c táº¿ do nhu cáº§u bá»™ nhá»› Ä‘Ã¡ng ká»ƒ. Sá»± gia tÄƒng nhanh chÃ³ng vá» kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n lÃ m tráº§m trá»ng thÃªm váº¥n Ä‘á» nÃ y. Báº¥t cháº¥p hiá»‡u quáº£ cá»§a nÃ³, tinh chá»‰nh Ä‘áº§y Ä‘á»§ Ä‘i kÃ¨m vá»›i chi phÃ­ tÃ­nh toÃ¡n cao [3,15,51]., nháº¥n máº¡nh má»™t sá»± Ä‘Ã¡nh Ä‘á»•i tÃ­nh toÃ¡n-hiá»‡u quáº£ (xem Báº£ng 1).

Äá»ƒ giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ nÃ y, cÃ¡c phÆ°Æ¡ng phÃ¡p thay tháº¿ nhÆ° ICL vÃ  RAG Ä‘Ã£ thu hÃºt sá»± chÃº Ã½. ICL vÃ  RAG cung cáº¥p má»™t lá»±a chá»n tÃ­nh toÃ¡n tháº¥p báº±ng cÃ¡ch loáº¡i bá» nhu cáº§u cáº­p nháº­t tham sá»‘. Tuy nhiÃªn, cÃ¡c ká»¹ thuáº­t nÃ y Ä‘i kÃ¨m vá»›i táº­p há»£p thÃ¡ch thá»©c riÃªng cá»§a nÃ³, bao gá»“m viá»‡c lá»±a chá»n cÃ¡c vÃ­ dá»¥ Ä‘áº¡i diá»‡n vÃ  Ä‘á»™ nháº¡y vá»›i thiáº¿t káº¿ prompt [37,74,90]. Trong thá»±c táº¿, Ä‘iá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n hiá»‡u quáº£ tháº¥p hÆ¡n so vá»›i tinh chá»‰nh, Ä‘áº·c biá»‡t lÃ  cho cÃ¡c tÃ¡c vá»¥ cÃ³ ngá»¯ cáº£nh cao phá»• biáº¿n trong ká»¹ thuáº­t pháº§n má»m.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 6 ---
6â€¢M. Weyssow et al.

Báº£ng 1. Sá»± Ä‘Ã¡nh Ä‘á»•i tÃ­nh toÃ¡n-hiá»‡u quáº£ cho má»—i ká»¹ thuáº­t tinh chá»‰nh mÃ´ hÃ¬nh.

Ká»¹ thuáº­t                Chi phÃ­ tÃ­nh toÃ¡n       Hiá»‡u quáº£
Tinh chá»‰nh Ä‘áº§y Ä‘á»§        cao [X]                cao [âœ“]
ICL vÃ  RAG              tháº¥p [âœ“]               tháº¥p [X]
PEFT                    tháº¥p [âœ“]               cao [âœ“]

0  5  10  15  20  24
Má»©c tiÃªu thá»¥ bá»™ nhá»› Ä‘á»‰nh (GB)
CodeT5+-220M-ft
CodeGen-350M-mono-ft
CodeT5+-770M-ft
CodeLlama-7B-QLoRA-4bit
CodeGen2-1B-lora
CodeGen2-3.7B-lora
CodeLlama-13B-QLoRA-4bit
CodeLlama-7B-lora
CodeGen2-7B-lora
CodeLlama-34B-QLoRA-4bit
3.54
5.84
8.16
9.16
9.8
14.08
15.01
19.06
20.29
23.59

HÃ¬nh 1. Má»©c tiÃªu thá»¥ bá»™ nhá»› GPU Ä‘á»‰nh trong quÃ¡ trÃ¬nh tinh chá»‰nh mÃ´ hÃ¬nh sá»­ dá»¥ng tinh chá»‰nh Ä‘áº§y Ä‘á»§ (ft), LoRA, vÃ  QLoRA.

Äá»ƒ vÆ°á»£t qua nhá»¯ng háº¡n cháº¿ nÃ y, chÃºng tÃ´i dá»± bÃ¡o sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c ká»¹ thuáº­t PEFT nhÆ° cÃ¡c giáº£i phÃ¡p Ä‘áº§y há»©a háº¹n, cung cáº¥p cÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh LLMs hiá»‡u quáº£ vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng hÆ¡n vá» máº·t tÃ­nh toÃ¡n. CÃ¡c phÆ°Æ¡ng phÃ¡p PEFT, nhÆ° LoRA vÃ  QLoRA, giá»›i háº¡n sá»‘ lÆ°á»£ng tham sá»‘ Ä‘Æ°á»£c cáº­p nháº­t, do Ä‘Ã³ giáº£m tiÃªu thá»¥ bá»™ nhá»› trong khi duy trÃ¬ hiá»‡u quáº£ cáº¡nh tranh vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§. Äiá»u nÃ y lÃ m cho PEFT Ä‘áº·c biá»‡t phÃ¹ há»£p cho cÃ¡c nhÃ  thá»±c hÃ nh cÃ³ quyá»n truy cáº­p háº¡n cháº¿ vÃ o tÃ i nguyÃªn tÃ­nh toÃ¡n. NhÆ° Ä‘Æ°á»£c minh há»a trong Báº£ng 1, PEFT Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng tá»‘i Æ°u giá»¯a chi phÃ­ tÃ­nh toÃ¡n vÃ  hiá»‡u quáº£. HÆ¡n ná»¯a, HÃ¬nh 1 cho tháº¥y ráº±ng báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT nhÆ° LoRA, cÃ¡c nhÃ  thá»±c hÃ nh cÃ³ thá»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh nhÆ° CodeLlama-7B mÃ  khÃ´ng vÆ°á»£t quÃ¡ 19GB bá»™ nhá»› GPU. Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh tháº­m chÃ­ lá»›n hÆ¡n, nhÆ° CodeLlama-34B, QLoRA vá»›i lÆ°á»£ng tá»­ hÃ³a cho phÃ©p tinh chá»‰nh trong giá»›i háº¡n cá»§a má»™t GPU VRAM 24GB.

Káº¿t luáº­n, PEFT trao quyá»n cho cÃ¡c ká»¹ sÆ° pháº§n má»m Ä‘á»ƒ vÆ°á»£t qua nhá»¯ng háº¡n cháº¿ tÃ i nguyÃªn, cho phÃ©p tinh chá»‰nh LLM hiá»‡u quáº£ trong cÃ¡c tÃ¡c vá»¥ cÃ³ ngá»¯ cáº£nh cao mÃ  khÃ´ng dá»±a vÃ o cÆ¡ sá»Ÿ háº¡ táº§ng tÃ­nh toÃ¡n Ä‘áº¯t Ä‘á». Äiá»u nÃ y lÃ m cho PEFT khÃ´ng chá»‰ lÃ  má»™t cÃ´ng cá»¥ thá»±c táº¿ mÃ  cÃ²n lÃ  cÃ´ng cá»¥ thiáº¿t yáº¿u Ä‘á»ƒ dÃ¢n chá»§ hÃ³a quyá»n truy cáº­p vÃ o kháº£ nÄƒng LLM.

4 PHÆ¯Æ NG PHÃP LUáº¬N

Trong pháº§n nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y thiáº¿t láº­p thÃ­ nghiá»‡m cá»§a nghiÃªn cá»©u chÃºng tÃ´i. ChÃºng tÃ´i tiáº¿n hÃ nh táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m trong má»™t ká»‹ch báº£n háº¡n cháº¿ tÃ i nguyÃªn. Cá»¥ thá»ƒ, táº¥t cáº£ cÃ¡c quy trÃ¬nh, tá»©c lÃ  tinh chá»‰nh vÃ  suy luáº­n, cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i quyá»n truy cáº­p vÃ o má»™t GPU 24GB duy nháº¥t. Má»¥c tiÃªu chÃ­nh cá»§a nghiÃªn cá»©u chÃºng tÃ´i lÃ  chá»©ng minh liá»‡u viá»‡c tinh chá»‰nh LLMs thÃ´ng qua PEFT cÃ³ kháº£ thi vÃ  mong muá»‘n hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y vÃ  cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n trong bá»‘i cáº£nh nÃ y.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 7 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢7

4.1 CÃ¢u há»i NghiÃªn cá»©u

Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i táº­p trung vÃ o cÃ¡c cÃ¢u há»i nghiÃªn cá»©u sau:
â€“RQ1: LLMs vÃ  SLMs hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o khi sá»­ dá»¥ng ICL trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy?

ChÃºng tÃ´i nghiÃªn cá»©u hiá»‡u quáº£ cÆ¡ sá»Ÿ cá»§a LLMs (â‰¥1B tham sá»‘) vÃ  SLMs (<1B tham sá»‘) cho sinh mÃ£ sá»­ dá»¥ng prompt zero-shot vÃ  ICL, nÆ¡i ğ‘› vÃ­ dá»¥ Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn Ä‘Æ°á»£c thÃªm vÃ o prompt Ä‘áº§u vÃ o. ChÃºng tÃ´i kiá»ƒm tra má»—i mÃ´ hÃ¬nh vá»›i lÃªn Ä‘áº¿n 16 vÃ­ dá»¥ ICL, do tÃ i nguyÃªn tÃ­nh toÃ¡n háº¡n cháº¿ cá»§a chÃºng tÃ´i.

ChÃºng tÃ´i nghiÃªn cá»©u hiá»‡u quáº£ cá»§a má»™t phá»• rá»™ng SLMs vÃ  LLMs cho sinh mÃ£ trÃªn hai bá»™ dá»¯ liá»‡u bao gá»“m mÃ£ cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau. ChÃºng tÃ´i chá»n má»™t loáº¡t rá»™ng cÃ¡c mÃ´ hÃ¬nh cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau, Ä‘Æ°á»£c pre-train trÃªn cÃ¡c codebase Ä‘a dáº¡ng vÃ  vá»›i cÃ¡c má»¥c tiÃªu há»c khÃ¡c nhau Ä‘á»ƒ nghiÃªn cá»©u cÃ¡ch nhá»¯ng yáº¿u tá»‘ nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u quáº£ cá»§a chÃºng.

â€“RQ2: LLMs vÃ  SLMs hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o khi sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy? Trong RQ nÃ y, chÃºng tÃ´i Ä‘iá»u tra liá»‡u cÃ¡c ká»¹ thuáº­t PEFT cÃ³ luÃ´n vÆ°á»£t trá»™i hÆ¡n ICL cho SLMs vÃ  LLMs. ChÃºng tÃ´i so sÃ¡nh cÃ¡c cáº¥u hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t cá»§a ICL trong RQ1 vá»›i cÃ¡c ká»¹ thuáº­t PEFT, bao gá»“m LoRA, IA3, Prompt tuning, Prefix tuning. HÆ¡n ná»¯a, chÃºng tÃ´i cÅ©ng Ä‘iá»u tra áº£nh hÆ°á»Ÿng cá»§a lÆ°á»£ng tá»­ hÃ³a vá»›i QLoRA-8bit vÃ  QLoRA-4bit trÃªn mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t cá»§a chÃºng tÃ´i vÃ  cÃ¡c biáº¿n thá»ƒ lá»›n hÆ¡n.

Äá»‘i vá»›i SLMs, chÃºng tÃ´i cÅ©ng bao gá»“m má»™t so sÃ¡nh vá»›i tinh chá»‰nh tham sá»‘ Ä‘áº§y Ä‘á»§, nhÆ° thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c nghiÃªn cá»©u SE trÆ°á»›c Ä‘Ã¢y [16,72,77,92]. ChÃºng tÃ´i khÃ´ng bao gá»“m tinh chá»‰nh tham sá»‘ Ä‘áº§y Ä‘á»§ cho LLMs, vÃ¬ nÃ³ khÃ´ng kháº£ thi trong ngÃ¢n sÃ¡ch tÃ­nh toÃ¡n cá»§a chÃºng tÃ´i.

â€“RQ3: LoRA so sÃ¡nh nhÆ° tháº¿ nÃ o vá»›i ICL vÃ  RAG trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy?
Trong RQ nÃ y, chÃºng tÃ´i so sÃ¡nh hiá»‡u quáº£ cá»§a LLM hoáº¡t Ä‘á»™ng tá»‘t nháº¥t Ä‘Æ°á»£c tinh chá»‰nh báº±ng LoRA vá»›i RAG. Thiáº¿t láº­p RAG cá»§a chÃºng tÃ´i bao gá»“m viá»‡c truy xuáº¥t lÃªn Ä‘áº¿n 16 vÃ­ dá»¥ tá»« táº­p huáº¥n luyá»‡n cÃ³ liÃªn quan cháº·t cháº½ Ä‘áº¿n prompt Ä‘áº§u vÃ o, tÆ°Æ¡ng tá»± nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t trÆ°á»›c Ä‘Ã¢y cho cÃ¡c tÃ¡c vá»¥ SE khÃ¡c nhau [39, 41, 70].

â€“RQ4: ChÃºng ta cÃ³ thá»ƒ nÃ¢ng cao hiá»‡u quáº£ cá»§a LLMs cho sinh mÃ£ trong bá»™ dá»¯ liá»‡u APPS báº±ng cÃ¡ch sá»­ dá»¥ng LoRA vÃ  QLoRA khÃ´ng? Cuá»‘i cÃ¹ng, chÃºng tÃ´i khÃ¡m phÃ¡ liá»‡u LLM Ä‘Æ°á»£c tinh chá»‰nh báº±ng LoRA vÃ  QLoRA cÃ³ cho tháº¥y cáº£i thiá»‡n trong tÃ­nh Ä‘Ãºng Ä‘áº¯n chá»©c nÄƒng trong bá»™ dá»¯ liá»‡u APPS. ChÃºng tÃ´i tinh chá»‰nh LLM hoáº¡t Ä‘á»™ng tá»‘t nháº¥t cá»§a chÃºng tÃ´i báº±ng LoRA vÃ  QLoRA trÃªn táº­p huáº¥n luyá»‡n cá»§a APPS, vÃ  bÃ¡o cÃ¡o trung bÃ¬nh cá»§a test cases Ä‘Æ°á»£c vÆ°á»£t qua cÅ©ng nhÆ° Pass@ğ‘˜ trÃªn táº­p kiá»ƒm tra cá»§a APPS cho cÃ¡c váº¥n Ä‘á» mÃ£ hÃ³a cáº¥p Ä‘á»™ giá»›i thiá»‡u, phá»ng váº¥n, vÃ  thi Ä‘áº¥u.

4.2 Bá»™ dá»¯ liá»‡u vÃ  TÃ¡c vá»¥

Trong suá»‘t nghiÃªn cá»©u cá»§a chÃºng tÃ´i, chÃºng tÃ´i so sÃ¡nh táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c nghiÃªn cá»©u trÃªn má»™t tÃ¡c vá»¥ sinh mÃ£ Python. TÃ¡c vá»¥ nÃ y Ä‘Ã£ thu hÃºt sá»± chÃº Ã½ Ä‘Ã¡ng ká»ƒ trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y [9,10,48,50,61] vá»›i sá»± xuáº¥t hiá»‡n cá»§a LLMs vÃ  kháº£ nÄƒng cá»§a chÃºng trong viá»‡c sinh mÃ£ Python theo phÆ°Æ¡ng thá»©c zero-shot, tá»©c lÃ  khÃ´ng cáº§n tinh chá»‰nh thÃªm. Cá»¥ thá»ƒ, cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ nhÆ° HumanEval [9] Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p sinh mÃ£ [3,9,82]. Máº·c dÃ¹ HumanEval Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i, nÃ³ thiáº¿u má»™t kho tÃ i liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh hoáº·c PEFT. VÃ¬ trá»ng tÃ¢m cá»§a nghiÃªn cá»©u chÃºng tÃ´i lÃ  chuyÃªn biá»‡t hÃ³a LLMs sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT, chÃºng tÃ´i Ä‘Ã£ chá»n khÃ´ng sá»­ dá»¥ng HumanEval. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i chá»n sá»­ dá»¥ng ba bá»™ dá»¯ liá»‡u sinh mÃ£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i khÃ¡c: bá»™ dá»¯ liá»‡u Conala [87], CodeAlpaca [8], vÃ  APPS [22]. Táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u cung cáº¥p má»™t sá»‘ lÆ°á»£ng lá»›n

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 8 ---
8â€¢M. Weyssow et al.

0   25  50  75  100 125 150 175 200
Äá»™ dÃ i token
0
100
200
300
400
500
600
700
Sá»‘ lÆ°á»£ng máº«u
APPS
CodeAlpacaPy
Conala

HÃ¬nh 2. PhÃ¢n phá»‘i Ä‘á»™ dÃ i token cá»§a cÃ¡c bá»™ dá»¯ liá»‡u Conala, CodeAlpacaPy, vÃ  APPS.

cÃ¡c vÃ­ dá»¥ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tinh chá»‰nh má»™t mÃ´ hÃ¬nh vÃ  Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c nghiÃªn cá»©u sinh mÃ£ trÆ°á»›c Ä‘Ã¢y vá»›i LLMs [71, 73, 88, 91].

Bá»™ dá»¯ liá»‡u Conala. ChÃºng tÃ´i sá»­ dá»¥ng má»™t phiÃªn báº£n Ä‘Æ°á»£c tuyá»ƒn chá»n cá»§a bá»™ dá»¯ liá»‡u Conala [91]. Bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p tá»« StackOverflow vÃ  chá»©a cÃ¡c cáº·p mÃ£ vÃ  Ã½ Ä‘á»‹nh ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘Æ°á»£c chÃº thÃ­ch thá»§ cÃ´ng. Má»—i Ã½ Ä‘á»‹nh ngÃ´n ngá»¯ tá»± nhiÃªn chá»©a gá»£i Ã½ vá» cÃ¡c biáº¿n Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn trong mÃ£ thá»±c táº¿, vÃ­ dá»¥, xem vÃ­ dá»¥ Ä‘áº§u tiÃªn trong Báº£ng 2, cung cáº¥p thÃªm ngá»¯ cáº£nh cho mÃ´ hÃ¬nh Ä‘á»ƒ sinh mÃ£ cÃ³ liÃªn quan. Trong HÃ¬nh 2, chÃºng tÃ´i bÃ¡o cÃ¡o phÃ¢n phá»‘i Ä‘á»™ dÃ i token cá»§a ba bá»™ dá»¯ liá»‡u. Trong Conala, háº§u háº¿t cÃ¡c giáº£i phÃ¡p mÃ£ Ä‘á»u ngáº¯n vÃ  má»™t dÃ²ng, lÃ m cho nÃ³ tÆ°Æ¡ng Ä‘á»‘i dá»… dÃ ng cho má»™t LLM Ä‘á»ƒ sinh cÃ¡c dá»± Ä‘oÃ¡n khá»›p chÃ­nh xÃ¡c. Trong phiÃªn báº£n Ä‘Æ°á»£c tuyá»ƒn chá»n nÃ y cá»§a bá»™ dá»¯ liá»‡u, cÃ¡c tÃ¡c giáº£ Ä‘áº£m báº£o ráº±ng má»—i máº«u trong táº­p xÃ¡c thá»±c vÃ  kiá»ƒm tra chá»©a Ã­t nháº¥t má»™t hÃ m Python khÃ´ng xuáº¥t hiá»‡n trong táº­p huáº¥n luyá»‡n. NgoÃ i ra, há» Ä‘áº£m báº£o ráº±ng cÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c thu tháº­p tá»« cÃ¹ng má»™t bÃ i Ä‘Äƒng StackOverflow xuáº¥t hiá»‡n trong cÃ¡c táº­p khÃ¡c nhau. Do Ä‘Ã³, chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº£m báº£o ráº±ng má»—i Ã½ Ä‘á»‹nh tá»± nhiÃªn trong táº­p kiá»ƒm tra khÃ´ng xuáº¥t hiá»‡n trong táº­p huáº¥n luyá»‡n. Bá»™ dá»¯ liá»‡u chá»©a 2,135/201/543 máº«u lÃ m táº­p huáº¥n luyá»‡n/xÃ¡c thá»±c/kiá»ƒm tra, tÆ°Æ¡ng á»©ng.

Bá»™ dá»¯ liá»‡u CodeAlpacaPy. ChÃºng tÃ´i xÃ¢y dá»±ng má»™t phiÃªn báº£n Python Ä‘Æ°á»£c tuyá»ƒn chá»n cá»§a bá»™ dá»¯ liá»‡u CodeAlpaca [8] báº±ng cÃ¡ch chá»n cá»¥ thá»ƒ cÃ¡c máº«u dá»¯ liá»‡u Python trong bá»™ dá»¯ liá»‡u CodeAlpaca. ChÃºng tÃ´i lá»c ra cÃ¡c máº«u mÃ£ khÃ´ng thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ­ch tÄ©nh Ä‘á»ƒ Ä‘áº£m báº£o bá»™ dá»¯ liá»‡u chá»‰ bao gá»“m cÃ¡c mÃ£ Python há»£p lá»‡ vá» máº·t cÃº phÃ¡p. NhÆ° Ä‘Æ°á»£c minh há»a trong vÃ­ dá»¥ dÆ°á»›i cÃ¹ng cá»§a Báº£ng 2 vÃ  trong HÃ¬nh 2, CodeAlpacaPy chá»©a cÃ¡c vÃ­ dá»¥ dÃ i hÆ¡n vÃ  phá»©c táº¡p hÆ¡n so vá»›i Conala, cho phÃ©p Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n hÆ¡n vá» PEFT cho sinh mÃ£. Bá»™ dá»¯ liá»‡u chá»©a 2,192/314/628 máº«u lÃ m táº­p huáº¥n luyá»‡n/xÃ¡c thá»±c/kiá»ƒm tra, tÆ°Æ¡ng á»©ng.

Bá»™ dá»¯ liá»‡u APPS. Bá»™ dá»¯ liá»‡u APPS bao gá»“m 10,000 váº¥n Ä‘á» sinh mÃ£, má»—i váº¥n Ä‘á» Ä‘Æ°á»£c ghÃ©p ná»‘i vá»›i cÃ¡c giáº£i phÃ¡p Python. Nhá»¯ng váº¥n Ä‘á» nÃ y Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh ba cáº¥p Ä‘á»™ khÃ³: giá»›i thiá»‡u, phá»ng váº¥n, vÃ  thi Ä‘áº¥u, vá»›i cÃ¡c giáº£i phÃ¡p thay Ä‘á»•i tá»« má»™t dÃ²ng Ä‘Æ¡n giáº£n Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n phá»©c táº¡p. ChÃºng ta cÃ³ thá»ƒ tháº¥y trong HÃ¬nh 2 vÃ  Báº£ng 2 ráº±ng APPS bao gá»“m cÃ¡c vÃ­ dá»¥ dÃ i hÆ¡n vÃ  phá»©c táº¡p hÆ¡n so vá»›i hai bá»™ dá»¯ liá»‡u khÃ¡c. Trung bÃ¬nh, má»—i váº¥n Ä‘á» Ä‘Æ°á»£c Ä‘i kÃ¨m vá»›i 21.2 test cases, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tÃ­nh Ä‘Ãºng Ä‘áº¯n chá»©c nÄƒng cá»§a mÃ£ Ä‘Æ°á»£c sinh. Bá»™ dá»¯ liá»‡u gá»‘c Ä‘Æ°á»£c chia thÃ nh 5,000 máº«u cho huáº¥n luyá»‡n vÃ  5,000 cho kiá»ƒm tra.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 9 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢9

Báº£ng 2. Tá»•ng quan vá» tÃ¡c vá»¥ sinh mÃ£, vá»›i ba vÃ­ dá»¥ Ä‘Æ°á»£c láº¥y tá»« cÃ¡c bá»™ dá»¯ liá»‡u Conala, CodeAlpacaPy, vÃ  APPS.

Conala
Prompt: ### Instruction:
map two lists 'keys' and 'values' into a dictionary
### Response:
Ground truth: dict([(k, v) for k, v in zip(keys, values)])

CodeAlpacaPy
Prompt: ### Instruction:
Write a function to calculate the standard deviation of data points in Python.
### Response:
Ground truth: def stdev(data):
    avg = sum(data) / len(data)
    total = 0
    for x in data:
        total += (x - avg) ** 2
    return (total / (len(data) - 1)) ** 0.5

APPS
Prompt: ### Instruction:
You are given a string s = s1 s2 . . . sn of length n, which only contains digits 1, 2,..., 9. A substring s[l...r] of s is a string slsl+1sl+2 ...sr. A substring s[l...r] of s is called even if the number represented by it is even. Find the number of even substrings of s. Note, that even if some substrings are equal as strings, but have different l and r, they are counted as different substrings. The first line contains an integer n (1â‰¤nâ‰¤65000) â€” the length of the string s. The second line contains a string s of length n. The string s consists only of digits 1, 2,..., 9. Print the number of even substrings of s.
### Response:
Ground truth: n = int(input())
s = input()
ans = 0
for i in range(n):
    for j in range(i, n):
        if int(s[i:j+1]) % 2 == 0:
            ans += 1
print(ans)

Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng 4,500 máº«u cho huáº¥n luyá»‡n, 500 cho xÃ¡c thá»±c, vÃ  750 cho kiá»ƒm tra, Ä‘áº£m báº£o phÃ¢n phá»‘i cÃ¢n báº±ng 250 máº«u kiá»ƒm tra cho má»—i cáº¥p Ä‘á»™ khÃ³.

Thiáº¿t káº¿ tÃ¡c vá»¥. Trong Báº£ng 2, chÃºng tÃ´i minh há»a tá»•ng quan vá» thiáº¿t káº¿ tÃ¡c vá»¥. Prompt cÃ³ dáº¡ng cá»§a má»™t máº«u hÆ°á»›ng dáº«n, nÆ¡i "### Instruction:" vÃ  "### Response:" Ä‘Ã³ng vai trÃ² phÃ¢n Ä‘á»‹nh hÆ°á»›ng dáº«n, tá»©c lÃ  Ã½ Ä‘á»‹nh ngÃ´n ngá»¯ tá»± nhiÃªn, vÃ  cÃ¢u tráº£ lá»i, tá»©c lÃ  sinh mÃ£. LÆ°u Ã½ ráº±ng thiáº¿t káº¿ prompt nÃ y cÃ³ thá»ƒ khÃ´ng tá»‘i Æ°u, nhÆ°ng loáº¡i máº«u hÆ°á»›ng dáº«n nÃ y Ä‘Ã£ cho tháº¥y hiá»‡u quáº£ trong cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y [36,89]. MÃ£ Ä‘Æ°á»£c sinh bá»Ÿi mÃ´ hÃ¬nh Ä‘Æ°á»£c so sÃ¡nh vá»›i ground truth Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cá»§a viá»‡c sinh. Trong quÃ¡ trÃ¬nh tinh chá»‰nh, chÃºng tÃ´i giáº£m thiá»ƒu má»™t hÃ m loss cross-entropy tá»± há»“i quy tiÃªu chuáº©n:

L = -âˆ‘(i=1 to T+1) Mi Â· log P(xi|x<i),

trong Ä‘Ã³:
Mi = {
    1, if xi â‰  -100
    0, otherwise
}

MÃ´ hÃ¬nh nháº­n má»™t ná»‘i káº¿t cá»§a prompt vÃ  ground truth lÃ m Ä‘áº§u vÃ o vÃ  dá»± Ä‘oÃ¡n má»—i token xi theo cÃ¡ch tá»± há»“i quy dá»±a trÃªn cÃ¡c token trÆ°á»›c Ä‘Ã³ x<i. LÆ°u Ã½ ráº±ng trong tÃ­nh toÃ¡n loss, chÃºng tÃ´i bá» qua cÃ¡c token tá»« máº«u hÆ°á»›ng dáº«n Ä‘á»ƒ buá»™c mÃ´ hÃ¬nh táº­p trung vÃ o viá»‡c sinh mÃ£. ChÃºng tÃ´i Ä‘áº·t giÃ¡ trá»‹ cá»§a cÃ¡c token hÆ°á»›ng dáº«n thÃ nh -100 vÃ  bá» qua chÃºng trong tÃ­nh toÃ¡n loss sá»­ dá»¥ng hÃ m chá»‰ bÃ¡o Mi. Táº¡i thá»i Ä‘iá»ƒm suy luáº­n, mÃ´ hÃ¬nh nháº­n prompt lÃ m Ä‘áº§u vÃ o vÃ  cá»‘ gáº¯ng sinh mÃ£ ground truth báº±ng cÃ¡ch sinh lÃªn Ä‘áº¿n 10 á»©ng viÃªn mÃ£.

4.3 ICL vÃ  RAG

ChÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m sá»­ dá»¥ng ICL vÃ  RAG trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy. Äá»‘i vá»›i cáº£ hai ká»¹ thuáº­t, chÃºng tÃ´i chá»n sá»‘ lÆ°á»£ng máº«u tá»‘i Ä‘a cÃ³ thá»ƒ vá»«a vá»›i bá»™ nhá»› GPU cá»§a chÃºng tÃ´i. Äá»‘i vá»›i ICL, chÃºng tÃ´i sá»­ dá»¥ng lÃªn Ä‘áº¿n 16 vÃ­ dá»¥ cho bá»™ dá»¯ liá»‡u Conala vÃ  8 vÃ­ dá»¥ cho CodeAlpacaPy. Nhá»¯ng vÃ­ dá»¥ nÃ y Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn tá»« cÃ¡c bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n tÆ°Æ¡ng á»©ng vÃ  Ä‘Æ°á»£c ná»‘i vá»›i prompt Ä‘áº§u vÃ o trong quÃ¡ trÃ¬nh suy luáº­n. Äá»‘i vá»›i RAG, chÃºng tÃ´i táº­n dá»¥ng GTE-small, má»™t mÃ´ hÃ¬nh embedding Ä‘a má»¥c Ä‘Ã­ch, nháº¹ vÆ°á»£t trá»™i hÆ¡n nhiá»u mÃ´ hÃ¬nh lá»›n hÆ¡n, bao gá»“m cÃ¡c embedding Ä‘á»™c quyá»n cá»§a OpenAI [34]. ChÃºng tÃ´i sinh embedding cho táº¥t cáº£ cÃ¡c hÆ°á»›ng dáº«n (loáº¡i trá»« mÃ£) trong cÃ¡c táº­p huáº¥n luyá»‡n. Táº¡i thá»i Ä‘iá»ƒm suy luáº­n, chÃºng tÃ´i truy xuáº¥t lÃªn Ä‘áº¿n 16 vÃ­ dá»¥ cho Conala vÃ  4 vÃ­ dá»¥ cho CodeAlpacaPy, chá»n nhá»¯ng vÃ­ dá»¥ cÃ³ hÆ°á»›ng dáº«n tÆ°Æ¡ng tá»± nháº¥t vá»›i Ä‘áº§u vÃ o kiá»ƒm tra. NhÆ° vá»›i ICL, cÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c truy xuáº¥t Ä‘Æ°á»£c ná»‘i vá»›i váº¥n Ä‘á» Ä‘áº§u vÃ o Ä‘á»ƒ hÆ°á»›ng dáº«n sinh mÃ£.

4.4 MÃ´ hÃ¬nh NgÃ´n ngá»¯ Nhá» vÃ  Lá»›n

Äá»ƒ thá»±c hiá»‡n má»™t phÃ¢n tÃ­ch toÃ n diá»‡n, chÃºng tÃ´i Ä‘Ã£ chá»n SLMs vÃ  LLMs cá»§a chÃºng tÃ´i theo má»™t sá»‘ tiÃªu chÃ­. Äáº§u tiÃªn, chÃºng tÃ´i chá»‰ xem xÃ©t cÃ¡c mÃ´ hÃ¬nh nguá»“n má»Ÿ. ChÃºng tÃ´i bá» qua cÃ¡c LLMs nguá»“n Ä‘Ã³ng nhÆ° Codex do khÃ´ng thá»ƒ tiáº¿p cáº­n cÃ¡c tham sá»‘ cá»§a chÃºng, Ä‘iá»u nÃ y lÃ m cho viá»‡c nghiÃªn cá»©u báº¥t ká»³ ká»¹ thuáº­t tinh chá»‰nh nÃ o trá»Ÿ nÃªn khÃ´ng kháº£ thi. Táº¥t cáº£ cÃ¡c checkpoint cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c nghiÃªn cá»©u cÃ³ thá»ƒ Ä‘Æ°á»£c truy cáº­p tá»± do, vÃ  Ä‘Ã£ Ä‘Æ°á»£c pre-train sá»­ dá»¥ng dá»¯ liá»‡u nguá»“n má»Ÿ. Thá»© hai, chÃºng tÃ´i chá»n LLMs, Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t hÃ nh trong hai nÄƒm qua. Cuá»‘i cÃ¹ng, Ä‘á»ƒ Ä‘iá»u tra tÃ¡c Ä‘á»™ng cá»§a viá»‡c má»Ÿ rá»™ng, chÃºng tÃ´i chá»n cÃ¡c mÃ´ hÃ¬nh vá»›i má»™t loáº¡t Ä‘a dáº¡ng cÃ¡c tham sá»‘. ChÃºng tÃ´i coi cÃ¡c mÃ´ hÃ¬nh cÃ³ Ã­t hÆ¡n 1B tham sá»‘ lÃ  SLMs, vÃ  nhá»¯ng mÃ´ hÃ¬nh khÃ¡c lÃ  LLMs. LÆ°u Ã½ ráº±ng chÃºng tÃ´i chá»n cÃ¡c mÃ´ hÃ¬nh phÃ¹ há»£p vá»›i má»™t GPU 24GB duy nháº¥t cho tinh chá»‰nh vÃ  suy luáº­n mÃ  khÃ´ng gÃ¢y trÃ n bá»™ nhá»›. Tá»•ng cá»™ng, chÃºng tÃ´i bao gá»“m 11 SLMs vÃ  LLMs tá»« cÃ¡c há» mÃ´ hÃ¬nh Ä‘a dáº¡ng Ä‘á»ƒ tiáº¿n hÃ nh thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i.

â€“SLMs. ChÃºng tÃ´i sá»­ dá»¥ng CodeGen-350M-mono [48], CodeT5+-220M [71], vÃ  CodeT5+-770M [71] lÃ m SLMs. CodeGen-350M-mono lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy vÃ  lÃ  phiÃªn báº£n nhá» cá»§a CodeGen Ä‘Æ°á»£c pre-train trÃªn cÃ¡c ngÃ´n ngá»¯ láº­p trÃ¬nh khÃ¡c nhau vÃ  Ä‘Æ°á»£c tinh chá»‰nh thÃªm trÃªn dá»¯ liá»‡u Python. CodeT5+-220M vÃ  CodeT5+-770M lÃ  cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ encoder-decoder cáº£i thiá»‡n CodeT5 báº±ng cÃ¡ch táº­n dá»¥ng giai Ä‘oáº¡n pre-training hai giai Ä‘oáº¡n trÃªn dá»¯ liá»‡u ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  mÃ£, vÃ  cÃ¡c má»¥c tiÃªu há»c má»›i.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 11 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢11

â€“CodeGen2 [47] lÃ  má»™t há» mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»±a trÃªn prefix káº¿t há»£p cÃ¡c lÆ°á»£c Ä‘á»“ há»c cá»§a encoder hai chiá»u vÃ  decoder má»™t chiá»u. CodeGen2 cáº£i thiá»‡n CodeGen [48], do Ä‘Ã³ chÃºng tÃ´i khÃ´ng bao gá»“m há» CodeGen trong Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i. CÃ¡c mÃ´ hÃ¬nh CodeGen2 Ä‘Æ°á»£c pre-train trÃªn phiÃªn báº£n khá»­ trÃ¹ng láº·p cá»§a TheStack [28] tráº£i rá»™ng má»™t loáº¡t ngÃ´n ngá»¯. ChÃºng tÃ´i sá»­ dá»¥ng CodeGen2-1B, CodeGen2-3.7B vÃ  CodeGen2-7B.

â€“CodeLlama [56] lÃ  má»™t há» LLMs dá»±a trÃªn Llama 2 [64]. Má»—i mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i Llama 2 vÃ  Ä‘Æ°á»£c pre-train thÃªm trÃªn mÃ£. CodeLlama cÃ³ ba biáº¿n thá»ƒ khÃ¡c nhau: CodeLlama chuyÃªn biá»‡t cho mÃ£, CodeLlama-Instruct chuyÃªn biá»‡t cho instruction-tuning vÃ  CodeLlama-Python chuyÃªn biá»‡t cho Python. ChÃºng tÃ´i sá»­ dá»¥ng CodeLlama-7B, CodeLlama-7B-Instruct vÃ  CodeLlama-7B-Python Ä‘á»ƒ khá»Ÿi Ä‘á»™ng thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i. Trong RQ4, chÃºng tÃ´i tinh chá»‰nh CodeLlama-13B-Python vÃ  CodeLlama-34B-Python sá»­ dá»¥ng QLoRA.

4.5 Metrics

ChÃºng tÃ´i Ä‘o lÆ°á»ng hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh thÃ´ng qua cÃ¡c metrics Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ´ng viá»‡c sinh mÃ£ trÆ°á»›c Ä‘Ã¢y. Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m trÃªn Conala vÃ  CodeAlpacaPy, chÃºng tÃ´i bÃ¡o cÃ¡o metrics Exact Match (EM) vÃ  CodeBLEU [55]. Vá»›i má»™t mÃ£ Ä‘Æ°á»£c sinh vÃ  má»™t ground truth, EM tráº£ vá» 1 náº¿u cáº£ hai mÃ£ giá»‘ng há»‡t nhau, ngÆ°á»£c láº¡i 0. Äá»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh trÃªn má»™t danh sÃ¡ch ğ‘˜âˆˆ[1,10] á»©ng viÃªn, chÃºng tÃ´i bÃ¡o cÃ¡o EM@ğ‘˜, tÃ­nh toÃ¡n trung bÃ¬nh cÃ¡c dá»± Ä‘oÃ¡n Ä‘Ãºng trong danh sÃ¡ch ğ‘˜ á»©ng viÃªn. Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i trÃªn bá»™ dá»¯ liá»‡u APPS, chÃºng tÃ´i bÃ¡o cÃ¡o hai metrics: sá»‘ lÆ°á»£ng test cases vÆ°á»£t qua trung bÃ¬nh vÃ  Pass@ğ‘˜. Sá»‘ lÆ°á»£ng test cases vÆ°á»£t qua trung bÃ¬nh Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nhÆ° tháº¿ nÃ o báº±ng cÃ¡ch Ä‘o lÆ°á»ng tá»· lá»‡ test cases mÃ  mÃ£ Ä‘Æ°á»£c sinh cá»§a nÃ³ vÆ°á»£t qua cho má»—i máº«u. NgÆ°á»£c láº¡i, Pass@ğ‘˜ lÃ  má»™t metric nghiÃªm ngáº·t hÆ¡n Ä‘o lÆ°á»ng tá»· lá»‡ pháº§n trÄƒm váº¥n Ä‘á» mÃ  Ã­t nháº¥t má»™t trong top ğ‘˜ máº«u mÃ£ Ä‘Æ°á»£c sinh vÆ°á»£t qua táº¥t cáº£ test cases, pháº£n Ã¡nh kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh trong viá»‡c táº¡o ra cÃ¡c giáº£i phÃ¡p hoÃ n toÃ n Ä‘Ãºng trong ğ‘˜ láº§n thá»­.

4.6 Chi tiáº¿t Triá»ƒn khai

Äá»‘i vá»›i táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i sá»­ dá»¥ng má»™t GPU NVIDIA RTX A5000 24GB duy nháº¥t. ChÃºng tÃ´i nghiÃªn cá»©u tá»•ng cá»™ng báº£y ká»¹ thuáº­t tinh chá»‰nh: Tinh chá»‰nh Ä‘áº§y Ä‘á»§, ICL, LoRA [24], IA3 [37], Prompt tuning [30], Prefix tuning [33], vÃ  QLoRA [13]. ChÃºng tÃ´i triá»ƒn khai táº¥t cáº£ cÃ¡c ká»¹ thuáº­t tinh chá»‰nh sá»­ dá»¥ng thÆ° viá»‡n HuggingFace [79] vÃ  PEFT [43].

ChÃºng tÃ´i chá»‰ sá»­ dá»¥ng tinh chá»‰nh Ä‘áº§y Ä‘á»§ cho SLMs, vÃ¬ viá»‡c tinh chá»‰nh táº¥t cáº£ cÃ¡c tham sá»‘ cá»§a LLMs lÃ  khÃ´ng thá»ƒ tÃ­nh toÃ¡n Ä‘Æ°á»£c trong giá»›i háº¡n bá»™ nhá»› GPU tá»‘i Ä‘a 24GB. ChÃºng tÃ´i Ä‘áº·t learning rate thÃ nh 5ğ‘’âˆ’5. Äá»‘i vá»›i LoRA vÃ  IA3, chÃºng tÃ´i Ã¡p dá»¥ng phÃ¢n tÃ¡ch ma tráº­n thá»© háº¡ng tháº¥p trÃªn cÃ¡c lá»›p attention cá»§a cÃ¡c mÃ´ hÃ¬nh vÃ  Ä‘áº·t ğ‘Ÿ=16 vÃ  ğ›¼=32. Äá»ƒ triá»ƒn khai QLoRA, chÃºng tÃ´i sá»­ dá»¥ng lÆ°á»£ng tá»­ hÃ³a 8-bit vÃ  4-bit [12]. ChÃºng tÃ´i Ä‘áº·t learning rate thÃ nh 3ğ‘’âˆ’4 cho LoRA, IA3 vÃ  QLoRA. Äá»‘i vá»›i Prompt tuning vÃ  Prefix tuning, chÃºng tÃ´i thÃªm vÃ o trÆ°á»›c má»™t táº­p há»£p 20 token áº£o cÃ³ thá»ƒ huáº¥n luyá»‡n liÃªn tá»¥c cho má»—i máº«u Ä‘áº§u vÃ o cá»§a cÃ¡c mÃ´ hÃ¬nh vÃ  Ã¡p dá»¥ng learning rates 3ğ‘’âˆ’3 vÃ  3ğ‘’âˆ’2.

ChÃºng tÃ´i sá»­ dá»¥ng optimizer Adafactor [60] vá»›i Ä‘á»™ chÃ­nh xÃ¡c float 16-bit cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh. ChÃºng tÃ´i tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh trong tá»‘i Ä‘a nÄƒm epochs vÃ  Ä‘Ã¡nh giÃ¡ chÃºng má»—i 0.2âˆ—ğ‘™ğ‘’ğ‘›(ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›_ğ‘ ğ‘’ğ‘¡) bÆ°á»›c tá»‘i Æ°u hÃ³a. ChÃºng tÃ´i tinh chá»‰nh táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh vá»›i batch size 8. ChÃºng tÃ´i chá»n checkpoint vá»›i evaluation loss tháº¥p nháº¥t cho suy luáº­n vÃ  tháº¥y ráº±ng beam search vá»›i beam size 10 mang láº¡i hiá»‡u quáº£ tá»‘t nháº¥t. Vá»›i phÃ¢n phá»‘i Ä‘á»™ dÃ i token khÃ¡c nhau vÃ  Ä‘á»™ phá»©c táº¡p cá»§a cÃ¡c bá»™ dá»¯ liá»‡u, chÃºng tÃ´i sinh mÃ£ vá»›i lÃªn Ä‘áº¿n 64,

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 12 ---
12 â€¢M. Weyssow et al.

0 1 2 3 4 5    8    16
Sá»‘ lÆ°á»£ng vÃ­ dá»¥ ngáº«u nhiÃªn
0
5
10
15
20
25
30
EM@10
Conala

0 1 2 3 4 5    8
Sá»‘ lÆ°á»£ng vÃ­ dá»¥ ngáº«u nhiÃªn
0
2
4
6
8
10
12
EM@10
CodeAlpacaPy

CodeLlama-7B-Python
CodeLlama-7B-Instruct
CodeLlama-7B
CodeGen2-7B
CodeGen2-3.7B
CodeGen2-1B
CodeGen-350M-mono
CodeT5+-770M
CodeT5+-220M

HÃ¬nh 3. [RQ1] â€“ Hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng ICL vá»›i sá»‘ lÆ°á»£ng vÃ­ dá»¥ ngáº«u nhiÃªn khÃ¡c nhau trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy.

128, vÃ  1024 tokens cho Conala, CodeAlpacaPy, vÃ  APPS, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i cÃ´ng khai mÃ£ cá»§a chÃºng tÃ´i táº¡i: https://github.com/martin-wey/peft-llm-code.

5 Káº¾T QUáº¢ THÃ NGHIá»†M

5.1 RQ1: Hiá»‡u quáº£ CÆ¡ sá»Ÿ cá»§a MÃ´ hÃ¬nh Sá»­ dá»¥ng Zero-Shot vÃ  ICL

ChÃºng tÃ´i báº¯t Ä‘áº§u báº±ng cÃ¡ch Ä‘iá»u tra hiá»‡u quáº£ cÆ¡ sá»Ÿ cá»§a táº¥t cáº£ SLMs vÃ  LLMs cho sinh mÃ£ dá»±a trÃªn khá»›p. Cá»¥ thá»ƒ, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p zero-shot vÃ  ICL vá»›i lÃªn Ä‘áº¿n 16 vÃ­ dá»¥ ngáº«u nhiÃªn Ä‘Æ°á»£c truy xuáº¥t cho bá»™ dá»¯ liá»‡u Conala vÃ  tÃ¡m cho bá»™ dá»¯ liá»‡u CodeAlpacaPy. LÃ½ do Ä‘áº±ng sau viá»‡c sá»­ dá»¥ng Ã­t vÃ­ dá»¥ hÆ¡n cho CodeAlpacaPy lÃ  vÃ¬ xem xÃ©t 16 vÃ­ dá»¥ dáº«n Ä‘áº¿n lá»—i háº¿t bá»™ nhá»› trong thiáº¿t láº­p cá»§a chÃºng tÃ´i. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng EM@10 vÃ  so sÃ¡nh chÃºng trÃªn hai bá»™ dá»¯ liá»‡u nÃ y trong HÃ¬nh 3. LÆ°u Ã½ ráº±ng kiáº¿n trÃºc CodeGen2 dáº«n Ä‘áº¿n viá»‡c sá»­ dá»¥ng bá»™ nhá»› GPU Ä‘Ã¡ng ká»ƒ hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ¡c, Ä‘iá»u nÃ y giáº£i thÃ­ch táº¡i sao chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ ICL vá»›i Ã­t vÃ­ dá»¥ hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ¡c.

Äáº§u tiÃªn, chÃºng tÃ´i quan sÃ¡t má»™t khoáº£ng cÃ¡ch Ä‘Ã¡ng ká»ƒ trong EM@10 giá»¯a hai bá»™ dá»¯ liá»‡u. Sá»± khÃ¡c biá»‡t nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi thá»±c táº¿ lÃ  bá»™ dá»¯ liá»‡u CodeAlpacaPy chá»©a cÃ¡c máº«u thÃ¡ch thá»©c hÆ¡n nhiá»u so vá»›i bá»™ dá»¯ liá»‡u Conala, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 2.

Thá»© hai, cÃ³ má»™t khoáº£ng cÃ¡ch Ä‘Ã¡ng chÃº Ã½ vá» hiá»‡u quáº£ giá»¯a SLMs vÃ  LLMs, báº¥t ká»ƒ sá»‘ lÆ°á»£ng vÃ­ dá»¥ Ä‘Æ°á»£c cung cáº¥p. Quan sÃ¡t nÃ y ná»•i báº­t nhá»¯ng Æ°u Ä‘iá»ƒm cá»§a pre-training quy mÃ´ lá»›n vÃ  viá»‡c sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n trong bá»‘i cáº£nh nÃ y.

Äá»‘i vá»›i bá»™ dá»¯ liá»‡u Conala, viá»‡c tÄƒng sá»‘ lÆ°á»£ng vÃ­ dá»¥ dáº«n Ä‘áº¿n Ä‘iá»ƒm EM@10 cao hÆ¡n. Tuy nhiÃªn, khi sá»­ dá»¥ng hÆ¡n tÃ¡m vÃ­ dá»¥, hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh báº¯t Ä‘áº§u giáº£m. Äá»‘i vá»›i bá»™ dá»¯ liá»‡u CodeAlpacaPy, má»™t xu hÆ°á»›ng tÆ°Æ¡ng tá»± Ä‘Æ°á»£c quan sÃ¡t, nhÆ°ng sá»‘ lÆ°á»£ng vÃ­ dá»¥ tá»‘i Æ°u nhá» hÆ¡n. Háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Ä‘áº¡t Ä‘iá»ƒm EM@10 tá»‘t nháº¥t khi sá»­ dá»¥ng ba hoáº·c bá»‘n vÃ­ dá»¥. Quan sÃ¡t nÃ y nháº¥n máº¡nh háº¡n cháº¿ cá»§a ICL, vÃ¬ viá»‡c thÃªm nhiá»u vÃ­ dá»¥ hÆ¡n dáº«n Ä‘áº¿n suy giáº£m hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 13 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢13

Cuá»‘i cÃ¹ng, cÃ¡c mÃ´ hÃ¬nh CodeLlama vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh trÃªn cáº£ hai bá»™ dá»¯ liá»‡u, Ä‘áº¡t EM@10 Ä‘á»‰nh 29.83 trÃªn Conala (CodeLlama-7B) vÃ  11.94 trÃªn CodeAlpacaPy (CodeLlama-7B-Python). NgÆ°á»£c láº¡i, cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n, nhÆ° CodeGen2-3.7B Ä‘áº¡t EM@10 23.94 vÃ  7.00 trÃªn Conala vÃ  CodeAlpacaPy, tÆ°Æ¡ng á»©ng.

Tráº£ lá»i cho RQ1: ICL cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh so vá»›i zero-shot. MÃ´ hÃ¬nh tá»‘t nháº¥t cá»§a chÃºng tÃ´i, CodeLlama-7B, Ä‘áº¡t Ä‘iá»ƒm EM@10 29.83 (7.73) vÃ  11.62 (7.01) trÃªn Conala vÃ  CodeAlpacaPy vá»›i ICL (zero-shot), tÆ°Æ¡ng á»©ng.

5.2 RQ2: Hiá»‡u quáº£ cá»§a MÃ´ hÃ¬nh sá»­ dá»¥ng Ká»¹ thuáº­t PEFT

ChÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ chi tiáº¿t vá» hiá»‡u quáº£ cá»§a SLMs vÃ  LLMs trÃªn sinh mÃ£ dá»±a trÃªn khá»›p cho cáº£ bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy trong Báº£ng 3.

SLMs vs. LLMs. CodeGen-350M-mono vá»›i LoRA thá»ƒ hiá»‡n hiá»‡u quáº£ tá»‘t nháº¥t trung bÃ¬nh trong sá»‘ cÃ¡c mÃ´ hÃ¬nh nhá», trong khi CodeLlama-7B-Python vá»›i LoRA lÃ  LLM tá»‘t nháº¥t trung bÃ¬nh. DÆ°á»›i cÃ¹ng má»™t háº¡n cháº¿ bá»™ nhá»› GPU 24GB, LLM tá»‘t nháº¥t vÆ°á»£t trá»™i hÆ¡n mÃ´ hÃ¬nh nhá» tá»‘t nháº¥t 39.8%, 41.7%, vÃ  47.1% (72.3%, 48.8%, vÃ  9.1%) trong EM@1, EM@10, vÃ  CodeBLEU liÃªn quan Ä‘áº¿n bá»™ dá»¯ liá»‡u Conala (CodeAlpacaPy), tÆ°Æ¡ng á»©ng.

SLMs. Trong sá»‘ SLMs, CodeGen-350M-mono cho tháº¥y hiá»‡u quáº£ cao nháº¥t trÃªn táº¥t cáº£ metrics trÃªn cáº£ hai bá»™ dá»¯ liá»‡u. Káº¿t quáº£ cá»§a chÃºng tÃ´i phÃ¹ há»£p vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y [48,73,91] Ä‘Ã£ xÃ¡c Ä‘á»‹nh CodeGen-350M-mono lÃ  má»™t SLM máº¡nh máº½ cho cÃ¡c tÃ¡c vá»¥ sinh mÃ£ Python. ThÃº vá»‹ lÃ , máº·c dÃ¹ yÃªu cáº§u tinh chá»‰nh khoáº£ng 1% tá»•ng sá»‘ tham sá»‘ cá»§a mÃ´ hÃ¬nh, LoRA xuáº¥t hiá»‡n nhÆ° ká»¹ thuáº­t tinh chá»‰nh tá»‘t nháº¥t, vÆ°á»£t trá»™i hÆ¡n tinh chá»‰nh Ä‘áº§y Ä‘á»§ vá»›i má»™t khoáº£ng cÃ¡ch Ä‘Ã¡ng ká»ƒ trÃªn gáº§n nhÆ° táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh. VÃ­ dá»¥, Ä‘iá»ƒm EM@10 cho CodeGen-350M-mono trÃªn bá»™ dá»¯ liá»‡u Conala, vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§, lÃ  18.42, trong khi nÃ³ tÄƒng vá»t lÃªn 25.60 vá»›i LoRA.

LLMs. Trong HÃ¬nh 4, chÃºng tÃ´i trÃ¬nh bÃ y má»™t phÃ¢n tÃ­ch so sÃ¡nh vá» hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh khi Ä‘Æ°á»£c tinh chá»‰nh báº±ng LoRA, táº­p trung vÃ o Ä‘iá»ƒm CodeBLEU vÃ  EM@10. Cáº£ hai biá»ƒu Ä‘á»“ Ä‘á»u rÃµ rÃ ng thiáº¿t láº­p cÃ¡c mÃ´ hÃ¬nh CodeLlama lÃ  LLMs hoáº¡t Ä‘á»™ng tá»‘t nháº¥t trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i. ÄÃ¡ng chÃº Ã½, CodeGen2-7B, máº·c dÃ¹ chia sáº» sá»‘ lÆ°á»£ng tham sá»‘ tÆ°Æ¡ng tá»±, tá»¥t háº­u so vá»›i táº¥t cáº£ cÃ¡c biáº¿n thá»ƒ CodeLlama-7B. KhÃ´ng cÃ³ gÃ¬ Ä‘Ã¡ng ngáº¡c nhiÃªn, viá»‡c khai thÃ¡c cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n dáº«n Ä‘áº¿n hiá»‡u quáº£ tá»‘t hÆ¡n. Vá»›i chi phÃ­ tÃ­nh toÃ¡n tháº¥p cá»§a cÃ¡c ká»¹ thuáº­t PEFT, viá»‡c táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n trong bá»‘i cáº£nh tÆ°Æ¡ng tá»± nhÆ° cá»§a chÃºng tÃ´i cÃ³ váº» pháº£n tÃ¡c dá»¥ng. Sau Ä‘Ã³, trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i chá»©ng minh ráº±ng tháº­m chÃ­ cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh thÃ´ng qua sá»± káº¿t há»£p cá»§a PEFT vá»›i lÆ°á»£ng tá»­ hÃ³a.

Ká»¹ thuáº­t PEFT tá»‘t nháº¥t. Tá»•ng thá»ƒ, LoRA xuáº¥t hiá»‡n nhÆ° ká»¹ thuáº­t PEFT hiá»‡u quáº£ nháº¥t trong sá»‘ nhá»¯ng ká»¹ thuáº­t Ä‘Æ°á»£c nghiÃªn cá»©u. Máº·c dÃ¹ Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° má»™t cáº£i tiáº¿n tÄƒng dáº§n so vá»›i LoRA [37], IA3 thÆ°á»ng cho tháº¥y Ä‘iá»ƒm sá»‘ tháº¥p hÆ¡n so vá»›i LoRA. Prompt tuning xuáº¥t hiá»‡n nhÆ° má»™t lá»±a chá»n tinh chá»‰nh kháº£ thi khÃ¡c, trong khi giáº£m thÃªm sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n. Tuy nhiÃªn, Prefix tuning khÃ´ng thÃ nh cÃ´ng trong viá»‡c thÃ­ch á»©ng hiá»‡u quáº£ cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n vá»›i cáº£ hai bá»™ dá»¯ liá»‡u.

PhÃ¢n tÃ­ch cá»§a chÃºng tÃ´i tiáº¿t lá»™ Ä‘iá»ƒm EM cao hÆ¡n Ä‘Ã¡ng ká»ƒ cho bá»™ dá»¯ liá»‡u Conala, cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho sá»± khÃ¡c biá»‡t vá» Ä‘á»™ phá»©c táº¡p tÃ¡c vá»¥ giá»¯a hai bá»™ dá»¯ liá»‡u (xem Pháº§n 4.2). Quan trá»ng cáº§n lÆ°u Ã½ ráº±ng Ä‘iá»ƒm CodeBLEU trÃªn Conala tÆ°Æ¡ng Ä‘á»‘i tháº¥p hÆ¡n do sá»± phá»¥ thuá»™c cá»§a metric vÃ o tÃ­nh toÃ¡n Ä‘á»“ thá»‹ dÃ²ng dá»¯ liá»‡u, cÃ³ thá»ƒ khÃ´ng pháº£i lÃºc nÃ o cÅ©ng cÃ³ sáºµn cho cÃ¡c vÃ­ dá»¥ mÃ£ nhá».

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 14 ---
14 â€¢M. Weyssow et al.

Báº£ng 3. [RQ2] â€“ So sÃ¡nh SLMs vÃ  LLMs sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t tinh chá»‰nh khÃ¡c nhau (xanh dÆ°Æ¡ng: phÆ°Æ¡ng phÃ¡p tinh chá»‰nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t cho má»—i mÃ´ hÃ¬nh, cam: mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t tá»•ng thá»ƒ).

                        Conala                                   CodeAlpacaPy
MÃ´ hÃ¬nh      Tinh chá»‰nh    # Params   EM@1    EM@10   CodeBLEU   EM@1    EM@10   CodeBLEU

SLMs
CodeT5+-220M Full FT       220M       3.87    8.84    16.70      3.98    7.64    25.49
             LoRA          2.7M       6.08    12.71   18.96      3.34    5.26    24.32
             IA3           0.17M      4.42    10.68   17.08      0.64    1.27    22.06
             Prompt tuning 0.03M      4.79    9.21    16.30      0.96    2.07    19.01
             Prefix tuning 0.18M      3.13    7.55    14.56      0.16    1.27    20.80

CodeT5+-770M Full FT       770M       4.05    8.29    15.11      3.19    6.21    27.73
             LoRA          7M         8.66    17.13   20.64      3.66    6.85    26.10
             IA3           0.4M       8.10    17.50   18.68      2.87    5.26    25.84
             Prompt tuning 0.04M      7.37    15.47   16.75      1.91    3.82    20.57
             Prefix tuning 0.5M       4.97    11.97   16.77      0.16    1.27    22.91

CodeGen-350M-mono Full FT  350M       7.92    18.42   14.68      2.23    5.73    21.78
             LoRA          1.3M       12.52   25.60   17.89      4.62    10.70   30.09
             IA3           0.16M      11.42   25.78   18.83      4.46    10.70   28.56
             Prompt tuning 0.02M      7.92    20.26   16.29      0.0     0.0     25.91
             Prefix tuning 0.4M       5.34    12.52   17.53      0.0     0.0     26.89

LLMs
CodeGen2-1B  LoRA          2M         9.39    23.02   19.76      3.82    9.08    23.48
             IA3           0.2M       10.13   22.84   18.64      3.82    9.87    24.42
             Prompt tuning 0.04M      11.97   22.65   18.38      0.80    2.07    18.17
             Prefix tuning 0.6M       5.89    15.84   18.46      0.0     0.32    13.68

CodeGen2-3.7B LoRA         4M         11.60   25.97   19.00      5.41    10.70   23.75
             IA3           0.5M       10.87   25.23   19.21      5.41    10.99   26.26
             Prompt tuning 0.08M      11.05   26.89   19.53      0.0     0.0     23.42
             Prefix tuning 1.3M       10.68   24.68   20.23      0.16    0.32    21.73

CodeGen2-7B  LoRA          8.3M       11.23   29.83   23.86      5.57    11.94   27.73
             IA3           1M         11.42   29.65   21.98      5.73    12.42   28.26
             Prompt tuning 0.08M      11.97   27.26   22.37      0.0     0.0     25.40
             Prefix tuning 2.6M       9.95    23.94   22.29      0.0     0.32    25.72

CodeLlama-7B LoRA          12.5M      20.07   39.31   25.33      7.33    16.24   32.05
             IA3           1M         17.68   37.20   23.19      8.12    15.45   30.47
             Prompt tuning 0.08M      19.15   38.12   25.01      0.32    0.48    31.55
             Prefix tuning 2.6M       8.47    19.52   23.19      0.16    0.16    28.09

CodeLlama-7B-Instruct LoRA 12.5M      17.68   36.28   24.27      7.01    17.04   31.42
             IA3           1M         15.84   36.10   24.71      8.12    16.72   31.01
             Prompt tuning 0.08M      18.97   35.54   25.77      1.59    3.50    31.14
             Prefix tuning 2.6M       10.13   18.23   23.66      0.64    0.96    31.27

CodeLlama-7B-Python LoRA  12.5M      17.50   36.28   24.27      7.96    15.92   32.84
             IA3           1M         14.55   31.12   24.74      8.76    16.56   29.82
             Prompt tuning 0.08M      16.76   37.02   26.31      0.96    3.03    33.46
             Prefix tuning 2.6M       9.76    22.47   19.47      0.0     0.0     30.71

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 15 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢15

15  20  25  30  35
EM@10
18
19
20
21
22
23
24
25
CodeBLEU

CodeT5+-220M    CodeT5+-770M
CodeGen2-1B
CodeGen-350M-mono    CodeGen2-3.7B    CodeGen2-7B    CodeLlama-7B-Python    CodeLlama-7B-Instruct    CodeLlama-7B

(a) Conala

6   8   10  12  14  16
EM@10
24
26
28
30
32
CodeBLEU

CodeT5+-220M    CodeT5+-770M
CodeGen2-1B    CodeGen2-3.7B    CodeGen-350M-mono
CodeGen2-7B    CodeLlama-7B    CodeLlama-7B-Python
CodeLlama-7B-Instruct

(b) CodeAlpacaPy

HÃ¬nh 4. [RQ2] â€“ Hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh báº±ng LoRA cho cáº£ hai bá»™ dá»¯ liá»‡u theo EM@10 vÃ  CodeBLEU.

áº¢nh hÆ°á»Ÿng cá»§a lÆ°á»£ng tá»­ hÃ³a vá»›i QLoRA. ChÃºng tÃ´i khÃ¡m phÃ¡ nhá»¯ng lá»£i Ã­ch tiá»m nÄƒng cá»§a viá»‡c sá»­ dá»¥ng QLoRA [13], má»™t ká»¹ thuáº­t hiá»‡u quáº£ tÃ­nh toÃ¡n káº¿t há»£p LoRA vá»›i lÆ°á»£ng tá»­ hÃ³a 8-bit hoáº·c 4-bit Ä‘á»ƒ tinh chá»‰nh LLMs. Trong HÃ¬nh 5, chÃºng tÃ´i hiá»ƒn thá»‹ Ä‘iá»ƒm EM@10 cho ba biáº¿n thá»ƒ mÃ´ hÃ¬nh CodeLlama: CodeLlama-7B-Python, CodeLlama-13B-Python, vÃ  CodeLlama-34B-Python, cÃ¹ng vá»›i má»©c tiÃªu thá»¥ bá»™ nhá»› GPU Ä‘á»‰nh luÃ´n dÆ°á»›i 24GB cho má»—i cáº¥u hÃ¬nh tinh chá»‰nh. Káº¿t quáº£ nháº¥n máº¡nh má»™t cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh lÆ°á»£ng tá»­ hÃ³a lá»›n hÆ¡n trÃªn Conala, vá»›i tÃ¡c Ä‘á»™ng vá»«a pháº£i hÆ¡n trÃªn CodeAlpacaPy. VÃ­ dá»¥, CodeLlama-34B-Python, Ä‘Æ°á»£c tinh chá»‰nh vá»›i QLoRA-4bit, Ä‘áº¡t Ä‘Æ°á»£c sá»± gia tÄƒng Ä‘Ã¡ng ká»ƒ 12.2% trong Ä‘iá»ƒm EM@10 cá»§a Conala (40.70) so vá»›i CodeLlama-7B-Python vá»›i LoRA (36.28). ÄÃ¡ng ngáº¡c nhiÃªn, QLoRA cÅ©ng mang láº¡i nhá»¯ng cáº£i thiá»‡n Ä‘Ã¡ng chÃº Ã½ so vá»›i LoRA cho CodeLlama-7B-Python trÃªn Conala, trong khi Ä‘áº¡t káº¿t quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng trÃªn CodeAlpacaPy. Viá»‡c Ã¡p dá»¥ng lÆ°á»£ng tá»­ hÃ³a cho phÃ©p sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c chá»©a trong má»™t GPU 24GB duy nháº¥t. Cá»¥ thá»ƒ, Ä‘á»‘i vá»›i CodeLlama-7B-Python, QLoRA-4bit Ä‘áº¡t Ä‘Æ°á»£c sá»± giáº£m Ä‘Ã¡ng ká»ƒ 2x trong viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‰nh trong khi cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘iá»ƒm EM@10.

Tráº£ lá»i cho RQ2: LLMs vá»›i PEFT luÃ´n vÃ  Ä‘Ã¡ng ká»ƒ vÆ°á»£t trá»™i hÆ¡n SLMs dÆ°á»›i cÃ¹ng giá»›i háº¡n GPU. Cá»¥ thá»ƒ, LLM hoáº¡t Ä‘á»™ng tá»‘t nháº¥t vá»›i PEFT vÆ°á»£t trá»™i hÆ¡n mÃ´ hÃ¬nh nhá» tá»‘t nháº¥t 39.8â€“72.3% vá» EM@ğ‘˜. Trong sá»‘ cÃ¡c ká»¹ thuáº­t PEFT khÃ¡c nhau, LoRA lÃ  hiá»‡u quáº£ nháº¥t. NgoÃ i ra, viá»‡c Ã¡p dá»¥ng lÆ°á»£ng tá»­ hÃ³a vá»›i LoRA dáº«n Ä‘áº¿n giáº£m máº¡nh viá»‡c sá»­ dá»¥ng GPU trong khi duy trÃ¬ hiá»‡u quáº£ trÃªn cáº£ hai bá»™ dá»¯ liá»‡u vÃ  chá»©a Ä‘Æ°á»£c viá»‡c tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n lÃªn Ä‘áº¿n 34B tham sá»‘.

5.3 RQ3: PhÃ¢n tÃ­ch So sÃ¡nh LoRA, ICL, vÃ  RAG

Trong RQ nÃ y, chÃºng tÃ´i nháº±m Ä‘iá»u tra liá»‡u cÃ¡c ká»¹ thuáº­t PEFT cÃ³ luÃ´n vÆ°á»£t trá»™i hÆ¡n ICL vÃ  RAG Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i khi Ã¡p dá»¥ng LLMs trong sinh mÃ£ dá»±a trÃªn khá»›p.

Trong HÃ¬nh 6, chÃºng tÃ´i so sÃ¡nh hiá»‡u quáº£ cá»§a SLMs vÃ  LLMs sá»­ dá»¥ng ICL vÃ  LoRA vá» CodeBLEU vÃ  EM@10. Trong hÃ¬nh nÃ y, chÃºng tÃ´i bÃ¡o cÃ¡o cÃ¡c metrics cao nháº¥t Ä‘áº¡t Ä‘Æ°á»£c trÃªn cÃ¡c cáº¥u hÃ¬nh ICL khÃ¡c nhau cho má»—i mÃ´ hÃ¬nh. Trong HÃ¬nh 7, chÃºng tÃ´i khÃ¡m phÃ¡ hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh CodeLlama sá»­ dá»¥ng RAG, vá»›i lÃªn Ä‘áº¿n 16 vÃ  4 vÃ­ dá»¥ Ä‘Æ°á»£c truy xuáº¥t cho Conala vÃ  CodeAlpacaPy, tÆ°Æ¡ng á»©ng. TÆ°Æ¡ng tá»± nhÆ° RQ1, chÃºng tÃ´i sá»­ dá»¥ng Ã­t vÃ­ dá»¥ hÆ¡n cho CodeAlpacaPy Ä‘á»ƒ trÃ¡nh lá»—i háº¿t bá»™ nhá»›. ChÃºng tÃ´i so sÃ¡nh hiá»‡u quáº£ cá»§a RAG vá»›i LoRA vÃ  Ä‘iá»ƒm EM@10 tá»‘t nháº¥t Ä‘áº¡t Ä‘Æ°á»£c báº±ng ICL.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 16 ---
16 â€¢M. Weyssow et al.

CL-7B   CL-13B   CL-34B
30
32
34
36
38
40
EM@10
36.3
37.4
38.5
37.8
38.1
40.7
Conala

CL-7B   CL-13B   CL-34B
0
5
10
15
15.9
16.4
17.4
15.8
16.4
17.4
CodeAlpacaPy

0
10
20
GPU peak (GB)
24 GB

0
10
20
24 GB

LoRA    QLoRA-8bit    QLoRA-4bit

HÃ¬nh 5. [RQ2] â€“ Hiá»‡u quáº£ vÃ  sá»­ dá»¥ng GPU cá»§a 7B, 13B, vÃ  34B CodeLlama-Python (CL) LLMs Ä‘Æ°á»£c tinh chá»‰nh báº±ng LoRA vÃ  QLoRA vá»›i lÆ°á»£ng tá»­ hÃ³a 8-bit vÃ  4-bit.

0   5   10  15  20  25  30  35  40
EM@10
12
15
18
21
24
27
CodeBLEU
Conala

Tinh chá»‰nh
LoRA
ICL

0   3   6   9   12  15
EM@10
18
20
22
24
26
28
30
32
CodeBLEU
CodeAlpacaPy

CodeT5+-220M
CodeT5+-770M
CodeGen-350M-mono
CodeGen2-1B
CodeGen2-3.7B
CodeGen2-7B
CodeLlama-7B
CodeLlama-7B-Instruct
CodeLlama-7B-Python

HÃ¬nh 6. [RQ3] â€“ So sÃ¡nh hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh báº±ng LoRA vÃ  ICL trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy.

LoRA vs. ICL. NhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 6, táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh vá»›i LoRA thá»ƒ hiá»‡n Ä‘iá»ƒm EM@10 cao hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i ICL trÃªn cáº£ hai bá»™ dá»¯ liá»‡u. VÃ­ dá»¥, CodeLlama-7B-Python vá»›i tinh chá»‰nh LoRA Ä‘áº¡t cáº£i thiá»‡n 23.1% trong EM@10 trÃªn Conala (36.28 cho LoRA so vá»›i 29.47 cho ICL). MÃ´ hÃ¬nh nÃ y

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 17 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢17

0
10
20
30
40
EM@10

LoRA: 39.31
ICL: 29.83

LoRA: 36.28
ICL: 27.99

LoRA: 36.28
ICL: 29.47

CL-7B    CL-7B-Instruct    CL-7B-Python

0
5
10
15
EM@10

LoRA: 16.24
ICL: 11.62

LoRA: 17.04
ICL: 10.35

LoRA: 15.92
ICL: 11.94

# Examples
1
2
3
4
5
8
16

HÃ¬nh 7. [RQ3] â€“ So sÃ¡nh hiá»‡u quáº£ cá»§a RAG vá»›i sá»‘ lÆ°á»£ng vÃ­ dá»¥ Ä‘Æ°á»£c truy xuáº¥t khÃ¡c nhau so vá»›i ICL vÃ  LoRA trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala (trÃªn) vÃ  CodeAlpacaPy (dÆ°á»›i). Äiá»ƒm ICL mÃ´ táº£ Ä‘iá»ƒm cao nháº¥t Ä‘áº¡t Ä‘Æ°á»£c cho má»—i mÃ´ hÃ¬nh trong RQ1.

pattern giá»¯ cho CodeAlpacaPy, vá»›i nhá»¯ng tÄƒng trÆ°á»Ÿng tÆ°Æ¡ng Ä‘á»‘i lá»›n hÆ¡n trong EM@10. Tuy nhiÃªn, chÃºng tÃ´i quan sÃ¡t má»™t sá»‘ biáº¿n Ä‘á»™ng trong Ä‘iá»ƒm CodeBLEU cho háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh trÃªn CodeAlpacaPy. VÃ­ dá»¥, CodeLlama-7B tháº¥y sá»± gia tÄƒng CodeBLEU 2.36 vá»›i LoRA. TrÃªn CoNala, tuy nhiÃªn, tÃ¡c Ä‘á»™ng cá»§a LoRA trÃªn CodeBLEU Ã­t rÃµ rÃ ng hÆ¡n so vá»›i ICL. Nhá»¯ng khÃ¡c biá»‡t nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi báº£n cháº¥t cá»§a cÃ¡c metrics: EM@10 báº£o thá»§ hÆ¡n, yÃªu cáº§u giáº£i phÃ¡p Ä‘Æ°á»£c sinh pháº£i khá»›p chÃ­nh xÃ¡c vá»›i ground truth, trong khi CodeBLEU cho Ä‘iá»ƒm cao hÆ¡n cho cÃ¡c giáº£i phÃ¡p gáº§n nhÆ°ng khÃ´ng chÃ­nh xÃ¡c. Sá»± phÃ¢n biá»‡t nÃ y ná»•i báº­t cÃ¡ch LoRA thÃ­ch á»©ng tá»‘t hÆ¡n cÃ¡c mÃ´ hÃ¬nh vá»›i cÃ¡c bá»™ dá»¯ liá»‡u downstream, Ä‘áº·c biá»‡t khi Ä‘á»™ chÃ­nh xÃ¡c lÃ  quan trá»ng.

RAG vs. ICL vs. LoRA. Trong viá»‡c so sÃ¡nh RAG, ICL, vÃ  LoRA trÃªn bá»™ dá»¯ liá»‡u CoNala, RAG thá»ƒ hiá»‡n hiá»‡u quáº£ cao hÆ¡n ICL nhÆ°ng khÃ´ng Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ cá»§a LoRA trÃªn táº¥t cáº£ ba biáº¿n thá»ƒ mÃ´ hÃ¬nh CodeLlama. ÄÃ¡ng chÃº Ã½, CodeLlama-7B Ä‘áº¡t tá»‘i Ä‘a 29.83 vÃ  35.17 EM@10 vá»›i ICL vÃ  RAG, tÆ°Æ¡ng á»©ng, trong khi mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh vá»›i LoRA Ä‘áº¡t EM@10 39.31.

Äá»‘i vá»›i cáº£ bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy, nhá»¯ng tÄƒng trÆ°á»Ÿng trong EM@10 trá»Ÿ nÃªn má»ng hÆ¡n khi chÃºng tÃ´i tÄƒng sá»‘ lÆ°á»£ng vÃ­ dá»¥ sá»­ dá»¥ng RAG. EM@10 bÃ£o hÃ²a á»Ÿ khoáº£ng 8â€“16 vÃ­ dá»¥ cho Conala vÃ  3â€“4 vÃ­ dá»¥ cho CodeAlpacaPy. HÆ¡n ná»¯a, chÃºng tÃ´i lÆ°u Ã½ ráº±ng Ä‘á»‘i vá»›i bá»™ dá»¯ liá»‡u CodeAlpacaPy thÃ¡ch thá»©c hÆ¡n, RAG mang láº¡i EM@10 tháº¥p hÆ¡n so vá»›i cÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn sá»­ dá»¥ng ICL, ná»•i báº­t nhá»¯ng háº¡n cháº¿ cá»§a RAG khi Ä‘á»™ phá»©c táº¡p váº¥n Ä‘á» tÄƒng. LoRA, tuy nhiÃªn, luÃ´n vÆ°á»£t trá»™i hÆ¡n cáº£ RAG vÃ  ICL trÃªn CodeAlpacaPy, ná»•i báº­t kháº£ nÄƒng Æ°u viá»‡t cá»§a nÃ³ trong viá»‡c thÃ­ch á»©ng vá»›i cÃ¡c bá»™ dá»¯ liá»‡u thÃ¡ch thá»©c hÆ¡n.

Tráº£ lá»i cho RQ3: LoRA Æ°u viá»‡t hÆ¡n ICL vÃ  RAG trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy trÃªn ba biáº¿n thá»ƒ CodeLlama-7B.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 18 ---
18 â€¢M. Weyssow et al.

Báº£ng 4. [RQ4] â€“ Hiá»‡u quáº£ cá»§a CodeLlama-7B-Instruct trÃªn bá»™ dá»¯ liá»‡u APPs trong zero-shot vÃ  sá»­ dá»¥ng LoRA, QLoRA-8bit, vÃ  QLoRA-4bit theo sá»‘ test cases vÆ°á»£t qua trung bÃ¬nh (Avg) vÃ  Pass@k (P@k).

                           Introductory            Interview              Competition
MÃ´ hÃ¬nh                    Avg    P@1  P@2  P@5   Avg    P@1  P@2  P@5   Avg    P@1  P@2  P@5
CodeLlama-7B-Instruct      13.66  4.16 6.24 8.80  13.44  0.80 1.32 2.40  6.27   0.56 1.00 2.00
+LoRA                      19.57  5.60 8.04 11.20 16.96  1.04 1.80 3.20  6.93   0.32 0.60 1.20
+QLoRA-8bit                17.63  3.68 5.40 7.60  15.53  1.04 1.64 2.40  7.59   0.24 0.48 1.20
+QLoRA-4bit                20.84  5.76 8.40 12.40 20.34  1.04 1.76 2.80  6.66   0.48 0.88 1.60

5.4 RQ4: KhÃ¡m phÃ¡ LoRA vÃ  QLoRA cho Sinh MÃ£ trÃªn APPs

Trong RQ cuá»‘i cÃ¹ng nÃ y, chÃºng tÃ´i khÃ¡m phÃ¡ kháº£ nÄƒng Ã¡p dá»¥ng rá»™ng hÆ¡n cá»§a LoRA vÃ  QLoRA, Ä‘á»ƒ nÃ¢ng cao hiá»‡u quáº£ cá»§a CodeLlama-7B-Instruct cho sinh mÃ£ dá»±a trÃªn thá»±c thi. LÃ½ do chá»n biáº¿n thá»ƒ instruct cá»§a CodeLlama-7B lÃ  vÃ¬ mÃ´ hÃ¬nh thÆ°á»ng cho tháº¥y hiá»‡u quáº£ cao hÆ¡n so vá»›i cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh khÃ¡c trÃªn APPs trong bÃ i bÃ¡o tiá»n Ä‘á» cá»§a CodeLlama [56]. ChÃºng tÃ´i khÃ´ng so sÃ¡nh LoRA vÃ  QLoRA vá»›i ICL vÃ  RAG cho bá»™ dá»¯ liá»‡u nÃ y vÃ¬ chÃºng yÃªu cáº§u tÄƒng Ä‘á»™ dÃ i prompt vÆ°á»£t quÃ¡ 2,048 tokens, dáº«n Ä‘áº¿n lá»—i háº¿t bá»™ nhá»›. Káº¿t quáº£ cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c tÃ³m táº¯t trong Báº£ng 4, táº­p trung vÃ o sá»‘ lÆ°á»£ng test cases vÆ°á»£t qua trung bÃ¬nh (Avg) vÃ  Pass@ğ‘˜ cho cÃ¡c tÃ¡c vá»¥ cáº¥p Ä‘á»™ giá»›i thiá»‡u, phá»ng váº¥n, vÃ  thi Ä‘áº¥u.

Äá»‘i vá»›i cáº£ tÃ¡c vá»¥ sinh mÃ£ cáº¥p Ä‘á»™ giá»›i thiá»‡u vÃ  phá»ng váº¥n, LoRA vÃ  QLoRA-8/4bit dáº«n Ä‘áº¿n nhá»¯ng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong sá»‘ lÆ°á»£ng test cases vÆ°á»£t qua trung bÃ¬nh. Cá»¥ thá»ƒ, QLoRA-4bit dáº«n Ä‘áº¿n sá»± gia tÄƒng Ä‘Ã¡ng chÃº Ã½ 52% trong sá»‘ lÆ°á»£ng tests vÆ°á»£t qua trung bÃ¬nh so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ. Vá» metrics Pass@ğ‘˜, cáº£ LoRA vÃ  QLoRA-4bit Ä‘á»u thá»ƒ hiá»‡n nhá»¯ng tÄƒng trÆ°á»Ÿng á»Ÿ cáº¥p Ä‘á»™ giá»›i thiá»‡u, vá»›i Pass@5 cáº£i thiá»‡n +3.60% so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ. Tuy nhiÃªn, nhá»¯ng cáº£i thiá»‡n nÃ y Ã­t Ä‘Ã¡ng ká»ƒ hÆ¡n cho sinh mÃ£ cáº¥p Ä‘á»™ phá»ng váº¥n vÃ  thi Ä‘áº¥u, pháº£n Ã¡nh Ä‘á»™ phá»©c táº¡p vÃ  thÃ¡ch thá»©c lá»›n hÆ¡n cá»§a nhá»¯ng tÃ¡c vá»¥ tiáº¿n bá»™ hÆ¡n nÃ y.

Tráº£ lá»i cho RQ4: LoRA vÃ  QLoRA nÃ¢ng cao hiá»‡u quáº£ cá»§a CodeLlama-7B-Instruct trÃªn APPs, Ä‘áº·c biá»‡t á»Ÿ cáº¥p Ä‘á»™ giá»›i thiá»‡u, vá»›i QLoRA-4bit tÄƒng sá»‘ lÆ°á»£ng test cases vÆ°á»£t qua trung bÃ¬nh lÃªn 52% vÃ  Pass@5 lÃªn 40%. Tuy nhiÃªn, nhá»¯ng cáº£i thiá»‡n Ã­t Ä‘Ã¡ng chÃº Ã½ hÆ¡n cho cÃ¡c tÃ¡c vá»¥ cáº¥p Ä‘á»™ phá»ng váº¥n vÃ  thi Ä‘áº¥u.

6 THáº¢O LUáº¬N

NghiÃªn cá»©u cá»§a chÃºng tÃ´i khÃ¡m phÃ¡ PEFTs Ä‘Æ°á»£c Ã¡p dá»¥ng cho code LLMs, lÃ m sÃ¡ng tá» tÃ¡c Ä‘á»™ng tÃ­ch cá»±c cá»§a nhá»¯ng á»©ng dá»¥ng nÃ y trong viá»‡c tinh chá»‰nh hiá»‡u quáº£ LLMs cho cÃ¡c bá»™ dá»¯ liá»‡u cá»¥ thá»ƒ tÃ¡c vá»¥ cho sinh mÃ£. Cá»¥ thá»ƒ, nghiÃªn cá»©u cá»§a chÃºng tÃ´i minh há»a tÃ­nh thá»±c táº¿ cá»§a viá»‡c tinh chá»‰nh LLMs sá»­ dá»¥ng PEFT, do Ä‘Ã³ giáº£m bá»›t sá»± phá»¥ thuá»™c cá»§a cÃ¡c nhÃ  thá»±c hÃ nh vÃ o cÆ¡ sá»Ÿ háº¡ táº§ng lá»›n vÃ  Ä‘áº¯t Ä‘á». CÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cÅ©ng chá»‰ ra má»™t sá»‘ lÄ©nh vá»±c Ä‘áº§y há»©a háº¹n cho khÃ¡m phÃ¡ tÆ°Æ¡ng lai, bao gá»“m Ä‘iá»u tra cÃ¡c ká»¹ thuáº­t hiá»‡u quáº£ trÃªn cÃ¡c cÃ i Ä‘áº·t tinh chá»‰nh Ä‘a dáº¡ng, trong quÃ¡ trÃ¬nh suy luáº­n, vÃ  cho cÃ¡c tÃ¡c vá»¥ SE khÃ¡c.

Ká»¹ thuáº­t hiá»‡u quáº£ cho LLMs vá» mÃ£. CÃ´ng viá»‡c cá»§a chÃºng tÃ´i nháº¥n máº¡nh cÃ¡c ká»¹ thuáº­t tinh chá»‰nh hiá»‡u quáº£, dÃ¢n chá»§ hÃ³a viá»‡c tinh chá»‰nh LLMs cho má»™t Ä‘á»‘i tÆ°á»£ng rá»™ng. Tuy nhiÃªn, nghiÃªn cá»©u cá»§a chÃºng tÃ´i khÃ´ng bao gá»“m khÃ¡m phÃ¡ cÃ¡c ká»¹ thuáº­t hiá»‡u quáº£ cho suy luáº­n chi phÃ­ tháº¥p. Máº·c dÃ¹ cÃ¡c ká»¹ thuáº­t PEFT yÃªu cáº§u thá»i gian tinh chá»‰nh bá»• sung so vá»›i ICL vÃ  RAG, Ä‘Ã¡ng chÃº Ã½ lÃ  nhá»¯ng ká»¹ thuáº­t nÃ y khÃ´ng Ã¡p Ä‘áº·t báº¥t ká»³ chi phÃ­ thá»i gian bá»• sung nÃ o trong quÃ¡ trÃ¬nh suy luáº­n. Tuy nhiÃªn, chÃºng tÃ´i thá»«a nháº­n sá»± cáº§n thiáº¿t cá»§a cÃ¡c Ä‘iá»u tra tÆ°Æ¡ng lai vá» cÃ¡c ká»¹ thuáº­t Ä‘á»ƒ giáº£m chi phÃ­ thá»i gian liÃªn quan Ä‘áº¿n LLMs trong quÃ¡ trÃ¬nh suy luáº­n.

PEFT vÃ  ICL/RAG lÃ  cÃ¡c ká»¹ thuáº­t khÃ´ng loáº¡i trá»« nhau cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng cÃ¹ng nhau. Tuy nhiÃªn, chÃºng tÃ´i quyáº¿t Ä‘á»‹nh khÃ´ng bao gá»“m cÃ¡c thÃ­ nghiá»‡m vá» viá»‡c Ã¡p dá»¥ng ICL/RAG cho LLMs Ä‘Æ°á»£c tinh chá»‰nh báº±ng PEFT. Trong thá»±c táº¿, viá»‡c tÄƒng sá»‘ lÆ°á»£ng vÃ­ dá»¥ ICL/RAG táº¡i suy luáº­n dáº«n Ä‘áº¿n tÄƒng overhead tÃ­nh toÃ¡n khi Ä‘á»™ dÃ i token cá»§a prompt má»Ÿ rá»™ng. Do Ä‘Ã³, chÃºng tÃ´i cho ráº±ng viá»‡c sá»­ dá»¥ng ICL/RAG trÃªn má»™t LLM Ä‘Æ°á»£c tinh chá»‰nh cÃ³ thá»ƒ pháº£n tÃ¡c dá»¥ng lÃ m tÄƒng yÃªu cáº§u tÃ­nh toÃ¡n, vÆ°á»£t trá»™i hÆ¡n nhá»¯ng lá»£i Ã­ch tiá»m nÄƒng.

Tá»« má»™t gÃ³c Ä‘á»™ khÃ¡c, cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y [18,78,83] nháº¥n máº¡nh nhu cáº§u xem xÃ©t cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c pre-train vÃ  LLMs vá» mÃ£ trong cÃ¡c cÃ i Ä‘áº·t há»c liÃªn tá»¥c. Trong mÃ´ hÃ¬nh nÃ y, mÃ´ hÃ¬nh pháº£i thÃ­ch á»©ng Ä‘á»™ng vá»›i dá»¯ liá»‡u má»›i theo thá»i gian trong khi báº£o toÃ n hiá»‡u suáº¥t trÃªn dá»¯ liá»‡u Ä‘Ã£ tháº¥y trÆ°á»›c Ä‘Ã¢y. Trong cÃ i Ä‘áº·t cá»¥ thá»ƒ cá»§a LLMs phÃ¡t triá»ƒn liÃªn tá»¥c, cÃ¡c ká»¹ thuáº­t PEFT cÃ³ kháº£ nÄƒng cung cáº¥p nhá»¯ng lá»£i Ã­ch cÃ³ giÃ¡ trá»‹. Tuy nhiÃªn,

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 19 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢19

váº«n chÆ°a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh liá»‡u cÃ¡c ká»¹ thuáº­t PEFT cÃ³ thá»ƒ thÃ­ch á»©ng hiá»‡u quáº£ LLMs trong cÃ i Ä‘áº·t há»c liÃªn tá»¥c cho cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n mÃ£, mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c giá»¯ láº¡i kiáº¿n thá»©c quÃ¡ khá»©.

Hiá»‡u quáº£ cá»§a QLoRA. TrÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u nghiÃªn cá»©u, chÃºng tÃ´i quan sÃ¡t ráº±ng QLoRA-4bit thá»ƒ hiá»‡n hiá»‡u quáº£ cáº¡nh tranh hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT khÃ¡c. ÄÃ¡ng chÃº Ã½, QLoRA-4bit vÆ°á»£t trá»™i hÆ¡n LoRA vÃ  QLoRA-8bit trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  APPs. ChÃºng tÃ´i giáº£ thuyáº¿t ráº±ng cáº£i thiá»‡n nÃ y xuáº¥t phÃ¡t tá»« hiá»‡u á»©ng Ä‘iá»u chuáº©n cá»§a viá»‡c giáº£m Ä‘á»™ chÃ­nh xÃ¡c trá»ng sá»‘ xuá»‘ng 4 bits, giÃºp á»•n Ä‘á»‹nh tinh chá»‰nh vÃ  giáº£m thiá»ƒu overfitting. Nhá»¯ng phÃ¡t hiá»‡n nÃ y ná»•i báº­t tiá»m nÄƒng cho cÃ¡c ká»¹ thuáº­t PEFT hiá»‡u quáº£ hÆ¡n, máº·c dÃ¹ cáº§n khÃ¡m phÃ¡ thÃªm Ä‘á»ƒ hiá»ƒu Ä‘áº§y Ä‘á»§ kháº£ nÄƒng Ã¡p dá»¥ng rá»™ng hÆ¡n cá»§a chÃºng.

PhÃ¡t hiá»‡n má»›i cho PEFT trong ká»¹ thuáº­t pháº§n má»m. CÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i trong RQ1 tiáº¿t lá»™ ráº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT vÆ°á»£t trá»™i hÆ¡n tinh chá»‰nh Ä‘áº§y Ä‘á»§ cho SLMs trong cÃ¡c tÃ¡c vá»¥ sinh mÃ£. Äiá»u nÃ y trÃ¡i ngÆ°á»£c vá»›i cÃ¡c nghiÃªn cá»©u quy mÃ´ lá»›n trÆ°á»›c Ä‘Ã¢y trong NLP, nhÆ° Ding et al. [14], Ä‘Ã£ chá»©ng minh tÃ­nh Æ°u viá»‡t cá»§a tinh chá»‰nh Ä‘áº§y Ä‘á»§ so vá»›i cÃ¡c ká»¹ thuáº­t nhÆ° LoRA, Prompt Tuning, vÃ  Prefix Tuning trÃªn má»™t loáº¡t rá»™ng cÃ¡c tÃ¡c vá»¥ NLP.

Trong bá»‘i cáº£nh ká»¹ thuáº­t pháº§n má»m, máº·c dÃ¹ cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y [38,40] Ä‘Ã£ cho tháº¥y ráº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT, nhÆ° LoRA, cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ cho SLMs, káº¿t quáº£ cá»§a chÃºng tÃ´i Ä‘i xa hÆ¡n. ChÃºng tÃ´i cho tháº¥y ráº±ng táº¥t cáº£ cÃ¡c ká»¹ thuáº­t PEFT Ä‘Æ°á»£c nghiÃªn cá»©u trong bÃ i bÃ¡o nÃ y vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ cho SLMs nhÆ° CodeGen-350M-mono vÃ  CodeT5+-770M trÃªn cÃ¡c bá»™ dá»¯ liá»‡u Conala vÃ  CodeAlpacaPy (xem Báº£ng 3), ná»•i báº­t nhá»¯ng Æ°u Ä‘iá»ƒm rÃµ rÃ ng cá»§a PEFT trong nhá»¯ng ká»‹ch báº£n nÃ y. Tuy nhiÃªn, do háº¡n cháº¿ tÃ i nguyÃªn, chÃºng tÃ´i khÃ´ng thá»ƒ Ä‘Ã¡nh giÃ¡ tinh chá»‰nh Ä‘áº§y Ä‘á»§ cho LLMs, Ä‘á»ƒ láº¡i khÃ´ng gian cho cÃ¡c nghiÃªn cá»©u tÆ°Æ¡ng lai khÃ¡m phÃ¡ thÃªm Ä‘iá»u nÃ y trong lÄ©nh vá»±c ká»¹ thuáº­t pháº§n má»m.

NgoÃ i ra, nghiÃªn cá»©u cá»§a chÃºng tÃ´i khÃ¡m phÃ¡ nhá»¯ng hiá»ƒu biáº¿t má»›i vá» lá»£i Ã­ch cá»§a QLoRA vÃ  hiá»‡u quáº£ so sÃ¡nh cá»§a LoRA so vá»›i RAG cho cÃ¡c tÃ¡c vá»¥ sinh mÃ£. Äáº§u tiÃªn, trong RQ3 vÃ  RQ4, chÃºng tÃ´i chá»©ng minh ráº±ng QLoRA cung cáº¥p hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c tháº­m chÃ­ Æ°u viá»‡t hÆ¡n so vá»›i LoRA trong khi cáº¯t giáº£m máº¡nh chi phÃ­ tÃ­nh toÃ¡n. Thá»© hai, chÃºng tÃ´i tiáº¿t lá»™ nhá»¯ng háº¡n cháº¿ cá»§a ICL vÃ  RAG, cho tháº¥y ráº±ng hiá»‡u quáº£ LLM cÃ³ xu hÆ°á»›ng bÃ£o hÃ²a khi nhiá»u vÃ­ dá»¥ Ä‘Æ°á»£c truy xuáº¥t hÆ¡n. NgÆ°á»£c láº¡i, nghiÃªn cá»©u cá»§a chÃºng tÃ´i ná»•i báº­t nhá»¯ng Æ°u Ä‘iá»ƒm nháº¥t quÃ¡n cá»§a cÃ¡c ká»¹ thuáº­t PEFT nhÆ° LoRA vÃ  QLoRA trong viá»‡c vÆ°á»£t qua nhá»¯ng háº¡n cháº¿ nÃ y.

TÃ¡c vá»¥ SE vÃ  Ä‘a tÃ¡c vá»¥. Äá»ƒ Ä‘áº£m báº£o má»™t nghiÃªn cá»©u táº­p trung, chÃºng tÃ´i trÃ¡nh thÃªm cÃ¡c tÃ¡c vá»¥ vÃ  bá»™ dá»¯ liá»‡u bá»• sung, ngÄƒn cháº·n má»™t táº­p há»£p phÃ¢n tÃ­ch quÃ¡ rá»™ng. KhÃ¡m phÃ¡ cÃ¡c ká»¹ thuáº­t PEFT cho LLMs trÃªn cÃ¡c tÃ¡c vá»¥ vÃ  bá»™ dá»¯ liá»‡u Ä‘a dáº¡ng lÃ  má»™t hÆ°á»›ng Ä‘áº§y há»©a háº¹n cho nghiÃªn cá»©u tÆ°Æ¡ng lai. Cá»¥ thá»ƒ, Lorahub [26], má»™t framework Ä‘Æ°á»£c giá»›i thiá»‡u gáº§n Ä‘Ã¢y cho há»c Ä‘a tÃ¡c vá»¥, chá»©ng minh ráº±ng má»™t composition cá»§a cÃ¡c module LoRA Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau cÃ³ thá»ƒ tá»•ng quÃ¡t hÃ³a cho cÃ¡c tÃ¡c vá»¥ má»›i, chÆ°a tháº¥y trong khi cung cáº¥p má»™t sá»± Ä‘Ã¡nh Ä‘á»•i hiá»‡u suáº¥t-hiá»‡u quáº£ máº¡nh máº½. ChÃºng tÃ´i tin ráº±ng viá»‡c Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng tá»± trong AI cho SE cÃ³ tiá»m nÄƒng lá»›n, Ä‘áº·c biá»‡t khi lÄ©nh vá»±c nghiÃªn cá»©u nháº±m tá»± Ä‘á»™ng hÃ³a má»™t loáº¡t rá»™ng cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n mÃ£.

7 CÃC Má»I ÄEá»ŒA Äá»I Vá»šI TÃNH Há»¢P Lá»†

TÃ­nh há»£p lá»‡ ngoáº¡i. Má»™t má»‘i Ä‘e dá»a chÃ­nh liÃªn quan Ä‘áº¿n viá»‡c lá»±a chá»n SLMs vÃ  LLMs cá»§a chÃºng tÃ´i. ChÃºng tÃ´i giáº£m thiá»ƒu má»‘i Ä‘e dá»a nÃ y báº±ng cÃ¡ch cáº©n tháº­n lá»±a chá»n má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c mÃ´ hÃ¬nh, nhÆ° Ä‘Æ°á»£c giáº£i thÃ­ch trong Pháº§n 4.4. Nhá»¯ng mÃ´ hÃ¬nh nÃ y bao gá»“m cÃ¡c há» LLMs khÃ¡c nhau, Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u pre-training riÃªng biá»‡t vÃ  cÃ¡c má»¥c tiÃªu há»c, vÃ  thay Ä‘á»•i vá» kÃ­ch thÆ°á»›c. HÆ¡n ná»¯a, chÃºng tÃ´i khÃ´ng chá»n cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh lá»›n hÆ¡n ngoáº¡i trá»« khi sá»­ dá»¥ng QLoRA, vÃ¬ cÃ¡c ká»¹ thuáº­t PEFT khÃ¡c, ICL, vÃ  RAG giá»›i háº¡n viá»‡c sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n trong háº¡n cháº¿ tÃ i nguyÃªn cá»§a chÃºng tÃ´i.

Má»™t má»‘i Ä‘e dá»a ngoáº¡i khÃ¡c Ä‘á»‘i vá»›i tÃ­nh há»£p lá»‡ liÃªn quan Ä‘áº¿n cháº¥t lÆ°á»£ng vÃ  tÃ­nh Ä‘áº¡i diá»‡n cá»§a cÃ¡c bá»™ dá»¯ liá»‡u tinh chá»‰nh. Äá»ƒ giáº£m bá»›t má»‘i quan tÃ¢m nÃ y, chÃºng tÃ´i chá»n bá»™ dá»¯ liá»‡u Conala, chá»©a cÃ¡c vÃ­ dá»¥ cháº¥t lÆ°á»£ng cao Ä‘Æ°á»£c khai thÃ¡c tá»« cÃ¡c bÃ i Ä‘Äƒng StackOverflow. NgoÃ i ra, bá»™ dá»¯ liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘áº¡i diá»‡n bá»Ÿi nhiá»u nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y [49,73,91] trÃªn cÃ¡c tÃ¡c vá»¥ sinh mÃ£. HÆ¡n ná»¯a, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ lÃ m phong phÃº má»—i Ã½ Ä‘á»‹nh ngÃ´n ngá»¯ tá»± nhiÃªn vá»›i gá»£i Ã½, nÃ¢ng cao sá»± liÃªn káº¿t cá»§a prompt Ä‘áº§u vÃ o vá»›i cÃ¡c Ã½ Ä‘á»‹nh con ngÆ°á»i cÃ³ thá»ƒ. Äá»ƒ lÃ m phong phÃº nghiÃªn cá»©u cá»§a chÃºng tÃ´i, chÃºng tÃ´i bao gá»“m CodeAlpacaPy nhÆ° má»™t bá»™ dá»¯ liá»‡u thá»© hai bao gá»“m cÃ¡c vÃ­ dá»¥ dÃ i hÆ¡n, mang láº¡i má»™t dÃ²ng phÃ¢n tÃ­ch khÃ¡c. ChÃºng tÃ´i khÃ´ng bao gá»“m cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ nhÆ° HumanEval [9] vÃ  MBPP [3], vÃ¬ chÃºng khÃ´ng bao gá»“m cÃ¡c vÃ­ dá»¥ huáº¥n luyá»‡n. Tuy nhiÃªn, Ä‘á»ƒ má»Ÿ rá»™ng thÃªm nghiÃªn cá»©u cá»§a chÃºng tÃ´i, chÃºng tÃ´i khÃ¡m phÃ¡ hiá»‡u quáº£ cá»§a LoRA vÃ  QLoRA cho sinh mÃ£ dá»±a trÃªn thá»±c thi trÃªn bá»™ dá»¯ liá»‡u APPs.

Cuá»‘i cÃ¹ng, khÃ­a cáº¡nh Ä‘Æ¡n ngÃ´n ngá»¯ cá»§a cÃ¡c bá»™ dá»¯ liá»‡u cá»§a chÃºng tÃ´i táº¡o thÃ nh má»™t má»‘i Ä‘e dá»a khÃ¡c Ä‘á»‘i vá»›i tÃ­nh há»£p lá»‡ ngoáº¡i. ChÃºng tÃ´i nghiÃªn cá»©u tinh chá»‰nh Ä‘áº§y Ä‘á»§, PEFT, ICL, vÃ  RAG cho sinh mÃ£ cá»§a cÃ¡c Ä‘oáº¡n mÃ£ Python. Tuy nhiÃªn, chÃºng tÃ´i dá»± Ä‘oÃ¡n ráº±ng PEFT cÅ©ng cÃ³ thá»ƒ Ã¡p dá»¥ng cho cÃ¡c ngÃ´n ngá»¯ láº­p trÃ¬nh khÃ¡c, xem xÃ©t kháº£ nÄƒng sinh áº¥n tÆ°á»£ng cá»§a LLMs trÃªn má»™t loáº¡t Ä‘a dáº¡ng cÃ¡c ngÃ´n ngá»¯ láº­p trÃ¬nh [2, 6].

TÃ­nh há»£p lá»‡ trong. Viá»‡c lá»±a chá»n hyperparameter cho cÃ¡c phÆ°Æ¡ng phÃ¡p PEFT táº¡o thÃ nh má»‘i Ä‘e dá»a chÃ­nh Ä‘á»‘i vá»›i tÃ­nh há»£p lá»‡ trong. Äá»‘i vá»›i má»—i ká»¹ thuáº­t PEFT, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c giÃ¡ trá»‹ hyperparameters Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y vá» PEFT cho cÃ¡c mÃ´ hÃ¬nh mÃ£ cÅ©ng nhÆ° trong cÃ¡c bÃ i bÃ¡o tiá»n Ä‘á» Ä‘Ã³ng gÃ³p cÃ¡c ká»¹ thuáº­t PEFT. NgoÃ i ra, vÃ¬ LoRA vá»›i ğ‘Ÿ=16 vÃ  ğ›¼=32 luÃ´n vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh ICL vÃ  RAG trÃªn ba mÃ´ hÃ¬nh hÃ ng Ä‘áº§u cá»§a chÃºng tÃ´i, viá»‡c tiáº¿n hÃ nh phÃ¢n tÃ­ch Ä‘á»™ nháº¡y hyperparameter chi tiáº¿t cá»§a LoRA cÃ³ thá»ƒ cá»§ng cá»‘ thÃªm Æ°u tháº¿ cá»§a PEFT so vá»›i ICL vÃ  RAG. CÃ´ng viá»‡c tÆ°Æ¡ng lai cÃ³ thá»ƒ khÃ¡m phÃ¡ Ä‘á»™ nháº¡y cá»§a cÃ¡c hyperparameters LoRA chÃ­nh, nhÆ° rank ğ‘Ÿ vÃ  scaling factor ğ›¼, trÃªn má»™t loáº¡t rá»™ng hÆ¡n cÃ¡c tÃ¡c vá»¥ ká»¹ thuáº­t pháº§n má»m.

TÃ­nh há»£p lá»‡ cáº¥u trÃºc. Viá»‡c lá»±a chá»n cÃ¡c metrics Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i táº¡o thÃ nh má»‘i Ä‘e dá»a chÃ­nh Ä‘á»‘i vá»›i tÃ­nh há»£p lá»‡ cáº¥u trÃºc. Äá»ƒ giáº£m thiá»ƒu má»‘i Ä‘e dá»a nÃ y, chÃºng tÃ´i chá»n cÃ¡c metrics Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y [22,32,42, 56,72,84] vá» sinh mÃ£. HÆ¡n ná»¯a, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ má»—i phÆ°Æ¡ng phÃ¡p sá»­ dá»¥ng EM@ğ‘˜ trÃªn Conala vÃ  CodeAlpacaPy, lÃ m phong phÃº phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i báº±ng cÃ¡ch tÃ­nh toÃ¡n khá»›p chÃ­nh xÃ¡c trÃªn cÃ¡c loáº¡t á»©ng viÃªn mÃ£ khÃ¡c nhau. TÆ°Æ¡ng tá»±, Ä‘á»‘i vá»›i APPs, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cÆ¡ sá»Ÿ vÃ  LoRA/QLoRA trÃªn Pass@ğ‘˜ vá»›i lÃªn Ä‘áº¿n 5 á»©ng viÃªn. Cuá»‘i cÃ¹ng, chÃºng tÃ´i khÃ´ng sá»­ dá»¥ng metrics Pass@ğ‘˜ vÃ¬ cÃ¡c bá»™ dá»¯ liá»‡u CoNaLa vÃ  CodeAlpacaPy khÃ´ng bao gá»“m unit tests. LÃ m phong phÃº cÃ¡c bá»™ dá»¯ liá»‡u vá»›i unit tests táº¡o thÃ nh má»™t lÄ©nh vá»±c thÃº vá»‹ cá»§a cÃ´ng viá»‡c tÆ°Æ¡ng lai.

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 21 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢21

8 CÃ”NG TRÃŒNH LIÃŠN QUAN

Trong pháº§n nÃ y, chÃºng tÃ´i tá»•ng quan cÃ¡c cÃ´ng trÃ¬nh hiá»‡n cÃ³ vá» LLMs cho sinh mÃ£ vÃ  Ä‘á»‘i chiáº¿u cÃ¡c Ä‘Ã³ng gÃ³p trÆ°á»›c Ä‘Ã¢y vá» thÃ­ch á»©ng mÃ´ hÃ¬nh hiá»‡u quáº£ cá»§a mÃ£ cho cÃ¡c tÃ¡c vá»¥ downstream vá»›i nghiÃªn cá»©u cá»§a chÃºng tÃ´i.

Sinh MÃ£ Tá»± Ä‘á»™ng. Má»™t pháº§n Ä‘Ã¡ng ká»ƒ cá»§a cÃ¡c ká»¹ thuáº­t sinh mÃ£ [1,4,21,63,72] dá»±a vÃ o cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn deep-learning. Xu hÆ°á»›ng má»›i nháº¥t trong sinh mÃ£ tá»± Ä‘á»™ng xoay quanh viá»‡c táº­n dá»¥ng LLMs nhÆ° cÃ¡c mÃ´ hÃ¬nh GPT [50] do nhá»¯ng Ä‘á»™t phÃ¡ Ä‘Ã¡ng ká»ƒ cá»§a chÃºng trong lÄ©nh vá»±c nÃ y. Má»™t vÃ­ dá»¥ Ä‘Ã¡ng chÃº Ã½ lÃ  Codex, Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Chen et al. [9], lÃ  má»™t phiÃªn báº£n Ä‘Æ°á»£c tinh chá»‰nh cá»§a GPT-3. CÃ¡c mÃ´ hÃ¬nh Ä‘Ã¡ng chÃº Ã½ khÃ¡c theo sau thÃ nh cÃ´ng cá»§a Codex bao gá»“m CodeGen [48], CodeGen2 [47] vÃ  CodeLlama [56]. Nhá»¯ng LLMs nÃ y hiá»‡u quáº£ dÃ¢n chá»§ hÃ³a hiá»‡u suáº¥t Ä‘á»™t phÃ¡ Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi Codex vÃ  mang nÃ³ Ä‘áº¿n vá»›i Ä‘á»‘i tÆ°á»£ng rá»™ng hÆ¡n. Tuy nhiÃªn, chi phÃ­ tÃ­nh toÃ¡n cao liÃªn quan Ä‘áº¿n tinh chá»‰nh Ä‘áº§y Ä‘á»§ cho LLMs Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t tá»‘i Æ°u lÃ  khÃ´ng thá»±c táº¿ cho háº§u háº¿t cÃ¡c nhÃ  nghiÃªn cá»©u vÃ  thá»±c hÃ nh. ChÃºng tÃ´i tin ráº±ng nghiÃªn cá»©u cá»§a chÃºng tÃ´i cÃ³ thá»ƒ lÃ m sÃ¡ng tá» cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ vÃ  tiáº¿t kiá»‡m chi phÃ­ hÆ¡n Ä‘á»ƒ tinh chá»‰nh nhá»¯ng LLMs nÃ y, giáº£m thiá»ƒu gÃ¡nh náº·ng tÃ­nh toÃ¡n liÃªn quan Ä‘áº¿n viá»‡c Ã¡p dá»¥ng chÃºng.

ThÃ­ch á»©ng Hiá»‡u quáº£ cá»§a MÃ´ hÃ¬nh vá» MÃ£. ThÃ­ch á»©ng hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh vá» mÃ£ bao gá»“m viá»‡c sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t Ä‘á»ƒ thÃ­ch á»©ng hiá»‡u quáº£ má»™t mÃ´ hÃ¬nh vá»›i má»™t bá»™ dá»¯ liá»‡u cá»¥ thá»ƒ tÃ¡c vá»¥ (xem Pháº§n 2). Trong bá»‘i cáº£nh nÃ y, thuáº­t ngá»¯ "hiá»‡u quáº£" Ä‘á» cáº­p Ä‘áº¿n viá»‡c lÃ m cho chi phÃ­ tÃ­nh toÃ¡n tinh chá»‰nh tháº¥p, vÃ­ dá»¥, sá»­ dá»¥ng LoRA, hoáº·c sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t khÃ´ng tham sá»‘ nhÆ° prompting vÃ  ICL.

Háº§u háº¿t nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ táº­p trung vÃ o viá»‡c sá»­ dá»¥ng ICL vÃ  prompting Ä‘á»ƒ thÃ­ch á»©ng cÃ¡c mÃ´ hÃ¬nh vá»›i cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n mÃ£ Ä‘a dáº¡ng. Gao et al. [17] thá»ƒ hiá»‡n nhá»¯ng Æ°u Ä‘iá»ƒm cá»§a ICL trong cÃ¡c tÃ¡c vá»¥ nhÆ° sá»­a lá»—i, tÃ³m táº¯t mÃ£, vÃ  tá»•ng há»£p chÆ°Æ¡ng trÃ¬nh. Há» ná»•i báº­t ráº±ng hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh trÃªn cÃ¡c tÃ¡c vá»¥ downstream bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi nhiá»u yáº¿u tá»‘, bao gá»“m lá»±a chá»n, sá»‘ lÆ°á»£ng, vÃ  thá»© tá»± cá»§a cÃ¡c vÃ­ dá»¥ prompt. CÃ¡c nghiÃªn cá»©u khÃ¡c [53,80] cÅ©ng chá»©ng minh ráº±ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c pre-train vÃ  LLMs nhÆ° Codex cÃ³ thá»ƒ xá»­ lÃ½ hiá»‡u quáº£ sá»­a lá»—i vÃ  sá»­a chá»¯a chÆ°Æ¡ng trÃ¬nh tá»± Ä‘á»™ng sá»­ dá»¥ng ICL. HÆ¡n ná»¯a, Geng et al. [19] chá»©ng minh kháº£ nÄƒng cá»§a Codex trong viá»‡c sinh multi-intent comment generation Ä‘á»ƒ mÃ´ táº£ chá»©c nÄƒng cá»§a má»™t phÆ°Æ¡ng phÃ¡p hoáº·c chi tiáº¿t triá»ƒn khai cá»§a nÃ³, cháº³ng háº¡n. Viá»‡c lá»±a chá»n cÃ¡c prompt cÃ³ liÃªn quan cho má»™t tÃ¡c vá»¥ vá»›i ICL lÃ  quan trá»ng Ä‘á»ƒ Ä‘áº£m báº£o hiá»‡u suáº¥t tá»‘t cá»§a má»™t LLM. CÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y [46,91] thiáº¿t káº¿ cÃ¡c ká»¹ thuáº­t lá»±a chá»n Ä‘á»ƒ truy xuáº¥t cÃ¡c vÃ­ dá»¥ prompt cÃ³ liÃªn quan cao Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho cÃ¡c tÃ¡c vá»¥ downstream, vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p lá»±a chá»n ngáº«u nhiÃªn. Cuá»‘i cÃ¹ng, nghiÃªn cá»©u gáº§n Ä‘Ã¢y [61] ná»•i báº­t nhá»¯ng Æ°u Ä‘iá»ƒm cá»§a viá»‡c truy xuáº¥t cÃ¡c vÃ­ dá»¥ prompt á»Ÿ cáº¥p Ä‘á»™ repository, cung cáº¥p cho LLMs thÃ´ng tin ngá»¯ cáº£nh cÃ³ giÃ¡ trá»‹ trong cÃ¡c prompt. Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i táº­n dá»¥ng ICL mÃ  khÃ´ng cÃ³ Ã½ Ä‘á»‹nh khÃ¡m phÃ¡ Ä‘áº§y Ä‘á»§ tiá»m nÄƒng cá»§a nÃ³. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i chá»n má»™t triá»ƒn khai Ä‘Æ¡n giáº£n cá»§a ICL báº±ng cÃ¡ch chá»n cÃ¡c vÃ­ dá»¥ few-shot ngáº«u nhiÃªn sá»­ dá»¥ng cÃ¡c seeds khÃ¡c nhau. Má»Ÿ rá»™ng nghiÃªn cá»©u nÃ y Ä‘á»ƒ káº¿t há»£p nhiá»u phÆ°Æ¡ng phÃ¡p ICL hÆ¡n sáº½ nÃ¢ng cao so sÃ¡nh vá»›i cÃ¡c ká»¹ thuáº­t PEFT cho mÃ£.

LiÃªn quan Ä‘áº¿n cÃ¡c ká»¹ thuáº­t PEFT, nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y trong trÃ­ tuá»‡ mÃ£ Ä‘Ã£ táº­p trung vÃ o Prompt tuning [30], Prefix-tuning [33] vÃ  Adapters [20,23,25,57,58]. Wang et al. [68] khá»Ÿi xÆ°á»›ng viá»‡c sá»­ dá»¥ng Prompt tuning cho cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n mÃ£ vÃ  chá»©ng minh tÃ­nh Æ°u viá»‡t cá»§a nÃ³ so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ cá»§a CodeT5 vÃ  CodeBERT trong dá»± Ä‘oÃ¡n lá»—i, tÃ³m táº¯t mÃ£, vÃ  dá»‹ch mÃ£. Goel et al. [20] khÃ¡m phÃ¡ viá»‡c sá»­ dá»¥ng adapters cá»¥ thá»ƒ ngÃ´n ngá»¯ láº­p trÃ¬nh cho viá»‡c chuyá»ƒn giao kiáº¿n thá»©c trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c pre-train, chá»©ng minh ráº±ng viá»‡c tinh chá»‰nh BERT vá»›i nhá»¯ng adapters nÃ y vÆ°á»£t trá»™i hÆ¡n CodeBERT trÃªn cloze test vÃ  phÃ¡t hiá»‡n clone mÃ£. Choi et al. [10] thiáº¿t káº¿ má»™t phÆ°Æ¡ng phÃ¡p Prefix tuning cá»¥ thá»ƒ mÃ£ trong má»™t kiáº¿n trÃºc sequence-to-sequence cho cÃ¡c tÃ¡c vá»¥ generation. NghiÃªn cá»©u cá»§a chÃºng tÃ´i khÃ¡c vá»›i ba cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y nÃ y vÃ¬ chÃºng táº­p trung vÃ o SLMs, trong khi chÃºng tÃ´i Ä‘á» xuáº¥t nghiÃªn cá»©u toÃ n diá»‡n Ä‘áº§u tiÃªn vá» cÃ¡c ká»¹ thuáº­t PEFT vá»›i LLMs cho sinh mÃ£. HÆ¡n ná»¯a, nghiÃªn cá»©u cá»§a chÃºng tÃ´i bao gá»“m LoRA, IA3, vÃ  QLoRA, mÃ  khÃ´ng cÃ³ cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y nÃ o trong trÃ­ tuá»‡ mÃ£ xem xÃ©t Ä‘á»ƒ tinh chá»‰nh hiá»‡u quáº£ LLMs vá» mÃ£. Wang et al. [69] thá»ƒ hiá»‡n tÃ­nh Æ°u viá»‡t cá»§a viá»‡c sá»­ dá»¥ng Adapters Ä‘á»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c pre-train so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§. CÃ¡c cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y Ä‘Ã£ Ä‘Ã³ng gÃ³p cÃ¡c nghiÃªn cá»©u thá»±c nghiá»‡m cho cÃ¡c tÃ¡c vá»¥ ká»¹ thuáº­t pháº§n má»m khÃ¡c nhau, bao gá»“m thay Ä‘á»•i mÃ£ [40], tÃ³m táº¯t mÃ£ [38,57], dá»± Ä‘oÃ¡n lá»—i [38], vÃ  phÃ¡t hiá»‡n clone mÃ£ [57], sá»­ dá»¥ng Adapter tuning vÃ  LoRA cho SLMs. NghiÃªn cá»©u cá»§a chÃºng tÃ´i khÃ¡c vá»›i nhá»¯ng cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y nÃ y, vÃ¬ chÃºng tÃ´i táº­p trung vÃ o LLMs. Máº·c dÃ¹ chÃºng tÃ´i khÃ´ng káº¿t há»£p Adapters trong Ä‘iá»u tra cá»§a chÃºng tÃ´i, chÃºng tÃ´i tin ráº±ng LoRA, IA3, Prompt tuning, Prefix tuning, vÃ  QLoRA cung cáº¥p má»™t phÃ¢n tÃ­ch Ä‘á»§ ká»¹ lÆ°á»¡ng vá» cÃ¡c ká»¹ thuáº­t PEFT. ChÃºng tÃ´i nháº­n ra giÃ¡ trá»‹ cá»§a viá»‡c khÃ¡m phÃ¡ cÃ¡c ká»¹ thuáº­t PEFT bá»• sung cho cÃ¡c tÃ¡c vá»¥ trÃ­ tuá»‡ mÃ£ khÃ¡c nhau trong tÆ°Æ¡ng lai.

9 Káº¾T LUáº¬N VÃ€ CÃ”NG VIá»†C TÆ¯Æ NG LAI

NghiÃªn cá»©u nÃ y thiáº¿t láº­p hiá»‡u quáº£ cá»§a cÃ¡c ká»¹ thuáº­t PEFT trong viá»‡c tinh chá»‰nh LLMs cho sinh mÃ£. PhÃ¢n tÃ­ch so sÃ¡nh cá»§a chÃºng tÃ´i trÃªn cÃ¡c ká»¹ thuáº­t hiá»‡u quáº£ tham sá»‘ khÃ¡c nhau, bao gá»“m LoRA, IA3, Prompt tuning, Prefix tuning, vÃ  QLoRA, tiáº¿t lá»™ tÃ­nh Æ°u viá»‡t cá»§a PEFT so vá»›i tinh chá»‰nh Ä‘áº§y Ä‘á»§ cho SLMs vÃ  ICL vÃ  RAG cho LLMs. HÆ¡n ná»¯a, nghiÃªn cá»©u cá»§a chÃºng tÃ´i minh há»a tÃ­nh thá»±c táº¿ cá»§a PEFT trong má»™t ká»‹ch báº£n tÃ i nguyÃªn háº¡n cháº¿, hiá»‡u quáº£ giáº£m thiá»ƒu sá»± phá»¥ thuá»™c vÃ o cÆ¡ sá»Ÿ háº¡ táº§ng tÃ­nh toÃ¡n lá»›n vÃ  Ä‘áº¯t Ä‘á». Theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, nghiÃªn cá»©u nÃ y lÃ  má»™t trong nhá»¯ng khÃ¡m phÃ¡ toÃ n diá»‡n Ä‘áº§u tiÃªn vá» cÃ¡c ká»¹ thuáº­t PEFT cho LLMs trong ká»¹ thuáº­t pháº§n má»m, gá»£i Ã½ má»™t con Ä‘Æ°á»ng Ä‘áº§y há»©a háº¹n cho nghiÃªn cá»©u tÆ°Æ¡ng lai. ChÃºng tÃ´i dá»± Ä‘oÃ¡n cÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i sáº½ truyá»n cáº£m há»©ng cho Ä‘iá»u tra thÃªm vá» viá»‡c Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t PEFT trong ká»¹ thuáº­t pháº§n má»m, vá»›i nhá»¯ng tÃ¡c Ä‘á»™ng cÃ³ thá»ƒ xa rá»™ng. CÃ´ng viá»‡c tÆ°Æ¡ng lai cá»§a chÃºng tÃ´i sáº½ má»Ÿ rá»™ng nghiÃªn cá»©u cho cÃ¡c tÃ¡c vá»¥ ká»¹ thuáº­t pháº§n má»m thay tháº¿ nhÆ° Ä‘Ã¡nh giÃ¡ mÃ£ tá»± Ä‘á»™ng vÃ  sinh comment. Cuá»‘i cÃ¹ng, chÃºng tÃ´i nháº±m xÃ¡c tháº­n thÃªm sá»± liÃªn quan cá»§a cÃ¡c ká»¹ thuáº­t PEFT trong cÃ¡c cÃ i Ä‘áº·t Ä‘a tÃ¡c vá»¥ vÃ  há»c liÃªn tá»¥c cho ká»¹ thuáº­t pháº§n má»m tá»± Ä‘á»™ng.

TÃ€I LIá»†U THAM KHáº¢O
[1]Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural language models of code. In International conference on machine learning . PMLR, 245â€“256.
[2]Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868 (2022).
[3]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al .2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).
[4]Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. 2016. Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989 (2016).
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al .2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877â€“1901.
[6]Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al .2023. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering (2023).
[7]Christel Chappuis, ValÃ©rie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia. 2022. Prompt-RSVQA: Prompting visual context to a language model for remote sensing visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 1372â€“1381.
[8]Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.com/sahil280114/codealpaca.
[9]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al .2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).
[10] YunSeok Choi and Jee-Hyong Lee. 2023. CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation. InFindings of the Association for Computational Linguistics: ACL 2023 . 5282â€“5297.
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al .2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).
[12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. arXiv preprint arXiv:2208.07339 (2022).
[13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314 (2023).
[14] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904 (2022).
[15] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al .2023. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence 5, 3 (2023), 220â€“235.
[16] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).
[17] Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, and Michael R Lyu. 2023. Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study. arXiv preprint arXiv:2304.07575 (2023).
[18] Shuzheng Gao, Hongyu Zhang, Cuiyun Gao, and Chaozheng Wang. 2023. Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models. arXiv preprint arXiv:2302.03482 (2023).
[19] Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning. (2024).
[20] Divyam Goel, Ramansh Grover, and Fatemeh H Fard. 2022. On the cross-modal transfer from natural language to code through adapter modules. In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension . 71â€“81.
[21] Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, and Graham Neubig. 2018. Retrieval-based neural code generation. arXiv preprint arXiv:1808.10025 (2018).
[22] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. NeurIPS (2021).
[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning . PMLR, 2790â€“2799.
[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).
[25] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. arXiv preprint arXiv:2304.01933 (2023).
[26] Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. 2023. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269 (2023).
[27] Harshit Joshi, JosÃ© Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan RadiÄek. 2023. Repair is nearly generation: Multilingual program repair with llms. In Proceedings of the AAAI Conference on Artificial Intelligence ,

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 24 ---
24 â€¢M. Weyssow et al.

Vol. 37. 5131â€“5140.
[28] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos MuÃ±oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 2022. The Stack: 3 TB of permissively licensed source code. Preprint (2022).
[29] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199â€“22213.
[30] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . 3045â€“3059.
[31] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al .2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459â€“9474.
[32] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. 2023. Skcoder: A sketch-based approach for automatic code generation. arXiv preprint arXiv:2302.06144 (2023).
[33] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).
[34] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards General Text Embeddings with Multi-stage Contrastive Learning. arXiv:2308.03281 [cs.CL] https://arxiv.org/abs/2308.03281
[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al .2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).
[36] W Liang, M Yuksekgonul, Y Mao, E Wu, and J Zou. 2023. GPT detectors are biased against non-native English writers (arXiv: 2304.02819). arXiv.
[37] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems 35 (2022), 1950â€“1965.
[38] Jiaxing Liu, Chaofeng Sha, and Xin Peng. 2023. An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE) . IEEE, 397â€“408.
[39] Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020. Retrieval-augmented generation for code summarization via hybrid gnn. arXiv preprint arXiv:2006.05405 (2020).
[40] Shuo Liu, Jacky Keung, Zhen Yang, Fang Liu, Qilin Zhou, and Yihan Liao. 2024. Delving into Parameter-Efficient Fine-Tuning in Code Change Learning: An Empirical Study. arXiv preprint arXiv:2402.06247 (2024).
[41] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. 2022. Reacc: A retrieval-augmented code completion framework. arXiv preprint arXiv:2203.07722 (2022).
[42] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al .2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).
[43] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. https://github.com/huggingface/peft.
[44] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2021. Recent advances in natural language processing via large pre-trained language models: A survey. Comput. Surveys (2021).
[45] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943 (2021).
[46] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt selection for code-related few-shot learning. InProceedings of the 45th International Conference on Software Engineering (ICSE'23) .
[47] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309 (2023).
[48] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. arXiv:2203.13474 [cs.LG]

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 25 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢25

[49] Sajad Norouzi, Keyi Tang, and Yanshuai Cao. 2021. Code generation from natural language with less prior knowledge and more monolingual data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) . 776â€“785.
[50] R OpenAI. 2023. GPT-4 technical report. arXiv (2023), 2303â€“08774.
[51] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730â€“27744.
[52] Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601 (2021).
[53] Julian Aron Prenner, Hlib Babii, and Romain Robbes. 2022. Can OpenAI's codex fix bugs? an evaluation on QuixBugs. In Proceedings of the Third International Workshop on Automated Program Repair . 69â€“75.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al .2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.
[55] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020).
[56] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, et al .2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).
[57] Iman Saberi, Fatemeh Fard, and Fuxiang Chen. 2024. Utilization of pre-trained language models for adapter-based knowledge transfer in software engineering. Empirical Software Engineering 29, 4 (2024), 94.
[58] Iman Saberi and Fatemeh H Fard. 2023. Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models. arXiv preprint arXiv:2303.06233 (2023).
[59] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023. Prompting large language models with answer heuristics for knowledge-based visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 14974â€“14983.
[60] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning . PMLR, 4596â€“4604.
[61] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2023. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning . PMLR, 31693â€“31715.
[62] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).
[63] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020. Treegen: A tree-based transformer architecture for code generation. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 8984â€“8991.
[64] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
[65] Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, et al .2023. Efficient methods for natural language processing: A survey. Transactions of the Association for Computational Linguistics 11 (2023), 826â€“860.
[66] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Chi conference on human factors in computing systems extended abstracts . 1â€“7.
[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[68] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R Lyu. 2022. No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 382â€“394.
[69] Deze Wang, Boxing Chen, Shanshan Li, Wei Luo, Shaoliang Peng, Wei Dong, and Xiangke Liao. 2023. One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization. arXiv preprint arXiv:2303.15822 (2023).

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 26 ---
26 â€¢M. Weyssow et al.

[70] Weishi Wang, Yue Wang, Shafiq Joty, and Steven CH Hoi. 2023. Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 146â€“158.
[71] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).
[72] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).
[73] Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022. Execution-Based Evaluation for Open-Domain Code Generation. arXiv preprint arXiv:2212.10481 (2022).
[74] Albert Webson and Ellie Pavlick. 2021. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247 (2021).
[75] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[76] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al .2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022).
[77] Martin Weyssow, Houari Sahraoui, and Bang Liu. 2022. Better modeling the programming world with code concept graphs-augmented multi-modal learning. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results . 21â€“25.
[78] Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. 2023. On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code. arXiv preprint arXiv:2305.04106 (2023).
[79] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, et al .2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).
[80] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated program repair in the era of large pre-trained language models. In Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association for Computing Machinery .
[81] Chunqiu Steven Xia and Lingming Zhang. 2022. Less training, more repairing please: revisiting automated program repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering . 959â€“971.
[82] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A Systematic Evaluation of Large Language Models of Code (MAPS 2022) . Association for Computing Machinery, New York, NY, USA, 1â€“10. https: //doi.org/10.1145/3520312.3534862
[83] Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, et al .2023. Exploring Continual Learning for Code Generation Models. arXiv preprint arXiv:2307.02435 (2023).
[84] Guang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang, Tingting Han, and Taolue Chen. 2023. ExploitGen: Template-augmented exploit code generation based on CodeBERT. Journal of Systems and Software 197 (2023), 111577.
[85] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. 2023. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 19187â€“19197.
[86] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow. In Proceedings of the 15th International Conference on Mining Software Repositories (Gothenburg, Sweden) (MSR '18) . Association for Computing Machinery, New York, NY, USA, 476â€“486. https://doi.org/10.1145/3196398.3196408
[87] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and natural language pairs from stack overflow. In 2018 IEEE/ACM 15th international conference on mining software repositories (MSR) . IEEE, 476â€“486.
[88] Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. 2023. Evaluating instruction-tuned large language models on code comprehension and generation. arXiv preprint arXiv:2308.01240 (2023).

, Vol. 1, No. 1, Article . NgÃ y xuáº¥t báº£n: ThÃ¡ng 12 nÄƒm 2024.

--- TRANG 27 ---
KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t Tinh Chá»‰nh Hiá»‡u Quáº£ Tham Sá»‘ cho Sinh MÃ£ vá»›i MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n â€¢27

[89] Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Yangyang Li, and Liwen Jing. 2023. Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media. arXiv preprint arXiv:2304.03087 (2023).
[90] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning . PMLR, 12697â€“12706.
[91] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Generating code by retrieving the docs. In The Eleventh International Conference on Learning Representations .
[92] Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing generalizability of codebert. In 2021 IEEE International Conference on Software Maintenance and Evolution (ICSME) . IEEE, 425â€“436.