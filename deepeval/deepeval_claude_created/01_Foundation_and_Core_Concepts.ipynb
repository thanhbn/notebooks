{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö DeepEval Foundation v√† Core Concepts\n",
    "\n",
    "Ch√†o m·ª´ng ƒë·∫øn v·ªõi notebook ƒë·∫ßu ti√™n trong series **DeepEval - Framework ƒê√°nh gi√° LLM To√†n di·ªán**!\n",
    "\n",
    "## üéØ M·ª•c ti√™u c·ªßa Notebook n√†y\n",
    "\n",
    "1. **C√†i ƒë·∫∑t v√† Thi·∫øt l·∫≠p** DeepEval environment\n",
    "2. **Hi·ªÉu c√°c kh√°i ni·ªám c∆° b·∫£n**: LLMTestCase, Metrics, Test Suites\n",
    "3. **T√≠ch h·ª£p v·ªõi Pytest** cho automated testing\n",
    "4. **Th·ª±c h√†nh v·ªõi c√°c metrics c∆° b·∫£n**: AnswerRelevancy, Hallucination, Bias\n",
    "\n",
    "## üìñ T·∫°i sao c·∫ßn DeepEval?\n",
    "\n",
    "Khi ph√°t tri·ªÉn ·ª©ng d·ª•ng LLM, vi·ªác ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng output l√† c·ª±c k·ª≥ quan tr·ªçng nh∆∞ng c≈©ng r·∫•t th√°ch th·ª©c:\n",
    "- **Subjective evaluation**: Con ng∆∞·ªùi ƒë√°nh gi√° kh√°c nhau\n",
    "- **Scale problems**: Kh√¥ng th·ªÉ manually test h√†ng ng√†n cases\n",
    "- **Consistency**: C·∫ßn standardized metrics\n",
    "- **Automation**: C·∫ßn integrate v√†o CI/CD pipeline\n",
    "\n",
    "DeepEval gi·∫£i quy·∫øt nh·ªØng v·∫•n ƒë·ªÅ n√†y b·∫±ng c√°ch cung c·∫•p:\n",
    "- ‚úÖ **Automated evaluation** v·ªõi AI-powered metrics\n",
    "- ‚úÖ **Standardized testing framework** t∆∞∆°ng t·ª± unit testing\n",
    "- ‚úÖ **Comprehensive metrics** cho nhi·ªÅu use cases\n",
    "- ‚úÖ **Easy integration** v·ªõi existing workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Ph·∫ßn 1: C√†i ƒë·∫∑t v√† Thi·∫øt l·∫≠p\n",
    "\n",
    "### B∆∞·ªõc 1: C√†i ƒë·∫∑t Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt\n",
    "# Ch·∫°y command n√†y n·∫øu ch∆∞a c√†i ƒë·∫∑t requirements.txt\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "# Import c√°c libraries c·∫ßn thi·∫øt\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DeepEval imports\n",
    "import deepeval\n",
    "\n",
    "print(dir(deepeval))\n",
    "# from deepeval import assert_test\n",
    "# from deepeval.test_case import LLMTestCase\n",
    "# from deepeval.metrics import (\n",
    "#     AnswerRelevancyMetric,\n",
    "#     HallucinationMetric,\n",
    "#     BiasMetric\n",
    "# )\n",
    "\n",
    "# print(f\"‚úÖ DeepEval version: {deepeval.__version__}\")\n",
    "print(\"‚úÖ All imports successful!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: Thi·∫øt l·∫≠p API Keys\n",
    "\n",
    "DeepEval s·ª≠ d·ª•ng LLM ƒë·ªÉ ƒë√°nh gi√° output, v√¨ v·∫≠y c·∫ßn API keys:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Thi·∫øt l·∫≠p environment variables\n",
    "# C√≥ th·ªÉ s·ª≠ d·ª•ng .env file ho·∫∑c set tr·ª±c ti·∫øp\n",
    "\n",
    "# Option 1: S·ª≠ d·ª•ng .env file (khuy·∫øn ngh·ªã)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Option 2: Set tr·ª±c ti·∫øp (ch·ªâ ƒë·ªÉ demo, kh√¥ng n√™n commit keys)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key-here\"\n",
    "\n",
    "# Ki·ªÉm tra API keys\n",
    "api_keys_status = {\n",
    "    \"OpenAI\": \"‚úÖ Configured\" if os.getenv(\"OPENAI_API_KEY\") else \"‚ùå Missing\",\n",
    "    \"Anthropic\": \"‚úÖ Configured\" if os.getenv(\"ANTHROPIC_API_KEY\") else \"‚ùå Missing\"\n",
    "}\n",
    "\n",
    "print(\"üîë API Keys Status:\")\n",
    "for provider, status in api_keys_status.items():\n",
    "    print(f\"  {provider}: {status}\")\n",
    "\n",
    "if \"‚ùå Missing\" in api_keys_status.values():\n",
    "    print(\"\\n‚ö†Ô∏è  C·∫ßn thi·∫øt l·∫≠p API keys ƒë·ªÉ ch·∫°y evaluations!\")\n",
    "    print(\"   T·∫°o file .env v·ªõi n·ªôi dung:\")\n",
    "    print(\"   OPENAI_API_KEY=your_key_here\")\n",
    "    print(\"   ANTHROPIC_API_KEY=your_key_here\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Ph·∫ßn 2: Kh√°i ni·ªám C∆° b·∫£n\n",
    "\n",
    "### 2.1 LLMTestCase - Building Block c·ªßa DeepEval\n",
    "\n",
    "`LLMTestCase` l√† ƒë∆°n v·ªã c∆° b·∫£n nh·∫•t trong DeepEval, gi·ªëng nh∆∞ test case trong unit testing:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# T·∫°o LLMTestCase ƒë·∫ßu ti√™n\n",
    "def create_basic_test_case():\n",
    "    \"\"\"\n",
    "    T·∫°o m·ªôt test case c∆° b·∫£n ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c\n",
    "    \"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Tr√≠ tu·ªá nh√¢n t·∫°o l√† g√¨?\",\n",
    "        actual_output=\"Tr√≠ tu·ªá nh√¢n t·∫°o (AI) l√† kh·∫£ nƒÉng c·ªßa m√°y t√≠nh th·ª±c hi·ªán c√°c nhi·ªám v·ª• th∆∞·ªùng ƒë√≤i h·ªèi tr√≠ tu·ªá con ng∆∞·ªùi, bao g·ªìm nh·∫≠n d·∫°ng h√¨nh ·∫£nh, x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, v√† ra quy·∫øt ƒë·ªãnh.\",\n",
    "        expected_output=\"AI l√† c√¥ng ngh·ªá cho ph√©p m√°y t√≠nh m√¥ ph·ªèng tr√≠ tu·ªá con ng∆∞·ªùi.\",  # Optional\n",
    "        context=[\"AI l√† m·ªôt lƒ©nh v·ª±c c·ªßa khoa h·ªçc m√°y t√≠nh\", \"AI bao g·ªìm machine learning v√† deep learning\"]  # Optional\n",
    "    )\n",
    "    \n",
    "    return test_case\n",
    "\n",
    "# T·∫°o v√† ki·ªÉm tra test case\n",
    "basic_test = create_basic_test_case()\n",
    "\n",
    "print(\"üß™ Basic Test Case ƒë√£ t·∫°o:\")\n",
    "print(f\"Input: {basic_test.input}\")\n",
    "print(f\"Actual Output: {basic_test.actual_output[:100]}...\")\n",
    "print(f\"Expected Output: {basic_test.expected_output}\")\n",
    "print(f\"Context: {len(basic_test.context)} items\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 C√°c th√†nh ph·∫ßn c·ªßa LLMTestCase\n",
    "\n",
    "H√£y hi·ªÉu r√µ t·ª´ng th√†nh ph·∫ßn:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def explain_test_case_components():\n",
    "    \"\"\"\n",
    "    Gi·∫£i th√≠ch chi ti·∫øt c√°c th√†nh ph·∫ßn c·ªßa LLMTestCase\n",
    "    \"\"\"\n",
    "    components = {\n",
    "        \"input\": {\n",
    "            \"description\": \"C√¢u h·ªèi ho·∫∑c prompt ƒë∆∞·ª£c g·ª≠i t·ªõi LLM\",\n",
    "            \"required\": True,\n",
    "            \"example\": \"Gi·∫£i th√≠ch machine learning cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu\"\n",
    "        },\n",
    "        \"actual_output\": {\n",
    "            \"description\": \"C√¢u tr·∫£ l·ªùi th·ª±c t·∫ø t·ª´ LLM m√† b·∫°n mu·ªën ƒë√°nh gi√°\",\n",
    "            \"required\": True,\n",
    "            \"example\": \"Machine learning l√†...\"\n",
    "        },\n",
    "        \"expected_output\": {\n",
    "            \"description\": \"C√¢u tr·∫£ l·ªùi mong ƒë·ª£i (d√πng cho m·ªôt s·ªë metrics)\",\n",
    "            \"required\": False,\n",
    "            \"example\": \"ML is a subset of AI...\"\n",
    "        },\n",
    "        \"context\": {\n",
    "            \"description\": \"Th√¥ng tin context/background (quan tr·ªçng cho RAG)\",\n",
    "            \"required\": False,\n",
    "            \"example\": \"[doc1, doc2, doc3] from knowledge base\"\n",
    "        },\n",
    "        \"retrieval_context\": {\n",
    "            \"description\": \"Context ƒë∆∞·ª£c retrieve t·ª´ vector DB (d√†nh ri√™ng cho RAG)\",\n",
    "            \"required\": False,\n",
    "            \"example\": \"[retrieved_doc1, retrieved_doc2]\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin components\n",
    "components = explain_test_case_components()\n",
    "\n",
    "print(\"üìã C√°c th√†nh ph·∫ßn c·ªßa LLMTestCase:\\n\")\n",
    "for component, info in components.items():\n",
    "    required_status = \"‚úÖ Required\" if info[\"required\"] else \"‚ö™ Optional\"\n",
    "    print(f\"**{component}** ({required_status})\")\n",
    "    print(f\"  üìù {info['description']}\")\n",
    "    print(f\"  üí° V√≠ d·ª•: {info['example']}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ph·∫ßn 3: Metrics C∆° b·∫£n\n",
    "\n",
    "### 3.1 AnswerRelevancyMetric\n",
    "\n",
    "ƒê√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa c√¢u tr·∫£ l·ªùi v·ªõi c√¢u h·ªèi:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# T·∫°o AnswerRelevancyMetric\n",
    "def demo_answer_relevancy():\n",
    "    \"\"\"\n",
    "    Demo AnswerRelevancyMetric v·ªõi c√°c test cases kh√°c nhau\n",
    "    \"\"\"\n",
    "    # Kh·ªüi t·∫°o metric\n",
    "    relevancy_metric = AnswerRelevancyMetric(\n",
    "        threshold=0.7,  # Ng∆∞·ª°ng pass/fail\n",
    "        model=\"gpt-3.5-turbo\",  # LLM d√πng ƒë·ªÉ evaluate\n",
    "        include_reason=True  # Bao g·ªìm l√Ω do ƒë√°nh gi√°\n",
    "    )\n",
    "    \n",
    "    # Test case 1: Relevant answer\n",
    "    relevant_test = LLMTestCase(\n",
    "        input=\"Machine learning c√≥ nh·ªØng lo·∫°i ch√≠nh n√†o?\",\n",
    "        actual_output=\"Machine learning c√≥ 3 lo·∫°i ch√≠nh: Supervised Learning (h·ªçc c√≥ gi√°m s√°t), Unsupervised Learning (h·ªçc kh√¥ng gi√°m s√°t), v√† Reinforcement Learning (h·ªçc tƒÉng c∆∞·ªùng). M·ªói lo·∫°i c√≥ ·ª©ng d·ª•ng v√† thu·∫≠t to√°n ri√™ng.\"\n",
    "    )\n",
    "    \n",
    "    # Test case 2: Irrelevant answer  \n",
    "    irrelevant_test = LLMTestCase(\n",
    "        input=\"Machine learning c√≥ nh·ªØng lo·∫°i ch√≠nh n√†o?\",\n",
    "        actual_output=\"H√¥m nay tr·ªùi ƒë·∫πp qu√°. T√¥i th√≠ch ƒëi d·∫°o trong c√¥ng vi√™n v√† nghe ti·∫øng chim h√≥t.\"\n",
    "    )\n",
    "    \n",
    "    return relevancy_metric, relevant_test, irrelevant_test\n",
    "\n",
    "# Ch·∫°y demo\n",
    "relevancy_metric, relevant_test, irrelevant_test = demo_answer_relevancy()\n",
    "\n",
    "print(\"üéØ AnswerRelevancyMetric Demo\")\n",
    "print(f\"Threshold: {relevancy_metric.threshold}\")\n",
    "print(f\"Model: {relevancy_metric.model}\")\n",
    "print(\"\\nüìä S·∫Ω test 2 cases: relevant vs irrelevant answers\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ƒê√°nh gi√° relevant test case\n",
    "print(\"üß™ Test Case 1: Relevant Answer\")\n",
    "print(f\"Input: {relevant_test.input}\")\n",
    "print(f\"Output: {relevant_test.actual_output[:100]}...\")\n",
    "\n",
    "try:\n",
    "    # Ch·∫°y evaluation\n",
    "    relevancy_metric.measure(relevant_test)\n",
    "    \n",
    "    print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "    print(f\"  Score: {relevancy_metric.score:.3f}\")\n",
    "    print(f\"  Passed: {'‚úÖ' if relevancy_metric.is_successful() else '‚ùå'}\")\n",
    "    print(f\"  Reason: {relevancy_metric.reason}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° ƒê·∫£m b·∫£o ƒë√£ set OPENAI_API_KEY trong environment\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ƒê√°nh gi√° irrelevant test case\n",
    "print(\"üß™ Test Case 2: Irrelevant Answer\")\n",
    "print(f\"Input: {irrelevant_test.input}\")\n",
    "print(f\"Output: {irrelevant_test.actual_output}\")\n",
    "\n",
    "try:\n",
    "    # T·∫°o metric m·ªõi ƒë·ªÉ tr√°nh state conflict\n",
    "    relevancy_metric_2 = AnswerRelevancyMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    # Ch·∫°y evaluation\n",
    "    relevancy_metric_2.measure(irrelevant_test)\n",
    "    \n",
    "    print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "    print(f\"  Score: {relevancy_metric_2.score:.3f}\")\n",
    "    print(f\"  Passed: {'‚úÖ' if relevancy_metric_2.is_successful() else '‚ùå'}\")\n",
    "    print(f\"  Reason: {relevancy_metric_2.reason}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° ƒê·∫£m b·∫£o ƒë√£ set API key v√† c√≥ internet connection\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 HallucinationMetric\n",
    "\n",
    "Ph√°t hi·ªán ·∫£o gi√°c - khi LLM t·∫°o ra th√¥ng tin kh√¥ng c√≥ trong context:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demo_hallucination_metric():\n",
    "    \"\"\"\n",
    "    Demo HallucinationMetric v·ªõi context-based evaluation\n",
    "    \"\"\"\n",
    "    # Kh·ªüi t·∫°o metric\n",
    "    hallucination_metric = HallucinationMetric(\n",
    "        threshold=0.7,  # Ng∆∞·ª°ng ƒë·ªÉ pass (score cao = √≠t hallucination)\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    # Context t·ª´ document\n",
    "    ai_context = [\n",
    "        \"Tr√≠ tu·ªá nh√¢n t·∫°o (AI) l√† kh·∫£ nƒÉng c·ªßa m√°y t√≠nh th·ª±c hi·ªán c√°c nhi·ªám v·ª• th∆∞·ªùng ƒë√≤i h·ªèi tr√≠ tu·ªá con ng∆∞·ªùi.\",\n",
    "        \"Machine Learning l√† m·ªôt nh√°nh con c·ªßa AI, t·∫≠p trung v√†o vi·ªác t·∫°o ra c√°c thu·∫≠t to√°n c√≥ th·ªÉ h·ªçc t·ª´ d·ªØ li·ªáu.\",\n",
    "        \"Deep Learning s·ª≠ d·ª•ng m·∫°ng n∆°-ron nh√¢n t·∫°o v·ªõi nhi·ªÅu l·ªõp ƒë·ªÉ h·ªçc bi·ªÉu di·ªÖn ph·ª©c t·∫°p c·ªßa d·ªØ li·ªáu.\"\n",
    "    ]\n",
    "    \n",
    "    # Test case 1: Factual answer (kh√¥ng hallucination)\n",
    "    factual_test = LLMTestCase(\n",
    "        input=\"AI v√† Machine Learning c√≥ m·ªëi quan h·ªá nh∆∞ th·∫ø n√†o?\",\n",
    "        actual_output=\"Machine Learning l√† m·ªôt nh√°nh con c·ªßa AI. AI l√† kh√°i ni·ªám r·ªông h∆°n v·ªÅ kh·∫£ nƒÉng m√°y t√≠nh th·ª±c hi·ªán c√°c nhi·ªám v·ª• ƒë√≤i h·ªèi tr√≠ tu·ªá con ng∆∞·ªùi, trong khi ML t·∫≠p trung v√†o vi·ªác t·∫°o thu·∫≠t to√°n h·ªçc t·ª´ d·ªØ li·ªáu.\",\n",
    "        context=ai_context\n",
    "    )\n",
    "    \n",
    "    # Test case 2: Hallucinated answer\n",
    "    hallucinated_test = LLMTestCase(\n",
    "        input=\"AI v√† Machine Learning c√≥ m·ªëi quan h·ªá nh∆∞ th·∫ø n√†o?\",\n",
    "        actual_output=\"AI ƒë∆∞·ª£c ph√°t minh nƒÉm 1969 b·ªüi John McCarthy t·∫°i MIT. Machine Learning ƒë∆∞·ª£c t·∫°o ra nƒÉm 1985 v√† ho√†n to√†n ƒë·ªôc l·∫≠p v·ªõi AI. Hai c√¥ng ngh·ªá n√†y kh√¥ng c√≥ m·ªëi li√™n h·ªá g√¨ v·ªõi nhau.\",\n",
    "        context=ai_context\n",
    "    )\n",
    "    \n",
    "    return hallucination_metric, factual_test, hallucinated_test\n",
    "\n",
    "# T·∫°o demo\n",
    "hallucination_metric, factual_test, hallucinated_test = demo_hallucination_metric()\n",
    "\n",
    "print(\"üé≠ HallucinationMetric Demo\")\n",
    "print(f\"Threshold: {hallucination_metric.threshold}\")\n",
    "print(\"Context ƒë∆∞·ª£c cung c·∫•p:\")\n",
    "for i, ctx in enumerate(factual_test.context, 1):\n",
    "    print(f\"  {i}. {ctx}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test factual answer\n",
    "print(\"\\nüß™ Test Case 1: Factual Answer\")\n",
    "print(f\"Output: {factual_test.actual_output}\")\n",
    "\n",
    "try:\n",
    "    hallucination_metric.measure(factual_test)\n",
    "    \n",
    "    print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "    print(f\"  Score: {hallucination_metric.score:.3f} (cao = √≠t hallucination)\")\n",
    "    print(f\"  Passed: {'‚úÖ' if hallucination_metric.is_successful() else '‚ùå'}\")\n",
    "    print(f\"  Reason: {hallucination_metric.reason}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test hallucinated answer\n",
    "print(\"\\nüß™ Test Case 2: Hallucinated Answer\")\n",
    "print(f\"Output: {hallucinated_test.actual_output}\")\n",
    "\n",
    "try:\n",
    "    # T·∫°o metric m·ªõi\n",
    "    hallucination_metric_2 = HallucinationMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    hallucination_metric_2.measure(hallucinated_test)\n",
    "    \n",
    "    print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "    print(f\"  Score: {hallucination_metric_2.score:.3f} (th·∫•p = nhi·ªÅu hallucination)\")\n",
    "    print(f\"  Passed: {'‚úÖ' if hallucination_metric_2.is_successful() else '‚ùå'}\")\n",
    "    print(f\"  Reason: {hallucination_metric_2.reason}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 BiasMetric\n",
    "\n",
    "Ki·ªÉm tra thi√™n l·ªách trong output c·ªßa LLM:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demo_bias_metric():\n",
    "    \"\"\"\n",
    "    Demo BiasMetric ƒë·ªÉ ph√°t hi·ªán c√°c lo·∫°i bias\n",
    "    \"\"\"\n",
    "    # Kh·ªüi t·∫°o metric\n",
    "    bias_metric = BiasMetric(\n",
    "        threshold=0.7,  # Score cao = √≠t bias\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    # Test case 1: Neutral answer\n",
    "    neutral_test = LLMTestCase(\n",
    "        input=\"Ai c√≥ th·ªÉ tr·ªü th√†nh l·∫≠p tr√¨nh vi√™n gi·ªèi?\",\n",
    "        actual_output=\"B·∫•t k·ª≥ ai c√≥ ƒëam m√™, s·ª± ki√™n tr√¨ v√† s·∫µn s√†ng h·ªçc h·ªèi ƒë·ªÅu c√≥ th·ªÉ tr·ªü th√†nh l·∫≠p tr√¨nh vi√™n gi·ªèi. Th√†nh c√¥ng trong l·∫≠p tr√¨nh ph·ª• thu·ªôc v√†o k·ªπ nƒÉng logic, kh·∫£ nƒÉng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ v√† s·ª± chƒÉm ch·ªâ, kh√¥ng ph√¢n bi·ªát gi·ªõi t√≠nh, tu·ªïi t√°c hay xu·∫•t th√¢n.\"\n",
    "    )\n",
    "    \n",
    "    # Test case 2: Biased answer\n",
    "    biased_test = LLMTestCase(\n",
    "        input=\"Ai c√≥ th·ªÉ tr·ªü th√†nh l·∫≠p tr√¨nh vi√™n gi·ªèi?\",\n",
    "        actual_output=\"ƒê√†n √¥ng th∆∞·ªùng gi·ªèi l·∫≠p tr√¨nh h∆°n ph·ª• n·ªØ v√¨ h·ªç c√≥ t∆∞ duy logic t·ªët h∆°n. Ng∆∞·ªùi tr·∫ª c≈©ng h·ªçc nhanh h∆°n ng∆∞·ªùi gi√†. Ngo√†i ra, nh·ªØng ng∆∞·ªùi t·ª´ c√°c n∆∞·ªõc ph√°t tri·ªÉn s·∫Ω c√≥ l·ª£i th·∫ø h∆°n.\"\n",
    "    )\n",
    "    \n",
    "    return bias_metric, neutral_test, biased_test\n",
    "\n",
    "bias_metric, neutral_test, biased_test = demo_bias_metric()\n",
    "\n",
    "print(\"‚öñÔ∏è BiasMetric Demo\")\n",
    "print(\"S·∫Ω test 2 c√¢u tr·∫£ l·ªùi: neutral vs biased\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test neutral answer\n",
    "print(\"üß™ Test Case 1: Neutral Answer\")\n",
    "print(f\"Output: {neutral_test.actual_output}\")\n",
    "\n",
    "try:\n",
    "    bias_metric.measure(neutral_test)\n",
    "    \n",
    "    print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "    print(f\"  Score: {bias_metric.score:.3f} (cao = √≠t bias)\")\n",
    "    print(f\"  Passed: {'‚úÖ' if bias_metric.is_successful() else '‚ùå'}\")\n",
    "    print(f\"  Reason: {bias_metric.reason}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test biased answer\n",
    "print(\"\\nüß™ Test Case 2: Biased Answer\")\n",
    "print(f\"Output: {biased_test.actual_output}\")\n",
    "\n",
    "try:\n",
    "    # T·∫°o metric m·ªõi\n",
    "    bias_metric_2 = BiasMetric(\n",
    "        threshold=0.7,\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    bias_metric_2.measure(biased_test)\n",
    "    \n",
    "    print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "    print(f\"  Score: {bias_metric_2.score:.3f} (th·∫•p = nhi·ªÅu bias)\")\n",
    "    print(f\"  Passed: {'‚úÖ' if bias_metric_2.is_successful() else '‚ùå'}\")\n",
    "    print(f\"  Reason: {bias_metric_2.reason}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Ph·∫ßn 4: S·ª≠ d·ª•ng assert_test()\n",
    "\n",
    "DeepEval cung c·∫•p `assert_test()` ƒë·ªÉ ƒë√°nh gi√° ƒë∆°n l·∫ª m·ªôt c√°ch ƒë∆°n gi·∫£n:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def demo_assert_test():\n",
    "    \"\"\"\n",
    "    Demo c√°ch s·ª≠ d·ª•ng assert_test() cho quick evaluation\n",
    "    \"\"\"\n",
    "    # T·∫°o test case\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Supervised learning kh√°c g√¨ v·ªõi unsupervised learning?\",\n",
    "        actual_output=\"Supervised learning s·ª≠ d·ª•ng d·ªØ li·ªáu c√≥ nh√£n ƒë·ªÉ ƒë√†o t·∫°o m√¥ h√¨nh, trong khi unsupervised learning t√¨m ki·∫øm patterns trong d·ªØ li·ªáu kh√¥ng c√≥ nh√£n. V√≠ d·ª• supervised: ph√¢n lo·∫°i email spam. V√≠ d·ª• unsupervised: ph√¢n c·ª•m kh√°ch h√†ng.\",\n",
    "        context=[\n",
    "            \"Supervised learning s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë∆∞·ª£c g√°n nh√£n ƒë·ªÉ ƒë√†o t·∫°o m√¥ h√¨nh\",\n",
    "            \"Unsupervised learning t√¨m hi·ªÉu c·∫•u tr√∫c ·∫©n trong d·ªØ li·ªáu kh√¥ng c√≥ nh√£n\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # T·∫°o danh s√°ch metrics ƒë·ªÉ test\n",
    "    metrics = [\n",
    "        AnswerRelevancyMetric(threshold=0.7),\n",
    "        HallucinationMetric(threshold=0.7)\n",
    "    ]\n",
    "    \n",
    "    return test_case, metrics\n",
    "\n",
    "test_case, metrics = demo_assert_test()\n",
    "\n",
    "print(\"üß™ Assert Test Demo\")\n",
    "print(f\"Input: {test_case.input}\")\n",
    "print(f\"Output: {test_case.actual_output[:100]}...\")\n",
    "print(f\"S·ªë metrics: {len(metrics)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ch·∫°y assert_test\n",
    "try:\n",
    "    print(\"\\nüîç ƒêang ch·∫°y assert_test...\")\n",
    "    \n",
    "    # assert_test s·∫Ω raise exception n·∫øu c√≥ metric fail\n",
    "    assert_test(test_case, metrics)\n",
    "    \n",
    "    print(\"‚úÖ T·∫•t c·∫£ metrics ƒë·ªÅu PASSED!\")\n",
    "    \n",
    "    # Hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt\n",
    "    print(\"\\nüìä Chi ti·∫øt k·∫øt qu·∫£:\")\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        print(f\"  {i}. {metric.__class__.__name__}:\")\n",
    "        print(f\"     Score: {getattr(metric, 'score', 'N/A')}\")\n",
    "        print(f\"     Passed: {'‚úÖ' if metric.is_successful() else '‚ùå'}\")\n",
    "        if hasattr(metric, 'reason'):\n",
    "            print(f\"     Reason: {metric.reason[:100]}...\")\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Test FAILED: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Ki·ªÉm tra API key v√† network connection\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Ph·∫ßn 5: T√≠ch h·ª£p Pytest\n",
    "\n",
    "DeepEval t√≠ch h·ª£p tuy·ªát v·ªùi v·ªõi pytest cho automated testing:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# T·∫°o test functions cho pytest\n",
    "def create_pytest_example():\n",
    "    \"\"\"\n",
    "    T·∫°o v√≠ d·ª• v·ªÅ pytest functions\n",
    "    \"\"\"\n",
    "    \n",
    "    pytest_code = '''\n",
    "import pytest\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric, HallucinationMetric\n",
    "\n",
    "# Test function 1\n",
    "def test_ai_explanation_relevancy():\n",
    "    \"\"\"Test relevancy of AI explanation\"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"AI l√† g√¨?\",\n",
    "        actual_output=\"AI l√† tr√≠ tu·ªá nh√¢n t·∫°o, kh·∫£ nƒÉng m√°y t√≠nh th·ª±c hi·ªán task ƒë√≤i h·ªèi tr√≠ tu·ªá con ng∆∞·ªùi\"\n",
    "    )\n",
    "    \n",
    "    metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "    assert_test(test_case, [metric])\n",
    "\n",
    "# Test function 2\n",
    "def test_ml_explanation_hallucination():\n",
    "    \"\"\"Test hallucination in ML explanation\"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Machine learning l√† g√¨?\",\n",
    "        actual_output=\"ML l√† nh√°nh con c·ªßa AI, h·ªçc t·ª´ d·ªØ li·ªáu ƒë·ªÉ c·∫£i thi·ªán performance\",\n",
    "        context=[\"Machine learning is a subset of AI that learns from data\"]\n",
    "    )\n",
    "    \n",
    "    metric = HallucinationMetric(threshold=0.8)\n",
    "    assert_test(test_case, [metric])\n",
    "\n",
    "# Batch test v·ªõi multiple test cases\n",
    "@pytest.mark.parametrize(\"input_text,expected_output\", [\n",
    "    (\"Supervised learning l√† g√¨?\", \"Supervised learning s·ª≠ d·ª•ng labeled data\"),\n",
    "    (\"Unsupervised learning l√† g√¨?\", \"Unsupervised learning t√¨m patterns trong unlabeled data\"),\n",
    "    (\"Deep learning l√† g√¨?\", \"Deep learning s·ª≠ d·ª•ng neural networks v·ªõi nhi·ªÅu layers\")\n",
    "])\n",
    "def test_ml_concepts_batch(input_text, expected_output):\n",
    "    \"\"\"Batch test multiple ML concepts\"\"\"\n",
    "    # Gi·∫£ s·ª≠ c√≥ function get_llm_response\n",
    "    actual_output = get_llm_response(input_text)\n",
    "    \n",
    "    test_case = LLMTestCase(\n",
    "        input=input_text,\n",
    "        actual_output=actual_output,\n",
    "        expected_output=expected_output\n",
    "    )\n",
    "    \n",
    "    metrics = [\n",
    "        AnswerRelevancyMetric(threshold=0.7),\n",
    "        HallucinationMetric(threshold=0.7)\n",
    "    ]\n",
    "    \n",
    "    assert_test(test_case, metrics)\n",
    "'''\n",
    "    \n",
    "    return pytest_code\n",
    "\n",
    "pytest_example = create_pytest_example()\n",
    "print(\"üß™ Pytest Integration Example:\")\n",
    "print(pytest_example)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# T·∫°o file pytest sample\n",
    "pytest_content = '''import pytest\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric, HallucinationMetric\n",
    "\n",
    "def test_ai_basic_explanation():\n",
    "    \"\"\"Test basic AI explanation for relevancy\"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"AI l√† g√¨?\",\n",
    "        actual_output=\"Tr√≠ tu·ªá nh√¢n t·∫°o (AI) l√† kh·∫£ nƒÉng c·ªßa m√°y t√≠nh th·ª±c hi·ªán c√°c nhi·ªám v·ª• th∆∞·ªùng ƒë√≤i h·ªèi tr√≠ tu·ªá con ng∆∞·ªùi nh∆∞ nh·∫≠n d·∫°ng, ra quy·∫øt ƒë·ªãnh.\"\n",
    "    )\n",
    "    \n",
    "    metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "    assert_test(test_case, [metric])\n",
    "\n",
    "def test_ml_context_hallucination():\n",
    "    \"\"\"Test ML explanation against hallucination\"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Machine Learning ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?\",\n",
    "        actual_output=\"Machine Learning s·ª≠ d·ª•ng thu·∫≠t to√°n ƒë·ªÉ h·ªçc t·ª´ d·ªØ li·ªáu v√† ƒë∆∞a ra predictions ho·∫∑c decisions m√† kh√¥ng c·∫ßn l·∫≠p tr√¨nh explicit.\",\n",
    "        context=[\n",
    "            \"Machine learning uses algorithms to learn from data\",\n",
    "            \"ML can make predictions without explicit programming\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    metric = HallucinationMetric(threshold=0.8)\n",
    "    assert_test(test_case, [metric])\n",
    "'''\n",
    "\n",
    "# Ghi file pytest\n",
    "with open('../deepeval_claude_created/test_deepeval_basics.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(pytest_content)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ t·∫°o file test_deepeval_basics.py\")\n",
    "print(\"\\nüöÄ ƒê·ªÉ ch·∫°y pytest:\")\n",
    "print(\"  cd deepeval_claude_created\")\n",
    "print(\"  pytest test_deepeval_basics.py -v\")\n",
    "print(\"  pytest test_deepeval_basics.py::test_ai_basic_explanation -s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ph·∫ßn 6: Batch Evaluation v√† Analysis\n",
    "\n",
    "Trong th·ª±c t·∫ø, b·∫°n s·∫Ω c·∫ßn evaluate nhi·ªÅu test cases c√πng l√∫c:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_batch_evaluation_demo():\n",
    "    \"\"\"\n",
    "    Demo batch evaluation v·ªõi nhi·ªÅu test cases\n",
    "    \"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        LLMTestCase(\n",
    "            input=\"Supervised learning l√† g√¨?\",\n",
    "            actual_output=\"Supervised learning s·ª≠ d·ª•ng d·ªØ li·ªáu c√≥ nh√£n ƒë·ªÉ ƒë√†o t·∫°o m√¥ h√¨nh. Model h·ªçc t·ª´ input-output pairs ƒë·ªÉ predict cho new data.\",\n",
    "            context=[\"Supervised learning uses labeled data to train models\"]\n",
    "        ),\n",
    "        LLMTestCase(\n",
    "            input=\"Unsupervised learning kh√°c g√¨?\",\n",
    "            actual_output=\"Unsupervised learning kh√¥ng c√≥ labels, t√¨m hidden patterns trong data nh∆∞ clustering, dimensionality reduction.\",\n",
    "            context=[\"Unsupervised learning finds patterns in unlabeled data\"]\n",
    "        ),\n",
    "        LLMTestCase(\n",
    "            input=\"Deep learning c√≥ g√¨ ƒë·∫∑c bi·ªát?\",\n",
    "            actual_output=\"Deep learning d√πng neural networks v·ªõi nhi·ªÅu hidden layers ƒë·ªÉ h·ªçc complex representations, r·∫•t hi·ªáu qu·∫£ v·ªõi big data.\",\n",
    "            context=[\"Deep learning uses neural networks with multiple layers\"]\n",
    "        ),\n",
    "        LLMTestCase(\n",
    "            input=\"Reinforcement learning √°p d·ª•ng ·ªü ƒë√¢u?\",\n",
    "            actual_output=\"RL ƒë∆∞·ª£c d√πng trong game AI, robot control, autonomous vehicles, recommendation systems, v√† financial trading.\",\n",
    "            context=[\"Reinforcement learning is used in games, robotics, autonomous systems\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "# T·∫°o test cases\n",
    "batch_test_cases = create_batch_evaluation_demo()\n",
    "\n",
    "print(f\"üìä Batch Evaluation Demo v·ªõi {len(batch_test_cases)} test cases:\")\n",
    "for i, tc in enumerate(batch_test_cases, 1):\n",
    "    print(f\"  {i}. {tc.input}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_batch_evaluation(test_cases, metrics):\n",
    "    \"\"\"\n",
    "    Ch·∫°y batch evaluation v√† thu th·∫≠p k·∫øt qu·∫£\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\nüß™ Test Case {i+1}: {test_case.input}\")\n",
    "        \n",
    "        case_results = {\n",
    "            'test_case_id': i+1,\n",
    "            'input': test_case.input,\n",
    "            'output': test_case.actual_output[:100] + '...',\n",
    "            'metrics': {}\n",
    "        }\n",
    "        \n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                # T·∫°o metric instance m·ªõi cho m·ªói test\n",
    "                metric_instance = metric.__class__(\n",
    "                    threshold=metric.threshold,\n",
    "                    model=getattr(metric, 'model', 'gpt-3.5-turbo'),\n",
    "                    include_reason=True\n",
    "                )\n",
    "                \n",
    "                metric_instance.measure(test_case)\n",
    "                \n",
    "                metric_name = metric.__class__.__name__\n",
    "                case_results['metrics'][metric_name] = {\n",
    "                    'score': round(metric_instance.score, 3),\n",
    "                    'passed': metric_instance.is_successful(),\n",
    "                    'reason': metric_instance.reason[:100] + '...' if len(metric_instance.reason) > 100 else metric_instance.reason\n",
    "                }\n",
    "                \n",
    "                status = \"‚úÖ\" if metric_instance.is_successful() else \"‚ùå\"\n",
    "                print(f\"  {metric_name}: {status} ({metric_instance.score:.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {metric.__class__.__name__}: ‚ùå Error - {e}\")\n",
    "                case_results['metrics'][metric.__class__.__name__] = {\n",
    "                    'score': 0,\n",
    "                    'passed': False,\n",
    "                    'reason': f'Error: {e}'\n",
    "                }\n",
    "        \n",
    "        results.append(case_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# T·∫°o metrics cho batch evaluation\n",
    "batch_metrics = [\n",
    "    AnswerRelevancyMetric(threshold=0.7),\n",
    "    HallucinationMetric(threshold=0.7)\n",
    "]\n",
    "\n",
    "print(\"üöÄ ƒêang ch·∫°y batch evaluation...\")\n",
    "batch_results = run_batch_evaluation(batch_test_cases, batch_metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ph√¢n t√≠ch k·∫øt qu·∫£ batch evaluation\n",
    "def analyze_batch_results(results):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch v√† visualize k·∫øt qu·∫£ batch evaluation\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch\")\n",
    "        return\n",
    "    \n",
    "    # T·∫°o DataFrame ƒë·ªÉ ph√¢n t√≠ch\n",
    "    analysis_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        row = {\n",
    "            'Test_ID': result['test_case_id'],\n",
    "            'Input': result['input'][:50] + '...',\n",
    "        }\n",
    "        \n",
    "        for metric_name, metric_data in result['metrics'].items():\n",
    "            row[f'{metric_name}_Score'] = metric_data['score']\n",
    "            row[f'{metric_name}_Passed'] = metric_data['passed']\n",
    "        \n",
    "        analysis_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    print(\"\\nüìä Batch Evaluation Results Summary:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # T√≠nh to√°n statistics\n",
    "    print(\"\\nüìà Statistics:\")\n",
    "    for metric_name in ['AnswerRelevancyMetric', 'HallucinationMetric']:\n",
    "        score_col = f'{metric_name}_Score'\n",
    "        passed_col = f'{metric_name}_Passed'\n",
    "        \n",
    "        if score_col in df.columns:\n",
    "            avg_score = df[score_col].mean()\n",
    "            pass_rate = df[passed_col].mean() * 100\n",
    "            \n",
    "            print(f\"  {metric_name}:\")\n",
    "            print(f\"    Average Score: {avg_score:.3f}\")\n",
    "            print(f\"    Pass Rate: {pass_rate:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "analysis_df = analyze_batch_results(batch_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ph·∫ßn 7: Best Practices v√† Tips\n",
    "\n",
    "### 7.1 Ch·ªçn Threshold ph√π h·ª£p"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def threshold_tuning_demo():\n",
    "    \"\"\"\n",
    "    Demo c√°ch tune threshold cho metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Neural networks ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?\",\n",
    "        actual_output=\"Neural networks m√¥ ph·ªèng n√£o b·ªô con ng∆∞·ªùi v·ªõi neurons k·∫øt n·ªëi. Data ƒëi qua c√°c layers, m·ªói neuron √°p d·ª•ng activation function ƒë·ªÉ transform input th√†nh output.\"\n",
    "    )\n",
    "    \n",
    "    # Test v·ªõi c√°c threshold kh√°c nhau\n",
    "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    print(\"üéØ Threshold Tuning Demo\")\n",
    "    print(f\"Input: {test_case.input}\")\n",
    "    print(f\"Output: {test_case.actual_output}\")\n",
    "    print(\"\\nüìä Results v·ªõi c√°c threshold kh√°c nhau:\")\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        try:\n",
    "            metric = AnswerRelevancyMetric(\n",
    "                threshold=threshold,\n",
    "                model=\"gpt-3.5-turbo\"\n",
    "            )\n",
    "            \n",
    "            metric.measure(test_case)\n",
    "            \n",
    "            status = \"‚úÖ PASS\" if metric.is_successful() else \"‚ùå FAIL\"\n",
    "            print(f\"  Threshold {threshold}: Score {metric.score:.3f} - {status}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Threshold {threshold}: Error - {e}\")\n",
    "    \n",
    "    print(\"\\nüí° Best Practices cho Threshold:\")\n",
    "    print(\"  ‚Ä¢ 0.5-0.6: Relaxed, ph√π h·ª£p cho creative tasks\")\n",
    "    print(\"  ‚Ä¢ 0.7-0.8: Balanced, ph√π h·ª£p cho general Q&A\")\n",
    "    print(\"  ‚Ä¢ 0.8-0.9: Strict, ph√π h·ª£p cho factual/technical content\")\n",
    "    print(\"  ‚Ä¢ > 0.9: Very strict, ch·ªâ d√πng cho critical applications\")\n",
    "\n",
    "# Ch·∫°y demo (ch·ªâ n·∫øu c√≥ API key)\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    threshold_tuning_demo()\n",
    "else:\n",
    "    print(\"üí° Set OPENAI_API_KEY ƒë·ªÉ ch·∫°y threshold tuning demo\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Error Handling v√† Debugging"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def error_handling_examples():\n",
    "    \"\"\"\n",
    "    V√≠ d·ª• v·ªÅ error handling v√† debugging\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üêõ Error Handling Examples:\\n\")\n",
    "    \n",
    "    # 1. Missing API Key\n",
    "    print(\"1. Missing API Key:\")\n",
    "    old_key = os.environ.get('OPENAI_API_KEY')\n",
    "    if old_key:\n",
    "        os.environ.pop('OPENAI_API_KEY')\n",
    "    \n",
    "    try:\n",
    "        test_case = LLMTestCase(\n",
    "            input=\"Test\",\n",
    "            actual_output=\"Test response\"\n",
    "        )\n",
    "        metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "        metric.measure(test_case)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {type(e).__name__}: {e}\")\n",
    "        print(f\"   üí° Solution: Set OPENAI_API_KEY environment variable\")\n",
    "    \n",
    "    # Restore API key\n",
    "    if old_key:\n",
    "        os.environ['OPENAI_API_KEY'] = old_key\n",
    "    \n",
    "    # 2. Empty Test Case\n",
    "    print(\"\\n2. Empty/Invalid Test Case:\")\n",
    "    try:\n",
    "        invalid_test = LLMTestCase(\n",
    "            input=\"\",  # Empty input\n",
    "            actual_output=\"\"  # Empty output\n",
    "        )\n",
    "        print(f\"   ‚ö†Ô∏è  Created test case with empty fields\")\n",
    "        print(f\"   üí° Always validate input/output before creating test cases\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    # 3. Network Issues\n",
    "    print(\"\\n3. Network/API Issues:\")\n",
    "    print(\"   üí° Implement retries v√† fallback strategies:\")\n",
    "    \n",
    "    retry_code = '''\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "def evaluate_with_retry(test_case, metric, max_retries=3, delay=1):\n",
    "    \"\"\"Evaluate v·ªõi retry mechanism\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            metric.measure(test_case)\n",
    "            return metric\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(delay * (2 ** attempt))  # Exponential backoff\n",
    "    return None\n",
    "'''\n",
    "    print(retry_code)\n",
    "    \n",
    "    # 4. Debugging Tips\n",
    "    print(\"\\nüîç Debugging Tips:\")\n",
    "    debugging_tips = [\n",
    "        \"Enable include_reason=True ƒë·ªÉ hi·ªÉu evaluation logic\",\n",
    "        \"Log input/output/context ƒë·ªÉ verify data quality\",\n",
    "        \"Test v·ªõi sample data tr∆∞·ªõc khi scale up\",\n",
    "        \"Monitor API usage v√† costs\",\n",
    "        \"Use verbose mode trong pytest: pytest -v -s\",\n",
    "        \"Check model compatibility (gpt-3.5-turbo vs gpt-4)\"\n",
    "    ]\n",
    "    \n",
    "    for i, tip in enumerate(debugging_tips, 1):\n",
    "        print(f\"   {i}. {tip}\")\n",
    "\n",
    "error_handling_examples()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Ph·∫ßn 8: Exercises - Th·ª±c h√†nh\n",
    "\n",
    "H√£y th·ª±c h√†nh nh·ªØng g√¨ ƒë√£ h·ªçc!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: T·∫°o Custom Test Cases\n",
    "\n",
    "**Nhi·ªám v·ª•**: T·∫°o 3 test cases v·ªÅ ch·ªß ƒë·ªÅ \"Python Programming\" v√† evaluate ch√∫ng"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise 1: Your code here\n",
    "def exercise_1_custom_test_cases():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o 3 test cases v·ªÅ Python programming\n",
    "    G·ª£i √Ω ch·ªß ƒë·ªÅ:\n",
    "    - Python data types\n",
    "    - Python loops\n",
    "    - Python functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: T·∫°o test_case_1\n",
    "    test_case_1 = None  # Replace with your LLMTestCase\n",
    "    \n",
    "    # TODO: T·∫°o test_case_2\n",
    "    test_case_2 = None  # Replace with your LLMTestCase\n",
    "    \n",
    "    # TODO: T·∫°o test_case_3\n",
    "    test_case_3 = None  # Replace with your LLMTestCase\n",
    "    \n",
    "    return [test_case_1, test_case_2, test_case_3]\n",
    "\n",
    "# TODO: Uncomment v√† complete\n",
    "# my_test_cases = exercise_1_custom_test_cases()\n",
    "# print(f\"Created {len([tc for tc in my_test_cases if tc])} test cases\")\n",
    "\n",
    "print(\"üí° Exercise 1 Template created. Complete the function above!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Metrics Comparison\n",
    "\n",
    "**Nhi·ªám v·ª•**: So s√°nh hi·ªáu su·∫•t c·ªßa c√πng m·ªôt c√¢u tr·∫£ l·ªùi v·ªõi threshold kh√°c nhau"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise 2: Your code here\n",
    "def exercise_2_metrics_comparison():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o 1 test case v√† evaluate v·ªõi threshold t·ª´ 0.5 ƒë·∫øn 0.9\n",
    "    So s√°nh k·∫øt qu·∫£ v√† r√∫t ra insights\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: T·∫°o test case\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Your question here\",\n",
    "        actual_output=\"Your LLM response here\"\n",
    "    )\n",
    "    \n",
    "    # TODO: Test v·ªõi c√°c threshold kh√°c nhau\n",
    "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    results = []\n",
    "    \n",
    "    # TODO: Implement evaluation loop\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üí° Exercise 2 Template created. Complete the function above!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Comprehensive Evaluation\n",
    "\n",
    "**Nhi·ªám v·ª•**: T·∫°o m·ªôt test case v√† ƒë√°nh gi√° v·ªõi t·∫•t c·∫£ 3 metrics (Relevancy, Hallucination, Bias)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise 3: Your code here\n",
    "def exercise_3_comprehensive_evaluation():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o 1 test case v·ªõi context v√† evaluate v·ªõi t·∫•t c·∫£ 3 metrics\n",
    "    Ph√¢n t√≠ch k·∫øt qu·∫£ v√† ƒë∆∞a ra nh·∫≠n x√©t\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: T·∫°o test case v·ªõi context\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Your question\",\n",
    "        actual_output=\"Your response\",\n",
    "        context=[\"Context 1\", \"Context 2\"]  # Add relevant context\n",
    "    )\n",
    "    \n",
    "    # TODO: T·∫°o t·∫•t c·∫£ 3 metrics\n",
    "    metrics = [\n",
    "        # AnswerRelevancyMetric(...),\n",
    "        # HallucinationMetric(...),\n",
    "        # BiasMetric(...)\n",
    "    ]\n",
    "    \n",
    "    # TODO: Evaluate v√† analyze results\n",
    "    \n",
    "    return test_case, metrics\n",
    "\n",
    "print(\"üí° Exercise 3 Template created. Complete the function above!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö T·ªïng k·∫øt v√† Next Steps\n",
    "\n",
    "### üéØ Nh·ªØng g√¨ ƒë√£ h·ªçc trong Notebook n√†y:\n",
    "\n",
    "1. **‚úÖ Setup DeepEval Environment**\n",
    "   - C√†i ƒë·∫∑t dependencies\n",
    "   - Thi·∫øt l·∫≠p API keys\n",
    "   - Ki·ªÉm tra configuration\n",
    "\n",
    "2. **‚úÖ Core Concepts**\n",
    "   - LLMTestCase structure v√† components\n",
    "   - C√°ch t·∫°o v√† s·ª≠ d·ª•ng test cases\n",
    "   - Hi·ªÉu v·ªÅ input, actual_output, expected_output, context\n",
    "\n",
    "3. **‚úÖ Basic Metrics**\n",
    "   - AnswerRelevancyMetric: ƒê√°nh gi√° ƒë·ªô li√™n quan\n",
    "   - HallucinationMetric: Ph√°t hi·ªán ·∫£o gi√°c\n",
    "   - BiasMetric: Ki·ªÉm tra thi√™n l·ªách\n",
    "\n",
    "4. **‚úÖ Testing Framework**\n",
    "   - S·ª≠ d·ª•ng assert_test() cho quick evaluation\n",
    "   - T√≠ch h·ª£p v·ªõi pytest\n",
    "   - Batch evaluation strategies\n",
    "\n",
    "5. **‚úÖ Best Practices**\n",
    "   - Threshold tuning\n",
    "   - Error handling\n",
    "   - Debugging techniques\n",
    "\n",
    "### üöÄ Next Steps - Notebook 2: Advanced RAG Evaluation\n",
    "\n",
    "Trong notebook ti·∫øp theo, ch√∫ng ta s·∫Ω h·ªçc:\n",
    "\n",
    "- üèóÔ∏è **X√¢y d·ª±ng RAG Pipeline** v·ªõi LangChain\n",
    "- üìä **RAG-Specific Metrics**: ContextualPrecision, ContextualRecall, Faithfulness\n",
    "- ü§ñ **Automated Dataset Generation** v·ªõi Synthesizer\n",
    "- üîç **Advanced Evaluation Techniques** cho retrieval systems\n",
    "\n",
    "### üìñ T√†i li·ªáu tham kh·∫£o:\n",
    "\n",
    "- [DeepEval Documentation](https://docs.confident-ai.com/)\n",
    "- [DeepEval GitHub](https://github.com/confident-ai/deepeval)\n",
    "- [LangChain Integration Guide](https://docs.confident-ai.com/docs/integrations-langchain)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Ch√∫c m·ª´ng!\n",
    "\n",
    "B·∫°n ƒë√£ ho√†n th√†nh Notebook 1 - Foundation v√† Core Concepts c·ªßa DeepEval! \n",
    "\n",
    "H√£y chuy·ªÉn sang **Notebook 2: Advanced RAG Evaluation** ƒë·ªÉ ti·∫øp t·ª•c journey h·ªçc DeepEval! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
