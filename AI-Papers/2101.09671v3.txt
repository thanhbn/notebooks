# 2101.09671v3.pdf
# Converted from PDF to TXT
# Source path: D:\llm\notebooks\AI-Papers\2101.09671v3.pdf
# File size: 1786864 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pruning and Quantization for Deep Neural Network Acceleration: A
Survey
Tailin Lianga,b, John Glossnera,b,c, Lei Wanga, Shaobo Shia,band Xiaotong Zhanga,<
aSchool of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China
bHua Xia General Processor Technologies, Beijing 100080, China
cGeneral Processor Technologies, Tarrytown, NY 10591, United States
ARTICLE INFO
Keywords :
convolutional neural network
neural network acceleration
neural network quantization
neural network pruning
low-bit mathematicsABSTRACT
Deep neural networks have been applied in many applications exhibiting extraordinary abilities in
the ﬁeld of computer vision. However, complex network architectures challenge eﬃcient real-time
deployment and require signiﬁcant computation resources and energy costs. These challenges can
be overcome through optimizations such as network compression. Network compression can often
be realized with little loss of accuracy. In some cases accuracy may even improve. This paper
providesasurveyontwotypesofnetworkcompression: pruningandquantization. Pruningcanbe
categorizedasstaticifitisperformedoﬄineordynamicif itisperformedatrun-time. Wecompare
pruningtechniquesanddescribecriteriausedtoremoveredundantcomputations. Wediscusstrade-oﬀs
in element-wise, channel-wise, shape-wise, ﬁlter-wise, layer-wise and even network-wise pruning.
Quantizationreducescomputationsbyreducingtheprecisionofthedatatype. Weights,biases,and
activations may be quantized typically to 8-bit integers although lower bit width implementations
are also discussed including binary neural networks. Both pruning and quantization can be used
independently or combined. We compare current techniques, analyze their strengths and weaknesses,
present compressed network accuracy results on a number of frameworks, and provide practical
guidance for compressing networks.
1. Introduction
DeepNeuralNetworks(DNNs)haveshownextraordinary
abilitiesincomplicatedapplicationssuchasimageclassiﬁca-
tion, object detection, voice synthesis, and semantic segmen-
tation [138]. Recent neural network designs with billions of
parametershavedemonstratedhuman-levelcapabilitiesbut
at the cost of signiﬁcant computational complexity. DNNs
with many parameters are also time-consuming to train [ 26].
Theselargenetworksarealsodiﬃculttodeployinembedded
environments. Bandwidth becomes a limiting factor when
moving weights and data between Compute Units (CUs) and
memory. Over-parameterizationisthepropertyofaneural
networkwhereredundantneuronsdonotimprovetheaccu-
racy of results. This redundancy can often be removed with
little or no accuracy loss [225].
Figure 1 shows three design considerations that may con-
tributetoover-parameterization: 1)networkstructure,2)net-
workoptimization,and3)hardwareacceleratordesign. These
design considerations are speciﬁc to Convolutional Neural
Networks (CNNs) but also generally relevant to DNNs.
Networkstructureencompassesthreeparts: 1)novelcom-
ponents,2)networkarchitecturesearch,and3)knowledgedis-
tillation. Novel components is the design of eﬃcient blocks
suchasseparableconvolution, inception blocks, andresidual
blocks. They are discussed in Section 2.4. Network com-
ponents also encompasses the types of connections within
layers. Fully connected deep neural networks require N2
<Corresponding author
tailin.liang@xs.ustb.edu.cn (T. Liang); jglossner@ustb.edu.cn (J.
Glossner); wanglei@ustb.edu.cn (L. Wang); sbshi@hxgpt.com (S. Shi);
zxt@ies.ustb.edu.cn (X. Zhang)
ORCID(s): 0000-0002-7643-912X (T. Liang)connections between neurons. Feed forward layers reduce
connectionsbyconsideringonlyconnectionsintheforward
path. This reduces the number of connections to N. Other
typesofcomponentssuchasdropoutlayerscanreducethe
number of connections even further.
Network Architecture Search (NAS) [ 63], also known as
network auto search, programmatically searches for a highly
eﬃcient network structure from a large predeﬁned search
space. An estimator is applied to each produced architecture.
While time-consuming to compute, the ﬁnal architecture of-
ten outperforms manually designed networks.
Knowledge Distillation (KD) [ 80,206] evolved from
knowledge transfer [ 27]. The goal is to generate a simpler
compressedmodelthatfunctionsaswellasalargermodel.
KDtrainsastudentnetworkthattriestoimitateateachernet-
work. The student network is usually but not always smaller
and shallower than the teacher. The trained student model
should be less computationally complex than the teacher.
Networkoptimization[ 137]includes: 1)computational
convolution optimization,2) parameterfactorization, 3) net-
work pruning, and 4) network quantization. Convolution op-
erationsaremoreeﬃcientthanfullyconnectedcomputations
because they keep high dimensional information as a 3D ten-
sorratherthanﬂatteningthetensorsintovectorsthatremoves
the original spatial information. This feature helps CNNs
to ﬁt the underlying structure of image data in particular.
Convolution layers also require signiﬁcantly less coeﬃcients
comparedtoFullyConnectedLayers(FCLs). Computational
convolution optimizations include Fast Fourier Transform
(FFT)basedconvolution[ 168],Winogradconvolution[ 135],
and the popular image to column (im2col) [ 34] approach.
We discuss im2col in detail in Section 2.3 since it is directly
T Liang et al.: Preprint submitted to Elsevier Page 1 of 41arXiv:2101.09671v3  [cs.CV]  15 Jun 2021

--- PAGE 2 ---
Survey on pruning and quantization
CNN Acceleration [40, 39, 142, 137, 194, 263, 182]
Network Optimization
Convolution Optimization
Factorization
Pruning [201, 24, 12, 250]
Quantization [131, 87]
Network Structure
Novel Components
Network Architecture Search [63]
Knowledge Distillation [80, 206]
Hardware Accelerator [151, 202]
Platform
CPU
GPU
ASIC
FPGA [86, 3, 234, 152]
Optimization
Lookup Table
Computation Reuse
Memory Optimization
...
Figure 1: CNN Acceleration Approaches: Follow the sense from designing to implementing, CNN acceleration could fall into three
categories, structure design (or generation), further optimization, and specialized hardware.
related to general pruning techniques.
Parameterfactorizationisatechniquethatdecomposes
higher-ranktensorsintolower-ranktensorssimplifyingmem-
oryaccessandcompressingmodelsize. Itworksbybreaking
large layers into many smaller ones, thereby reducing the
numberofcomputations. Itcanbeappliedtobothconvolu-
tional and fully connected layers. This technique can also be
applied with pruning and quantization.
Network pruning [ 201,24,12,250] involves removing
parameters that don’t impact network accuracy. Pruning can
beperformedinmanywaysandisdescribedextensivelyin
Section 3.
Networkquantization[ 131,87]involvesreplacingdatatypes
with reduced width datatypes. For example, replacing 32-bit
Floating Point (FP32) with 8-bit Integers (INT8). The val-
ues can often be encoded to preserve more information than
simpleconversion. Quantizationis describedextensively in
Section 4.
Hardware accelerators [ 151,202] are designed primarily
for network acceleration. At a high level they encompass
entireprocessorplatformsandoftenincludehardwareopti-
mized for neural networks. Processor platforms include spe-
cialized Central Processing Unit (CPU) instructions, Graph-
icsProcessingUnits(GPUs),ApplicationSpeciﬁcIntegrated
Circuits (ASICs), and Field Programmable Gate Arrays (FP-
GAs).
CPUs have been optimized with specialized Artiﬁcial
Intelligence (AI) instructions usually within specialized Sin-
gleInstructionMultipleData(SIMD)units[ 49,11]. While
CPUscanbeusedfortraining,theyhaveprimarilybeenused
forinferenceinsystemsthatdonothavespecializedinference
accelerators.
GPUs have been used for both training and inference.
nVidia has specialized tensor units incorporated into their
GPUs that are optimized for neural network acceleration
[186]. AMD [ 7], ARM [ 10], and Imagination [ 117] also
have GPUs with instructions for neural network acceleration.
Specialized ASICs have also been designed for neural
networkacceleration. Theytypicallytargetinferenceatthe
edge,insecuritycameras,oronmobiledevices. Examplesinclude: GeneralProcessorTechnologies(GPT)[ 179],ARM,
nVidia, and 60+ others [ 202] all have processors targeting
thisspace. ASICsmayalsotargetbothtrainingandinference
in datacenters. Tensorprocessing units(TPU) fromGoogle
[125], Habana from Intel [ 169], Kunlun from Baidu [ 191],
HanguangfromAlibaba[ 124],andIntelligenceProcessing
Unit (IPU) from Graphcore [121].
Programmable reconﬁgurable FPGAs have been used for
neural network acceleration [ 86,3,234,152]. FPGAs are
widelyusedbyresearchersduetolongASICdesigncycles.
Neural network libraries are available from Xilinx [ 128] and
Intel[69]. Speciﬁcneuralnetworkacceleratorsarealsobeing
integrated into FPGA fabrics [ 248,4,203]. Because FPGAs
operate at the gate level, they are often used in low-bit width
and binary neural networks [178, 267, 197].
Neural network speciﬁc optimizations are typically in-
corporatedintocustomASIChardware. Lookuptablescan
be used to accelerate trigonometric activation functions [ 46]
or directly generate results for low bit-width arithmetic [ 65],
partial products can be stored in special registers and reused
[38],andmemoryaccessorderingwithspecializedaddress-
inghardwarecanallreducethenumberofcyclestocompute
a neural network output [ 126]. Hardware accelerators are
not the primary focus of this paper. However, we do note
hardware implementations that incorporate speciﬁc accelera-
tion techniques. Further background information on eﬃcient
processingandhardwareimplementationsofDNNscanbe
found in [225].
We summarize our main contributions as follows:
•Weprovideareviewoftwonetworkcompressiontech-
niques: pruningandquantization. Wediscussmethods
of compression, mathematical formulations, and com-
pare current State-Of-The-Art (SOTA) compression
methods.
•Weclassifypruningtechniquesintostaticanddynamic
methods,dependingiftheyaredoneoﬄineoratrun-
time, respectively.
•We analyze and quantitatively compare quantization
T Liang et al.: Preprint submitted to Elsevier Page 2 of 41

--- PAGE 3 ---
Survey on pruning and quantization
techniques and frameworks.
•We provide practical guidance on quantization and
pruning.
This paper focuses primarily on network optimization
for convolutional neural networks. It is organized as fol-
lows: InSection2wegiveanintroductiontoneuralnetworks
andspeciﬁcallyconvolutionalneuralnetworks. Wealsode-
scribe some of the network optimizations of convolutions.
In Section 3 we describe both static and dynamic pruning
techniques. InSection4wediscussquantizationanditsef-
fectonaccuracy. Wealsocomparequantizationlibrariesand
frameworks. We then present quantized accuracy results for
a number of common networks. We present conclusions and
provideguidanceonappropriateapplicationuseinSection5.
Finally, we present concluding comments in Section 7.
2. Convolutional Neural Network
Convolutionalneuralnetworksareaclassoffeed-forward
DNNsthatuseconvolutionoperationstoextractfeaturesfrom
a data source. CNNs have been most successfully applied to
visual-relatedtaskshowevertheyhavefounduseinnatural
languageprocessing[ 95],speechrecognition[ 2],recommen-
dationsystems[ 214],malwaredetection[ 223],andindustrial
sensors time series prediction[ 261]. To provide a better un-
derstanding of optimization techniques, in this section, we
introduce the two phases of CNN deployment - training and
inference, discuss types of convolution operations, describe
BatchNormalization(BN)asanaccelerationtechniquefor
training,describepoolingasatechniquetoreducecomplexity,
and describe the exponential growth in parameters deployed
in modern network structures.
2.1. Deﬁnitions
This section summarizes terms and deﬁnitions used to
describe neural networks as well as acronyms collected in
Table 1.
•Coeﬃcient - A constant by which an algebraic term is
multiplied. Typically, a coeﬃcient is multiplied by the
data in a CNN ﬁlter.
•Parameter - All the factors of a layer, including coeﬃ-
cients and biases.
•Hyperparameter-Apredeﬁnedparameterbeforenet-
work training, or ﬁne-tunning (re-training).
•Activation( AËRhwc)-Theactivated(e.g.,ReLu,
Leaky, Tanh, etc.) output of one layer in a multi-layer
network architecture, typically in height h, widthw,
andchannel c. Thehwmatrixissometimescalled
anactivationmap. Wealsodenoteactivationasoutput
(O) when the activation function does not matter.
•Feature( FËRhwc)-Theinputdataofonelayer,to
distinguish the output A. Generally the feature for the
current layer is the activation of the previous layer.•Kernel ( kËRk1k2) - Convolutional coeﬃcients for a
channel, excluding biases. Typically they are square
(e.g.k1=k2) and sized 1, 3, 7.
•Filter ( wËRk1k2cn) - Comprises all of the kernels
corresponding to the cchannels of input features. The
ﬁlter’s number, n, results in diﬀerent output channels.
•Weights - Two common uses: 1) kernel coeﬃcients
whendescribingpartofanetwork,and2)allthetrained
parametersinaneuralnetworkmodelwhendiscussing
the entire network.
2.2. Training and Inference
CNNsaredeployedasatwostepprocess: 1)trainingand
2)inference. Trainingisperformedﬁrstwiththeresultbeing
eitheracontinuousnumericalvalue(regression)oradiscrete
class label (classiﬁcation). Classiﬁcation training involves
applying a given annotated dataset as an input to the CNN,
propagatingitthroughthenetwork,andcomparingtheoutput
classiﬁcation to the ground-truth label. The network weights
arethenupdated typicallyusingabackpropagationstrategy
such as Stochastic Gradient Descent (SGD) to reduce clas-
siﬁcationerrors. Thisperformsasearchforthebestweight
values. Backpropogation is performed iteratively until a min-
imumacceptableerrorisreachedornofurtherreductionin
error is achieved. Backpropagation is compute intensive and
traditionallyperformedindatacentersthattakeadvantageof
dedicated GPUs or specialized training accelerators such as
TPUs.
Fine-tuningisdeﬁnedasretrainingapreviouslytrained
model. Itiseasiertorecovertheaccuracyofaquantizedor
pruned model with ﬁne-tuning versus training from scratch.
CNNinferenceclassiﬁcationtakesapreviouslytrained
classiﬁcationmodelandpredictstheclassfrominputdatanot
in the training dataset. Inference is not as computationally
intensive as training and can be executed on edge, mobile,
and embedded devices. The size of the inference network
executing on mobile devices may be limited due to memory,
bandwidth,orprocessingconstraints[ 79]. Pruningdiscussed
in Section 3 and quantization discussed in Section 4 are two
techniques that can alleviate these constraints.
In this paper, we focus on the acceleration of CNN in-
ference classiﬁcation. We compare techniques using stan-
dardbenchmarkssuchasImageNet[ 122],CIFAR[ 132],and
MNIST [139]. The compression techniquesare generaland
the choice of application domain doesn’t restrict its use in
object detection, natural language processing, etc.
2.3. Convolution Operations
ThetopofFigure2showsa3-channelimage(e.g.,RGB)
asinputtoaconvolutionallayer. Becausetheinputimagehas
3channels,theconvolutionkernelmustalsohave3channels.
Inthisﬁgurefour 223convolutionﬁltersareshown,each
consisting of three 2  2kernels. Data is received from all
3 channels simultaneously. 12 image values are multiplied
withthekernelweightsproducingasingleoutput. Thekernel
is moved across the 3-channel image sharing the 12 weights.
T Liang et al.: Preprint submitted to Elsevier Page 3 of 41

--- PAGE 4 ---
Survey on pruning and quantization
Table 1
Acronyms and Abbreviations
Acronym Explanation
2D Two Dimensional
3D Three Dimensional
FP16 16-Bit Floating-Point
FP32 32-Bit Floating-Point
INT16 16-Bit Integer
INT8 8-Bit Integer
IR Intermediate Representation
OFA One-For-All
RGB Red, Green, And Blue
SOTA State of The Art
AI Artiﬁcial Inteligence
BN Batch Normalization
CBN Conditional Batch Normalization
CNN Convolutional Neural Network
DNN Deep Neural Network
EBP Expectation Back Propagation
FCL Fully Connected Layer
FCN Fully Connected Networks
FLOP Floating-Point Operation
GAP Global Average Pooling
GEMM General Matrix Multiply
GFLOP Giga Floating-Point Operation
ILSVRC Imagenet Large Visual Recognition Challenge
Im2col Image To Column
KD Knowledge Distillation
LRN Local Response Normalization
LSTM Long Short Term Memory
MAC Multiply Accumulate
NAS Network Architecture Search
NN Neural Network
PTQ Post Training Quantization
QAT Quantization Aware Training
ReLU Rectiﬁed Linear Unit
RL Reinforcement Learning
RNN Recurrent Neural Network
SGD Stochastic Gradient Descent
STE Straight-Through Estimator
ASIC Application Speciﬁc Integrated Circuit
AVX-512 Advance Vector Extension 512
CPU Central Processing Unit
CU Computing Unit
FPGA Field Programmable Gate Array
GPU Graphic Processing Unit
HSA Heterogeneous System Architecture
ISA Instruction Set Architectures
PE Processing Element
SIMD Single Instruction Multiple Data
SoC System on Chip
DPP Determinantal Point Process
FFT Fast Fourier Transfer
FMA Fused Multiply-Add
KL-divergence Kullback-Leibler Divergence
LASSO Least Absolute Shrinkage And Selection Operator
MDP Markov Decision Process
OLS Ordinary Least Squares
If the input image is 12  12  3 the resulting output will
be11  11  1 (using a stride of 1 and no padding). The
ﬁltersworkbyextractingmultiplesmallerbitmapsknown
as feature maps. If more ﬁlters are desired to learn diﬀerent
features they can be easily added. In this case 4 ﬁlters are
shown resulting in 4 feature maps.
The standardconvolution operation canbe computed in
Depth -wise Convolution  Point -wise ConvolutionSeparable  ConvolutionStandard ConvolutionFigure 2: Separable Convolution: A standard convolution is
decomposed into depth-wise convolution and point-wise convo-
lution to reduce both the model size and computations.
parallel using a GEneral Matrix Multiply (GEMM) library
[60]. Figure 3 shows a parallel column approach. The 3D
tensors are ﬁrst ﬂattened into 2D matrices. The resulting
matrices are multiplied by the convolutional kernel which
takeseachinputneuron(features),multipliesit,andgenerates
output neurons (activations) for the next layer [138].
11
2211
2201
1001
1010
0110
0121
2121
2112
2012
20
12
110
3
02212
110
3
02202
031
2
11002
031
2
11012
010
3
33212
010
3
3321420
15241420
15241224
17261224
1726
12
2011
1302
3103
32
11
1302
2203
2211
1012
2101
13
01
1333
3212
2011
1302
3103
32
11
1302
2203
2211
1012
2101
13
01
1333
3211
10
20
21
12
11
12
11
01
12
12
001412
2024
1517
2426* = * =Output 
Features
Kernels
Input
Features
(Activations)11
1111
11
Figure 3: Convolution Performance Optimization: From tradi-
tional convolution (dot squared) to image to column (im2col) -
GEMM approach, adopted from [ 34]. The red and green boxes
indicate ﬁlter-wise and shape-wise elements, respectively.
Fl+1
n=Al
n= activateTMÉ
m=1 Wl
mn<Fl
m+bl
nU
(1)
Equation1showsthelayer-wisemathematicalrepresenta-
tionoftheconvolutionlayerwhere Wrepresentstheweights
(ﬁlters)ofthetensorwith minputchannelsand noutputchan-
nels,brepresents the bias vector, and Flrepresents the input
feature tensor (typically from the activation of previous layer
Al*1).Alis the activated convolutional output. The goal
of compression is to reduce the size of the WandF(orA)
without aﬀecting accuracy.
T Liang et al.: Preprint submitted to Elsevier Page 4 of 41

--- PAGE 5 ---
Survey on pruning and quantization
Figure 4: Fully Connected Layer: Each node in a layer connects
to all the nodes in the next layer, and every line corresponds to
a weight value
Figure4showsaFCL-alsocalleddenselayerordense
connect. Every neuron is connected to each other neuron
in a crossbar conﬁguration requiring many weights. As an
example, if the input and output channel are 1024 and 1000,
respectively, the number of parameters in the ﬁlter will be
a million by 1024  1000 . As the image size grows or the
number of features increase, the number of weights grows
rapidly.
2.4. Eﬃcient Structure
The bottom of Figure 2 shows separable convolution im-
plemented in MobileNet [ 105]. Separable convolution as-
semblesadepth-wiseconvolutionfollowedbyapoint-wise
convolution. A depth-wise convolution groups the input fea-
ture by channel, and treats each channel as a single input
tensor generating activations with the same number of chan-
nels. Point-wiseconvolutionisastandardconvolutionwith
1  1kernels. It extracts mutual information across the chan-
nelswithminimumcomputationoverhead. Forthe 12123
image previously discussed, a standard convolution needs
2  2  3  4 multiplies to generate 1  1outputs. Separable
convolution needs only 2  2  3 for depth-wise convolution
and1  1  3  4 forpoint-wiseconvolution. Thisreduces
computations by half from 48 to 24. The number of weights
is also reduced from 48 to 24.
1c 3c 5c
1c
3c
5c
3p
Figure 5: Inception Block: The inception block computes
multiple convolutions with one input tensor in parallel, which
extends the receptive ﬁeld by mixing the size of kernels. The
yellow - brown coloured cubes are convolutional kernels sized
1, 3, and 5. The blue cube corresponds to a 3  3pooling
operation.
Thereceptiveﬁeldisthesizeofafeaturemapusedina
convolutionalkernel. Toextractdatawithalargereceptive
ﬁledandhighprecision,cascadedlayersshouldbeappliedasinthetopofFigure5. However,thenumberofcomputa-
tions can be reduced by expanding the network width with
fourtypesofﬁltersasshowninFigure5. Theconcatenated
resultperformsbetterthanoneconvolutionallayerwithsame
computation workloads [226].
………………
………
Figure 6: Conventional Network Block (top), Residual Net-
work Block (middle), and Densely Connected Network Block
(bottom)
Aresidualnetworkarchitectureblock[ 98]isafeedfor-
wardlayerwithashortcircuitbetweenlayersasshowninthe
middleofFigure6. Theshortcircuitkeepsinformationfrom
the previous block to increase accuracy and avoid vanish-
inggradientsduringtraining. Residualnetworkshelpdeep
networks grow in depth by directly transferring information
between deeper and shallower layers.
The bottom of Figure 6 shows the densely connected
convolutionalblockfromDenseNets[ 109],thisblockextends
both the network depth and the receptive ﬁeld by delivering
thefeatureofformerlayerstoallthelaterlayersinadense
blockusingconcatenation. ResNetstransferoutputsfroma
single previous layer. DenseNets build connections across
layers to fully utilize previous features. This provides weight
eﬃciencies.
2.5. Batch Normalization
BNwasintroducedin2015tospeedupthetrainingphase,
and to improve the neural network performance [ 119]. Most
SOTA neural networks apply BN after a convolutional layer.
BNaddressesinternalcovariateshift(analteringofthenet-
work activation distribution caused by modiﬁcations to pa-
rametersduringtraining)bynormalizinglayerinputs. This
has been shown to reduce training time up to 14. Santurkar
[210] argues that the eﬃciency of BN is from its ability to
smooth values during optimization.
y=x*ù
2++(2)
Equation 2 gives the formula for computing inference
BN, where xandyare the input feature and the output of
BN,andare learned parameters, andare the mean
value and standard deviation calculated from the training set,
andis the additional small value (e.g., 1e-6) to prevent the
denominatorfrombeing0. ThevariablesofEquation2are
determinedinthetrainingpassandintegratedintothetrained
T Liang et al.: Preprint submitted to Elsevier Page 5 of 41

--- PAGE 6 ---
Survey on pruning and quantization
weights. Ifthefeaturesinonechannelsharethesameparame-
ters,thenitturnstoalineartransformoneachoutputchannel.
Channel-wise BN parameters potentially helps channel-wise
pruning. BN could also raise the performance of the cluster-
based quantize technique by reducing parameter dependency
[48].
SincetheparametersoftheBNoperationarenotmodiﬁed
intheinferencephase,theymaybecombinedwiththetrained
weightsandbiases. ThisiscalledBNfoldingorBNmerging.
Equation 3 show an example of BN folding. The new weight
W¨andbias b¨arecalculatedusingthepretrainedweights W
andBNparametersfromEquation2. Sincethenewweight
is computed after training and prior to inference, the number
of multipliesare reducedand thereforeBN folding decreases
inference latency and computational complexity.
W¨=Wù
2+;b¨=b*ù
2++(3)
2.6. Pooling
Poolingwasﬁrstpublishedinthe1980swithneocogni-
tron [71]. The technique takes a group of values and reduces
them to a single value. The selection of the single replace-
ment value can be computed as an average of the values
(average pooling) or simply selecting the maximum value
(max pooling).
Pooling destroys spatial information as it is a form of
down-sampling. Thewindowsizedeﬁnestheareaofvaluesto
bepooled. Forimageprocessingitisusuallyasquarewindow
withtypicalsizesbeing 2  2,3  3or4  4. Smallwindows
allow enough information to be propagated to successive
layerswhilereducingthetotalnumberofcomputations[ 224].
Global pooling is a technique where, instead of reducing
aneighborhoodofvalues,anentirefeaturemapisreducedto
asinglevalue[ 154]. GlobalAveragePooling(GAP)extracts
informationfrommulti-channelfeaturesandcanbeusedwith
dynamic pruning [153, 42].
Capsulestructureshavebeenproposedasanalternative
to pooling. Capsule networks replace the scalar neuron with
vectors. The vectors represent a speciﬁc entity with more
detailedinformation,suchaspositionandsizeofanobject.
Capsule networks void loss of spatial information by captur-
ing it in the vector representation. Rather than reducing a
neighborhoodofvaluestoasinglevalue,capsulenetworks
perform a dynamic routing algorithm to remove connections
[209].
2.7. Parameters
Figure 7 show top-1 accuracy percent verses the number
ofoperationsneededforanumberofpopularneuralnetworks
[23]. The number of parameters in each network is repre-
sented by the size of the circle. A trend (not shown in the
ﬁgure) is a yearly increase in parameter complexity. In 2012,
AlexNet [ 133] was published with 60 million parameters. In
2013,VGG[ 217]wasintroducedwith133millionparameters
andachieved71.1%top-1accuracy. Thesewerepartofthe
ImageNet large scale visual recognition challenge (ILSVRC)
[207]. Thecompetition’smetricwastop-1absoluteaccuracy.
Figure 7: Popular CNN Models: Top-1 accuracy vs GFLOPs
and model size, adopted from [23]
Execution time was not a factor. This incentivized neural
network designs with signiﬁcant redundancy. As of 2020,
models with more than 175 billion parameters have been
published [26].
Networksthatexecuteindatacenterscanaccommodate
modelswith alargenumber ofparameters. Inresource con-
strained environments such as edge and mobile deployments,
reduced parameter models have been designed. For exam-
ple, GoogLeNet [ 226] achieves similar top-1 accuracy of
69.78% as VGG-16 but with only 7 million parameters. Mo-
bileNet [105] has70% top-1accuracy with only4.2 million
parameters and only 1.14 Giga FLoating-point OPerations
(GFLOPs). A more detailed network comparison can be
found in [5].
3. Pruning
Networkpruningisanimportanttechniqueforbothmem-
orysizeandbandwidthreduction. Intheearly1990s,pruning
techniques were developed to reduce a trained large network
into a smaller network without requiring retraining [ 201].
Thisallowedneuralnetworkstobedeployedinconstrained
environmentssuchasembeddedsystems. Pruningremoves
redundant parameters or neurons that do not signiﬁcantly
contribute to the accuracy of results. This condition may
arise when the weight coeﬃcients are zero, close to zero,
or are replicated. Pruning consequently reduces the com-
putational complexity. If pruned networks are retrained it
provides the possibility of escaping a previous local minima
[43] and further improve accuracy.
Researchonnetworkpruningcanroughlybecategorized
as sensitivity calculation and penalty-term methods [ 201].
Signiﬁcant recent research interest has continued showing
improvementsforbothnetworkpruningcategoriesorafur-
T Liang et al.: Preprint submitted to Elsevier Page 6 of 41

--- PAGE 7 ---
Survey on pruning and quantization
Network Model Target  LocatingNetwork
PruningTraining/Tuning
Network ModelPruning
StrategyDecision
ComponetsRuntime PruningStatic Pruning
Dynamic Pruning
Figure 8: Pruning Categories: Static pruning is performed oﬄine prior to inference while Dynamic pruning is performed at runtime.
ther combination of them.
Recently,newnetworkpruningtechniqueshavebeencre-
ated. Modernpruningtechniquesmaybeclassiﬁedbyvarious
aspectsincluding: 1) structured andunstructured pruning de-
pendingiftheprunednetworkissymmetricornot,2) neuron
andconnection pruning depending on the pruned element
type,or3) staticanddynamic pruning . Figure8showsthe
processingdiﬀerencesbetweenstaticanddynamicpruning.
Static pruning hasallpruningstepsperformedoﬄineprior
to inference while dynamic pruning is performed during run-
time. Whilethereisoverlapbetweenthecategories,inthis
paper we will use static pruning anddynamic pruning for
classiﬁcation of network pruning techniques.
Figure 9 shows a granularity of pruning opportunities.
Thefourrectanglesontherightsidecorrespondtothefour
brown ﬁlters in the top of Figure 2. Pruning can occur
onanelement-by-element,row-by-row,column-by-column,
ﬁlter-by-ﬁlter, or layer-by-layer basis. Typically element-by-
element has the smallest sparsity impact, and results in a
unstructured model. Sparsity decreases from left-to-right in
Figure 9.
channel -wise element -wise filter -wise shape -wise layer -wise
Figure 9: Pruning Opportunities: Diﬀerent network sparsity
results from the granularity of pruned structures. Shape-wise
pruning was proposed by Wen [241].
arg min
pL=N.x;W/ *Np.x;Wp/
whereNp.x;Wp/ =P.N.x;W//(4)
Independent of categorization, pruning can be described
mathematicallyasEquation4. Nrepresentstheentireneuralnetworkwhichcontainsaseriesoflayers(e.g.,convolutional
layer, pooling layer, etc.) with xas input.Lrepresents the
prunednetworkwith Npperformancelosscomparedtothe
unprunednetwork. Networkperformanceistypicallydeﬁned
as accuracy in classiﬁcation. The pruning function, P./,
results in a diﬀerent network conﬁguration Npalong with
theprunedweights Wp. Thefollowingsectionsareprimarily
concernedwiththeinﬂuenceof P./onNp. Wealsoconsider
how to obtain Wp.
3.1. Static Pruning
Staticpruningisanetworkoptimizationtechniquethat
removes neurons oﬄine from the network aftertraining and
before inference. During inference, no additional pruning
ofthenetworkisperformed. Staticpruningcommonlyhas
threeparts: 1)selectionofparameterstoprune,2)themethod
of pruning the neurons, and 3) optionally ﬁne-tuning or re-
training [ 92]. Retraining may improve the performance of
the pruned network to achieve comparable accuracy to the
unpruned network but may require signiﬁcant oﬄine compu-
tation time and energy.
3.1.1. Pruning Criteria
As a result of network redundancy, neurons or connec-
tionscanoftenberemovedwithoutsigniﬁcantlossofaccu-
racy. AsshowninEquation1,thecoreoperationofanetwork
is a convolution operation. It involves three parts: 1) input
features as produced by the previous layer, 2) weights pro-
ducedfromthetrainingphase,and3)biasvaluesproduced
from the training phase. The output of the convolution op-
erationmayresultineitherzerovaluedweightsorfeatures
that lead to a zero output. Another possibility is that similar
weights or features may be produced. These may be merged
for distributive convolutions.
Anearlymethodtoprunenetworksisbrute-forcepruning.
Inthismethodtheentirenetworkistraversedelement-wise
andweightsthatdonotaﬀectaccuracyareremoved. Adisad-
vantageofthisapproachisthelargesolutionspacetotraverse.
A typical metric to determine which values to prune is given
bythelp-norm, s.t.pË ^N;Ø`,whereNisnaturalnumber.
Thelp-norm of a vector xwhich consists of nelements is
T Liang et al.: Preprint submitted to Elsevier Page 7 of 41

--- PAGE 8 ---
Survey on pruning and quantization
mathematically described by Equation 5.
ñxñp=HnÉ
i=1óóxióópI1
p
(5)
Among the widely applied measurements, the l1-norm
is also known as the Manhattan norm and thel2-norm is
also known as the Euclidean norm . The corresponding l1
andl2regularization have the names LASSO(least absolute
shrinkage and selection operator) and Ridge, respectively
[230]. The diﬀerence between the l2-norm pruned tensor
and an unpruned tensor is called the l2-distance. Sometimes
researchers also use the term l0-norm deﬁned as the total
number of nonzero elements in a vector.
arg min
;h
n
l
njNÉ
i=1H
yi**pÉ
j=1jxijI2i
n
m
nk
subject topÉ
jóóójóóóÍt(6)
EquationEquation6mathematicallydescribes l2LASSO
regularization. Considerasampleconsistingof Ncases,each
of which consists of pcovariates and a single outcome yi.
Letxi= .xi1;:::;xip/Tbe the standardized covariate vec-
tor for thei-th case (input feature in DNNs), so we have³
ixij_N= 0;³
ix2
ij_N= 1.represents the coeﬃcients
= .1;:::;p/T(weights) and tis apredeﬁned tunningpa-
rameter that determines the sparsity. The LASSO estimate 
is 0 when the average of yiis 0 because for all t, the solution
foris=y. Ifthe constraint is³p
j2
jÍtthenthe Equa-
tion6becomesRidgeregression. Removingtheconstraint
will results in the Ordinary Least Squares (OLS) solution.
argmin
ËR$1
Nñy*Xñ2
2+ññ1%
(7)
Equation6canbesimpliﬁedintotheso-calledLagrangian
form shownin Equation7. TheLagrangianmultiplier trans-
latestheobjectivefunction f.x/andconstraint g.x/ = 0into
theformatof L.x;/ =f.x/ *g.x/,Wherethe ññpisthe
standardlp-norm, theXis the covariate matrix that contains
xij,andisthedatadependentparameterrelatedto tfrom
Equation 6.
Bothmagnitude-basedpruningandpenaltybasedpruning
may generate zero values or near-zero values for the weights.
In this section we discuss both methods and their impact.
Magnitude-based pruning: It has been proposed and is
widely accepted that trained weights with large values are
more important than trained weights with smaller values
[143]. This observation is the key to magnitude-based meth-
ods. Magnitude-basedpruningmethodsseektoidentifyun-
neededweightsorfeaturestoremovethemfromruntimeeval-
uation. Unneeded values may be pruned either in the kernelorattheactivationmap. Themostintuitivemagnitude-based
pruning methods is to prune all zero-valued weights or all
weights within an absolute value threshold.
LeCunasfarbackas1990proposedOptimalBrainDam-
age(OBD)toprunesinglenon-essentialweights[ 140]. By
usingthesecondderivative(Hessianmatrix)ofthelossfunc-
tion, this static pruning technique reduced network param-
etersbyaquarter. Forasimpliﬁedderivativecomputation,
OBDfunctionsunderthreeassumptions: 1) quadratic -the
costfunctionisnear-quadratic,2) extremal-thepruningis
done after the network converged, and 3) diagonal - sums
up the error of individual weights by pruning the result of
the error caused by their co-consequence. This research also
suggestedthatthesparsityofDNNscouldprovideopportuni-
ties to accelerate network performance. Later Optimal Brain
Surgeon (OBS) [ 97] extended OBD with a similar second-
order method but removed the diagonal assumption in OBD.
OBS considers the Hessian matrix is usually non-diagonal
for most applications. OBS improved the neuron removal
precision with up to a 90% reduction in weights for XOR
networks.
These early methods reduced the number of connections
based on the second derivative of the loss function. The
trainingproceduredidnotconsiderfuturepruningbutstillre-
sulted in networks that were amenable to pruning. They also
suggested that methods based on Hessian pruning would ex-
hibithigheraccuracythanthoseprunedwithonlymagnitude-
based algorithms [ 97]. More recent DNNs exhibit larger
weightvalueswhencomparedtoearlyDNNs. EarlyDNNs
were also much shallower with orders of magnitude less neu-
rons. GPT-3[ 26],forexample,contains175-billionparam-
eterswhileVGG-16[ 217]containsjust133-millionparam-
eters. Calculating the Hessian matrix during training for
networks with the complexity of GPT-3 is not currently fea-
sible as it has the complexity of O.W2/. Because of this
simpler magnitude-based algorithms have been developed
[177, 141].
Filter-wise pruning [ 147] uses thel1-norm to remove
ﬁlters that do not aﬀect the accuracy of the classiﬁcation.
Pruningentireﬁltersandtheirrelatedfeaturemapsresulted
in a reduced inference cost of 34% for VGG-16 and 38% for
ResNet-110ontheCIFAR-10datasetwithimprovedaccuracy
0.75% and 0.02%, respectively.
Mostnetworkpruningmethodschoosetomeasureweights
rather than activations when rating the eﬀectiveness of prun-
ing [88]. However, activations may also be an indicator to
prune corresponding weights. Average Percentage Of Zeros
(APoZ) [106] was introduced to judge if one output activa-
tion map is contributing to the result. Certain activation
functions,particularlyrectiﬁcationsuchasRectiﬁedLinear
Unit (ReLU), may result in a high percentage of zeros in
activations and thus be amenable to pruning. Equation 8
shows the deﬁnition of APoZ.i/
cof thec-th neuron in the i-th
layer, where O.i/
cdenotesthe activation, Nisthenumberof
calibration(validation)images,and Misthedimensionof
T Liang et al.: Preprint submitted to Elsevier Page 8 of 41

--- PAGE 9 ---
Survey on pruning and quantization
activation map. f.true/ = 1andf.false/ = 0.
APoZ.i/
c= APoZ O.i/
c=N³
k=0M³
j=0f
O.i/
c;j.k/ = 0
NM(8)
Similarly, inbound pruning [ 195], also an activation tech-
nique, considers channels that do not contribute to the result.
Ifthetopactivationchannelinthestandardconvolutionof
Figure 2 are determined to be less contributing, the corre-
sponding channel of the ﬁlter in the bottom of the ﬁgure will
be removed. After pruning this technique achieved about
1:5compression.
Filter-wise pruning using a threshold from the sum of
ﬁlters’ absolute values can directly take advantage of the
structureinthenetwork. Inthisway,theratioofprunedto
unpruned neurons (i.e. the pruning ratio) is positively cor-
related to the percentage of kernel weights with zero values,
which can be further improved by penalty-based methods.
Penalty-based pruning: Inpenalty-basedpruning,thegoal
istomodifyanerrorfunctionoraddotherconstraints,known
asbiasterms,inthetrainingprocess. Apenaltyvalueisused
to update some weights to zero or near zero values. These
values are then pruned.
Hanson [ 96] explored hyperbolic and exponential bias
termsforpruninginthelate80s. Thismethodusesweight
decay in backpropagation to determine if a neuron should be
pruned. Low-valued weights are replaced by zeros. Residual
zero valued weights after training are then used to prune
unneeded neurons.
Feature selection [ 55] is a technique that selects a subset
of relevant features that contribute to the result. It is also
knownasattributeselectionorvariableselection. Featurese-
lectionhelpsalgorithmsavoidingover-ﬁttingandaccelerates
both training and inference by removing features and/or con-
nectionsthatdon’tcontributetotheresults. Featureselection
also aids model understanding by simplifying them to the
mostimportantfeatures. PruninginDNNscanbeconsidered
to be a kind of feature selection [123].
LASSO was previously introduced as a penalty term.
LASSO shrinks the least absolute valued feature’s corre-
sponding weights. This increases weight sparsity. This op-
eration is also referred to as LASSO feature selection and
has been shown to perform better than traditional procedures
suchasOLSbyselectingthemostsigniﬁcantlycontributed
variablesinsteadofusingallthevariables. Thisleadtoap-
proximately 60% more sparsity than OLS [181].
Element-wise pruning may result in an unstructured net-
workorganizations. Thisleadstosparseweightmatricesthat
are not eﬃciently executed on instruction set processors. In
additiontheyareusuallyhardtocompressoracceleratewith-
out specialized hardware support [ 91]. Group LASSO [ 260]
mitigatestheseineﬃcienciesbyusingastructuredpruning
method that removes entire groups of neurons while main-
taining structure in the network organization [17].
GroupLASSOisdesignedtoensurethatallthevariables
sorted into one group could be either included or excluded
	  	  	  	  	  	  	  	  shortcut 
depth-wise 	  	  	  	  	  	  	  	  	  	  	  	  filter-wise channel-wise 	  	  	  	  
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	   …	  	  	   	  	  	  	  shape-wise W(l)nl,:,:,:(1)W(l):,cl,:,:(2)W(l):,cl,ml,kl(3)W(l)(4)
1W(l)nl,:,:,:(1)W(l):,cl,:,:(2)W(l):,cl,ml,kl(3)W(l)(4)
1W(l)nl,:,:,:(1)W(l):,cl,:,:(2)W(l):,cl,ml,kl(3)W(l)(4)
1W(l)nl,:,:,:(1)W(l):,cl,:,:(2)W(l):,cl,ml,kl(3)W(l)(4)
1Figure 10: Types of Sparsity Geometry, adopted from [241]
as a whole. Equation 9 gives the pruning constraint where X
andinEquation7arereplacedbythehigherdimensional
Xjandjfor thejgroups.
argmin
ËRph
n
l
njôôôôôôy*JÉ
j=1Xjjôôôôôô2
2+JÉ
j=1ôôôjôôôKji
n
m
nk(9)
Figure 10shows GroupLASSO withgroupshapes used
in Structured Sparsity Learning (SSL) [ 241]. Weights are
splitintomultiplegroups. Unneededgroupsofweightsare
removed using LASSO feature selection. Groups may be
determined based on geometry, computational complexity,
groupsparsity,etc. SSLdescribesanexamplewheregroup
sparsity in row and column directions may be used to reduce
the execution time of GEMM. SSL has shown improved
inference times on AlexNet with both CPUs and GPUs by
5:1and3:1, respectively.
Group-wisebraindamage[ 136]alsointroducedthegroup
LASSO constraint but applied it to ﬁlters. This simulates
braindamageandintroducessparsity. Itachieved 2speedup
with0.7%ILSVRC-2012accuracylossontheVGGNetwork.
SparseConvolutionalNeuralNetworks(SCNN)[ 17]take
advantage of two-stage tensor decomposition. By decompos-
ing the input feature map and convolutional kernels, the ten-
sors are transformed into two tensor multiplications. Group
LASSO is then applied. SCNN also proposed a hardware
friendlyalgorithmtofurtheracceleratesparsematrixcompu-
tations. Theyachieved 2:47to6:88speed-uponvarious
types of convolution.
Networkslimming[ 158]appliesLASSOonthescaling
factors of BN. BN normalizes the activation by statistical
parameterswhichareobtainedduringthetrainingphase. Net-
workslimminghastheeﬀectofintroducingforwardinvisible
additionalparameterswithoutadditionaloverhead. Speciﬁ-
cally,bysettingtheBNscalerparametertozero,channel-wise
pruningisenabled. Theyachieved82.5%sizereductionwith
VGG and 30.4% computation compression without loss of
accuracy on ILSVRC-2012.
Sparse structure selection [ 111] is a generalized network
slimming method. It prunes by applying LASSO to sparse
scaling factors in neurons, groups, or residual blocks. Using
animprovedgradientmethod,AcceleratedProximalGradi-
ent(APG),theproposedmethodshowsbetterperformance
withoutﬁne-tunningachieving 4speed-uponVGG-16with
3.93% ILSVRC-2012 top-1 accuracy loss.
T Liang et al.: Preprint submitted to Elsevier Page 9 of 41

--- PAGE 10 ---
Survey on pruning and quantization
Dropout: While not speciﬁcally a technique to prune net-
works, dropout does reduce the number of parameters [ 222].
It was originally designed as a stochastic regularizer to avoid
over-ﬁtting of data [ 103]. The technique randomly omits a
percentage of neurons typically up to 50%, This dropoutop-
erationbreaksoﬀpartoftheconnectionsbetweenneuronsto
avoidco-adaptations. Dropoutcouldalsoberegardedasan
operationthatseparatelytrainsmanysub-networksandtakes
the averageof themduring theinference phase. Dropout in-
creases training overhead but it does not aﬀect the inference
time.
Sparsevariationaldropout[ 176]addedadropouthyper-
parameter called the dropout rate to reduce the weights of
VGG-likenetworksby 68. Duringtrainingthedropoutrate
canbeusedtoidentifysingleweightstoprune. Thiscanalso
be applied with other compression approaches for further
reduction in weights.
Redundancies: Thegoalofnorm-basedpruningalgorithms
istoremovezeros. Thisimpliesthatthedistributionofvalues
shouldwideenoughtoretainsomevaluesbutcontainenough
values close to zero such that a smaller network organization
isstillaccurate. Thisdoesnotholdinsomecircumstances.
Forexample,ﬁltersthathavesmallnormdeviationsoralarge
minimumnormhave smallsearchspacesmakingitdiﬃcult
toprunebasedonathreshold[ 100]. Evenwhenparameter
values are wide enough, in some networks smaller values
may still play an important role in producing results. One
exampleofthisiswhenlargevaluedparameterssaturate[ 64].
Inthesecasesmagnitude-basedpruningofzerovaluesmay
decrease result accuracy.
Similarly,penalty-basedpruningmaycausenetworkac-
curacy loss. In this case, the ﬁlters identiﬁed as unneeded
due to similarcoeﬃcient values in other ﬁlters mayactually
be required. Removing them may signiﬁcantly decrease net-
work accuracy [ 88]. Section 3.1.2 describes techniques to
undopruningbytuningtheweightstominimizenetworkloss
while this section describes redundancy based pruning.
Using BN parameters, feature map channel distances can
be computed by layer [ 266]. Using a clustering approach
for distance, nearby features can be tuned. An advantage
of clustering is that redundancy is not measured with an
absolute distance but a relative value. With about 60 epochs
of training they were able to prune the network resulting
ina50%reductioninFLOPs(includingnon-convolutional
operations) with a reduction in accuracy of only 1% for both
top-1 and top-5 on the ImageNet dataset.
Filter pruning via geometric median (FPGM) [ 100] iden-
tiﬁesﬁlterstoprunebymeasuringthe l2-distanceusingthe
geometric median. FPGM found 42% FLOPs reduction with
0.05%top-1accuracydroponILSVRC-2012withResNet-
101.
The reduce and reused (also described as outbound)
method [195] prunes entire ﬁlters by computing the statis-
tical variance of each ﬁlter’s output using a calibration set.
Filters with low variance are pruned. The outbound method
obtained 2:37acceleration with 1.52% accuracy loss onLabeledFacesintheWild(LFW)dataset[ 110]intheﬁled
of face recognition.
A method that iteratively removes redundant neurons for
FCLswithoutrequiringspecialvalidationdataisproposed
in [221]. This approach measures the similarity of weight
groupsafter anormalization. Itremoves redundantweights
and merges the weights into a single value. This lead to a
34.89% reduction of FCL weights on AlexNet with 2.24%
top-1 accuracy loss on ILSVRC-2012.
Comparingwiththesimilaritybasedapproachabove,DI-
VersityNETworks(DIVNET)[ 167]considersthecalculation
redundancy based on the activations. DIVNET introduces
DeterminantalPointProcess(DPP)[ 166]asapruningtool.
DPP sorts neurons into categories including dropped and
retained. Instead of forcing the removal of elements with
lowcontributionfactors,theyfusetheneuronsbyaprocess
named re-weighting. Re-weighting works by minimizing the
impactofneuronremoval. Thisminimizespruninginﬂuence
and mitigates network information loss. They found 3% loss
on CIFAR-10 dataset when compressing the network into
half weight.
ThiNet [164] adopts statistics information from the next
layer to determine the importance of ﬁlters. Ituses a greedy
search to prune the channel that has the smallest reconstruc-
tion cost in the next layer. ThiNet prunes layer-by-layer in-
stead of globally to minimize large errors in classiﬁcation
accuracy. It also prunes less during each training epoch to
allow for coeﬃcient stability. The pruning ratio is a prede-
ﬁned hyper-parameter and the runtime complexity is directly
relatedtothepruningratio. ThiNetcompressedResNet-50
FLOPs to 44.17% with a top-1 accuracy reduction of 1.87%.
He[101]adoptsLASSOregressioninsteadofagreedy
algorithm to estimate the channels. Speciﬁcally, in one itera-
tion,theﬁrststepistoevaluatethemostimportantchannel
using thel1-norm. The next step is to prune the correspond-
ing channel that has the smallest Mean Square Error (MSE).
Comparedtoanunprunednetwork,thisapproachobtained
2accelerationofResNet-50onILSVRC-2012withabout
1.4%accuracylossontop-5,anda 4reductioninexecution
timewithtop-5accuracylossof1.0%forVGG-16. Theau-
thors categorize their approach as dynamic inference-time
channel pruning. However it requires 5000 images for cal-
ibration with 10 samples per image and more importantly
resultsinastaticallyprunednetwork. Thuswehaveplaced
it under static pruning.
3.1.2. Pruning combined with Tuning or Retraining
Pruningremovesnetworkredundanciesandhasthebene-
ﬁtofreducingthenumberofcomputationswithoutsigniﬁcant
impactonaccuracyforsomenetworkarchitectures. However,
astheestimationcriterionisnotalwaysaccurate,someim-
portantelementsmaybeeliminatedresultinginadecreasein
accuracy. Becauseofthelossofaccuracy,time-consuming
ﬁne-tuning or re-training may be employed to increase accu-
racy [258].
Deep compression [ 92], for example, describes a static
methodtopruneconnectionsthatdon’tcontributetoclassi-
T Liang et al.: Preprint submitted to Elsevier Page 10 of 41

--- PAGE 11 ---
Survey on pruning and quantization
ﬁcation accuracy. In addition to feature map pruning they
alsoremoveweightswithsmallvalues. Afterpruningthey
re-train the network to improve accuracy. This process is
performed iteratively three times resulting in a 9to13
reduction in total parameters with no loss of accuracy. Most
of the removed parameters were from FCLs.
Recoverable Pruning: Prunedelementsusuallycannotbe
recovered. This may result in reduced network capability.
Recovering lost network capability requires signiﬁcant re-
training. Deep compression required millions of iterations to
retrainthenetwork[ 92]. Toavoidthisshortcoming,manyap-
proaches adopt recoverable pruning algorithms. The pruned
elements may also be involved in the subsequent training
process and adjust themselves to ﬁt the pruned network.
Guo[88]describesarecoverablepruningmethodusing
binary mask matrices to indicate whether a single weight
valueisprunedornot. The l1-normprunedweightscanbe
stochasticallysplicedbackintothenetwork. Usingthisap-
proach AlexNet was able to be reduced by a factor of 17:7
with no accuracy loss. Re-training iterations were signiﬁ-
cantlyreducedto14.58%ofDeepcompression[ 92]. How-
everthistypeofpruningstillresultsinanasymmetricnetwork
complicating hardware implementation.
Soft Filter Pruning (SFP) [ 99] further extended recov-
erable pruning using a dimension of ﬁlter. SFP obtained
structuredcompressionresultswithanadditionalbeneﬁtor
reduced inference time. Furthermore, SFP can be used on
diﬃculttocompressnetworksachievinga29.8%speed-up
onResNet-50with1.54%ILSVRC-2012top-1accuracyloss.
Comparing with Guo’s recoverable weight [ 88] technique,
SFP achieves inference speed-ups closer to theoretical re-
sultsongeneralpurposehardwarebytakingadvantageofthe
structure of the ﬁlter.
Increasing Sparsity: Anothermotivationtoapplyﬁne-tuning
istoincreasenetworksparsity. Sparseconstraints[ 270]ap-
plied low rank tensor constraints [ 157] and group sparsity
[57]achievinga70%reductionofneuronswitha0.57%drop
of AlexNet in ILSVRC-2012 top-1 accuracy.
Adaptive Sparsity: Nomatterwhatkindofpruningcriteria
isapplied,alayer-wisepruningratiousuallyrequiresahuman
decision. Toohigharatioresultinginveryhighsparsitymay
cause the network to diverge requiring heavy re-tuning.
Networkslimming[ 158],previouslydiscussed,addresses
this problem by automatically computing layer-wise sparsity.
This achieved a 20model size compression, 5computing
reduction, and less than 0.1% accuracy loss on the VGG
network.
Pruningcanalsobeperformedusingamin-maxoptimiza-
tion module [ 218] that maintains network accuracy during
tuningbykeepingapruningratio. Thistechniquecompressed
the VGG network by a factor of 17:5and resulted in atheo-
retical execution time (FLOPs) of 15:56~of the unpruned
network. Asimilarapproachwasproposedwithanestima-
tion of weights sets [ 33]. By avoiding the use of a greedy
search to keep the best pruning ratio, they achieved the sameResNetclassiﬁcationaccuracywithonly5%to10%sizeof
original weights.
AutoPruner [ 163] integrated the pruning and ﬁne-tuning
of a three-stagepipeline as an independenttraining-friendly
layer. The layer helped gradually prune during training even-
tuallyresultinginalesscomplexnetwork. AutoPrunerpruned
73.59%ofcomputeoperationsonVGG-16with2.39%ILSVRC-
2012 top-1 loss. ResNet-50 resulted in a 65.80% of compute
operations with 3.10% loss of accuracy.
Training from Scratch: Observation shows that network
training eﬃciency and accuracy is inversely proportional
to structure sparsity. The more dense the network, the less
training time [ 94,147,70]. This is one reason that current
pruningtechniquestendtofollowatrain-prune-tunepipeline
rather than training a pruned structure from scratch.
However,thelotterytickethypothesis[ 70]showsthatitis
notofprimaryimportancetopreservetheoriginalweightsbut
the initialization. Experiments show that dense, randomly-
initialized pruned sub-networks can be trained eﬀectively
and reach comparable accuracy to the original network with
the same number of training iterations. Furthermore, stan-
dard pruning techniques can uncover the aforementioned
sub-networksfromalargeoversizednetwork-the Winning
Tickets. In contrast with current static pruning techniques,
thelotterytickethypothesisafteraperiodoftimedropsall
well-trained weights and resets them to an initial random
state. This technique found that ResNet-18 could maintain
comparableperformancewithapruningratioupto88.2%on
the CIFAR-10 dataset.
Towards Better Accuracy: Byreducingthenumberofnet-
work parameters, pruning techniques can also help to reduce
over-ﬁtting. Dense-Sparse-Dense (DSD) training [ 93] helps
various networkimproveclassiﬁcation accuracy by1.1% to
4.3%. DSDusesathreestagepipeline: 1)densetrainingto
identifyimportantconnections,2)pruneinsigniﬁcantweights
andsparsetrainingwithasparsityconstrainttotakereduce
the number of parameters, and 3) re-dense the structure to
recovertheoriginalsymmetricstructure,thisalsoincrease
themodelcapacity. TheDSDapproachhasalsoshownim-
pressiveperformanceontheothertypeofdeepnetworkssuch
asRecurrentNeuralNetworks(RNNs)andLongShortTerm
Memory networks (LSTMs).
3.2. Dynamic Pruning
Except for recoverable techniques, static pruning perma-
nentlydestroystheoriginalnetworkstructurewhichmaylead
toadecreaseinmodelcapability. Techniqueshavebeenre-
searchedtorecoverlostnetworkcapabilitiesbutoncepruned
andre-trained,thestaticpruningapproachcan’trecoverde-
stroyedinformation. Additionally,observationsshowsthat
theimportance ofneuronbinding isinput-independent [ 73].
Dynamic pruning determines at runtime which layers,
channels, or neurons will not participate in further activity.
Dynamic pruning can overcome limitations of static prun-
ing by taking advantage of changing input data potentially
reducingcomputation,bandwidth,andpowerdissipation. Dy-
T Liang et al.: Preprint submitted to Elsevier Page 11 of 41

--- PAGE 12 ---
Survey on pruning and quantization
Exit ExitNetwork A Network B Network C
ExitAdditional Connections 
attached to Network ADecision Components
Input
Image(s)Pruning 
Decision
Exit1.Additional connections or side networks?
2.Layer -wise pruning or channel -wise?
3.One -shot information input or layer -wise?
4.How to calculate the score?
5.Predefined thresholds or dynamical?
6.Continue, skip or exit computing?
7.How to train the decision components?
Cascade NetworkSide 
Network
Exit12
345
67
6
Network Data
Decision Data
Network InfoNetwork Data
Decision Data
Network Info
... ...
Figure 11: Dynamic Pruning System Considerations
namic pruning typically doesn’t perform runtime ﬁne-tuning
orre-training. InFigure11,weshowanoverviewofdynamic
pruningsystems. Themostimportantconsiderationisthede-
cision system that decides what to prune. The related issues
are:
1.The type of the decision components: a) additional
connections attached to the original network used dur-
ing the inference phase and/or the training phase, b)
characteristicsoftheconnectionsthatcanbelearned
by standard backpropagation algorithms [ 73], and c) a
side decision network which tends to perform well but
is often diﬃcult to train [153].
2.The pruning level (shape): a) channel-wise [ 153,73,
42], b) layer-wise [ 145], c) block-wise [ 246], or d)
network-wise [ 25]. The pruning level chosen inﬂu-
ences hardware design.
3.Inputdata: a)one-shotinformationfeeding[ 246]feeds
the entire input to the decision system, and b) layer-
wise information feeding [ 25,68] where a window of
dataisiterativelyfedtothedecisionsystemalongwith
the forwarding.
4.Computing a decision score: lp-norm [73], or b) other
approaches [108].
5.Scorecomparison: a)humanexperience/experiment
results [145] or b) automatic threshold or dynamic
mechanisms [108].
6.Stopping criteria: a) in the case of layer-wise and
network-wisepruning,somepruningalgorithmsskip
theprunedlayer/network[ 19,246],b)somealgorithms
dynamically choose the data path [ 189,259], and c)endingthecomputationandoutputingthepredicting
results[68,145,148]. Inthiscasetheremaininglayers
are considered to be pruned.
7.Training the decision component: a) attached con-
nections can be trained along with the original net-
work [145,148,73], b) side networks are typically
trainedusingreinforcementlearning(RL)algorithms
[19, 153, 189, 246].
For instruction set processors, feature maps or the number
of ﬁlters used to identify objects is a large portion of band-
widthusage[ 225]-especiallyfordepth-wiseorpoint-wise
convolutions where features consume a larger portion of the
bandwidth [ 47]. Dynamic tuning may also be applied to stat-
ically pruned networks potentially further reducing compute
and bandwidth requirements.
A drawback of dynamic pruning is that the criteria to
determinewhichelementstoprunemustbecomputedatrun-
time. This adds overhead to the system requiring additional
compute, bandwidth, and power. A trade-oﬀ between dy-
namic pruning overhead, reduced network computation, and
accuracy loss, should be considered. One method to miti-
gatepowerconsumptioninhibitscomputationsfrom0-valued
parameters within a Processing Element (PE) [153].
3.2.1. Conditional Computing
Conditional computing involves activating an optimal
part of a network without activating the entire network. Non-
activated neurons are considered to be pruned. They do
notparticipateintheresulttherebyreducingthenumberof
computations required. Conditional computing applies to
T Liang et al.: Preprint submitted to Elsevier Page 12 of 41

--- PAGE 13 ---
Survey on pruning and quantization
training and inference [20, 56].
ConditionalcomputinghasasimilaritywithRLinthat
they both learn a pattern to achieve a reward. Bengio [ 19]
splitthenetworkintoseveralblocksandformulatestheblock
chosen policies as an RL problem. This approach consists
of only fully connected neural networks and achieved a 5:3
speed-up on CIFAR-10 dataset without loss of accuracy.
3.2.2. Reinforcement Learning Adaptive Networks
Adaptive networks aim to accelerating network inference
by conditionally determining early exits. A trade-oﬀ be-
tween network accuracy and computation can be applied
using thresholds. Adaptive networks have multiple interme-
diate classiﬁers to provide the ability of an early exit. A
cascade network is atype ofadaptive network. Cascade net-
worksarethecombinationsofserialnetworkswhichallhave
outputlayersratherthanper-layeroutputs. Cascadenetworks
have a natural advantage of an early exit by not requiring
all output layers to be computed. If the early accuracy of a
cascade network is not suﬃcient, inference could potentially
be dispatched to a cloud device [ 145,25]. A disadvantage of
adaptivenetworks isthatthey usually needhyper-parameters
optimizedmanually(e.g.,conﬁdencescore[ 145]). Thisintro-
ducesautomationchallengesaswellasclassiﬁcationaccuracy
loss. Theyfound28.75%testerroronCIFAR-10whenset-
ting the threshold to 0.5. A threshold of 0.99 lowered the
error to 15.74% at a cost of 3x to inference time.
Acascadingnetwork[ 189]isanadaptivenetworkwith
an RL trained Composer that can determine a reasonable
computation graph for each input. An adaptive controller
Policy Preferences is used to intelligently enhance the Com-
poserallowing an adjustment of the network computation
graphfromsub-graphs. The Composer performsmuchbetter
intermsofaccuracythanthebaselinenetworkwiththesame
numberofcomputation-involvedparametersonamodiﬁed
dataset, namely Wide-MNIST. For example, when invoking
1k parameters, the baseline achieves 72% accuracy while the
Composer obtained 85%.
BlockDrop[ 246]introduceda policy network thattrained
usingRLtomakeanimage-speciﬁcdeterminationwhether
a residual network block should participate in the follow-
ing computation. While the other approaches compute an
exit conﬁdence score per layer, the policy network runs only
once when an image is loaded. It generates a boolean vec-
tor that indicates which residual blocks are activate or in-
active. BlockDrop adds more ﬂexibility to the early exit
mechanism by allowing a decision tobe made on any block
and not just early blocks in Spatially Adaptive Computation
Time(SACT)[ 68]. ThisisdiscussedfurtherinSection3.2.3.
BlockDrop achieves an average speed-up of 20% on ResNet-
101forILSVRC-2012withoutaccuracyloss. Experiments
using the CIFAR dataset showed better performance than
other SOTA counterparts at that time [68, 82, 147].
Runtime Neural Pruning (RNP) [ 153] is a framework
that prunes neural networks dynamically. RNP formulates
the feature selection problem as a MarkovDecision Process
(MDP)andthentrainsanRNN-baseddecisionnetworkbyRL. The MDP reward function in the state-action-reward
sequence is computation eﬃciency. Rather than removing
layers,asidenetworkofRNPpredictswhichfeaturemapsare
not needed. They found 2:3to5:9reduction in execution
timewithtop-5accuracylossfrom2.32%to4.89%forVGG-
16.
3.2.3. Diﬀerentiable Adaptive Networks
Mostoftheaforementioneddecisioncomponentsarenon-
diﬀerential, thus computationally expensive RL is adopted
for training. A number of techniques have been developed to
reduce training complexity by using diﬀerentiable methods.
Dynamic channel pruning [ 73] proposes a method to dy-
namically select which channel to skip or to process using
Feature Boosting and Suppression (FBS). FBS is a side net-
work that guideschannel ampliﬁcationand omission. FBSis
trainedalongwithconvolutionalnetworksusingSGDwith
LASSOconstraints. Theselectingindicatorcanbemerged
into BN parameters. FBS achieved 5acceleration on VGG-
16 with 0.59% ILSVRC-2012 top-5 accuracy loss, and 2
acceleration on ResNet-18 with 2.54% top-1, 1.46% top-5
accuracy loss.
Another approach, Dynamic Channel Pruning (DCP)
[42] dynamically prunes channels using a channel thresh-
old weighting (T-Weighting) decision. Speciﬁcally, this mod-
ule prunes the channels whose score is lower than a given
threshold. The score is calculated by a T-sigmoid activation
function, which is mathematically described in Equation 10,
where.x/ = 1_.1 +e*x/isthesigmoidfunction. Theinput
to the T-sigmoid activation function is down sampled by a
FCL from the feature maps. The threshold is found using
iterative training which can be a computationally expensive
process. DCP increased VGG-16 top-5 error by 4.77% on
ILSVRC-2012for 5computationspeed-up. Bycomparison,
RNP increased VGG-16 top-5 error by 4.89% [153].
h.x/ =T
.x/;ifx>T
0;otherwise(10)
ThecascadingneuralnetworkbyLeroux[ 145]reduced
the average inference time of overfeat network [ 211] by 40%
with a 2% ILSVRC-2012 top-1 accuracy loss. Their criteria
forearlyexitisbasedontheconﬁdencescoregeneratedbyan
outputlayer. Theauxiliarylayersweretrainedwithgeneral
backpropagation. Theadjustablescorethresholdprovidesa
trade-oﬀ between accuracy and eﬃciency.
Bolukbasi [ 25] reports a system that contains a com-
bination of other SOTA networks (e.g., AlexNet, ResNet,
GoogLeNet, etc.). A policy adaptively chooses a point to
exitearly. This policycanbetrained byminimizingits cost
function. They format the system as a directed acyclic graph
withvariouspre-trainednetworksasbasiccomponents. They
evaluate this graph to determine leaf nodes for early exit.
The cascade of acyclic graphs with a combination of various
networks reduces computations while maintaining predic-
tion accuracy. ILSVRC-2012 experimentsshowResNet-50
acceleration of 2:8with 1% top-5 accuracy loss and 1:9
speed-up with no accuracy loss.
T Liang et al.: Preprint submitted to Elsevier Page 13 of 41

--- PAGE 14 ---
Survey on pruning and quantization
ConsideringthesimilarityofRNNsandresidualnetworks
[83], Spatially Adaptive Computation Time (SACT) [ 68]
explored an early stop mechanism of residual networks in
the spatial domain. SACT can be applied to various tasks
includingimageclassiﬁcation,objectdetection,andimage
segmentation. SACT achievedabout 20% accelerationwith
no accuracy loss for ResNet-101 on ILSVRC-2012.
To meet the computation constraints, Multi-Scale Dense
Networks (MSDNets) [ 108] designed an adaptive network
usingtwotechniques: 1)an anytime-prediction togenerate
prediction results at many nodes to facilitate the network’s
early exit and 2) batch computational budget to enforce a
simplerexitcriteriasuchasacomputationlimit. MSDNets
combine multi-scale feature maps [ 265] and dense connec-
tivity[109]toenableaccurateearlyexitwhilemaintaining
higher accuracy. The classiﬁers are diﬀerentiable so that
MSDNets can be trained using stochastic gradient descent.
MSDNets achieve 2:2speed-up at the same accuracy for
ResNet-50 on ILSVRC-2012 dataset.
To address the training complexity of adaptive networks,
Li [148] proposed two methods. The ﬁrst method is gradient
equilibrium(GE).Thistechniquehelpsbackbonenetworks
converge by using multiple intermediate classiﬁers across
multiple diﬀerent network layers. This improves the gradi-
ent imbalance issue found in MSDNets [ 108]. The second
method is an Inline Subnetwork Collaboration (ISC) and a
One-For-Allknowledgedistillation(OFA).Insteadofinde-
pendently training diﬀerent exits, ISC takes early predictions
into later predictors to enhance their input information. OFA
supervisesalltheintermediateexitsusingaﬁnalclassiﬁer. At
asameILSVRC-2012top-1accuracyof73.1%,theirnetwork
takes only one-third the computational budget of ResNet.
SlimmableNeuralNetworks(SNN)[ 259]areatypeof
networksthatcanbeexecutedatdiﬀerentwidths. Alsoknown
as switchable networks, the network enables dynamically
selectingnetworkarchitectures(width)withoutmuchcompu-
tationoverhead. Switchablenetworksaredesignedtoadap-
tivelyandeﬃcientlymaketrade-oﬀsbetweenaccuracyand
on-device inference latency across diﬀerent hardware plat-
forms. SNN found that the diﬀerence of feature mean and
variance may lead to training faults. SNN solves this issue
with a novel switchable BN technique and then trains a wide
enough network. Unlike cascade networks which primar-
ily beneﬁt from speciﬁc blocks, SNN can be applied with
manymoretypesofoperations. AsBNalreadyhastwopa-
rametersasmentionedin Section2,thenetworkswitchthat
controls the network width comes with little additional cost.
SNN increased top-1 error by 1.4% on ILSVRC-2012 while
achieving about 2speed-up.
3.3. Comparisons
Pruningtechniquesarediverseanddiﬃculttocompare.
Shrinkbench [ 24] is auniﬁed benchmark frameworkaiming
to provide pruning performance comparisons.
Thereexistambiguitiesaboutthevalueofthepre-trained
weights. Liu [ 160] argues that the pruned model could be
trained from scratch using a random weight initialization.This implies the pruned architecture itself iscrucial tosuc-
cess. Bythisobservation,thepruningalgorithmscouldbe
seenasatypeofNAS.Liuconcludedthatbecausetheweight
values can be re-trained, by themselves they are not eﬃca-
cious. However,thelotterytickethypothesis[ 70]achieved
comparable accuracy only when the weight initialization
was exactly the same as the unpruned model. Glae [ 72]
resolvedthediscrepancybyshowingthatwhatreallymatters
isthepruningform. Speciﬁcally,unstructuredpruningcan
only be ﬁne-tuned to restore accuracy but structured pruning
can be trained from scratch. In addition, they explored the
performance of dropout and l0regularization. The results
showed that simple magnitude based pruning can perform
better. Theydevelopedamagnitudebasedpruningalgorithm
and showed the pruned ResNet-50 obtained higher accuracy
than SOTA at the same computational complexity.
4. Quantization
Quantizationisknownas the process of approximating
a continuous signal by a set of discrete symbols or integer
values. Clustering and parameter sharing also fall within
this deﬁnition [ 92]. Partial quantization uses clustering al-
gorithms such as k-means to quantize weight states and then
storetheparametersinacompressedﬁle. Theweightscanbe
decompressed usingeither a lookup table ora lineartransfor-
mation. Thisistypicallyperformedduringruntimeinference.
Thisschemeonlyreducesthestoragecostofamodel. This
is discussed in Section 4.2.4. In this section we focus on
numerical low-bit quantization.
Compressing CNNs by reducing precision values has
beenpreviouslyproposed. Convertingﬂoating-pointparame-
tersintolownumericalprecisiondatatypesforquantizingneu-
ral networks was proposed as far back as the 1990s [ 67,14].
Renewed interest in quantization began in the 2010s when 8-
bitweightvalueswereshowntoaccelerateinferencewithout
a signiﬁcant drop in accuracy [233].
HistoricallymostnetworksaretrainedusingFP32num-
bers[225]. FormanynetworksanFP32representationhas
greater precision than needed. Converting FP32 parameters
to lower bit representations can signiﬁcantly reduce band-
width, energy, and on-chip area.
Figure12showstheevolutionofquantizationtechniques.
Initially,onlyweightswerequantized. Byquantizing,cluster-
ing,andsharing,weightstoragerequirementscanbereduced
by nearly 4. Han [92] combined these techniques to reduce
weightstoragerequirementsfrom27MBto6.9MB.Posttrain-
ing quantization involves taking a trained model, quantizing
the weights,and thenre-optimizing themodel to generatea
quantized model with scales [ 16]. Quantization-aware train-
ing involves ﬁne-tuning a stable full precision model or re-
trainingthequantizedmodel. Duringthisprocessreal-valued
weights are often down-scaled to integer values - typically
8-bits[120]. Saturatedquantizationcanbeusedtogenerate
feature scales using a calibratation algorithm with a calibra-
tion set. Quantized activations show similar distributions
with previous real-valued data [ 173]. Kullback-Leibler di-
T Liang et al.: Preprint submitted to Elsevier Page 14 of 41

--- PAGE 15 ---
Survey on pruning and quantization
post train
quantizecluster/
sharing
non-saturatedcalibrated
saturatedquantize -aware 
training
weights
activation
floating point floating pointquantize -aware 
training
Figure 12: Quantization Evolution: The development of quantization techniques, from left to right. Purple rectangles indicated
quantized data while blue rectangles represent full precision 32-bit ﬂoating point format.
vergence(KL-divergence,alsoknownasrelativeentropyor
informationdivergence)calibratedquantizationistypically
appliedandcanacceleratethenetworkwithoutaccuracyloss
for many well known models [ 173]. Fine-tuning can also be
applied with this approach.
KL-divergence is a measure to show the relative entropy
of probability distributions between two sets. Equation 11
givestheequationforKL-divergence. PandQaredeﬁned
asdiscreteprobabilitydistributionsonthesameprobability
space. Speciﬁcally, Pis the original data (ﬂoating-point)
distribution that falls in several bins. Qis the quantized data
histogram.
DKL.PñQ/ =NÉ
i=0P.xi/ log0P.xi/
Q.xi/1
(11)
Depending upon the processor and execution environ-
ment,quantizedparameterscanoftenaccelerateneuralnet-
work inference.
Quantization research can becategorized into two focus
areas: 1)quantizationawaretraining(QAT)and2)posttrain-
ingquantization(PTQ).Thediﬀerencedependsonwhether
training progress is is taken into account during training. Al-
ternatively,wecouldalsocategorizequantizationbywhere
dataisgroupedforquantization: 1)layer-wiseand2)channel-
wise. Further, while evaluatingparameter widths,we could
further classify by length: N-bit quantization.
Reduced precisiontechniques donot alwaysachieve the
expected speedup. For example, INT8 inference doesn’t
achieve exactly 4speedup over 32-bit ﬂoating point due
to the additional operations of quantization and dequanti-
zation. For instance, Google’s TensorFlow-Lite [ 227] and
nVidia’sTensorRT[ 173]INT8inferencespeedupisabout
2-3. Batchsizeisthecapabilitytoprocessmorethanone
imageintheforwardpass. Usinglargerbatchsizes,Tensor
RT does achieve 3-4acceleration with INT8 [173].
Section 8 summarizes current quantization techniques
usedontheILSVRC-2012datasetalongwiththeirbit-widths
for weights and activation.
4.1. Quantization Algebra
Xq=f.sg.Xr/ +z/ (12)There are many methods to quantize a given network. Gener-
ally,theyareformulatedasEquation12where sisascalar
that can be calculated using various methods. g./is the
clampfunctionappliedtoﬂoating-pointvalues Xrperform-
ing the quantization. zis the zero-point to adjust the true
zeroin someasymmetricalquantization approaches. f./is
theroundingfunction. Thissectionintroducesquantization
using the mathematical framework of Equation 12.
clamp .x;; / =max.min.x;/;/ (13)
Equation 13 deﬁnes a clamp function. The min-max
methodisgivenbyEquation14where [m;M ]arethebounds
for the minimum and maximum values of the parameters, re-
spectively.nis the maximum representable number derived
fromthebit-width(e.g., 256 = 28incaseof8-bit),and z;s
are the same as in Equation 12. zis typically non-zero in the
min-max method [120].
g.x/ =clamp .x;m;M /
s=n* 1
M*m; z=m .1 *n/
M*m
wherem= min^ Xi`; M= max^ Xi`(14)
Themax-absmethodusesasymmetryboundshownin
Equation 15. The quantization scale sis calculated from
the largest one Ramong the data to be quantized. Since the
bound is symmetrical, the zero point zwill be zero. In such
a situation, the overhead of computing an oﬀset-involved
convolutionwillbereducedbutthedynamicrangeisreduced
sincethevalidrangeisnarrower. Thisisespeciallynoticeable
forReLUactivateddatawhereallofwhichvaluesfallonthe
positive axis.
g.x/ =clamp .x;*M;M /
s=n* 1
R; z= 0
whereR= max^abs^Xi``(15)
Quantization can be applied on input features F, weights
W, and biases b. Taking feature Fand weights Was an
example(ignoringthebiases)andusingthe min-maxmethod
gives Equation 16. The subscripts randqdenote the real-
valued and quantized data, respectively. The maxsuﬃx is
T Liang et al.: Preprint submitted to Elsevier Page 15 of 41

--- PAGE 16 ---
Survey on pruning and quantization
fromRin Equation 15, while sf= .n* 1/_Fmax; sw=
.n* 1/_Wmax.
Fq=n* 1
FmaxFr;Wq=n* 1
WmaxWr (16)
Integer quantized convolution is shown in Equation 17
andfollowsthesameformasconvolutionwithrealvalues. In
Equation 17, the <denotes theconvolution operation, Fthe
feature, Wthe weights, and Oq, the quantized convolution
result. Numerous third party libraries support this type of in-
tegerquantizedconvolutionacceleration. Theyarediscussed
in Section 4.3.2.
Oq=Fq<Wqs.t.F;WËZ (17)
De-quantizing converts the quantized value Oqback to
ﬂoating-point Orusing the feature scales sfand weights
scalessw. Asymmetricexamplewith z= 0isshowninEqua-
tion18. Thisisusefulforlayersthatprocessﬂoating-point
tensors. Quantization libraries are discussed in Section 4.3.2.
Or=Oq
sfsw=OqFmax
.n* 1/Wmax
.n* 1/(18)
Inmostcircumstances,consecutivelayerscancompute
with quantized parameters. This allows dequantization to
be merged in one operation as in Equation 19. Fl+1
qis the
quantizedfeaturefornextlayerand sl+1
fisthefeaturescale
for next layer.
Fl+1
q=Oqsl+1
f
sfsw(19)
The activation function can be placed following either
thequantizedoutput Oq,thede-quantizedoutput Or,orafter
a re-quantized output Fl+1
q. The diﬀerent locations may lead
to diﬀerent numerical outcomes since they typically have
diﬀerent precision.
Similartoconvolutionallayers,FCLscanalsobequan-
tized. K-means clustering can be used to aid in the compres-
sion of weights. In 2014 Gong [ 76] used k-means clustering
on FCLs and achieved a compression ratio of more than 20
with 1% top-5 accuracy loss.
Bias terms in neural networks introduce intercepts in
linear equations. They are typically regarded as constants
that help the network to train and best ﬁt given data. Bias
quantization is not widely mentioned in the literature. [ 120]
maintained 32-bit biases while quantizing weights to 8-bit.
Since biases account for minimal memory usage (e.g. 12
values for a 10-in/12-out FCL vs 120 weight values) it is
recommendedtoleavebiasesinfullprecision. Ifbiasquan-
tization is performed it can be a multiplication by both the
featurescaleandweightscale[ 120],asshowninEquation20.
However, in some circumstances they may have their own
scalefactor. Forexample,whenthebit-lengthsarelimitedto
be shorter than the multiplication results.
sb=swsf;bq=brsb (20)4.2. Quantization Methodology
WedescribePTQandQATquantizationapproachesbased
onback-propagationuse. Wecanalsocategorizethembased
onbit-width. Inthefollowingsubsections,weintroducecom-
mon quantization methods. In Section 4.2.1 low bit-width
quantization is discussed. In Section 4.2.2 and Section 4.2.3
specialcasesoflowbit-widthquantizationisdiscussed. In
Section 4.2.5 diﬃculties with training quantized networks
are discussed. Finally, in Section 4.2.4, alternate approached
to quantization are discussed.
4.2.1. Lower Numerical Precision
Halfprecisionﬂoatingpoint(16-bitﬂoating-point,FP16)
hasbeenwidelyusedinnVidiaGPUsandASICaccelerators
with minimal accuracy loss [ 54]. Mixed precision training
with weights, activations, and gradients using FP16 while
the accumulated error for updating weights remains in FP32
hasshownSOTAperformance-sometimesevenimproved
performance [172].
Researchers[ 165,98,233]haveshownthatFP32parame-
tersproducedduringtrainingcanbereducedto8-bitintegers
forinferencewithoutsigniﬁcantlossofaccuracy. Jacob[ 120]
applied8-bitintegersforbothtrainingandinference,withan
accuracy loss of 1.5% on ResNet-50. Xilinx [ 212] showed
that8-bitnumericalprecisioncouldalsoachievelosslessper-
formancewithonlyonebatchinferencetoadjustquantization
parameters and without retraining.
Quantization can be considered an exhaustive search op-
timizing the scale found to reduce an error term. Given a
ﬂoating-pointnetwork,thequantizerwilltakeaninitialscale,
typically calculated by minimizing the l2-error, and use it
to quantize the ﬁrst layer weights. Then the quantizer will
adjust the scale to ﬁnd the lowest output error. It performans
this operation on every layer.
IntegerArithmetic-onlyInference(IAI)[ 120]proposed
a practical quantization scheme able to be adopted by indus-
try using standard datatypes. IAI trades oﬀ accuracy and
inference latency by compressing compact networks into in-
tegers. Previoustechniquesonlycompressedtheweightsof
redundantnetworksresultinginbetterstorageeﬃciency. IAI
quantizesz0in Equation 12 requiring additional zero-
pointhandlingbutresultinginhighereﬃciencybymaking
use of unsigned 8-bit integers. The data-ﬂow is described in
Figure 13. TensorFlow-Lite [ 120,131] deployed IAI with
an accuracy loss of 2.1% using ResNet-150 on the ImageNet
dataset. This is described in more detail in Section 4.3.2.
Datatypes other than INT8 have been used to quantize
parameters. Fixedpoint,wheretheradixpointisnotatthe
right-mostbinarydigit,isoneformatthathasbeenfoundtobe
useful. Itprovideslittlelossorevenhigheraccuracybutwith
a lower computation budget. Dynamic scaled ﬁxed-point
representation [ 233] obtained a 4acceleration on CPUs.
However,itrequiresspecializedhardwareincluding16-bit
ﬁxed-point [ 89], 16-bit ﬂex point [ 130], and 12-bit opera-
tions using dynamic ﬁxed-point format (DFXP) [ 51]. The
specialized hardware is mentioned in Section 4.3.3.
T Liang et al.: Preprint submitted to Elsevier Page 16 of 41

--- PAGE 17 ---
Survey on pruning and quantization
weights
uint8
ReLU6
feature
uint8conv +biases
uint32
activation
uint8
Figure 13: Integer Arithmetic-only Inference: The convolution
operation takes unsigned int8 weights and inputs, accumulates
them to unsigned int32, and then performs a 32-bit addi-
tion with biases. The ReLU6 operation outputs 8-bit integers.
Adopted from [120]
4.2.2. Logarithmic Quantization
Bit-shiftoperationsareinexpensivetoimplementinhard-
ware compared to multiplication operations. FPGA imple-
mentations [ 6] speciﬁcally beneﬁt by converting ﬂoating-
point multiplication into bit shifts. Network inference can
be further optimized if weights are also constrained to be
power-of-two with variable-length encoding. Logarithmic
quantization takes advantage of this by being able to express
a larger dynamic range compared to linear quantization.
Inspiredbybinarizednetworks[ 52],introducedinSec-
tion4.2.3,Lin[ 156]forcedtheneuronoutputintoapower-
of-two value. This converts multiplications into bit-shift
operations by quantizing the representations at each layer of
thebinarizednetwork. Bothtrainingandinferencetimeare
thus reduced by eliminating multiplications.
Incremental Network Quantization (INQ) [ 269] replaces
weights with power-of-two values. This reduces computa-
tion time by converting multiplies into shifts. INQ weight
quantizationisperformediteratively. Inoneiteration,weight
pruning-inspiredweightpartitioningisperformedusinggroup-
wisequantization. Theseweightsarethenﬁne-tunedbyusing
a pruning-like measurement [ 92,88]. Group-wise retraining
ﬁne-tunes a subset of weights in full precision to preserve
ensemble accuracy. The other weights are converted into
power-of-two format. After multiple iterations most of the
full precision weights are converted to power-of-two. The
ﬁnal networks have weights from 2 (ternary) to 5 bits with
valuesnearzerosettozero. Resultsofgroup-wiseiterative
quantization show lower error rates than a random power-of-
two strategy. Speciﬁcally, INQ obtained 71compression
with 0.52% top-1 accuracy loss on the ILSVRC-2012 with
AlexNet.
LogarithmicNeuralNetworks(LogNN)[ 175]quantize
weightsandfeaturesintoalog-basedrepresentation. Loga-
rithmicbackpropagationduringtrainingisperformedusing
shift operations. Bases other than log2can be used. logù
2
basedarithmeticisdescribedasatrade-oﬀbetweendynamic
rangeandrepresentationprecision. log2showed 7compres-
sion with 6.2% top-5 accuracy loss on AlexNet, while logù
2
showed 1.7% top-5 accuracy loss.
Shift convolutional neural networks (ShiftCNN) [ 84] im-
prove eﬃciency by quantizing and decomposing the real-
valued weights matrix into an NtimesBranged bit-shift,
and encoding them with code-books Cas shown in Equa-tion 21.idxi.n/is the index for the ithweights in the nth
code-book. Each coded weight wican be indexed by the
NB-bit expression.
wi=NÉ
n=1Cnidxi.n/
Cn=$
0;,2*n+1;,2*n;,2*n*1;§;,2*n*âM_2ã+2%
whereM= 2B* 1(21)
Notethatthenumberofcode-books Cncanbegreaterthan
one. Thismeanstheencodedweightmightbeacombination
of multiple shift operations. This property allows ShiftCNN
to expand to a relatively large-scale quantization or to shrink
tobinarizedorternaryweights. Wediscussternaryweightsin
Section4.2.3. ShiftCNNwasdeployedonanFPGAplatform
and achieved comparable accuracy on the ImageNet dataset
with75%powersavingandupto 1090clockcyclespeed-up.
ShiftCNNachievesthisimpressiveresultwithoutrequiringre-
training. With N= 2andB= 4encoding,SqueezeNet[ 115]
hasonly1.01%top-1accuracyloss. ThelossforGoogLeNet,
ResNet-18,andResNet-50is0.39%,0.54%,and0.67%,re-
spectively,Whilecompressingtheweightsinto7/32ofthe
originalsize. Thisimpliesthattheweightshavesigniﬁcant
redundancy.
Based on LogNN, Cai [ 30] proposed improvements by
disabling activation quantization toreduce overhead during
inference. Thisalsoreducedtheclampboundhyperparameter
tuning during training. These changes resulted in many low-
valuedweightsthatareroundedtothenearestvalueduring
encoding. As 2ns.t.nËNincreases quantized weights
sparsity asnincreases. In this research, nis allowed to be
real-valuednumbersas nËRtoquantizetheweights. This
makes weight quantization more complex. However, a code-
book helps to reduce the complexity.
In 2019, Huawei proposed DeepShift, a method of sav-
ing computing power by shift convolution [ 62]. DeepShift
removedall ﬂoating-pointmultiply operationsandreplaced
them with bit reverse and bit shift. The quantized weight
Wqtransformation isshown mathematicallyin Equation 22,
whereSisasignmatrix, Pisashiftmatrix,and Zistheset
of integers.
Wq=S 2P;s.t.PËZ;SË ^*1;0;+1` (22)
Results indicate that DeepShift networks cannot be easily
trained from scratch. They also show that shift-format net-
works do not directly learn for lager datasets such as Im-
agenet. Similar to INQ, they show that ﬁne-tuning a pre-
trained network can improve performance. For example,
with the same conﬁguration of 32-bit activations and 6-bit
shift-format weights, the top-1 ILSVRC-2012 accuracy loss
on ResNet-18 for trained from scratch and tuned from a pre-
trained model are 4.48% and 1.09%, respectively.
DeepShiftproposesmodelswithdiﬀerentialbackpropa-
gationforgeneratingshiftcoeﬃcientsduringtheretraining
process. DeepShift-Q[ 62]istrainedwithﬂoating-pointpa-
rametersinbackpropagationwithvaluesroundedtoasuitable
T Liang et al.: Preprint submitted to Elsevier Page 17 of 41

--- PAGE 18 ---
Survey on pruning and quantization
format during inference. DeepShift-PS directly adopts the
shiftPand signSparameters as trainable parameters.
Since logarithmic encoding has larger dynamic range,
redundantnetworksparticularlybeneﬁt. However,lessredun-
dantnetworksshow signiﬁcantaccuracyloss. For example,
VGG-16whichisaredundantnetworkshows1.31%accuracy
loss on top-1 while DenseNet-121 shows 4.02% loss.
4.2.3. Plus-minus Quantization
Plus-minus quantization was in 1990 [ 208]. This tech-
nique reduces all weights to 1-bit representations. Similar
to logarithmic quantization, expensive multiplications are
removed. Inthissection,weprovideanoverviewofsigniﬁ-
cant binarized network results. Simons [ 216] and Qin [ 198]
provide an in-depth review of BNNs.
Binarizedneuralnetworks(BNN)haveonly1-bitweights
and often 1-bit activations. 0 and 1 are encoded to represent
-1and+1,respectively. Convolutionscanbeseparatedinto
multiplies and additions. In binary arithmetic, single bit
operations can be performed using and,xnor, and bit-count.
We follow the introduction from [ 273] to explain bit-wise
operation. Single bit ﬁxedpoint dotproducts are calculated
as in Equation 23, where andis a bit-wise AND operation
andbitcountcounts the number of 1’s in the bit string.
xy= bitcount.and. x;y//;s.t.Åi;xi;yiË ^0;1`(23)
This can be extended into multi-bit computations as in Equa-
tion 24 [53].xandyare M-bit and K-bit ﬁxed point inte-
gers,subjectto x=³M*1
m=0cm.x/2mandy=³K*1
k=0ck.y/2k
, where .cm.x//M*1
m=0and.ck.y//K*1
k=0are bit vectors.
xy =M*1É
m=0K*1É
k=02m+kbitcountand cm.x/;ck.y/;
s.t.cm.x/i;ck.y/iË ^0;1`Åi;m;k:(24)
By removing complicated ﬂoating-point multiplications,
networks are dramatically simpliﬁed with simple accumula-
tionhardware. Binarizationnotonlyreducesthenetworksize
by up-to 32 , but also drastically reduces memory usage re-
sultinginsigniﬁcantlylowerenergyconsumption[ 174,112].
However, reducing 32-bit parameters into a single bit results
inasigniﬁcantlossofinformation,whichdecreasespredic-
tion accuracy. Most quantized binary networkssigniﬁcantly
under-perform compared to 32-bit competitors.
Therearetwoprimarymethodstoreduceﬂoating-point
valuesintoasinglebit: 1)stochasticand2)deterministic[ 52].
Stochastic methodsconsider globalstatistics orthe valueof
input data to determine the probability of some parameter to
be -1 or +1. Deterministic binarization directly computes
the bit value based on a threshold, usually 0, resulting in a
sign function. Deterministic binarization is much simpler to
implement in hardware.
Binary Connect (BC), proposed by Courbariaux [ 52],
is an early stochastic approach to binarize neural networks.
They binarized the weights both in forward and backward
propagation. Equation25showsthestochasticbinarizationxbwitha hard sigmoid probability.x/. Boththeactivations
and the gradients use 32-bit single precision ﬂoating point.
The trained BC network shows 1.18% classiﬁcation error
onthesmallMNISTdatasetbut8.27%classiﬁcationonthe
larger CIFAR-10 dataset.
xb=T
+1;with probability p=.x/
*1;with probability 1 *p
where.x/ = clampx+ 1
2;0;1(25)
Courbariaux extended BC networks by binarizing the
activations. HenamedthemBinaryNets[ 53],whichisrecog-
nized as the ﬁrst BNN. They also report a customized binary
matrix multiplication GPU kernel that accelerates the calcu-
lation by 7. BNN is considered the ﬁrst binarized neural
network where both weights and activations are quantized
to binary values [ 216]. Considering the hardware cost of
stochasticbinarization,theymadeatrade-oﬀtoapplydeter-
ministicbinarizationinmost circumstances. BNN reported
0.86% erroron MNIST,2.53% erroron SVHN,and 10.15%
error on CIFAR-10. The ILSVRC-2012 dataset accuracy
resultsforbinarizedAlexNetandGoogleNetare36.1%top-1
and 47.1%, respectively while the FP32 original networks
achieve 57% and 68%, respectively [112].
Rastegari [ 200] explored binary weight networks (BWN)
on the ILSVRC dataset with AlexNet and achieved the same
classiﬁcationaccuracyasthesingleprecisionversion. The
key is a scaling factor ËR+applied to an entire layer of
binarizedweights B. Thisresultsinsimilarweightsvalues
as if they were computed using FP32 WùB. They also
appliedweightbinarizationonResNet-18andGoogLeNet,
resulting in 9.5% and 5.8% top-1 accuracy loss compared
totheFP32version,respectively. Theyalsoextendedbina-
rizationto activationscalledXNOR-Netand evaluatediton
the large ILSVRC-2012 dataset. Compared to BNN, XNOR-
Net also applied a scaling factor on the input feature and a
rearrangement of the network structure (swapping the con-
volution,activation,andBN).Finally,XNOR-Netachieved
44.2%top-1classiﬁcationaccuracyonILSVRC-2012with
AlexNet, while accelerating execution time 58on CPUs.
The attachedscaling factorextended thebinarized value ex-
pression,whichreducedthenetworkdistortionandleadto
better ImageNet accuracy.
DoReFa-Net [ 272] also adopts plus-minus arithmetic for
quantized network. DoReFa additionally quantizes gradients
to low-bit widths within 8-bit expressions during the back-
wardpass. Thegradientsarequantizedstochasticallyinback
propagation. For example, it takes 1 bit to represent weights
layer-wise, 2-bit activations, and 6-bits for gradients. We
describetrainingdetailsinSection4.2.5. Theyfound9.8%
top-1 accuracy loss on AlexNet with ILSVRC-2012 using
the1-2-6combination. Theresultforthe1-4-32combination
is 2.9%.
Li [146] and Leng [ 144] showed that for ternary weights
(*1;0;and+ 1), in Ternary Weight Networks (TWN), only
aslightaccuracylosswasrealized. ComparedtoBNN,TWN
has an additional value to reduce information loss while still
T Liang et al.: Preprint submitted to Elsevier Page 18 of 41

--- PAGE 19 ---
Survey on pruning and quantization
keepingcomputationalcomplexitysimilartoBNN’s. Ternary
logic may be implemented very eﬃciently in hardware, as
theadditionalvalue(zero)donotactuallyparticipateincom-
putations [ 50]. TWN adopts the l2-distance to ﬁnd the scale
andformatstheweightsinto *1;0;and+ 1withathreshold
generated by an assumption that the weighs are uniformly
distributed such as in [*a;a]. This resulted in up to 16
modelcompressionwith3.6%ResNet-18top-1accuracyloss
on ILSVRC-2012.
TrainedTernaryQuantization(TTQ)[ 274]extendedTWN
byintroducingtwodynamicconstraintstoadjustthequantiza-
tionthreshold. TTQoutperformedthefullprecisionAlexNet
ontheILSVRC-2012top-1classiﬁcationaccuracyby0.3%.
It also outperformed TWN by 3%.
Ternary Neural Networks (TNN) [ 6] extend TWN by
quantizing the activations into ternary values. A teacher net-
work is trained with full precision and then using transfer
learning the same structure is used but replacing the full
precision values with a ternarized student in a layer-wise
greedymethod. Asmalldiﬀerencebetweenthereal-valued
teacher network and the ternarized student network is that
theyactivatetheoutputwithaternaryoutputactivationfunc-
tiontosimulatetherealTNNoutput. TNNachieves1.67%
MNISTclassiﬁcationerrorand12.11%classiﬁcationerror
on CIFAR10. TNN has slightly lower accuracy compared to
TWN (an additional 1.02% MNIST error).
IntelproposedFine-GrainedQuantization(FGQ)[ 170]
togeneralizeternaryweightsbysplittingthemintoseveral
groups and with independent ternary values. The FGQ quan-
tizedResNet-101networkachieved73.85%top-1accuracyon
theImageNetdataset(comparedwith77.5%forthebaseline)
using four groups weights and without re-training. FGQ also
showed improvements in (re)training demonstrating a top-1
accuracy improvement from 48% on non-trained to 71.1%
top-1onResNet-50. ResNet-50’sbaselineaccuracyis75%.
Four groups FGQ with ternary weights and low bit-width
activations achieves about 9acceleration.
MeliusNet [ 21] is a binary neural network that consist
of two types of binary blocks. To mitigate drawbacks of
low bit width networks, reduced information quality, and
reduced network capacity, MeliusNet used a combination
ofdense block [22] which increases network channels by
concatenating derived channels from the input to improve
capacity and improvement block [161] which improves the
quality of features by adding additional convolutional acti-
vations onto existing extra channels from dense block . They
achievedaccuracyresultscomparabletoMobileNetonthe
ImageNet dataset with MeliusNet-59 reporting 70.7% top-
1 accuracy while requiring only 0.532 BFLOPs. A similar
sized 17MB MobileNet required 0.569 BFLOPs achieving
70.6% accuracy.
AdderNet[ 35]isanothertechniquethatreplacesmultiply
arithmeticbutallowslargerthan1-bitparameters. Itreplaces
all convolutions with addition. Equation 26 shows that for a
standard convolution, AdderNet formulates it as a similaritymeasure problem
Y.m;n;t / =dÉ
i=0dÉ
j=0cinÉ
k=0S.X.m+i;n+j;k/;F.i;j;k;t //(26)
where FËRddcincoutis a ﬁlter,dis the kernel size, cinis
aninputchanneland coutisanoutputchannel. XËRhwcin
standsfortheinputfeatureheight handwidthw. Withthis
formulation, the output Yis calculated with the similarity
S.;/, i.e.,S.x;y/ =xyfor conventional convolution
wherethesimilaritymeasureiscalculatedbycrosscorrela-
tion. Equation27mathematicallydescribesAdderNet,which
replaces the multiply with subtraction. The l1-distance is
applied to calculate the distance between the ﬁlter and the
input feature. By replacing multiplications with subtractions,
AdderNet speeds up inference by transforming 3.9 billion
multiplications into subtractions with a loss in ResNet-50
accuracy of 1.3%.
Y.m;n;t / = *dÉ
i=0dÉ
j=0cinÉ
k=0ðX.m+i;n+j;k/*F.i;j;k;t /ð
(27)
NAS can be applied to BNN construction. Shen [ 213]
adoptedevolutionaryalgorithmstoﬁndcompactbutaccurate
models achieving 69.65% top-1 accuracy on ResNet-18 with
ImageNet at 2:8speed-up. This is better performance than
the 32-bit single precision baseline ResNet-18 accuracy of
69.6%. However, the search approach is time consuming
taking 1440 hours on an nVidia V100 GPU to search 50k
ImageNet images to process an initial network.
4.2.4. Other Approaches to Quantization
Weightsharingbyvectorquantizationcanalsobeconsid-
ered a type of quantization. In order to compress parameters
toreducememoryspaceusage,parameterscanbeclustered
andshared. K-meansisawidelyusedclusteringalgorithm
and has been successfully applied to DNNs with minimal
lossofaccuracy[ 76,243,143]achieving16-24timescom-
pressionwith1%accuracylossontheILSVRC-2012dataset
[76, 243].
HashNet[ 37]usesahashtoclusterweights. Eachhash
groupisreplacedwithasingleﬂoating-pointweightvalue.
This was applied to FCLs and shallow CNN models. They
foundacompressionfactorof 64outperformsequivalent-
sizednetworksonMNISTandsevenotherdatasetstheyeval-
uated.
In 2016 Han applied Huﬀman coding with Deep Com-
pression[ 92]. Thecombinationofweightsharing,pruning,
and huﬀman coding achieved 49compression on VGG-16
withnolossofaccuracyonILSVRC-2012,whichwasSOTA
at the time.
TheHessianmethodwasappliedtomeasuretheimpor-
tance of network parameters and therefore improve weight
quantization [ 45]. They minimized the average Hessian
weighted quantization errors to cluster parameters. They
found compression ratios of 40.65 on AlexNet with 0.94%
T Liang et al.: Preprint submitted to Elsevier Page 19 of 41

--- PAGE 20 ---
Survey on pruning and quantization
accuracylossonILSVRC-2012. Weightregularizationcan
slightlyimprovetheaccuracyofquantizednetworksbype-
nalizingweightswithlargemagnitudes[ 215]. Experiments
showedthat l2regularizationimproved8-bitquantizedMo-
bileNet top-1 accuracy by 0.23% on ILSVRC-2012.
BN has proved to have many advantages including ad-
dressingtheinternalcovariateshiftissue[ 119]. Itcanalso
be considered a type of quantization. However, quantization
performed with BN may have numerical instabilities. The
BN layer has nonlinear square and square root operations.
Lowbitrepresentationsmaybeproblematicwhenusingnon-
linearoperations. Tosolvethis, l1-normBN[ 245]hasonly
linearoperationsinbothforwardandbackwardtraining. It
provided 1:5speedup at half the power on FPGA platforms
and can be used with both training and inference.
4.2.5. Quantization-aware Training
Mostquantizationmethodsuseaglobal(layer-wise)quan-
tization to reduce the full precision model into a reduced bit
model. Thuscanresultinnon-negligibleaccuracyloss. Asig-
niﬁcant drawback of quantization is information loss caused
bytheirreversibleprecisionreducingtransform. Accuracy
lossisparticularlyvisibleinbinarynetworksandshallownet-
works. ApplyingbinaryweightsandactivationstoResNet-34
or GoogLeNet resulted in 29.10% and 24.20% accuracy loss,
respectively [ 53]. It has been shown that backward propaga-
tionﬁne-tunes(retrains)aquantizednetworkandcanrecover
lossesinaccuracycausedbythequantizationprocess[ 171].
The retraining is even resilient to binarization information
distortions. Thustrainingalgorithmsplayacrucialrolewhen
using quantization. In this section, we introduce (re)training
of quantized networks.
BNN Training: Forabinarizednetworkthathasbinaryval-
ued weights it is not eﬀective to update the weights using
gradient decent methods due to typically small derivatives.
Early quantized networks were trained with a variation of
Bayesian inference named Expectation Back Propagation
(EBP) [220,41]. This method assigns limited parameter pre-
cision(e.g.,binarized)weightsandactivations. EBPinfers
networkswithquantizedweightsbyupdatingtheposterior
distributionsovertheweights. Theposteriordistributionsare
updatedbydiﬀerentiatingtheparametersofthebackpropa-
gation.
BinaryConnect [ 52] adopted the probabilistic idea of
EBP but instead of optimizing the weights posterior distri-
bution, BC preserved ﬂoating-point weights for updates and
then quantized them into binary values. The real-valued
weights update using the back propagated error by simply
ignoring the binarization in the update.
AbinarizedNetworkhasonly1-bitparameters- ,1quan-
tized from a sign function. Single bit parameters are non-
diﬀerentiable and therefore it is not possible to calculate gra-
dientsneededforparameterupdating[ 208]. SGDalgorithms
have been shown to need 6 to 8 bits to be eﬀective [ 180]. To
workaroundtheselimitationstheStraight-ThroughEstima-
tor (STE), previously introduced by Hinton [ 102], was ap-plied for propagating gradients by using discretization [ 112].
Equation 28 show the STE for sign binarization, where c
denotes the cost function, wris the real-valued weights, and
wbis the binarized weight produced by the signfunction.
STEbypassesthebinarizationfunctiontodirectlycalculate
real-valuedgradients. Theﬂoating-pointweightsarethenup-
dated using methods like SGD. To avoid real-valued weights
approaching inﬁnity, BNNs typically clamp ﬂoating-point
weights to the desired range of ,1[112].
Forward :wb= sign wr
Backward :)c
)wr=)c
)wb1ðwrðf1(28)
Unlike the forward phase where weights and activations
are produced with deterministic quantization, in the gradient
phase,the lowbitgradientsshould begeneratedby stochas-
tic quantization [ 89,271]. DoReFa [ 272] ﬁrst successfully
trainedanetworkwithgradientbit-widthslessthaneightand
achieved a comparable result with k-bit quantization arith-
metic. Thislowbit-widthgradient schemecouldaccelerate
training in edge devices with little impact to network accu-
racy but minimal inference acceleration compared to BNNs.
DoReFaquantizestheweights,features,andgradientsinto
many levels obtaining a larger dynamic range than BNNs.
TheytrainedAlexNetonImageNetfromscratchwith1-bit
weights,2-bitactivations,and6-bitgradients. Theyobtained
46.1% top-1 accuracy (9.8% loss comparing with the full
precisioncounterpart). Equation29showstheweightquan-
tizingapproach. wistheweights(thesameasinEquation28),
limitis a limit function applied to the weights keeping them
in the range of [0, 1], and quantizekquantizes the weights
intok-levels. Feature quantization is performed using the
fk
= quantizekfunction.
fk
w= 2 quantizek limit.wr/* 1
where quantizek.wr/ =1
2k* 1round  2k* 1wr;
andlimit.x/ =tanh.x/
2 max.ðtanh.x/ð/+1
2(29)
InDoReFa, gradientquantizationisshowninEquation30,
where dr=)c_)ris the backprogagated gradient of the cost
functioncto outputr.
fk
= 2 max0.ðdrð/4
quantizek0
dr
2 max0.ðdrð/+1
21
*1
25
(30)
Asindeepfeedforwardnetworks,theexplodinggradi-
ent problem can cause BNN’s not to train. To address this
issue,Hou[ 104]formulatedthebinarizationeﬀectonthenet-
work loss as an optimization problem which was solved by a
proximal Newton’s algorithm with diagonal Hessian approx-
imationthatdirectlyminimizesthelosswithrespecttothe
binaryweights. Thisoptimizationfound0.09%improvement
on MNIST dataset compared with BNN.
T Liang et al.: Preprint submitted to Elsevier Page 20 of 41

--- PAGE 21 ---
Survey on pruning and quantization
Alpha-Blending(AB)[ 162]wasproposedasareplace-
ment for STE. Since STE directly sets the quantization func-
tion gradients to 1, a hypothesis was made that STE tuned
networks could suﬀer accuracy losses. Figure 14 shows that
AB introduces an additional scale coeﬃcient . Real-valued
weightsandquantizedweightsarebothkept. Duringtraining
is gradually raised to 1 until a fully quantized network is
realized.
weights
float
quantizer
weights
binaryfeature
float
convSTE
(a) Straight-through Estimator
weights
float
quantizer
weights
binaryfeature
float
convAB
+α1-α (b) Alpha-Blending Approach
Figure 14: STE and AB: STE directly bypasses the quan-
tizer while AB calculates gradients for real-valued weights by
introducing additional coeﬃcients [162]
Low Numerical Precision Training: Training with low
numerical precision involves taking the low precision values
into both forward and backward propagation while maintain-
ingthefullprecisionaccumulatedresults. MixedPrecision
[172,54] training uses FP16 or 16-bit integer (INT16) for
weightprecision. Thishasbeenshowntobeinaccuratefor
gradientvalues. AsshowninFigure15,fullprecisionweights
aremaintainedforgradientupdating,whileotheroperands
usehalf-ﬂoat. A loss scaling techniqueisappliedtokeepvery
small magnitude gradients from aﬀecting the computation
sinceanyvaluelessthan 2*24becomeszeroinhalf-precision
[172]. Speciﬁcally, a scaler is introduced to the loss value
before backpropagation. Typically, the scaler is a bit-shift
optimalvalue 2nobtainedempirically orbystatistical infor-
mation.
In TensorFlow-Lite [ 120], training proceeds with real
values while quantization eﬀects are simulated in the for-
ward pass. Real-valued parameters are quantized to lower
precisionbeforeconvolutionallayers. BNlayersarefolded
into convolution layers. More details are described in Sec-
tion 4.3.2.
As in binarized networks, STE can also be applied to
reduced precision training such as 8-bit integers [131].
4.3. Quantization Deployment
In this section, we describe implementations of quanti-
zation deployed in popular frameworks and hardware. In
Section4.3.1wegiveanintroductiontodeploymentissues.
weights
FP32
quantizer
float2half
weights
FP16feature
FP32
convF16feature
FP16quantizer
float2halfF32 F16optimizer
loss scaling
activation
FP16input layerF16Figure 15: Mixed Precision Training [ 172]: FP16 is applied
in the forward and backward pass, while FP32 weights are
maintained for the update.
InSection4.3.2,wediscussdeeplearninglibrariesandframe-
works. WeintroducetheirspeciﬁcationinTable2andthen
compare their performance in Table 3. We also discuss hard-
wareimplementationsof DNNsinSection4.3.3. Dedicated
hardwareisdesignedorprogrammedtosupporteﬃcientpro-
cessingofquantizednetworks. SpecializedCPUandGPU
operationsarediscussed. Finally,inSection4.3.4wediscuss
DNN compilers.
4.3.1. Deployment Introduction
Withsigniﬁcantresourcecapability,largeorganizations
and institutions usually have their own proprietary solutions
for applications and heterogeneous platforms. Their support
to the quantization is either inference only or as well as train-
ing. The frameworks don’t always follow the same idea of
quantization. Thereforetherearediﬀerencesbetweenthem,
so performs.
With DNNs being applied in many application areas, the
issueofeﬃcientuseofhardwarehasreceivedconsiderable
attention. Multicoreprocessorsandacceleratorshavebeen
developed to accelerate DNN processing. Many types of
accelerators have been deployed, including CPUs with in-
structionenhancements,GPUs,FPGAs,andspecializedAI
accelerators. Often accelerators are incorporated as part of a
heterogeneous system. A Heterogeneous System Architec-
ture(HSA)allowsthediﬀerentprocessorstointegrateinto
a system to simultaneously access shared memory. For ex-
ample, CPUs and GPUs using cache coherent shared virtual
memory on the same System of Chip (SoC) or connected by
T Liang et al.: Preprint submitted to Elsevier Page 21 of 41

--- PAGE 22 ---
Survey on pruning and quantization
Table 2
Low Precision Libraries Using Quantization: QAT is quantization-aware training, PTQ is
post-training quantization, and oﬀset indicates the zero point zin Equation 12.
Name Institution Core Lib Precision Method Platform Open-sourced
ARM CMSIS NN [129] Arm CMSIS 8-bit deploy only Arm Cortex-M Processor No
MACE [247] XiaoMi - 8-bit QAT and PTQ Mobile - CPU, Hexagon Chips, MTK APU Yes
MKL-DNN [204] Intel - 8-bit PTQ, mixed oﬀset, and QAT Intel AVX Core Yes
NCNN [229] Tencent - 8-bit PTQ w/o oﬀset Mobile Platform Yes
Paddle [13] Baidu - 8-bit QAT and PTQ w/o oﬀset Mobile Platform Yes
QNNPACK [61] Fackbook - 8-bit PTQ w/ oﬀset Mobile Platform Yes
Ristretto [90] LEPS gemm 3 method QAT Desktop Platform Yes
SNPE [228] Qualcomm - 16/8-bit PTQ w/ oﬀset, max-min Snapdragon CPU, GPU, DSP No
Tensor-RT [173] nVidia - 8-bit PTQ w/o oﬀset nVidia GPU Yes
TF-Lite [1] Google gemmlowp 8-bit PTQ w/ oﬀset Mobile Platform Yes
PCIewithplatformatomicscansharethesameaddressspace
[74]. Floating-pointarithmeticunitsconsumemoreenergy
andtakelongertocomputecomparedtointegerarithmetic
units. Consequently, low-bitwidth architectures are designed
to accelerate computation [ 179]. Specialized algorithms and
eﬃcient hardware can accelerate neural network processing
during both training and inference [202].
4.3.2. Eﬃcient Kernels
Typically low precision inference in only executed on
convolutional layers. Intermediate values passed between
layers use 32-bit ﬂoating-point. This makes many of the
frameworks amenable to modiﬁcations.
Table 2 gives a list of major low precision acceleration
frameworks and libraries. Most of them use INT8 precision.
Wewillnextdescribesomepopularandopen-sourcelibraries
in more detail.
Tensor RT [232,242] is an nVidia developed C++ library
thatfacilitateshigh-performanceinferenceonNVIDIAGPUs.
It is a low precision inference library that eliminates the bias
terminconvolutionallayers. Itrequiresacalibrationsetto
adjustthequantizationthresholdsforeachlayerorchannel.
Afterwards the quantizedparametersarerepresented by 32-
bit ﬂoating-point scalar and INT8 weights.
TensorRTtakesapre-trainedﬂoating-pointmodeland
generates a reusable optimized 8-bit integer or 16-bit half
ﬂoat model. The optimizer performs network proﬁling, layer
fusion, memory management, and operation concurrency.
Equation31showstheconvolution-dequantizationdataﬂow
in Tensor RT for 8-bit integers. The intermediate result of
convolutionbyINT8inputfeature Fi8andweights Wi8are
accumulatedintoINT32tensor Oi32. Theyaredequantized
by dividing by the feature and weight scales sf;sw.
Oi32=Fi8<Wi8;Of32=Oi32
sfsw(31)
TensorRTappliesavariantof max-absquantizationto
reduce storage requirements and calculation time of the zero
point termzin Equation 15 by ﬁnding the proper thresh-
old instead of the absolute value in the ﬂoating-point tensor.
KL-divergence is introduced to make a trade-oﬀ between
numericaldynamicrangeandprecisionoftheINT8represen-tation [173]. KL calibration can signiﬁcantly help to avoid
accuracy loss.
Themethodtraversesapredeﬁnedpossiblerangeofscales
andcalculates theKL-divergences forallthe points. Itthen
selects the scale which minimizes the KL-divergence. KL-
divergence is widely used in many post training acceler-
ation frameworks. nVidia found a model calibrated with
125 images showed only 0.36% top-1 accuracy loss using
GoogLeNet on the Imagenet dataset.
Intel MKL-DNN [204]isanoptimizedcomputinglibrary
forIntelprocessorswithIntelAVX-512,AVX-2,andSSE4.2
InstructionSetArchitectures(ISA).ThelibraryusesFP32for
trainingandinference. Inferencecanalsobeperformedusing
8-bits in convolutional layers, ReLU activations, and pool-
ing layers. It also uses Winograd convolutions. MKL-DNN
uses max-absquantization shown in Equation 15, where the
feature adopts unsigned 8-bit integer nf= 256and signed
8-bitintegerweights nw= 128. Theroundingfunction f./
in Equation 12 uses nearest integer rounding. Equation 32
shows the quantization applied on a given tensor or each
channel in a tensor. The maximum of weights Rwand fea-
turesRfis calculated from the maximum of the absolute
value(nearestintegerrounding)ofthetensor TfandTw. The
featurescale sfandweightsscale swaregeneratedusing Rw
andRf. Then quantized 8-bit signed integer weights Ws8,
8-bitunsignedintegerfeature Fu8and32-bitunsignedinte-
gerbiases Bu32aregeneratedusingthescalesandanearest
rounding function ññ.
R^f;w`=max..abs.T^f;w`//
sf=255
Rf; sw=127
Rw
Ws8=ñswWf32ñË [*127;127]
Fu8=ñsfFf32ñË [0;255]
Bs32=ñsfswBf32ñË [*231;231* 1](32)
Anaﬃnetransformationusing8-bitmultipliersand32-
bit accumulates results in Equation 33 with the same scale
factorsasdeﬁnedinEquation32and <denotingconvolution.
T Liang et al.: Preprint submitted to Elsevier Page 22 of 41

--- PAGE 23 ---
Survey on pruning and quantization
It is an approximation since rounding is ignored.
Os32=Ws8<Fu8+bs32
ùsfsw Wf32<Ff32+bf32
=sfswOf32(33)
Equation34istheaﬃnetransformationwithFP32format.
Dis the dequantization factor.
Of32=Wf32<Ff32+bf32
ù1
sfswOs32=DOs32
whereD=1
sfsw(34)
Weightquantizationisdonepriortoinference. Activation
quantization factors are prepared by sampling the validation
datasettoﬁndasuitablerange(similartoTensorRT).The
quantizationfactorscanbeeither FP32inthesupportedde-
vices,orroundedtothenearestpower-of-twoformattoenable
bit-shifts. Rounding reduces accuracy by about 1%.
MKL-DNNassumesactivationsarenon-negative(ReLU
activated). LocalResponseNormalization(LRN),afunction
to pick the local maximum in a local distribution, is used
to avoid over-ﬁtting. BN, FCL, and soft-max using 8-bit
inference are not currently supported.
TensorFlow-Lite (TF-Lite) [1] is an open source frame-
workbyGoogleforperforminginferenceonmobileorem-
beddeddevices. Itconsistsoftwosetsoftoolsforconverting
andinterpretingquantizednetworks. BothPTQandQATare
available in TF-Lite.
GEMM low-precision (Gemmlowp) [78]isaGoogleopen
sourcegemmlibraryforlowprecisioncalculationsonmobile
and embedded devices. It is used in TF-Lite. Gemmlowp
uses asymmetric quantzation as shown in Equation 35 where
F;W;Odenotes feature, weights and output, respectively.
sf;sware the scales for feature and weights, respectively.
Ff32is Feature value in 32-bit ﬂoating. Similarly, Wf32is
the Weight value in 32-bit ﬂoating point. Fq;Wqare the
quantizedFeaturesandWeights,respectively. Asymmetric
quantization introduces the zero points ( zfandzw). This
produces a more accurate numerical encoding.
Of32=Ff32<Wf32
=sf .Fq+zf/ <sw .Wq+zw/
=sfsw .Fq+zf/ < .Wq+zk/(35)
The underlined part in Equation 35 is the most compu-
tationallyintensive. Inadditiontotheconvolution,thezero
point also requires calculation. Gemmlowp reduces many
multi-add operations by multiplying an all-ones matrix as
thebiasmatrixPandQin Equation 36. This allows four
multiplies to be dispatched in a three stage pipeline [ 131], toproduce the quantized output Oq.F;W;zare the same as in
Equation 35.
Oq= .Fq+zfP/ < .Wq+zwQ/
=Fq<Wq
+zfPWq
+zwQFq
+zfzwPQ(36)
Ristretto [90] is a tool for Caﬀe quantization. It uses re-
training to adjust the quantized parameters. Ristretto uses
a three-part quantization strategy: 1) a modiﬁed ﬁxed-point
formatDynamicFixedPoint(DFP)whichpermitsthelimited
bit-width precision to dynamically carry data, 2) bit-width
reducedﬂoating-pointnumberscalledminiﬂoatwhichfol-
lowstheIEEE-754standard[ 219],and3)integerpowerof
2 weights that force parameters into power of 2 values to
replace multiplies with bit shift operations.
DPF is shown in Equation 37 where stakes one sign bit,
FLdenotesthefractional length,and xisthemantissa. The
total bit-width is B. This quantization can encode data from
various ranges to a proper format by adjusting the fractional
length.
.*1/s2-FLB*2É
i=02ixi (37)
A bit shift convolution conversion is shown in Equa-
tion 38. The convolution by input Fjand weights Wjand
biasbiare transformed into shift arithmetic by rounding the
weights to the nearest power of 2 values. Power of 2 weights
provides inference acceleration while dynamic ﬁxed point
provides better accuracy.
Oi=É
jFjWj+bi
ùÉ
jFj~round log2 Wj+bi(38)
NCNN[229]isastandaloneframeworkfromTencentforef-
ﬁcientinferenceonmobiledevices. InspiredbyRistrettoand
Tensor-RT,itworkswithmultipleoperatingsystemsandsup-
ports low precision inference [ 28]. It performs channel-wise
quantization with KL calibration. The quantization results
in 0.04% top-1 accuracy loss on ILSVRC-2012. NCNN has
implementations optimized for ARM NEON. NCNN also
replaces 3  3convolutions with simpler Winograd convolu-
tions [135].
Mobile AI Compute Engine (MACE) [247]fromXiaomi
supports both post-training quantization and quantization-
aware training. Quantization-aware training is recommended
as it exhibits lower accuracy loss . Post-training quantiza-
tionrequiresstatisticalinformationfromactivationscollected
T Liang et al.: Preprint submitted to Elsevier Page 23 of 41

--- PAGE 24 ---
Survey on pruning and quantization
whileperforminginference. Thisistypicallyperformedwith
batch calibration of input data. MACE also supports proces-
sorimplementationsoptimizedforARMNEONandQual-
comm’s Hexagon digital signal processor. OpenCL accelera-
tionisalsosupported. Winogradconvolutionscanbeapplied
for further acceleration as discussed in Section 4.2.2.
Quantized Neural Network PACKage (QNNPACK) [61]
is a Facebook produced open-source library optimized for
edgecomputingespeciallyformobilelowprecisionneural
networkinference. Ithasthesamemethodofquantizationas
TF-Liteincludingusingazero-point. Thelibraryhasbeen
integratedintoPyTorch[ 193]toprovideusersahigh-level
interface. Inadditionto Winogradand FFTconvolution op-
erations, the library has optimized gemm for cache indexing
andfeaturepacking. QNNPACKhasafullcompiledsolution
for many mobile devices and has been deployed on millions
of devices with Facebook applications.
PanelDotproduct(PDOT)isakeyfeatureofQNNPACK’s
highly eﬃcient gemm library. It assumes computing eﬃ-
ciencyislimitedwithmemory,cache,andbandwidthinstead
of Multiply and Accumulate (MAC) performance. PDOT
computesmultipledotproductsinparallelasshowninFig-
ure 16. Rather than loading just two operands per MAC
operation, PDOT loads multiple columns and rows. This im-
proves convolution performance about 1:41 2:23speedup
for MobileNet on mobile devices [61].
= ×
= ×
Figure 16: PDOT: computing dot product for several points
in parallel.
Paddle[13]appliesbothQATandPTQquantizationwith
usingzero-points. Thedequantizationoperationcanbeper-
formedpriortoconvolutionasshowninEquation39. Pad-
dle uses this feature to do ﬂoating-point gemm-based con-
volutions with quantize-dequantized weights and features
withintheframeworkdata-path. Itintroducesquantization
errorwhile maintainingthedatain formatof ﬂoating-point.
Thisquantize-dequantize-convolutionpipelineiscalledsimu-
quantize and its results are approximately equal to a FP32-
>INT8->Convolutional->FP32 (quantize - convolutional -
dequantize) three stage model.
Simu-quantize maintains the data at each phase in 32-
bit ﬂoating-point facilitating backward propagation. In the
Paddle framework, during backpropagation, gradients are
addedtotheoriginal32-bitﬂoating-pointweightsratherthanthe quantized or the quantize-dequantized weights.
Of32= .Fq
.n* 1/Fmax/ < .Wq
.n* 1/Wmax/(39)
Paddle uses max-absin three ways to quantize parame-
ters: 1)theaverageofthemaxabsolutevalueinacalculation
window, 2) the max absolute value during a calculation win-
dow, and 3) a sliding average of the max absolute value of
thewindow. ThethirdmethodisdescribedinEquation40,
whereVis the max absolute value in the current batch, Vtis
theaveragevalueoftheslidingwindow,and kisacoeﬃcient
chosen by default as 0.9.
The Paddle framework uses a specialized toolset, Pad-
dleSlim, which supports Quantization, Pruning, Network
Architecture Search, and Knowledge Distilling. They found
86.47%sizereductionofResNet-50, with1.71%ILSVRC-
2012 top-1 accuracy loss.
Vt= .1 *k/ V+kVt*1 (40)
4.3.3. Hardware Platforms
Figure 17 shows AI chips, cards, and systems plotted
by peak operations verses power in log scale originally pub-
lished in [ 202]. Three normalizing lines are shown at 100
GOPS/Watt, 1 TOP/Watt, and 10 TOPs/Watt. Hardware plat-
formsareclassiﬁedalongseveraldimensionsincluding: 1)
trainingorinference,2)chip,card,orsystemformfactors,3)
datacenteror mobile,and 4)numerical precision. Wefocus
on low precision general and specialized hardware in this
section.
Programmable Hardware: Quantized networks with less
than 8-bits of precision are typically implemented in FPGAs
but may also be executed on general purpose processors.
BNN’s have been implemented on a Xilinx Zynq het-
erogeneousFPGAplatform[ 267]. Theyhavealsobeenim-
plemented on Intel Xeon CPUs and Intel Arria 10 FPGA
heterogeneous platformsby dispatching bitoperation to FP-
GAsandotheroperationstoCPUs[ 178]. Theheterogeneous
system shares the same memory address space. Training
is typically mapped to CPUs. FINN [ 231] is a specialized
framework for BNN inference on FPGAs. It contains bi-
narized fully connected, convolutional, and pooling layers.
When deployed on a Zynq-7000 SoC, FINN has achieved
12.36 million images per second on the MNIST dataset with
4.17% accuracy loss.
Binarized weights with 3-bit features have been imple-
mented on Xilinx Zynq FPGAs and Arm NEON processors
[196]. Theﬁrstandlastlayerofthenetworkuse8-bitquanti-
tiesbutallotherlayersusebinaryweightsand3-bitactivation
values. On an embedded platform, Zynq XCZU3EG, they
performed 16 images per second for inference. To accel-
erate Tiny-YOLO inference, signiﬁcant eﬀorts were taken
including: 1)replacingmax-poolwithstride2convolution,
2) replacing leaky ReLU with ReLU, and 3) revising the hid-
den layer output channel. The improved eﬃciency on the
T Liang et al.: Preprint submitted to Elsevier Page 24 of 41

--- PAGE 25 ---
Survey on pruning and quantization
Table 3
Low Precision Libraries versus Accuracy for Common Networks in Multiple Frameworks.
Name Framework MethodAccuracy Float Accuracy Quant Accuracy Diﬀ
Top-1 Top-5 Top-1 Top-5 Top-1 Top-5
AlexNet TensorRT [173] PTQ, w/o oﬀset 57.08% 80.06% 57.05% 80.06% -0.03% 0.00%
Ristretto [90] Dynamic FP 56.90% 80.09% 56.14% 79.50% -0.76% -0.59%
Ristretto [90] Miniﬂoat 56.90% 80.09% 52.26% 78.23% -4.64% -1.86%
Ristretto [90] Pow-of-two 56.90% 80.09% 53.57% 78.25% -3.33% -1.84%
GoogleNet NCNN [28] PTQ, w/o oﬀset 68.50% 88.84% 68.62% 88.68% 0.12% -0.16%
TensorRT [173] PTQ, w/o oﬀset 68.57% 88.83% 68.12% 88.64% -0.45% -0.19%
Ristretto [90] Dynamic FP 68.93% 89.16% 68.37% 88.63% -0.56% -0.53%
Ristretto [90] Miniﬂoat 68.93% 89.16% 64.02% 87.69% -4.91% -1.47%
Ristretto [90] Pow-of-two 68.93% 89.16% 57.63% 81.38% -11.30% -7.78%
Inception v3 TF-Lite [77] PTQ 78.00% 93.80% 77.20% - -0.80% -
TF-Lite [77] QAT 78.00% 93.80% 77.50% 93.70% -0.50% -0.10%
MobileNet v1 NCNN [28] PTQ, w/o oﬀset 67.26% 87.92% 66.74% 87.43% -0.52% -0.49%
Paddle [13] QAT+Pruning 70.91% - 69.20% - -1.71% -
TF-Lite [77] PTQ 70.90% - 65.70% - -5.20% -
TF-Lite [77] QAT 70.90% - 70.00% - -0.90% -
MobileNet v2 QNNPACK [61] PTQ, w/ oﬀset 71.90% - 72.14% - 0.24% -
TF-Lite [77] PTQ 71.90% - 63.70% - -8.20% -
TF-Lite [77] QAT 71.90% - 70.90% - -1.00% -
ResNet-101 TensorRT [173] PTQ, w/o oﬀset 74.39% 91.78% 74.40% 91.73% 0.01% -0.05%
TF-Lite [77] PTQ 77.00% - 76.80% - -0.20% -
ResNet-152 TensorRT [173] PTQ, w/o oﬀset 74.78% 91.82% 74.70% 91.78% -0.08% -0.04%
ResNet-18 NCNN [28] PTQ, w/o oﬀset 65.49% 86.56% 65.30% 86.52% -0.19% -0.04%
ResNet-50 NCNN [28] PTQ, w/o oﬀset 71.80% 89.90% 71.76% 90.06% -0.04% 0.16%
TensorRT [173] PTQ, w/o oﬀset 73.23% 91.18% 73.10% 91.06% -0.13% -0.12%
SqueezeNet NCNN [28] PTQ, w/o oﬀset 57.78% 79.88% 57.82% 79.84% 0.04% -0.04%
Ristretto [90] Dynamic FP 57.68% 80.37% 57.21% 79.99% -0.47% -0.38%
Ristretto [90] Miniﬂoat 57.68% 80.37% 54.80% 78.28% -2.88% -2.09%
Ristretto [90] Pow-of-two 57.68% 80.37% 41.60% 67.37% -16.08% -13.00%
VGG-19 TensorRT [173] PTQ, w/o oﬀset 68.41% 88.78% 68.38% 88.70% -0.03% -0.08%
FPGAfrom2.5to5framespersecondwith1.3%accuracy
loss.
TNN [6] is deployed on an FPGA with specialized com-
putationunitsoptimizedforternaryvaluemultiplication. A
speciﬁcFPGA structure(dimensions)isdetermined during
synthesistoimprovehardwareeﬃciency. OntheSakura-X
FPGA board they achieved 255k MNIST image classiﬁca-
tions per second with an accuracy of 98.14%. A scalable de-
signimplementedona XilinxVirtex-7 VC709boarddramat-
icallyreducedhardwareresourcesandpowerconsumption
butatasigniﬁcantlyreducedthroughputof27kCIFAR-10
images per second [ 197]. Power consumption for CIFAR-10
was 6.8 Watts.
Reducinghardwarecostsisakeyobjectiveoflogarithmic
hardware. Xu [ 249] adoptedù
2based logarithmic quanti-
zation with 5-bits of resolution. This showed 50.8% top-1
accuracyanddissipatedaquarterofthepowerwhileusing
half the chip area. Half precision inference has a top-1 accu-
racy of 53.8%.General Hardware: In addition to specialized hardware,
INT8 quantization has been widely adopted in many general
purpose processor architectures. In this section we provide a
high-leveloverview. Adetailedsurveyonhardwareeﬃciency
for processing DNNs can be found in [202].
CNN acceleration on ARM CPUs was originally im-
plemented by ARM advanced SIMD extensions known as
NEON.TheARM8.2ISAextensionaddedNEONsupport
for 8-bit integer matrix operations [ 8]. These were imple-
mented in the CPU IP cores Cortex-A75 and A55 [ 9] as well
as the Mali-G76 GPU IP core [ 10]. These cores have been
integratedintotheKirinSoCbyHuawei,QualcommSnap-
dragon SoC, MediaTek Helio SoC, and Samsung Exynos
[116]. ForexampleonExynos9825Octa,8-bitintegerquan-
tizedMobileNetv2canprocessanimagein19ms(52images
per second) using the Mali-G76 [116].
Intelimprovedtheintegerperformanceabout33%with
Intel Advanced Vector Extension 512 (AVX-512) ISA [ 204].
This512-bitSIMDISAextensionincludedaFusedMultiply-
T Liang et al.: Preprint submitted to Elsevier Page 25 of 41

--- PAGE 26 ---
Survey on pruning and quantization
Presentation Name -26of Author Initials  MM/DD/YY
Peak Power (W)Neural Network Processing PerformancePeak GOps/Second
Slide courtesy of Albert Reuther, MIT Lincoln Laboratory Supercomputing CenterComputation Precision
ChipCardSystemForm FactorInferenceTrainingComputation TypeLegendInt8Int8 -> Int16Float16Float16 -> Float32Float32Float64Int1Int2Int12 -> Int16Int16Int32
1 TeraOps/W10 TeraOps/W
100 GigaOps/WDGX-1
MIT EyerissMovidiusXJetsonTX1JetsonTX2XavierDGX-StationDGX-2WaveSystemWaveDPU
TrueNorthSysGraphCoreNodeGraphCoreC2K80P100V100
2xSkyLakeSPPhi7210FPhi7290FArriaGX1150NervanaGoyaTPU3TPU1TPU2Turing
TPUEdgeTrueNorthZynq-020ArriaGX1155
Zynq-020XilinxClusterZCU102AIStormCambriconCambriconBaidu
RockchipDianNaoDaDianNaoShiDianNaoPuDianNaoZynq-020S835A12Mali-76Mali-75S845Stratix-VArriaGX1150ArriaGX1150Nervana2
Zynq-060ArriaGX1150ArriaGX1150AMD-MI6AMD-MI60Very Low PowerCell GPUsMobileFPGAsData Center SystemsData Center Chips & Cards
Figure 17: Hardware platforms for neural networks eﬃciency deploy, adopted from [202].
Add (FMA) instruction.
LowprecisioncomputationonnVidiaGPUswasenabled
sincethePascalseriesofGPUs[ 184]. TheTuringGPUarchi-
tecture [188] introduced specialized units to processes INT4
andINT8. Thisprovidesreal-timeintegerperformanceonAI
algorithmsusedingames. Forembeddedplatforms,nVidia
developedJetsonplatforms[ 187]. TheyuseCUDAMaxwell
cores[183]thatcanprocesshalf-precisiontypes. Forthedata
center, nVidia developed the extremely high performance
DGX system [ 185]. It contains multiple high-end GPUs
interconnected using nVidia’s proprietary bus nVLINK. A
DGXsystemcanperform4-bitintegerto32-bitﬂoatingpoint
operations.
4.3.4. DNN Compilers
Heterogeneous neural networks hardware accelerators
areacceleratingdeeplearningalgorithmdeployment[ 202].
Oftenexchangeformatscanbeusedtoimport/exportmodels.
Further, compilers have been developed to optimize models
andgeneratecodeforspeciﬁcprocessors. Howeverseveral
challenges remain:
•Network Parsing: Developers design neural network
models on diﬀerent platforms using various frame-
works and programming languages. However, they
have common parts, such as convolution, activation,
pooling,etc. Parsingtoolsanalyzethemodelcomposi-
tions and transfer them into the uniﬁed representation.
•StructureOptimization: Themodelmaycontainopera-
tions used in training that aren’t required for inference.
Tool-kits and compilers should optimize these struc-
tures (e.g. BN folding as discussed in Section 2.5).
•IntermediateRepresentation(IR):Anoptimizedmodelshouldbeproperlystoredforfurtherdeployment. Since
the inference engine is uncertain, the stored IR should
include the model architecture and the trained weights.
Acompilercanthenreadthemodelandoptimizeitfor
a speciﬁc inference engine.
•Compression: Compilers and optimizers should op-
tionally be able to automatically compress arbitrary
network structures using pruning and quantization.
•Deployment: The ﬁnal optimized model should be
mapped
to the target engine(s) which may be heterogeneous.
Open Neural Network Exchange (ONNX) [ 190] is an
open-source tool to parse AI models written for a variety
diverse frameworks. It imports and exports models using
an open-source format facilitating the translation of neural
network models between frameworks. It is thus capable of
network parsing provided low-level operations are deﬁned in
all target frameworks.
TVM [36], Glow [ 205], OpenVINO [ 118], and MLIR
[134]aredeeplearningcompilers. Theydiﬀerfromframe-
works such as Caﬀe in that they store intermediate repre-
sentations and optimize those to map models onto speciﬁc
hardwareengines. Theytypicallyintegratebothquantization-
awaretrainingandcalibration-basedpost-trainingquantiza-
tion. Wesummarizekeyfeaturesbelow. Theyperformallthe
operationsnotedinourlist. Adetailedsurveycanbefound
in [149].
TVM [36] leverages the eﬃciency of quantization by
enabling deployment of quantized models from PyTorch and
TF-Lite. As a compiler, TVM has the ability to map the
modelongeneralhardwaresuchasIntel’sAVXandnVidia’s
CUDA.
T Liang et al.: Preprint submitted to Elsevier Page 26 of 41

--- PAGE 27 ---
Survey on pruning and quantization
Glow [205] enables quantization with zero points and
convertsthedatainto8-bitsignedintegersusingacalibration-
based method. Neither Glow or TVM currently support
quantization-aware training although they both announced
future support for it [205].
MLIR [134] and OpenVINO [ 118] have sophisticated
quantizationsupportincludingquantization-awaretraining.
OpenVINOintegratesitinTensorFlowandPyTorchwhile
MLIR natively supports quantization-aware training. This
allows users to ﬁne-tune an optimized model when it doesn’t
satisfy accuracy criteria.
4.4. Quantization Reduces Over-ﬁtting
In addition to accelerating neural networks, quantization
has also been found in some cases to result in higher accu-
racy. As examples: 1)3-bitweightsVGG-16outperformsits
fullprecisioncounterpartby1.1%top-1[ 144],2)AlexNetre-
duces1.0%top-1errorofthereferencewith2-bitweightsand
8-bitactivations[ 66],3)ResNet-34with4-bitweightsandac-
tivationobtained74.52%top-1accuracywhilethe32-bitver-
sion is 73.59% [ 174], 4) Zhou showed a quantized model re-
ducedtheclassiﬁcationerrorby0.15%,2.28%,0.13%,0.71%,
and1.59%onAlexNet,VGG-16,GoogLeNet,ResNet-18and
ResNet-50, respectively [ 269], and 5) Xu showed reduced
bit quantized networks help to reduce over-ﬁtting on Fully
Connected Networks (FCNs). By taking advantage of strict
constraints in biomedical image segmentation they improved
segmentationaccuracyby1%combinedwitha 6:4memory
usage reduction [251].
5. Summary
InthissectionwesummarizetheresultsofPruningand
Quantization.
5.1. Pruning
Section 3 shows pruning is an important technique for
compressing neural networks. In this paper, we discussed
pruning techniques categorized as 1) static pruning and 2)
dynamic pruning. Previously, static pruning was the domi-
nantareaofresearch. Recently,dynamicpruninghasbecome
afocusbecauseitcanfurtherimproveperformanceevenif
static pruning has ﬁrst been performed.
Pruning can be performed in multiple ways. Element-
wise pruning improves weight compression and storage.
Channel-wise and shape-wise pruning can be accelerated
withspecialized hardwareandcomputation libraries. Filter-
wise and layer-wise pruning can dramatically reduce compu-
tational complexity.
Though pruning sometimes introduces incremental im-
provementinaccuracybyescapingalocalminima[ 12],ac-
curacy improvements are better realized by switching to a
better network architecture [ 24]. For example, a separable
block may provide better accuracy with reduced computa-
tional complexity [ 105]. Considering the evolution of net-
work structures, performance may also be bottlenecked by
the structure itself. From this point of view, Network Archi-
tecture Search and Knowledge Distillation can be options forfurther compression. Network pruning can be viewed as a
subset of NAS but with a smaller searching space. This is
especially true when the pruned architecture no longer needs
to use weights fromtheunprunednetwork (see Section 3.3).
In addition, some NAS techniques can also be applied to the
pruning approach including borrowing trained coeﬃcients
and reinforcement learning search.
Typically, compression is evaluated on large data-sets
such as the ILSVRC-2012 dataset with one thousand object
categories. In practice, resource constraints in embedded
devicesdon’tallowalargecapacityofoptimizednetworks.
Compressingamodeltobestﬁtaconstrainedenvironment
should consider but not be limited to the deployment envi-
ronment, target device, speed/compression trade-oﬀs, and
accuracy requirements [29].
Based on the reviewed pruning techniques, we recom-
mend the following for eﬀective pruning:
•Uniform pruning introduces accuracy loss therefore
setting the pruning ratio to vary by layers is better
[159].
•Dynamic pruning may result in higher accuracy and
maintain higher network capacity [246].
•Structurallypruninganetworkmaybeneﬁtfromma-
turinglibrariesespeciallywhenpruningatahighlevel
[241].
•Training a pruned model from scratch sometimes, but
not always (see Section 3.3), is more eﬃcient than
tunning from the unpruned weights [160].
•Penalty-based pruning typically reduces accuracy loss
compared with magnitude-based pruning [ 255]. How-
ever, recent eﬀorts are narrowing the gap [72].
5.2. Quantization
Section 4 discusses quantization techniques. It describes
binarizedquantizedneuralnetworks,andreducedprecision
networks,alongwiththeirtrainingmethods. Wedescribed
low-bit dataset validation techniques and results. We also
list the accuracy of popular quantization frameworks and
described hardware implementations in Section 4.3.
Quantization usually results in a loss of accuracy due
toinformationlostduringthequantizationprocess. Thisis
particularly evident on compact networks. Most of the early
lowbitquantizationapproachesonlycompareperformance
onsmalldatasets(e.g.,MNIST,andCIFAR-10)[ 58,94,156,
200,235,269]. However, observations showed that some
quantized networks could outperform the original network
(see: Section 4.4). Additionally, non-uniform distribution
data may lead to further deterioration in quantization per-
formance[ 275]. Sometimesthiscanbeamelioratedbynor-
malization in ﬁne-tuning [ 172] or by non-linear quantization
(e.g., log representation) [175].
Advancedquantizationtechniqueshaveimprovedaccu-
racy. Asymmetric quantization [ 120] maintains higher dy-
namic range by using a zero point in addition to a regular
T Liang et al.: Preprint submitted to Elsevier Page 27 of 41

--- PAGE 28 ---
Survey on pruning and quantization
scaleparameter. Overheadsintroducedbythezeropointwere
minimized by pipelining the processing unit. Calibration
basedquantization[ 173]removedzeropointsandreplaced
them with precise scales obtained from a calibrating dataset.
Quantization-aware training was shown to further improve
quantization accuracy.
8-bit quantization is widely applied in practice as a good
trade-oﬀbetweenaccuracyandcompression. Itcaneasilybe
deployed on current processors and custom hardware. Mini-
malaccuracylossisexperiencedespeciallywhenquantization-
aware training is enabled. Binarized networks have also
achieved reasonable accuracy with specialized hardware de-
signs.
Though BN has advantages to help training and prun-
ing,anissuewithBNisthatitmayrequirealargedynamic
rangeacrossasinglelayerkernelorbetweendiﬀerentchan-
nels. Thismaymakelayer-wisequantizationmorediﬃcult.
Because of this per channel quantization is recommended
[131].
To achieve better accuracy following quantization, we
recommend:
•Use asymmetrical quantization. It preserves ﬂexibility
over the quantization range even though it has compu-
tational overheads [120].
•Quantize the weights rather than the activations. Acti-
vation is more sensitive to numerical precision [75].
•Donotquantizebiases. Theydonotrequiresigniﬁcant
storage. Highprecisionbiasesinalllayers[ 114],and
ﬁrst/last layers [ 200,272], maintain higher network
accuracy.
•Quantizekernelschannel-wiseinsteadoflayer-wiseto
signiﬁcantly improve accuracy [131].
•Fine-tunethequantizedmodel. Itreducestheaccuracy
gap betweenthe quantized modeland thereal-valued
model [244].
•Initiallytrainusinga32-bitﬂoatingpointmodel. Low-
bitquantizedmodelcanbediﬃculttotrainfromscratch
- especially compact models on large-scaled data-sets
[272].
•The sensitivity of quantization is ordered as gradients,
activations, and then weights [272].
•Stochastic quantization of gradients is necessary when
training quantized models [89, 272].
6. Future Work
Althoughpunningandquantizationalgorithmshelpre-
ducethecomputationcostandbandwidthburden,thereare
stillareasforimprovement. Inthissectionwehighlightfuture
work to further improvement quantization and prunning.
Automatic Compression. Low bit width quantization
cancausesigniﬁcantaccuracyloss,especiallywhenthequan-
tizedbit-widthisverynarrowandthedatasetislarge[ 272,155]. Automatic quantization is a technique to automatically
searchquantizationencodingtoevaluateaccuracylossverses
compressionratio. Similarly,automaticprunningisatech-
nique to automatically search diﬀerent prunning approaches
toevaluatethesparsityratioversusaccuracy. Similartohy-
perparameter tuning [ 257], this can be performed without
humaninterventionusinganynumberofsearchtechniques
(e.g. random search, genetic search, etc.).
CompressiononOtherTypesofNeuralNetworks. Cur-
rent compression research is primarily focused on CNNs.
Morespeciﬁcally,researchisprimarilydirectedtowardsCNN
classiﬁcationtasks. Futureworkshouldalsoconsiderother
typesofapplicationssuchasobjectdetection,speechrecogni-
tion,languagetranslation,etc. Networkcompressionverses
accuracy for diﬀerent applications is an interesting area of
research.
HardwareAdaptation. Hardwareimplementationsmay
limittheeﬀectivenessofpruningalgorithms. Forexample,
element-wise pruning only slightly reduces computations or
bandwidth when using im2col-gemm on GPU [ 264]. Sim-
ilarly, shape-wise pruning is not typically able to be imple-
mented on dedicated CNN accelerators. Hardware-software
co-design of compression techniques for hardware acceler-
ators should be considered to achieve the best system eﬃ-
ciency.
Global Methods. Network optimizations are typically
applied separately without information from one optimiza-
tion informing any other optimization. Recently, approaches
that consider optimization eﬀectiveness at multiple layers
have been proposed. [ 150] discusses pruning combined with
tensor factorization that results in better overall compression.
Similar techniques can be considered using diﬀerent types
and levels of compression and factorization.
7. Conclusions
Deepneuralnetworkshavebeenappliedinmanyapplica-
tionsexhibitingextraordinaryabilitiesintheﬁeldofcomputer
vision. However, complexnetworkarchitectureschallenge
eﬃcientreal-timedeploymentandrequiresigniﬁcantcompu-
tationresourcesandenergycosts. Thesechallengescanbe
overcomethroughoptimizationssuchasnetworkcompres-
sion. Networkcompressioncanoftenberealizedwithlittle
loss of accuracy. In some cases accuracy may even improve.
Pruning can be categorized as static (Section 3.1) if it is
performed oﬄine or dynamic (Section 3.2) if it is performed
atrun-time. Thecriteriaappliedtoremovingredundantcom-
putations if often just a simple magnitude of weights with
valuesnearzerobeingpruned. Morecomplicatedmethods
includecheckingthe lp-norm. TechniquessuchasLASSO
and Ridge are built around l1andl2norms. Pruning can
be performed element-wise, channel-wise, shape-wise, ﬁlter-
wise, layer-wise and even network-wise. Each has trade-oﬀs
in compression, accuracy, and speedup.
Quantizationreducescomputationsbyreducingthepreci-
sion of the datatype. Most networks are trained using 32-bit
ﬂoatingpoint. Weights,biases,andactivationsmaythenbe
T Liang et al.: Preprint submitted to Elsevier Page 28 of 41

--- PAGE 29 ---
Survey on pruning and quantization
quantizedtypicallyto8-bitintegers. Lowerbitwidthquan-
tizationshavebeenperformedwithsinglebitbeingtermed
a binary neural network. It is diﬃcult to (re)train very low
bitwidthneuralnetworks. Asinglebitisnotdiﬀerentiable
therebyprohibitingbackpropagation. Lowerbitwidthscause
diﬃcultiesfor computinggradients. Theadvantageof quan-
tization is signiﬁcantly improved performance (usually 2-3x)
anddramaticallyreducedstoragerequirements. Inaddition
todescribinghowquantizationisperformedwealsoincluded
anoverviewofpopularlibrariesandframeworksthatsupport
quantization. We further provided a comparison of accuracy
foranumberofnetworksusingdiﬀerentframeworksTable2.
Inthispaper,wesummarizedpruningandquantization
techniques. Pruning removes redundant computations that
have limited contribution to a result. Quantization reduces
computationsbyreducingtheprecisionofthedatatype. Both
canbeusedindependentlyorincombinationtoreducestorage
requirements and accelerate inference.
T Liang et al.: Preprint submitted to Elsevier Page 29 of 41

--- PAGE 30 ---
Survey on pruning and quantization
8. Quantization Performance Results
Table 4: Quantization Network Performance on
ILSVRC2012 for various bit-widths of the weights W
and activation A (aka. feature)
Model DeploymentBit-width Acc. DropRef.W A Top-1 Top-5
AlexNet QuantNet 1 32 -1.70% -1.50% [253]
BWNH 1 32 -1.40% -0.70% [107]
SYQ 2 8 -1.00% -0.60% [66]
TSQ 2 2 -0.90% -0.30% [239]
INQ 5 32 -0.87% -1.39% [269]
PACT 4 3 -0.60% -1.00% [44]
QIL 4 4 -0.20% - [127]
Mixed-Precision 16 16 -0.16% - [172]
PACT 32 5 -0.10% -0.20% [44]
QIL 5 5 -0.10% - [127]
QuantNet 3( ±4) 32 -0.10% -0.10% [253]
ELNN 3( ±4) 32 0.00% 0.20% [144]
DoReFa-Net 32 3 0.00% -0.90% [272]
TensorRT 8 8 0.03% 0.00% [173]
PACT 2 2 0.10% -0.70% [44]
PACT 32 2 0.20% -0.20% [44]
DoReFa-Net 32 5 0.20% -0.50% [272]
QuantNet 3( ±2) 32 0.30% 0.00% [253]
DoReFa-Net 32 4 0.30% -0.50% [272]
WRPN 2 32 0.40% - [174]
DFP16 16 16 0.49% 0.59% [54]
PACT 3 2 0.50% -0.10% [44]
PACT 4 2 0.50% -0.10% [44]
SYQ 1 8 0.50% 0.80% [66]
QIL 3 3 0.50% - [127]
FP8 8 8 0.50% - [237]
BalancedQ 32 2 0.60% -2.00% [273]
ELNN 3( ±2) 32 0.80% 0.60% [144]
SYQ 1 4 0.90% 0.80% [66]
QuantNet 2 32 0.90% 0.30% [253]
FFN 2 32 1.00% 0.30% [238]
DoReFa-Net 32 2 1.00% 0.10% [272]
Uniﬁed INT8 8 8 1.00% - [275]
DeepShift-PS 6 32 1.19% 0.67% [62]
WEQ 4 4 1.20% 1.00% [192]
LQ-NETs 2 32 1.30% 0.80% [262]
SYQ 2 2 1.30% 1.00% [66]
LQ-NETs 1 2 1.40% 1.40% [262]
BalancedQ 2 2 1.40% -1.00% [273]
WRPN-2x 8 8 1.50% - [174]
DoReFa-Net 1 4 1.50% - [272]
DeepShift-Q 6 32 1.55% 0.81% [62]
WRPN-2x 32 8 1.60% - [174]
WEQ 3 4 1.60% 1.10% [192]
WRPN-2x 8 4 1.70% - [174]
WRPN-2x 4 8 1.70% - [174]
SYQ 1 2 1.70% 1.60% [66]
ELNN 2 32 1.80% 1.80% [144]
WRPN-2x 4 4 1.90% - [174]
WRPN-2x 32 4 1.90% - [174]
GoogLeNet Mixed-Precision 16 16 -0.10% - [172]
DeepShift-PS 6 32 -0.09% -0.09% [62]
DFP16 16 16 -0.08% 0.00% [54]
AngleEye 16 16 0.05% 0.45% [85]
AngleEye 16 16 0.05% 0.45% [85]
ShiftCNN 3 4 0.05% 0.09% [84]
DeepShift-Q 6 32 0.27% 0.29% [62]
LogQuant 32 6 0.36% 0.28% [30]
ShiftCNN 2 4 0.39% 0.29% [84]
TensorRT 8 8 0.45% 0.19% [173]
LogQuant 6 32 0.64% 0.67% [30]
INQ 5 32 0.76% 0.25% [269]
ELNN 3( ±4) 32 2.40% 1.40% [144]
ELNN 3( ±2) 32 2.80% 1.60% [144]
LogQuant 6 6 3.43% 0.78% [30]
QNN 4 4 5.10% 7.80% [113]
QNN 6 6 5.20% 8.10% [113]
ELNN 2 32 5.60% 3.50% [144]
BWN 1 32 5.80% 4.80% [200]
AngleEye 8 8 6.00% 3.20% [85]
TWN 2 32 7.50% 4.80% [146]
ELNN 1 32 8.40% 5.70% [144]
BWN 2 32 9.70% 6.50% [200]Model Deployment W A Top-1 Top-5 Ref.
ShiftCNN 1 4 11.26% 7.36% [84]
LogQuant 32 3 13.50% 8.93% [30]
LogQuant 3 3 18.07% 12.85% [30]
LogQuant 4 4 18.57% 13.21% [30]
LogQuant 32 4 18.57% 13.21% [30]
BNN 1 1 24.20% 20.90% [53]
AngleEye 6 6 52.10% 57.35% [85]
MobileNet HAQ-Cloud 6 6 -0.38% -0.23% [236]
V1 HAQ-Edge 6 6 -0.38% -0.34% [236]
MelinusNet59 1 1 -0.10% - [21]
HAQ-Edge 5 5 0.24% 0.08% [236]
PACT 6 6 0.36% 0.26% [44]
PACT 6 6 0.36% 0.26% [44]
HAQ-Cloud 5 5 0.85% 0.48% [236]
HAQ-Edge 4 4 3.42% 1.95% [236]
PACT 5 5 3.82% 2.20% [44]
PACT 5 5 3.82% 2.20% [44]
HAQ-Cloud 4 4 5.49% 3.25% [236]
PACT 4 4 8.38% 5.66% [44]
MobileNet HAQ-Edge 6 6 -0.08% -0.11% [236]
V2 HAQ-Cloud 6 6 -0.04% 0.01% [236]
Uniﬁed INT8 8 8 0.00% - [275]
PACT 6 6 0.56% 0.25% [44]
HAQ-Edge 5 5 0.91% 0.34% [236]
HAQ-Cloud 5 5 2.36% 1.31% [236]
PACT 5 5 2.97% 1.67% [44]
HAQ-Cloud 4 4 4.80% 2.79% [236]
HAQ-Edge 4 4 4.82% 2.92% [236]
PACT 4 4 10.42% 6.53% [44]
ResNet-18 RangeBN 8 8 -0.60% - [15]
LBM 8 8 -0.60% - [268]
QuantNet 5 32 -0.30% -0.10% [253]
QIL 5 5 -0.20% - [127]
QuantNet 3( ±4) 32 -0.10% -0.10% [253]
ShiftCNN 3 4 0.03% 0.12% [84]
LQ-NETs 4 32 0.20% 0.50% [262]
QIL 3 32 0.30% 0.30% [127]
LPBN 32 5 0.30% 0.40% [31]
QuantNet 3( ±2) 32 0.40% 0.20% [253]
PACT 32 4 0.40% 0.30% [44]
SeerNet 4 1 0.42% 0.18% [32]
ShiftCNN 2 4 0.54% 0.34% [84]
PACT 5 5 0.60% 0.30% [44]
INQ 4 32 0.62% 0.10% [269]
Uniﬁed INT8 8 8 0.63% - [275]
QIL 5 5 0.80% - [127]
LQ-NETs 3( ±4) 32 0.90% 0.80% [262]
QIL 3 3 1.00% - [127]
DeepShift-Q 6 32 1.09% 0.47% [62]
ELNN 3( ±2) 32 1.10% 0.70% [144]
PACT 32 3 1.20% 0.70% [44]
PACT 4 4 1.20% 0.60% [44]
QuantNet 2 32 1.20% 0.60% [253]
ELNN 3( ±4) 32 1.30% 0.60% [144]
DeepShift-PS 6 32 1.44% 0.67% [62]
ABC-Net 5 32 1.46% 1.18% [155]
ELNN 3( ±2) 32 1.60% 1.10% [144]
DoReFa-Net 32 5 1.70% 1.00% [272]
SYQ 2 8 1.90% 1.40% [66]
DoReFa-Net 32 4 1.90% 1.10% [272]
LQ-NETs 3 3 2.00% 1.60% [262]
DoReFa-Net 5 5 2.00% 1.30% [272]
ELNN 2 32 2.10% 1.50% [144]
QIL 2 32 2.10% 1.30% [127]
DoReFa-Net 32 3 2.10% 1.40% [272]
QIL 4 4 2.20% - [127]
LQ-NETs 2 32 2.20% 1.60% [262]
GroupNet-8 1 1 2.20% 1.40% [276]
PACT 3 3 2.30% 1.40% [44]
DoReFa-Net 4 4 2.30% 1.50% [272]
TTN 2 32 2.50% 1.80% [274]
TTQ 2 32 2.70% 2.00% [277]
AddNN 32 32 2.80% 1.50% [35]
ELNN 2 32 2.80% 1.50% [144]
LPBN 32 4 2.90% 1.70% [31]
PACT 32 2 2.90% 2.00% [44]
DoReFa-Net 3 3 2.90% 2.00% [272]
QuantNet 1 32 3.10% 1.90% [253]
INQ 2 32 3.10% 1.90% [269]
ResNet-34 WRPN-2x 4 4 -0.93% - [174]
WRPN-2x 4 8 -0.89% - [174]
T Liang et al.: Preprint submitted to Elsevier Page 30 of 41

--- PAGE 31 ---
Survey on pruning and quantization
Model Deployment W A Top-1 Top-5 Ref.
QIL 4 4 0.00% - [127]
QIL 5 5 0.00% - [127]
WRPN-2x 4 2 0.01% - [174]
WRPN-2x 2 4 0.09% - [174]
WRPN-2x 2 2 0.27% - [174]
SeerNet 4 1 0.35% 0.17% [32]
Uniﬁed INT8 8 8 0.39% - [275]
LCCL 0.43% 0.17% [59]
QIL 3 3 0.60% - [127]
WRPN-3x 1 1 0.90% - [174]
WRPN-3x 1 1 1.21% - [174]
GroupNet-8 1 1 1.40% 1.00% [276]
dLAC 2 16 1.67% 0.89% [235]
LQ-NETs 3 3 1.90% 1.20% [262]
GroupNet**-5 1 1 2.70% 2.10% [276]
IR-Net 1 32 2.90% 1.80% [199]
QIL 2 2 3.10% - [127]
WRPN-2x 1 1 3.40% - [174]
WRPN-2x 1 1 3.74% - [174]
LQ-NETs 2 2 4.00% 2.30% [262]
GroupNet-5 1 1 4.70% 3.40% [276]
ABC-Net 5 5 4.90% 3.10% [155]
HWGQ 1 32 5.10% 3.40% [31]
WAGEUBN 8 8 5.18% - [254]
ABC-Net 3 3 6.60% 3.90% [155]
LQ-NETs 1 2 6.70% 4.40% [262]
LQ-NETs 4 4 6.70% 4.40% [262]
BCGD 1 4 7.60% 4.70% [256]
HWGQ 1 2 9.00% 5.60% [31]
IR-Net 1 1 9.50% 6.20% [199]
CI-BCNN (add) 1 1 11.07% 6.39% [240]
Bi-Real 1 1 11.10% 7.40% [252]
WRPN-1x 1 1 12.80% - [174]
WRPN 1 1 13.05% - [174]
CI-BCNN 1 1 13.59% 8.65% [240]
DoReFa-Net 1 4 14.60% - [272]
DoReFa-Net 1 2 20.40% - [272]
ABC-Net 1 1 20.90% 14.80% [155]
BNN 1 1 29.10% 24.20% [272]
ResNet-50 Mixed-Precision 16 16 -0.12% - [172]
DFP16 16 16 -0.07% -0.06% [54]
QuantNet 5 32 0.00% 0.00% [253]
LQ-NETs 4 32 0.00% 0.10% [262]
FGQ 32 32 0.00% - [170]
TensorRT 8 8 0.13% 0.12% [173]
PACT 5 5 0.20% -0.20% [44]
QuantNet 3( ±4) 32 0.20% 0.00% [253]
Uniﬁed INT8 8 8 0.26% - [275]
ShiftCNN 3 4 0.29% 0.15% [84]
ShiftCNN 3 4 0.31% 0.16% [84]
PACT 4 4 0.40% -0.10% [44]
LPBN 32 5 0.40% 0.40% [81]
ShiftCNN 2 4 0.67% 0.41% [84]
DeepShift-Q 6 32 0.81% 0.21% [62]
DeepShift-PS 6 32 0.84% 0.31% [62]
PACT 5 32 0.90% 0.20% [44]
QuantNet 3( ±2) 32 0.90% 0.40% [253]
PACT 4 32 1.00% 0.20% [44]
dLAC 2 16 1.20% - [235]
QuantNet 2 32 1.20% 0.60% [253]
AddNN 32 32 1.30% 1.20% [35]
LQ-NETs 4 4 1.30% 0.80% [262]
LQ-NETs 2 32 1.30% 0.90% [262]
INQ 5 32 1.32% 0.41% [269]
PACT 3 32 1.40% 0.50% [44]
IAO 8 8 1.50% - [120]
PACT 3 3 1.60% 0.50% [44]
HAQ 2MP 4MP 1.91% - [236]
HAQ MP MP 2.09% - [236]
LQ-NETs 3 3 2.20% 1.60% [262]
LPBN 32 4 2.20% 1.20% [81]
Deep Comp. 3 MP 2.29% - [92]
PACT 4 2 2.40% 1.20% [44]
ShiftCNN 2 4 2.49% 1.64% [84]
FFN 2 32 2.50% 1.30% [238]
UNIQ 4 8 2.60% - [18]
QuantNet 1 32 3.20% 1.70% [253]
SYQ 2 8 3.70% 2.10% [66]
FGQ-TWN 2 8 4.29% - [170]
PACT 2 2 4.70% 2.60% [44]
LQ-NETs 2 2 4.90% 2.90% [262]Model Deployment W A Top-1 Top-5 Ref.
SYQ 1 8 5.40% 3.40% [66]
DoReFa-Net 4 4 5.50% 3.30% [272]
DoReFa-Net 5 5 5.50% -0.20% [272]
FGQ 2 8 5.60% - [170]
ABC-Net 5 5 6.30% 3.50% [155]
FGQ-TWN 2 4 6.67% - [170]
HWGQ 1 2 6.90% 4.60% [31]
ResNet-100 IAO 8 8 1.40% - [120]
ResNet-101 TensorRT 8 8 -0.01% 0.05% [173]
FGQ-TWN 2 8 3.65% - [170]
FGQ-TWN 2 4 6.81% - [170]
ResNet-150 IAO 8 8 2.10% - [120]
ResNet-152 TensorRT 8 8 0.08% 0.04% [173]
dLAC 2 16 1.20% 0.64% [235]
SqueezeNet AngleEye 16 16 0.00% 0.01% [85]
ShiftCNN 3 4 0.01% 0.01% [84]
ShiftCNN 2 4 1.01% 0.71% [84]
AngleEye 8 8 1.42% 1.05% [85]
AngleEye 6 6 28.13% 27.43% [85]
ShiftCNN 1 4 35.39% 35.09% [84]
VGG-16 ELNN 3( ±4) 32 -1.10% -1.00% [144]
ELNN 3( ±2) 32 -0.60% -0.80% [144]
AngleEye 16 16 0.09% -0.05% [85]
DFP16 16 16 0.11% 0.29% [54]
AngleEye 8 8 0.21% 0.08% [85]
SeerNet 4 1 0.28% 0.10% [32]
DeepShift-Q 6 32 0.29% 0.11% [62]
FFN 2 32 0.30% -0.20% [238]
DeepShift-PS 6 32 0.47% 0.30% [62]
DeepShift-Q 6 32 0.72% 0.29% [62]
INQ 5 32 0.77% 0.08% [62]
TWN 2 32 1.10% 0.30% [146]
ELNN 2 32 2.00% 0.90% [144]
TSQ 2 2 2.00% 0.70% [239]
AngleEye 16 16 2.15% 1.49% [85]
BWN 2 32 2.20% 1.20% [200]
AngleEye 8 8 2.35% 1.76% [85]
ELNN 1 32 3.30% 1.80% [144]
AngleEye 6 6 9.07% 6.58% [85]
AngleEye 6 6 22.38% 17.75% [85]
LogQuant 3 3 - 0.99% [30]
LogQuant 4 4 - 0.51% [30]
LogQuant 6 6 - 0.83% [30]
LogQuant 32 3 - 0.82% [30]
LogQuant 32 4 - 0.36% [30]
LogQuant 32 6 - 0.31% [30]
LogQuant 6 32 - 0.76% [30]
LDR 5 4 - 0.90% [175]
LogNN 5 4 - 1.38% [175]
T Liang et al.: Preprint submitted to Elsevier Page 31 of 41

--- PAGE 32 ---
Survey on pruning and quantization
References
[1]Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro,
C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S.,
Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz,
R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R.,
Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner,
B.,Sutskever,I.,Talwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,
V.,Viegas,F.,Vinyals,O.,Warden,P.,Wattenberg,M.,Wicke,M.,
Yu,Y.,Zheng,X.,2016. TensorFlow: Large-ScaleMachineLearn-
ing on Heterogeneous Distributed Systems. arXiv preprint arXiv:
1603.04467 URL: https://arxiv.org/abs/1603.04467 .
[2]Abdel-Hamid, O., Mohamed, A.r., Jiang, H., Deng, L., Penn, G.,
Yu,D.,2014. ConvolutionalNeuralNetworksforSpeechRecogni-
tion. IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing22,1533–1545.URL: http://ieeexplore.ieee.org/document/
6857341/, doi: 10.1109/TASLP.2014.2339736 .
[3]Abdelouahab, K., Pelcat, M., Serot, J., Berry, F., 2018. Accelerating
CNN inference on FPGAs: A Survey. ArXiv preprint URL: http:
//arxiv.org/abs/1806.01683 .
[4]AchronixSemiconductorCorporation,2020. FPGAsEnabletheNext
Generation of Communication and Networking Solutions. White
Paper WP021, 1–15.
[5]Albanie, 2020. convnet-burden. URL: https://github.com/albanie/
convnet-burden .
[6]Alemdar,H.,Leroy,V.,Prost-Boucle,A.,Petrot,F.,2017. Ternary
neural networks for resource-eﬃcient AI applications, in: 2017 Inter-
nationalJointConferenceonNeuralNetworks(IJCNN),IEEE.pp.
2547–2554. URL: https://ieeexplore.ieee.org/abstract/document/
7966166/, doi: 10.1109/IJCNN.2017.7966166 .
[7]AMD, . Radeon Instinct ™MI25 Accelerator. URL: https://www.
amd.com/en/products/professional-graphics/instinct-mi25 .
[8]Arm, 2015. ARM Architecture Reference Man-
ual ARMv8, for ARMv8-A architecture proﬁle.
https://developer.arm.com/documentation/ddi0487/latest. URL:
https://developer.arm.com/documentation/ddi0487/latest .
[9]Arm, 2020. Arm Cortex-M Processor Comparison Table. URL:
https://developer.arm.com/ip-products/processors/cortex-a .
[10]Arm, Graphics, C., 2020. MALI-G76 High-Performance
GPU for Complex Graphics Features and Bene ts High Perfor-
mance for Mixed Realities. URL: https://www.arm.com/products/
silicon-ip-multimedia/gpu/mali-g76 .
[11]ARM, Reddy, V.G., 2008. Neon technology introduction. ARM Cor-
poration , 1–34URL: http://caxapa.ru/thumbs/301908/AT_-_NEON_
for_Multimedia_Applications.pdf .
[12]Augasta, M.G., Kathirvalavakumar, T., 2013. Pruning algorithms of
neuralnetworks-Acomparativestudy. OpenComputerScience3,
105–115. doi: 10.2478/s13537-013-0109-x .
[13]Baidu, 2019. PArallel Distributed DeepLEarning: MachineLearn-
ingFrameworkfromIndustrialPractice. URL: https://github.com/
PaddlePaddle/Paddle .
[14]Balzer, W., Takahashi, M., Ohta, J., Kyuma, K., 1991. Weight
quantizationinBoltzmannmachines. NeuralNetworks4,405–409.
doi:10.1016/0893-6080(91)90077-I .
[15]Banner, R., Hubara, I., Hoﬀer, E., Soudry, D., 2018.
Scalable methods for 8-bit training of neural networks,
in: Advances in Neural Information Processing Systems
(NIPS), pp. 5145–5153. URL: http://papers.nips.cc/paper/
7761-scalable-methods-for-8-bit-training-of-neural-networks .
[16]Banner,R.,Nahshan,Y.,Soudry,D.,2019. Posttraining4-bitquanti-
zationofconvolutionalnetworksforrapid-deployment,in: Advances
in Neural Information Processing Systems (NIPS), pp. 7950–7958.
[17]BaoyuanLiu,MinWang,Foroosh,H.,Tappen,M.,Penksy,M.,2015.
SparseConvolutionalNeuralNetworks,in: 2015IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR),IEEE.pp.806–
814. URL: http://ieeexplore.ieee.org/document/7298681/ , doi: 10.
1109/CVPR.2015.7298681 .
[18]Baskin, C., Schwartz, E., Zheltonozhskii, E., Liss, N., Giryes, R.,
Bronstein, A.M., Mendelson, A., 2018. UNIQ: Uniform Noise In-jection for Non-Uniform Quantization of Neural Networks. arXiv
preprint arXiv:1804.10969 URL: http://arxiv.org/abs/1804.10969 .
[19]Bengio, E., Bacon, P.L., Pineau, J., Precup, D., 2015. Conditional
ComputationinNeural Networksforfaster models. ArXiv preprint
URL: http://arxiv.org/abs/1511.06297 .
[20]Bengio, Y., 2013. Estimating or Propagating Gradients Through
Stochastic Neurons. ArXiv preprint URL: http://arxiv.org/abs/
1305.2982 .
[21]Bethge,J.,Bartz,C.,Yang,H.,Chen,Y.,Meinel,C.,2020.MeliusNet:
Can Binary Neural Networks Achieve MobileNet-level Accuracy?
ArXiv preprint URL: http://arxiv.org/abs/2001.05936 .
[22]Bethge, J., Yang, H., Bornstein, M., Meinel, C., 2019. Binary-
DenseNet: Developing an architecture for binary neural networks.
Proceedings - 2019 International Conference on Computer Vision
Workshop, ICCVW 2019 , 1951–1960doi: 10.1109/ICCVW.2019.00244 .
[23]Bianco,S.,Cadene,R.,Celona,L.,Napoletano,P.,2018. Benchmark
analysisofrepresentativedeepneuralnetworkarchitectures. IEEE
Access 6, 64270–64277. doi: 10.1109/ACCESS.2018.2877890 .
[24]Blalock, D., Ortiz, J.J.G., Frankle, J., Guttag, J., 2020. What is
the State of Neural Network Pruning? ArXiv preprint URL: http:
//arxiv.org/abs/2003.03033 .
[25]Bolukbasi, T., Wang, J., Dekel, O., Saligrama, V., 2017. Adaptive
NeuralNetworksforEﬃcient Inference. Thirty-fourthInternational
ConferenceonMachineLearningURL: https://arxiv.org/abs/1702.
07811http://arxiv.org/abs/1702.07811 .
[26]Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.,Dhariwal,
P.,Neelakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,Agarwal,S.,
Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,
Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford,A., Sutskever,I., Amodei,D., 2020. Language Models
are Few-Shot Learners. ArXiv preprint URL: http://arxiv.org/abs/
2005.14165 .
[27]Buciluˇa,C.,Caruana,R.,Niculescu-Mizil,A.,2006. Modelcompres-
sion,in: Proceedingsofthe12thACMSIGKDDinternationalcon-
ference on Knowledge discovery and data mining - KDD ’06, ACM
Press, New York, New York, USA. p. 535. URL: https://dl.acm.
org/doi/abs/10.1145/1150402.1150464 , doi: 10.1145/1150402.1150464 .
[28]BUG1989, 2019. BUG1989/caﬀe-int8-convert-tools: Generate a
quantization parameter ﬁle for ncnn framework int8 inference. URL:
https://github.com/BUG1989/caffe-INT8-convert-tools .
[29]Cai,H.,Gan,C.,Wang,T.,Zhang,Z.,Han,S.,2019. Once-for-All:
TrainOneNetworkandSpecializeitforEﬃcientDeployment. ArXiv
preprint , 1–15URL: http://arxiv.org/abs/1908.09791 .
[30]Cai, J., Takemoto, M., Nakajo, H., 2018. A Deep Look into Loga-
rithmic Quantization of Model Parameters in Neural Networks, in:
Proceedings of the 10th International Conference on Advances in
InformationTechnology-IAIT2018,ACMPress,NewYork,New
York, USA. pp. 1–8. URL: http://dl.acm.org/citation.cfm?doid=
3291280.3291800 , doi: 10.1145/3291280.3291800 .
[31]Cai, Z., He, X., Sun, J., Vasconcelos, N., 2017. Deep Learning with
Low Precision by Half-Wave Gaussian Quantization, in: 2017 IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR),
IEEE. pp. 5406–5414. URL: http://ieeexplore.ieee.org/document/
8100057/, doi: 10.1109/CVPR.2017.574 .
[32]Cao, S., Ma, L., Xiao, W., Zhang, C., Liu, Y., Zhang, L.,
Nie, L., Yang, Z., 2019. SeerNet : Predicting Convolu-
tional Neural Network Feature-Map Sparsity through Low-
Bit Quantization. Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) URL:
http://openaccess.thecvf.com/content_CVPR_2019/papers/Cao_
SeerNet_Predicting_Convolutional_Neural_Network_Feature-Map_
Sparsity_Through_Low-Bit_Quantization_CVPR_2019_paper.pdf .
[33]Carreira-Perpinan, M.A., Idelbayev, Y., 2018. "Learning-
Compression" Algorithms for Neural Net Pruning, in: IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),
IEEE.pp.8532–8541. URL: https://ieeexplore.ieee.org/document/
8578988/, doi: 10.1109/CVPR.2018.00890 .
T Liang et al.: Preprint submitted to Elsevier Page 32 of 41

--- PAGE 33 ---
Survey on pruning and quantization
[34]Chellapilla,K.,Puri,S.,Simard,P.,2006. HighPerformanceCon-
volutional Neural Networks for Document Processing, in: Tenth
International Workshop on Frontiers in Handwriting Recognition.
URL: https://hal.inria.fr/inria-00112631/ , doi: 10.1.1.137.482 .
[35]Chen,H.,Wang,Y.,Xu,C.,Shi,B.,Xu,C.,Tian,Q.,Xu,C.,2020.
AdderNet: DoWeReallyNeedMultiplicationsinDeepLearning?,in:
Proceedingsof theIEEE/CVFConference onComputerVision and
Pattern Recognition (CVPR), pp. 1468–1477. URL: http://arxiv.
org/abs/1912.13200 .
[36]Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Cowan, M.,
Shen,H.,Wang,L.,Hu,Y.,Ceze,L.,Guestrin,C.,Krishnamurthy,
A.,2018. TVM:Anautomatedend-to-endoptimizingcompilerfor
deep learning, in: Proceedings of the 13th USENIX Symposium
onOperatingSystems DesignandImplementation,OSDI2018, pp.
579–594. URL: http://arxiv.org/abs/1802.04799 .
[37]Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y., 2015.
Compressing neural networks with the hashing trick., in: In Inter-
national Conference on Machine Learning, pp. 2285–2294. URL:
http://arxiv.org/abs/1504.04788 .
[38]Chen, Y., Chen, T., Xu, Z., Sun, N., Temam, O., 2016. DianNao
family: Energy-Eﬃcient Hardware Accelerators for Machine
Learning. Communications of the ACM 59, 105–112. URL:
10.1145/2594446%5Cnhttps://ejwl.idm.oclc.org/login?url=http:
//search.ebscohost.com/login.aspx?direct=true&db=bth&AN=
95797996&site=ehost-livehttp://dl.acm.org/citation.cfm?doid=
3013530.2996864 , doi: 10.1145/2996864 .
[39]Cheng, J., Wang, P.s., Li, G., Hu, Q.h., Lu, H.q., 2018. Recent ad-
vancesineﬃcientcomputationofdeepconvolutionalneuralnetworks.
Frontiers of Information Technology & Electronic Engineering
19, 64–77. URL: http://link.springer.com/10.1631/FITEE.1700789 ,
doi:10.1631/FITEE.1700789 .
[40]Cheng, Y., Wang, D., Zhou, P., Zhang, T., 2017. A Survey of Model
Compression and Acceleration for Deep Neural Networks. ArXiv
preprint URL: http://arxiv.org/abs/1710.09282 .
[41]Cheng, Z., Soudry, D., Mao, Z., Lan, Z., 2015. Training Binary Mul-
tilayerNeuralNetworksforImageClassiﬁcationusingExpectation
Backpropagation. ArXiv preprint URL: http://cn.arxiv.org/pdf/
1503.03562.pdfhttp://arxiv.org/abs/1503.03562 .
[42]Chiliang, Z., Tao, H., Yingda, G., Zuochang, Y., 2019. Accelerating
ConvolutionalNeuralNetworkswith DynamicChannelPruning, in:
2019 Data Compression Conference (DCC), IEEE. pp. 563–563.
URL: https://ieeexplore.ieee.org/document/8712710/ , doi: 10.1109/
DCC.2019.00075 .
[43]Choi, B., Lee, J.H., Kim, D.H., 2008. Solving local minima
problem with large number of hidden nodes on two-layered feed-
forward artiﬁcialneural networks. Neurocomputing71, 3640–3643.
doi:10.1016/j.neucom.2008.04.004 .
[44]Choi, J., Wang, Z., Venkataramani, S., Chuang, P.I.j., Srinivasan, V.,
Gopalakrishnan,K.,2018. PACT:ParameterizedClippingActivation
for Quantized Neural Networks. ArXiv preprint , 1–15URL: http:
//arxiv.org/abs/1805.06085 .
[45]Choi,Y.,El-Khamy,M.,Lee,J.,2017a.TowardstheLimitofNetwork
Quantization, in: International Conference on Learning Represen-
tations(ICLR),IEEE. URL: https://arxiv.org/abs/1612.01543http:
//arxiv.org/abs/1612.01543 .
[46]Choi, Y., Member, S.S., Bae, D., Sim, J., Member, S.S., Choi, S.,
Kim, M., Member, S.S., Kim, L.s.S., Member, S.S., 2017b. Energy-
Eﬃcient Design of Processing Element for Convolutional Neural
Network. IEEE Transactions on Circuits and Systems II: Express
Briefs 64, 1332–1336. URL: http://ieeexplore.ieee.org/document/
7893765/, doi: 10.1109/TCSII.2017.2691771 .
[47]Chollet, F., Google, C., 2017. Xception : Deep Learning with
Depthwise Separable Convolutions, in: The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 1251–
1258. URL: http://ieeexplore.ieee.org/document/8099678/ ,doi: 10.
1109/CVPR.2017.195 .
[48]Choudhary, T., Mishra, V., Goswami, A., Sarangapani, J., 2020.
A comprehensive survey on model compression and acceleration.Artiﬁcial Intelligence Review 53, 5113–5155. URL: https://doi.
org/10.1007/s10462-020-09816-7 , doi: 10.1007/s10462-020-09816-7 .
[49]Cornea,M.,2015. Intel ®AVX-512InstructionsandTheirUsein
the Implementation of Math Functions. Intel Corporation .
[50]Cotofana, S., Vassiliadis, S., Logic, T., Addition, B., Addition, S.,
1997. LowWeightandFan-InNeuralNetworksforBasicArithmetic
Operations,in: 15thIMACSWorldCongress,pp.227–232. doi: 10.
1.1.50.4450 .
[51]Courbariaux,M.,Bengio,Y.,David,J.P.,2014. Trainingdeepneu-
ral networks with low precision multiplications, in: International
Conference on Learning Representations(ICLR), pp. 1–10. URL:
http://arxiv.org/abs/1412.7024 , doi: arXiv:1412.7024 .
[52]Courbariaux, M., Bengio, Y., David, J.P., 2015. BinaryConnect:
TrainingDeepNeuralNetworkswithbinaryweightsduringpropa-
gations, in: Advances in Neural Information Processing Systems
(NIPS), pp. 1–9. URL: http://arxiv.org/abs/1511.00363 , doi: 10.
5555/2969442.2969588 .
[53]Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., Bengio,
Y., 2016. Binarized Neural Networks: Training Deep Neural
Networks with Weights and Activations Constrained to +1 or -1.
ArXivpreprintURL: https://github.com/MatthieuCourbariaux/http:
//arxiv.org/abs/1602.02830 .
[54] Das,D., Mellempudi,N.,Mudigere, D.,Kalamkar,D., Avancha,S.,
Banerjee,K.,Sridharan,S.,Vaidyanathan,K.,Kaul,B.,Georganas,
E., Heinecke, A., Dubey, P., Corbal, J., Shustrov, N., Dubtsov,
R., Fomenko, E., Pirogov, V., 2018. Mixed Precision Training of
Convolutional Neural Networks using Integer Operations, in: In-
ternational Conference on Learning Representations(ICLR),
pp. 1–11. URL: https://www.anandtech.com/show/11741/
hot-chips-intel-knights-mill-live-blog-445pm-pt-1145pm-utchttp:
//arxiv.org/abs/1802.00930 .
[55] Dash, M., Liu, H., 1997. Feature selection for classiﬁcation. Intelli-
gent Data Analysis 1, 131–156. doi: 10.3233/IDA-1997-1302 .
[56]Davis, A., Arel, I., 2013. Low-Rank Approximationsfor Conditional
FeedforwardComputationinDeepNeuralNetworks,in: International
ConferenceonLearningRepresentationsWorkshops(ICLRW),pp.
1–10. URL: http://arxiv.org/abs/1312.4461 .
[57]Deng, W., Yin, W., Zhang, Y., 2013. Group sparse optimiza-
tion by alternating direction method, in: Van De Ville, D.,
Goyal, V.K., Papadakis, M. (Eds.), Wavelets and Sparsity XV,
p. 88580R. URL: http://proceedings.spiedigitallibrary.org/
proceeding.aspx?doi=10.1117/12.2024410 , doi: 10.1117/12.2024410 .
[58]Dettmers, T., 2015. 8-Bit Approximations for Parallelism
in Deep Learning, in: International Conference on Learn-
ing Representations(ICLR). URL: https://github.com/soumith/
convnet-benchmarkshttp://arxiv.org/abs/1511.04561 .
[59]Dong,X., Huang,J., Yang,Y.,Yan, S.,2017. Moreisless: Amore
complicatednetworkwithlessinferencecomplexity. Proceedings-
30th IEEE Conference on Computer Vision and Pattern Recognition,
CVPR20172017-Janua,1895–1903. URL: http://arxiv.org/abs/
1703.08651 , doi: 10.1109/CVPR.2017.205 .
[60]Dongarra,J.J.,DuCroz,J.,Hammarling,S.,Duﬀ,I.S.,1990. Aset
oflevel3basiclinearalgebrasubprograms. ACMTransactionson
Mathematical Software (TOMS) 16, 1–17. doi: 10.1145/77626.79170 .
[61]Dukhan, M., Yiming, W., Hao, L., Lu, H., 2019. QNNPACK:
Open source library for optimized mobile deep learning - Facebook
Engineering. URL: https://engineering.fb.com/ml-applications/
qnnpack/.
[62]Elhoushi, M., Chen, Z., Shaﬁq, F., Tian, Y.H., Li, J.Y., 2019.
DeepShift: Towards Multiplication-Less Neural Networks. ArXiv
preprint URL: http://arxiv.org/abs/1905.13298 .
[63]Elsken, T., Metzen, J.H., Hutter, F., 2019. Neural Architec-
ture Search. Journal of Machine Learning Research 20, 63–
77. URL: http://link.springer.com/10.1007/978-3-030-05318-5_3 ,
doi:10.1007/978-3-030-05318-5{\_}3 .
[64]Engelbrecht, A.P., 2001. A new pruning heuristic based on variance
analysis of sensitivity information. IEEE Transactions on Neural
Networks 12, 1386–1389. doi: 10.1109/72.963775 .
T Liang et al.: Preprint submitted to Elsevier Page 33 of 41

--- PAGE 34 ---
Survey on pruning and quantization
[65]Esser,S.K.,Merolla,P.A.,Arthur,J.V.,Cassidy,A.S.,Appuswamy,
R.,Andreopoulos,A.,Berg,D.J.,McKinstry,J.L.,Melano,T.,Barch,
D.R., di Nolfo, C., Datta, P., Amir, A., Taba, B., Flickner, M.D.,
Modha, D.S., 2016. Convolutionalnetworks forfast, energy-eﬃcient
neuromorphic computing. Proceedings of the National Academy
ofSciences113,11441–11446. URL: http://www.pnas.org/lookup/
doi/10.1073/pnas.1604850113 , doi: 10.1073/pnas.1604850113 .
[66]Faraone, J., Fraser, N., Blott, M., Leong, P.H., 2018. SYQ: Learning
Symmetric Quantization for Eﬃcient Deep Neural Networks, in:
Proceedingsof theIEEE/CVFConference onComputerVision and
Pattern Recognition (CVPR).
[67]Fiesler, E., Choudry, A., Caulﬁeld, H.J., 1990. Weight discretization
paradigmforoptical neuralnetworks. OpticalInterconnectionsand
Networks 1281, 164. doi: 10.1117/12.20700 .
[68]Figurnov,M.,Collins,M.D.,Zhu,Y.,Zhang,L.,Huang,J.,Vetrov,
D., Salakhutdinov, R., 2017. Spatially Adaptive Computation Time
for Residual Networks, in: IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), IEEE. pp. 1790–1799.
URL: http://ieeexplore.ieee.org/document/8099677/ , doi: 10.1109/
CVPR.2017.194 .
[69]FPGA, I., . Intel ®FPGA Development Tools - Intel
FPGA. URL: https://www.intel.com/content/www/us/en/software/
programmable/overview.html .
[70]Frankle, J., Carbin, M., 2019. The lottery ticket hypothesis: Finding
sparse, trainable neural networks, in: International Conference on
Learning Representations(ICLR). URL: http://arxiv.org/abs/1803.
03635.
[71]Fukushima, K., 1988. Neocognitron: A hierarchical neural network
capableof visualpattern recognition. NeuralNetworks 1, 119–130.
doi:10.1016/0893-6080(88)90014-7 .
[72]Gale, T., Elsen,E., Hooker, S., 2019. The Stateof Sparsity in Deep
Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1902.
09574.
[73]Gao,X.,Zhao,Y.,Dudziak,L.,Mullins,R.,Xu,C.Z.,Dudziak,L.,
Mullins, R., Xu, C.Z., 2019. Dynamic Channel Pruning: Feature
Boosting and Suppression, in: International Conference on Learning
Representations(ICLR),pp.1–14. URL: http://arxiv.org/abs/1810.
05331.
[74]Glossner, J., Blinzer, P., Takala, J., 2016. HSA-enabled DSPs and
accelerators. 2015 IEEE GlobalConference onSignal and Informa-
tion Processing, GlobalSIP 2015 , 1407–1411doi: 10.1109/GlobalSIP.
2015.7418430 .
[75]Gong, R., Liu, X., Jiang, S., Li, T., Hu, P., Lin, J., Yu, F., Yan, J.,
2019. Diﬀerentiablesoftquantization: Bridgingfull-precisionand
low-bitneuralnetworks,in: ProceedingsoftheIEEEInternational
Conference on Computer Vision (ICCV), pp. 4851–4860. doi: 10.
1109/ICCV.2019.00495 .
[76]Gong, Y., Liu, L., Yang, M., Bourdev, L., 2014. Compressing Deep
Convolutional Networks using Vector Quantization, in: International
ConferenceonLearningRepresentations(ICLR). URL: http://arxiv.
org/abs/1412.6115 .
[77]Google, . Hosted models | TensorFlow Lite. URL: https://www.
tensorflow.org/lite/guide/hosted_models .
[78]Google, 2018. google/gemmlowp: Low-precision matrix mul-
tiplication. https://github.com/google/gemmlowp. URL: https:
//github.com/google/gemmlowp .
[79]Gordon,A.,Eban,E.,Nachum,O.,Chen,B.,Wu,H.,Yang,T.J.,Choi,
E., 2018. MorphNet: Fast & Simple Resource-Constrained Struc-
ture Learningof Deep Networks, in: Proceedings ofthe IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),
IEEE.pp.1586–1595. URL: https://ieeexplore.ieee.org/document/
8578269/, doi: 10.1109/CVPR.2018.00171 .
[80]Gou,J.,Yu,B.,Maybank,S.J.,Tao,D.,2020.KnowledgeDistillation:
A Survey. ArXiv preprint URL: http://arxiv.org/abs/2006.05525 .
[81]Graham, B., 2017. Low-Precision Batch-Normalized Activations.
ArXiv preprint , 1–16URL: http://arxiv.org/abs/1702.08231 .
[82]Graves, A., 2016. Adaptive Computation Time for Recurrent Neural
Networks. ArXiv preprint , 1–19URL: http://arxiv.org/abs/1603.08983.
[83]Greﬀ, K., Srivastava, R.K., Schmidhuber, J., 2016. Highway and
Residual Networks learn Unrolled Iterative Estimation, in: Inter-
national Conference on Learning Representations(ICLR), pp. 1–14.
URL: http://arxiv.org/abs/1612.07771 .
[84]Gudovskiy, D.A., Rigazio, L., 2017. ShiftCNN: Generalized Low-
Precision Architecture for Inference of Convolutional Neural Net-
works. ArXiv preprint URL: http://arxiv.org/abs/1706.02393 .
[85]Guo, K., Sui, L., Qiu, J., Yu, J., Wang, J., Yao, S., Han, S., Wang, Y.,
Yang, H., 2018. Angel-Eye: A complete design ﬂow for mapping
CNNontoembeddedFPGA. IEEETransactionsonComputer-Aided
DesignofIntegratedCircuitsandSystems37,35–47. URL: https://
ieeexplore.ieee.org/abstract/document/7930521/ , doi: 10.1109/TCAD.
2017.2705069 .
[86]Guo, K., Zeng, S., Yu, J., Wang, Y., Yang, H., 2017. A Survey of
FPGA-Based Neural Network Accelerator. ACM Transactions on
Reconﬁgurable Technology and Systems 9. URL: http://arxiv.org/
abs/1712.08934https://arxiv.org/abs/1712.08934 .
[87]Guo, Y., 2018. A Survey on Methods and Theories of Quantized
Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1808.
04752.
[88]Guo, Y., Yao, A., Chen, Y., 2016. Dynamic Network Surgery for
Eﬃcient DNNs, in: Advances in Neural Information Processing Sys-
tems (NIPS), pp. 1379–1387. URL: http://papers.nips.cc/paper/
6165-dynamic-network-surgery-for-efficient-dnns .
[89]Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P., 2015.
Deep learning with limited numerical precision, in: International
Conference on Machine Learning (ICML), pp. 1737–1746.
[90]Gysel, P., Pimentel, J., Motamedi, M., Ghiasi, S., 2018. Ristretto: A
FrameworkforEmpiricalStudyofResource-EﬃcientInferencein
ConvolutionalNeuralNetworks. IEEETransactionsonNeuralNet-
worksandLearningSystems29,1–6.URL: https://ieeexplore.ieee.
org/abstract/document/8318896/ , doi: 10.1109/TNNLS.2018.2808319 .
[91]Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.A.,
Dally, W.J., 2016a. EIE: Eﬃcient Inference Engine on Compressed
Deep Neural Network, in: 2016 ACM/IEEE 43rd Annual Inter-
national Symposium on Computer Architecture (ISCA), IEEE. pp.
243–254. URL: http://ieeexplore.ieee.org/document/7551397/http:
//arxiv.org/abs/1602.01528 , doi: 10.1109/ISCA.2016.30 .
[92]Han, S., Mao, H., Dally, W.J., 2016b. Deep compression: Compress-
ingdeepneuralnetworkswithpruning,trainedquantizationandHuﬀ-
mancoding,in: InternationalConferenceonLearningRepresenta-
tions(ICLR), pp. 199–203. URL: http://arxiv.org/abs/1510.00149 .
[93]Han, S., Pool, J., Narang, S., Mao, H., Gong, E., Tang, S., Elsen,
E.,Vajda,P.,Paluri,M.,Tran,J.,Catanzaro,B.,Dally,W.J.,2016c.
DSD:Dense-Sparse-DenseTrainingforDeepNeuralNetworks,in:
International Conference on Learning Representations(ICLR). URL:
http://arxiv.org/abs/1607.04381 .
[94]Han, S., Pool, J., Tran, J., Dally, W.J., 2015. Learning both Weights
and Connections for Eﬃcient Neural Networks, in: Advances in
Neural Information Processing Systems (NIPS), pp. 1135–1143.
URL: http://arxiv.org/abs/1506.02626 ,doi: 10.1016/S0140-6736(95)
92525-2.
[95]Hannun,A.,Case,C.,Casper,J.,Catanzaro,B.,Diamos,G.,Elsen,E.,
Prenger,R.,Satheesh,S.,Sengupta,S.,Coates,A.,Ng,A.Y.,2014.
Deep Speech: Scaling up end-to-end speech recognition. ArXiv
preprint , 1–12URL: http://arxiv.org/abs/1412.5567 .
[96]HANSON, S., 1989. Comparing biases for minimal network con-
struction with back-propagation, in: Advances in Neural Information
Processing Systems (NIPS), pp. 177–185.
[97]Hassibi, B., Stork, D.G., Wolﬀ, G.J., 1993. Optimal brain surgeon
and general network pruning. doi: 10.1109/icnn.1993.298572 .
[98]He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep Residual Learn-
ing for Image Recognition, in: IEEE/CVF Conference on Com-
puterVisionandPatternRecognition(CVPR),IEEE.pp.171–180.
URL: http://arxiv.org/abs/1512.03385http://ieeexplore.ieee.org/
document/7780459/ , doi: 10.3389/fpsyg.2013.00124 .
[99]He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y., 2018. Soft Filter
T Liang et al.: Preprint submitted to Elsevier Page 34 of 41

--- PAGE 35 ---
Survey on pruning and quantization
PruningforAcceleratingDeepConvolutionalNeuralNetworks,in:
ProceedingsoftheTwenty-SeventhInternationalJointConferenceon
Artiﬁcial Intelligence (IJCAI-18), International Joint Conferences on
ArtiﬁcialIntelligenceOrganization,California.pp.2234–2240. URL:
http://arxiv.org/abs/1808.06866 , doi: 10.24963/ijcai.2018/309 .
[100]He, Y., Liu, P., Wang, Z., Hu, Z., Yang, Y., 2019. Filter Pruning
via Geometric Median for Deep Convolutional Neural Networks
Acceleration.IEEE/CVFConferenceonComputerVisionandPattern
Recognition (CVPR) URL: http://arxiv.org/abs/1811.00250 .
[101]He, Y., Zhang, X., Sun, J., 2017. Channel Pruning for Accel-
erating Very Deep Neural Networks, in: IEEE International
Conference on Computer Vision (ICCV), IEEE. pp. 1398–1406.
URL: http://openaccess.thecvf.com/content_ICCV_2017/papers/He_
Channel_Pruning_for_ICCV_2017_paper.pdfhttp://ieeexplore.ieee.
org/document/8237417/ , doi: 10.1109/ICCV.2017.155 .
[102] Hinton, G.,2012. Neuralnetworks for machinelearning. Technical
Report. Coursera.
[103]Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhut-
dinov, R.R., 2012. Improving neural networks by preventing co-
adaptation of feature detectors. ArXiv preprint , 1–18URL: http:
//arxiv.org/abs/1207.0580 .
[104]Hou, L., Yao, Q., Kwok, J.T., 2017. Loss-aware Binarization of
Deep Networks, in: International Conference on Learning Represen-
tations(ICLR). URL: http://arxiv.org/abs/1611.01600 .
[105]Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W.,
Weyand, T., Andreetto, M., Adam, H., 2017. MobileNets: Eﬃ-
cientConvolutionalNeuralNetworksforMobileVisionApplications.
ArXiv preprint URL: http://arxiv.org/abs/1704.04861 .
[106]Hu,H.,Peng,R.,Tai,Y.W.,Tang,C.K.,2016. NetworkTrimming:
AData-DrivenNeuronPruningApproachtowardsEﬃcientDeepAr-
chitectures. ArXiv preprint URL: http://arxiv.org/abs/1607.03250 .
[107]Hu, Q., Wang, P., Cheng, J., 2018. From hashing to CNNs: Train-
ing binary weight networks via hashing, in: AAAI Conference on
Artiﬁcial Intelligence, pp. 3247–3254.
[108]Huang,G.,Chen,D.,Li,T.,Wu,F.,VanDerMaaten,L.,Weinberger,
K., 2018. Multi-scale dense networks for resource eﬃcient image
classiﬁcation,in: InternationalConferenceonLearningRepresenta-
tions(ICLR). URL: http://image-net.org/challenges/talks/ .
[109]Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017.
Densely Connected Convolutional Networks, in: IEEE/CVF Confer-
enceonComputerVisionandPatternRecognition(CVPR),IEEE.pp.
2261–2269. URL: https://ieeexplore.ieee.org/document/8099726/ ,
doi:10.1109/CVPR.2017.243 .
[110]Huang, G.B., Learned-miller, E., 2014. Labeled faces in the wild:
Updates and new reporting procedures. Dept. Comput. Sci., Univ.
Massachusetts Amherst, Amherst, MA, USA, Tech. Rep 14, 1–5.
[111]Huang, Z., Wang, N., 2018. Data-Driven Sparse Structure Selec-
tionforDeepNeuralNetworks,in: LectureNotesinComputerSci-
ence(includingsubseriesLectureNotesinArtiﬁcialIntelligenceand
Lecture Notes in Bioinformatics). volume 11220 LNCS, pp. 317–
334. URL: http://link.springer.com/10.1007/978-3-030-01270-0_
19, doi: 10.1007/978-3-030-01270-0{\_}19 .
[112]Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio,
Y., 2016a. Binarized Neural Networks, in: Advances in Neural
Information Processing Systems (NIPS), pp. 4114–4122. URL:
http://papers.nips.cc/paper/6573-binarized-neural-networks .
[113]Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y.,
2016b. Quantized Neural Networks: Training Neural Networks with
LowPrecisionWeightsandActivations.JournalofMachineLearning
Research 18 18, 187–1. URL: http://arxiv.org/abs/1609.07061 .
[114]Hwang, K., Sung, W., 2014. Fixed-point feedforward deep neural
networkdesignusingweights+1,0,and-1,in: 2014IEEEWorkshop
on Signal Processing Systems (SiPS), IEEE. pp. 1–6. URL: https://
ieeexplore.ieee.org/abstract/document/6986082/ , doi: 10.1109/SiPS.
2014.6986082 .
[115]Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J.,
Keutzer, K., 2016. SqueezeNet: AlexNet-level accuracy with 50x
fewer parameters and <0.5MB model size, in: ArXiv e-prints.URL: https://arxiv.org/abs/1602.07360http://arxiv.org/abs/1602.
07360, doi: 10.1007/978-3-319-24553-9 .
[116]Ignatov, A., Timofte, R., Kulik, A., Yang, S., Wang, K., Baum, F.,
Wu, M., Xu, L., Van Gool, L., 2019. AI benchmark: All about
deep learning on smartphones in 2019. Proceedings - 2019 In-
ternational Conference on Computer Vision Workshop, ICCVW
2019 , 3617–3635URL: https://developer.arm.com/documentation/
ddi0487/latest , doi: 10.1109/ICCVW.2019.00447 .
[117]Imagination, . PowerVR - embedded graphics processors
powering iconic products. URL: https://www.imgtec.com/
graphics-processors/ .
[118]Intel,. OpenVINO ™Toolkit. URL: https://docs.openvinotoolkit.
org/latest/index.html .
[119]Ioﬀe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep
networktrainingbyreducinginternalcovariateshift,in: International
Conference on Machine Learning (ICML), pp. 448–456. URL: http:
//arxiv.org/abs/1502.03167 .
[120]Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.,
Adam, H., Kalenichenko, D., 2018. Quantization and Training of
Neural Networks for Eﬃcient Integer-Arithmetic-Only Inference, in:
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR),IEEE.pp.2704–2713. URL: https://ieeexplore.ieee.org/
document/8578384/ , doi: 10.1109/CVPR.2018.00286 .
[121]Jia,Z.,Tillman,B.,Maggioni,M.,Scarpazza,D.P.,2019. Dissect-
ingthegraphcoreIPUarchitectureviamicrobenchmarking. ArXiv
preprint .
[122]JiaDeng,WeiDong,Socher,R.,Li-JiaLi,KaiLi,LiFei-Fei,2009.
ImageNet: A large-scale hierarchical image database. IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),
248–255doi: 10.1109/cvprw.2009.5206848 .
[123]JianchangMao,Mohiuddin,K.,Jain,A.,1994.Parsimoniousnetwork
designandfeatureselectionthroughnodepruning,in: Proceedings
ofthe12thIAPRInternationalConferenceonPatternRecognition,
Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5),
IEEE Comput. Soc. Press. pp. 622–624. URL: http://ieeexplore.
ieee.org/document/577060/ , doi: 10.1109/icpr.1994.577060 .
[124]Jiao,Y.,Han,L.,Long,X.,2020. Hanguang800NPU–TheUltimate
AI Inference Solution for Data Centers, in: 2020 IEEE Hot Chips 32
Symposium (HCS),IEEE. pp. 1–29. URL: https://ieeexplore.ieee.
org/document/9220619/ , doi: 10.1109/HCS49909.2020.9220619 .
[125]Jouppi, N.P., Borchers, A., Boyle, R., Cantin, P.l., Chao, C., Clark,
C., Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Young, C.,
Ghaemmaghami, T.V., Gottipati, R., Gulland, W., Hagmann, R., Ho,
C.R., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Patil, N.,
Jaﬀey,A.,Jaworski,A.,Kaplan,A.,Khaitan,H.,Killebrew,D.,Koch,
A., Kumar, N., Lacy, S., Laudon, J., Law, J., Patterson, D., Le, D.,
Leary,C.,Liu,Z.,Lucke,K.,Lundin,A.,MacKean,G.,Maggiore,A.,
Mahony,M.,Miller,K.,Nagarajan,R.,Agrawal,G.,Narayanaswami,
R.,Ni,R.,Nix,K.,Norrie,T.,Omernick,M.,Penukonda,N.,Phelps,
A.,Ross,J.,Ross,M.,Salek,A.,Bajwa,R.,Samadiani,E.,Severn,C.,
Sizikov,G.,Snelham,M.,Souter,J.,Steinberg,D.,Swing,A.,Tan,
M.,Thorson,G.,Tian,B.,Bates,S.,Toma,H.,Tuttle,E.,Vasudevan,
V., Walter, R., Wang, W., Wilcox, E., Yoon, D.H., Bhatia, S., Boden,
N.,2017. In-DatacenterPerformanceAnalysisofaTensorProcessing
Unit. ACMSIGARCHComputerArchitectureNews45,1–12. URL:
http://dl.acm.org/citation.cfm?doid=3140659.3080246 ,doi: 10.1145/
3140659.3080246 .
[126]Judd, P., Delmas, A., Sharify, S., Moshovos, A., 2017. Cnvlutin2:
Ineﬀectual-Activation-and-Weight-Free Deep Neural Network Com-
puting. ArXiv preprint , 1–6URL: https://arxiv.org/abs/1705.
00125.
[127]Jung,S.,Son,C.,Lee,S.,Son,J.,Kwak,Y.,Han,J.J.,Hwang,S.J.,
Choi, C., 2018. Learning to Quantize Deep Networks by Optimizing
Quantization Intervals with Task Loss. Revue Internationale de la
Croix-RougeetBulletininternationaldesSociétésdelaCroix-Rouge
URL: http://arxiv.org/abs/1808.05779 , doi: arXiv:1808.05779v2 .
[128]Kathail, V., 2020. Xilinx Vitis Uniﬁed Software Platform, in: Pro-
ceedings of the 2020 ACM/SIGDA International Symposium on
T Liang et al.: Preprint submitted to Elsevier Page 35 of 41

--- PAGE 36 ---
Survey on pruning and quantization
Field-ProgrammableGateArrays,ACM,NewYork,NY,USA.pp.
173–174. URL: https://dl.acm.org/doi/10.1145/3373087.3375887 ,
doi:10.1145/3373087.3375887 .
[129]Keil, 2018. CMSIS NN Software Library. URL: https://
arm-software.github.io/CMSIS_5/NN/html/index.html .
[130]Köster, U., Webb, T.J., Wang, X., Nassar, M., Bansal, A.K., Consta-
ble,W.H.,Elibol,O.H., Gray,S.,Hall,S.,Hornof,L.,Khosrowshahi,
A., Kloss, C., Pai, R.J., Rao, N., 2017. Flexpoint: An Adaptive
NumericalFormatforEﬃcientTrainingofDeepNeuralNetworks.
ArXiv preprint URL: http://arxiv.org/abs/1711.02213 .
[131]Krishnamoorthi, R., 2018. Quantizing deep convolutional net-
worksforeﬃcientinference: Awhitepaper. ArXivpreprint8,667–
668. URL: http://cn.arxiv.org/pdf/1806.08342.pdfhttp://arxiv.
org/abs/1806.08342 , doi: arXiv:1806.08342v1 .
[132]Krizhevsky, A., 2009. Learning Multiple Layers of Features from
Tiny Images. Science Department, University of Toronto, Tech.
doi:10.1.1.222.9220 .
[133]Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet Clas-
siﬁcationwithDeepConvolutionalNeuralNetworks,in: Advances
in Neural Information Processing Systems (NIPS), pp. 1–9. URL:
http://code.google.com/p/cuda-convnet/ , doi: http://dx.doi.org/10.
1016/j.protcy.2014.09.007 .
[134]Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pien-
aar,J.,Riddle,R.,Shpeisman,T.,Vasilache,N.,Zinenko,O.,2020.
MLIR:ACompilerInfrastructurefortheEndofMoore’sLaw. ArXiv
preprint URL: http://arxiv.org/abs/2002.11054 .
[135]Lavin, A., Gray, S., 2016. Fast Algorithms for Convolu-
tional Neural Networks, in: IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), IEEE. pp. 4013–
4021. URL: http://ieeexplore.ieee.org/document/7780804/http://
arxiv.org/abs/1312.5851 , doi: 10.1109/CVPR.2016.435 .
[136]Lebedev, V., Lempitsky, V., 2016. Fast ConvNets Using Group-Wise
Brain Damage, in: IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), IEEE. pp. 2554–2564. URL:
http://openaccess.thecvf.com/content_cvpr_2016/html/Lebedev_
Fast_ConvNets_Using_CVPR_2016_paper.htmlhttp://ieeexplore.ieee.
org/document/7780649/ , doi: 10.1109/CVPR.2016.280 .
[137]Lebedev, V., Lempitsky, V., 2018. Speeding-up convolutional
neural networks: A survey. BULLETIN OF THE POLISH
ACADEMY OF SCIENCES TECHNICAL SCIENCES 66,
2018. URL: http://www.czasopisma.pan.pl/Content/109869/PDF/
05_799-810_00925_Bpast.No.66-6_31.12.18_K2.pdf?handler=pdfhttp:
//www.czasopisma.pan.pl/Content/109869/PDF/05_799-810_00925_
Bpast.No.66-6_31.12.18_K2.pdf , doi: 10.24425/bpas.2018.125927 .
[138]Lecun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521,
436–444. doi: 10.1038/nature14539 .
[139]LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P., 1998. Gradient-
based learning applied to document recognition. Proceedings of the
IEEE86,2278–2323. URL: http://ieeexplore.ieee.org/document/
726791/, doi: 10.1109/5.726791 .
[140]LeCun,Y.,Denker,J.S.,Solla,S.A.,1990. OptimalBrainDamage,
in: AdvancesinNeuralInformationProcessingSystems(NIPS),p.
598–605. doi: 10.5555/109230.109298 .
[141]Lee,N.,Ajanthan,T.,Torr,P.H.,2019. SnIP:Single-shotnetwork
pruningbasedonconnectionsensitivity,in: InternationalConference
on Learning Representations(ICLR).
[142]Lei, J., Gao, X., Song, J., Wang, X.L., Song, M.L., 2018. Survey
of Deep Neural Network Model Compression. Ruan Jian Xue
Bao/JournalofSoftware29,251–266.URL: https://www.scopus.com/
inward/record.uri?eid=2-s2.0-85049464636&doi=10.13328%2Fj.cnki.
jos.005428&partnerID=40&md5=5a79dfdff4a05f188c5d553fb3b3123a ,
doi:10.13328/j.cnki.jos.005428 .
[143]Lei,W.,Chen,H., Wu,Y., 2017. CompressingDeep Convolutional
Networks Using K-means Based on Weights Distribution, in: Pro-
ceedings of the 2nd International Conference on Intelligent Informa-
tionProcessing-IIP’17,ACMPress,NewYork,NewYork,USA.pp.
1–6. URL: http://dl.acm.org/citation.cfm?doid=3144789.3144803 ,
doi:10.1145/3144789.3144803 .[144]Leng,C.,Li,H.,Zhu,S.,Jin,R.,2018. ExtremelyLowBitNeural
Network: SqueezetheLastBitOutwithADMM. TheThirty-Second
AAAIConferenceonArtiﬁcialIntelligence(AAAI-18)URL: http:
//arxiv.org/abs/1707.09870 .
[145]Leroux,S.,Bohez,S.,DeConinck,E.,Verbelen,T.,Vankeirsbilck,
B., Simoens, P., Dhoedt, B., 2017. The cascading neural network:
building the Internet of Smart Things. Knowledge and Informa-
tionSystems52,791–814. URL: http://link.springer.com/10.1007/
s10115-017-1029-1 , doi: 10.1007/s10115-017-1029-1 .
[146]Li, F., Zhang, B., Liu, B., 2016. Ternary Weight Networks, in:
AdvancesinNeuralInformationProcessingSystems(NIPS). URL:
http://arxiv.org/abs/1605.04711 .
[147]Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P., 2017a.
Pruning Filters for Eﬃcient ConvNets, in: International Conference
onLearningRepresentations(ICLR). URL: http://arxiv.org/abs/
1608.08710 , doi: 10.1029/2009GL038531 .
[148]Li, H., Zhang, H., Qi, X., Ruigang, Y., Huang, G., 2019. Im-
provedTechniquesforTrainingAdaptiveDeepNetworks,in: 2019
IEEE/CVF International Conference on Computer Vision (ICCV),
IEEE.pp.1891–1900. URL: https://ieeexplore.ieee.org/document/
9010043/, doi: 10.1109/ICCV.2019.00198 .
[149]Li,M.,Liu,Y.I.,Liu,X.,Sun,Q.,You,X.I.N.,Yang,H.,Luan,Z.,
Gan,L.,Yang,G.,Qian,D.,2020a. TheDeepLearningCompiler:
A Comprehensive Survey. ArXiv preprint 1, 1–36. URL: http:
//arxiv.org/abs/2002.03794 .
[150]Li, Y., Gu, S., Mayer, C., Van Gool, L., Timofte, R., 2020b.
Group Sparsity: The Hinge Between Filter Pruning and Decom-
position for Network Compression, in: 2020 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), IEEE. pp.
8015–8024. URL: https://ieeexplore.ieee.org/document/9157445/ ,
doi:10.1109/CVPR42600.2020.00804 .
[151]Li, Z., Wang, Y., Zhi, T., Chen, T., 2017b. A survey of neural
networkaccelerators. FrontiersofComputerScience11,746–761.
URL: http://link.springer.com/10.1007/s11704-016-6159-1 , doi: 10.
1007/s11704-016-6159-1 .
[152]Li, Z., Zhang, Y., Wang, J., Lai, J., 2020c. A survey of FPGA design
for AI era. Journal of Semiconductors 41. doi: 10.1088/1674-4926/
41/2/021402 .
[153]Lin, J., Rao, Y., Lu, J., Zhou, J., 2017a. Runtime Neural
Pruning, in: Advances in Neural Information Processing Sys-
tems(NIPS),pp.2178–2188. URL: https://papers.nips.cc/paper/
6813-runtime-neural-pruning.pdf .
[154]Lin,M.,Chen,Q.,Yan,S.,2014. Networkinnetwork,in: Interna-
tional Conference on Learning Representations(ICLR), pp. 1–10.
[155]Lin, X., Zhao, C., Pan, W., 2017b. Towards accurate binary convolu-
tionalneuralnetwork,in: AdvancesinNeuralInformationProcessing
Systems (NIPS), pp. 345–353.
[156]Lin, Z., Courbariaux, M., Memisevic, R., Bengio, Y., 2016.
Neural Networks with Few Multiplications, in: Interna-
tional Conference on Learning Representations(ICLR). URL:
https://github.com/hantek/http://arxiv.org/abs/1510.03009https:
//arxiv.org/abs/1510.03009 .
[157]Liu,J.,Musialski,P.,Wonka,P.,Ye,J.,2013. TensorCompletionfor
EstimatingMissingValuesinVisualData. IEEETransactionsonPat-
ternAnalysisandMachineIntelligence35,208–220. URL: http://
ieeexplore.ieee.org/document/6138863/ , doi: 10.1109/TPAMI.2012.39 .
[158]Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C., 2017. Learn-
ing Eﬃcient Convolutional Networks through Network Slimming,
in: IEEE International Conference on Computer Vision (ICCV),
IEEE. pp. 2755–2763. URL: http://ieeexplore.ieee.org/document/
8237560/, doi: 10.1109/ICCV.2017.298 .
[159]Liu,Z.,Mu,H.,Zhang,X.,Guo,Z.,Yang,X.,Cheng,T.K.T.,Sun,J.,
2019a. MetaPruning: Meta Learning for Automatic Neural Network
ChannelPruning,in: IEEEInternationalConferenceonComputer
Vision. URL: http://arxiv.org/abs/1903.10258 .
[160]Liu, Z., Sun, M., Zhou, T., Huang, G., Darrell, T., 2019b. Re-
thinking the Value of Network Pruning, in: International Confer-
ence on Learning Representations (ICLR), pp. 1–11. URL: http:
T Liang et al.: Preprint submitted to Elsevier Page 36 of 41

--- PAGE 37 ---
Survey on pruning and quantization
//arxiv.org/abs/1810.05270 .
[161]Liu, Z., Wu, B., Luo, W., Yang, X., Liu, W., Cheng, K.T., 2018. Bi-
Real Net: Enhancing the performance of 1-bit CNNs with improved
representational capability and advanced training algorithm. Lecture
Notes in Computer Science (including subseries Lecture Notes in
ArtiﬁcialIntelligence and LectureNotesin Bioinformatics) 11219
LNCS, 747–763. doi: 10.1007/978-3-030-01267-0{\_}44 .
[162]Liu, Z.G., Mattina, M., 2019. Learning low-precision neural net-
works without Straight-Through Estimator (STE), in: IJCAI Inter-
national Joint Conference on Artiﬁcial Intelligence, International
Joint Conferences on Artiﬁcial Intelligence Organization, California.
pp. 3066–3072. URL: https://www.ijcai.org/proceedings/2019/425 ,
doi:10.24963/ijcai.2019/425 .
[163]Luo,J.H.,Wu,J.,2020. AutoPruner: Anend-to-endtrainableﬁlter
pruning method for eﬃcient deep model inference. Pattern Recogni-
tion107,107461. URL: https://linkinghub.elsevier.com/retrieve/
pii/S0031320320302648 , doi: 10.1016/j.patcog.2020.107461 .
[164]Luo,J.H.H.,Wu,J.,Lin,W.,2017. ThiNet: AFilterLevelPruning
Method for Deep Neural Network Compression. Proceedings of the
IEEEInternationalConferenceonComputerVision(ICCV)2017-
Octob, 5068–5076. URL: http://ieeexplore.ieee.org/document/
8237803/, doi: 10.1109/ICCV.2017.541 .
[165]Ma, Y., Suda, N., Cao, Y., Seo, J.S., Vrudhula, S., 2016. Scalable
and modularized RTL compilation of Convolutional Neural Net-
works onto FPGA. FPL 2016 - 26th International Conference on
Field-ProgrammableLogicandApplicationsdoi: 10.1109/FPL.2016.
7577356.
[166]Macchi,O.,1975. CoincidenceApproachToStochasticPointPro-
cess. Advances in Applied Probability 7, 83–122. doi: 10.1017/
s0001867800040313 .
[167]Mariet, Z., Sra, S., 2016. Diversity Networks: Neural Network
Compression Using Determinantal Point Processes, in: International
Conference on Learning Representations(ICLR), pp. 1–13. URL:
http://arxiv.org/abs/1511.05077 .
[168]Mathieu, M., Henaﬀ, M., LeCun, Y., 2013. Fast Training of Con-
volutional Networks through FFTs. ArXiv preprint URL: http:
//arxiv.org/abs/1312.5851 .
[169]Medina, E., 2019. Habana Labs presentation. 2019 IEEE Hot Chips
31 Symposium, HCS 2019 doi: 10.1109/HOTCHIPS.2019.8875670 .
[170]Mellempudi, N., Kundu, A., Mudigere, D., Das, D., Kaul, B., Dubey,
P., 2017. Ternary Neural Networks with Fine-Grained Quantization.
ArXiv preprint URL: http://arxiv.org/abs/1705.01462 .
[171]Merolla,P.,Appuswamy,R.,Arthur,J.,Esser,S.K.,Modha,D.,2016.
Deepneuralnetworksarerobusttoweightbinarizationandothernon-
linear distortions. ArXiv preprint URL: https://arxiv.org/abs/1606.
01981http://arxiv.org/abs/1606.01981 .
[172]Micikevicius,P.,Narang,S.,Alben,J.,Diamos,G.,Elsen,E.,Garcia,
D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., Wu,
H., 2017. Mixed Precision Training, in: International Conference on
Learning Representations(ICLR). URL: http://arxiv.org/abs/1710.
03740.
[173]Migacz, S., 2017. 8-bit inference with TensorRT. GPU Technol-
ogyConference2,7. URL: https://on-demand.gputechconf.com/gtc/
2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf .
[174]Mishra, A., Nurvitadhi, E., Cook, J.J., Marr, D., 2018. WRPN:
Widereduced-precisionnetworks,in: InternationalConferenceon
Learning Representations(ICLR), pp. 1–11.
[175]Miyashita, D., Lee, E.H., Murmann, B., 2016. Convolu-
tional Neural Networks using Logarithmic Data Representation.
ArXiv preprint URL: http://cn.arxiv.org/pdf/1603.01025.pdfhttp:
//arxiv.org/abs/1603.01025 .
[176]Molchanov,D.,Ashukha,A.,Vetrov,D.,2017. Variationaldropout
sparsiﬁes deep neural networks, in: International Conference on
Machine Learning (ICML), pp. 3854–3863. URL: https://dl.acm.
org/citation.cfm?id=3305939 .
[177]Molchanov,P.,Tyree,S.,Karras,T.,Aila,T.,Kautz,J.,2016. Pruning
Convolutional Neural Networks for Resource Eﬃcient Inference, in:
InternationalConferenceonLearningRepresentations(ICLR),pp.1–17. URL: http://arxiv.org/abs/1611.06440 .
[178]Moss, D.J.M., Nurvitadhi, E., Sim, J., Mishra, A., Marr, D., Sub-
haschandra, S., Leong, P.H.W., 2017. High performance binary
neuralnetworksontheXeon+FPGA ™platform,in: 201727thInter-
national Conference on Field Programmable Logic and Applications
(FPL),IEEE.pp.1–4. URL: https://ieeexplore.ieee.org/abstract/
document/8056823/ , doi: 10.23919/FPL.2017.8056823 .
[179]Moudgill, M., Glossner, J., Huang, W., Tian, C., Xu, C., Yang, N.,
Wang,L.,Liang,T.,Shi,S.,Zhang,X.,Iancu,D.,Nacer,G.,Li,K.,
2020. HeterogeneousEdgeCNNHardwareAccelerator,in: The12th
InternationalConferenceonWirelessCommunicationsandSignal
Processing, pp. 6–11.
[180]Muller, L.K., Indiveri, G., 2015. Rounding Methods for Neural
Networks with Low Resolution Synaptic Weights. ArXiv preprint
URL: http://arxiv.org/abs/1504.05767 .
[181]Muthukrishnan,R.,Rohini,R.,2016. LASSO:Afeatureselection
technique in predictive modeling for machine learning, in: 2016
IEEEInternationalConferenceonAdvancesinComputerApplica-
tions(ICACA),IEEE.pp.18–20. URL: http://ieeexplore.ieee.org/
document/7887916/ , doi: 10.1109/ICACA.2016.7887916 .
[182]Neill, J.O., 2020. An Overview of Neural Network Compression.
ArXiv preprint , 1–73URL: http://arxiv.org/abs/2006.03669 .
[183]NVIDIA Corporation, 2014. NVIDIA GeForce GTX 980 Featur-
ingMaxwell,TheMostAdvancedGPUEverMade. WhitePaper,
1–32URL: http://international.download.nvidia.com/geforce-com/
international/pdfs/GeForce_GTX_980_Whitepaper_FINAL.PDF .
[184]NVIDIACorporation,2015. NVIDIATeslaP100. WhitePaperURL:
https://www.nvidia.com/en-us/data-center/tesla-p100/ .
[185]NVIDIA Corporation, 2017a. NVIDIA DGX-1 With Tesla V100
System Architecture. White PaperURL: http://images.nvidia.com/
content/pdf/dgx1-v100-system-architecture-whitepaper.pdf .
[186]NVIDIA Corporation, 2017b. NVIDIA Tesla V100
GPU Volta Architecture. White Paper , 53URL:
http://images.nvidia.com/content/volta-architecture/pdf/
volta-architecture-whitepaper.pdf%0Ahttp://www.nvidia.com/
content/gated-pdfs/Volta-Architecture-Whitepaper-v1.1.pdf .
[187]NVIDIA Corporation, 2018a. NVIDIA A100 Tensor Core GPU.
White Paper , 20–21.
[188]NVIDIA Corporation, 2018b. NVIDIA Turing GPU Architecture.
White Paper URL: https://gpltech.com/wp-content/uploads/2018/
11/NVIDIA-Turing-Architecture-Whitepaper.pdf .
[189]Odena, A., Lawson, D., Olah, C., 2017. Changing Model Behav-
ior at Test-Time Using Reinforcement Learning, in: International
ConferenceonLearningRepresentationsWorkshops(ICLRW),In-
ternational Conference on Learning Representations, ICLR. URL:
http://arxiv.org/abs/1702.07780 .
[190]ONNX, . onnx/onnx: Open standard for machine learning interoper-
ability. URL: https://github.com/onnx/onnx .
[191]Ouyang, J., Noh, M., Wang, Y., Qi, W., Ma, Y., Gu, C., Kim, S.,
Hong, K.i., Bae, W.K., Zhao, Z., Wang, J., Wu, P., Gong, X., Shi, J.,
Zhu,H.,Du,X.,2020. BaiduKunlunAnAIprocessorfordiversiﬁed
workloads,in: 2020IEEEHotChips32Symposium(HCS),IEEE.pp.
1–18. URL: https://ieeexplore.ieee.org/document/9220641/ ,doi: 10.
1109/HCS49909.2020.9220641 .
[192]Park, E., Ahn, J., Yoo, S., 2017. Weighted-Entropy-Based Quan-
tization for Deep Neural Networks, in: IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), IEEE. pp.
7197–7205. URL: http://ieeexplore.ieee.org/document/8100244/ ,
doi:10.1109/CVPR.2017.761 .
[193]Paszke, A., Gross, S., Bradbury, J., Lin, Z., Devito, Z., Massa, F.,
Steiner, B., Killeen, T., Yang, E., 2019. PyTorch : An Imperative
Style , High-Performance Deep Learning Library. ArXiv preprint .
[194]Pilipovi/uni0107,R.,Buli/uni0107,P.,Risojevi/uni0107,V.,2018. Compressionofconvolu-
tionalneuralnetworks: Ashortsurvey,in: 201817thInternational
Symposium onINFOTEH-JAHORINA, INFOTEH2018 -Proceed-
ings, IEEE. pp. 1–6. URL: https://ieeexplore.ieee.org/document/
8345545/, doi: 10.1109/INFOTEH.2018.8345545 .
[195]Polyak, A., Wolf, L., 2015. Channel-level acceleration of deep
T Liang et al.: Preprint submitted to Elsevier Page 37 of 41

--- PAGE 38 ---
Survey on pruning and quantization
face representations. IEEE Access 3, 2163–2175. URL: http:
//ieeexplore.ieee.org/document/7303876/ , doi: 10.1109/ACCESS.2015.
2494536.
[196]Preuser,T.B.,Gambardella,G.,Fraser,N.,Blott,M.,2018. Inference
of quantized neural networks on heterogeneous all-programmable
devices, in: 2018 Design, Automation & Test in Europe Conference
& Exhibition (DATE), IEEE. pp. 833–838. URL: http://ieeexplore.
ieee.org/document/8342121/ , doi: 10.23919/DATE.2018.8342121 .
[197]Prost-Boucle,A.,Bourge,A.,Petrot,F.,Alemdar,H.,Caldwell,N.,
Leroy,V.,2017. Scalablehigh-performancearchitectureforconvo-
lutional ternary neural networks on FPGA, in: 2017 27th Interna-
tional Conference on Field Programmable Logic and Applications
(FPL), IEEE. pp. 1–7. URL: https://hal.archives-ouvertes.fr/
hal-01563763http://ieeexplore.ieee.org/document/8056850/ , doi: 10.
23919/FPL.2017.8056850 .
[198]Qin, H., Gong, R., Liu, X., Bai, X., Song, J., Sebe, N.,
2020a. Binary neural networks: A survey. Pattern Recognition
105, 107281. URL: https://linkinghub.elsevier.com/retrieve/pii/
S0031320320300856 , doi: 10.1016/j.patcog.2020.107281 .
[199]Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., Song, J.,
2020b. Forward and Backward Information Retention for Accu-
rate Binary Neural Networks, in: IEEE/CVF Conference on Com-
puterVisionandPatternRecognition(CVPR),IEEE.pp.2247–2256.
URL: https://ieeexplore.ieee.org/document/9157443/ , doi: 10.1109/
CVPR42600.2020.00232 .
[200]Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A., 2016.
XNOR-Net: ImageNet Classiﬁcation Using Binary Convolutional
Neural Networks, in: European Conference on Computer Vi-
sion, Springer. pp. 525–542. URL: http://arxiv.org/abs/1603.
05279http://link.springer.com/10.1007/978-3-319-46493-0_32 ,
doi:10.1007/978-3-319-46493-0{\_}32 .
[201]Reed, R., 1993. Pruning Algorithms - A Survey. IEEE Transactions
onNeuralNetworks4, 740–747. URL: http://ieeexplore.ieee.org/
document/248452/ , doi: 10.1109/72.248452 .
[202]Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., Kep-
ner, J., 2019. Survey and Benchmarking of Machine Learning Ac-
celerators, in: 2019 IEEE High Performance Extreme Computing
Conference (HPEC), IEEE. pp. 1–9. URL: https://ieeexplore.ieee.
org/document/8916327/ , doi: 10.1109/HPEC.2019.8916327 .
[203]Richard Chuang, Oliyide, O., Garrett, B., 2020. Introducing the
Intel®Vision Accelerator Design with Intel ®Arria®10 FPGA.
White Paper .
[204]Rodriguez, A., Segal, E., Meiri, E., Fomenko, E., Kim,
Y.J., Shen, H., 2018. Lower Numerical Precision Deep
Learning Inference and Training. Intel White Paper , 1–
19URL: https://software.intel.com/sites/default/files/managed/
db/92/Lower-Numerical-Precision-Deep-Learning-Jan2018.pdf .
[205]Rotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng, S., Dzhabarov,
R., Gibson, N., Hegeman, J., Lele, M., Levenstein, R., Montgomery,
J.,Maher,B.,Nadathur,S.,Olesen, J.,Park,J.,Rakhov,A.,Smelyan-
skiy,M.,Wang,M.,2018.Glow: Graphloweringcompilertechniques
for neural networks. ArXiv preprint .
[206]Ruﬀy, F., Chahal, K., 2019. The State of Knowledge Distillation
for Classiﬁcation. ArXivpreprint URL: http://arxiv.org/abs/1912.
10850.
[207]Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma,
S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg,
A.C., Fei-Fei, L., 2015. ImageNet Large Scale Visual Recogni-
tion Challenge. International Journal of Computer Vision 115, 211–
252. URL: http://link.springer.com/10.1007/s11263-015-0816-y ,
doi:10.1007/s11263-015-0816-y .
[208]Saad,D.,Marom,E.,1990. TrainingFeedForwardNetswithBinary
WeightsViaaModiﬁedCHIRAlgorithm. ComplexSystems4, 573–
586. URL: https://www.complex-systems.com/pdf/04-5-5.pdf .
[209]Sabour,S.,Frosst,N.,Hinton,G.E.,2017. Dynamicroutingbetween
capsules, in: Advances in Neural Information Processing Systems
(NIPS), pp. 3857–3867.
[210]Santurkar, S., Tsipras, D., Ilyas, A., Madry, A., 2018. How doesbatchnormalizationhelpoptimization?,in: AdvancesinNeuralIn-
formation Processing Systems (NIPS), pp. 2483–2493.
[211]Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun,
Y., 2013. OverFeat: Integrated Recognition, Localization and Detec-
tion using Convolutional Networks, in: International Conference on
Learning Representations(ICLR). URL: http://arxiv.org/abs/1312.
6229.
[212]Settle,S.O.,Bollavaram,M.,D’Alberto,P.,Delaye,E.,Fernandez,
O.,Fraser,N.,Ng,A.,Sirasao,A.,Wu,M.,2018. QuantizingConvo-
lutionalNeuralNetworksforLow-PowerHigh-ThroughputInference
Engines. ArXiv preprint URL: http://arxiv.org/abs/1805.07941 .
[213]Shen, M., Han, K., Xu, C., Wang, Y., 2019. Searching for accu-
rate binary neural architectures. Proceedings - 2019 International
ConferenceonComputerVisionWorkshop,ICCVW2019,2041–
2044doi: 10.1109/ICCVW.2019.00256 .
[214]Shen, X., Yi, B., Zhang, Z., Shu, J., Liu, H., 2016. Automatic
Recommendation Technology for Learning Resources with Con-
volutional Neural Network, in: Proceedings - 2016 International
Symposium on Educational Technology, ISET 2016, pp. 30–34.
doi:10.1109/ISET.2016.12 .
[215]Sheng, T., Feng, C., Zhuo, S., Zhang, X., Shen, L., Aleksic, M.,
2018. A Quantization-Friendly Separable Convolution for Mo-
bileNets. 2018 1st Workshop on Energy Eﬃcient Machine Learn-
ing and Cognitive Computing for Embedded Applications (EMC2) ,
14–18URL: https://ieeexplore.ieee.org/document/8524017/ ,doi: 10.
1109/EMC2.2018.00011 .
[216]Simons,T.,Lee,D.J.,2019. Areviewofbinarizedneuralnetworks.
Electronics (Switzerland) 8. doi: 10.3390/electronics8060661 .
[217]Simonyan, K., Zisserman, A., 2014. Very Deep Convolutional
Networks for Large-Scale Image Recognition, in: International
Conference on Learning Representations(ICLR), pp. 1–14. URL:
http://arxiv.org/abs/1409.1556 .
[218]Singh,P.,KumarVerma,V.,Rai,P.,Namboodiri,V.P.,2019. Play
andPrune: AdaptiveFilterPruningforDeepModelCompression,in:
Proceedings of the Twenty-Eighth International Joint Conference on
ArtiﬁcialIntelligence,InternationalJointConferencesonArtiﬁcial
IntelligenceOrganization,California.pp.3460–3466. URL: https://
www.ijcai.org/proceedings/2019/480 , doi: 10.24963/ijcai.2019/480 .
[219]Society,I.C.,Committee,M.S.,2008. IEEEStandardforFloating-
Point Arithmetic. IEEE Std 754-2008 2008, 1–70. doi: 10.1109/
IEEESTD.2008.4610935 .
[220]Soudry,D.,Hubara,I.,Meir,R.,2014. Expectationbackpropagation:
Parameter-freetrainingofmultilayerneuralnetworkswithcontinuous
ordiscreteweights,in: AdvancesinNeuralInformationProcessing
Systems (NIPS), pp. 963–971. URL: https://dl.acm.org/doi/abs/
10.5555/2968826.2968934 .
[221]Srinivas, S., Babu, R.V., 2015. Data-free parameter pruning for
Deep Neural Networks, in: Procedings of the British Machine Vi-
sion Conference 2015, British Machine Vision Association. pp.
1–31. URL: http://www.bmva.org/bmvc/2015/papers/paper031/index.
htmlhttp://arxiv.org/abs/1507.06149 , doi: 10.5244/C.29.31 .
[222]Srivastava, N., Hinton, G., ..., A.K.T.j.o.m., 2014, U., 2014.
Dropout: asimplewaytopreventneuralnetworksfromoverﬁtting.
The journal of machine learning research 15, 1929–1958. URL:
http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.
pdf?utm_content=buffer79b43&utm_medium=social&utm_source=
twitter.com&utm_campaign=buffer , doi: 10.5555/2627435.2670313 .
[223]Sun, J., Luo, X., Gao, H., Wang, W., Gao, Y., Yang, X., 2020. Cate-
gorizingMalwareviaAWord2Vec-basedTemporalConvolutional
Network Scheme. Journal of Cloud Computing 9. doi: 10.1186/
s13677-020-00200-y .
[224]Sun, M., Song, Z., Jiang, X., Pan, J., Pang, Y., 2017. Learning
PoolingforConvolutionalNeuralNetwork.Neurocomputing224,96–
104. URL: http://dx.doi.org/10.1016/j.neucom.2016.10.049 ,doi: 10.
1016/j.neucom.2016.10.049 .
[225]Sze, V., Chen, Y.H.H., Yang, T.J.J., Emer, J.S., 2017. Eﬃcient
Processing of Deep Neural Networks: A Tutorial and Survey. Pro-
ceedings of the IEEE 105, 2295–2329. URL: http://ieeexplore.
T Liang et al.: Preprint submitted to Elsevier Page 38 of 41

--- PAGE 39 ---
Survey on pruning and quantization
ieee.org/document/8114708/ , doi: 10.1109/JPROC.2017.2761740 .
[226]Szegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,
Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper
with convolutions, in: Proceedings of the IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, IEEE. pp.
1–9. URL: http://ieeexplore.ieee.org/document/7298594/ , doi: 10.
1109/CVPR.2015.7298594 .
[227]TansorFlow, . Fixed Point Quantization. URL: https://www.
tensorflow.org/lite/guide .
[228]Technologies,Q.,2019. SnapdragonNeuralProcessingEngineSDK.
URL: https://developer.qualcomm.com/docs/snpe/index.html .
[229]Tencent,2019. NCNNisahigh-performanceneuralnetworkinfer-
ence framework optimized for the mobile platform. URL: https:
//github.com/Tencent/ncnn .
[230]Tishbirani, R., 1996. Regression shrinkage and selection via the
Lasso. URL: https://statweb.stanford.edu/~tibs/lasso/lasso.pdf .
[231]Umuroglu, Y., Fraser, N.J., Gambardella, G., Blott, M., Leong, P.,
Jahre, M., Vissers, K., 2016. FINN: A Framework for Fast, Scal-
ableBinarizedNeuralNetworkInference. Proceedingsofthe2017
ACM/SIGDAInternationalSymposiumonField-ProgrammableGate
Arrays - FPGA ’17 , 65–74URL: http://dl.acm.org/citation.cfm?
doid=3020078.3021744 , doi: 10.1145/3020078.3021744 .
[232]Vanholder,H.,2016. EﬃcientInferencewithTensorRT. Technical
Report.
[233]Vanhoucke,V.,Senior,A.,Mao,M.Z.,2011. Improvingthespeed
of neural networks on CPUs URL: https://research.google/pubs/
pub37631/ .
[234]Venieris, S.I., Kouris, A., Bouganis, C.S., 2018. Toolﬂows for Map-
ping Convolutional Neural Networks on FPGAs. ACM Comput-
ingSurveys51,1–39. URL: http://dl.acm.org/citation.cfm?doid=
3212709.3186332 , doi: 10.1145/3186332 .
[235]Venkatesh, G., Nurvitadhi, E., Marr, D., 2017. Accelerating
Deep Convolutional Networks using low-precision and sparsity,
in: 2017 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), IEEE. pp. 2861–2865. URL:
https://arxiv.org/pdf/1610.00324.pdfhttp://ieeexplore.ieee.org/
document/7952679/ , doi: 10.1109/ICASSP.2017.7952679 .
[236]Wang, K., Liu, Z., Lin, Y., Lin, J., Han, S., 2019a. HAQ: Hardware-
Aware Automated Quantization With Mixed Precision, in: 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), IEEE. pp. 8604–8612. URL: http://arxiv.org/abs/1811.
08886https://ieeexplore.ieee.org/document/8954415/ , doi: 10.1109/
CVPR.2019.00881 .
[237]Wang,N.,Choi,J.,Brand,D.,Chen,C.Y.,Gopalakrishnan,K.,2018a.
Trainingdeep neural networkswith 8-bitﬂoating pointnumbers, in:
Advances in Neural Information Processing Systems (NIPS), pp.
7675–7684.
[238]Wang, P., Cheng, J., 2017. Fixed-Point Factorized Networks, in:
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR),IEEE.pp.3966–3974. URL: http://ieeexplore.ieee.org/
document/8099905/ , doi: 10.1109/CVPR.2017.422 .
[239]Wang, P., Hu, Q., Zhang, Y., Zhang, C., Liu, Y., Cheng, J., 2018b.
Two-Step Quantization for Low-bit Neural Networks, in: Proceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition (CVPR), pp. 4376–4384. doi: 10.1109/CVPR.2018.00460 .
[240]Wang, Z., Lu, J., Tao, C., Zhou, J., Tian, Q., 2019b. Learning
channel-wise interactions for binary convolutional neural networks,
in: ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition (CVPR), pp. 568–577. doi: 10.1109/CVPR.
2019.00066 .
[241]Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H., 2016. Learning
Structured Sparsity in Deep Neural Networks, in: Advances in
Neural Information Processing Systems (NIPS), IEEE. pp. 2074–
2082. URL: https://dl.acm.org/doi/abs/10.5555/3157096.3157329 ,
doi:10.1016/j.ccr.2008.06.009 .
[242]Wu,H.,Judd,P.,Zhang,X.,Isaev,M.,Micikevicius,P.,2020. Integer
quantization for deep learning inference: Principles and empirical
evaluation. ArXiv preprint , 1–20.[243]Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J., 2016. Quantized Con-
volutional Neural Networks for Mobile Devices, in: IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR), IEEE.
pp. 4820–4828. URL: http://arxiv.org/abs/1512.06473http://
ieeexplore.ieee.org/document/7780890/ , doi: 10.1109/CVPR.2016.521 .
[244]Wu, S., Li, G., Chen, F., Shi, L., 2018a. Training and Inference with
IntegersinDeepNeuralNetworks,in: InternationalConferenceon
LearningRepresentations(ICLR). URL: http://arxiv.org/abs/1802.
04680.
[245]Wu, S., Li, G., Deng, L., Liu, L., Wu, D., Xie, Y., Shi, L., 2019.
L1-Norm Batch Normalization for Eﬃcient Training of Deep Neural
Networks. IEEETransactionsonNeuralNetworksandLearningSys-
tems30,2043–2051. URL: https://ieeexplore.ieee.org/abstract/
document/8528524/https://ieeexplore.ieee.org/document/8528524/ ,
doi:10.1109/TNNLS.2018.2876179 .
[246]Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L.S., Grau-
man, K., Feris, R., 2018b. BlockDrop: Dynamic Inference Paths
in Residual Networks, in: IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), IEEE. pp. 8817–8826.
URL: https://ieeexplore.ieee.org/document/8579017/ , doi: 10.1109/
CVPR.2018.00919 .
[247]Xiaomi, 2019. MACE is a deep learning inference framework
optimized for mobile heterogeneous computing platforms. URL:
https://github.com/XiaoMi/mace/ .
[248]Xilinx,Inc,2018. AcceleratingDNNswithXilinxAlveoAccelerator
Cards (WP504). White Paper 504, 1–11. URL: www.xilinx.com1 .
[249]Xu, J., Huan, Y., Zheng, L.R., Zou, Z., 2019. A Low-Power Arith-
metic Element for Multi-Base Logarithmic Computation on Deep
NeuralNetworks,in: InternationalSystemonChipConference,IEEE.
pp.260–265.URL: https://ieeexplore.ieee.org/document/8618560/ ,
doi:10.1109/SOCC.2018.8618560 .
[250]Xu, S., Huang, A., Chen, L., Zhang, B., 2020. Convolutional Neural
NetworkPruning: ASurvey,in: 202039thChineseControlConfer-
ence (CCC), IEEE. pp. 7458–7463. URL: https://ieeexplore.ieee.
org/document/9189610/ , doi: 10.23919/CCC50068.2020.9189610 .
[251]Xu, X., Lu, Q., Yang, L., Hu, S., Chen, D., Hu, Y., Shi, Y., 2018a.
Quantization of Fully Convolutional Networks for Accurate Biomed-
ical Image Segmentation, in: Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR), pp.
8300–8308. doi: 10.1109/CVPR.2018.00866 .
[252]Xu,Z.,Hsu,Y.C.,Huang,J.,2018b. Trainingshallowandthinnet-
works for acceleration via knowledge distillation with conditional
adversarial networks, in: International Conference on Learning Rep-
resentations (ICLR) - Workshop.
[253]Yang,J.,Shen,X.,Xing,J.,Tian,X.,Li,H.,Deng,B.,Huang,J.,Hua,
X.s., 2019. Quantization Networks, in: 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), IEEE. pp.
7300–7308. URL: https://ieeexplore.ieee.org/document/8953531/ ,
doi:10.1109/CVPR.2019.00748 .
[254]Yang,Y.,Deng,L.,Wu,S.,Yan,T.,Xie,Y.,Li,G.,2020. Training
high-performanceandlarge-scaledeepneuralnetworkswithfull8-bit
integers. Neural Networks 125, 70–82. doi: 10.1016/j.neunet.2019.
12.027.
[255]Ye, J., Lu, X., Lin, Z., Wang, J.Z., 2018. Rethinking the Smaller-
Norm-Less-Informative Assumption in Channel Pruning of Convolu-
tion Layers. ArXiv preprint URL: http://arxiv.org/abs/1802.00124 .
[256]Yin, P., Zhang, S., Lyu, J., Osher, S., Qi, Y., Xin, J., 2019.
Blended coarse gradient descent for full quantization of deep neu-
ral networks. Research in Mathematical Sciences 6. doi: 10.1007/
s40687-018-0177-6 .
[257]Yogatama, D., Mann, G., 2014. Eﬃcient Transfer Learning Method
for Automatic Hyperparameter Tuning, in: Kaski, S., Corander, J.
(Eds.), Proceedingsof theSeventeenth InternationalConferenceon
ArtiﬁcialIntelligenceandStatistics,PMLR,Reykjavik,Iceland.pp.
1077–1085. URL: http://proceedings.mlr.press/v33/yogatama14.
html.
[258]Yu, J., Lukefahr, A., Palframan, D., Dasika, G., Das, R., Mahlke, S.,
2017.Scalpel: CustomizingDNNpruningtotheunderlyinghardware
T Liang et al.: Preprint submitted to Elsevier Page 39 of 41

--- PAGE 40 ---
Survey on pruning and quantization
parallelism. ACM SIGARCH Computer Architecture News 45, 548–
560. URL: http://dl.acm.org/citation.cfm?doid=3140659.3080215 ,
doi:10.1145/3140659.3080215 .
[259]Yu, J., Yang, L., Xu, N., Yang, J., Huang, T., 2018. Slimmable Neu-
ralNetworks,in: InternationalConferenceonLearning Representa-
tions(ICLR), International Conference on Learning Representations,
ICLR. pp. 1–12. URL: http://arxiv.org/abs/1812.08928 .
[260]Yuan, M., Lin, Y., 2006. Model selection and estimation
in regression with grouped variables. Journal of the Royal
Statistical Society: Series B (Statistical Methodology) 68, 49–
67. URL: http://doi.wiley.com/10.1111/j.1467-9868.2005.00532.x ,
doi:10.1111/j.1467-9868.2005.00532.x .
[261]Yuan,Z.,Hu,J.,Wu,D.,Ban,X.,2020. Adual-attentionrecurrent
neural network method for deep cone thickener underﬂow concen-
tration prediction. Sensors (Switzerland) 20, 1–18. doi: 10.3390/
s20051260 .
[262]Zhang, D., Yang, J., Ye, D., Hua, G., 2018. LQ-Nets: Learned
quantizationforhighlyaccurateandcompactdeepneuralnetworks,
in: Lecture Notes in Computer Science (including subseries Lecture
Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics),
pp. 373–390. doi: 10.1007/978-3-030-01237-3{\_}23 .
[263]Zhang, Q., Zhang, M., Chen, T., Sun, Z., Ma, Y., Yu, B., 2019a.
Recent Advances in Convolutional Neural Network Acceleration.
Neurocomputing 323, 37–51. URL: https://linkinghub.elsevier.
com/retrieve/pii/S0925231218311007 , doi: 10.1016/j.neucom.2018.09.
038.
[264]Zhang, S., Du, Z., Zhang, L., Lan, H., Liu, S., Li, L., Guo, Q.,
Chen, T., Chen, Y., 2016a. Cambricon-X: An accelerator for
sparse neural networks, in: 2016 49th Annual IEEE/ACM Inter-
nationalSymposiumonMicroarchitecture(MICRO),IEEE.pp.1–12.
URL: http://ieeexplore.ieee.org/document/7783723/ , doi: 10.1109/
MICRO.2016.7783723 .
[265]Zhang,S.,Wu,Y.,Che,T.,Lin,Z.,Memisevic,R.,Salakhutdinov,R.,
Bengio,Y.,2016b. Architecturalcomplexitymeasuresofrecurrent
neural networks, in: Advances in Neural Information Processing
Systems (NIPS), pp. 1830–1838.
[266]Zhang,Y.,Zhao,C.,Ni,B.,Zhang, J.,Deng,H.,2019b. Exploiting
Channel Similarity for Accelerating Deep Convolutional Neural Net-
works. ArXivpreprint,1–14URL: http://arxiv.org/abs/1908.02620 .
[267]Zhao,R.,Song,W.,Zhang,W.,Xing,T.,Lin,J.H.,Srivastava,M.,
Gupta,R.,Zhang,Z.,2017. AcceleratingBinarizedConvolutional
Neural Networks with Software-Programmable FPGAs, in: Proceed-
ings of the 2017 ACM/SIGDA International Symposium on Field-
ProgrammableGateArrays-FPGA’17,ACMPress,NewYork,New
York,USA.pp.15–24. URL: http://dl.acm.org/citation.cfm?doid=
3020078.3021741 , doi: 10.1145/3020078.3021741 .
[268]Zhong,K.,Zhao,T.,Ning,X.,Zeng,S.,Guo,K.,Wang,Y.,Yang,H.,
2020. TowardsLowerBitMultiplicationforConvolutionalNeural
Network Training. ArXiv preprint URL: http://arxiv.org/abs/2006.
02804.
[269]Zhou, A., Yao, A., Guo, Y., Xu, L., Chen, Y., 2017a.
Incremental Network Quantization: Towards Lossless
CNNs with Low-Precision Weights, in: International
Conference on Learning Representations(ICLR). URL:
https://github.com/Zhouaojun/Incremental-http://arxiv.org/
abs/1702.03044http://cn.arxiv.org/pdf/1702.03044.pdf .
[270]Zhou, H., Alvarez, J.M., Porikli, F., 2016a. Less Is More: To-
wards Compact CNNs, in: European Conference on Computer
Vision, pp. 662–677. URL: https://link.springer.com/chapter/
10.1007/978-3-319-46493-0_40http://link.springer.com/10.1007/
978-3-319-46493-0_40 , doi: 10.1007/978-3-319-46493-0{\_}40 .
[271]Zhou,S., Kannan,R.,Prasanna, V.K., 2018. Acceleratinglow rank
matrixcompletiononFPGA,in: 2017InternationalConferenceon
Reconﬁgurable Computingand FPGAs, ReConFig 2017, IEEE. pp.
1–7. URL: http://ieeexplore.ieee.org/document/8279771/ , doi: 10.
1109/RECONFIG.2017.8279771 .
[272]Zhou,S.,Wu,Y.,Ni,Z.,Zhou,X.,Wen,H.,Zou,Y.,2016b. DoReFa-
Net: Training Low Bitwidth Convolutional Neural Networks withLow Bitwidth Gradients. ArXiv preprint abs/1606.0, 1–13. URL:
https://arxiv.org/abs/1606.06160 .
[273]Zhou,S.C.,Wang,Y.Z.,Wen,H.,He,Q.Y.,Zou,Y.H.,2017b. Bal-
ancedQuantization: AnEﬀectiveandEﬃcientApproachtoQuan-
tizedNeuralNetworks. JournalofComputerScienceandTechnology
32, 667–682. doi: 10.1007/s11390-017-1750-y .
[274]Zhu, C., Han, S., Mao, H., Dally, W.J., 2017. Trained Ternary Quan-
tization,in: InternationalConferenceonLearningRepresentations
(ICLR), pp. 1–10. URL: http://arxiv.org/abs/1612.01064 .
[275]Zhu,F.,Gong,R.,Yu,F.,Liu,X.,Wang,Y.,Li,Z.,Yang,X.,Yan,J.,
. Towards Uniﬁed INT8 Training for Convolutional Neural Network,
in: ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition (CVPR). URL: http://arxiv.org/abs/1912.
12607.
[276]Zhuang, B., Shen, C., Tan, M., Liu, L., Reid, I., 2019. Structured
binaryneuralnetworksforaccurateimageclassiﬁcationandsemantic
segmentation.ProceedingsoftheIEEEComputerSocietyConference
onComputerVisionandPatternRecognition2019-June,413–422.
doi:10.1109/CVPR.2019.00050 .
[277]Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V., 2017. Learning
Transferable Architectures for Scalable Image Recognition. Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , 8697–8710URL: https://ieeexplore.
ieee.org/abstract/document/8579005/ .
T Liang et al.: Preprint submitted to Elsevier Page 40 of 41

--- PAGE 41 ---
Survey on pruning and quantization
TailinLiang receivedtheB.E.degreeinComputer
Science and B.B.Afrom the University ofScience
and Technology Beijing in 2017. He is currently
working toward a Ph.D. degree in Computer Sci-
enceattheSchoolofComputerandCommunication
Engineering, University of Science and Technol-
ogy Beijing. His current research interests include
deep learning domain-speciﬁc processors and co-
designed optimization algorithms.
JohnGlossner receivedthePh.D.degreeinElec-
trical Engineering from TU Delft in 2001. He is
the Director of the Computer Architecture, Hetero-
geneousComputing,andAILabattheUniversity
of Science and Technology Beijing. He is also the
CEOofOptimumSemiconductorTechnologiesand
President of both the Heterogeneous System Archi-
tecture Foundation and Wireless Innovation Forum.
John’sresearchinterestsincludethedesignofhet-
erogeneous computing systems, computer architec-
ture,embeddedsystems,digitalsignalprocessors,
software deﬁned radios, artiﬁcial intelligence algo-
rithms, and machine learning systems.
Lei Wang receivedtheB.E.andPh.D.degreesin
2006 and 2012 from the University of Science and
Technology Beijing. He then served as an assistant
researcherattheInstituteofAutomationoftheChi-
neseAcademyofSciencesduring2012-2015. He
was a joint Ph.D. of Electronic Engineering at The
University of Texas at Dallas during 2009-2011.
Currently,heisanadjunctprofessorattheSchool
ofComputerandCommunicationEngineering,Uni-
versity of Science and Technology Beijing.
Shaobo Shi received the B.E. and Ph.D. degrees
in 2008 and 2014 from the University of Science
andTechnologyBeijing. Hethenservedasanassis-
tant researcher at the Institute of Automation of the
Chinese Academy of Sciences during 2014-2017.
Currently, he is a deep learning domain-speciﬁc
processor engineer at Huaxia General Processor
Technology. As well serveas an adjunct professor
at the School of Computer and Communication En-
gineering, University of Science and Technology
Beijing.
Xiaotong Zhang receivedtheM.E.andPh.D.de-
grees from theUniversityof Science and Technol-
ogyBeijingin1997and2000,respectively,where
he was a professorof Computer Science and Tech-
nology. Hisresearchinterestincludesthequalityof
wirelesschannelsandnetworks,wirelesssensornet-
works, networks management, cross-layer design
andresourceallocationofbroadbandandwireless
networks, and the signal processing of communica-
tion and computer architecture.
T Liang et al.: Preprint submitted to Elsevier Page 41 of 41
