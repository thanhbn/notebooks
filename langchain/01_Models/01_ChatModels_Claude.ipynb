{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models v·ªõi Claude - H∆∞·ªõng d·∫´n chi ti·∫øt\n",
    "\n",
    "Notebook n√†y cung c·∫•p h∆∞·ªõng d·∫´n to√†n di·ªán v·ªÅ c√°ch s·ª≠ d·ª•ng `ChatAnthropic` trong LangChain ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi c√°c m√¥ h√¨nh Claude.\n",
    "\n",
    "## N·ªôi dung ch√≠nh:\n",
    "1. Gi·ªõi thi·ªáu v·ªÅ Chat Models\n",
    "2. C√°c m√¥ h√¨nh Claude kh·∫£ d·ª•ng\n",
    "3. Tham s·ªë c·∫•u h√¨nh quan tr·ªçng\n",
    "4. C√°c lo·∫°i tin nh·∫Øn (Messages)\n",
    "5. Ph∆∞∆°ng th·ª©c invoke v√† stream\n",
    "6. C√°c use case n√¢ng cao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup m√¥i tr∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='../../.env')\n",
    "\n",
    "# Ki·ªÉm tra API key\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not api_key or api_key == 'your-api-key-here':\n",
    "    raise ValueError(\"Vui l√≤ng c·∫•u h√¨nh ANTHROPIC_API_KEY trong file .env\")\n",
    "\n",
    "print(\"‚úÖ Setup th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gi·ªõi thi·ªáu v·ªÅ Chat Models\n",
    "\n",
    "### Chat Models l√† g√¨?\n",
    "Chat Models l√† c√°c m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho c√°c cu·ªôc h·ªôi tho·∫°i. Kh√°c v·ªõi Text Models truy·ªÅn th·ªëng:\n",
    "- **Input**: Danh s√°ch c√°c messages (tin nh·∫Øn) thay v√¨ text ƒë∆°n thu·∫ßn\n",
    "- **Output**: AIMessage object ch·ª©a response c√≥ c·∫•u tr√∫c\n",
    "- **Context**: Duy tr√¨ ng·ªØ c·∫£nh h·ªôi tho·∫°i t·ªët h∆°n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. C√°c m√¥ h√¨nh Claude kh·∫£ d·ª•ng\n",
    "\n",
    "Anthropic cung c·∫•p 3 d√≤ng model Claude 3 v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm kh√°c nhau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh s√°ch c√°c model Claude 3 v√† ƒë·∫∑c ƒëi·ªÉm\n",
    "claude_models = {\n",
    "    \"claude-3-opus-20240229\": {\n",
    "        \"description\": \"M·∫°nh nh·∫•t, ph√π h·ª£p cho task ph·ª©c t·∫°p\",\n",
    "        \"context_window\": 200000,\n",
    "        \"cost\": \"Cao nh·∫•t\",\n",
    "        \"use_cases\": [\"Ph√¢n t√≠ch ph·ª©c t·∫°p\", \"Creative writing\", \"Code generation n√¢ng cao\"]\n",
    "    },\n",
    "    \"claude-3-sonnet-20240229\": {\n",
    "        \"description\": \"C√¢n b·∫±ng gi·ªØa hi·ªáu nƒÉng v√† chi ph√≠\",\n",
    "        \"context_window\": 200000,\n",
    "        \"cost\": \"Trung b√¨nh\",\n",
    "        \"use_cases\": [\"Chatbot\", \"T√≥m t·∫Øt vƒÉn b·∫£n\", \"Code review\"]\n",
    "    },\n",
    "    \"claude-3-haiku-20240307\": {\n",
    "        \"description\": \"Nhanh v√† ti·∫øt ki·ªám chi ph√≠\",\n",
    "        \"context_window\": 200000,\n",
    "        \"cost\": \"Th·∫•p nh·∫•t\",\n",
    "        \"use_cases\": [\"Classification\", \"Q&A ƒë∆°n gi·∫£n\", \"Data extraction\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin c√°c model\n",
    "for model_name, info in claude_models.items():\n",
    "    print(f\"\\nü§ñ {model_name}\")\n",
    "    print(f\"   üìù {info['description']}\")\n",
    "    print(f\"   üìä Context window: {info['context_window']:,} tokens\")\n",
    "    print(f\"   üí∞ Chi ph√≠: {info['cost']}\")\n",
    "    print(f\"   üéØ Use cases: {', '.join(info['use_cases'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tham s·ªë c·∫•u h√¨nh quan tr·ªçng\n",
    "\n",
    "Khi kh·ªüi t·∫°o ChatAnthropic, c√≥ nhi·ªÅu tham s·ªë quan tr·ªçng c·∫ßn hi·ªÉu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• v·ªÅ c√°c tham s·ªë kh√°c nhau\n",
    "# 1. Model v·ªõi temperature cao (creative)\n",
    "creative_model = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.9,  # 0-1: c√†ng cao c√†ng creative/random\n",
    "    max_tokens=500,   # Gi·ªõi h·∫°n ƒë·ªô d√†i output\n",
    "    anthropic_api_key=api_key\n",
    ")\n",
    "\n",
    "# 2. Model v·ªõi temperature th·∫•p (deterministic)\n",
    "precise_model = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.1,  # Output ·ªïn ƒë·ªãnh, √≠t bi·∫øn ƒë·ªïi\n",
    "    max_tokens=500,\n",
    "    anthropic_api_key=api_key\n",
    ")\n",
    "\n",
    "# 3. Model v·ªõi c·∫•u h√¨nh ƒë·∫ßy ƒë·ªß\n",
    "full_config_model = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    top_p=0.95,       # Nucleus sampling\n",
    "    top_k=40,         # Top-k sampling\n",
    "    timeout=30,       # Timeout trong gi√¢y\n",
    "    max_retries=2,    # S·ªë l·∫ßn retry khi l·ªói\n",
    "    anthropic_api_key=api_key\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o 3 model v·ªõi c·∫•u h√¨nh kh√°c nhau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So s√°nh output v·ªõi temperature kh√°c nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test c√πng m·ªôt prompt v·ªõi temperature kh√°c nhau\n",
    "test_prompt = \"Vi·∫øt m·ªôt c√¢u m·ªü ƒë·∫ßu cho c√¢u chuy·ªán v·ªÅ AI\"\n",
    "\n",
    "print(\"üé® Creative Model (temp=0.9):\")\n",
    "creative_response = creative_model.invoke(test_prompt)\n",
    "print(creative_response.content)\n",
    "\n",
    "print(\"\\nüéØ Precise Model (temp=0.1):\")\n",
    "precise_response = precise_model.invoke(test_prompt)\n",
    "print(precise_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. C√°c lo·∫°i tin nh·∫Øn (Message Types)\n",
    "\n",
    "LangChain h·ªó tr·ª£ nhi·ªÅu lo·∫°i message kh√°c nhau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import th√™m c√°c message types\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    FunctionMessage,\n",
    "    ToolMessage\n",
    ")\n",
    "\n",
    "# Kh·ªüi t·∫°o model cho v√≠ d·ª•\n",
    "chat = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.7,\n",
    "    anthropic_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SystemMessage - ƒê·ªãnh nghƒ©a vai tr√≤ v√† h√†nh vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 1: AI nh∆∞ m·ªôt chuy√™n gia\n",
    "messages = [\n",
    "    SystemMessage(content=\"\"\"B·∫°n l√† m·ªôt chuy√™n gia v·ªÅ Machine Learning v·ªõi 10 nƒÉm kinh nghi·ªám.\n",
    "    H√£y tr·∫£ l·ªùi c√°c c√¢u h·ªèi m·ªôt c√°ch chuy√™n nghi·ªáp, s·ª≠ d·ª•ng thu·∫≠t ng·ªØ k·ªπ thu·∫≠t khi c·∫ßn thi·∫øt,\n",
    "    v√† lu√¥n ƒë∆∞a ra v√≠ d·ª• c·ª• th·ªÉ.\"\"\"),\n",
    "    HumanMessage(content=\"Gradient Descent l√† g√¨?\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(\"üéì Expert Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 2: AI gi·∫£i th√≠ch cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu\n",
    "messages = [\n",
    "    SystemMessage(content=\"\"\"B·∫°n l√† m·ªôt gi√°o vi√™n ki√™n nh·∫´n, gi·∫£i th√≠ch m·ªçi th·ª© m·ªôt c√°ch\n",
    "    ƒë∆°n gi·∫£n nh·∫•t c√≥ th·ªÉ. Tr√°nh s·ª≠ d·ª•ng thu·∫≠t ng·ªØ ph·ª©c t·∫°p v√† lu√¥n d√πng v√≠ d·ª• t·ª´ cu·ªôc s·ªëng h√†ng ng√†y.\"\"\"),\n",
    "    HumanMessage(content=\"Gradient Descent l√† g√¨?\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(\"üë∂ Beginner-Friendly Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Chu·ªói h·ªôi tho·∫°i v·ªõi nhi·ªÅu messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√¢y d·ª±ng m·ªôt cu·ªôc h·ªôi tho·∫°i ƒë·∫ßy ƒë·ªß\n",
    "conversation = [\n",
    "    SystemMessage(content=\"B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch, lu√¥n nh·ªõ ng·ªØ c·∫£nh cu·ªôc tr√≤ chuy·ªán.\"),\n",
    "    HumanMessage(content=\"T√¥i ƒëang h·ªçc Python. B·∫°n c√≥ th·ªÉ g·ª£i √Ω m·ªôt d·ª± √°n cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu kh√¥ng?\"),\n",
    "    AIMessage(content=\"T√¥i g·ª£i √Ω b·∫°n l√†m m·ªôt ·ª©ng d·ª•ng To-Do List ƒë∆°n gi·∫£n. ƒê√¢y l√† d·ª± √°n tuy·ªát v·ªùi cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu v√¨ n√≥ bao g·ªìm c√°c kh√°i ni·ªám c∆° b·∫£n nh∆∞: input/output, l∆∞u tr·ªØ d·ªØ li·ªáu, v√† x·ª≠ l√Ω logic.\"),\n",
    "    HumanMessage(content=\"Nghe hay ƒë·∫•y! T√¥i n√™n b·∫Øt ƒë·∫ßu t·ª´ ƒë√¢u?\")\n",
    "]\n",
    "\n",
    "# G·ª≠i to√†n b·ªô conversation\n",
    "response = chat.invoke(conversation)\n",
    "print(\"üí¨ Conversation Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ph∆∞∆°ng th·ª©c Invoke vs Stream\n",
    "\n",
    "### 6.1 Invoke - Nh·∫≠n response ƒë·∫ßy ƒë·ªß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ƒêo th·ªùi gian v·ªõi invoke\n",
    "start_time = time.time()\n",
    "\n",
    "response = chat.invoke(\"Gi·∫£i th√≠ch v·ªÅ Neural Networks trong 3 c√¢u\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚è±Ô∏è Th·ªùi gian response: {end_time - start_time:.2f} gi√¢y\\n\")\n",
    "print(\"üìù Response:\")\n",
    "print(response.content)\n",
    "print(f\"\\nüìä Metadata:\")\n",
    "print(f\"- ID: {response.id}\")\n",
    "print(f\"- Model: {response.response_metadata.get('model', 'N/A')}\")\n",
    "print(f\"- Usage: {response.response_metadata.get('usage', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Stream - Nh·∫≠n response theo t·ª´ng ph·∫ßn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream cho tr·∫£i nghi·ªám real-time\n",
    "print(\"üåä Streaming Response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "stream_start = time.time()\n",
    "first_token_time = None\n",
    "token_count = 0\n",
    "\n",
    "for chunk in chat.stream(\"Vi·∫øt m·ªôt ƒëo·∫°n vƒÉn ng·∫Øn v·ªÅ t∆∞∆°ng lai c·ªßa AI trong gi√°o d·ª•c\"):\n",
    "    if first_token_time is None:\n",
    "        first_token_time = time.time()\n",
    "    \n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    token_count += 1\n",
    "\n",
    "stream_end = time.time()\n",
    "\n",
    "print(f\"\\n\\nüìä Stream Statistics:\")\n",
    "print(f\"- Time to first token: {first_token_time - stream_start:.2f}s\")\n",
    "print(f\"- Total time: {stream_end - stream_start:.2f}s\")\n",
    "print(f\"- Chunks received: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Async Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Async invoke\n",
    "async def async_chat_example():\n",
    "    # G·ªçi nhi·ªÅu requests ƒë·ªìng th·ªùi\n",
    "    prompts = [\n",
    "        \"ƒê·ªãnh nghƒ©a Machine Learning trong 1 c√¢u\",\n",
    "        \"ƒê·ªãnh nghƒ©a Deep Learning trong 1 c√¢u\",\n",
    "        \"ƒê·ªãnh nghƒ©a AI trong 1 c√¢u\"\n",
    "    ]\n",
    "    \n",
    "    # T·∫°o tasks\n",
    "    tasks = [chat.ainvoke(prompt) for prompt in prompts]\n",
    "    \n",
    "    # Ch·ªù t·∫•t c·∫£ ho√†n th√†nh\n",
    "    start = time.time()\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"‚ö° Async execution time: {end - start:.2f}s\\n\")\n",
    "    \n",
    "    for prompt, response in zip(prompts, responses):\n",
    "        print(f\"‚ùì {prompt}\")\n",
    "        print(f\"‚úÖ {response.content}\\n\")\n",
    "\n",
    "# Ch·∫°y async function\n",
    "await async_chat_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use Cases N√¢ng Cao\n",
    "\n",
    "### 7.1 Structured Output v·ªõi Response Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y√™u c·∫ßu output c√≥ c·∫•u tr√∫c\n",
    "structured_prompt = \"\"\"Ph√¢n t√≠ch c√¢u sau v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ theo format JSON:\n",
    "C√¢u: \"LangChain l√† m·ªôt framework m·∫°nh m·∫Ω cho vi·ªác ph√°t tri·ªÉn ·ª©ng d·ª•ng AI.\"\n",
    "\n",
    "Format mong mu·ªën:\n",
    "{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"key_entities\": [list of important entities],\n",
    "    \"main_topic\": \"topic of the sentence\",\n",
    "    \"confidence\": 0.0-1.0\n",
    "}\"\"\"\n",
    "\n",
    "response = chat.invoke(structured_prompt)\n",
    "print(\"üìã Structured Output:\")\n",
    "print(response.content)\n",
    "\n",
    "# Parse JSON response\n",
    "import json\n",
    "try:\n",
    "    parsed_response = json.loads(response.content)\n",
    "    print(\"\\n‚úÖ Parsed successfully:\")\n",
    "    for key, value in parsed_response.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è Response kh√¥ng ph·∫£i JSON h·ª£p l·ªá\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of Thought ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ ph·ª©c t·∫°p\n",
    "cot_messages = [\n",
    "    SystemMessage(content=\"\"\"B·∫°n l√† m·ªôt chuy√™n gia gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ. \n",
    "    Khi ƒë∆∞·ª£c h·ªèi, h√£y:\n",
    "    1. Ph√¢n t√≠ch v·∫•n ƒë·ªÅ t·ª´ng b∆∞·ªõc\n",
    "    2. Gi·∫£i th√≠ch logic c·ªßa m·ªói b∆∞·ªõc\n",
    "    3. ƒê∆∞a ra k·∫øt lu·∫≠n cu·ªëi c√πng\"\"\"),\n",
    "    HumanMessage(content=\"\"\"M·ªôt c·ª≠a h√†ng b√°n 3 lo·∫°i s·∫£n ph·∫©m:\n",
    "    - S·∫£n ph·∫©m A: gi√° 100k, l·ª£i nhu·∫≠n 30%\n",
    "    - S·∫£n ph·∫©m B: gi√° 200k, l·ª£i nhu·∫≠n 20%  \n",
    "    - S·∫£n ph·∫©m C: gi√° 150k, l·ª£i nhu·∫≠n 25%\n",
    "    \n",
    "    N·∫øu b√°n ƒë∆∞·ª£c 10A, 5B, v√† 8C trong ng√†y, t·ªïng l·ª£i nhu·∫≠n l√† bao nhi√™u?\"\"\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(cot_messages)\n",
    "print(\"üß† Chain of Thought Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Multi-turn Conversation v·ªõi Context Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qu·∫£n l√Ω context trong h·ªôi tho·∫°i d√†i\n",
    "class ConversationManager:\n",
    "    def __init__(self, chat_model, max_history=10):\n",
    "        self.chat = chat_model\n",
    "        self.history = []\n",
    "        self.max_history = max_history\n",
    "        self.system_message = SystemMessage(\n",
    "            content=\"B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh, lu√¥n nh·ªõ context cu·ªôc tr√≤ chuy·ªán.\"\n",
    "        )\n",
    "    \n",
    "    def add_message(self, role, content):\n",
    "        if role == \"human\":\n",
    "            self.history.append(HumanMessage(content=content))\n",
    "        elif role == \"ai\":\n",
    "            self.history.append(AIMessage(content=content))\n",
    "        \n",
    "        # Gi·ªõi h·∫°n history\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history = self.history[-self.max_history:]\n",
    "    \n",
    "    def get_response(self, user_input):\n",
    "        # Th√™m user input\n",
    "        self.add_message(\"human\", user_input)\n",
    "        \n",
    "        # T·∫°o messages list v·ªõi system message\n",
    "        messages = [self.system_message] + self.history\n",
    "        \n",
    "        # G·ªçi model\n",
    "        response = self.chat.invoke(messages)\n",
    "        \n",
    "        # L∆∞u response\n",
    "        self.add_message(\"ai\", response.content)\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "# Demo conversation manager\n",
    "conv_manager = ConversationManager(chat)\n",
    "\n",
    "# Cu·ªôc h·ªôi tho·∫°i m·∫´u\n",
    "conversation_flow = [\n",
    "    \"Xin ch√†o! T√¥i mu·ªën h·ªçc v·ªÅ Python.\",\n",
    "    \"T√¥i ƒë√£ bi·∫øt c√°c ki·∫øn th·ª©c c∆° b·∫£n r·ªìi. B∆∞·ªõc ti·∫øp theo l√† g√¨?\",\n",
    "    \"OOP nghe ph·ª©c t·∫°p qu√°. B·∫°n c√≥ th·ªÉ cho v√≠ d·ª• ƒë∆°n gi·∫£n kh√¥ng?\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(conversation_flow, 1):\n",
    "    print(f\"\\nüë§ Turn {i} - User: {user_input}\")\n",
    "    response = conv_manager.get_response(user_input)\n",
    "    print(f\"ü§ñ Claude: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices v√† Tips\n",
    "\n",
    "### 8.1 Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException\n",
    "from anthropic import RateLimitError, APIError\n",
    "\n",
    "def safe_chat_invoke(chat_model, messages, max_retries=3):\n",
    "    \"\"\"Invoke chat v·ªõi error handling ƒë·∫ßy ƒë·ªß\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = chat_model.invoke(messages)\n",
    "            return response\n",
    "        \n",
    "        except RateLimitError as e:\n",
    "            print(f\"‚ö†Ô∏è Rate limit reached. Waiting 60s... (Attempt {attempt + 1}/{max_retries})\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        except APIError as e:\n",
    "            print(f\"‚ùå API Error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test error handling\n",
    "test_messages = [HumanMessage(content=\"Hello Claude!\")]\n",
    "safe_response = safe_chat_invoke(chat, test_messages)\n",
    "if safe_response:\n",
    "    print(\"‚úÖ Safe invoke successful:\", safe_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Token Counting v√† Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ∆Ø·ªõc t√≠nh token v√† chi ph√≠\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"∆Ø·ªõc t√≠nh s·ªë token (rough estimate)\"\"\"\n",
    "    # Quy t·∫Øc th√¥: ~1 token = 4 k√Ω t·ª± cho ti·∫øng Anh\n",
    "    # Ti·∫øng Vi·ªát c√≥ th·ªÉ kh√°c\n",
    "    return len(text) / 4\n",
    "\n",
    "def estimate_cost(input_tokens, output_tokens, model=\"claude-3-sonnet-20240229\"):\n",
    "    \"\"\"∆Ø·ªõc t√≠nh chi ph√≠ (USD)\"\"\"\n",
    "    # Gi√° tham kh·∫£o (c√≥ th·ªÉ thay ƒë·ªïi)\n",
    "    pricing = {\n",
    "        \"claude-3-opus-20240229\": {\"input\": 0.015, \"output\": 0.075},\n",
    "        \"claude-3-sonnet-20240229\": {\"input\": 0.003, \"output\": 0.015},\n",
    "        \"claude-3-haiku-20240307\": {\"input\": 0.00025, \"output\": 0.00125}\n",
    "    }\n",
    "    \n",
    "    if model in pricing:\n",
    "        input_cost = (input_tokens / 1000) * pricing[model][\"input\"]\n",
    "        output_cost = (output_tokens / 1000) * pricing[model][\"output\"]\n",
    "        return input_cost + output_cost\n",
    "    return 0\n",
    "\n",
    "# Demo\n",
    "test_input = \"Gi·∫£i th√≠ch v·ªÅ Machine Learning\"\n",
    "response = chat.invoke(test_input)\n",
    "\n",
    "input_tokens = estimate_tokens(test_input)\n",
    "output_tokens = estimate_tokens(response.content)\n",
    "\n",
    "print(f\"üìä Token Usage Estimate:\")\n",
    "print(f\"- Input tokens: ~{input_tokens:.0f}\")\n",
    "print(f\"- Output tokens: ~{output_tokens:.0f}\")\n",
    "print(f\"- Total tokens: ~{input_tokens + output_tokens:.0f}\")\n",
    "\n",
    "cost = estimate_cost(input_tokens, output_tokens)\n",
    "print(f\"\\nüí∞ Estimated cost: ${cost:.6f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. T·ªïng k·∫øt\n",
    "\n",
    "Trong notebook n√†y, ch√∫ng ta ƒë√£ h·ªçc v·ªÅ:\n",
    "\n",
    "1. **Chat Models** - Hi·ªÉu r√µ v·ªÅ chat models v√† c√°ch ch√∫ng ho·∫°t ƒë·ªông\n",
    "2. **C√°c model Claude** - So s√°nh Opus, Sonnet, v√† Haiku\n",
    "3. **Tham s·ªë c·∫•u h√¨nh** - Temperature, max_tokens, v√† c√°c tham s·ªë kh√°c\n",
    "4. **Message Types** - SystemMessage, HumanMessage, AIMessage\n",
    "5. **Invoke vs Stream** - Khi n√†o d√πng ph∆∞∆°ng th·ª©c n√†o\n",
    "6. **Async operations** - X·ª≠ l√Ω ƒë·ªìng th·ªùi nhi·ªÅu requests\n",
    "7. **Advanced patterns** - Structured output, CoT, conversation management\n",
    "8. **Best practices** - Error handling, token counting, cost estimation\n",
    "\n",
    "### Next Steps:\n",
    "- Th·ª≠ nghi·ªám v·ªõi c√°c model kh√°c nhau\n",
    "- X√¢y d·ª±ng chatbot ho√†n ch·ªânh\n",
    "- T√≠ch h·ª£p v·ªõi vector stores cho RAG\n",
    "- Implement streaming trong web applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference card\n",
    "print(\"üéØ CLAUDE CHAT MODELS - QUICK REFERENCE\\n\")\n",
    "print(\"üìå Basic Usage:\")\n",
    "print(\"   chat = ChatAnthropic(model='claude-3-sonnet-20240229', temperature=0.7)\")\n",
    "print(\"   response = chat.invoke('Your prompt here')\\n\")\n",
    "\n",
    "print(\"üìå With Messages:\")\n",
    "print(\"   messages = [\")\n",
    "print(\"       SystemMessage(content='You are a helpful assistant'),\")\n",
    "print(\"       HumanMessage(content='Hello!')\")\n",
    "print(\"   ]\")\n",
    "print(\"   response = chat.invoke(messages)\\n\")\n",
    "\n",
    "print(\"üìå Streaming:\")\n",
    "print(\"   for chunk in chat.stream('Your prompt'):\")\n",
    "print(\"       print(chunk.content, end='')\\n\")\n",
    "\n",
    "print(\"üìå Key Parameters:\")\n",
    "print(\"   - temperature: 0.0-1.0 (creativity)\")\n",
    "print(\"   - max_tokens: output length limit\")\n",
    "print(\"   - model: opus/sonnet/haiku variants\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
