{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Focus: Contrastive Triplet Learning\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "- Master contrastive learning principles and triplet loss formulation\n",
    "- Understand hard negative mining strategies for Arabic text\n",
    "- Implement advanced negative sampling techniques\n",
    "- Explore InfoNCE limitations and solutions\n",
    "- Develop effective triplet dataset construction methods\n",
    "\n",
    "## 📚 Paper Context\n",
    "**From GATE Paper (Section 1):**\n",
    "> *\"At the heart of many highly effective embedding models lies contrastive learning, a paradigm that optimizes the quality of representation by pulling semantically similar (positive) samples closer while pushing dissimilar (negative) samples apart. Despite the versatility and success of contrastive learning, most existing text embedding pipelines rely on InfoNCE loss with in-batch negative samples, achieving robust representations predominantly by using large batch sizes and numerous negative samples.\"*\n",
    "\n",
    "**Key Challenges Identified:**\n",
    "1. **InfoNCE Limitations**: Not sufficient for all downstream tasks\n",
    "2. **Sentence-level Tasks**: STS benefits less from InfoNCE-based training\n",
    "3. **Fine-grained Similarity**: InfoNCE struggles with subtle semantic differences\n",
    "4. **Negative Sampling**: Quality of negatives crucial for learning\n",
    "5. **Arabic Specificity**: Need for Arabic-tailored contrastive learning\n",
    "\n",
    "## 🔑 GATE's Innovation\n",
    "GATE uses **Arabic NLI triplet datasets** with curated hard negatives, moving beyond simple InfoNCE to capture fine-grained semantic relationships crucial for Arabic text understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup for Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for contrastive learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced sampling and mining\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"🔥 Contrastive Learning Environment Ready!\")\n",
    "print(f\"📱 Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"🎯 Focus: Advanced contrastive and triplet learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧮 Mathematical Foundation: Contrastive Learning\n",
    "\n",
    "### Understanding InfoNCE and Triplet Loss Formulations\n",
    "Let's implement and analyze the mathematical foundations of contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLearningFoundation:\n",
    "    \"\"\"Mathematical foundation and analysis of contrastive learning\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07, margin=0.5):\n",
    "        self.temperature = temperature\n",
    "        self.margin = margin\n",
    "        \n",
    "    def infonce_loss(self, anchor, positive, negatives, return_components=False):\n",
    "        \"\"\"\n",
    "        Compute InfoNCE loss\n",
    "        \n",
    "        InfoNCE: L = -log(exp(sim(a,p)/τ) / (exp(sim(a,p)/τ) + Σ exp(sim(a,n)/τ)))\n",
    "        \n",
    "        Args:\n",
    "            anchor: [batch_size, dim] anchor embeddings\n",
    "            positive: [batch_size, dim] positive embeddings  \n",
    "            negatives: [batch_size, num_negatives, dim] negative embeddings\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
    "        \n",
    "        # Compute negative similarities\n",
    "        neg_sims = []\n",
    "        for i in range(batch_size):\n",
    "            anchor_expanded = anchor[i:i+1].expand(negatives.size(1), -1)\n",
    "            neg_sim = F.cosine_similarity(anchor_expanded, negatives[i], dim=1) / self.temperature\n",
    "            neg_sims.append(neg_sim)\n",
    "        \n",
    "        # Compute InfoNCE loss\n",
    "        losses = []\n",
    "        for i in range(batch_size):\n",
    "            numerator = torch.exp(pos_sim[i])\n",
    "            denominator = numerator + torch.sum(torch.exp(neg_sims[i]))\n",
    "            loss = -torch.log(numerator / denominator)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        final_loss = torch.stack(losses).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': [neg_sim.detach() for neg_sim in neg_sims],\n",
    "                'individual_losses': torch.stack(losses).detach()\n",
    "            }\n",
    "            return final_loss, components\n",
    "        \n",
    "        return final_loss\n",
    "    \n",
    "    def triplet_loss(self, anchor, positive, negative, return_components=False):\n",
    "        \"\"\"\n",
    "        Compute triplet loss with margin\n",
    "        \n",
    "        Triplet: L = max(0, margin + sim(a,n) - sim(a,p))\n",
    "        \n",
    "        Args:\n",
    "            anchor: [batch_size, dim] anchor embeddings\n",
    "            positive: [batch_size, dim] positive embeddings\n",
    "            negative: [batch_size, dim] negative embeddings\n",
    "        \"\"\"\n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1)\n",
    "        neg_sim = F.cosine_similarity(anchor, negative, dim=1)\n",
    "        \n",
    "        # Triplet loss with margin\n",
    "        loss = torch.clamp(self.margin + neg_sim - pos_sim, min=0.0).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': neg_sim.detach(),\n",
    "                'margin_violations': (neg_sim - pos_sim + self.margin).detach()\n",
    "            }\n",
    "            return loss, components\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def multiple_negatives_ranking_loss(self, anchor, positive, negatives, return_components=False):\n",
    "        \"\"\"\n",
    "        Multiple Negatives Ranking Loss (used in sentence-transformers)\n",
    "        \n",
    "        Similar to InfoNCE but designed for ranking tasks\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
    "        \n",
    "        # Compute all negative similarities\n",
    "        all_neg_sims = []\n",
    "        for i in range(batch_size):\n",
    "            for j in range(negatives.size(1)):\n",
    "                neg_sim = F.cosine_similarity(\n",
    "                    anchor[i:i+1], negatives[i, j:j+1], dim=1\n",
    "                ) / self.temperature\n",
    "                all_neg_sims.append(neg_sim)\n",
    "        \n",
    "        # Combine positive and negative similarities\n",
    "        all_sims = torch.cat([pos_sim] + all_neg_sims)\n",
    "        \n",
    "        # Create labels (first batch_size are positives)\n",
    "        labels = torch.arange(batch_size).to(anchor.device)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        logits = all_sims.view(batch_size, -1)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'logits': logits.detach(),\n",
    "                'labels': labels.detach(),\n",
    "                'positive_similarities': pos_sim.detach()\n",
    "            }\n",
    "            return loss, components\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def demonstrate_loss_behaviors(self):\n",
    "        \"\"\"Demonstrate different loss function behaviors\"\"\"\n",
    "        print(\"🧮 Contrastive Loss Function Analysis\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Create sample embeddings\n",
    "        batch_size, dim, num_negatives = 4, 128, 3\n",
    "        \n",
    "        anchor = torch.randn(batch_size, dim)\n",
    "        positive = torch.randn(batch_size, dim)\n",
    "        negatives = torch.randn(batch_size, num_negatives, dim)\n",
    "        \n",
    "        # Make positives more similar to anchors\n",
    "        positive = 0.7 * anchor + 0.3 * positive\n",
    "        \n",
    "        # Make some negatives harder (more similar to anchor)\n",
    "        negatives[:, 0] = 0.3 * anchor + 0.7 * negatives[:, 0]  # Hard negatives\n",
    "        \n",
    "        print(f\"📊 Sample Setup:\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Embedding dimension: {dim}\")\n",
    "        print(f\"   Number of negatives: {num_negatives}\")\n",
    "        print(f\"   Temperature: {self.temperature}\")\n",
    "        print(f\"   Margin: {self.margin}\")\n",
    "        \n",
    "        # Test InfoNCE\n",
    "        print(f\"\\n🔍 InfoNCE Loss Analysis:\")\n",
    "        infonce_loss, infonce_components = self.infonce_loss(\n",
    "            anchor, positive, negatives, return_components=True\n",
    "        )\n",
    "        print(f\"   Loss value: {infonce_loss.item():.4f}\")\n",
    "        print(f\"   Positive similarities: {infonce_components['positive_similarities'].tolist()}\")\n",
    "        \n",
    "        # Test Triplet Loss\n",
    "        print(f\"\\n🔍 Triplet Loss Analysis:\")\n",
    "        triplet_loss, triplet_components = self.triplet_loss(\n",
    "            anchor, positive, negatives[:, 0], return_components=True  # Use first negative\n",
    "        )\n",
    "        print(f\"   Loss value: {triplet_loss.item():.4f}\")\n",
    "        print(f\"   Positive similarities: {triplet_components['positive_similarities'].tolist()}\")\n",
    "        print(f\"   Negative similarities: {triplet_components['negative_similarities'].tolist()}\")\n",
    "        print(f\"   Margin violations: {triplet_components['margin_violations'].tolist()}\")\n",
    "        \n",
    "        # Test Multiple Negatives Ranking\n",
    "        print(f\"\\n🔍 Multiple Negatives Ranking Loss:\")\n",
    "        mnr_loss, mnr_components = self.multiple_negatives_ranking_loss(\n",
    "            anchor, positive, negatives, return_components=True\n",
    "        )\n",
    "        print(f\"   Loss value: {mnr_loss.item():.4f}\")\n",
    "        print(f\"   Logits shape: {mnr_components['logits'].shape}\")\n",
    "        \n",
    "        return {\n",
    "            'infonce': (infonce_loss, infonce_components),\n",
    "            'triplet': (triplet_loss, triplet_components),\n",
    "            'mnr': (mnr_loss, mnr_components)\n",
    "        }\n",
    "    \n",
    "    def analyze_temperature_effects(self):\n",
    "        \"\"\"Analyze effects of temperature on contrastive learning\"\"\"\n",
    "        print(\"\\n🌡️ Temperature Effects on Contrastive Learning\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Sample similarities\n",
    "        similarities = torch.tensor([0.9, 0.7, 0.5, 0.3, 0.1, -0.1])\n",
    "        temperatures = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "        \n",
    "        print(f\"📊 Raw similarities: {similarities.tolist()}\")\n",
    "        print(f\"\\nTemperature effects on softmax distribution:\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            scaled = similarities / temp\n",
    "            softmax_probs = F.softmax(scaled, dim=0)\n",
    "            \n",
    "            print(f\"\\n   τ = {temp:4.2f}:\")\n",
    "            print(f\"      Scaled: {scaled.tolist()}\")\n",
    "            print(f\"      Softmax: {softmax_probs.tolist()}\")\n",
    "            print(f\"      Max prob: {torch.max(softmax_probs).item():.3f}\")\n",
    "            print(f\"      Entropy: {-torch.sum(softmax_probs * torch.log(softmax_probs + 1e-8)).item():.3f}\")\n",
    "            \n",
    "            # Concentration analysis\n",
    "            concentration = (softmax_probs[0] / torch.sum(softmax_probs[1:])).item()\n",
    "            print(f\"      Concentration ratio: {concentration:.3f}\")\n",
    "        \n",
    "        return temperatures, similarities\n",
    "\n",
    "# Initialize foundation and run analysis\n",
    "contrastive_foundation = ContrastiveLearningFoundation(temperature=0.07, margin=0.5)\n",
    "loss_analysis = contrastive_foundation.demonstrate_loss_behaviors()\n",
    "temperature_analysis = contrastive_foundation.analyze_temperature_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Hard Negative Mining Strategies\n",
    "\n",
    "### Advanced Negative Sampling for Arabic Text\n",
    "The quality of negative samples is crucial for effective contrastive learning. Let's implement sophisticated mining strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ArabicTriplet:\n",
    "    \"\"\"Data structure for Arabic text triplets\"\"\"\n",
    "    anchor: str\n",
    "    positive: str\n",
    "    negative: str\n",
    "    anchor_embedding: Optional[torch.Tensor] = None\n",
    "    positive_embedding: Optional[torch.Tensor] = None\n",
    "    negative_embedding: Optional[torch.Tensor] = None\n",
    "    difficulty: float = 0.0  # Mining difficulty score\n",
    "    category: str = \"general\"  # Semantic category\n",
    "\n",
    "class HardNegativeMiner:\n",
    "    \"\"\"Advanced hard negative mining for Arabic text\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=768, similarity_threshold=0.7):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        \n",
    "        # Strategies for negative mining\n",
    "        self.mining_strategies = {\n",
    "            'random': self.random_negative_mining,\n",
    "            'hard': self.hard_negative_mining,\n",
    "            'semi_hard': self.semi_hard_negative_mining,\n",
    "            'cluster_based': self.cluster_based_mining,\n",
    "            'semantic': self.semantic_category_mining\n",
    "        }\n",
    "    \n",
    "    def create_mock_arabic_corpus(self):\n",
    "        \"\"\"Create mock Arabic text corpus for demonstration\"\"\"\n",
    "        \n",
    "        corpus = {\n",
    "            'education': [\n",
    "                \"الطالب يدرس في المكتبة بجد واجتهاد\",\n",
    "                \"المعلم يشرح الدرس للطلاب في الفصل\",\n",
    "                \"الجامعة تقدم برامج تعليمية متنوعة ومتميزة\",\n",
    "                \"البحث العلمي يساهم في تطوير المعرفة\",\n",
    "                \"التعليم الإلكتروني أصبح ضرورة في العصر الحديث\"\n",
    "            ],\n",
    "            'technology': [\n",
    "                \"الذكاء الاصطناعي يغير مستقبل التكنولوجيا\",\n",
    "                \"الحاسوب أداة مهمة في العمل والحياة\",\n",
    "                \"الإنترنت يربط العالم ببعضه البعض\",\n",
    "                \"البرمجيات تسهل الكثير من المهام اليومية\",\n",
    "                \"الهواتف الذكية تحتوي على تقنيات متقدمة\"\n",
    "            ],\n",
    "            'health': [\n",
    "                \"الرياضة تحافظ على صحة الجسم والعقل\",\n",
    "                \"التغذية الصحية أساس الحياة السليمة\",\n",
    "                \"الطبيب يعالج المرضى بخبرة ومهارة\",\n",
    "                \"المستشفى يقدم خدمات طبية متطورة\",\n",
    "                \"الوقاية خير من العلاج في جميع الأحوال\"\n",
    "            ],\n",
    "            'culture': [\n",
    "                \"الثقافة العربية تحتوي على تراث غني ومتنوع\",\n",
    "                \"الشعر العربي له تاريخ طويل ومجيد\",\n",
    "                \"الفنون التراثية تعبر عن هوية الشعوب\",\n",
    "                \"اللغة العربية لغة الضاد والبيان\",\n",
    "                \"الأدب العربي يحتوي على كنوز من المعرفة\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return corpus\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Generate mock embeddings for texts\"\"\"\n",
    "        embeddings = torch.randn(len(texts), self.embedding_dim)\n",
    "        \n",
    "        # Add some semantic structure (texts with similar words get similar embeddings)\n",
    "        for i, text1 in enumerate(texts):\n",
    "            words1 = set(text1.split())\n",
    "            for j, text2 in enumerate(texts):\n",
    "                if i != j:\n",
    "                    words2 = set(text2.split())\n",
    "                    overlap = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "                    \n",
    "                    # Make embeddings more similar based on word overlap\n",
    "                    if overlap > 0.2:\n",
    "                        embeddings[j] = 0.3 * embeddings[i] + 0.7 * embeddings[j]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def random_negative_mining(self, anchor_text: str, positive_text: str, \n",
    "                              corpus: Dict[str, List[str]], num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Random negative sampling (baseline)\"\"\"\n",
    "        all_texts = []\n",
    "        for category_texts in corpus.values():\n",
    "            all_texts.extend(category_texts)\n",
    "        \n",
    "        # Remove anchor and positive from candidates\n",
    "        candidates = [text for text in all_texts if text not in [anchor_text, positive_text]]\n",
    "        \n",
    "        return random.sample(candidates, min(num_negatives, len(candidates)))\n",
    "    \n",
    "    def hard_negative_mining(self, anchor_embedding: torch.Tensor, positive_embedding: torch.Tensor,\n",
    "                            candidate_embeddings: torch.Tensor, candidate_texts: List[str],\n",
    "                            num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Hard negative mining - select most similar negatives\"\"\"\n",
    "        # Compute similarities with anchor\n",
    "        similarities = F.cosine_similarity(\n",
    "            anchor_embedding.unsqueeze(0), candidate_embeddings, dim=1\n",
    "        )\n",
    "        \n",
    "        # Get indices of most similar (hardest) negatives\n",
    "        _, hard_indices = torch.topk(similarities, min(num_negatives, len(similarities)))\n",
    "        \n",
    "        return [candidate_texts[idx] for idx in hard_indices.tolist()]\n",
    "    \n",
    "    def semi_hard_negative_mining(self, anchor_embedding: torch.Tensor, positive_embedding: torch.Tensor,\n",
    "                                 candidate_embeddings: torch.Tensor, candidate_texts: List[str],\n",
    "                                 num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Semi-hard negative mining - negatives closer to anchor than positive\"\"\"\n",
    "        # Compute similarities\n",
    "        anchor_pos_sim = F.cosine_similarity(anchor_embedding, positive_embedding, dim=0)\n",
    "        anchor_neg_sims = F.cosine_similarity(\n",
    "            anchor_embedding.unsqueeze(0), candidate_embeddings, dim=1\n",
    "        )\n",
    "        \n",
    "        # Semi-hard condition: similarity(anchor, negative) > similarity(anchor, positive)\n",
    "        semi_hard_mask = anchor_neg_sims > anchor_pos_sim\n",
    "        semi_hard_indices = torch.where(semi_hard_mask)[0]\n",
    "        \n",
    "        if len(semi_hard_indices) == 0:\n",
    "            # Fallback to hard negatives if no semi-hard found\n",
    "            return self.hard_negative_mining(\n",
    "                anchor_embedding, positive_embedding, candidate_embeddings, candidate_texts, num_negatives\n",
    "            )\n",
    "        \n",
    "        # Select from semi-hard negatives\n",
    "        selected_indices = semi_hard_indices[\n",
    "            torch.randperm(len(semi_hard_indices))[:min(num_negatives, len(semi_hard_indices))]\n",
    "        ]\n",
    "        \n",
    "        return [candidate_texts[idx] for idx in selected_indices.tolist()]\n",
    "    \n",
    "    def cluster_based_mining(self, anchor_embedding: torch.Tensor, positive_embedding: torch.Tensor,\n",
    "                            candidate_embeddings: torch.Tensor, candidate_texts: List[str],\n",
    "                            num_negatives: int = 5, num_clusters: int = 3) -> List[str]:\n",
    "        \"\"\"Cluster-based negative mining for diverse negatives\"\"\"\n",
    "        # Perform clustering on candidate embeddings\n",
    "        if len(candidate_embeddings) < num_clusters:\n",
    "            return self.random_negative_mining(None, None, {'all': candidate_texts}, num_negatives)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(candidate_embeddings.numpy())\n",
    "        \n",
    "        # Sample negatives from different clusters\n",
    "        negatives = []\n",
    "        negatives_per_cluster = max(1, num_negatives // num_clusters)\n",
    "        \n",
    "        for cluster_id in range(num_clusters):\n",
    "            cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "            if len(cluster_indices) > 0:\n",
    "                selected = np.random.choice(\n",
    "                    cluster_indices, \n",
    "                    min(negatives_per_cluster, len(cluster_indices)), \n",
    "                    replace=False\n",
    "                )\n",
    "                negatives.extend([candidate_texts[idx] for idx in selected])\n",
    "        \n",
    "        return negatives[:num_negatives]\n",
    "    \n",
    "    def semantic_category_mining(self, anchor_text: str, positive_text: str,\n",
    "                                corpus: Dict[str, List[str]], num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Semantic category-based mining - avoid same category negatives\"\"\"\n",
    "        # Find anchor category\n",
    "        anchor_category = None\n",
    "        for category, texts in corpus.items():\n",
    "            if anchor_text in texts:\n",
    "                anchor_category = category\n",
    "                break\n",
    "        \n",
    "        # Sample from different categories\n",
    "        negatives = []\n",
    "        for category, texts in corpus.items():\n",
    "            if category != anchor_category:\n",
    "                available = [text for text in texts if text not in [anchor_text, positive_text]]\n",
    "                if available:\n",
    "                    negatives.extend(random.sample(available, min(2, len(available))))\n",
    "        \n",
    "        return negatives[:num_negatives]\n",
    "    \n",
    "    def create_arabic_triplets(self, corpus: Dict[str, List[str]], \n",
    "                              strategy: str = 'hard', num_triplets: int = 20) -> List[ArabicTriplet]:\n",
    "        \"\"\"Create Arabic triplets using specified mining strategy\"\"\"\n",
    "        print(f\"🎯 Creating Arabic Triplets with {strategy} mining\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        triplets = []\n",
    "        all_texts = []\n",
    "        \n",
    "        # Collect all texts\n",
    "        for category_texts in corpus.values():\n",
    "            all_texts.extend(category_texts)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        all_embeddings = self.generate_embeddings(all_texts)\n",
    "        text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n",
    "        \n",
    "        triplet_count = 0\n",
    "        \n",
    "        # Create triplets for each category\n",
    "        for category, texts in corpus.items():\n",
    "            if len(texts) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Create positive pairs within category\n",
    "            for i in range(len(texts)):\n",
    "                for j in range(i + 1, len(texts)):\n",
    "                    if triplet_count >= num_triplets:\n",
    "                        break\n",
    "                        \n",
    "                    anchor_text = texts[i]\n",
    "                    positive_text = texts[j]\n",
    "                    \n",
    "                    # Get embeddings\n",
    "                    anchor_emb = text_to_embedding[anchor_text]\n",
    "                    positive_emb = text_to_embedding[positive_text]\n",
    "                    \n",
    "                    # Mine negatives based on strategy\n",
    "                    if strategy in ['hard', 'semi_hard', 'cluster_based']:\n",
    "                        # Get candidate negatives (exclude same category for diversity)\n",
    "                        candidate_texts = []\n",
    "                        candidate_embeddings = []\n",
    "                        \n",
    "                        for other_category, other_texts in corpus.items():\n",
    "                            if other_category != category:\n",
    "                                candidate_texts.extend(other_texts)\n",
    "                                candidate_embeddings.extend([\n",
    "                                    text_to_embedding[text] for text in other_texts\n",
    "                                ])\n",
    "                        \n",
    "                        if candidate_embeddings:\n",
    "                            candidate_embeddings = torch.stack(candidate_embeddings)\n",
    "                            negatives = self.mining_strategies[strategy](\n",
    "                                anchor_emb, positive_emb, candidate_embeddings, candidate_texts, 1\n",
    "                            )\n",
    "                        else:\n",
    "                            negatives = self.random_negative_mining(anchor_text, positive_text, corpus, 1)\n",
    "                    else:\n",
    "                        negatives = self.mining_strategies[strategy](anchor_text, positive_text, corpus, 1)\n",
    "                    \n",
    "                    if negatives:\n",
    "                        negative_text = negatives[0]\n",
    "                        negative_emb = text_to_embedding[negative_text]\n",
    "                        \n",
    "                        # Calculate difficulty score\n",
    "                        pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=0).item()\n",
    "                        neg_sim = F.cosine_similarity(anchor_emb, negative_emb, dim=0).item()\n",
    "                        difficulty = neg_sim - pos_sim + 1.0  # Higher = more difficult\n",
    "                        \n",
    "                        triplet = ArabicTriplet(\n",
    "                            anchor=anchor_text,\n",
    "                            positive=positive_text,\n",
    "                            negative=negative_text,\n",
    "                            anchor_embedding=anchor_emb,\n",
    "                            positive_embedding=positive_emb,\n",
    "                            negative_embedding=negative_emb,\n",
    "                            difficulty=difficulty,\n",
    "                            category=category\n",
    "                        )\n",
    "                        \n",
    "                        triplets.append(triplet)\n",
    "                        triplet_count += 1\n",
    "                        \n",
    "                        print(f\"   Triplet {triplet_count}: {category} (difficulty: {difficulty:.3f})\")\n",
    "                        print(f\"      Anchor: {anchor_text[:50]}...\")\n",
    "                        print(f\"      Positive: {positive_text[:50]}...\")\n",
    "                        print(f\"      Negative: {negative_text[:50]}...\")\n",
    "                        print(f\"      Pos sim: {pos_sim:.3f}, Neg sim: {neg_sim:.3f}\")\n",
    "                        print()\n",
    "                \n",
    "                if triplet_count >= num_triplets:\n",
    "                    break\n",
    "            \n",
    "            if triplet_count >= num_triplets:\n",
    "                break\n",
    "        \n",
    "        print(f\"✅ Created {len(triplets)} Arabic triplets using {strategy} mining\")\n",
    "        return triplets\n",
    "    \n",
    "    def analyze_mining_strategies(self, corpus: Dict[str, List[str]]) -> Dict[str, List[ArabicTriplet]]:\n",
    "        \"\"\"Compare different mining strategies\"\"\"\n",
    "        print(\"\\n🔍 Comparing Mining Strategies\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        strategies_results = {}\n",
    "        \n",
    "        for strategy in ['random', 'hard', 'semi_hard', 'semantic']:\n",
    "            print(f\"\\n📊 Testing {strategy} strategy:\")\n",
    "            triplets = self.create_arabic_triplets(corpus, strategy, num_triplets=5)\n",
    "            strategies_results[strategy] = triplets\n",
    "            \n",
    "            # Analyze strategy characteristics\n",
    "            if triplets:\n",
    "                difficulties = [t.difficulty for t in triplets]\n",
    "                avg_difficulty = np.mean(difficulties)\n",
    "                std_difficulty = np.std(difficulties)\n",
    "                \n",
    "                print(f\"   Average difficulty: {avg_difficulty:.3f}\")\n",
    "                print(f\"   Difficulty std: {std_difficulty:.3f}\")\n",
    "                print(f\"   Difficulty range: {min(difficulties):.3f} - {max(difficulties):.3f}\")\n",
    "        \n",
    "        return strategies_results\n",
    "\n",
    "# Initialize miner and create corpus\n",
    "hard_negative_miner = HardNegativeMiner()\n",
    "arabic_corpus = hard_negative_miner.create_mock_arabic_corpus()\n",
    "\n",
    "print(\"📚 Arabic Corpus Created:\")\n",
    "for category, texts in arabic_corpus.items():\n",
    "    print(f\"   {category}: {len(texts)} texts\")\n",
    "\n",
    "# Test different mining strategies\n",
    "mining_comparison = hard_negative_miner.analyze_mining_strategies(arabic_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Advanced Triplet Training Framework\n",
    "\n",
    "### Complete Implementation for Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicTripletTrainer:\n",
    "    \"\"\"Advanced triplet training framework for Arabic text embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=768, margin=0.5, temperature=0.07, \n",
    "                 mining_strategy='hard', curriculum_learning=True):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "        self.mining_strategy = mining_strategy\n",
    "        self.curriculum_learning = curriculum_learning\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'losses': [],\n",
    "            'difficulties': [],\n",
    "            'positive_similarities': [],\n",
    "            'negative_similarities': [],\n",
    "            'margin_violations': []\n",
    "        }\n",
    "        \n",
    "        # Curriculum learning parameters\n",
    "        self.curriculum_stages = {\n",
    "            'easy': {'difficulty_threshold': 0.3, 'epochs': 2},\n",
    "            'medium': {'difficulty_threshold': 0.6, 'epochs': 3},\n",
    "            'hard': {'difficulty_threshold': 1.0, 'epochs': 5}\n",
    "        }\n",
    "        \n",
    "    def triplet_loss_with_hard_negatives(self, anchor, positive, negative, \n",
    "                                        return_components=False):\n",
    "        \"\"\"\n",
    "        Advanced triplet loss with multiple negatives and adaptive margin\n",
    "        \"\"\"\n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1)\n",
    "        neg_sim = F.cosine_similarity(anchor, negative, dim=1)\n",
    "        \n",
    "        # Adaptive margin based on positive similarity\n",
    "        adaptive_margin = self.margin * (1.0 - pos_sim.detach())\n",
    "        \n",
    "        # Triplet loss with adaptive margin\n",
    "        loss = torch.clamp(adaptive_margin + neg_sim - pos_sim, min=0.0)\n",
    "        \n",
    "        # Add hard negative penalty (encourage diversity)\n",
    "        hard_negative_penalty = torch.clamp(neg_sim - 0.8, min=0.0) * 0.1\n",
    "        \n",
    "        total_loss = (loss + hard_negative_penalty).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'base_loss': loss.mean().item(),\n",
    "                'hard_penalty': hard_negative_penalty.mean().item(),\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': neg_sim.detach(),\n",
    "                'adaptive_margins': adaptive_margin.detach(),\n",
    "                'margin_violations': (neg_sim - pos_sim + adaptive_margin).detach()\n",
    "            }\n",
    "            return total_loss, components\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def contrastive_loss_with_temperature(self, anchor, positive, negatives,\n",
    "                                         return_components=False):\n",
    "        \"\"\"\n",
    "        InfoNCE-style loss with learnable temperature\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        \n",
    "        # Compute positive similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
    "        \n",
    "        # Compute negative similarities\n",
    "        neg_sims = []\n",
    "        for i in range(batch_size):\n",
    "            if negatives.dim() == 3:  # [batch_size, num_negatives, dim]\n",
    "                anchor_expanded = anchor[i:i+1].expand(negatives.size(1), -1)\n",
    "                neg_sim = F.cosine_similarity(anchor_expanded, negatives[i], dim=1) / self.temperature\n",
    "            else:  # [batch_size, dim] - single negative per sample\n",
    "                neg_sim = F.cosine_similarity(anchor[i:i+1], negatives[i:i+1], dim=1) / self.temperature\n",
    "            neg_sims.append(neg_sim)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        losses = []\n",
    "        for i in range(batch_size):\n",
    "            numerator = torch.exp(pos_sim[i])\n",
    "            denominator = numerator + torch.sum(torch.exp(neg_sims[i]))\n",
    "            loss = -torch.log(numerator / (denominator + 1e-8))\n",
    "            losses.append(loss)\n",
    "        \n",
    "        total_loss = torch.stack(losses).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': [neg_sim.detach() for neg_sim in neg_sims],\n",
    "                'individual_losses': torch.stack(losses).detach(),\n",
    "                'temperature': self.temperature\n",
    "            }\n",
    "            return total_loss, components\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def curriculum_training_step(self, triplets: List[ArabicTriplet], \n",
    "                                current_stage: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform curriculum learning training step\n",
    "        \"\"\"\n",
    "        stage_config = self.curriculum_stages[current_stage]\n",
    "        \n",
    "        # Filter triplets by difficulty\n",
    "        filtered_triplets = [\n",
    "            t for t in triplets \n",
    "            if t.difficulty <= stage_config['difficulty_threshold']\n",
    "        ]\n",
    "        \n",
    "        if not filtered_triplets:\n",
    "            print(f\"⚠️ No triplets found for {current_stage} stage\")\n",
    "            return {'loss': 0.0, 'num_samples': 0}\n",
    "        \n",
    "        print(f\"\\n📚 Training Stage: {current_stage}\")\n",
    "        print(f\"   Difficulty threshold: {stage_config['difficulty_threshold']}\")\n",
    "        print(f\"   Available triplets: {len(filtered_triplets)}\")\n",
    "        \n",
    "        # Prepare batch data\n",
    "        anchors = torch.stack([t.anchor_embedding for t in filtered_triplets])\n",
    "        positives = torch.stack([t.positive_embedding for t in filtered_triplets])\n",
    "        negatives = torch.stack([t.negative_embedding for t in filtered_triplets])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, components = self.triplet_loss_with_hard_negatives(\n",
    "            anchors, positives, negatives, return_components=True\n",
    "        )\n",
    "        \n",
    "        # Record training history\n",
    "        self.training_history['losses'].append(loss.item())\n",
    "        self.training_history['difficulties'].extend([t.difficulty for t in filtered_triplets])\n",
    "        self.training_history['positive_similarities'].extend(\n",
    "            components['positive_similarities'].tolist()\n",
    "        )\n",
    "        self.training_history['negative_similarities'].extend(\n",
    "            components['negative_similarities'].tolist()\n",
    "        )\n",
    "        self.training_history['margin_violations'].extend(\n",
    "            components['margin_violations'].tolist()\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        pos_sim_mean = components['positive_similarities'].mean().item()\n",
    "        neg_sim_mean = components['negative_similarities'].mean().item()\n",
    "        margin_violations = (components['margin_violations'] > 0).float().mean().item()\n",
    "        \n",
    "        results = {\n",
    "            'loss': loss.item(),\n",
    "            'base_loss': components['base_loss'],\n",
    "            'hard_penalty': components['hard_penalty'],\n",
    "            'num_samples': len(filtered_triplets),\n",
    "            'pos_sim_mean': pos_sim_mean,\n",
    "            'neg_sim_mean': neg_sim_mean,\n",
    "            'margin_violations': margin_violations,\n",
    "            'avg_difficulty': np.mean([t.difficulty for t in filtered_triplets])\n",
    "        }\n",
    "        \n",
    "        print(f\"   Loss: {loss.item():.4f}\")\n",
    "        print(f\"   Positive similarity: {pos_sim_mean:.3f}\")\n",
    "        print(f\"   Negative similarity: {neg_sim_mean:.3f}\")\n",
    "        print(f\"   Margin violations: {margin_violations:.1%}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def full_curriculum_training(self, triplets: List[ArabicTriplet]) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Complete curriculum learning training\n",
    "        \"\"\"\n",
    "        print(\"🎓 Starting Curriculum Learning Training\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for stage in ['easy', 'medium', 'hard']:\n",
    "            stage_results = []\n",
    "            \n",
    "            for epoch in range(self.curriculum_stages[stage]['epochs']):\n",
    "                print(f\"\\n🔄 Epoch {epoch + 1}/{self.curriculum_stages[stage]['epochs']}\")\n",
    "                \n",
    "                epoch_result = self.curriculum_training_step(triplets, stage)\n",
    "                epoch_result['stage'] = stage\n",
    "                epoch_result['epoch'] = epoch + 1\n",
    "                \n",
    "                stage_results.append(epoch_result)\n",
    "            \n",
    "            all_results[stage] = stage_results\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def analyze_training_dynamics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze training dynamics and convergence\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 Training Dynamics Analysis\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        if not self.training_history['losses']:\n",
    "            print(\"⚠️ No training history available\")\n",
    "            return {}\n",
    "        \n",
    "        losses = self.training_history['losses']\n",
    "        pos_sims = self.training_history['positive_similarities']\n",
    "        neg_sims = self.training_history['negative_similarities']\n",
    "        violations = self.training_history['margin_violations']\n",
    "        \n",
    "        analysis = {\n",
    "            'loss_trend': 'decreasing' if losses[-1] < losses[0] else 'increasing',\n",
    "            'loss_reduction': (losses[0] - losses[-1]) / losses[0] * 100,\n",
    "            'avg_loss': np.mean(losses),\n",
    "            'loss_std': np.std(losses),\n",
    "            'final_loss': losses[-1],\n",
    "            'avg_pos_similarity': np.mean(pos_sims),\n",
    "            'avg_neg_similarity': np.mean(neg_sims),\n",
    "            'similarity_gap': np.mean(pos_sims) - np.mean(neg_sims),\n",
    "            'violation_rate': np.mean([v > 0 for v in violations]),\n",
    "            'training_stability': 1.0 / (1.0 + np.std(losses[-5:]) if len(losses) >= 5 else np.std(losses))\n",
    "        }\n",
    "        \n",
    "        print(f\"📈 Loss Trend: {analysis['loss_trend']}\")\n",
    "        print(f\"📉 Loss Reduction: {analysis['loss_reduction']:.1f}%\")\n",
    "        print(f\"🎯 Final Loss: {analysis['final_loss']:.4f}\")\n",
    "        print(f\"✅ Positive Similarity: {analysis['avg_pos_similarity']:.3f}\")\n",
    "        print(f\"❌ Negative Similarity: {analysis['avg_neg_similarity']:.3f}\")\n",
    "        print(f\"📏 Similarity Gap: {analysis['similarity_gap']:.3f}\")\n",
    "        print(f\"⚠️ Violation Rate: {analysis['violation_rate']:.1%}\")\n",
    "        print(f\"🔒 Training Stability: {analysis['training_stability']:.3f}\")\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Test triplet training with different strategies\n",
    "def test_triplet_training_strategies():\n",
    "    \"\"\"Test different triplet training strategies\"\"\"\n",
    "    print(\"\\n🧪 Testing Triplet Training Strategies\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    strategies = ['hard', 'semi_hard', 'random']\n",
    "    results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n🎯 Testing {strategy} strategy:\")\n",
    "        \n",
    "        # Create triplets with this strategy\n",
    "        triplets = hard_negative_miner.create_arabic_triplets(\n",
    "            arabic_corpus, strategy=strategy, num_triplets=10\n",
    "        )\n",
    "        \n",
    "        if triplets:\n",
    "            # Initialize trainer\n",
    "            trainer = ArabicTripletTrainer(\n",
    "                mining_strategy=strategy,\n",
    "                curriculum_learning=True\n",
    "            )\n",
    "            \n",
    "            # Run curriculum training\n",
    "            training_results = trainer.full_curriculum_training(triplets)\n",
    "            \n",
    "            # Analyze training dynamics\n",
    "            dynamics = trainer.analyze_training_dynamics()\n",
    "            \n",
    "            results[strategy] = {\n",
    "                'training_results': training_results,\n",
    "                'dynamics': dynamics,\n",
    "                'num_triplets': len(triplets)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive testing\n",
    "training_strategy_results = test_triplet_training_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Advanced Visualization and Analysis\n",
    "\n",
    "### Understanding Contrastive Learning Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_contrastive_learning_analysis():\n",
    "    \"\"\"Create comprehensive visualizations of contrastive learning\"\"\"\n",
    "    \n",
    "    # Create comprehensive subplot layout\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Mining Strategy Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    if mining_comparison:\n",
    "        strategies = list(mining_comparison.keys())\n",
    "        avg_difficulties = []\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            triplets = mining_comparison[strategy]\n",
    "            if triplets:\n",
    "                avg_difficulties.append(np.mean([t.difficulty for t in triplets]))\n",
    "            else:\n",
    "                avg_difficulties.append(0)\n",
    "        \n",
    "        bars = ax1.bar(strategies, avg_difficulties, alpha=0.8, \n",
    "                      color=['skyblue', 'orange', 'green', 'red'])\n",
    "        ax1.set_title('Average Difficulty by Mining Strategy', fontweight='bold')\n",
    "        ax1.set_ylabel('Average Difficulty Score')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, avg_difficulties):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: Loss Function Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    if loss_analysis:\n",
    "        loss_types = ['InfoNCE', 'Triplet', 'MNR']\n",
    "        loss_values = [\n",
    "            loss_analysis['infonce'][0].item(),\n",
    "            loss_analysis['triplet'][0].item(),\n",
    "            loss_analysis['mnr'][0].item()\n",
    "        ]\n",
    "        \n",
    "        ax2.bar(loss_types, loss_values, alpha=0.8, color=['blue', 'green', 'purple'])\n",
    "        ax2.set_title('Loss Function Comparison', fontweight='bold')\n",
    "        ax2.set_ylabel('Loss Value')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Training Strategy Performance\n",
    "    ax3 = axes[0, 2]\n",
    "    if training_strategy_results:\n",
    "        strategies = list(training_strategy_results.keys())\n",
    "        final_losses = []\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            dynamics = training_strategy_results[strategy]['dynamics']\n",
    "            if 'final_loss' in dynamics:\n",
    "                final_losses.append(dynamics['final_loss'])\n",
    "            else:\n",
    "                final_losses.append(0)\n",
    "        \n",
    "        ax3.bar(strategies, final_losses, alpha=0.8, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "        ax3.set_title('Final Training Loss by Strategy', fontweight='bold')\n",
    "        ax3.set_ylabel('Final Loss')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Similarity Gap Analysis\n",
    "    ax4 = axes[1, 0]\n",
    "    if training_strategy_results:\n",
    "        strategies = list(training_strategy_results.keys())\n",
    "        similarity_gaps = []\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            dynamics = training_strategy_results[strategy]['dynamics']\n",
    "            if 'similarity_gap' in dynamics:\n",
    "                similarity_gaps.append(dynamics['similarity_gap'])\n",
    "            else:\n",
    "                similarity_gaps.append(0)\n",
    "        \n",
    "        ax4.bar(strategies, similarity_gaps, alpha=0.8, color=['gold', 'silver', 'bronze'])\n",
    "        ax4.set_title('Positive-Negative Similarity Gap', fontweight='bold')\n",
    "        ax4.set_ylabel('Similarity Gap')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Temperature Effects Visualization\n",
    "    ax5 = axes[1, 1]\n",
    "    if temperature_analysis:\n",
    "        temperatures, similarities = temperature_analysis\n",
    "        \n",
    "        # Show softmax distribution for different temperatures\n",
    "        for i, temp in enumerate(temperatures[:3]):  # Show first 3 temperatures\n",
    "            scaled = similarities / temp\n",
    "            softmax_probs = F.softmax(scaled, dim=0)\n",
    "            \n",
    "            ax5.plot(range(len(softmax_probs)), softmax_probs.numpy(), \n",
    "                    'o-', label=f'τ={temp}', alpha=0.8)\n",
    "        \n",
    "        ax5.set_title('Temperature Effects on Softmax', fontweight='bold')\n",
    "        ax5.set_xlabel('Similarity Rank')\n",
    "        ax5.set_ylabel('Softmax Probability')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Training Dynamics\n",
    "    ax6 = axes[1, 2]\n",
    "    if training_strategy_results:\n",
    "        for strategy, results in training_strategy_results.items():\n",
    "            if 'dynamics' in results and 'training_stability' in results['dynamics']:\n",
    "                stability = results['dynamics']['training_stability']\n",
    "                violation_rate = results['dynamics']['violation_rate']\n",
    "                \n",
    "                ax6.scatter(violation_rate, stability, s=100, alpha=0.7, label=strategy)\n",
    "        \n",
    "        ax6.set_title('Training Stability vs Violation Rate', fontweight='bold')\n",
    "        ax6.set_xlabel('Margin Violation Rate')\n",
    "        ax6.set_ylabel('Training Stability')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_contrastive_learning_insights():\n",
    "    \"\"\"Generate insights from contrastive learning analysis\"\"\"\n",
    "    print(\"\\n💡 Contrastive Learning Insights\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Mining strategy insights\n",
    "    if mining_comparison:\n",
    "        print(\"\\n🎯 Mining Strategy Analysis:\")\n",
    "        \n",
    "        for strategy, triplets in mining_comparison.items():\n",
    "            if triplets:\n",
    "                avg_difficulty = np.mean([t.difficulty for t in triplets])\n",
    "                difficulty_std = np.std([t.difficulty for t in triplets])\n",
    "                \n",
    "                print(f\"   {strategy:>12}: avg_difficulty={avg_difficulty:.3f}, std={difficulty_std:.3f}\")\n",
    "                \n",
    "                # Strategy characteristics\n",
    "                if strategy == 'hard':\n",
    "                    print(f\"                 → Best for challenging models, may cause training instability\")\n",
    "                elif strategy == 'semi_hard':\n",
    "                    print(f\"                 → Balanced approach, good convergence properties\")\n",
    "                elif strategy == 'random':\n",
    "                    print(f\"                 → Baseline approach, stable but slower learning\")\n",
    "                elif strategy == 'semantic':\n",
    "                    print(f\"                 → Category-aware, good for structured datasets\")\n",
    "    \n",
    "    # Training strategy insights\n",
    "    if training_strategy_results:\n",
    "        print(\"\\n🚂 Training Strategy Performance:\")\n",
    "        \n",
    "        best_strategy = None\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        for strategy, results in training_strategy_results.items():\n",
    "            dynamics = results['dynamics']\n",
    "            if 'similarity_gap' in dynamics:\n",
    "                score = dynamics['similarity_gap'] * dynamics['training_stability']\n",
    "                \n",
    "                print(f\"   {strategy:>12}: gap={dynamics['similarity_gap']:.3f}, stability={dynamics['training_stability']:.3f}, score={score:.3f}\")\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_strategy = strategy\n",
    "        \n",
    "        if best_strategy:\n",
    "            print(f\"\\n🏆 Best Strategy: {best_strategy} (score: {best_score:.3f})\")\n",
    "    \n",
    "    # Loss function insights\n",
    "    if loss_analysis:\n",
    "        print(\"\\n🧮 Loss Function Characteristics:\")\n",
    "        \n",
    "        infonce_loss = loss_analysis['infonce'][0].item()\n",
    "        triplet_loss = loss_analysis['triplet'][0].item()\n",
    "        mnr_loss = loss_analysis['mnr'][0].item()\n",
    "        \n",
    "        print(f\"   InfoNCE: {infonce_loss:.4f} → Good for large-scale contrastive learning\")\n",
    "        print(f\"   Triplet: {triplet_loss:.4f} → Direct optimization of similarity relationships\")\n",
    "        print(f\"   MNR: {mnr_loss:.4f} → Balanced approach for ranking tasks\")\n",
    "        \n",
    "        # Recommend best loss\n",
    "        losses = {'InfoNCE': infonce_loss, 'Triplet': triplet_loss, 'MNR': mnr_loss}\n",
    "        best_loss = min(losses, key=losses.get)\n",
    "        print(f\"\\n💡 Recommended loss: {best_loss} (lowest value: {losses[best_loss]:.4f})\")\n",
    "\n",
    "def practical_implementation_recommendations():\n",
    "    \"\"\"Provide practical recommendations for contrastive learning\"\"\"\n",
    "    print(\"\\n🛠️ Practical Implementation Recommendations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    recommendations = {\n",
    "        \"🎯 Negative Mining Strategy\": [\n",
    "            \"Start with semi-hard negatives for stable training\",\n",
    "            \"Use hard negatives only after initial convergence\",\n",
    "            \"Implement dynamic difficulty adjustment\",\n",
    "            \"Mix strategies during training for robustness\"\n",
    "        ],\n",
    "        \"🌡️ Temperature Optimization\": [\n",
    "            \"Start with τ=0.07 and adjust based on loss behavior\",\n",
    "            \"Lower temperature (0.01-0.05) for sharper distributions\",\n",
    "            \"Higher temperature (0.1-0.5) for smoother learning\",\n",
    "            \"Use learnable temperature when possible\"\n",
    "        ],\n",
    "        \"📚 Curriculum Learning\": [\n",
    "            \"Begin with easy negatives (low similarity to anchor)\",\n",
    "            \"Gradually increase difficulty as model learns\",\n",
    "            \"Monitor margin violation rates as difficulty indicator\",\n",
    "            \"Adjust stage thresholds based on convergence\"\n",
    "        ],\n",
    "        \"🔧 Training Optimization\": [\n",
    "            \"Use batch sizes of 64-256 for good negative sampling\",\n",
    "            \"Implement gradient clipping for training stability\",\n",
    "            \"Monitor positive/negative similarity gap\",\n",
    "            \"Apply data augmentation for Arabic text variation\"\n",
    "        ],\n",
    "        \"📊 Evaluation Metrics\": [\n",
    "            \"Track similarity gap as primary metric\",\n",
    "            \"Monitor margin violation rates for difficulty\",\n",
    "            \"Use retrieval metrics (MRR, nDCG) for validation\",\n",
    "            \"Test on diverse Arabic dialects for robustness\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, tips in recommendations.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"   • {tip}\")\n",
    "\n",
    "# Run comprehensive analysis\n",
    "visualize_contrastive_learning_analysis()\n",
    "generate_contrastive_learning_insights()\n",
    "practical_implementation_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Insights and Learning Takeaways\n",
    "\n",
    "### Mastering Contrastive Learning for Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_contrastive_learning_mastery():\n",
    "    \"\"\"Comprehensive summary of contrastive learning mastery\"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        \"🧮 Mathematical Mastery\": [\n",
    "            \"InfoNCE optimizes ranking through temperature-scaled softmax\",\n",
    "            \"Triplet loss directly optimizes similarity relationships with margin\",\n",
    "            \"Temperature scaling controls distribution sharpness and learning speed\",\n",
    "            \"Multiple negatives ranking balances classification and similarity objectives\",\n",
    "            \"Adaptive margins improve training stability and convergence\"\n",
    "        ],\n",
    "        \"🎯 Mining Strategy Excellence\": [\n",
    "            \"Hard negatives accelerate learning but may cause instability\",\n",
    "            \"Semi-hard negatives provide optimal balance of challenge and stability\",\n",
    "            \"Cluster-based mining ensures negative diversity across semantic space\",\n",
    "            \"Semantic category mining leverages domain structure effectively\",\n",
    "            \"Dynamic difficulty adjustment enables curriculum learning\"\n",
    "        ],\n",
    "        \"📚 Curriculum Learning Innovation\": [\n",
    "            \"Progressive difficulty increases model robustness gradually\",\n",
    "            \"Stage-based training prevents mode collapse in early phases\",\n",
    "            \"Difficulty thresholds should align with model capacity\",\n",
    "            \"Margin violation rates indicate optimal difficulty progression\",\n",
    "            \"Multi-stage approach improves final performance significantly\"\n",
    "        ],\n",
    "        \"🚀 Training Optimization\": [\n",
    "            \"Batch size affects negative sampling quality and diversity\",\n",
    "            \"Gradient clipping essential for hard negative training stability\",\n",
    "            \"Learning rate scheduling improves convergence in later stages\",\n",
    "            \"Mixed precision training enables larger batch sizes efficiently\",\n",
    "            \"Regular evaluation prevents overfitting to training negatives\"\n",
    "        ],\n",
    "        \"🔬 Arabic-Specific Adaptations\": [\n",
    "            \"Morphological variations require careful negative selection\",\n",
    "            \"Dialectal differences enhance negative diversity naturally\",\n",
    "            \"Root-pattern relationships inform semantic similarity mining\",\n",
    "            \"Cross-dialectal triplets improve generalization\",\n",
    "            \"Arabic-specific augmentations increase training robustness\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"🎓 Contrastive Learning Mastery\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, points in insights.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for point in points:\n",
    "            print(f\"   • {point}\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def connection_to_gate_innovation():\n",
    "    \"\"\"Connect insights to GATE's contrastive learning innovations\"\"\"\n",
    "    print(\"\\n🔗 Connection to GATE's Innovation\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    print(\"📋 How GATE Advances Contrastive Learning:\")\n",
    "    \n",
    "    gate_innovations = {\n",
    "        \"Beyond InfoNCE Limitations\": [\n",
    "            \"Identifies InfoNCE insufficiency for sentence-level tasks\",\n",
    "            \"Implements task-specific loss functions for better performance\",\n",
    "            \"Uses Arabic NLI triplets for fine-grained similarity learning\"\n",
    "        ],\n",
    "        \"Arabic-Tailored Negative Mining\": [\n",
    "            \"Curates hard negatives from Arabic NLI datasets\",\n",
    "            \"Leverages linguistic structure for better negative selection\",\n",
    "            \"Addresses Arabic-specific semantic challenges\"\n",
    "        ],\n",
    "        \"Hybrid Training Integration\": [\n",
    "            \"Combines contrastive learning with classification objectives\",\n",
    "            \"Balances similarity learning with semantic understanding\",\n",
    "            \"Integrates with Matryoshka representation learning\"\n",
    "        ],\n",
    "        \"Performance Achievements\": [\n",
    "            \"20-25% improvement over larger models (OpenAI)\",\n",
    "            \"State-of-the-art Arabic STS performance on MTEB\",\n",
    "            \"Robust performance across multiple embedding dimensions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for innovation, details in gate_innovations.items():\n",
    "        print(f\"\\n🎯 {innovation}:\")\n",
    "        for detail in details:\n",
    "            print(f\"   ✓ {detail}\")\n",
    "    \n",
    "    print(f\"\\n🏆 Result: GATE demonstrates that thoughtful contrastive learning\")\n",
    "    print(f\"   design can outperform larger, more resource-intensive models\")\n",
    "\n",
    "def advanced_research_directions():\n",
    "    \"\"\"Outline advanced research directions in contrastive learning\"\"\"\n",
    "    print(\"\\n🔬 Advanced Research Directions\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    directions = {\n",
    "        \"🧬 Meta-Learning for Mining\": [\n",
    "            \"Learn optimal negative sampling strategies automatically\",\n",
    "            \"Adapt mining difficulty based on model performance\",\n",
    "            \"Personalize negative selection for different domains\"\n",
    "        ],\n",
    "        \"🌍 Cross-Lingual Contrastive Learning\": [\n",
    "            \"Multi-lingual triplet construction and evaluation\",\n",
    "            \"Zero-shot transfer through cross-lingual negatives\",\n",
    "            \"Language-agnostic similarity learning frameworks\"\n",
    "        ],\n",
    "        \"🎭 Multi-Modal Integration\": [\n",
    "            \"Text-image contrastive learning for Arabic content\",\n",
    "            \"Audio-text alignment for Arabic speech processing\",\n",
    "            \"Multi-modal negative mining strategies\"\n",
    "        ],\n",
    "        \"⚡ Efficiency Optimizations\": [\n",
    "            \"Approximate negative sampling for large-scale training\",\n",
    "            \"Hierarchical negative caching mechanisms\",\n",
    "            \"Dynamic batch construction for optimal mining\"\n",
    "        ],\n",
    "        \"🎯 Task-Specific Adaptations\": [\n",
    "            \"Question-answering focused contrastive objectives\",\n",
    "            \"Document retrieval optimized negative mining\",\n",
    "            \"Conversational AI contrastive learning strategies\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for direction, aspects in directions.items():\n",
    "        print(f\"\\n{direction}:\")\n",
    "        for aspect in aspects:\n",
    "            print(f\"   • {aspect}\")\n",
    "\n",
    "# Generate comprehensive insights\n",
    "contrastive_insights = summarize_contrastive_learning_mastery()\n",
    "connection_to_gate_innovation()\n",
    "advanced_research_directions()\n",
    "\n",
    "print(\"\\n🎓 Learning Completion Summary\")\n",
    "print(\"=\" * 35)\n",
    "print(\"✅ Contrastive learning mathematics thoroughly mastered\")\n",
    "print(\"✅ Advanced negative mining strategies implemented\")\n",
    "print(\"✅ Curriculum learning framework developed\")\n",
    "print(\"✅ Arabic-specific adaptations understood\")\n",
    "print(\"✅ Training optimization techniques learned\")\n",
    "print(\"✅ Connection to GATE's innovations established\")\n",
    "\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"   • Apply contrastive learning to your domain\")\n",
    "print(\"   • Experiment with novel negative mining strategies\")\n",
    "print(\"   • Integrate with multi-modal learning systems\")\n",
    "print(\"   • Contribute to Arabic NLP research advancement\")\n",
    "\n",
    "print(\"\\n🎉 Congratulations! You have completed all GATE focused learning notebooks:\")\n",
    "print(\"   1. ✅ Matryoshka Representation Learning\")\n",
    "print(\"   2. ✅ Hybrid Loss Architecture\")\n",
    "print(\"   3. ✅ Arabic NLP Challenges\")\n",
    "print(\"   4. ✅ Contrastive Triplet Learning\")\n",
    "print(\"\\n🌟 You are now equipped to implement and extend GATE's innovations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}