{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Semantic Data Cleaning Pipeline\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook provides a deep dive into the semantic data cleaning pipeline that transforms noisy code review datasets into high-quality training data. We'll explore how removing noisy comments paradoxically improves model performance despite reducing dataset size.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to build an end-to-end data cleaning pipeline using LLMs\n",
    "2. The impact of data quality vs. data quantity on model performance\n",
    "3. How to create controlled experiments for fair comparison\n",
    "4. Practical strategies for dataset curation at scale\n",
    "\n",
    "**Paper Reference**: Section V - Impact on Comment Generation Accuracy (RQ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Foundation\n",
    "\n",
    "### 1.1 The Data Quality Paradox\n",
    "\n",
    "The paper reveals a counterintuitive finding: **smaller, cleaner datasets outperform larger, noisy ones**.\n",
    "\n",
    "Key statistics from the paper:\n",
    "- **Original dataset**: 117,739 training samples (64% valid)\n",
    "- **Cleaned with GPT-3.5**: 39,625 samples (85% valid) - 66% reduction\n",
    "- **Cleaned with Llama3**: 87,872 samples (75% valid) - 25% reduction\n",
    "- **Result**: 7.5-13% improvement in BLEU-4 scores\n",
    "\n",
    "### 1.2 Why Quality Matters More Than Quantity\n",
    "\n",
    "1. **Noise propagation**: Models learn and amplify patterns from noisy data\n",
    "2. **Signal clarity**: Clean data provides clearer learning signals\n",
    "3. **Efficiency**: Less data means faster training with better results\n",
    "\n",
    "### 1.3 The Cleaning Process\n",
    "\n",
    "The semantic cleaning pipeline:\n",
    "1. **Classification**: Use LLMs to classify each comment as valid/noisy\n",
    "2. **Filtering**: Retain only comments predicted as valid\n",
    "3. **Validation**: Compare against controlled datasets\n",
    "4. **Evaluation**: Measure impact on downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain for LLM integration\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# For parallel processing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import asyncio\n",
    "\n",
    "# Set style and seed\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures for Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReviewComment:\n",
    "    \"\"\"Enhanced data structure for code review comments\"\"\"\n",
    "    id: str\n",
    "    comment_text: str\n",
    "    code_diff: str\n",
    "    project: str\n",
    "    timestamp: datetime\n",
    "    \n",
    "    # Original labels (if available)\n",
    "    original_label: Optional[str] = None\n",
    "    \n",
    "    # Cleaning metadata\n",
    "    predicted_label: Optional[str] = None\n",
    "    confidence: Optional[float] = None\n",
    "    cleaning_model: Optional[str] = None\n",
    "    cleaning_timestamp: Optional[datetime] = None\n",
    "    explanation: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class CleaningStats:\n",
    "    \"\"\"Statistics for the cleaning process\"\"\"\n",
    "    original_size: int\n",
    "    cleaned_size: int\n",
    "    removed_size: int\n",
    "    \n",
    "    original_valid_ratio: float\n",
    "    cleaned_valid_ratio: float\n",
    "    \n",
    "    reduction_percentage: float\n",
    "    improvement_percentage: float\n",
    "    \n",
    "    processing_time: float\n",
    "    cost_estimate: float\n",
    "    \n",
    "    # Distribution of confidence scores\n",
    "    confidence_distribution: Dict[str, List[float]] = field(default_factory=dict)\n",
    "\n",
    "@dataclass \n",
    "class DatasetPartition:\n",
    "    \"\"\"Represents a dataset partition (train/val/test)\"\"\"\n",
    "    name: str\n",
    "    comments: List[ReviewComment]\n",
    "    stats: Optional[CleaningStats] = None\n",
    "\n",
    "print(\"Data structures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Mock CodeReviewer Dataset\n",
    "\n",
    "Based on the statistics from Table II in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_dataset(size: int = 1000, valid_ratio: float = 0.64) -> List[ReviewComment]:\n",
    "    \"\"\"Generate mock dataset matching paper's statistics\"\"\"\n",
    "    \n",
    "    # Valid comment templates\n",
    "    valid_templates = [\n",
    "        \"Consider extracting this logic into a separate method for better reusability\",\n",
    "        \"This variable name should follow camelCase convention: {}\",\n",
    "        \"Add error handling for the case when {} is null\",\n",
    "        \"This method is too long. Consider breaking it down into smaller functions\",\n",
    "        \"Use a constant instead of this magic number {}\",\n",
    "        \"Add unit tests to cover this edge case\",\n",
    "        \"This could be simplified using a ternary operator\",\n",
    "        \"Consider using dependency injection here for better testability\",\n",
    "        \"Add documentation explaining the purpose of this algorithm\",\n",
    "        \"This recursive approach might cause stack overflow for large inputs\"\n",
    "    ]\n",
    "    \n",
    "    # Noisy comment templates  \n",
    "    noisy_templates = [\n",
    "        \"Why this change?\",\n",
    "        \"What does this do?\",\n",
    "        \"I don't understand this\",\n",
    "        \"Is this necessary?\",\n",
    "        \"???\",\n",
    "        \"Not sure about this\",\n",
    "        \"hmm\",\n",
    "        \"This looks weird\",\n",
    "        \"Why not use the old approach?\",\n",
    "        \"What's the purpose of this line?\"\n",
    "    ]\n",
    "    \n",
    "    # Code diff templates\n",
    "    code_diffs = [\n",
    "        \"+ this.config = loadConfig();\",\n",
    "        \"- return data.process()\\n+ return data.validate().process()\",\n",
    "        \"+ if (user == null) throw new Error('User required');\",\n",
    "        \"- for (int i = 0; i < 100; i++)\\n+ for (int i = 0; i < MAX_ITERATIONS; i++)\",\n",
    "        \"+ logger.debug('Processing item: ' + item.id);\"\n",
    "    ]\n",
    "    \n",
    "    # Projects\n",
    "    projects = [\"apache/commons\", \"spring/framework\", \"tensorflow/models\", \n",
    "               \"facebook/react\", \"microsoft/vscode\"]\n",
    "    \n",
    "    dataset = []\n",
    "    n_valid = int(size * valid_ratio)\n",
    "    \n",
    "    # Generate valid comments\n",
    "    for i in range(n_valid):\n",
    "        template = random.choice(valid_templates)\n",
    "        comment_text = template.format(\n",
    "            random.choice([\"userId\", \"configValue\", \"MAX_SIZE\", \"responseData\"])\n",
    "        ) if \"{}\" in template else template\n",
    "        \n",
    "        dataset.append(ReviewComment(\n",
    "            id=f\"comment_{i}\",\n",
    "            comment_text=comment_text,\n",
    "            code_diff=random.choice(code_diffs),\n",
    "            project=random.choice(projects),\n",
    "            timestamp=datetime.now(),\n",
    "            original_label=\"valid\"\n",
    "        ))\n",
    "    \n",
    "    # Generate noisy comments\n",
    "    for i in range(n_valid, size):\n",
    "        dataset.append(ReviewComment(\n",
    "            id=f\"comment_{i}\",\n",
    "            comment_text=random.choice(noisy_templates),\n",
    "            code_diff=random.choice(code_diffs),\n",
    "            project=random.choice(projects),\n",
    "            timestamp=datetime.now(),\n",
    "            original_label=\"noisy\"\n",
    "        ))\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "# Generate datasets matching paper sizes (scaled down for demo)\n",
    "train_data = generate_mock_dataset(1000, 0.64)  # Scaled from 117,739\n",
    "val_data = generate_mock_dataset(100, 0.64)     # Scaled from 10,319\n",
    "test_data = generate_mock_dataset(100, 0.64)    # Scaled from 10,169\n",
    "\n",
    "print(f\"Generated mock datasets:\")\n",
    "print(f\"  Training: {len(train_data)} samples\")\n",
    "print(f\"  Validation: {len(val_data)} samples\")\n",
    "print(f\"  Test: {len(test_data)} samples\")\n",
    "print(f\"\\nValid ratio: {sum(1 for c in train_data if c.original_label == 'valid') / len(train_data):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing the Semantic Data Cleaner\n",
    "\n",
    "This is the core component that implements the cleaning approach from Section V-A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticDataCleaner:\n",
    "    \"\"\"Production-grade semantic data cleaner using LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\", batch_size: int = 10):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0.1)\n",
    "        \n",
    "        # Best performing prompt from RQ1\n",
    "        self.prompt = self._create_classification_prompt()\n",
    "        self.parser = JsonOutputParser(pydantic_object=self._get_output_schema())\n",
    "        \n",
    "    def _get_output_schema(self):\n",
    "        class Classification(BaseModel):\n",
    "            label: str = Field(description=\"'valid' or 'noisy'\")\n",
    "            confidence: float = Field(description=\"0.0 to 1.0\")\n",
    "            explanation: str = Field(description=\"Brief reasoning\")\n",
    "        return Classification\n",
    "    \n",
    "    def _create_classification_prompt(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Create the best performing prompt (P_DEFINITION with RNL)\"\"\"\n",
    "        \n",
    "        template = \"\"\"You are an experienced code reviewer. Classify this review comment as 'valid' or 'noisy'.\n",
    "\n",
    "Valid comments: Provide clear suggestions for code improvement with specific actions.\n",
    "Noisy comments: Vague, unclear, or don't request specific changes.\n",
    "\n",
    "Comment: {comment}\n",
    "\n",
    "Return JSON with label, confidence, and brief explanation.\"\"\"\n",
    "        \n",
    "        return ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    def classify_comment(self, comment: ReviewComment) -> Dict:\n",
    "        \"\"\"Classify a single comment\"\"\"\n",
    "        chain = self.prompt | self.llm | self.parser\n",
    "        \n",
    "        try:\n",
    "            result = chain.invoke({\"comment\": comment.comment_text})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            # Default to noisy on error\n",
    "            return {\n",
    "                \"label\": \"noisy\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"explanation\": f\"Classification error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def clean_dataset(self, \n",
    "                     dataset: List[ReviewComment],\n",
    "                     progress_callback=None) -> Tuple[DatasetPartition, DatasetPartition, CleaningStats]:\n",
    "        \"\"\"\n",
    "        Clean a dataset by removing noisy comments.\n",
    "        \n",
    "        Returns:\n",
    "            - Cleaned dataset partition\n",
    "            - Removed dataset partition  \n",
    "            - Cleaning statistics\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        cleaned_comments = []\n",
    "        removed_comments = []\n",
    "        confidence_scores = {\"valid\": [], \"noisy\": []}\n",
    "        \n",
    "        # Process in batches for efficiency\n",
    "        total_batches = len(dataset) // self.batch_size + (1 if len(dataset) % self.batch_size else 0)\n",
    "        \n",
    "        for batch_idx in tqdm(range(total_batches), desc=\"Cleaning batches\"):\n",
    "            batch_start = batch_idx * self.batch_size\n",
    "            batch_end = min((batch_idx + 1) * self.batch_size, len(dataset))\n",
    "            batch = dataset[batch_start:batch_end]\n",
    "            \n",
    "            # Process batch\n",
    "            for comment in batch:\n",
    "                classification = self.classify_comment(comment)\n",
    "                \n",
    "                # Update comment metadata\n",
    "                comment.predicted_label = classification[\"label\"]\n",
    "                comment.confidence = classification[\"confidence\"]\n",
    "                comment.explanation = classification[\"explanation\"]\n",
    "                comment.cleaning_model = self.model_name\n",
    "                comment.cleaning_timestamp = datetime.now()\n",
    "                \n",
    "                # Sort into cleaned or removed\n",
    "                if classification[\"label\"] == \"valid\":\n",
    "                    cleaned_comments.append(comment)\n",
    "                    confidence_scores[\"valid\"].append(classification[\"confidence\"])\n",
    "                else:\n",
    "                    removed_comments.append(comment)\n",
    "                    confidence_scores[\"noisy\"].append(classification[\"confidence\"])\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(batch_idx + 1, total_batches)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Calculate valid ratios\n",
    "        original_valid = sum(1 for c in dataset if c.original_label == \"valid\") / len(dataset)\n",
    "        cleaned_valid = sum(1 for c in cleaned_comments if c.original_label == \"valid\") / len(cleaned_comments) if cleaned_comments else 0\n",
    "        \n",
    "        stats = CleaningStats(\n",
    "            original_size=len(dataset),\n",
    "            cleaned_size=len(cleaned_comments),\n",
    "            removed_size=len(removed_comments),\n",
    "            original_valid_ratio=original_valid,\n",
    "            cleaned_valid_ratio=cleaned_valid,\n",
    "            reduction_percentage=(len(removed_comments) / len(dataset)) * 100,\n",
    "            improvement_percentage=((cleaned_valid - original_valid) / original_valid) * 100 if original_valid > 0 else 0,\n",
    "            processing_time=processing_time,\n",
    "            cost_estimate=self._estimate_cost(len(dataset)),\n",
    "            confidence_distribution=confidence_scores\n",
    "        )\n",
    "        \n",
    "        # Create partitions\n",
    "        cleaned_partition = DatasetPartition(\n",
    "            name=f\"cleaned_{self.model_name}\",\n",
    "            comments=cleaned_comments,\n",
    "            stats=stats\n",
    "        )\n",
    "        \n",
    "        removed_partition = DatasetPartition(\n",
    "            name=f\"removed_{self.model_name}\",\n",
    "            comments=removed_comments\n",
    "        )\n",
    "        \n",
    "        return cleaned_partition, removed_partition, stats\n",
    "    \n",
    "    def _estimate_cost(self, n_comments: int) -> float:\n",
    "        \"\"\"Estimate cleaning cost based on paper's figures\"\"\"\n",
    "        # Paper: $50 for 128,058 comments with GPT-3.5\n",
    "        cost_per_comment = 50 / 128058\n",
    "        return n_comments * cost_per_comment\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = SemanticDataCleaner(model_name=\"gpt-3.5-turbo\")\n",
    "print(\"Semantic data cleaner initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the Cleaning Process\n",
    "\n",
    "Let's clean our mock dataset and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the training dataset\n",
    "print(\"Cleaning training dataset...\")\n",
    "cleaned_train, removed_train, train_stats = cleaner.clean_dataset(train_data[:100])  # Use subset for demo\n",
    "\n",
    "print(\"\\n=== Cleaning Statistics ===\")\n",
    "print(f\"Original size: {train_stats.original_size}\")\n",
    "print(f\"Cleaned size: {train_stats.cleaned_size} ({train_stats.cleaned_size/train_stats.original_size:.1%})\")\n",
    "print(f\"Removed: {train_stats.removed_size} ({train_stats.reduction_percentage:.1f}%)\")\n",
    "print(f\"\\nValid ratio:\")\n",
    "print(f\"  Original: {train_stats.original_valid_ratio:.1%}\")\n",
    "print(f\"  Cleaned: {train_stats.cleaned_valid_ratio:.1%}\")\n",
    "print(f\"  Improvement: {train_stats.improvement_percentage:+.1f}%\")\n",
    "print(f\"\\nProcessing time: {train_stats.processing_time:.1f}s\")\n",
    "print(f\"Estimated cost: ${train_stats.cost_estimate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Cleaning Results\n",
    "\n",
    "Let's create comprehensive visualizations to understand the cleaning impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cleaning_results(stats: CleaningStats, cleaned: DatasetPartition, removed: DatasetPartition):\n",
    "    \"\"\"Create visualizations for cleaning results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Dataset size comparison\n",
    "    sizes = [stats.original_size, stats.cleaned_size, stats.removed_size]\n",
    "    labels = ['Original', 'Cleaned', 'Removed']\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "    \n",
    "    axes[0, 0].bar(labels, sizes, color=colors)\n",
    "    axes[0, 0].set_ylabel('Number of Comments')\n",
    "    axes[0, 0].set_title('Dataset Size Comparison')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (label, size) in enumerate(zip(labels[1:], sizes[1:])):\n",
    "        pct = size / stats.original_size * 100\n",
    "        axes[0, 0].text(i+1, size + stats.original_size*0.01, f'{pct:.1f}%', ha='center')\n",
    "    \n",
    "    # 2. Valid ratio improvement\n",
    "    ratios = [stats.original_valid_ratio, stats.cleaned_valid_ratio]\n",
    "    labels = ['Original', 'Cleaned']\n",
    "    \n",
    "    bars = axes[0, 1].bar(labels, ratios, color=['#95a5a6', '#27ae60'])\n",
    "    axes[0, 1].set_ylabel('Valid Comment Ratio')\n",
    "    axes[0, 1].set_title('Data Quality Improvement')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, ratio in zip(bars, ratios):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{ratio:.1%}', ha='center')\n",
    "    \n",
    "    # 3. Confidence distribution\n",
    "    if stats.confidence_distribution['valid']:\n",
    "        axes[0, 2].hist(stats.confidence_distribution['valid'], bins=20, alpha=0.7, \n",
    "                       label='Valid', color='#2ecc71')\n",
    "    if stats.confidence_distribution['noisy']:\n",
    "        axes[0, 2].hist(stats.confidence_distribution['noisy'], bins=20, alpha=0.7,\n",
    "                       label='Noisy', color='#e74c3c')\n",
    "    axes[0, 2].set_xlabel('Confidence Score')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "    axes[0, 2].set_title('Classification Confidence Distribution')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # 4. Classification accuracy (if original labels available)\n",
    "    tp = sum(1 for c in cleaned.comments if c.original_label == 'valid')\n",
    "    fp = sum(1 for c in cleaned.comments if c.original_label == 'noisy')\n",
    "    tn = sum(1 for c in removed.comments if c.original_label == 'noisy')\n",
    "    fn = sum(1 for c in removed.comments if c.original_label == 'valid')\n",
    "    \n",
    "    cm = [[tp, fn], [fp, tn]]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred Valid', 'Pred Noisy'],\n",
    "                yticklabels=['True Valid', 'True Noisy'],\n",
    "                ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Classification Performance')\n",
    "    \n",
    "    # 5. Comment length distribution\n",
    "    cleaned_lengths = [len(c.comment_text.split()) for c in cleaned.comments]\n",
    "    removed_lengths = [len(c.comment_text.split()) for c in removed.comments]\n",
    "    \n",
    "    axes[1, 1].boxplot([cleaned_lengths, removed_lengths], \n",
    "                      labels=['Cleaned', 'Removed'])\n",
    "    axes[1, 1].set_ylabel('Comment Length (words)')\n",
    "    axes[1, 1].set_title('Comment Length Distribution')\n",
    "    \n",
    "    # 6. Cost-benefit analysis\n",
    "    paper_stats = {\n",
    "        'GPT-3.5': {'reduction': 66, 'improvement': 13.0, 'cost': 50},\n",
    "        'Llama3': {'reduction': 25, 'improvement': 7.5, 'cost': 0},\n",
    "        'Manual': {'reduction': 0, 'improvement': 0, 'cost': 25600}\n",
    "    }\n",
    "    \n",
    "    methods = list(paper_stats.keys())\n",
    "    costs = [paper_stats[m]['cost'] for m in methods]\n",
    "    improvements = [paper_stats[m]['improvement'] for m in methods]\n",
    "    \n",
    "    ax2 = axes[1, 2].twinx()\n",
    "    \n",
    "    bars1 = axes[1, 2].bar(np.arange(len(methods)) - 0.2, costs, 0.4, \n",
    "                          label='Cost ($)', color='#e74c3c')\n",
    "    bars2 = ax2.bar(np.arange(len(methods)) + 0.2, improvements, 0.4,\n",
    "                    label='BLEU Improvement (%)', color='#2ecc71')\n",
    "    \n",
    "    axes[1, 2].set_xlabel('Method')\n",
    "    axes[1, 2].set_ylabel('Cost ($)', color='#e74c3c')\n",
    "    ax2.set_ylabel('BLEU Improvement (%)', color='#2ecc71')\n",
    "    axes[1, 2].set_xticks(np.arange(len(methods)))\n",
    "    axes[1, 2].set_xticklabels(methods)\n",
    "    axes[1, 2].set_title('Cost vs. Performance (from paper)')\n",
    "    axes[1, 2].set_yscale('log')  # Log scale for cost due to large difference\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "visualize_cleaning_results(train_stats, cleaned_train, removed_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating Controlled Datasets\n",
    "\n",
    "To ensure fair comparison, we need controlled datasets with the same size as cleaned ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlledDatasetCreator:\n",
    "    \"\"\"Create controlled datasets for fair comparison\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_controlled_dataset(original: List[ReviewComment], \n",
    "                                target_size: int,\n",
    "                                strategy: str = \"random\") -> DatasetPartition:\n",
    "        \"\"\"\n",
    "        Create controlled dataset by sampling from original.\n",
    "        \n",
    "        Strategies:\n",
    "        - random: Random sampling\n",
    "        - stratified: Maintain original valid/noisy ratio\n",
    "        - recent: Select most recent comments\n",
    "        \"\"\"\n",
    "        \n",
    "        if strategy == \"random\":\n",
    "            sampled = random.sample(original, min(target_size, len(original)))\n",
    "            \n",
    "        elif strategy == \"stratified\":\n",
    "            # Maintain original ratio\n",
    "            valid_comments = [c for c in original if c.original_label == \"valid\"]\n",
    "            noisy_comments = [c for c in original if c.original_label == \"noisy\"]\n",
    "            \n",
    "            valid_ratio = len(valid_comments) / len(original)\n",
    "            n_valid = int(target_size * valid_ratio)\n",
    "            n_noisy = target_size - n_valid\n",
    "            \n",
    "            sampled_valid = random.sample(valid_comments, min(n_valid, len(valid_comments)))\n",
    "            sampled_noisy = random.sample(noisy_comments, min(n_noisy, len(noisy_comments)))\n",
    "            \n",
    "            sampled = sampled_valid + sampled_noisy\n",
    "            random.shuffle(sampled)\n",
    "            \n",
    "        elif strategy == \"recent\":\n",
    "            # Sort by timestamp and take most recent\n",
    "            sorted_comments = sorted(original, key=lambda c: c.timestamp, reverse=True)\n",
    "            sampled = sorted_comments[:target_size]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        # Calculate stats\n",
    "        valid_ratio = sum(1 for c in sampled if c.original_label == \"valid\") / len(sampled)\n",
    "        \n",
    "        return DatasetPartition(\n",
    "            name=f\"controlled_{strategy}\",\n",
    "            comments=sampled,\n",
    "            stats=CleaningStats(\n",
    "                original_size=len(original),\n",
    "                cleaned_size=len(sampled),\n",
    "                removed_size=len(original) - len(sampled),\n",
    "                original_valid_ratio=sum(1 for c in original if c.original_label == \"valid\") / len(original),\n",
    "                cleaned_valid_ratio=valid_ratio,\n",
    "                reduction_percentage=(1 - len(sampled)/len(original)) * 100,\n",
    "                improvement_percentage=0,  # No improvement expected\n",
    "                processing_time=0,\n",
    "                cost_estimate=0,\n",
    "                confidence_distribution={}\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Create controlled datasets\n",
    "controller = ControlledDatasetCreator()\n",
    "\n",
    "# Match the size of cleaned dataset\n",
    "controlled_random = controller.create_controlled_dataset(\n",
    "    train_data, \n",
    "    train_stats.cleaned_size,\n",
    "    strategy=\"random\"\n",
    ")\n",
    "\n",
    "controlled_stratified = controller.create_controlled_dataset(\n",
    "    train_data,\n",
    "    train_stats.cleaned_size, \n",
    "    strategy=\"stratified\"\n",
    ")\n",
    "\n",
    "print(\"=== Controlled Dataset Statistics ===\")\n",
    "print(f\"\\nRandom sampling:\")\n",
    "print(f\"  Size: {len(controlled_random.comments)}\")\n",
    "print(f\"  Valid ratio: {controlled_random.stats.cleaned_valid_ratio:.1%}\")\n",
    "\n",
    "print(f\"\\nStratified sampling:\")\n",
    "print(f\"  Size: {len(controlled_stratified.comments)}\")\n",
    "print(f\"  Valid ratio: {controlled_stratified.stats.cleaned_valid_ratio:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Simulating Model Training Impact\n",
    "\n",
    "Let's simulate how different datasets affect model training (Section V-B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainingSimulator:\n",
    "    \"\"\"Simulate the impact of data cleaning on model training\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = \"CodeReviewer\"):\n",
    "        self.model_type = model_type\n",
    "        self.training_history = []\n",
    "        \n",
    "    def simulate_training(self, dataset: DatasetPartition, epochs: int = 10) -> Dict:\n",
    "        \"\"\"\n",
    "        Simulate training on a dataset.\n",
    "        \n",
    "        In real implementation, this would fine-tune actual models.\n",
    "        Here we simulate based on paper's findings.\n",
    "        \"\"\"\n",
    "        print(f\"\\nSimulating {self.model_type} training on {dataset.name}...\")\n",
    "        print(f\"Dataset size: {len(dataset.comments)}\")\n",
    "        print(f\"Valid ratio: {dataset.stats.cleaned_valid_ratio:.1%}\")\n",
    "        \n",
    "        # Simulate training metrics based on data quality\n",
    "        base_bleu = 5.73  # Original CodeReviewer BLEU-4 from paper\n",
    "        \n",
    "        # Quality affects final performance\n",
    "        quality_factor = dataset.stats.cleaned_valid_ratio\n",
    "        \n",
    "        # Size affects convergence speed\n",
    "        size_factor = len(dataset.comments) / 1000  # Normalized\n",
    "        \n",
    "        history = {\n",
    "            'epochs': [],\n",
    "            'loss': [],\n",
    "            'bleu': [],\n",
    "            'val_bleu': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Simulate loss decay\n",
    "            loss = 2.0 * np.exp(-epoch * quality_factor * 0.3) + 0.2\n",
    "            \n",
    "            # Simulate BLEU improvement\n",
    "            if 'cleaned' in dataset.name:\n",
    "                # Cleaned data improves faster and reaches higher BLEU\n",
    "                improvement = 0.13 * (1 - np.exp(-epoch * 0.5))  # Up to 13% improvement\n",
    "            elif 'controlled' in dataset.name:\n",
    "                # Controlled shows minimal improvement\n",
    "                improvement = 0.02 * (1 - np.exp(-epoch * 0.3))\n",
    "            else:\n",
    "                # Original dataset baseline\n",
    "                improvement = 0\n",
    "            \n",
    "            bleu = base_bleu * (1 + improvement)\n",
    "            val_bleu = bleu * 0.95  # Validation slightly lower\n",
    "            \n",
    "            history['epochs'].append(epoch + 1)\n",
    "            history['loss'].append(loss)\n",
    "            history['bleu'].append(bleu) \n",
    "            history['val_bleu'].append(val_bleu)\n",
    "            \n",
    "            if epoch % 3 == 0:\n",
    "                print(f\"  Epoch {epoch+1}: Loss={loss:.3f}, BLEU={bleu:.2f}\")\n",
    "        \n",
    "        # Final results\n",
    "        final_results = {\n",
    "            'dataset': dataset.name,\n",
    "            'final_bleu': history['bleu'][-1],\n",
    "            'improvement': (history['bleu'][-1] - base_bleu) / base_bleu * 100,\n",
    "            'training_time': len(dataset.comments) * 0.01,  # Simulated time\n",
    "            'history': history\n",
    "        }\n",
    "        \n",
    "        self.training_history.append(final_results)\n",
    "        return final_results\n",
    "    \n",
    "    def compare_results(self) -> pd.DataFrame:\n",
    "        \"\"\"Compare training results across datasets\"\"\"\n",
    "        \n",
    "        comparison = []\n",
    "        for result in self.training_history:\n",
    "            comparison.append({\n",
    "                'Dataset': result['dataset'],\n",
    "                'Final BLEU-4': f\"{result['final_bleu']:.2f}\",\n",
    "                'Improvement': f\"{result['improvement']:+.1f}%\",\n",
    "                'Training Time': f\"{result['training_time']:.1f}h\"\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(comparison)\n",
    "\n",
    "# Simulate training on different datasets\n",
    "trainer = ModelTrainingSimulator(\"CodeReviewer\")\n",
    "\n",
    "# Train on original (baseline)\n",
    "original_partition = DatasetPartition(\n",
    "    name=\"original\",\n",
    "    comments=train_data[:100],\n",
    "    stats=CleaningStats(\n",
    "        original_size=100,\n",
    "        cleaned_size=100,\n",
    "        removed_size=0,\n",
    "        original_valid_ratio=0.64,\n",
    "        cleaned_valid_ratio=0.64,\n",
    "        reduction_percentage=0,\n",
    "        improvement_percentage=0,\n",
    "        processing_time=0,\n",
    "        cost_estimate=0\n",
    "    )\n",
    ")\n",
    "\n",
    "results_original = trainer.simulate_training(original_partition)\n",
    "results_cleaned = trainer.simulate_training(cleaned_train)\n",
    "results_controlled = trainer.simulate_training(controlled_random)\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n=== Training Results Comparison ===\")\n",
    "comparison_df = trainer.compare_results()\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizing Training Impact\n",
    "\n",
    "Let's visualize how data cleaning affects training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_comparison(training_history: List[Dict]):\n",
    "    \"\"\"Visualize training curves for different datasets\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    colors = {'original': '#3498db', 'cleaned_gpt-3.5-turbo': '#2ecc71', \n",
    "              'controlled_random': '#e67e22'}\n",
    "    \n",
    "    # 1. Loss curves\n",
    "    for result in training_history:\n",
    "        color = colors.get(result['dataset'], '#95a5a6')\n",
    "        axes[0].plot(result['history']['epochs'], \n",
    "                    result['history']['loss'],\n",
    "                    label=result['dataset'].replace('_', ' ').title(),\n",
    "                    color=color,\n",
    "                    linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. BLEU improvement\n",
    "    for result in training_history:\n",
    "        color = colors.get(result['dataset'], '#95a5a6')\n",
    "        axes[1].plot(result['history']['epochs'],\n",
    "                    result['history']['bleu'],\n",
    "                    label=result['dataset'].replace('_', ' ').title(),\n",
    "                    color=color,\n",
    "                    linewidth=2)\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('BLEU-4 Score')\n",
    "    axes[1].set_title('BLEU-4 Score Evolution')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add paper's reported improvements as reference lines\n",
    "    axes[1].axhline(y=5.73, color='gray', linestyle='--', alpha=0.5, label='Original baseline')\n",
    "    axes[1].axhline(y=6.47, color='green', linestyle='--', alpha=0.5, label='Paper: Cleaned (+13%)')\n",
    "    \n",
    "    # 3. Final performance comparison\n",
    "    datasets = [r['dataset'].replace('_', '\\n') for r in training_history]\n",
    "    final_bleus = [r['final_bleu'] for r in training_history]\n",
    "    improvements = [r['improvement'] for r in training_history]\n",
    "    \n",
    "    bars = axes[2].bar(range(len(datasets)), final_bleus, \n",
    "                      color=[colors.get(r['dataset'], '#95a5a6') for r in training_history])\n",
    "    \n",
    "    # Add improvement percentages on bars\n",
    "    for i, (bar, imp) in enumerate(zip(bars, improvements)):\n",
    "        height = bar.get_height()\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2, height + 0.05,\n",
    "                    f'{imp:+.1f}%', ha='center', fontsize=10)\n",
    "    \n",
    "    axes[2].set_xticks(range(len(datasets)))\n",
    "    axes[2].set_xticklabels(datasets)\n",
    "    axes[2].set_ylabel('Final BLEU-4 Score')\n",
    "    axes[2].set_title('Final Performance Comparison')\n",
    "    axes[2].set_ylim(5, 7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training comparison\n",
    "visualize_training_comparison(trainer.training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyzing Different Cleaning Models\n",
    "\n",
    "The paper tests two LLMs for cleaning: GPT-3.5 and Llama3. Let's compare their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cleaning_models():\n",
    "    \"\"\"Compare different LLMs for data cleaning (Table II from paper)\"\"\"\n",
    "    \n",
    "    # Statistics from the paper\n",
    "    cleaning_models = {\n",
    "        'GPT-3.5': {\n",
    "            'training_cleaned': 39625,\n",
    "            'training_original': 117739,\n",
    "            'reduction': 66.3,\n",
    "            'precision_valid': 85.1,\n",
    "            'recall_noisy': 88.8,\n",
    "            'bleu_improvement': 13.0,\n",
    "            'cost': 50,\n",
    "            'time_hours': 39\n",
    "        },\n",
    "        'Llama3': {\n",
    "            'training_cleaned': 87872,\n",
    "            'training_original': 117739,\n",
    "            'reduction': 25.4,\n",
    "            'precision_valid': 75.3,\n",
    "            'recall_noisy': 51.0,\n",
    "            'bleu_improvement': 7.5,\n",
    "            'cost': 0,  # Open source\n",
    "            'time_hours': 15\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    models = list(cleaning_models.keys())\n",
    "    \n",
    "    # 1. Dataset reduction comparison\n",
    "    reductions = [cleaning_models[m]['reduction'] for m in models]\n",
    "    kept = [100 - r for r in reductions]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, kept, width, label='Kept', color='#2ecc71')\n",
    "    axes[0, 0].bar(x + width/2, reductions, width, label='Removed', color='#e74c3c')\n",
    "    axes[0, 0].set_ylabel('Percentage (%)')\n",
    "    axes[0, 0].set_title('Dataset Size After Cleaning')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Classification performance\n",
    "    metrics = ['Precision (Valid)', 'Recall (Noisy)']\n",
    "    gpt_scores = [cleaning_models['GPT-3.5']['precision_valid'], \n",
    "                  cleaning_models['GPT-3.5']['recall_noisy']]\n",
    "    llama_scores = [cleaning_models['Llama3']['precision_valid'],\n",
    "                   cleaning_models['Llama3']['recall_noisy']]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    axes[0, 1].bar(x - width/2, gpt_scores, width, label='GPT-3.5', color='#3498db')\n",
    "    axes[0, 1].bar(x + width/2, llama_scores, width, label='Llama3', color='#9b59b6')\n",
    "    axes[0, 1].set_ylabel('Score (%)')\n",
    "    axes[0, 1].set_title('Classification Performance')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(metrics)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].set_ylim(0, 100)\n",
    "    \n",
    "    # 3. BLEU improvement vs dataset size\n",
    "    sizes = [cleaning_models[m]['training_cleaned'] for m in models]\n",
    "    improvements = [cleaning_models[m]['bleu_improvement'] for m in models]\n",
    "    \n",
    "    axes[1, 0].scatter(sizes, improvements, s=200, alpha=0.7)\n",
    "    for i, model in enumerate(models):\n",
    "        axes[1, 0].annotate(model, (sizes[i], improvements[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Cleaned Dataset Size')\n",
    "    axes[1, 0].set_ylabel('BLEU-4 Improvement (%)')\n",
    "    axes[1, 0].set_title('Performance vs Dataset Size Trade-off')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Cost-efficiency analysis\n",
    "    costs = [cleaning_models[m]['cost'] for m in models]\n",
    "    times = [cleaning_models[m]['time_hours'] for m in models]\n",
    "    \n",
    "    # Normalize by improvement for efficiency metric\n",
    "    cost_per_improvement = [c/i if c > 0 else 0 for c, i in zip(costs, improvements)]\n",
    "    time_per_improvement = [t/i for t, i in zip(times, improvements)]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    axes[1, 1].bar(x - width/2, cost_per_improvement, width, \n",
    "                  label='$/% improvement', color='#e74c3c')\n",
    "    axes[1, 1].bar(x + width/2, time_per_improvement, width,\n",
    "                  label='Hours/% improvement', color='#f39c12')\n",
    "    axes[1, 1].set_ylabel('Efficiency Metric')\n",
    "    axes[1, 1].set_title('Cleaning Efficiency Comparison')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(models)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n=== Cleaning Model Comparison Summary ===\")\n",
    "    comparison_data = []\n",
    "    for model, stats in cleaning_models.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model,\n",
    "            'Dataset Reduction': f\"{stats['reduction']:.1f}%\",\n",
    "            'Precision (Valid)': f\"{stats['precision_valid']:.1f}%\",\n",
    "            'BLEU Improvement': f\"{stats['bleu_improvement']:.1f}%\",\n",
    "            'Cost': f\"${stats['cost']}\" if stats['cost'] > 0 else \"Free\",\n",
    "            'Time': f\"{stats['time_hours']}h\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "compare_cleaning_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Production Pipeline Implementation\n",
    "\n",
    "Let's create a production-ready cleaning pipeline based on the paper's findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionCleaningPipeline:\n",
    "    \"\"\"Production-ready data cleaning pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 cleaning_model: str = \"gpt-3.5-turbo\",\n",
    "                 batch_size: int = 100,\n",
    "                 save_intermediate: bool = True):\n",
    "        \n",
    "        self.cleaning_model = cleaning_model\n",
    "        self.batch_size = batch_size\n",
    "        self.save_intermediate = save_intermediate\n",
    "        \n",
    "        self.cleaner = SemanticDataCleaner(\n",
    "            model_name=cleaning_model,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        self.cleaning_log = []\n",
    "        \n",
    "    def clean_full_dataset(self, \n",
    "                          train_path: str,\n",
    "                          val_path: str,\n",
    "                          output_dir: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Clean complete dataset (train + validation).\n",
    "        \n",
    "        Args:\n",
    "            train_path: Path to training data\n",
    "            val_path: Path to validation data  \n",
    "            output_dir: Directory to save cleaned datasets\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with cleaning results and statistics\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Starting production cleaning pipeline...\")\n",
    "        print(f\"Model: {self.cleaning_model}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Clean training set\n",
    "        print(\"\\n1. Cleaning training dataset...\")\n",
    "        train_data = self._load_data(train_path)\n",
    "        cleaned_train, removed_train, train_stats = self.cleaner.clean_dataset(train_data)\n",
    "        \n",
    "        if self.save_intermediate:\n",
    "            self._save_partition(cleaned_train, f\"{output_dir}/train_cleaned.json\")\n",
    "            self._save_partition(removed_train, f\"{output_dir}/train_removed.json\")\n",
    "        \n",
    "        results['train'] = {\n",
    "            'cleaned': cleaned_train,\n",
    "            'removed': removed_train,\n",
    "            'stats': train_stats\n",
    "        }\n",
    "        \n",
    "        # Clean validation set\n",
    "        print(\"\\n2. Cleaning validation dataset...\")\n",
    "        val_data = self._load_data(val_path)\n",
    "        cleaned_val, removed_val, val_stats = self.cleaner.clean_dataset(val_data)\n",
    "        \n",
    "        if self.save_intermediate:\n",
    "            self._save_partition(cleaned_val, f\"{output_dir}/val_cleaned.json\")\n",
    "            self._save_partition(removed_val, f\"{output_dir}/val_removed.json\")\n",
    "        \n",
    "        results['val'] = {\n",
    "            'cleaned': cleaned_val,\n",
    "            'removed': removed_val,\n",
    "            'stats': val_stats\n",
    "        }\n",
    "        \n",
    "        # Generate summary report\n",
    "        self._generate_report(results, f\"{output_dir}/cleaning_report.txt\")\n",
    "        \n",
    "        # Log results\n",
    "        self.cleaning_log.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'model': self.cleaning_model,\n",
    "            'results': results\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _load_data(self, path: str) -> List[ReviewComment]:\n",
    "        \"\"\"Load data from file (mock implementation)\"\"\"\n",
    "        # In production, implement actual data loading\n",
    "        return generate_mock_dataset(100)\n",
    "    \n",
    "    def _save_partition(self, partition: DatasetPartition, path: str):\n",
    "        \"\"\"Save partition to file\"\"\"\n",
    "        # In production, implement actual saving logic\n",
    "        print(f\"  Saved {len(partition.comments)} comments to {path}\")\n",
    "    \n",
    "    def _generate_report(self, results: Dict, path: str):\n",
    "        \"\"\"Generate comprehensive cleaning report\"\"\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"DATA CLEANING REPORT\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report.append(f\"Model: {self.cleaning_model}\")\n",
    "        report.append(f\"\\nSUMMARY\")\n",
    "        report.append(\"-\" * 30)\n",
    "        \n",
    "        total_original = 0\n",
    "        total_cleaned = 0\n",
    "        \n",
    "        for split, data in results.items():\n",
    "            stats = data['stats']\n",
    "            total_original += stats.original_size\n",
    "            total_cleaned += stats.cleaned_size\n",
    "            \n",
    "            report.append(f\"\\n{split.upper()} SET:\")\n",
    "            report.append(f\"  Original: {stats.original_size:,} comments\")\n",
    "            report.append(f\"  Cleaned: {stats.cleaned_size:,} comments ({stats.reduction_percentage:.1f}% reduction)\")\n",
    "            report.append(f\"  Valid ratio: {stats.original_valid_ratio:.1%} → {stats.cleaned_valid_ratio:.1%}\")\n",
    "            report.append(f\"  Processing time: {stats.processing_time:.1f}s\")\n",
    "        \n",
    "        report.append(f\"\\nTOTAL:\")\n",
    "        report.append(f\"  Original: {total_original:,} comments\")\n",
    "        report.append(f\"  Cleaned: {total_cleaned:,} comments\")\n",
    "        report.append(f\"  Overall reduction: {(1 - total_cleaned/total_original)*100:.1f}%\")\n",
    "        \n",
    "        report.append(f\"\\nEXPECTED IMPROVEMENTS (based on paper):\")\n",
    "        report.append(f\"  BLEU-4: +7.5% to +13.0%\")\n",
    "        report.append(f\"  Information score: +24%\")\n",
    "        report.append(f\"  Relevance score: +11%\")\n",
    "        \n",
    "        report_text = \"\\n\".join(report)\n",
    "        print(\"\\n\" + report_text)\n",
    "        \n",
    "        # In production, save to file\n",
    "        # with open(path, 'w') as f:\n",
    "        #     f.write(report_text)\n",
    "\n",
    "# Demonstrate production pipeline\n",
    "pipeline = ProductionCleaningPipeline(\n",
    "    cleaning_model=\"gpt-3.5-turbo\",\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "# Run cleaning (mock paths for demo)\n",
    "results = pipeline.clean_full_dataset(\n",
    "    train_path=\"/path/to/train.json\",\n",
    "    val_path=\"/path/to/val.json\",\n",
    "    output_dir=\"/path/to/output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Insights and Best Practices\n",
    "\n",
    "### Main Findings from the Paper:\n",
    "\n",
    "1. **Quality > Quantity**: 66% smaller dataset achieves 13% better performance\n",
    "2. **Valid Ratio Improvement**: From 64% to 85% with GPT-3.5\n",
    "3. **Cost Efficiency**: $50 vs $25,600 for manual cleaning\n",
    "4. **Time Efficiency**: 39 hours vs 2,000+ hours manually\n",
    "\n",
    "### Best Practices for Production:\n",
    "\n",
    "1. **Choose the Right Model**:\n",
    "   - GPT-3.5: Best precision (85.1%) but higher cost\n",
    "   - Llama3: Good balance of performance and cost (free)\n",
    "\n",
    "2. **Optimize Batch Processing**:\n",
    "   - Use parallel processing for large datasets\n",
    "   - Save intermediate results for recovery\n",
    "\n",
    "3. **Quality Control**:\n",
    "   - Monitor confidence scores\n",
    "   - Sample and manually verify borderline cases\n",
    "   - Track cleaning statistics over time\n",
    "\n",
    "4. **Iterative Improvement**:\n",
    "   - Start with aggressive cleaning (high precision)\n",
    "   - Gradually adjust thresholds based on results\n",
    "   - Consider ensemble approaches for critical applications\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "1. **Active Learning**: Focus manual review on uncertain cases\n",
    "2. **Domain Adaptation**: Fine-tune classifiers for specific projects\n",
    "3. **Multi-stage Cleaning**: Combine multiple models for better results\n",
    "4. **Continuous Monitoring**: Track model performance drift over time\n",
    "\n",
    "This semantic data cleaning pipeline demonstrates that thoughtful data curation can dramatically improve model performance while reducing computational costs - a win-win for production ML systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}