{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèóÔ∏è DeepSeek-Coder-V2: Mixture-of-Experts (MoE) Architecture\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "Hi·ªÉu s√¢u v·ªÅ ki·∫øn tr√∫c **Mixture-of-Experts (MoE)** ƒë∆∞·ª£c s·ª≠ d·ª•ng trong DeepSeek-Coder-V2, bao g·ªìm:\n",
    "\n",
    "1. **MoE Fundamentals**: Nguy√™n l√Ω ho·∫°t ƒë·ªông v√† architecture design\n",
    "2. **Parameter Efficiency**: L√†m th·∫ø n√†o MoE ƒë·∫°t efficiency v·ªõi sparse activation\n",
    "3. **Routing Mechanism**: Expert selection v√† load balancing\n",
    "4. **Implementation Details**: Code implementation t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao\n",
    "5. **Performance Analysis**: So s√°nh MoE vs Dense models\n",
    "\n",
    "## üìö Paper References\n",
    "\n",
    "**Section 3.2: Model Architecture**\n",
    "> \"Our architecture aligns with that of DeepSeekV2. The hyperparameters settings, 16B and 236B, correspond to those used in DeepSeek-V2-Lite and DeepSeek-V2, respectively.\"\n",
    "\n",
    "**Key MoE Statistics from Paper:**\n",
    "- DeepSeek-Coder-V2: **236B total params**, **21B active params** (8.9% efficiency)\n",
    "- DeepSeek-Coder-V2-Lite: **16B total params**, **2.4B active params** (15% efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ MoE Architecture Learning Environment Ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† MoE Theory Deep Dive\n",
    "\n",
    "### üí° What is Mixture-of-Experts?\n",
    "\n",
    "**Mixture-of-Experts** l√† m·ªôt ki·∫øn tr√∫c neural network s·ª≠ d·ª•ng multiple specialized sub-networks (experts) thay v√¨ m·ªôt dense network l·ªõn.\n",
    "\n",
    "### üîë Key Concepts:\n",
    "\n",
    "1. **Experts**: C√°c sub-networks chuy√™n bi·ªát\n",
    "2. **Gating Network**: Router quy·∫øt ƒë·ªãnh expert n√†o ƒë∆∞·ª£c activate\n",
    "3. **Sparse Activation**: Ch·ªâ m·ªôt subset experts ƒë∆∞·ª£c s·ª≠ d·ª•ng cho m·ªói input\n",
    "4. **Load Balancing**: ƒê·∫£m b·∫£o experts ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÅu\n",
    "\n",
    "### üìä Mathematical Foundation:\n",
    "\n",
    "Cho input $x$, MoE output ƒë∆∞·ª£c t√≠nh:\n",
    "\n",
    "$$y = \\sum_{i=1}^{n} G(x)_i \\cdot E_i(x)$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- $G(x)_i$: Gating weight cho expert $i$\n",
    "- $E_i(x)$: Output c·ªßa expert $i$\n",
    "- $n$: S·ªë l∆∞·ª£ng experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"Single Expert trong MoE Layer\n",
    "    \n",
    "    M·ªói expert l√† m·ªôt feed-forward network ƒë∆°n gi·∫£n\n",
    "    t∆∞∆°ng t·ª± nh∆∞ FFN trong Transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Two-layer MLP v·ªõi SwiGLU activation (nh∆∞ trong DeepSeek)\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)  # Gate projection\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)  # Down projection\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=False)  # Up projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"SwiGLU activation: SiLU(x @ W1) * (x @ W3) @ W2\"\"\"\n",
    "        gate = F.silu(self.w1(x))  # SiLU activation\n",
    "        up = self.w3(x)\n",
    "        return self.w2(self.dropout(gate * up))\n",
    "\n",
    "class TopKGating(nn.Module):\n",
    "    \"\"\"Top-K Gating mechanism cho MoE\n",
    "    \n",
    "    Ch·ªçn top-k experts cho m·ªói token v√† t√≠nh gating weights\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_experts: int, top_k: int = 2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Gating network: simple linear layer\n",
    "        self.gate = nn.Linear(d_model, num_experts, bias=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute top-k gating\n",
    "        \n",
    "        Returns:\n",
    "            gates: Gating weights [batch_size, seq_len, top_k]\n",
    "            indices: Expert indices [batch_size, seq_len, top_k]  \n",
    "            load: Load balancing loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute gating logits\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Top-k selection\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        \n",
    "        # Softmax over top-k experts\n",
    "        top_k_gates = F.softmax(top_k_logits, dim=-1)\n",
    "        \n",
    "        # Load balancing loss (auxiliary loss ƒë·ªÉ ƒë·∫£m b·∫£o experts ƒë∆∞·ª£c d√πng ƒë·ªÅu)\n",
    "        gates_mean = F.softmax(logits, dim=-1).mean(dim=[0, 1])  # [num_experts]\n",
    "        load_loss = self.num_experts * torch.sum(gates_mean * gates_mean)\n",
    "        \n",
    "        return top_k_gates, top_k_indices, load_loss\n",
    "\n",
    "# Demo basic components\n",
    "print(\"üß™ Testing MoE Components:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test Expert\n",
    "d_model, d_ff = 512, 2048\n",
    "expert = Expert(d_model, d_ff)\n",
    "x = torch.randn(2, 10, d_model)  # [batch=2, seq_len=10, d_model=512]\n",
    "expert_out = expert(x)\n",
    "print(f\"‚úÖ Expert output shape: {expert_out.shape}\")\n",
    "\n",
    "# Test Gating\n",
    "num_experts, top_k = 8, 2\n",
    "gating = TopKGating(d_model, num_experts, top_k)\n",
    "gates, indices, load_loss = gating(x)\n",
    "print(f\"‚úÖ Gating weights shape: {gates.shape}\")\n",
    "print(f\"‚úÖ Expert indices shape: {indices.shape}\")\n",
    "print(f\"‚úÖ Load balancing loss: {load_loss.item():.4f}\")\n",
    "print(f\"üìä Selected experts for first token: {indices[0, 0].tolist()}\")\n",
    "print(f\"üìä Gating weights for first token: {gates[0, 0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Complete MoE Layer Implementation\n",
    "\n",
    "### üìã DeepSeek-V2 MoE Architecture Details:\n",
    "\n",
    "Theo paper, DeepSeek-V2 s·ª≠ d·ª•ng:\n",
    "- **Multi-head Latent Attention (MLA)**: Efficient attention mechanism\n",
    "- **DeepSeekMoE**: Shared experts + Routed experts\n",
    "- **Expert specialization**: Different experts cho different types of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Complete MoE Layer Implementation\n",
    "    \n",
    "    Based on DeepSeek-V2 architecture v·ªõi shared + routed experts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        d_ff: int,\n",
    "        num_experts: int,\n",
    "        num_shared_experts: int = 2,\n",
    "        top_k: int = 6,  # DeepSeek-V2 uses top-6\n",
    "        dropout: float = 0.1,\n",
    "        expert_capacity_factor: float = 1.25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_experts = num_experts\n",
    "        self.num_shared_experts = num_shared_experts\n",
    "        self.top_k = top_k\n",
    "        self.expert_capacity_factor = expert_capacity_factor\n",
    "        \n",
    "        # Shared experts (always activated)\n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(num_shared_experts)\n",
    "        ])\n",
    "        \n",
    "        # Routed experts (sparsely activated)\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gate = TopKGating(d_model, num_experts, top_k)\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass through MoE layer\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            output: MoE output [batch_size, seq_len, d_model]\n",
    "            aux_loss: Auxiliary loss for load balancing\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 1. Shared experts (always computed)\n",
    "        shared_output = torch.zeros_like(x)\n",
    "        for expert in self.shared_experts:\n",
    "            shared_output += expert(x)\n",
    "        \n",
    "        # 2. Routed experts (sparsely computed)\n",
    "        gates, indices, aux_loss = self.gate(x)\n",
    "        \n",
    "        # Reshape for expert computation\n",
    "        flat_x = x.view(-1, d_model)  # [batch_size * seq_len, d_model]\n",
    "        flat_gates = gates.view(-1, self.top_k)  # [batch_size * seq_len, top_k]\n",
    "        flat_indices = indices.view(-1, self.top_k)  # [batch_size * seq_len, top_k]\n",
    "        \n",
    "        # Compute routed expert outputs\n",
    "        routed_output = torch.zeros_like(flat_x)\n",
    "        \n",
    "        for i, expert in enumerate(self.routed_experts):\n",
    "            # Find tokens routed to this expert\n",
    "            expert_mask = (flat_indices == i)\n",
    "            if expert_mask.any():\n",
    "                # Get token indices and weights for this expert\n",
    "                token_indices, expert_positions = torch.where(expert_mask)\n",
    "                \n",
    "                if len(token_indices) > 0:\n",
    "                    # Expert computation for selected tokens\n",
    "                    expert_tokens = flat_x[token_indices]\n",
    "                    expert_output = expert(expert_tokens)\n",
    "                    \n",
    "                    # Weight by gating values\n",
    "                    weights = flat_gates[token_indices, expert_positions].unsqueeze(-1)\n",
    "                    weighted_output = expert_output * weights\n",
    "                    \n",
    "                    # Accumulate in routed_output\n",
    "                    routed_output[token_indices] += weighted_output\n",
    "        \n",
    "        # Reshape back\n",
    "        routed_output = routed_output.view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 3. Combine shared + routed outputs\n",
    "        output = shared_output + routed_output\n",
    "        \n",
    "        # 4. Residual connection v√† layer norm\n",
    "        output = self.norm(x + output)\n",
    "        \n",
    "        return output, aux_loss\n",
    "    \n",
    "    def get_expert_usage_stats(self, x: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"Analyze expert usage patterns\"\"\"\n",
    "        with torch.no_grad():\n",
    "            gates, indices, _ = self.gate(x)\n",
    "            \n",
    "            # Count expert usage\n",
    "            expert_counts = torch.zeros(self.num_experts)\n",
    "            for i in range(self.num_experts):\n",
    "                expert_counts[i] = (indices == i).sum().item()\n",
    "            \n",
    "            total_selections = indices.numel()\n",
    "            usage_percentages = expert_counts / total_selections * 100\n",
    "            \n",
    "            return {\n",
    "                'expert_usage': usage_percentages.tolist(),\n",
    "                'max_usage': usage_percentages.max().item(),\n",
    "                'min_usage': usage_percentages.min().item(),\n",
    "                'usage_std': usage_percentages.std().item(),\n",
    "                'total_selections': total_selections\n",
    "            }\n",
    "\n",
    "# Test complete MoE layer\n",
    "print(\"\\nüèóÔ∏è Testing Complete MoE Layer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# MoE configuration similar to DeepSeek-V2\n",
    "moe_config = {\n",
    "    'd_model': 512,\n",
    "    'd_ff': 2048,\n",
    "    'num_experts': 64,  # DeepSeek-V2 uses 160 experts\n",
    "    'num_shared_experts': 2,\n",
    "    'top_k': 6,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "moe_layer = MoELayer(**moe_config)\n",
    "\n",
    "# Test forward pass\n",
    "batch_size, seq_len = 4, 32\n",
    "x = torch.randn(batch_size, seq_len, moe_config['d_model'])\n",
    "\n",
    "output, aux_loss = moe_layer(x)\n",
    "print(f\"‚úÖ MoE output shape: {output.shape}\")\n",
    "print(f\"‚úÖ Auxiliary loss: {aux_loss.item():.6f}\")\n",
    "\n",
    "# Analyze expert usage\n",
    "usage_stats = moe_layer.get_expert_usage_stats(x)\n",
    "print(f\"üìä Expert usage statistics:\")\n",
    "print(f\"   Max usage: {usage_stats['max_usage']:.2f}%\")\n",
    "print(f\"   Min usage: {usage_stats['min_usage']:.2f}%\")\n",
    "print(f\"   Usage std: {usage_stats['usage_std']:.2f}%\")\n",
    "print(f\"   Total selections: {usage_stats['total_selections']}\")\n",
    "\n",
    "# Calculate parameter efficiency\n",
    "total_params = sum(p.numel() for p in moe_layer.parameters())\n",
    "shared_params = sum(p.numel() for expert in moe_layer.shared_experts for p in expert.parameters())\n",
    "single_expert_params = sum(p.numel() for p in moe_layer.routed_experts[0].parameters())\n",
    "active_expert_params = moe_config['top_k'] * single_expert_params\n",
    "gate_params = sum(p.numel() for p in moe_layer.gate.parameters())\n",
    "\n",
    "active_params = shared_params + active_expert_params + gate_params\n",
    "efficiency = active_params / total_params * 100\n",
    "\n",
    "print(f\"\\n‚ö° Parameter Efficiency:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Active parameters: {active_params:,}\")\n",
    "print(f\"   Efficiency: {efficiency:.1f}%\")\n",
    "print(f\"   Similar to DeepSeek-Coder-V2: 8.9% (21B/236B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä MoE vs Dense Model Comparison\n",
    "\n",
    "### üî¨ Performance Analysis\n",
    "\n",
    "So s√°nh hi·ªáu su·∫•t t√≠nh to√°n v√† memory gi·ªØa MoE v√† Dense models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFFN(nn.Module):\n",
    "    \"\"\"Dense Feed-Forward Network ƒë·ªÉ so s√°nh v·ªõi MoE\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate = F.silu(self.w1(x))\n",
    "        up = self.w3(x)\n",
    "        output = self.w2(self.dropout(gate * up))\n",
    "        return self.norm(x + output)\n",
    "\n",
    "def benchmark_models(d_model: int, seq_len: int, batch_size: int = 1, num_trials: int = 10):\n",
    "    \"\"\"Benchmark MoE vs Dense models\"\"\"\n",
    "    \n",
    "    # Model configurations\n",
    "    configs = {\n",
    "        'MoE-64': {\n",
    "            'model': MoELayer(d_model, d_model*4, num_experts=64, top_k=6),\n",
    "            'type': 'MoE'\n",
    "        },\n",
    "        'MoE-32': {\n",
    "            'model': MoELayer(d_model, d_model*4, num_experts=32, top_k=4),\n",
    "            'type': 'MoE'\n",
    "        },\n",
    "        'Dense-Large': {\n",
    "            'model': DenseFFN(d_model, d_model*8),  # Larger to match MoE capacity\n",
    "            'type': 'Dense'\n",
    "        },\n",
    "        'Dense-Small': {\n",
    "            'model': DenseFFN(d_model, d_model*4),\n",
    "            'type': 'Dense'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        model = config['model']\n",
    "        model.eval()\n",
    "        \n",
    "        # Parameter count\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # Memory usage (approximate)\n",
    "        if config['type'] == 'MoE':\n",
    "            # Only count active parameters for memory\n",
    "            shared_params = sum(p.numel() for expert in model.shared_experts for p in expert.parameters())\n",
    "            single_expert_params = sum(p.numel() for p in model.routed_experts[0].parameters())\n",
    "            active_params = shared_params + model.top_k * single_expert_params\n",
    "        else:\n",
    "            active_params = total_params\n",
    "        \n",
    "        # Speed benchmark\n",
    "        import time\n",
    "        times = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Warmup\n",
    "            for _ in range(3):\n",
    "                if config['type'] == 'MoE':\n",
    "                    _ = model(x)\n",
    "                else:\n",
    "                    _ = model(x)\n",
    "            \n",
    "            # Benchmark\n",
    "            for _ in range(num_trials):\n",
    "                start_time = time.time()\n",
    "                if config['type'] == 'MoE':\n",
    "                    output, aux_loss = model(x)\n",
    "                else:\n",
    "                    output = model(x)\n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "        \n",
    "        results[name] = {\n",
    "            'total_params': total_params,\n",
    "            'active_params': active_params,\n",
    "            'efficiency': active_params / total_params * 100,\n",
    "            'avg_time_ms': avg_time,\n",
    "            'type': config['type']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"üèÉ Running MoE vs Dense Benchmark...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "benchmark_results = benchmark_models(\n",
    "    d_model=512, \n",
    "    seq_len=128, \n",
    "    batch_size=4, \n",
    "    num_trials=20\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Model':<15} {'Type':<6} {'Total Params':<12} {'Active Params':<12} {'Efficiency':<10} {'Time (ms)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, stats in benchmark_results.items():\n",
    "    print(f\"{name:<15} {stats['type']:<6} {stats['total_params']:<12,} \"\n",
    "          f\"{stats['active_params']:<12,} {stats['efficiency']:<10.1f}% \"\n",
    "          f\"{stats['avg_time_ms']:<10.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = list(benchmark_results.keys())\n",
    "total_params = [benchmark_results[m]['total_params']/1e6 for m in models]  # Convert to millions\n",
    "active_params = [benchmark_results[m]['active_params']/1e6 for m in models]\n",
    "times = [benchmark_results[m]['avg_time_ms'] for m in models]\n",
    "colors = ['red' if benchmark_results[m]['type'] == 'MoE' else 'blue' for m in models]\n",
    "\n",
    "# Parameter comparison\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, total_params, width, label='Total', alpha=0.7)\n",
    "axes[0].bar(x + width/2, active_params, width, label='Active', alpha=0.7)\n",
    "axes[0].set_xlabel('Models')\n",
    "axes[0].set_ylabel('Parameters (Millions)')\n",
    "axes[0].set_title('Parameter Count Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45)\n",
    "axes[0].legend()\n",
    "\n",
    "# Efficiency\n",
    "efficiency = [benchmark_results[m]['efficiency'] for m in models]\n",
    "bars = axes[1].bar(models, efficiency, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Parameter Efficiency (%)')\n",
    "axes[1].set_title('Parameter Efficiency')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add DeepSeek-V2 reference line\n",
    "axes[1].axhline(y=8.9, color='orange', linestyle='--', alpha=0.8, label='DeepSeek-V2 (8.9%)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Timing\n",
    "bars = axes[2].bar(models, times, color=colors, alpha=0.7)\n",
    "axes[2].set_ylabel('Inference Time (ms)')\n",
    "axes[2].set_title('Inference Speed')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add legend for colors\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='red', alpha=0.7, label='MoE'),\n",
    "                   Patch(facecolor='blue', alpha=0.7, label='Dense')]\n",
    "fig.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(\"‚Ä¢ MoE models achieve higher capacity with similar active parameters\")\n",
    "print(\"‚Ä¢ Parameter efficiency varies with expert count and top-k\")\n",
    "print(\"‚Ä¢ Inference speed depends on routing overhead vs computation savings\")\n",
    "print(\"‚Ä¢ DeepSeek-V2's 8.9% efficiency is achieved through careful architecture design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Expert Specialization Analysis\n",
    "\n",
    "### üß™ Understanding How Experts Specialize\n",
    "\n",
    "Ph√¢n t√≠ch c√°ch c√°c experts trong MoE h·ªçc specialization cho different types of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expert_specialization(moe_layer: MoELayer, test_samples: Dict[str, torch.Tensor]):\n",
    "    \"\"\"Analyze expert specialization patterns\n",
    "    \n",
    "    Args:\n",
    "        moe_layer: Trained MoE layer\n",
    "        test_samples: Dict of different input types\n",
    "    \"\"\"\n",
    "    \n",
    "    specialization_stats = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample_type, sample_data in test_samples.items():\n",
    "            gates, indices, _ = moe_layer.gate(sample_data)\n",
    "            \n",
    "            # Count expert usage for this sample type\n",
    "            expert_counts = torch.zeros(moe_layer.num_experts)\n",
    "            for i in range(moe_layer.num_experts):\n",
    "                expert_counts[i] = (indices == i).sum().item()\n",
    "            \n",
    "            # Normalize to percentages\n",
    "            total_selections = indices.numel()\n",
    "            usage_percentages = expert_counts / total_selections * 100\n",
    "            \n",
    "            specialization_stats[sample_type] = {\n",
    "                'usage': usage_percentages.numpy(),\n",
    "                'top_experts': torch.topk(usage_percentages, 5).indices.tolist(),\n",
    "                'concentration': torch.std(usage_percentages).item()\n",
    "            }\n",
    "    \n",
    "    return specialization_stats\n",
    "\n",
    "def create_synthetic_code_patterns(d_model: int, seq_len: int = 32, batch_size: int = 8):\n",
    "    \"\"\"Create synthetic data patterns to simulate different code types\"\"\"\n",
    "    \n",
    "    patterns = {}\n",
    "    \n",
    "    # Different \"code\" patterns (simulated v·ªõi different distributions)\n",
    "    \n",
    "    # 1. \"Function definitions\" - structured patterns\n",
    "    func_pattern = torch.randn(batch_size, seq_len, d_model)\n",
    "    func_pattern[:, :5] *= 2.0  # Strong start pattern\n",
    "    patterns['functions'] = func_pattern\n",
    "    \n",
    "    # 2. \"Control flow\" - repetitive patterns  \n",
    "    control_pattern = torch.randn(batch_size, seq_len, d_model)\n",
    "    control_pattern[:, ::3] *= 1.5  # Periodic patterns\n",
    "    patterns['control_flow'] = control_pattern\n",
    "    \n",
    "    # 3. \"Data structures\" - complex patterns\n",
    "    data_pattern = torch.randn(batch_size, seq_len, d_model)\n",
    "    data_pattern += torch.sin(torch.arange(seq_len).float()).unsqueeze(0).unsqueeze(-1) * 0.5\n",
    "    patterns['data_structures'] = data_pattern\n",
    "    \n",
    "    # 4. \"Comments\" - sparse patterns\n",
    "    comment_pattern = torch.randn(batch_size, seq_len, d_model) * 0.5\n",
    "    patterns['comments'] = comment_pattern\n",
    "    \n",
    "    # 5. \"Math expressions\" - dense patterns\n",
    "    math_pattern = torch.randn(batch_size, seq_len, d_model) * 1.5\n",
    "    math_pattern += torch.cos(torch.arange(seq_len).float()).unsqueeze(0).unsqueeze(-1) * 0.3\n",
    "    patterns['math_expressions'] = math_pattern\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Create test patterns\n",
    "print(\"üé® Creating Synthetic Code Patterns...\")\n",
    "test_patterns = create_synthetic_code_patterns(d_model=512)\n",
    "\n",
    "# Initialize a fresh MoE layer for specialization analysis\n",
    "specialization_moe = MoELayer(\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_experts=32,  # Fewer experts for clearer visualization\n",
    "    top_k=4\n",
    ")\n",
    "\n",
    "# Simulate \"training\" by running patterns through the model multiple times\n",
    "print(\"üèãÔ∏è Simulating Expert Specialization Training...\")\n",
    "specialization_moe.train()\n",
    "\n",
    "# Simple training simulation\n",
    "optimizer = torch.optim.Adam(specialization_moe.parameters(), lr=0.001)\n",
    "for epoch in range(50):  # Quick training simulation\n",
    "    total_loss = 0\n",
    "    for pattern_type, pattern_data in test_patterns.items():\n",
    "        optimizer.zero_grad()\n",
    "        output, aux_loss = specialization_moe(pattern_data)\n",
    "        \n",
    "        # Simple reconstruction loss + auxiliary loss\n",
    "        recon_loss = F.mse_loss(output, pattern_data)\n",
    "        loss = recon_loss + 0.01 * aux_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "# Analyze specialization\n",
    "specialization_moe.eval()\n",
    "print(\"\\nüî¨ Analyzing Expert Specialization...\")\n",
    "specialization_results = analyze_expert_specialization(specialization_moe, test_patterns)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "pattern_types = list(test_patterns.keys())\n",
    "num_experts = specialization_moe.num_experts\n",
    "\n",
    "for i, pattern_type in enumerate(pattern_types):\n",
    "    if i < len(axes):\n",
    "        usage = specialization_results[pattern_type]['usage']\n",
    "        top_experts = specialization_results[pattern_type]['top_experts']\n",
    "        concentration = specialization_results[pattern_type]['concentration']\n",
    "        \n",
    "        # Bar plot of expert usage\n",
    "        bars = axes[i].bar(range(num_experts), usage, alpha=0.7)\n",
    "        \n",
    "        # Highlight top experts\n",
    "        for expert_idx in top_experts[:3]:  # Top 3\n",
    "            bars[expert_idx].set_color('red')\n",
    "            bars[expert_idx].set_alpha(0.9)\n",
    "        \n",
    "        axes[i].set_title(f'{pattern_type.replace(\"_\", \" \").title()}\\nConcentration: {concentration:.2f}')\n",
    "        axes[i].set_xlabel('Expert Index')\n",
    "        axes[i].set_ylabel('Usage (%)')\n",
    "        axes[i].set_ylim(0, max(usage) * 1.1)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(pattern_types) < len(axes):\n",
    "    axes[-1].remove()\n",
    "\n",
    "plt.suptitle('Expert Specialization Patterns\\n(Red bars = Top 3 experts for each pattern type)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìä Specialization Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pattern_type, stats in specialization_results.items():\n",
    "    top_experts = stats['top_experts'][:3]\n",
    "    concentration = stats['concentration']\n",
    "    max_usage = max(stats['usage'])\n",
    "    \n",
    "    print(f\"{pattern_type.replace('_', ' ').title():>15}: \"\n",
    "          f\"Top experts: {top_experts}, \"\n",
    "          f\"Max usage: {max_usage:.1f}%, \"\n",
    "          f\"Concentration: {concentration:.2f}\")\n",
    "\n",
    "print(\"\\nüîç Specialization Insights:\")\n",
    "print(\"‚Ä¢ Higher concentration = more specialized experts\")\n",
    "print(\"‚Ä¢ Different patterns activate different expert subsets\")\n",
    "print(\"‚Ä¢ Real DeepSeek-V2 shows similar specialization on code vs text vs math\")\n",
    "print(\"‚Ä¢ Specialization improves with more training and better routing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Advanced MoE Techniques\n",
    "\n",
    "### üîß Load Balancing v√† Expert Dropout\n",
    "\n",
    "Implementing advanced techniques ƒë·ªÉ improve MoE training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMoELayer(nn.Module):\n",
    "    \"\"\"Advanced MoE with enhanced load balancing and expert dropout\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        num_experts: int,\n",
    "        top_k: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        expert_dropout: float = 0.1,\n",
    "        load_balancing_loss_coef: float = 0.01,\n",
    "        router_z_loss_coef: float = 0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.expert_dropout = expert_dropout\n",
    "        self.load_balancing_loss_coef = load_balancing_loss_coef\n",
    "        self.router_z_loss_coef = router_z_loss_coef\n",
    "        \n",
    "        # Experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Enhanced router v·ªõi noise for better load balancing\n",
    "        self.router = nn.Linear(d_model, num_experts, bias=False)\n",
    "        self.noise_generator = nn.Linear(d_model, num_experts, bias=False)\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def add_noise_to_logits(self, logits: torch.Tensor, noise_epsilon: float = 1e-2) -> torch.Tensor:\n",
    "        \"\"\"Add noise to router logits for better load balancing\"\"\"\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * noise_epsilon\n",
    "            return logits + noise\n",
    "        return logits\n",
    "    \n",
    "    def compute_routing_weights(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Compute routing weights v·ªõi enhanced load balancing\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Router logits\n",
    "        logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Add noise during training\n",
    "        logits = self.add_noise_to_logits(logits)\n",
    "        \n",
    "        # Top-k selection\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        top_k_weights = F.softmax(top_k_logits, dim=-1)\n",
    "        \n",
    "        # Compute auxiliary losses\n",
    "        aux_losses = {}\n",
    "        \n",
    "        # 1. Load balancing loss\n",
    "        router_probs = F.softmax(logits, dim=-1)\n",
    "        expert_usage = router_probs.mean(dim=[0, 1])  # [num_experts]\n",
    "        \n",
    "        # Coefficient of variation for load balancing\n",
    "        cv_squared = (expert_usage.var() / expert_usage.mean().clamp(min=1e-10))\n",
    "        load_balancing_loss = self.num_experts * cv_squared\n",
    "        aux_losses['load_balancing'] = load_balancing_loss\n",
    "        \n",
    "        # 2. Router z-loss (ƒë·ªÉ prevent large logits)\n",
    "        router_z_loss = torch.logsumexp(logits, dim=-1).mean()\n",
    "        aux_losses['router_z'] = router_z_loss\n",
    "        \n",
    "        return top_k_weights, top_k_indices, aux_losses\n",
    "    \n",
    "    def expert_dropout_mask(self, expert_indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply expert dropout during training\"\"\"\n",
    "        if self.training and self.expert_dropout > 0:\n",
    "            # Randomly drop some expert selections\n",
    "            dropout_mask = torch.rand_like(expert_indices.float()) > self.expert_dropout\n",
    "            return dropout_mask\n",
    "        return torch.ones_like(expert_indices, dtype=torch.bool)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Forward pass v·ªõi advanced MoE techniques\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute routing\n",
    "        weights, indices, aux_losses = self.compute_routing_weights(x)\n",
    "        \n",
    "        # Apply expert dropout\n",
    "        dropout_mask = self.expert_dropout_mask(indices)\n",
    "        \n",
    "        # Flatten for expert computation\n",
    "        flat_x = x.view(-1, d_model)\n",
    "        flat_weights = weights.view(-1, self.top_k)\n",
    "        flat_indices = indices.view(-1, self.top_k)\n",
    "        flat_dropout_mask = dropout_mask.view(-1, self.top_k)\n",
    "        \n",
    "        # Expert computation\n",
    "        expert_outputs = torch.zeros_like(flat_x)\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Find tokens assigned to this expert\n",
    "            expert_mask = (flat_indices == expert_idx) & flat_dropout_mask\n",
    "            \n",
    "            if expert_mask.any():\n",
    "                token_indices, k_indices = torch.where(expert_mask)\n",
    "                \n",
    "                if len(token_indices) > 0:\n",
    "                    # Expert computation\n",
    "                    expert_tokens = flat_x[token_indices]\n",
    "                    expert_output = expert(expert_tokens)\n",
    "                    \n",
    "                    # Apply weights\n",
    "                    expert_weights = flat_weights[token_indices, k_indices].unsqueeze(-1)\n",
    "                    weighted_output = expert_output * expert_weights\n",
    "                    \n",
    "                    # Accumulate\n",
    "                    expert_outputs[token_indices] += weighted_output\n",
    "        \n",
    "        # Reshape and apply residual + norm\n",
    "        expert_outputs = expert_outputs.view(batch_size, seq_len, d_model)\n",
    "        output = self.norm(x + expert_outputs)\n",
    "        \n",
    "        # Combine auxiliary losses\n",
    "        total_aux_loss = (\n",
    "            self.load_balancing_loss_coef * aux_losses['load_balancing'] +\n",
    "            self.router_z_loss_coef * aux_losses['router_z']\n",
    "        )\n",
    "        \n",
    "        aux_losses['total'] = total_aux_loss\n",
    "        \n",
    "        return output, aux_losses\n",
    "\n",
    "# Test advanced MoE\n",
    "print(\"üöÄ Testing Advanced MoE Layer:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "advanced_moe = AdvancedMoELayer(\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_experts=32,\n",
    "    top_k=4,\n",
    "    expert_dropout=0.1,\n",
    "    load_balancing_loss_coef=0.01,\n",
    "    router_z_loss_coef=0.001\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(4, 32, 512)\n",
    "output, aux_losses = advanced_moe(x)\n",
    "\n",
    "print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "print(f\"üìä Auxiliary losses:\")\n",
    "for loss_name, loss_value in aux_losses.items():\n",
    "    print(f\"   {loss_name}: {loss_value.item():.6f}\")\n",
    "\n",
    "# Compare load balancing v·ªõi v√† without advanced techniques\n",
    "def compare_load_balancing(num_trials: int = 100):\n",
    "    \"\"\"Compare load balancing between basic and advanced MoE\"\"\"\n",
    "    \n",
    "    basic_moe = MoELayer(d_model=512, d_ff=2048, num_experts=32, top_k=4)\n",
    "    advanced_moe_test = AdvancedMoELayer(d_model=512, d_ff=2048, num_experts=32, top_k=4)\n",
    "    \n",
    "    basic_usage_stds = []\n",
    "    advanced_usage_stds = []\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        x = torch.randn(4, 32, 512)\n",
    "        \n",
    "        # Basic MoE\n",
    "        with torch.no_grad():\n",
    "            basic_stats = basic_moe.get_expert_usage_stats(x)\n",
    "            basic_usage_stds.append(basic_stats['usage_std'])\n",
    "        \n",
    "        # Advanced MoE\n",
    "        with torch.no_grad():\n",
    "            advanced_moe_test.eval()\n",
    "            _, indices_adv, _ = advanced_moe_test.compute_routing_weights(x)\n",
    "            expert_counts = torch.zeros(32)\n",
    "            for i in range(32):\n",
    "                expert_counts[i] = (indices_adv == i).sum().item()\n",
    "            usage_percentages = expert_counts / indices_adv.numel() * 100\n",
    "            advanced_usage_stds.append(usage_percentages.std().item())\n",
    "    \n",
    "    return np.mean(basic_usage_stds), np.mean(advanced_usage_stds)\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Comparing Load Balancing...\")\n",
    "basic_std, advanced_std = compare_load_balancing(50)\n",
    "print(f\"Basic MoE usage std: {basic_std:.2f}%\")\n",
    "print(f\"Advanced MoE usage std: {advanced_std:.2f}%\")\n",
    "print(f\"Improvement: {((basic_std - advanced_std) / basic_std * 100):.1f}% reduction in usage variance\")\n",
    "\n",
    "print(\"\\nüéØ Advanced MoE Benefits:\")\n",
    "print(\"‚Ä¢ Better load balancing through noise injection\")\n",
    "print(\"‚Ä¢ Expert dropout prevents overfitting\")\n",
    "print(\"‚Ä¢ Router z-loss stabilizes training\")\n",
    "print(\"‚Ä¢ Multiple auxiliary losses guide optimization\")\n",
    "print(\"‚Ä¢ Similar techniques used in DeepSeek-V2 and other SOTA MoE models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Summary & Key Takeaways\n",
    "\n",
    "### üìã What We Learned v·ªÅ MoE Architecture\n",
    "\n",
    "1. **Core Concept**: MoE enables scaling model capacity without proportional compute increase\n",
    "2. **Parameter Efficiency**: DeepSeek-V2 achieves 8.9% efficiency (21B/236B active/total)\n",
    "3. **Expert Specialization**: Different experts learn to handle different types of input patterns\n",
    "4. **Load Balancing**: Critical for preventing expert collapse v√† ensuring utilization\n",
    "5. **Advanced Techniques**: Noise injection, expert dropout, multiple auxiliary losses\n",
    "\n",
    "### üî¨ Research Directions\n",
    "\n",
    "1. **Dynamic Expert Selection**: Adaptive top-k based on input complexity\n",
    "2. **Hierarchical MoE**: Multi-level expert routing\n",
    "3. **Cross-lingual Expert Sharing**: Experts specialized for different programming languages\n",
    "4. **Hardware-aware MoE**: Optimizing cho specific accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final MoE architecture summary\n",
    "def create_moe_architecture_summary():\n",
    "    \"\"\"Create comprehensive MoE architecture summary\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. DeepSeek-V2 MoE Configuration\n",
    "    ax1 = axes[0, 0]\n",
    "    models = ['DeepSeek-Coder-V2\\n(236B)', 'DeepSeek-Coder-V2-Lite\\n(16B)']\n",
    "    total_params = [236, 16]\n",
    "    active_params = [21, 2.4]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, total_params, width, label='Total Parameters (B)', alpha=0.7, color='lightblue')\n",
    "    bars2 = ax1.bar(x + width/2, active_params, width, label='Active Parameters (B)', alpha=0.7, color='darkblue')\n",
    "    \n",
    "    ax1.set_ylabel('Parameters (Billions)')\n",
    "    ax1.set_title('DeepSeek-V2 MoE Parameter Efficiency')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add efficiency labels\n",
    "    for i, (total, active) in enumerate(zip(total_params, active_params)):\n",
    "        efficiency = active / total * 100\n",
    "        ax1.text(i, total + 5, f'{efficiency:.1f}%\\nefficiency', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Expert routing visualization\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Create mock routing heatmap\n",
    "    num_tokens = 20\n",
    "    num_experts = 16\n",
    "    top_k = 4\n",
    "    \n",
    "    routing_matrix = np.zeros((num_tokens, num_experts))\n",
    "    \n",
    "    # Simulate realistic routing patterns\n",
    "    for token in range(num_tokens):\n",
    "        # Each token selects top_k experts\n",
    "        selected_experts = np.random.choice(num_experts, top_k, replace=False)\n",
    "        weights = np.random.dirichlet(np.ones(top_k))  # Softmax-like weights\n",
    "        routing_matrix[token, selected_experts] = weights\n",
    "    \n",
    "    im = ax2.imshow(routing_matrix, cmap='Blues', aspect='auto')\n",
    "    ax2.set_xlabel('Expert Index')\n",
    "    ax2.set_ylabel('Token Index')\n",
    "    ax2.set_title('Expert Routing Pattern\\n(Darker = Higher Weight)')\n",
    "    plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 3. Load balancing comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Simulate load balancing scenarios\n",
    "    scenarios = ['Random\\nRouting', 'Basic\\nMoE', 'Advanced\\nMoE\\n(w/ Load Balancing)']\n",
    "    usage_stds = [15.2, 8.7, 3.1]  # Lower is better\n",
    "    \n",
    "    bars = ax3.bar(scenarios, usage_stds, color=['red', 'orange', 'green'], alpha=0.7)\n",
    "    ax3.set_ylabel('Expert Usage Std Dev (%)')\n",
    "    ax3.set_title('Load Balancing Effectiveness\\n(Lower = Better Balance)')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, usage_stds):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Scaling comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    model_sizes = [1, 7, 13, 30, 70, 175]  # Model sizes in billions\n",
    "    dense_compute = [s**1.2 for s in model_sizes]  # Dense scaling (superlinear)\n",
    "    moe_compute = [s**0.8 for s in model_sizes]    # MoE scaling (sublinear)\n",
    "    \n",
    "    ax4.plot(model_sizes, dense_compute, 'o-', label='Dense Models', linewidth=2, markersize=6)\n",
    "    ax4.plot(model_sizes, moe_compute, 's-', label='MoE Models', linewidth=2, markersize=6)\n",
    "    \n",
    "    ax4.set_xlabel('Model Size (B parameters)')\n",
    "    ax4.set_ylabel('Relative Compute Cost')\n",
    "    ax4.set_title('Compute Scaling: Dense vs MoE')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('MoE Architecture Deep Dive: Key Concepts & Benefits', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key statistics\n",
    "    print(\"üéØ MoE Architecture Key Numbers:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä DeepSeek-Coder-V2 Parameter Efficiency: {21/236*100:.1f}%\")\n",
    "    print(f\"üìä DeepSeek-Coder-V2-Lite Parameter Efficiency: {2.4/16*100:.1f}%\")\n",
    "    print(f\"üéØ Top-K Routing: K=6 for optimal quality/efficiency tradeoff\")\n",
    "    print(f\"‚öñÔ∏è Load Balancing: Critical for expert utilization\")\n",
    "    print(f\"üöÄ Compute Scaling: MoE enables sublinear scaling vs model size\")\n",
    "    \n",
    "    print(\"\\nüí° Key Implementation Insights:\")\n",
    "    print(\"‚Ä¢ Shared + Routed experts architecture\")\n",
    "    print(\"‚Ä¢ SwiGLU activation in experts\")\n",
    "    print(\"‚Ä¢ Noise injection for load balancing\")\n",
    "    print(\"‚Ä¢ Multiple auxiliary losses\")\n",
    "    print(\"‚Ä¢ Expert dropout for regularization\")\n",
    "    print(\"‚Ä¢ Hardware-efficient sparse computation\")\n",
    "\n",
    "create_moe_architecture_summary()\n",
    "\n",
    "print(\"\\nüéâ MoE Architecture Deep Dive Complete!\")\n",
    "print(\"\\nüìö Further Reading:\")\n",
    "print(\"‚Ä¢ Switch Transformer (Google, 2021)\")\n",
    "print(\"‚Ä¢ GLaM: Efficient Scaling of Language Models (Google, 2021)\")\n",
    "print(\"‚Ä¢ PaLM-2 Technical Report (Google, 2023)\")\n",
    "print(\"‚Ä¢ DeepSeek-V2 Paper (2024)\")\n",
    "print(\"\\n‚ú® Next: Explore YARN Context Extension! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}