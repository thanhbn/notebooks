{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Multi-Label BCE Loss in T-FREE\n",
    "\n",
    "## Learning Objectives\n",
    "1. **Understand the fundamental difference between single-label and multi-label classification**\n",
    "2. **Explore why T-FREE requires multi-label loss instead of traditional cross-entropy**\n",
    "3. **Implement and analyze the multi-label BCE loss mechanism**\n",
    "4. **Visualize and compare the loss behavior for different word predictions**\n",
    "\n",
    "## Paper Context\n",
    "\n",
    "From the T-FREE paper (Deiseroth et al., 2025):\n",
    "\n",
    "> \"T-FREE inherently exploits morphological similarities and allows for strong compression of embedding layers... through sparse activation patterns over character triplets\" (Section 1)\n",
    "\n",
    "> \"The backbone of the language model will remain free of subword tokenization as we directly embed each word in the input text with sparse activation patterns over hashed character triplets\" (Page 1)\n",
    "\n",
    "The key innovation here is that **each word activates multiple trigrams simultaneously**, requiring a loss function that can handle multiple positive labels per example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Foundation\n",
    "\n",
    "### Traditional Single-Label Classification\n",
    "\n",
    "In traditional language models with tokenizers:\n",
    "- Each token maps to **exactly one** vocabulary ID\n",
    "- Loss function: Cross-Entropy (CE)\n",
    "- Mathematical formulation:\n",
    "\n",
    "$$\\mathcal{L}_{CE} = -\\sum_{i=1}^{|V|} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "where $y_i \\in \\{0, 1\\}$ and $\\sum_i y_i = 1$ (one-hot encoding)\n",
    "\n",
    "### T-FREE Multi-Label Classification\n",
    "\n",
    "In T-FREE:\n",
    "- Each word activates **multiple trigrams**\n",
    "- Loss function: Binary Cross-Entropy (BCE) with multi-label\n",
    "- Mathematical formulation:\n",
    "\n",
    "$$\\mathcal{L}_{BCE} = -\\frac{1}{|V|}\\sum_{i=1}^{|V|} [y_i \\log(\\sigma(\\hat{y}_i)) + (1-y_i) \\log(1-\\sigma(\\hat{y}_i))]$$\n",
    "\n",
    "where $y_i \\in \\{0, 1\\}$ and $\\sum_i y_i \\geq 1$ (multi-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Single-Label vs Multi-Label\n",
    "\n",
    "Let's visualize the fundamental difference between these two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate single-label vs multi-label encoding\n",
    "vocab_size = 10\n",
    "\n",
    "# Single-label (traditional tokenizer)\n",
    "single_label = torch.zeros(vocab_size)\n",
    "single_label[3] = 1  # Only one position is active\n",
    "\n",
    "# Multi-label (T-FREE)\n",
    "multi_label = torch.zeros(vocab_size)\n",
    "multi_label[[1, 3, 5, 7]] = 1  # Multiple positions are active\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Single-label visualization\n",
    "ax1.bar(range(vocab_size), single_label.numpy(), color='blue', alpha=0.7)\n",
    "ax1.set_title('Single-Label Encoding (Traditional Tokenizer)', fontsize=14)\n",
    "ax1.set_xlabel('Vocabulary Index')\n",
    "ax1.set_ylabel('Activation')\n",
    "ax1.set_ylim(-0.1, 1.1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Multi-label visualization\n",
    "ax2.bar(range(vocab_size), multi_label.numpy(), color='green', alpha=0.7)\n",
    "ax2.set_title('Multi-Label Encoding (T-FREE)', fontsize=14)\n",
    "ax2.set_xlabel('Vocabulary Index (Trigram Hash)')\n",
    "ax2.set_ylabel('Activation')\n",
    "ax2.set_ylim(-0.1, 1.1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Single-label: {single_label.sum().item()} active positions\")\n",
    "print(f\"Multi-label: {multi_label.sum().item()} active positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Multi-Label BCE Loss\n",
    "\n",
    "Now let's implement the multi-label BCE loss as used in T-FREE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-label Binary Cross-Entropy Loss for T-FREE.\n",
    "    \n",
    "    This loss function handles the case where multiple trigrams\n",
    "    are active for a single word prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reduction='mean', label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Raw model outputs [batch_size, vocab_size]\n",
    "            targets: Multi-hot encoded targets [batch_size, vocab_size]\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss value\n",
    "        \"\"\"\n",
    "        # Apply label smoothing if specified\n",
    "        if self.label_smoothing > 0:\n",
    "            targets = targets * (1 - self.label_smoothing) + \\\n",
    "                     self.label_smoothing / targets.size(-1)\n",
    "        \n",
    "        # Calculate BCE loss with logits\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "# Compare with traditional cross-entropy\n",
    "class SingleLabelCELoss(nn.Module):\n",
    "    \"\"\"Traditional Cross-Entropy Loss for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert multi-hot to single label (for comparison)\n",
    "        single_targets = targets.argmax(dim=-1)\n",
    "        return self.ce_loss(logits, single_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Behavior Analysis\n",
    "\n",
    "Let's analyze how the multi-label BCE loss behaves differently from traditional cross-entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data to demonstrate loss behavior\n",
    "batch_size = 4\n",
    "vocab_size = 100\n",
    "\n",
    "# Generate random logits\n",
    "logits = torch.randn(batch_size, vocab_size)\n",
    "\n",
    "# Create multi-hot targets (T-FREE style)\n",
    "targets = torch.zeros(batch_size, vocab_size)\n",
    "for i in range(batch_size):\n",
    "    # Each word activates 3-8 trigrams\n",
    "    num_active = np.random.randint(3, 9)\n",
    "    active_indices = np.random.choice(vocab_size, num_active, replace=False)\n",
    "    targets[i, active_indices] = 1\n",
    "\n",
    "# Calculate losses\n",
    "bce_loss_fn = MultiLabelBCELoss()\n",
    "ce_loss_fn = SingleLabelCELoss()\n",
    "\n",
    "bce_loss = bce_loss_fn(logits, targets)\n",
    "ce_loss = ce_loss_fn(logits, targets)\n",
    "\n",
    "print(f\"Multi-label BCE Loss: {bce_loss.item():.4f}\")\n",
    "print(f\"Single-label CE Loss: {ce_loss.item():.4f}\")\n",
    "\n",
    "# Visualize the target distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(targets.numpy(), cmap='Blues', cbar_kws={'label': 'Activation'})\n",
    "plt.title('Multi-Hot Target Encoding for T-FREE (4 words)', fontsize=14)\n",
    "plt.xlabel('Trigram Hash Index')\n",
    "plt.ylabel('Word Sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Analysis\n",
    "\n",
    "One key advantage of multi-label BCE is how gradients are distributed across multiple active positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAnalyzer:\n",
    "    \"\"\"Analyze gradient behavior for different loss functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 100):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def compute_gradients(self, logits: torch.Tensor, targets: torch.Tensor, \n",
    "                         loss_type: str = 'bce') -> torch.Tensor:\n",
    "        \"\"\"Compute gradients with respect to logits.\"\"\"\n",
    "        logits = logits.clone().requires_grad_(True)\n",
    "        \n",
    "        if loss_type == 'bce':\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        else:  # ce\n",
    "            single_targets = targets.argmax(dim=-1)\n",
    "            loss = F.cross_entropy(logits, single_targets)\n",
    "            \n",
    "        loss.backward()\n",
    "        return logits.grad\n",
    "    \n",
    "    def visualize_gradient_distribution(self):\n",
    "        \"\"\"Compare gradient distributions between BCE and CE losses.\"\"\"\n",
    "        # Create sample data\n",
    "        logits = torch.randn(1, self.vocab_size)\n",
    "        \n",
    "        # Multi-hot target with 5 active positions\n",
    "        targets = torch.zeros(1, self.vocab_size)\n",
    "        active_positions = [10, 25, 40, 55, 70]\n",
    "        targets[0, active_positions] = 1\n",
    "        \n",
    "        # Compute gradients\n",
    "        bce_grads = self.compute_gradients(logits, targets, 'bce')\n",
    "        ce_grads = self.compute_gradients(logits.clone(), targets, 'ce')\n",
    "        \n",
    "        # Visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # BCE gradients\n",
    "        ax1.bar(range(self.vocab_size), bce_grads[0].numpy(), \n",
    "                color='green', alpha=0.7)\n",
    "        ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        for pos in active_positions:\n",
    "            ax1.axvline(x=pos, color='red', linestyle='--', alpha=0.5)\n",
    "        ax1.set_title('Multi-Label BCE Gradients', fontsize=14)\n",
    "        ax1.set_ylabel('Gradient Magnitude')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # CE gradients\n",
    "        ax2.bar(range(self.vocab_size), ce_grads[0].numpy(), \n",
    "                color='blue', alpha=0.7)\n",
    "        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax2.axvline(x=active_positions[0], color='red', linestyle='--', alpha=0.5)\n",
    "        ax2.set_title('Single-Label CE Gradients', fontsize=14)\n",
    "        ax2.set_xlabel('Vocabulary Index')\n",
    "        ax2.set_ylabel('Gradient Magnitude')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return bce_grads, ce_grads\n",
    "\n",
    "# Analyze gradients\n",
    "analyzer = GradientAnalyzer()\n",
    "bce_grads, ce_grads = analyzer.visualize_gradient_distribution()\n",
    "\n",
    "print(\"\\nGradient Statistics:\")\n",
    "print(f\"BCE - Non-zero gradients: {(bce_grads != 0).sum().item()}\")\n",
    "print(f\"CE - Non-zero gradients: {(ce_grads != 0).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Morphological Similarity and Loss\n",
    "\n",
    "A key insight from the T-FREE paper is how multi-label BCE naturally handles morphological similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphologicalLossAnalyzer:\n",
    "    \"\"\"Analyze how multi-label BCE handles morphologically similar words.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def extract_trigrams(self, word: str) -> List[str]:\n",
    "        \"\"\"Extract trigrams from a word (T-FREE style).\"\"\"\n",
    "        padded_word = f\"_{word}_\"\n",
    "        trigrams = []\n",
    "        for i in range(len(padded_word) - 2):\n",
    "            trigrams.append(padded_word[i:i+3])\n",
    "        return trigrams\n",
    "    \n",
    "    def trigrams_to_indices(self, trigrams: List[str]) -> List[int]:\n",
    "        \"\"\"Convert trigrams to vocabulary indices using hash.\"\"\"\n",
    "        indices = []\n",
    "        for trigram in trigrams:\n",
    "            # Simple hash function for demonstration\n",
    "            hash_value = sum(ord(c) for c in trigram)\n",
    "            index = hash_value % self.vocab_size\n",
    "            indices.append(index)\n",
    "        return list(set(indices))  # Remove duplicates\n",
    "    \n",
    "    def create_target_vector(self, word: str) -> torch.Tensor:\n",
    "        \"\"\"Create multi-hot target vector for a word.\"\"\"\n",
    "        trigrams = self.extract_trigrams(word)\n",
    "        indices = self.trigrams_to_indices(trigrams)\n",
    "        \n",
    "        target = torch.zeros(self.vocab_size)\n",
    "        target[indices] = 1\n",
    "        return target\n",
    "    \n",
    "    def analyze_morphological_similarity(self):\n",
    "        \"\"\"Analyze loss behavior for morphologically similar words.\"\"\"\n",
    "        # Word families to analyze\n",
    "        word_families = [\n",
    "            ['run', 'runs', 'running', 'runner'],\n",
    "            ['happy', 'happier', 'happiest', 'happiness'],\n",
    "            ['compute', 'computer', 'computing', 'computation']\n",
    "        ]\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "        \n",
    "        for idx, family in enumerate(word_families):\n",
    "            # Create target vectors\n",
    "            targets = [self.create_target_vector(word) for word in family]\n",
    "            \n",
    "            # Calculate overlap matrix\n",
    "            overlap_matrix = torch.zeros(len(family), len(family))\n",
    "            for i in range(len(family)):\n",
    "                for j in range(len(family)):\n",
    "                    overlap = (targets[i] * targets[j]).sum()\n",
    "                    total = targets[i].sum() + targets[j].sum() - overlap\n",
    "                    overlap_matrix[i, j] = overlap / total if total > 0 else 0\n",
    "            \n",
    "            # Visualize\n",
    "            im = axes[idx].imshow(overlap_matrix.numpy(), cmap='YlOrRd', \n",
    "                                  vmin=0, vmax=1, aspect='auto')\n",
    "            axes[idx].set_xticks(range(len(family)))\n",
    "            axes[idx].set_yticks(range(len(family)))\n",
    "            axes[idx].set_xticklabels(family, rotation=45)\n",
    "            axes[idx].set_yticklabels(family)\n",
    "            axes[idx].set_title(f'Trigram Overlap: {\" → \".join(family)}', \n",
    "                               fontsize=12)\n",
    "            \n",
    "            # Add text annotations\n",
    "            for i in range(len(family)):\n",
    "                for j in range(len(family)):\n",
    "                    axes[idx].text(j, i, f'{overlap_matrix[i, j]:.2f}',\n",
    "                                  ha='center', va='center')\n",
    "            \n",
    "            # Colorbar\n",
    "            plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return word_families\n",
    "\n",
    "# Analyze morphological similarities\n",
    "morph_analyzer = MorphologicalLossAnalyzer()\n",
    "word_families = morph_analyzer.analyze_morphological_similarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Implementation Considerations\n",
    "\n",
    "Let's explore practical aspects of implementing multi-label BCE loss in a T-FREE model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFreeLanguageHead(nn.Module):\n",
    "    \"\"\"\n",
    "    T-FREE language modeling head with multi-label BCE loss.\n",
    "    \n",
    "    This demonstrates the practical implementation as described in the paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, vocab_size: int, \n",
    "                 label_smoothing: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Projection layer\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = MultiLabelBCELoss(\n",
    "            reduction='mean', \n",
    "            label_smoothing=label_smoothing\n",
    "        )\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, \n",
    "                targets: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, hidden_size]\n",
    "            targets: [batch_size, seq_len, vocab_size] multi-hot encoded\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with logits and optional loss\n",
    "        \"\"\"\n",
    "        # Project to vocabulary size\n",
    "        logits = self.proj(hidden_states)\n",
    "        \n",
    "        output = {'logits': logits}\n",
    "        \n",
    "        if targets is not None:\n",
    "            # Reshape for loss calculation\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits_flat = logits.view(-1, vocab_size)\n",
    "            targets_flat = targets.view(-1, vocab_size)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(logits_flat, targets_flat)\n",
    "            output['loss'] = loss\n",
    "            \n",
    "            # Calculate accuracy metrics\n",
    "            with torch.no_grad():\n",
    "                predictions = torch.sigmoid(logits_flat) > 0.5\n",
    "                correct = (predictions == targets_flat).float()\n",
    "                \n",
    "                # Per-position accuracy\n",
    "                accuracy = correct.mean()\n",
    "                \n",
    "                # Exact match accuracy (all trigrams correct)\n",
    "                exact_match = (correct.sum(dim=1) == vocab_size).float().mean()\n",
    "                \n",
    "                output['accuracy'] = accuracy\n",
    "                output['exact_match'] = exact_match\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Demonstrate usage\n",
    "hidden_size = 768\n",
    "vocab_size = 8000  # T-FREE uses smaller vocabulary\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Create model head\n",
    "lm_head = TFreeLanguageHead(hidden_size, vocab_size)\n",
    "\n",
    "# Mock hidden states from transformer\n",
    "hidden_states = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Create multi-hot targets\n",
    "targets = torch.zeros(batch_size, seq_len, vocab_size)\n",
    "for b in range(batch_size):\n",
    "    for s in range(seq_len):\n",
    "        # Each position activates 5-10 trigrams\n",
    "        num_active = np.random.randint(5, 11)\n",
    "        active_indices = np.random.choice(vocab_size, num_active, replace=False)\n",
    "        targets[b, s, active_indices] = 1\n",
    "\n",
    "# Forward pass\n",
    "output = lm_head(hidden_states, targets)\n",
    "\n",
    "print(\"T-FREE Language Head Output:\")\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "print(f\"Accuracy: {output['accuracy'].item():.4f}\")\n",
    "print(f\"Exact Match: {output['exact_match'].item():.4f}\")\n",
    "print(f\"\\nLogits shape: {output['logits'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Landscape Visualization\n",
    "\n",
    "Let's visualize how the loss landscape differs between single-label and multi-label approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_landscape():\n",
    "    \"\"\"Compare loss landscapes for single vs multi-label approaches.\"\"\"\n",
    "    \n",
    "    # Create a simple 2D parameter space\n",
    "    param_range = np.linspace(-2, 2, 50)\n",
    "    X, Y = np.meshgrid(param_range, param_range)\n",
    "    \n",
    "    # Fixed target (multi-hot)\n",
    "    vocab_size = 10\n",
    "    target = torch.zeros(1, vocab_size)\n",
    "    target[0, [1, 3, 5, 7]] = 1\n",
    "    \n",
    "    # Calculate losses for different parameter values\n",
    "    bce_losses = np.zeros_like(X)\n",
    "    ce_losses = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            # Create logits based on parameters\n",
    "            logits = torch.zeros(1, vocab_size)\n",
    "            logits[0, [1, 3, 5, 7]] = torch.tensor([X[i, j], Y[i, j], \n",
    "                                                    -X[i, j], -Y[i, j]])\n",
    "            \n",
    "            # BCE loss\n",
    "            bce_loss = F.binary_cross_entropy_with_logits(logits, target)\n",
    "            bce_losses[i, j] = bce_loss.item()\n",
    "            \n",
    "            # CE loss (using first active position as target)\n",
    "            ce_loss = F.cross_entropy(logits, torch.tensor([1]))\n",
    "            ce_losses[i, j] = ce_loss.item()\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # BCE loss landscape\n",
    "    contour1 = ax1.contourf(X, Y, bce_losses, levels=20, cmap='viridis')\n",
    "    ax1.set_title('Multi-Label BCE Loss Landscape', fontsize=14)\n",
    "    ax1.set_xlabel('Parameter 1')\n",
    "    ax1.set_ylabel('Parameter 2')\n",
    "    plt.colorbar(contour1, ax=ax1, label='Loss')\n",
    "    \n",
    "    # CE loss landscape\n",
    "    contour2 = ax2.contourf(X, Y, ce_losses, levels=20, cmap='plasma')\n",
    "    ax2.set_title('Single-Label CE Loss Landscape', fontsize=14)\n",
    "    ax2.set_xlabel('Parameter 1')\n",
    "    ax2.set_ylabel('Parameter 2')\n",
    "    plt.colorbar(contour2, ax=ax2, label='Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"BCE Loss - Min: {bce_losses.min():.4f}, Max: {bce_losses.max():.4f}\")\n",
    "    print(f\"CE Loss - Min: {ce_losses.min():.4f}, Max: {ce_losses.max():.4f}\")\n",
    "\n",
    "visualize_loss_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Implications\n",
    "\n",
    "The paper mentions significant parameter reduction. Let's analyze the computational implications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceAnalyzer:\n",
    "    \"\"\"Analyze computational performance of multi-label BCE.\"\"\"\n",
    "    \n",
    "    def compare_memory_usage(self):\n",
    "        \"\"\"Compare memory usage between approaches.\"\"\"\n",
    "        # Model configurations\n",
    "        traditional_vocab = 128000  # Llama-3 vocabulary\n",
    "        tfree_vocab = 8000  # T-FREE vocabulary\n",
    "        hidden_size = 4096\n",
    "        \n",
    "        # Calculate parameter counts\n",
    "        traditional_params = traditional_vocab * hidden_size\n",
    "        tfree_params = tfree_vocab * hidden_size\n",
    "        \n",
    "        # Memory in MB (assuming float32)\n",
    "        traditional_memory = (traditional_params * 4) / (1024 ** 2)\n",
    "        tfree_memory = (tfree_params * 4) / (1024 ** 2)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Parameter comparison\n",
    "        models = ['Traditional\\n(128k vocab)', 'T-FREE\\n(8k vocab)']\n",
    "        params = [traditional_params / 1e6, tfree_params / 1e6]\n",
    "        \n",
    "        bars1 = ax1.bar(models, params, color=['blue', 'green'], alpha=0.7)\n",
    "        ax1.set_ylabel('Parameters (Millions)')\n",
    "        ax1.set_title('Head Layer Parameter Count', fontsize=14)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, param in zip(bars1, params):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                    f'{param:.1f}M', ha='center', va='bottom')\n",
    "        \n",
    "        # Memory comparison\n",
    "        memory = [traditional_memory, tfree_memory]\n",
    "        bars2 = ax2.bar(models, memory, color=['blue', 'green'], alpha=0.7)\n",
    "        ax2.set_ylabel('Memory (MB)')\n",
    "        ax2.set_title('Head Layer Memory Usage', fontsize=14)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, mem in zip(bars2, memory):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "                    f'{mem:.1f} MB', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate reduction\n",
    "        param_reduction = (1 - tfree_params / traditional_params) * 100\n",
    "        print(f\"\\nParameter Reduction: {param_reduction:.1f}%\")\n",
    "        print(f\"Memory Savings: {traditional_memory - tfree_memory:.1f} MB\")\n",
    "        \n",
    "        return param_reduction\n",
    "\n",
    "# Analyze performance\n",
    "perf_analyzer = PerformanceAnalyzer()\n",
    "reduction = perf_analyzer.compare_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Multi-Label Techniques\n",
    "\n",
    "Let's explore advanced techniques for multi-label learning in T-FREE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMultiLabelBCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced multi-label BCE with focal loss and class weighting.\n",
    "    \n",
    "    These techniques can help with imbalanced trigram distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, \n",
    "                 pos_weight: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Focal loss for multi-label classification.\n",
    "        \n",
    "        Focal loss helps focus on hard examples by down-weighting\n",
    "        easy examples.\n",
    "        \"\"\"\n",
    "        # Calculate probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Basic BCE loss\n",
    "        if self.pos_weight is not None:\n",
    "            bce_loss = F.binary_cross_entropy_with_logits(\n",
    "                logits, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "            )\n",
    "        else:\n",
    "            bce_loss = F.binary_cross_entropy_with_logits(\n",
    "                logits, targets, reduction='none'\n",
    "            )\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
    "            focal_weight = alpha_t * focal_weight\n",
    "        \n",
    "        # Combine\n",
    "        focal_loss = focal_weight * bce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "# Demonstrate advanced loss behavior\n",
    "def demonstrate_focal_loss():\n",
    "    \"\"\"Show how focal loss affects training dynamics.\"\"\"\n",
    "    vocab_size = 100\n",
    "    batch_size = 4\n",
    "    \n",
    "    # Create imbalanced targets (some trigrams are rare)\n",
    "    targets = torch.zeros(batch_size, vocab_size)\n",
    "    # Common trigrams\n",
    "    targets[:, :20] = torch.bernoulli(torch.ones(batch_size, 20) * 0.7)\n",
    "    # Rare trigrams\n",
    "    targets[:, 80:] = torch.bernoulli(torch.ones(batch_size, 20) * 0.1)\n",
    "    \n",
    "    # Random logits\n",
    "    logits = torch.randn(batch_size, vocab_size)\n",
    "    \n",
    "    # Calculate different losses\n",
    "    standard_bce = MultiLabelBCELoss()\n",
    "    focal_bce = AdvancedMultiLabelBCE(alpha=0.25, gamma=2.0)\n",
    "    \n",
    "    standard_loss = standard_bce(logits, targets)\n",
    "    focal_loss = focal_bce(logits, targets)\n",
    "    \n",
    "    print(\"Loss Comparison:\")\n",
    "    print(f\"Standard BCE: {standard_loss.item():.4f}\")\n",
    "    print(f\"Focal BCE: {focal_loss.item():.4f}\")\n",
    "    \n",
    "    # Visualize per-position losses\n",
    "    with torch.no_grad():\n",
    "        standard_losses = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction='none'\n",
    "        ).mean(dim=0)\n",
    "        \n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal_weight = (1 - pt) ** 2.0\n",
    "        focal_losses = (focal_weight * F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction='none'\n",
    "        )).mean(dim=0)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    x = np.arange(vocab_size)\n",
    "    \n",
    "    plt.bar(x - 0.2, standard_losses.numpy(), width=0.4, \n",
    "            label='Standard BCE', alpha=0.7)\n",
    "    plt.bar(x + 0.2, focal_losses.numpy(), width=0.4, \n",
    "            label='Focal BCE', alpha=0.7)\n",
    "    \n",
    "    plt.axvline(x=20, color='red', linestyle='--', alpha=0.5, \n",
    "                label='Common/Rare boundary')\n",
    "    plt.axvline(x=80, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('Vocabulary Index')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Per-Position Loss: Standard vs Focal BCE', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_focal_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "### 1. **Multi-Label Nature of T-FREE**\n",
    "- Each word activates multiple trigrams simultaneously\n",
    "- Binary Cross-Entropy (BCE) naturally handles this multi-hot encoding\n",
    "- Traditional Cross-Entropy would lose information by forcing single-label prediction\n",
    "\n",
    "### 2. **Gradient Distribution Benefits**\n",
    "- BCE distributes gradients across all active trigrams\n",
    "- This leads to more stable training dynamics\n",
    "- Morphologically similar words share gradients through common trigrams\n",
    "\n",
    "### 3. **Memory Efficiency**\n",
    "- Vocabulary reduction from 128k to 8k tokens\n",
    "- Parameter reduction of >85% in embedding/head layers\n",
    "- Maintains performance through sparse representations\n",
    "\n",
    "### 4. **Morphological Awareness**\n",
    "- Words with similar spellings naturally share trigrams\n",
    "- This creates implicit morphological understanding\n",
    "- No need to learn separate embeddings for each word variant\n",
    "\n",
    "### 5. **Practical Considerations**\n",
    "- Label smoothing can help with overfitting\n",
    "- Focal loss variants can handle imbalanced trigram distributions\n",
    "- Exact match accuracy is a useful metric alongside per-position accuracy\n",
    "\n",
    "## References\n",
    "\n",
    "Deiseroth, B., Brack, M., Schramowski, P., Kersting, K., & Weinbach, S. (2025). T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings. *arXiv preprint arXiv:2406.19223v2*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}