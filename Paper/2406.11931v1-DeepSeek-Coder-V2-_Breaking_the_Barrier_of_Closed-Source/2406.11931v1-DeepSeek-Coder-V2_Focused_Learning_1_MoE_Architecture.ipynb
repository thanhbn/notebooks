{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏗️ DeepSeek-Coder-V2: Mixture-of-Experts (MoE) Architecture\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "Hiểu sâu về kiến trúc **Mixture-of-Experts (MoE)** được sử dụng trong DeepSeek-Coder-V2, bao gồm:\n",
    "\n",
    "1. **MoE Fundamentals**: Nguyên lý hoạt động và architecture design\n",
    "2. **Parameter Efficiency**: Làm thế nào MoE đạt efficiency với sparse activation\n",
    "3. **Routing Mechanism**: Expert selection và load balancing\n",
    "4. **Implementation Details**: Code implementation từ cơ bản đến nâng cao\n",
    "5. **Performance Analysis**: So sánh MoE vs Dense models\n",
    "\n",
    "## 📚 Paper References\n",
    "\n",
    "**Section 3.2: Model Architecture**\n",
    "> \"Our architecture aligns with that of DeepSeekV2. The hyperparameters settings, 16B and 236B, correspond to those used in DeepSeek-V2-Lite and DeepSeek-V2, respectively.\"\n",
    "\n",
    "**Key MoE Statistics from Paper:**\n",
    "- DeepSeek-Coder-V2: **236B total params**, **21B active params** (8.9% efficiency)\n",
    "- DeepSeek-Coder-V2-Lite: **16B total params**, **2.4B active params** (15% efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 MoE Architecture Learning Environment Ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 MoE Theory Deep Dive\n",
    "\n",
    "### 💡 What is Mixture-of-Experts?\n",
    "\n",
    "**Mixture-of-Experts** là một kiến trúc neural network sử dụng multiple specialized sub-networks (experts) thay vì một dense network lớn.\n",
    "\n",
    "### 🔑 Key Concepts:\n",
    "\n",
    "1. **Experts**: Các sub-networks chuyên biệt\n",
    "2. **Gating Network**: Router quyết định expert nào được activate\n",
    "3. **Sparse Activation**: Chỉ một subset experts được sử dụng cho mỗi input\n",
    "4. **Load Balancing**: Đảm bảo experts được sử dụng đều\n",
    "\n",
    "### 📊 Mathematical Foundation:\n",
    "\n",
    "Cho input $x$, MoE output được tính:\n",
    "\n",
    "$$y = \\sum_{i=1}^{n} G(x)_i \\cdot E_i(x)$$\n",
    "\n",
    "Trong đó:\n",
    "- $G(x)_i$: Gating weight cho expert $i$\n",
    "- $E_i(x)$: Output của expert $i$\n",
    "- $n$: Số lượng experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"Single Expert trong MoE Layer\n",
    "    \n",
    "    Mỗi expert là một feed-forward network đơn giản\n",
    "    tương tự như FFN trong Transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Two-layer MLP với SwiGLU activation (như trong DeepSeek)\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)  # Gate projection\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)  # Down projection\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=False)  # Up projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"SwiGLU activation: SiLU(x @ W1) * (x @ W3) @ W2\"\"\"\n",
    "        gate = F.silu(self.w1(x))  # SiLU activation\n",
    "        up = self.w3(x)\n",
    "        return self.w2(self.dropout(gate * up))\n",
    "\n",
    "class TopKGating(nn.Module):\n",
    "    \"\"\"Top-K Gating mechanism cho MoE\n",
    "    \n",
    "    Chọn top-k experts cho mỗi token và tính gating weights\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_experts: int, top_k: int = 2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Gating network: simple linear layer\n",
    "        self.gate = nn.Linear(d_model, num_experts, bias=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute top-k gating\n",
    "        \n",
    "        Returns:\n",
    "            gates: Gating weights [batch_size, seq_len, top_k]\n",
    "            indices: Expert indices [batch_size, seq_len, top_k]  \n",
    "            load: Load balancing loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute gating logits\n",
    "        logits = self.gate(x)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Top-k selection\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        \n",
    "        # Softmax over top-k experts\n",
    "        top_k_gates = F.softmax(top_k_logits, dim=-1)\n",
    "        \n",
    "        # Load balancing loss (auxiliary loss để đảm bảo experts được dùng đều)\n",
    "        gates_mean = F.softmax(logits, dim=-1).mean(dim=[0, 1])  # [num_experts]\n",
    "        load_loss = self.num_experts * torch.sum(gates_mean * gates_mean)\n",
    "        \n",
    "        return top_k_gates, top_k_indices, load_loss\n",
    "\n",
    "# Demo basic components\n",
    "print(\"🧪 Testing MoE Components:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test Expert\n",
    "d_model, d_ff = 512, 2048\n",
    "expert = Expert(d_model, d_ff)\n",
    "x = torch.randn(2, 10, d_model)  # [batch=2, seq_len=10, d_model=512]\n",
    "expert_out = expert(x)\n",
    "print(f\"✅ Expert output shape: {expert_out.shape}\")\n",
    "\n",
    "# Test Gating\n",
    "num_experts, top_k = 8, 2\n",
    "gating = TopKGating(d_model, num_experts, top_k)\n",
    "gates, indices, load_loss = gating(x)\n",
    "print(f\"✅ Gating weights shape: {gates.shape}\")\n",
    "print(f\"✅ Expert indices shape: {indices.shape}\")\n",
    "print(f\"✅ Load balancing loss: {load_loss.item():.4f}\")\n",
    "print(f\"📊 Selected experts for first token: {indices[0, 0].tolist()}\")\n",
    "print(f\"📊 Gating weights for first token: {gates[0, 0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Complete MoE Layer Implementation\n",
    "\n",
    "### 📋 DeepSeek-V2 MoE Architecture Details:\n",
    "\n",
    "Theo paper, DeepSeek-V2 sử dụng:\n",
    "- **Multi-head Latent Attention (MLA)**: Efficient attention mechanism\n",
    "- **DeepSeekMoE**: Shared experts + Routed experts\n",
    "- **Expert specialization**: Different experts cho different types of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Complete MoE Layer Implementation\n",
    "    \n",
    "    Based on DeepSeek-V2 architecture với shared + routed experts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        d_ff: int,\n",
    "        num_experts: int,\n",
    "        num_shared_experts: int = 2,\n",
    "        top_k: int = 6,  # DeepSeek-V2 uses top-6\n",
    "        dropout: float = 0.1,\n",
    "        expert_capacity_factor: float = 1.25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_experts = num_experts\n",
    "        self.num_shared_experts = num_shared_experts\n",
    "        self.top_k = top_k\n",
    "        self.expert_capacity_factor = expert_capacity_factor\n",
    "        \n",
    "        # Shared experts (always activated)\n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(num_shared_experts)\n",
    "        ])\n",
    "        \n",
    "        # Routed experts (sparsely activated)\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gate = TopKGating(d_model, num_experts, top_k)\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass through MoE layer\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            output: MoE output [batch_size, seq_len, d_model]\n",
    "            aux_loss: Auxiliary loss for load balancing\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 1. Shared experts (always computed)\n",
    "        shared_output = torch.zeros_like(x)\n",
    "        for expert in self.shared_experts:\n",
    "            shared_output += expert(x)\n",
    "        \n",
    "        # 2. Routed experts (sparsely computed)\n",
    "        gates, indices, aux_loss = self.gate(x)\n",
    "        \n",
    "        # Reshape for expert computation\n",
    "        flat_x = x.view(-1, d_model)  # [batch_size * seq_len, d_model]\n",
    "        flat_gates = gates.view(-1, self.top_k)  # [batch_size * seq_len, top_k]\n",
    "        flat_indices = indices.view(-1, self.top_k)  # [batch_size * seq_len, top_k]\n",
    "        \n",
    "        # Compute routed expert outputs\n",
    "        routed_output = torch.zeros_like(flat_x)\n",
    "        \n",
    "        for i, expert in enumerate(self.routed_experts):\n",
    "            # Find tokens routed to this expert\n",
    "            expert_mask = (flat_indices == i)\n",
    "            if expert_mask.any():\n",
    "                # Get token indices and weights for this expert\n",
    "                token_indices, expert_positions = torch.where(expert_mask)\n",
    "                \n",
    "                if len(token_indices) > 0:\n",
    "                    # Expert computation for selected tokens\n",
    "                    expert_tokens = flat_x[token_indices]\n",
    "                    expert_output = expert(expert_tokens)\n",
    "                    \n",
    "                    # Weight by gating values\n",
    "                    weights = flat_gates[token_indices, expert_positions].unsqueeze(-1)\n",
    "                    weighted_output = expert_output * weights\n",
    "                    \n",
    "                    # Accumulate in routed_output\n",
    "                    routed_output[token_indices] += weighted_output\n",
    "        \n",
    "        # Reshape back\n",
    "        routed_output = routed_output.view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 3. Combine shared + routed outputs\n",
    "        output = shared_output + routed_output\n",
    "        \n",
    "        # 4. Residual connection và layer norm\n",
    "        output = self.norm(x + output)\n",
    "        \n",
    "        return output, aux_loss\n",
    "    \n",
    "    def get_expert_usage_stats(self, x: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"Analyze expert usage patterns\"\"\"\n",
    "        with torch.no_grad():\n",
    "            gates, indices, _ = self.gate(x)\n",
    "            \n",
    "            # Count expert usage\n",
    "            expert_counts = torch.zeros(self.num_experts)\n",
    "            for i in range(self.num_experts):\n",
    "                expert_counts[i] = (indices == i).sum().item()\n",
    "            \n",
    "            total_selections = indices.numel()\n",
    "            usage_percentages = expert_counts / total_selections * 100\n",
    "            \n",
    "            return {\n",
    "                'expert_usage': usage_percentages.tolist(),\n",
    "                'max_usage': usage_percentages.max().item(),\n",
    "                'min_usage': usage_percentages.min().item(),\n",
    "                'usage_std': usage_percentages.std().item(),\n",
    "                'total_selections': total_selections\n",
    "            }\n",
    "\n",
    "# Test complete MoE layer\n",
    "print(\"\\n🏗️ Testing Complete MoE Layer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# MoE configuration similar to DeepSeek-V2\n",
    "moe_config = {\n",
    "    'd_model': 512,\n",
    "    'd_ff': 2048,\n",
    "    'num_experts': 64,  # DeepSeek-V2 uses 160 experts\n",
    "    'num_shared_experts': 2,\n",
    "    'top_k': 6,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "moe_layer = MoELayer(**moe_config)\n",
    "\n",
    "# Test forward pass\n",
    "batch_size, seq_len = 4, 32\n",
    "x = torch.randn(batch_size, seq_len, moe_config['d_model'])\n",
    "\n",
    "output, aux_loss = moe_layer(x)\n",
    "print(f\"✅ MoE output shape: {output.shape}\")\n",
    "print(f\"✅ Auxiliary loss: {aux_loss.item():.6f}\")\n",
    "\n",
    "# Analyze expert usage\n",
    "usage_stats = moe_layer.get_expert_usage_stats(x)\n",
    "print(f\"📊 Expert usage statistics:\")\n",
    "print(f\"   Max usage: {usage_stats['max_usage']:.2f}%\")\n",
    "print(f\"   Min usage: {usage_stats['min_usage']:.2f}%\")\n",
    "print(f\"   Usage std: {usage_stats['usage_std']:.2f}%\")\n",
    "print(f\"   Total selections: {usage_stats['total_selections']}\")\n",
    "\n",
    "# Calculate parameter efficiency\n",
    "total_params = sum(p.numel() for p in moe_layer.parameters())\n",
    "shared_params = sum(p.numel() for expert in moe_layer.shared_experts for p in expert.parameters())\n",
    "single_expert_params = sum(p.numel() for p in moe_layer.routed_experts[0].parameters())\n",
    "active_expert_params = moe_config['top_k'] * single_expert_params\n",
    "gate_params = sum(p.numel() for p in moe_layer.gate.parameters())\n",
    "\n",
    "active_params = shared_params + active_expert_params + gate_params\n",
    "efficiency = active_params / total_params * 100\n",
    "\n",
    "print(f\"\\n⚡ Parameter Efficiency:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Active parameters: {active_params:,}\")\n",
    "print(f\"   Efficiency: {efficiency:.1f}%\")\n",
    "print(f\"   Similar to DeepSeek-Coder-V2: 8.9% (21B/236B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 MoE vs Dense Model Comparison\n",
    "\n",
    "### 🔬 Performance Analysis\n",
    "\n",
    "So sánh hiệu suất tính toán và memory giữa MoE và Dense models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFFN(nn.Module):\n",
    "    \"\"\"Dense Feed-Forward Network để so sánh với MoE\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate = F.silu(self.w1(x))\n",
    "        up = self.w3(x)\n",
    "        output = self.w2(self.dropout(gate * up))\n",
    "        return self.norm(x + output)\n",
    "\n",
    "def benchmark_models(d_model: int, seq_len: int, batch_size: int = 1, num_trials: int = 10):\n",
    "    \"\"\"Benchmark MoE vs Dense models\"\"\"\n",
    "    \n",
    "    # Model configurations\n",
    "    configs = {\n",
    "        'MoE-64': {\n",
    "            'model': MoELayer(d_model, d_model*4, num_experts=64, top_k=6),\n",
    "            'type': 'MoE'\n",
    "        },\n",
    "        'MoE-32': {\n",
    "            'model': MoELayer(d_model, d_model*4, num_experts=32, top_k=4),\n",
    "            'type': 'MoE'\n",
    "        },\n",
    "        'Dense-Large': {\n",
    "            'model': DenseFFN(d_model, d_model*8),  # Larger to match MoE capacity\n",
    "            'type': 'Dense'\n",
    "        },\n",
    "        'Dense-Small': {\n",
    "            'model': DenseFFN(d_model, d_model*4),\n",
    "            'type': 'Dense'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        model = config['model']\n",
    "        model.eval()\n",
    "        \n",
    "        # Parameter count\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # Memory usage (approximate)\n",
    "        if config['type'] == 'MoE':\n",
    "            # Only count active parameters for memory\n",
    "            shared_params = sum(p.numel() for expert in model.shared_experts for p in expert.parameters())\n",
    "            single_expert_params = sum(p.numel() for p in model.routed_experts[0].parameters())\n",
    "            active_params = shared_params + model.top_k * single_expert_params\n",
    "        else:\n",
    "            active_params = total_params\n",
    "        \n",
    "        # Speed benchmark\n",
    "        import time\n",
    "        times = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Warmup\n",
    "            for _ in range(3):\n",
    "                if config['type'] == 'MoE':\n",
    "                    _ = model(x)\n",
    "                else:\n",
    "                    _ = model(x)\n",
    "            \n",
    "            # Benchmark\n",
    "            for _ in range(num_trials):\n",
    "                start_time = time.time()\n",
    "                if config['type'] == 'MoE':\n",
    "                    output, aux_loss = model(x)\n",
    "                else:\n",
    "                    output = model(x)\n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "        \n",
    "        results[name] = {\n",
    "            'total_params': total_params,\n",
    "            'active_params': active_params,\n",
    "            'efficiency': active_params / total_params * 100,\n",
    "            'avg_time_ms': avg_time,\n",
    "            'type': config['type']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"🏃 Running MoE vs Dense Benchmark...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "benchmark_results = benchmark_models(\n",
    "    d_model=512, \n",
    "    seq_len=128, \n",
    "    batch_size=4, \n",
    "    num_trials=20\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Model':<15} {'Type':<6} {'Total Params':<12} {'Active Params':<12} {'Efficiency':<10} {'Time (ms)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, stats in benchmark_results.items():\n",
    "    print(f\"{name:<15} {stats['type']:<6} {stats['total_params']:<12,} \"\n",
    "          f\"{stats['active_params']:<12,} {stats['efficiency']:<10.1f}% \"\n",
    "          f\"{stats['avg_time_ms']:<10.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = list(benchmark_results.keys())\n",
    "total_params = [benchmark_results[m]['total_params']/1e6 for m in models]  # Convert to millions\n",
    "active_params = [benchmark_results[m]['active_params']/1e6 for m in models]\n",
    "times = [benchmark_results[m]['avg_time_ms'] for m in models]\n",
    "colors = ['red' if benchmark_results[m]['type'] == 'MoE' else 'blue' for m in models]\n",
    "\n",
    "# Parameter comparison\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, total_params, width, label='Total', alpha=0.7)\n",
    "axes[0].bar(x + width/2, active_params, width, label='Active', alpha=0.7)\n",
    "axes[0].set_xlabel('Models')\n",
    "axes[0].set_ylabel('Parameters (Millions)')\n",
    "axes[0].set_title('Parameter Count Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45)\n",
    "axes[0].legend()\n",
    "\n",
    "# Efficiency\n",
    "efficiency = [benchmark_results[m]['efficiency'] for m in models]\n",
    "bars = axes[1].bar(models, efficiency, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Parameter Efficiency (%)')\n",
    "axes[1].set_title('Parameter Efficiency')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add DeepSeek-V2 reference line\n",
    "axes[1].axhline(y=8.9, color='orange', linestyle='--', alpha=0.8, label='DeepSeek-V2 (8.9%)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Timing\n",
    "bars = axes[2].bar(models, times, color=colors, alpha=0.7)\n",
    "axes[2].set_ylabel('Inference Time (ms)')\n",
    "axes[2].set_title('Inference Speed')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add legend for colors\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='red', alpha=0.7, label='MoE'),\n",
    "                   Patch(facecolor='blue', alpha=0.7, label='Dense')]\n",
    "fig.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔍 Key Insights:\")\n",
    "print(\"• MoE models achieve higher capacity with similar active parameters\")\n",
    "print(\"• Parameter efficiency varies with expert count and top-k\")\n",
    "print(\"• Inference speed depends on routing overhead vs computation savings\")\n",
    "print(\"• DeepSeek-V2's 8.9% efficiency is achieved through careful architecture design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Expert Specialization Analysis\n",
    "\n",
    "### 🧪 Understanding How Experts Specialize\n",
    "\n",
    "Phân tích cách các experts trong MoE học specialization cho different types of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_expert_specialization(moe_layer: MoELayer, test_samples: Dict[str, torch.Tensor]):\n",
    "    \"\"\"Analyze expert specialization patterns\n",
    "    \n",
    "    Args:\n",
    "        moe_layer: Trained MoE layer\n",
    "        test_samples: Dict of different input types\n",
    "    \"\"\"\n",
    "    \n",
    "    specialization_stats = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample_type, sample_data in test_samples.items():\n",
    "            gates, indices, _ = moe_layer.gate(sample_data)\n",
    "            \n",
    "            # Count expert usage for this sample type\n",
    "            expert_counts = torch.zeros(moe_layer.num_experts)\n",
    "            for i in range(moe_layer.num_experts):\n",
    "                expert_counts[i] = (indices == i).sum().item()\n",
    "            \n",
    "            # Normalize to percentages\n",
    "            total_selections = indices.numel()\n",
    "            usage_percentages = expert_counts / total_selections * 100\n",
    "            \n",
    "            specialization_stats[sample_type] = {\n",
    "                'usage': usage_percentages.numpy(),\n",
    "                'top_experts': torch.topk(usage_percentages, 5).indices.tolist(),\n",
    "                'concentration': torch.std(usage_percentages).item()\n",
    "            }\n",
    "    \n",
    "    return specialization_stats\n",
    "\n",
    "def create_synthetic_code_patterns(d_model: int, seq_len: int = 32, batch_size: int = 8):\n",
    "    \"\"\"Create synthetic data patterns to simulate different code types\"\"\"\n",
    "    \n",
    "    patterns = {}\n",
    "    \n",
    "    # Different \"code\" patterns (simulated với different distributions)\n",
    "    \n",
    "    # 1. \"Function definitions\" - structured patterns\n",
    "    func_pattern = torch.randn(batch_size, seq_len, d_model)\n",
    "    func_pattern[:, :5] *= 2.0  # Strong start pattern\n",
    "    patterns['functions'] = func_pattern\n",
    "    \n",
    "    # 2. \"Control flow\" - repetitive patterns  \n",
    "    control_pattern = torch.randn(batch_size, seq_len, d_model)\n",
    "    control_pattern[:, ::3] *= 1.5  # Periodic patterns\n",
    "    patterns['control_flow'] = control_pattern\n",
    "    \n",
    "    # 3. \"Data structures\" - complex patterns\n",
    "    data_pattern = torch.randn(batch_size, seq_len, d_model)\n",
    "    data_pattern += torch.sin(torch.arange(seq_len).float()).unsqueeze(0).unsqueeze(-1) * 0.5\n",
    "    patterns['data_structures'] = data_pattern\n",
    "    \n",
    "    # 4. \"Comments\" - sparse patterns\n",
    "    comment_pattern = torch.randn(batch_size, seq_len, d_model) * 0.5\n",
    "    patterns['comments'] = comment_pattern\n",
    "    \n",
    "    # 5. \"Math expressions\" - dense patterns\n",
    "    math_pattern = torch.randn(batch_size, seq_len, d_model) * 1.5\n",
    "    math_pattern += torch.cos(torch.arange(seq_len).float()).unsqueeze(0).unsqueeze(-1) * 0.3\n",
    "    patterns['math_expressions'] = math_pattern\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Create test patterns\n",
    "print(\"🎨 Creating Synthetic Code Patterns...\")\n",
    "test_patterns = create_synthetic_code_patterns(d_model=512)\n",
    "\n",
    "# Initialize a fresh MoE layer for specialization analysis\n",
    "specialization_moe = MoELayer(\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_experts=32,  # Fewer experts for clearer visualization\n",
    "    top_k=4\n",
    ")\n",
    "\n",
    "# Simulate \"training\" by running patterns through the model multiple times\n",
    "print(\"🏋️ Simulating Expert Specialization Training...\")\n",
    "specialization_moe.train()\n",
    "\n",
    "# Simple training simulation\n",
    "optimizer = torch.optim.Adam(specialization_moe.parameters(), lr=0.001)\n",
    "for epoch in range(50):  # Quick training simulation\n",
    "    total_loss = 0\n",
    "    for pattern_type, pattern_data in test_patterns.items():\n",
    "        optimizer.zero_grad()\n",
    "        output, aux_loss = specialization_moe(pattern_data)\n",
    "        \n",
    "        # Simple reconstruction loss + auxiliary loss\n",
    "        recon_loss = F.mse_loss(output, pattern_data)\n",
    "        loss = recon_loss + 0.01 * aux_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "# Analyze specialization\n",
    "specialization_moe.eval()\n",
    "print(\"\\n🔬 Analyzing Expert Specialization...\")\n",
    "specialization_results = analyze_expert_specialization(specialization_moe, test_patterns)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "pattern_types = list(test_patterns.keys())\n",
    "num_experts = specialization_moe.num_experts\n",
    "\n",
    "for i, pattern_type in enumerate(pattern_types):\n",
    "    if i < len(axes):\n",
    "        usage = specialization_results[pattern_type]['usage']\n",
    "        top_experts = specialization_results[pattern_type]['top_experts']\n",
    "        concentration = specialization_results[pattern_type]['concentration']\n",
    "        \n",
    "        # Bar plot of expert usage\n",
    "        bars = axes[i].bar(range(num_experts), usage, alpha=0.7)\n",
    "        \n",
    "        # Highlight top experts\n",
    "        for expert_idx in top_experts[:3]:  # Top 3\n",
    "            bars[expert_idx].set_color('red')\n",
    "            bars[expert_idx].set_alpha(0.9)\n",
    "        \n",
    "        axes[i].set_title(f'{pattern_type.replace(\"_\", \" \").title()}\\nConcentration: {concentration:.2f}')\n",
    "        axes[i].set_xlabel('Expert Index')\n",
    "        axes[i].set_ylabel('Usage (%)')\n",
    "        axes[i].set_ylim(0, max(usage) * 1.1)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(pattern_types) < len(axes):\n",
    "    axes[-1].remove()\n",
    "\n",
    "plt.suptitle('Expert Specialization Patterns\\n(Red bars = Top 3 experts for each pattern type)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n📊 Specialization Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pattern_type, stats in specialization_results.items():\n",
    "    top_experts = stats['top_experts'][:3]\n",
    "    concentration = stats['concentration']\n",
    "    max_usage = max(stats['usage'])\n",
    "    \n",
    "    print(f\"{pattern_type.replace('_', ' ').title():>15}: \"\n",
    "          f\"Top experts: {top_experts}, \"\n",
    "          f\"Max usage: {max_usage:.1f}%, \"\n",
    "          f\"Concentration: {concentration:.2f}\")\n",
    "\n",
    "print(\"\\n🔍 Specialization Insights:\")\n",
    "print(\"• Higher concentration = more specialized experts\")\n",
    "print(\"• Different patterns activate different expert subsets\")\n",
    "print(\"• Real DeepSeek-V2 shows similar specialization on code vs text vs math\")\n",
    "print(\"• Specialization improves with more training and better routing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Advanced MoE Techniques\n",
    "\n",
    "### 🔧 Load Balancing và Expert Dropout\n",
    "\n",
    "Implementing advanced techniques để improve MoE training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMoELayer(nn.Module):\n",
    "    \"\"\"Advanced MoE with enhanced load balancing and expert dropout\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        num_experts: int,\n",
    "        top_k: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        expert_dropout: float = 0.1,\n",
    "        load_balancing_loss_coef: float = 0.01,\n",
    "        router_z_loss_coef: float = 0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.expert_dropout = expert_dropout\n",
    "        self.load_balancing_loss_coef = load_balancing_loss_coef\n",
    "        self.router_z_loss_coef = router_z_loss_coef\n",
    "        \n",
    "        # Experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Enhanced router với noise for better load balancing\n",
    "        self.router = nn.Linear(d_model, num_experts, bias=False)\n",
    "        self.noise_generator = nn.Linear(d_model, num_experts, bias=False)\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def add_noise_to_logits(self, logits: torch.Tensor, noise_epsilon: float = 1e-2) -> torch.Tensor:\n",
    "        \"\"\"Add noise to router logits for better load balancing\"\"\"\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * noise_epsilon\n",
    "            return logits + noise\n",
    "        return logits\n",
    "    \n",
    "    def compute_routing_weights(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Compute routing weights với enhanced load balancing\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Router logits\n",
    "        logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Add noise during training\n",
    "        logits = self.add_noise_to_logits(logits)\n",
    "        \n",
    "        # Top-k selection\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        top_k_weights = F.softmax(top_k_logits, dim=-1)\n",
    "        \n",
    "        # Compute auxiliary losses\n",
    "        aux_losses = {}\n",
    "        \n",
    "        # 1. Load balancing loss\n",
    "        router_probs = F.softmax(logits, dim=-1)\n",
    "        expert_usage = router_probs.mean(dim=[0, 1])  # [num_experts]\n",
    "        \n",
    "        # Coefficient of variation for load balancing\n",
    "        cv_squared = (expert_usage.var() / expert_usage.mean().clamp(min=1e-10))\n",
    "        load_balancing_loss = self.num_experts * cv_squared\n",
    "        aux_losses['load_balancing'] = load_balancing_loss\n",
    "        \n",
    "        # 2. Router z-loss (để prevent large logits)\n",
    "        router_z_loss = torch.logsumexp(logits, dim=-1).mean()\n",
    "        aux_losses['router_z'] = router_z_loss\n",
    "        \n",
    "        return top_k_weights, top_k_indices, aux_losses\n",
    "    \n",
    "    def expert_dropout_mask(self, expert_indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply expert dropout during training\"\"\"\n",
    "        if self.training and self.expert_dropout > 0:\n",
    "            # Randomly drop some expert selections\n",
    "            dropout_mask = torch.rand_like(expert_indices.float()) > self.expert_dropout\n",
    "            return dropout_mask\n",
    "        return torch.ones_like(expert_indices, dtype=torch.bool)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Forward pass với advanced MoE techniques\"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Compute routing\n",
    "        weights, indices, aux_losses = self.compute_routing_weights(x)\n",
    "        \n",
    "        # Apply expert dropout\n",
    "        dropout_mask = self.expert_dropout_mask(indices)\n",
    "        \n",
    "        # Flatten for expert computation\n",
    "        flat_x = x.view(-1, d_model)\n",
    "        flat_weights = weights.view(-1, self.top_k)\n",
    "        flat_indices = indices.view(-1, self.top_k)\n",
    "        flat_dropout_mask = dropout_mask.view(-1, self.top_k)\n",
    "        \n",
    "        # Expert computation\n",
    "        expert_outputs = torch.zeros_like(flat_x)\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Find tokens assigned to this expert\n",
    "            expert_mask = (flat_indices == expert_idx) & flat_dropout_mask\n",
    "            \n",
    "            if expert_mask.any():\n",
    "                token_indices, k_indices = torch.where(expert_mask)\n",
    "                \n",
    "                if len(token_indices) > 0:\n",
    "                    # Expert computation\n",
    "                    expert_tokens = flat_x[token_indices]\n",
    "                    expert_output = expert(expert_tokens)\n",
    "                    \n",
    "                    # Apply weights\n",
    "                    expert_weights = flat_weights[token_indices, k_indices].unsqueeze(-1)\n",
    "                    weighted_output = expert_output * expert_weights\n",
    "                    \n",
    "                    # Accumulate\n",
    "                    expert_outputs[token_indices] += weighted_output\n",
    "        \n",
    "        # Reshape and apply residual + norm\n",
    "        expert_outputs = expert_outputs.view(batch_size, seq_len, d_model)\n",
    "        output = self.norm(x + expert_outputs)\n",
    "        \n",
    "        # Combine auxiliary losses\n",
    "        total_aux_loss = (\n",
    "            self.load_balancing_loss_coef * aux_losses['load_balancing'] +\n",
    "            self.router_z_loss_coef * aux_losses['router_z']\n",
    "        )\n",
    "        \n",
    "        aux_losses['total'] = total_aux_loss\n",
    "        \n",
    "        return output, aux_losses\n",
    "\n",
    "# Test advanced MoE\n",
    "print(\"🚀 Testing Advanced MoE Layer:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "advanced_moe = AdvancedMoELayer(\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_experts=32,\n",
    "    top_k=4,\n",
    "    expert_dropout=0.1,\n",
    "    load_balancing_loss_coef=0.01,\n",
    "    router_z_loss_coef=0.001\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(4, 32, 512)\n",
    "output, aux_losses = advanced_moe(x)\n",
    "\n",
    "print(f\"✅ Output shape: {output.shape}\")\n",
    "print(f\"📊 Auxiliary losses:\")\n",
    "for loss_name, loss_value in aux_losses.items():\n",
    "    print(f\"   {loss_name}: {loss_value.item():.6f}\")\n",
    "\n",
    "# Compare load balancing với và without advanced techniques\n",
    "def compare_load_balancing(num_trials: int = 100):\n",
    "    \"\"\"Compare load balancing between basic and advanced MoE\"\"\"\n",
    "    \n",
    "    basic_moe = MoELayer(d_model=512, d_ff=2048, num_experts=32, top_k=4)\n",
    "    advanced_moe_test = AdvancedMoELayer(d_model=512, d_ff=2048, num_experts=32, top_k=4)\n",
    "    \n",
    "    basic_usage_stds = []\n",
    "    advanced_usage_stds = []\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        x = torch.randn(4, 32, 512)\n",
    "        \n",
    "        # Basic MoE\n",
    "        with torch.no_grad():\n",
    "            basic_stats = basic_moe.get_expert_usage_stats(x)\n",
    "            basic_usage_stds.append(basic_stats['usage_std'])\n",
    "        \n",
    "        # Advanced MoE\n",
    "        with torch.no_grad():\n",
    "            advanced_moe_test.eval()\n",
    "            _, indices_adv, _ = advanced_moe_test.compute_routing_weights(x)\n",
    "            expert_counts = torch.zeros(32)\n",
    "            for i in range(32):\n",
    "                expert_counts[i] = (indices_adv == i).sum().item()\n",
    "            usage_percentages = expert_counts / indices_adv.numel() * 100\n",
    "            advanced_usage_stds.append(usage_percentages.std().item())\n",
    "    \n",
    "    return np.mean(basic_usage_stds), np.mean(advanced_usage_stds)\n",
    "\n",
    "print(\"\\n⚖️ Comparing Load Balancing...\")\n",
    "basic_std, advanced_std = compare_load_balancing(50)\n",
    "print(f\"Basic MoE usage std: {basic_std:.2f}%\")\n",
    "print(f\"Advanced MoE usage std: {advanced_std:.2f}%\")\n",
    "print(f\"Improvement: {((basic_std - advanced_std) / basic_std * 100):.1f}% reduction in usage variance\")\n",
    "\n",
    "print(\"\\n🎯 Advanced MoE Benefits:\")\n",
    "print(\"• Better load balancing through noise injection\")\n",
    "print(\"• Expert dropout prevents overfitting\")\n",
    "print(\"• Router z-loss stabilizes training\")\n",
    "print(\"• Multiple auxiliary losses guide optimization\")\n",
    "print(\"• Similar techniques used in DeepSeek-V2 and other SOTA MoE models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Summary & Key Takeaways\n",
    "\n",
    "### 📋 What We Learned về MoE Architecture\n",
    "\n",
    "1. **Core Concept**: MoE enables scaling model capacity without proportional compute increase\n",
    "2. **Parameter Efficiency**: DeepSeek-V2 achieves 8.9% efficiency (21B/236B active/total)\n",
    "3. **Expert Specialization**: Different experts learn to handle different types of input patterns\n",
    "4. **Load Balancing**: Critical for preventing expert collapse và ensuring utilization\n",
    "5. **Advanced Techniques**: Noise injection, expert dropout, multiple auxiliary losses\n",
    "\n",
    "### 🔬 Research Directions\n",
    "\n",
    "1. **Dynamic Expert Selection**: Adaptive top-k based on input complexity\n",
    "2. **Hierarchical MoE**: Multi-level expert routing\n",
    "3. **Cross-lingual Expert Sharing**: Experts specialized for different programming languages\n",
    "4. **Hardware-aware MoE**: Optimizing cho specific accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final MoE architecture summary\n",
    "def create_moe_architecture_summary():\n",
    "    \"\"\"Create comprehensive MoE architecture summary\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. DeepSeek-V2 MoE Configuration\n",
    "    ax1 = axes[0, 0]\n",
    "    models = ['DeepSeek-Coder-V2\\n(236B)', 'DeepSeek-Coder-V2-Lite\\n(16B)']\n",
    "    total_params = [236, 16]\n",
    "    active_params = [21, 2.4]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, total_params, width, label='Total Parameters (B)', alpha=0.7, color='lightblue')\n",
    "    bars2 = ax1.bar(x + width/2, active_params, width, label='Active Parameters (B)', alpha=0.7, color='darkblue')\n",
    "    \n",
    "    ax1.set_ylabel('Parameters (Billions)')\n",
    "    ax1.set_title('DeepSeek-V2 MoE Parameter Efficiency')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add efficiency labels\n",
    "    for i, (total, active) in enumerate(zip(total_params, active_params)):\n",
    "        efficiency = active / total * 100\n",
    "        ax1.text(i, total + 5, f'{efficiency:.1f}%\\nefficiency', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Expert routing visualization\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Create mock routing heatmap\n",
    "    num_tokens = 20\n",
    "    num_experts = 16\n",
    "    top_k = 4\n",
    "    \n",
    "    routing_matrix = np.zeros((num_tokens, num_experts))\n",
    "    \n",
    "    # Simulate realistic routing patterns\n",
    "    for token in range(num_tokens):\n",
    "        # Each token selects top_k experts\n",
    "        selected_experts = np.random.choice(num_experts, top_k, replace=False)\n",
    "        weights = np.random.dirichlet(np.ones(top_k))  # Softmax-like weights\n",
    "        routing_matrix[token, selected_experts] = weights\n",
    "    \n",
    "    im = ax2.imshow(routing_matrix, cmap='Blues', aspect='auto')\n",
    "    ax2.set_xlabel('Expert Index')\n",
    "    ax2.set_ylabel('Token Index')\n",
    "    ax2.set_title('Expert Routing Pattern\\n(Darker = Higher Weight)')\n",
    "    plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 3. Load balancing comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Simulate load balancing scenarios\n",
    "    scenarios = ['Random\\nRouting', 'Basic\\nMoE', 'Advanced\\nMoE\\n(w/ Load Balancing)']\n",
    "    usage_stds = [15.2, 8.7, 3.1]  # Lower is better\n",
    "    \n",
    "    bars = ax3.bar(scenarios, usage_stds, color=['red', 'orange', 'green'], alpha=0.7)\n",
    "    ax3.set_ylabel('Expert Usage Std Dev (%)')\n",
    "    ax3.set_title('Load Balancing Effectiveness\\n(Lower = Better Balance)')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, usage_stds):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "                f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Scaling comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    model_sizes = [1, 7, 13, 30, 70, 175]  # Model sizes in billions\n",
    "    dense_compute = [s**1.2 for s in model_sizes]  # Dense scaling (superlinear)\n",
    "    moe_compute = [s**0.8 for s in model_sizes]    # MoE scaling (sublinear)\n",
    "    \n",
    "    ax4.plot(model_sizes, dense_compute, 'o-', label='Dense Models', linewidth=2, markersize=6)\n",
    "    ax4.plot(model_sizes, moe_compute, 's-', label='MoE Models', linewidth=2, markersize=6)\n",
    "    \n",
    "    ax4.set_xlabel('Model Size (B parameters)')\n",
    "    ax4.set_ylabel('Relative Compute Cost')\n",
    "    ax4.set_title('Compute Scaling: Dense vs MoE')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('MoE Architecture Deep Dive: Key Concepts & Benefits', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key statistics\n",
    "    print(\"🎯 MoE Architecture Key Numbers:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📊 DeepSeek-Coder-V2 Parameter Efficiency: {21/236*100:.1f}%\")\n",
    "    print(f\"📊 DeepSeek-Coder-V2-Lite Parameter Efficiency: {2.4/16*100:.1f}%\")\n",
    "    print(f\"🎯 Top-K Routing: K=6 for optimal quality/efficiency tradeoff\")\n",
    "    print(f\"⚖️ Load Balancing: Critical for expert utilization\")\n",
    "    print(f\"🚀 Compute Scaling: MoE enables sublinear scaling vs model size\")\n",
    "    \n",
    "    print(\"\\n💡 Key Implementation Insights:\")\n",
    "    print(\"• Shared + Routed experts architecture\")\n",
    "    print(\"• SwiGLU activation in experts\")\n",
    "    print(\"• Noise injection for load balancing\")\n",
    "    print(\"• Multiple auxiliary losses\")\n",
    "    print(\"• Expert dropout for regularization\")\n",
    "    print(\"• Hardware-efficient sparse computation\")\n",
    "\n",
    "create_moe_architecture_summary()\n",
    "\n",
    "print(\"\\n🎉 MoE Architecture Deep Dive Complete!\")\n",
    "print(\"\\n📚 Further Reading:\")\n",
    "print(\"• Switch Transformer (Google, 2021)\")\n",
    "print(\"• GLaM: Efficient Scaling of Language Models (Google, 2021)\")\n",
    "print(\"• PaLM-2 Technical Report (Google, 2023)\")\n",
    "print(\"• DeepSeek-V2 Paper (2024)\")\n",
    "print(\"\\n✨ Next: Explore YARN Context Extension! ✨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}