{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning 1: Evaluation Metrics for Semantic Search\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "- Master the mathematical foundations of nDCG, MRR, and mAP metrics\n",
    "- Understand why these metrics are crucial for semantic search evaluation\n",
    "- Implement metrics from scratch with step-by-step explanations\n",
    "- Analyze how different ranking scenarios affect metric scores\n",
    "\n",
    "## 📚 Paper Context\n",
    "**Section 3.2**: \"Assessing the proficiency and effectiveness of various semantic search methodologies necessitates the use of specific evaluation metrics. Our attention centers on the pivotal metrics: Normalized Discounted Cumulative Gain (nDCG), Mean Reciprocal Rank (MRR), and Mean Average Precision (mAP).\"\n",
    "\n",
    "## 🔬 Why These Metrics Matter\n",
    "Unlike simple accuracy metrics, these evaluation measures account for:\n",
    "- **Ranking Quality**: Position matters in search results\n",
    "- **Graded Relevance**: Documents can be highly relevant, somewhat relevant, or irrelevant\n",
    "- **User Experience**: Earlier results are more valuable to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📊 Evaluation Metrics Learning Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalized Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "### Mathematical Foundation\n",
    "From the paper (Equation 1-2):\n",
    "\n",
    "$$nDCG@k = \\frac{DCG@k}{IDCG@k}$$\n",
    "\n",
    "$$DCG@k = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}$$\n",
    "\n",
    "Where:\n",
    "- `DCG@k`: Discounted Cumulative Gain at rank k\n",
    "- `IDCG@k`: Ideal DCG (maximum possible DCG)\n",
    "- `rel_i`: Graded relevance of result at position i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCGCalculator:\n",
    "    \"\"\"Step-by-step nDCG calculation with detailed explanations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.calculation_steps = []\n",
    "    \n",
    "    def calculate_dcg(self, relevance_scores: List[int], k: int = None, verbose: bool = True) -> float:\n",
    "        \"\"\"Calculate Discounted Cumulative Gain with detailed steps\"\"\"\n",
    "        if k is None:\n",
    "            k = len(relevance_scores)\n",
    "        \n",
    "        k = min(k, len(relevance_scores))\n",
    "        dcg = 0.0\n",
    "        calculation_details = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n🔢 Calculating DCG@{k}:\")\n",
    "            print(\"Position | Relevance | 2^rel - 1 | log₂(pos+1) | Contribution\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        for i in range(k):\n",
    "            position = i + 1\n",
    "            relevance = relevance_scores[i]\n",
    "            \n",
    "            # DCG formula components\n",
    "            numerator = (2 ** relevance) - 1\n",
    "            denominator = np.log2(position + 1)\n",
    "            contribution = numerator / denominator\n",
    "            \n",
    "            dcg += contribution\n",
    "            \n",
    "            calculation_details.append({\n",
    "                'position': position,\n",
    "                'relevance': relevance,\n",
    "                'numerator': numerator,\n",
    "                'denominator': denominator,\n",
    "                'contribution': contribution\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"{position:8d} | {relevance:9d} | {numerator:9.0f} | {denominator:10.3f} | {contribution:12.4f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n📊 Total DCG@{k} = {dcg:.4f}\")\n",
    "        \n",
    "        return dcg, calculation_details\n",
    "    \n",
    "    def calculate_ideal_dcg(self, relevance_scores: List[int], k: int = None, verbose: bool = True) -> float:\n",
    "        \"\"\"Calculate Ideal DCG (best possible ranking)\"\"\"\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n⭐ Ideal ranking: {ideal_scores}\")\n",
    "        \n",
    "        ideal_dcg, _ = self.calculate_dcg(ideal_scores, k, verbose)\n",
    "        return ideal_dcg\n",
    "    \n",
    "    def calculate_ndcg(self, relevance_scores: List[int], k: int = None, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate nDCG with complete breakdown\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n🎯 nDCG Calculation for relevance scores: {relevance_scores}\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        # Calculate actual DCG\n",
    "        dcg, dcg_details = self.calculate_dcg(relevance_scores, k, verbose)\n",
    "        \n",
    "        # Calculate ideal DCG\n",
    "        ideal_dcg = self.calculate_ideal_dcg(relevance_scores, k, verbose)\n",
    "        \n",
    "        # Calculate nDCG\n",
    "        if ideal_dcg == 0:\n",
    "            ndcg = 0.0\n",
    "        else:\n",
    "            ndcg = dcg / ideal_dcg\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n🏆 Final nDCG@{k or len(relevance_scores)} = {dcg:.4f} / {ideal_dcg:.4f} = {ndcg:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'ndcg': ndcg,\n",
    "            'dcg': dcg,\n",
    "            'ideal_dcg': ideal_dcg,\n",
    "            'dcg_details': dcg_details\n",
    "        }\n",
    "\n",
    "# Demonstrate nDCG calculation\n",
    "ndcg_calc = NDCGCalculator()\n",
    "\n",
    "# Example from paper: relevance scores (0=irrelevant, 1=somewhat relevant, 2=very relevant)\n",
    "example_relevance = [2, 1, 0, 1, 2]  # Ranking: very, somewhat, irrelevant, somewhat, very\n",
    "result = ndcg_calc.calculate_ndcg(example_relevance, k=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean Reciprocal Rank (MRR)\n",
    "\n",
    "### Mathematical Foundation\n",
    "From the paper (Equation 3):\n",
    "\n",
    "$$MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$$\n",
    "\n",
    "Where:\n",
    "- `|Q|`: Total number of queries\n",
    "- `rank_i`: Rank position of first very relevant document for query i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRRCalculator:\n",
    "    \"\"\"Mean Reciprocal Rank calculation with detailed analysis\"\"\"\n",
    "    \n",
    "    def find_first_relevant_rank(self, relevance_scores: List[int], \n",
    "                                predictions: List[float], \n",
    "                                min_relevance: int = 2) -> int:\n",
    "        \"\"\"Find rank of first highly relevant document\"\"\"\n",
    "        # Sort by predictions (highest first)\n",
    "        sorted_indices = np.argsort(predictions)[::-1]\n",
    "        \n",
    "        for rank, idx in enumerate(sorted_indices, 1):\n",
    "            if relevance_scores[idx] >= min_relevance:\n",
    "                return rank\n",
    "        \n",
    "        return float('inf')  # No relevant document found\n",
    "    \n",
    "    def calculate_mrr_single_query(self, relevance_scores: List[int], \n",
    "                                  predictions: List[float], \n",
    "                                  verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate MRR for a single query with detailed breakdown\"\"\"\n",
    "        \n",
    "        # Find first relevant rank\n",
    "        first_relevant_rank = self.find_first_relevant_rank(relevance_scores, predictions)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        if first_relevant_rank == float('inf'):\n",
    "            rr = 0.0\n",
    "        else:\n",
    "            rr = 1.0 / first_relevant_rank\n",
    "        \n",
    "        if verbose:\n",
    "            # Show ranking process\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            print(f\"\\n🔍 MRR Calculation:\")\n",
    "            print(\"Rank | Doc Index | Prediction | Relevance | Very Relevant?\")\n",
    "            print(\"-\" * 55)\n",
    "            \n",
    "            for rank, idx in enumerate(sorted_indices, 1):\n",
    "                is_very_relevant = \"✅ YES\" if relevance_scores[idx] >= 2 else \"❌ No\"\n",
    "                print(f\"{rank:4d} | {idx:9d} | {predictions[idx]:10.3f} | {relevance_scores[idx]:9d} | {is_very_relevant}\")\n",
    "                \n",
    "                if relevance_scores[idx] >= 2 and rank == first_relevant_rank:\n",
    "                    print(f\"     ⬆️ First very relevant document found at rank {rank}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\n📊 Reciprocal Rank = 1/{first_relevant_rank} = {rr:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'reciprocal_rank': rr,\n",
    "            'first_relevant_rank': first_relevant_rank,\n",
    "            'sorted_ranking': list(np.argsort(predictions)[::-1])\n",
    "        }\n",
    "    \n",
    "    def calculate_mrr_multiple_queries(self, queries_data: List[Dict], verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate MRR across multiple queries\"\"\"\n",
    "        reciprocal_ranks = []\n",
    "        query_details = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n📋 MRR Calculation for {len(queries_data)} Queries\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        for i, query_data in enumerate(queries_data):\n",
    "            if verbose:\n",
    "                print(f\"\\n🔍 Query {i+1}: {query_data.get('query', f'Query {i+1}')}\")\n",
    "            \n",
    "            result = self.calculate_mrr_single_query(\n",
    "                query_data['relevance'], \n",
    "                query_data['predictions'], \n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            reciprocal_ranks.append(result['reciprocal_rank'])\n",
    "            query_details.append(result)\n",
    "        \n",
    "        mean_rr = np.mean(reciprocal_ranks)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n🏆 Mean Reciprocal Rank = {mean_rr:.4f}\")\n",
    "            print(f\"📊 Individual RRs: {[f'{rr:.3f}' for rr in reciprocal_ranks]}\")\n",
    "        \n",
    "        return {\n",
    "            'mrr': mean_rr,\n",
    "            'reciprocal_ranks': reciprocal_ranks,\n",
    "            'query_details': query_details\n",
    "        }\n",
    "\n",
    "# Demonstrate MRR calculation\n",
    "mrr_calc = MRRCalculator()\n",
    "\n",
    "# Example single query\n",
    "example_relevance = [0, 2, 1, 0, 1]  # Second document is very relevant\n",
    "example_predictions = [0.9, 0.3, 0.7, 0.1, 0.5]  # Our model's confidence scores\n",
    "\n",
    "single_result = mrr_calc.calculate_mrr_single_query(example_relevance, example_predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mean Average Precision (mAP)\n",
    "\n",
    "### Mathematical Foundation\n",
    "From the paper (Equations 4-5):\n",
    "\n",
    "$$AP = \\frac{\\sum_{k=1}^{n} (P(k) \\times rel_k)}{\\text{# relevant documents}}$$\n",
    "\n",
    "$$mAP = \\frac{\\sum_{q=1}^{Q} AP_q}{Q}$$\n",
    "\n",
    "Where:\n",
    "- `P(k)`: Precision at cutoff k\n",
    "- `rel_k`: Relevancy score at rank k\n",
    "- `AP_q`: Average Precision for query q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPCalculator:\n",
    "    \"\"\"Mean Average Precision calculation with comprehensive analysis\"\"\"\n",
    "    \n",
    "    def calculate_precision_at_k(self, relevance_scores: List[int], \n",
    "                                sorted_indices: List[int], \n",
    "                                k: int) -> float:\n",
    "        \"\"\"Calculate precision at cutoff k\"\"\"\n",
    "        relevant_count = 0\n",
    "        \n",
    "        for i in range(min(k, len(sorted_indices))):\n",
    "            idx = sorted_indices[i]\n",
    "            if relevance_scores[idx] > 0:  # Any relevance > 0\n",
    "                relevant_count += 1\n",
    "        \n",
    "        return relevant_count / k if k > 0 else 0.0\n",
    "    \n",
    "    def calculate_ap_single_query(self, relevance_scores: List[int], \n",
    "                                 predictions: List[float], \n",
    "                                 k: int = None, \n",
    "                                 verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate Average Precision for single query\"\"\"\n",
    "        \n",
    "        if k is None:\n",
    "            k = len(relevance_scores)\n",
    "        \n",
    "        # Sort by predictions (highest first)\n",
    "        sorted_indices = list(np.argsort(predictions)[::-1])\n",
    "        total_relevant = sum(1 for rel in relevance_scores if rel > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            return {'ap': 0.0, 'precision_at_k': [], 'calculation_steps': []}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n📊 Average Precision Calculation (k={k}):\")\n",
    "            print(f\"Total relevant documents: {total_relevant}\")\n",
    "            print(\"\\nRank | Doc | Pred | Rel | P@k | Rel? | Contribution\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        precision_sum = 0.0\n",
    "        precision_at_k_values = []\n",
    "        calculation_steps = []\n",
    "        \n",
    "        for rank in range(1, min(k + 1, len(sorted_indices) + 1)):\n",
    "            idx = sorted_indices[rank - 1]\n",
    "            relevance = relevance_scores[idx]\n",
    "            prediction = predictions[idx]\n",
    "            \n",
    "            # Calculate precision at this rank\n",
    "            precision_at_rank = self.calculate_precision_at_k(relevance_scores, sorted_indices, rank)\n",
    "            precision_at_k_values.append(precision_at_rank)\n",
    "            \n",
    "            # Add to precision sum if document is relevant\n",
    "            is_relevant = relevance > 0\n",
    "            contribution = precision_at_rank if is_relevant else 0.0\n",
    "            precision_sum += contribution\n",
    "            \n",
    "            calculation_steps.append({\n",
    "                'rank': rank,\n",
    "                'doc_idx': idx,\n",
    "                'prediction': prediction,\n",
    "                'relevance': relevance,\n",
    "                'precision_at_k': precision_at_rank,\n",
    "                'is_relevant': is_relevant,\n",
    "                'contribution': contribution\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                relevant_symbol = \"✅\" if is_relevant else \"❌\"\n",
    "                print(f\"{rank:4d} | {idx:3d} | {prediction:.2f} | {relevance:3d} | {precision_at_rank:.3f} | {relevant_symbol:2s} | {contribution:.4f}\")\n",
    "        \n",
    "        # Calculate final Average Precision\n",
    "        ap = precision_sum / total_relevant\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n🎯 Average Precision = {precision_sum:.4f} / {total_relevant} = {ap:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'ap': ap,\n",
    "            'precision_at_k': precision_at_k_values,\n",
    "            'calculation_steps': calculation_steps,\n",
    "            'precision_sum': precision_sum,\n",
    "            'total_relevant': total_relevant\n",
    "        }\n",
    "    \n",
    "    def calculate_map_multiple_queries(self, queries_data: List[Dict], \n",
    "                                      k: int = None, \n",
    "                                      verbose: bool = True) -> Dict:\n",
    "        \"\"\"Calculate Mean Average Precision across multiple queries\"\"\"\n",
    "        \n",
    "        ap_scores = []\n",
    "        query_details = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n📋 mAP Calculation for {len(queries_data)} Queries\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        for i, query_data in enumerate(queries_data):\n",
    "            if verbose:\n",
    "                print(f\"\\n🔍 Query {i+1}: {query_data.get('query', f'Query {i+1}')}\")\n",
    "            \n",
    "            result = self.calculate_ap_single_query(\n",
    "                query_data['relevance'], \n",
    "                query_data['predictions'], \n",
    "                k=k, \n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            ap_scores.append(result['ap'])\n",
    "            query_details.append(result)\n",
    "        \n",
    "        mean_ap = np.mean(ap_scores)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n🏆 Mean Average Precision (mAP@{k or 'all'}) = {mean_ap:.4f}\")\n",
    "            print(f\"📊 Individual APs: {[f'{ap:.3f}' for ap in ap_scores]}\")\n",
    "        \n",
    "        return {\n",
    "            'map': mean_ap,\n",
    "            'ap_scores': ap_scores,\n",
    "            'query_details': query_details\n",
    "        }\n",
    "\n",
    "# Demonstrate mAP calculation\n",
    "map_calc = MAPCalculator()\n",
    "\n",
    "# Example with more complex relevance pattern\n",
    "example_relevance = [1, 0, 2, 1, 0]  # Mixed relevance levels\n",
    "example_predictions = [0.8, 0.2, 0.6, 0.9, 0.1]  # Our model's predictions\n",
    "\n",
    "ap_result = map_calc.calculate_ap_single_query(example_relevance, example_predictions, k=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Analysis: Understanding Metric Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ranking_scenarios():\n",
    "    \"\"\"Compare how different ranking scenarios affect all three metrics\"\"\"\n",
    "    \n",
    "    # Define different ranking scenarios\n",
    "    scenarios = {\n",
    "        'Perfect_Ranking': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.9, 0.8, 0.7, 0.6, 0.1],\n",
    "            'description': 'Perfect ranking: highly relevant docs first'\n",
    "        },\n",
    "        'Worst_Ranking': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.1, 0.2, 0.3, 0.4, 0.9],\n",
    "            'description': 'Worst ranking: irrelevant docs first'\n",
    "        },\n",
    "        'Mixed_Ranking': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.7, 0.3, 0.9, 0.1, 0.5],\n",
    "            'description': 'Mixed ranking: some relevant docs early'\n",
    "        },\n",
    "        'Late_Success': {\n",
    "            'relevance': [2, 2, 1, 1, 0],\n",
    "            'predictions': [0.1, 0.2, 0.3, 0.9, 0.8],\n",
    "            'description': 'Late success: relevant docs at end'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"🔄 Comparative Analysis of Ranking Scenarios\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for scenario_name, scenario_data in scenarios.items():\n",
    "        print(f\"\\n📊 Scenario: {scenario_name}\")\n",
    "        print(f\"Description: {scenario_data['description']}\")\n",
    "        print(f\"Relevance: {scenario_data['relevance']}\")\n",
    "        print(f\"Predictions: {scenario_data['predictions']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        ndcg_result = ndcg_calc.calculate_ndcg(scenario_data['relevance'], k=3, verbose=False)\n",
    "        mrr_result = mrr_calc.calculate_mrr_single_query(scenario_data['relevance'], scenario_data['predictions'], verbose=False)\n",
    "        map_result = map_calc.calculate_ap_single_query(scenario_data['relevance'], scenario_data['predictions'], k=3, verbose=False)\n",
    "        \n",
    "        results.append({\n",
    "            'Scenario': scenario_name,\n",
    "            'nDCG@3': ndcg_result['ndcg'],\n",
    "            'MRR': mrr_result['reciprocal_rank'],\n",
    "            'mAP@3': map_result['ap'],\n",
    "            'Description': scenario_data['description']\n",
    "        })\n",
    "        \n",
    "        print(f\"nDCG@3: {ndcg_result['ndcg']:.4f}\")\n",
    "        print(f\"MRR:    {mrr_result['reciprocal_rank']:.4f}\")\n",
    "        print(f\"mAP@3:  {map_result['ap']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run comparative analysis\n",
    "comparison_df = compare_ranking_scenarios()\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metrics = ['nDCG@3', 'MRR', 'mAP@3']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    bars = ax.bar(comparison_df['Scenario'], comparison_df[metric], color=colors[i], alpha=0.8)\n",
    "    ax.set_title(f'{metric} Across Scenarios', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, comparison_df[metric]):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_comparison_scenarios.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Summary Table:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Metric Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_metric_calculator():\n",
    "    \"\"\"Interactive tool to understand how metrics respond to different inputs\"\"\"\n",
    "    \n",
    "    print(\"🎮 Interactive Metric Calculator\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Experiment with different relevance and prediction patterns!\")\n",
    "    print(\"\\nRelevance scale: 0=Irrelevant, 1=Somewhat Relevant, 2=Very Relevant\")\n",
    "    \n",
    "    # Pre-defined experiments\n",
    "    experiments = [\n",
    "        {\n",
    "            'name': 'Early Success',\n",
    "            'relevance': [2, 1, 0, 0, 1],\n",
    "            'predictions': [0.9, 0.8, 0.3, 0.2, 0.7]\n",
    "        },\n",
    "        {\n",
    "            'name': 'Late Discovery', \n",
    "            'relevance': [0, 0, 1, 2, 1],\n",
    "            'predictions': [0.2, 0.3, 0.9, 0.8, 0.7]\n",
    "        },\n",
    "        {\n",
    "            'name': 'Mixed Results',\n",
    "            'relevance': [1, 0, 2, 1, 0],\n",
    "            'predictions': [0.6, 0.8, 0.4, 0.7, 0.5]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for exp in experiments:\n",
    "        print(f\"\\n🧪 Experiment: {exp['name']}\")\n",
    "        print(f\"Relevance:   {exp['relevance']}\")\n",
    "        print(f\"Predictions: {exp['predictions']}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ndcg_res = ndcg_calc.calculate_ndcg(exp['relevance'], k=3, verbose=False)\n",
    "        mrr_res = mrr_calc.calculate_mrr_single_query(exp['relevance'], exp['predictions'], verbose=False)\n",
    "        map_res = map_calc.calculate_ap_single_query(exp['relevance'], exp['predictions'], k=3, verbose=False)\n",
    "        \n",
    "        results_summary.append({\n",
    "            'Experiment': exp['name'],\n",
    "            'nDCG@3': ndcg_res['ndcg'],\n",
    "            'MRR': mrr_res['reciprocal_rank'],\n",
    "            'mAP@3': map_res['ap']\n",
    "        })\n",
    "        \n",
    "        print(f\"📊 Results: nDCG@3={ndcg_res['ndcg']:.3f}, MRR={mrr_res['reciprocal_rank']:.3f}, mAP@3={map_res['ap']:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(results_summary)\n",
    "\n",
    "interactive_results = interactive_metric_calculator()\n",
    "\n",
    "# Create radar chart for metric comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "metrics = ['nDCG@3', 'MRR', 'mAP@3']\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, (_, row) in enumerate(interactive_results.iterrows()):\n",
    "    values = [row['nDCG@3'], row['MRR'], row['mAP@3']]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Experiment'], color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Metric Performance Comparison\\n(Radar Chart)', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Interactive Results Summary:\")\n",
    "print(interactive_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Insights and Learning Summary\n",
    "\n",
    "### 🔍 What We've Learned\n",
    "\n",
    "1. **nDCG (Normalized Discounted Cumulative Gain)**:\n",
    "   - Emphasizes **graded relevance** and **position importance**\n",
    "   - Higher scores for relevant documents at top positions\n",
    "   - Normalizes by ideal ranking for fair comparison\n",
    "   - Best for scenarios with multiple relevance levels\n",
    "\n",
    "2. **MRR (Mean Reciprocal Rank)**:\n",
    "   - Focuses on **first highly relevant result**\n",
    "   - Critical for applications where users need quick answers\n",
    "   - Sensitive to top-ranked results\n",
    "   - Ideal for question-answering systems\n",
    "\n",
    "3. **mAP (Mean Average Precision)**:\n",
    "   - Considers **all relevant documents**\n",
    "   - Balances precision across different cutoff points\n",
    "   - Good for comprehensive retrieval evaluation\n",
    "   - Useful when recall matters as much as precision\n",
    "\n",
    "### 🎯 Why These Metrics Matter for Arabic Semantic Search\n",
    "\n",
    "As highlighted in the paper, Arabic language presents unique challenges:\n",
    "- **Complex morphology**: Same root can have many forms\n",
    "- **Dialectal variations**: Different regions use different expressions\n",
    "- **Limited resources**: Fewer labeled datasets available\n",
    "\n",
    "These metrics help evaluate how well semantic search systems handle these challenges by providing nuanced performance measures that go beyond simple accuracy.\n",
    "\n",
    "### 🚀 Practical Applications\n",
    "\n",
    "- **Customer Support**: MRR critical for quick problem resolution\n",
    "- **Document Search**: mAP important for comprehensive retrieval\n",
    "- **Question Answering**: nDCG valuable for ranked answer lists\n",
    "- **RAG Systems**: All three metrics crucial for retrieval quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Metric sensitivity analysis\n",
    "position_effect = []\n",
    "for pos in range(1, 6):\n",
    "    # Move a very relevant document to different positions\n",
    "    relevance = [0] * 5\n",
    "    relevance[pos-1] = 2\n",
    "    predictions = [0.1] * 5\n",
    "    predictions[pos-1] = 0.9\n",
    "    \n",
    "    ndcg_res = ndcg_calc.calculate_ndcg(relevance, k=5, verbose=False)\n",
    "    mrr_res = mrr_calc.calculate_mrr_single_query(relevance, predictions, verbose=False)\n",
    "    map_res = map_calc.calculate_ap_single_query(relevance, predictions, k=5, verbose=False)\n",
    "    \n",
    "    position_effect.append({\n",
    "        'Position': pos,\n",
    "        'nDCG@5': ndcg_res['ndcg'],\n",
    "        'MRR': mrr_res['reciprocal_rank'],\n",
    "        'mAP@5': map_res['ap']\n",
    "    })\n",
    "\n",
    "pos_df = pd.DataFrame(position_effect)\n",
    "\n",
    "ax1.plot(pos_df['Position'], pos_df['nDCG@5'], 'o-', label='nDCG@5', linewidth=2, markersize=8)\n",
    "ax1.plot(pos_df['Position'], pos_df['MRR'], 's-', label='MRR', linewidth=2, markersize=8)\n",
    "ax1.plot(pos_df['Position'], pos_df['mAP@5'], '^-', label='mAP@5', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Position of Relevant Document')\n",
    "ax1.set_ylabel('Metric Score')\n",
    "ax1.set_title('Position Sensitivity Analysis')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Relevance level impact\n",
    "relevance_levels = [0, 1, 2]\n",
    "relevance_impact = []\n",
    "\n",
    "for rel_level in relevance_levels:\n",
    "    relevance = [rel_level, 0, 0, 0, 0]\n",
    "    predictions = [0.9, 0.1, 0.1, 0.1, 0.1]\n",
    "    \n",
    "    ndcg_res = ndcg_calc.calculate_ndcg(relevance, k=3, verbose=False)\n",
    "    mrr_res = mrr_calc.calculate_mrr_single_query(relevance, predictions, verbose=False)\n",
    "    map_res = map_calc.calculate_ap_single_query(relevance, predictions, k=3, verbose=False)\n",
    "    \n",
    "    relevance_impact.append({\n",
    "        'Relevance Level': rel_level,\n",
    "        'nDCG@3': ndcg_res['ndcg'],\n",
    "        'MRR': mrr_res['reciprocal_rank'],\n",
    "        'mAP@3': map_res['ap']\n",
    "    })\n",
    "\n",
    "rel_df = pd.DataFrame(relevance_impact)\n",
    "\n",
    "width = 0.25\n",
    "x = np.arange(len(relevance_levels))\n",
    "ax2.bar(x - width, rel_df['nDCG@3'], width, label='nDCG@3', alpha=0.8)\n",
    "ax2.bar(x, rel_df['MRR'], width, label='MRR', alpha=0.8)\n",
    "ax2.bar(x + width, rel_df['mAP@3'], width, label='mAP@3', alpha=0.8)\n",
    "ax2.set_xlabel('Relevance Level')\n",
    "ax2.set_ylabel('Metric Score')\n",
    "ax2.set_title('Impact of Relevance Levels')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['Irrelevant (0)', 'Somewhat (1)', 'Very (2)'])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cutoff sensitivity (k parameter)\n",
    "k_values = range(1, 6)\n",
    "relevance = [2, 1, 0, 1, 2]\n",
    "predictions = [0.9, 0.7, 0.3, 0.6, 0.8]\n",
    "\n",
    "k_sensitivity = []\n",
    "for k in k_values:\n",
    "    ndcg_res = ndcg_calc.calculate_ndcg(relevance, k=k, verbose=False)\n",
    "    map_res = map_calc.calculate_ap_single_query(relevance, predictions, k=k, verbose=False)\n",
    "    \n",
    "    k_sensitivity.append({\n",
    "        'k': k,\n",
    "        'nDCG@k': ndcg_res['ndcg'],\n",
    "        'mAP@k': map_res['ap']\n",
    "    })\n",
    "\n",
    "k_df = pd.DataFrame(k_sensitivity)\n",
    "\n",
    "ax3.plot(k_df['k'], k_df['nDCG@k'], 'o-', label='nDCG@k', linewidth=2, markersize=8)\n",
    "ax3.plot(k_df['k'], k_df['mAP@k'], 's-', label='mAP@k', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Cutoff k')\n",
    "ax3.set_ylabel('Metric Score')\n",
    "ax3.set_title('Cutoff Sensitivity Analysis')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Summary insights\n",
    "ax4.axis('off')\n",
    "insights_text = \"\"\"\n",
    "📊 KEY INSIGHTS FROM ANALYSIS\n",
    "\n",
    "🎯 Position Matters:\n",
    "• MRR most sensitive to top positions\n",
    "• nDCG gradually decreases with position\n",
    "• mAP considers all relevant documents\n",
    "\n",
    "📈 Relevance Levels:\n",
    "• nDCG best captures graded relevance\n",
    "• MRR binary: relevant vs irrelevant\n",
    "• mAP treats all relevant docs equally\n",
    "\n",
    "⚙️ Cutoff Effects:\n",
    "• Lower k values favor precision\n",
    "• Higher k values include more context\n",
    "• Choice depends on application needs\n",
    "\n",
    "🔍 For Arabic Semantic Search:\n",
    "• Use all three metrics together\n",
    "• nDCG for ranking quality\n",
    "• MRR for user experience\n",
    "• mAP for comprehensive evaluation\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, insights_text, transform=ax4.transAxes, fontsize=11, \n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comprehensive_metrics_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎓 EVALUATION METRICS MASTERY COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "✅ What you've mastered:\n",
    "• Deep understanding of nDCG, MRR, and mAP calculations\n",
    "• Implementation of metrics from mathematical foundations\n",
    "• Analysis of metric behaviors in different scenarios\n",
    "• Practical insights for Arabic semantic search evaluation\n",
    "• Interactive exploration of metric sensitivities\n",
    "\n",
    "🚀 Ready to apply these metrics to real-world semantic search systems!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Papers (PDF Utils)",
   "language": "python",
   "name": "ai-papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}