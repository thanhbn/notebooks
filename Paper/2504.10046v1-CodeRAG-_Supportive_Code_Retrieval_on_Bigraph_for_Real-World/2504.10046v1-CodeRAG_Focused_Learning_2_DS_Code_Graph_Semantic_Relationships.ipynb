{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeRAG Focused Learning 2: DS-Code Graph with Semantic Relationships\n",
    "\n",
    "**M·ª•c ti√™u**: Hi·ªÉu s√¢u v·ªÅ DS-Code Graph v√† c√°ch x√¢y d·ª±ng semantic relationships gi·ªØa c√°c code elements\n",
    "\n",
    "**Paper Reference**: Section 3.2 - DS-Code Graph Construction\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Kh√°i ni·ªám c·ªët l√µi\n",
    "\n",
    "### T·ª´ Paper (Section 3.2):\n",
    "> *\"Different from existing code graphs, DS-code graph not only models dependency relationships but also introduces semantic relationships among nodes.\"*\n",
    "\n",
    "> *\"DS-code graph contains four node types: Module, Class, Method, Function. DS-code graph contains five edge types: Import, Contain, Inherit, Call, Similarity.\"*\n",
    "\n",
    "### ƒê·∫∑c ƒëi·ªÉm ph·ª©c t·∫°p:\n",
    "1. **Multi-type Node Schema**: 4 lo·∫°i nodes v·ªõi hierarchy r√µ r√†ng\n",
    "2. **Hybrid Edge System**: K·∫øt h·ª£p dependency v√† semantic relationships\n",
    "3. **Embedding-based Similarity**: S·ª≠ d·ª•ng embeddings ƒë·ªÉ t√≠nh semantic similarity\n",
    "4. **Graph Database Storage**: L∆∞u tr·ªØ hi·ªáu qu·∫£ v·ªõi Neo4j\n",
    "5. **Language-Specific Design**: Schema tailored cho Python characteristics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "# For embeddings and similarity computation\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For code parsing\n",
    "import tree_sitter\n",
    "from tree_sitter import Language, Parser\n",
    "\n",
    "# Set environment\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö L√Ω thuy·∫øt s√¢u: DS-Code Graph Schema\n",
    "\n",
    "### From Paper Section 3.2:\n",
    "\n",
    "**Node Types:**\n",
    "- **Module**: represents a code file\n",
    "- **Class**: a class defined in the repository  \n",
    "- **Method**: a method defined in the class\n",
    "- **Function**: a function code defined in the repository\n",
    "\n",
    "**Edge Types:**\n",
    "- **Import**: modules' dependency relationships\n",
    "- **Contain**: source code of a node contains the counterpart of another node\n",
    "- **Inherit**: class inheritance relationships\n",
    "- **Call**: invoking relationship among code snippets\n",
    "- **Similarity**: two nodes have similar semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CodeNode:\n",
    "    \"\"\"Enhanced Code Node v·ªõi full attributes t·ª´ paper\"\"\"\n",
    "    id: str\n",
    "    node_type: str  # Module, Class, Method, Function\n",
    "    name: str\n",
    "    source_code: str\n",
    "    signature: str\n",
    "    file_path: str\n",
    "    \n",
    "    # Additional attributes for different node types\n",
    "    class_name: Optional[str] = None  # For methods\n",
    "    line_start: Optional[int] = None\n",
    "    line_end: Optional[int] = None\n",
    "    complexity_score: float = 0.0\n",
    "    \n",
    "    # Embeddings and similarity\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    semantic_hash: Optional[str] = None\n",
    "    \n",
    "    # Dependencies\n",
    "    imports: List[str] = field(default_factory=list)\n",
    "    calls: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.semantic_hash is None:\n",
    "            self.semantic_hash = self._compute_semantic_hash()\n",
    "    \n",
    "    def _compute_semantic_hash(self) -> str:\n",
    "        \"\"\"Compute semantic hash for quick similarity checks\"\"\"\n",
    "        content = f\"{self.name}_{self.node_type}_{self.source_code}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "\n",
    "@dataclass \n",
    "class CodeEdge:\n",
    "    \"\"\"Enhanced Code Edge v·ªõi attributes\"\"\"\n",
    "    source_id: str\n",
    "    target_id: str\n",
    "    edge_type: str  # import, contain, inherit, call, similarity\n",
    "    confidence: float = 1.0\n",
    "    weight: float = 1.0\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "class AdvancedCodeParser:\n",
    "    \"\"\"Advanced code parser v·ªõi tree-sitter support\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.import_patterns = [\n",
    "            r'import\\s+([\\w\\.]+)',\n",
    "            r'from\\s+([\\w\\.]+)\\s+import'\n",
    "        ]\n",
    "        \n",
    "    def parse_file(self, file_path: str, content: str) -> List[CodeNode]:\n",
    "        \"\"\"Parse file and extract all code nodes\"\"\"\n",
    "        nodes = []\n",
    "        \n",
    "        # Add module node\n",
    "        module_node = CodeNode(\n",
    "            id=f\"module:{file_path}\",\n",
    "            node_type=\"Module\",\n",
    "            name=Path(file_path).stem,\n",
    "            source_code=content,\n",
    "            signature=f\"module {file_path}\",\n",
    "            file_path=file_path,\n",
    "            imports=self._extract_imports(content)\n",
    "        )\n",
    "        nodes.append(module_node)\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            \n",
    "            # Extract classes and functions\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.ClassDef):\n",
    "                    class_nodes = self._extract_class_node(node, content, file_path)\n",
    "                    nodes.extend(class_nodes)\n",
    "                elif isinstance(node, ast.FunctionDef):\n",
    "                    # Only top-level functions (not methods)\n",
    "                    if not any(isinstance(parent, ast.ClassDef) \n",
    "                             for parent in ast.walk(tree) \n",
    "                             if hasattr(parent, 'body') and node in getattr(parent, 'body', [])):\n",
    "                        func_node = self._extract_function_node(node, content, file_path)\n",
    "                        nodes.append(func_node)\n",
    "                        \n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error parsing {file_path}: {e}\")\n",
    "            \n",
    "        return nodes\n",
    "    \n",
    "    def _extract_imports(self, content: str) -> List[str]:\n",
    "        \"\"\"Extract import statements\"\"\"\n",
    "        imports = []\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.Import):\n",
    "                    for alias in node.names:\n",
    "                        imports.append(alias.name)\n",
    "                elif isinstance(node, ast.ImportFrom):\n",
    "                    if node.module:\n",
    "                        imports.append(node.module)\n",
    "        except:\n",
    "            pass\n",
    "        return imports\n",
    "    \n",
    "    def _extract_class_node(self, node: ast.ClassDef, content: str, file_path: str) -> List[CodeNode]:\n",
    "        \"\"\"Extract class and its methods\"\"\"\n",
    "        nodes = []\n",
    "        \n",
    "        # Extract class source code\n",
    "        source_lines = content.split('\\n')\n",
    "        class_source = '\\n'.join(source_lines[node.lineno-1:node.end_lineno])\n",
    "        \n",
    "        # Extract base classes for inheritance\n",
    "        base_classes = [base.id if isinstance(base, ast.Name) else str(base) \n",
    "                       for base in node.bases]\n",
    "        \n",
    "        class_node = CodeNode(\n",
    "            id=f\"class:{file_path}:{node.name}\",\n",
    "            node_type=\"Class\",\n",
    "            name=node.name,\n",
    "            source_code=class_source,\n",
    "            signature=f\"class {node.name}({', '.join(base_classes)}):\",\n",
    "            file_path=file_path,\n",
    "            line_start=node.lineno,\n",
    "            line_end=node.end_lineno\n",
    "        )\n",
    "        nodes.append(class_node)\n",
    "        \n",
    "        # Extract methods\n",
    "        for method in node.body:\n",
    "            if isinstance(method, ast.FunctionDef):\n",
    "                method_source = '\\n'.join(source_lines[method.lineno-1:method.end_lineno])\n",
    "                \n",
    "                method_node = CodeNode(\n",
    "                    id=f\"method:{file_path}:{node.name}:{method.name}\",\n",
    "                    node_type=\"Method\",\n",
    "                    name=method.name,\n",
    "                    source_code=method_source,\n",
    "                    signature=self._extract_signature(method),\n",
    "                    file_path=file_path,\n",
    "                    class_name=node.name,\n",
    "                    line_start=method.lineno,\n",
    "                    line_end=method.end_lineno,\n",
    "                    calls=self._extract_function_calls(method)\n",
    "                )\n",
    "                nodes.append(method_node)\n",
    "                \n",
    "        return nodes\n",
    "    \n",
    "    def _extract_function_node(self, node: ast.FunctionDef, content: str, file_path: str) -> CodeNode:\n",
    "        \"\"\"Extract function node\"\"\"\n",
    "        source_lines = content.split('\\n')\n",
    "        func_source = '\\n'.join(source_lines[node.lineno-1:node.end_lineno])\n",
    "        \n",
    "        return CodeNode(\n",
    "            id=f\"function:{file_path}:{node.name}\",\n",
    "            node_type=\"Function\",\n",
    "            name=node.name,\n",
    "            source_code=func_source,\n",
    "            signature=self._extract_signature(node),\n",
    "            file_path=file_path,\n",
    "            line_start=node.lineno,\n",
    "            line_end=node.end_lineno,\n",
    "            calls=self._extract_function_calls(node)\n",
    "        )\n",
    "    \n",
    "    def _extract_signature(self, node: ast.FunctionDef) -> str:\n",
    "        \"\"\"Extract function/method signature\"\"\"\n",
    "        args = [arg.arg for arg in node.args.args]\n",
    "        return f\"def {node.name}({', '.join(args)}):\"\n",
    "    \n",
    "    def _extract_function_calls(self, node: ast.FunctionDef) -> List[str]:\n",
    "        \"\"\"Extract function calls within a function/method\"\"\"\n",
    "        calls = []\n",
    "        for child in ast.walk(node):\n",
    "            if isinstance(child, ast.Call):\n",
    "                if isinstance(child.func, ast.Name):\n",
    "                    calls.append(child.func.id)\n",
    "                elif isinstance(child.func, ast.Attribute):\n",
    "                    calls.append(child.func.attr)\n",
    "        return calls\n",
    "\n",
    "# Test parser\n",
    "parser = AdvancedCodeParser()\n",
    "\n",
    "test_code = '''\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def process_data(self, data):\n",
    "        cleaned = self.clean_data(data)\n",
    "        return self.validate_data(cleaned)\n",
    "    \n",
    "    def clean_data(self, data):\n",
    "        return [x for x in data if x is not None]\n",
    "    \n",
    "    def validate_data(self, data):\n",
    "        return len(data) > 0\n",
    "\n",
    "def utility_function(x, y):\n",
    "    processor = DataProcessor({\"strict\": True})\n",
    "    return processor.process_data([x, y])\n",
    "'''\n",
    "\n",
    "print(\"Testing Advanced Code Parser...\")\n",
    "nodes = parser.parse_file(\"test.py\", test_code)\n",
    "print(f\"Extracted {len(nodes)} nodes:\")\n",
    "for node in nodes:\n",
    "    print(f\"  {node.node_type}: {node.name} (calls: {node.calls})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Deep Dive: Semantic Relationship Construction\n",
    "\n",
    "### Key Innovation t·ª´ paper:\n",
    "> *\"Besides dependency relationships, we also construct semantic relationships between nodes in the DS-code graph... we use a reliable embedding model to encode the source code of each node and complete semantic relationships according to their vectors' cosine similarities.\"*\n",
    "\n",
    "### Advanced Features:\n",
    "1. **Multi-level Similarity**: Code, signature, v√† semantic similarity\n",
    "2. **Embedding Optimization**: Caching v√† efficient computation\n",
    "3. **Threshold Tuning**: Dynamic threshold cho different node types\n",
    "4. **Similarity Metrics**: Multiple metrics beyond cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRelationshipBuilder:\n",
    "    \"\"\"Advanced semantic relationship builder\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model=\"openai\", cache_embeddings=True):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.cache_embeddings = cache_embeddings\n",
    "        self.embedding_cache = {}\n",
    "        \n",
    "        # Initialize embedding models\n",
    "        if embedding_model == \"openai\":\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "        else:\n",
    "            # Fallback to TF-IDF\n",
    "            self.tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "            \n",
    "        # Similarity thresholds by node type pairs\n",
    "        self.similarity_thresholds = {\n",
    "            ('Function', 'Function'): 0.75,\n",
    "            ('Method', 'Method'): 0.70,\n",
    "            ('Function', 'Method'): 0.65,\n",
    "            ('Class', 'Class'): 0.60,\n",
    "            ('Module', 'Module'): 0.50\n",
    "        }\n",
    "        \n",
    "    def compute_embeddings(self, nodes: List[CodeNode]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute embeddings for all nodes v·ªõi caching\"\"\"\n",
    "        embeddings_map = {}\n",
    "        \n",
    "        # Check cache first\n",
    "        texts_to_embed = []\n",
    "        nodes_to_embed = []\n",
    "        \n",
    "        for node in nodes:\n",
    "            cache_key = f\"{node.semantic_hash}_{self.embedding_model}\"\n",
    "            \n",
    "            if self.cache_embeddings and cache_key in self.embedding_cache:\n",
    "                embeddings_map[node.id] = self.embedding_cache[cache_key]\n",
    "            else:\n",
    "                # Prepare text for embedding\n",
    "                text = self._prepare_text_for_embedding(node)\n",
    "                texts_to_embed.append(text)\n",
    "                nodes_to_embed.append(node)\n",
    "        \n",
    "        # Compute embeddings for remaining nodes\n",
    "        if texts_to_embed:\n",
    "            print(f\"Computing embeddings for {len(texts_to_embed)} nodes...\")\n",
    "            \n",
    "            if self.embedding_model == \"openai\":\n",
    "                try:\n",
    "                    batch_embeddings = self.embeddings.embed_documents(texts_to_embed)\n",
    "                except Exception as e:\n",
    "                    print(f\"OpenAI embedding error: {e}, falling back to TF-IDF\")\n",
    "                    batch_embeddings = self._compute_tfidf_embeddings(texts_to_embed)\n",
    "            else:\n",
    "                batch_embeddings = self._compute_tfidf_embeddings(texts_to_embed)\n",
    "            \n",
    "            # Store results\n",
    "            for node, embedding in zip(nodes_to_embed, batch_embeddings):\n",
    "                embedding_array = np.array(embedding)\n",
    "                embeddings_map[node.id] = embedding_array\n",
    "                \n",
    "                # Cache if enabled\n",
    "                if self.cache_embeddings:\n",
    "                    cache_key = f\"{node.semantic_hash}_{self.embedding_model}\"\n",
    "                    self.embedding_cache[cache_key] = embedding_array\n",
    "        \n",
    "        return embeddings_map\n",
    "    \n",
    "    def _prepare_text_for_embedding(self, node: CodeNode) -> str:\n",
    "        \"\"\"Prepare node text for embedding\"\"\"\n",
    "        # Different strategies based on node type\n",
    "        if node.node_type == \"Module\":\n",
    "            # For modules, use imports and top-level structure\n",
    "            text = f\"Module {node.name} imports: {' '.join(node.imports)}\"\n",
    "        elif node.node_type in [\"Function\", \"Method\"]:\n",
    "            # For functions/methods, use signature + docstring + key statements\n",
    "            docstring = self._extract_docstring(node.source_code)\n",
    "            text = f\"{node.signature} {docstring} calls: {' '.join(node.calls)}\"\n",
    "        elif node.node_type == \"Class\":\n",
    "            # For classes, use class signature + method names\n",
    "            text = f\"{node.signature} {node.source_code[:200]}\"\n",
    "        else:\n",
    "            text = node.source_code\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _extract_docstring(self, source_code: str) -> str:\n",
    "        \"\"\"Extract docstring from source code\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(source_code)\n",
    "            if tree.body and isinstance(tree.body[0], (ast.FunctionDef, ast.ClassDef)):\n",
    "                return ast.get_docstring(tree.body[0]) or \"\"\n",
    "        except:\n",
    "            pass\n",
    "        return \"\"\n",
    "    \n",
    "    def _compute_tfidf_embeddings(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        \"\"\"Fallback TF-IDF embeddings\"\"\"\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf.fit_transform(texts)\n",
    "            return [tfidf_matrix[i].toarray().flatten() for i in range(len(texts))]\n",
    "        except:\n",
    "            # Ultimate fallback: random embeddings\n",
    "            return [np.random.rand(100) for _ in texts]\n",
    "    \n",
    "    def find_semantic_similarities(self, nodes: List[CodeNode], embeddings_map: Dict[str, np.ndarray]) -> List[CodeEdge]:\n",
    "        \"\"\"Find semantic similarity relationships\"\"\"\n",
    "        similarity_edges = []\n",
    "        \n",
    "        # Compare all pairs\n",
    "        for i, node1 in enumerate(nodes):\n",
    "            for node2 in nodes[i+1:]:\n",
    "                if node1.id == node2.id:\n",
    "                    continue\n",
    "                \n",
    "                # Skip if same file and same type (likely similar by design)\n",
    "                if (node1.file_path == node2.file_path and \n",
    "                    node1.node_type == node2.node_type and\n",
    "                    node1.node_type == \"Module\"):\n",
    "                    continue\n",
    "                \n",
    "                # Get embeddings\n",
    "                if node1.id not in embeddings_map or node2.id not in embeddings_map:\n",
    "                    continue\n",
    "                \n",
    "                emb1 = embeddings_map[node1.id]\n",
    "                emb2 = embeddings_map[node2.id]\n",
    "                \n",
    "                # Compute similarity\n",
    "                similarity = self._compute_similarity(emb1, emb2)\n",
    "                \n",
    "                # Check threshold\n",
    "                threshold = self._get_threshold(node1.node_type, node2.node_type)\n",
    "                \n",
    "                if similarity >= threshold:\n",
    "                    edge = CodeEdge(\n",
    "                        source_id=node1.id,\n",
    "                        target_id=node2.id,\n",
    "                        edge_type=\"similarity\",\n",
    "                        confidence=similarity,\n",
    "                        weight=similarity,\n",
    "                        metadata={\n",
    "                            'similarity_score': similarity,\n",
    "                            'threshold': threshold,\n",
    "                            'node_types': (node1.node_type, node2.node_type)\n",
    "                        }\n",
    "                    )\n",
    "                    similarity_edges.append(edge)\n",
    "        \n",
    "        print(f\"Found {len(similarity_edges)} semantic similarity relationships\")\n",
    "        return similarity_edges\n",
    "    \n",
    "    def _compute_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "        \"\"\"Compute cosine similarity between embeddings\"\"\"\n",
    "        try:\n",
    "            # Normalize vectors\n",
    "            norm1 = np.linalg.norm(emb1)\n",
    "            norm2 = np.linalg.norm(emb2)\n",
    "            \n",
    "            if norm1 == 0 or norm2 == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(emb1, emb2) / (norm1 * norm2)\n",
    "            return float(similarity)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _get_threshold(self, type1: str, type2: str) -> float:\n",
    "        \"\"\"Get similarity threshold for node type pair\"\"\"\n",
    "        key = (type1, type2) if (type1, type2) in self.similarity_thresholds else (type2, type1)\n",
    "        return self.similarity_thresholds.get(key, 0.65)  # Default threshold\n",
    "    \n",
    "    def analyze_similarity_patterns(self, edges: List[CodeEdge]) -> Dict[str, any]:\n",
    "        \"\"\"Analyze patterns in semantic similarities\"\"\"\n",
    "        analysis = {\n",
    "            'total_similarities': len(edges),\n",
    "            'by_node_types': defaultdict(int),\n",
    "            'similarity_distribution': [],\n",
    "            'high_confidence': 0,\n",
    "            'cross_file_similarities': 0\n",
    "        }\n",
    "        \n",
    "        for edge in edges:\n",
    "            # Node type patterns\n",
    "            node_types = edge.metadata.get('node_types', ('Unknown', 'Unknown'))\n",
    "            type_key = f\"{node_types[0]}-{node_types[1]}\"\n",
    "            analysis['by_node_types'][type_key] += 1\n",
    "            \n",
    "            # Similarity scores\n",
    "            similarity = edge.metadata.get('similarity_score', 0)\n",
    "            analysis['similarity_distribution'].append(similarity)\n",
    "            \n",
    "            if similarity >= 0.8:\n",
    "                analysis['high_confidence'] += 1\n",
    "            \n",
    "            # Cross-file similarities\n",
    "            source_file = edge.source_id.split(':')[1] if ':' in edge.source_id else \"\"\n",
    "            target_file = edge.target_id.split(':')[1] if ':' in edge.target_id else \"\"\n",
    "            if source_file != target_file:\n",
    "                analysis['cross_file_similarities'] += 1\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Test semantic relationship builder\n",
    "print(\"\\nTesting Semantic Relationship Builder...\")\n",
    "semantic_builder = SemanticRelationshipBuilder(embedding_model=\"tfidf\")  # Use TF-IDF for demo\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings_map = semantic_builder.compute_embeddings(nodes)\n",
    "print(f\"Computed embeddings for {len(embeddings_map)} nodes\")\n",
    "\n",
    "# Find similarities\n",
    "similarity_edges = semantic_builder.find_semantic_similarities(nodes, embeddings_map)\n",
    "analysis = semantic_builder.analyze_similarity_patterns(similarity_edges)\n",
    "\n",
    "print(f\"\\nSemantic Analysis Results:\")\n",
    "print(f\"Total similarities: {analysis['total_similarities']}\")\n",
    "print(f\"High confidence (>=0.8): {analysis['high_confidence']}\")\n",
    "print(f\"Cross-file similarities: {analysis['cross_file_similarities']}\")\n",
    "print(f\"Node type patterns: {dict(analysis['by_node_types'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üï∏Ô∏è Complete DS-Code Graph Construction\n",
    "\n",
    "### Integration of All Relationship Types:\n",
    "K·∫øt h·ª£p t·∫•t c·∫£ dependency v√† semantic relationships th√†nh complete graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSCodeGraph:\n",
    "    \"\"\"Complete DS-Code Graph implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, semantic_builder: SemanticRelationshipBuilder):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.nodes = {}  # id -> CodeNode\n",
    "        self.edges = {}  # (source, target) -> CodeEdge\n",
    "        self.parser = AdvancedCodeParser()\n",
    "        self.semantic_builder = semantic_builder\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'nodes_by_type': defaultdict(int),\n",
    "            'edges_by_type': defaultdict(int)\n",
    "        }\n",
    "    \n",
    "    def build_from_repository(self, repository: Dict[str, str]):\n",
    "        \"\"\"Build complete DS-Code Graph from repository\"\"\"\n",
    "        print(\"Building DS-Code Graph...\")\n",
    "        \n",
    "        # Step 1: Extract all nodes\n",
    "        print(\"Step 1: Extracting code nodes...\")\n",
    "        all_nodes = []\n",
    "        for file_path, content in repository.items():\n",
    "            file_nodes = self.parser.parse_file(file_path, content)\n",
    "            all_nodes.extend(file_nodes)\n",
    "        \n",
    "        # Add nodes to graph\n",
    "        for node in all_nodes:\n",
    "            self.add_node(node)\n",
    "        \n",
    "        print(f\"Extracted {len(all_nodes)} nodes\")\n",
    "        \n",
    "        # Step 2: Extract dependency relationships\n",
    "        print(\"Step 2: Extracting dependency relationships...\")\n",
    "        dependency_edges = self._extract_dependency_relationships(all_nodes)\n",
    "        \n",
    "        for edge in dependency_edges:\n",
    "            self.add_edge(edge)\n",
    "        \n",
    "        print(f\"Added {len(dependency_edges)} dependency relationships\")\n",
    "        \n",
    "        # Step 3: Extract semantic relationships\n",
    "        print(\"Step 3: Extracting semantic relationships...\")\n",
    "        embeddings_map = self.semantic_builder.compute_embeddings(all_nodes)\n",
    "        semantic_edges = self.semantic_builder.find_semantic_similarities(all_nodes, embeddings_map)\n",
    "        \n",
    "        for edge in semantic_edges:\n",
    "            self.add_edge(edge)\n",
    "        \n",
    "        print(f\"Added {len(semantic_edges)} semantic relationships\")\n",
    "        \n",
    "        # Update statistics\n",
    "        self._update_statistics()\n",
    "        \n",
    "        print(f\"\\nDS-Code Graph completed:\")\n",
    "        print(f\"Total nodes: {len(self.nodes)}\")\n",
    "        print(f\"Total edges: {len(self.edges)}\")\n",
    "        \n",
    "    def add_node(self, node: CodeNode):\n",
    "        \"\"\"Add node to graph\"\"\"\n",
    "        self.graph.add_node(node.id, **node.__dict__)\n",
    "        self.nodes[node.id] = node\n",
    "        self.stats['nodes_by_type'][node.node_type] += 1\n",
    "    \n",
    "    def add_edge(self, edge: CodeEdge):\n",
    "        \"\"\"Add edge to graph\"\"\"\n",
    "        if edge.source_id in self.nodes and edge.target_id in self.nodes:\n",
    "            self.graph.add_edge(\n",
    "                edge.source_id, \n",
    "                edge.target_id,\n",
    "                edge_type=edge.edge_type,\n",
    "                confidence=edge.confidence,\n",
    "                weight=edge.weight,\n",
    "                **edge.metadata\n",
    "            )\n",
    "            self.edges[(edge.source_id, edge.target_id)] = edge\n",
    "            self.stats['edges_by_type'][edge.edge_type] += 1\n",
    "    \n",
    "    def _extract_dependency_relationships(self, nodes: List[CodeNode]) -> List[CodeEdge]:\n",
    "        \"\"\"Extract all dependency relationships\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Create lookup maps\n",
    "        modules_by_path = {node.file_path: node for node in nodes if node.node_type == \"Module\"}\n",
    "        functions_by_name = defaultdict(list)\n",
    "        methods_by_name = defaultdict(list)\n",
    "        classes_by_name = defaultdict(list)\n",
    "        \n",
    "        for node in nodes:\n",
    "            if node.node_type == \"Function\":\n",
    "                functions_by_name[node.name].append(node)\n",
    "            elif node.node_type == \"Method\":\n",
    "                methods_by_name[node.name].append(node)\n",
    "            elif node.node_type == \"Class\":\n",
    "                classes_by_name[node.name].append(node)\n",
    "        \n",
    "        for node in nodes:\n",
    "            # 1. Import relationships\n",
    "            if node.node_type == \"Module\" and node.imports:\n",
    "                for import_name in node.imports:\n",
    "                    # Find imported module\n",
    "                    for other_node in nodes:\n",
    "                        if (other_node.node_type == \"Module\" and \n",
    "                            (import_name in other_node.file_path or import_name == other_node.name)):\n",
    "                            edges.append(CodeEdge(\n",
    "                                source_id=node.id,\n",
    "                                target_id=other_node.id,\n",
    "                                edge_type=\"import\",\n",
    "                                confidence=0.9\n",
    "                            ))\n",
    "            \n",
    "            # 2. Containment relationships\n",
    "            if node.node_type == \"Module\":\n",
    "                # Module contains classes and functions\n",
    "                for other_node in nodes:\n",
    "                    if (other_node.file_path == node.file_path and \n",
    "                        other_node.node_type in [\"Class\", \"Function\"]):\n",
    "                        edges.append(CodeEdge(\n",
    "                            source_id=node.id,\n",
    "                            target_id=other_node.id,\n",
    "                            edge_type=\"contain\",\n",
    "                            confidence=1.0\n",
    "                        ))\n",
    "            elif node.node_type == \"Class\":\n",
    "                # Class contains methods\n",
    "                for other_node in nodes:\n",
    "                    if (other_node.node_type == \"Method\" and \n",
    "                        other_node.class_name == node.name and\n",
    "                        other_node.file_path == node.file_path):\n",
    "                        edges.append(CodeEdge(\n",
    "                            source_id=node.id,\n",
    "                            target_id=other_node.id,\n",
    "                            edge_type=\"contain\",\n",
    "                            confidence=1.0\n",
    "                        ))\n",
    "            \n",
    "            # 3. Call relationships\n",
    "            if node.node_type in [\"Function\", \"Method\"] and node.calls:\n",
    "                for call_name in node.calls:\n",
    "                    # Find called functions/methods\n",
    "                    target_nodes = functions_by_name.get(call_name, []) + methods_by_name.get(call_name, [])\n",
    "                    \n",
    "                    for target_node in target_nodes:\n",
    "                        if target_node.id != node.id:  # Don't call self\n",
    "                            confidence = 0.8 if target_node.file_path == node.file_path else 0.6\n",
    "                            edges.append(CodeEdge(\n",
    "                                source_id=node.id,\n",
    "                                target_id=target_node.id,\n",
    "                                edge_type=\"call\",\n",
    "                                confidence=confidence,\n",
    "                                metadata={'call_name': call_name}\n",
    "                            ))\n",
    "            \n",
    "            # 4. Inheritance relationships (simplified)\n",
    "            if node.node_type == \"Class\":\n",
    "                # Extract base classes from signature\n",
    "                if '(' in node.signature and ')' in node.signature:\n",
    "                    base_part = node.signature.split('(')[1].split(')')[0]\n",
    "                    if base_part.strip() and base_part.strip() != \"\":\n",
    "                        base_classes = [bc.strip() for bc in base_part.split(',')]\n",
    "                        \n",
    "                        for base_class in base_classes:\n",
    "                            if base_class and base_class != \"object\":\n",
    "                                target_nodes = classes_by_name.get(base_class, [])\n",
    "                                for target_node in target_nodes:\n",
    "                                    edges.append(CodeEdge(\n",
    "                                        source_id=node.id,\n",
    "                                        target_id=target_node.id,\n",
    "                                        edge_type=\"inherit\",\n",
    "                                        confidence=0.9,\n",
    "                                        metadata={'base_class': base_class}\n",
    "                                    ))\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _update_statistics(self):\n",
    "        \"\"\"Update graph statistics\"\"\"\n",
    "        self.stats['total_nodes'] = len(self.nodes)\n",
    "        self.stats['total_edges'] = len(self.edges)\n",
    "        self.stats['density'] = nx.density(self.graph)\n",
    "        \n",
    "        # Calculate connectivity metrics\n",
    "        if self.graph.nodes():\n",
    "            self.stats['avg_degree'] = sum(dict(self.graph.degree()).values()) / len(self.graph.nodes())\n",
    "            self.stats['max_degree'] = max(dict(self.graph.degree()).values())\n",
    "        \n",
    "    def get_one_hop_neighbors(self, node_id: str, edge_types: Optional[List[str]] = None) -> List[Tuple[str, Dict]]:\n",
    "        \"\"\"Get one-hop neighbors with optional edge type filtering\"\"\"\n",
    "        neighbors = []\n",
    "        \n",
    "        if node_id not in self.graph:\n",
    "            return neighbors\n",
    "        \n",
    "        # Outgoing edges\n",
    "        for successor in self.graph.successors(node_id):\n",
    "            edge_data = self.graph.get_edge_data(node_id, successor)\n",
    "            if edge_types is None or edge_data.get('edge_type') in edge_types:\n",
    "                neighbors.append((successor, edge_data))\n",
    "        \n",
    "        # Incoming edges  \n",
    "        for predecessor in self.graph.predecessors(node_id):\n",
    "            edge_data = self.graph.get_edge_data(predecessor, node_id)\n",
    "            if edge_types is None or edge_data.get('edge_type') in edge_types:\n",
    "                neighbors.append((predecessor, edge_data))\n",
    "        \n",
    "        return neighbors\n",
    "    \n",
    "    def visualize_subgraph(self, center_node_id: str, max_depth: int = 2, \n",
    "                          edge_types: Optional[List[str]] = None, figsize=(16, 12)):\n",
    "        \"\"\"Visualize subgraph around a center node\"\"\"\n",
    "        \n",
    "        if center_node_id not in self.nodes:\n",
    "            print(f\"Node {center_node_id} not found\")\n",
    "            return\n",
    "        \n",
    "        # BFS to find subgraph\n",
    "        subgraph_nodes = {center_node_id}\n",
    "        current_level = {center_node_id}\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            next_level = set()\n",
    "            for node_id in current_level:\n",
    "                neighbors = self.get_one_hop_neighbors(node_id, edge_types)\n",
    "                for neighbor_id, _ in neighbors[:5]:  # Limit to 5 neighbors per node\n",
    "                    if neighbor_id not in subgraph_nodes:\n",
    "                        next_level.add(neighbor_id)\n",
    "                        subgraph_nodes.add(neighbor_id)\n",
    "            \n",
    "            current_level = next_level\n",
    "            if not current_level:\n",
    "                break\n",
    "        \n",
    "        # Create subgraph\n",
    "        subgraph = self.graph.subgraph(subgraph_nodes)\n",
    "        \n",
    "        if len(subgraph.nodes()) == 0:\n",
    "            print(\"No nodes to visualize\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Layout\n",
    "        pos = nx.spring_layout(subgraph, k=3, iterations=50)\n",
    "        \n",
    "        # Node colors by type\n",
    "        node_colors = {\n",
    "            'Module': 'lightcoral',\n",
    "            'Class': 'lightblue',\n",
    "            'Method': 'lightgreen', \n",
    "            'Function': 'lightyellow'\n",
    "        }\n",
    "        \n",
    "        colors = []\n",
    "        sizes = []\n",
    "        for node_id in subgraph.nodes():\n",
    "            node = self.nodes[node_id]\n",
    "            colors.append(node_colors.get(node.node_type, 'gray'))\n",
    "            # Center node larger\n",
    "            sizes.append(2000 if node_id == center_node_id else 1000)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(subgraph, pos,\n",
    "                              node_color=colors,\n",
    "                              node_size=sizes,\n",
    "                              alpha=0.8)\n",
    "        \n",
    "        # Draw edges by type\n",
    "        edge_colors = {\n",
    "            'import': 'blue',\n",
    "            'contain': 'green', \n",
    "            'inherit': 'purple',\n",
    "            'call': 'red',\n",
    "            'similarity': 'orange'\n",
    "        }\n",
    "        \n",
    "        edge_styles = {\n",
    "            'similarity': 'dashed',\n",
    "            'inherit': 'dotted'\n",
    "        }\n",
    "        \n",
    "        for edge_type, color in edge_colors.items():\n",
    "            edges = [(u, v) for u, v, d in subgraph.edges(data=True) \n",
    "                    if d.get('edge_type') == edge_type]\n",
    "            \n",
    "            if edges:\n",
    "                style = edge_styles.get(edge_type, 'solid')\n",
    "                width = 3 if edge_type == 'call' else 2\n",
    "                alpha = 0.6 if edge_type == 'similarity' else 0.8\n",
    "                \n",
    "                nx.draw_networkx_edges(subgraph, pos,\n",
    "                                      edgelist=edges,\n",
    "                                      edge_color=color,\n",
    "                                      width=width,\n",
    "                                      alpha=alpha,\n",
    "                                      style=style)\n",
    "        \n",
    "        # Draw labels\n",
    "        labels = {node_id: self.nodes[node_id].name for node_id in subgraph.nodes()}\n",
    "        nx.draw_networkx_labels(subgraph, pos, labels, font_size=8)\n",
    "        \n",
    "        plt.title(f'DS-Code Graph: {self.nodes[center_node_id].name}\\n'\n",
    "                 f'(Depth: {max_depth}, Nodes: {len(subgraph.nodes())}, Edges: {len(subgraph.edges())})',\n",
    "                 fontsize=14)\n",
    "        \n",
    "        # Legend\n",
    "        legend_elements = []\n",
    "        for edge_type, color in edge_colors.items():\n",
    "            if any(d.get('edge_type') == edge_type for _, _, d in subgraph.edges(data=True)):\n",
    "                legend_elements.append(plt.Line2D([0], [0], color=color, lw=2, label=edge_type.title()))\n",
    "        \n",
    "        if legend_elements:\n",
    "            plt.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print subgraph statistics\n",
    "        print(f\"\\nSubgraph Statistics:\")\n",
    "        print(f\"Center node: {self.nodes[center_node_id].name} ({self.nodes[center_node_id].node_type})\")\n",
    "        print(f\"Nodes: {len(subgraph.nodes())}\")\n",
    "        print(f\"Edges: {len(subgraph.edges())}\")\n",
    "        \n",
    "        edge_type_counts = defaultdict(int)\n",
    "        for _, _, d in subgraph.edges(data=True):\n",
    "            edge_type_counts[d.get('edge_type', 'unknown')] += 1\n",
    "        \n",
    "        print(\"Edge types:\", dict(edge_type_counts))\n",
    "\n",
    "# Test complete DS-Code Graph\n",
    "complex_repository = {\n",
    "    'auth.py': '''\n",
    "import hashlib\n",
    "from typing import Optional\n",
    "\n",
    "class User:\n",
    "    def __init__(self, username: str, email: str):\n",
    "        self.username = username\n",
    "        self.email = email\n",
    "        self.password_hash = None\n",
    "    \n",
    "    def set_password(self, password: str):\n",
    "        \"\"\"Set user password with hashing\"\"\"\n",
    "        self.password_hash = hash_password(password)\n",
    "    \n",
    "    def verify_password(self, password: str) -> bool:\n",
    "        \"\"\"Verify user password\"\"\"\n",
    "        return verify_password_hash(password, self.password_hash)\n",
    "\n",
    "def hash_password(password: str) -> str:\n",
    "    \"\"\"Hash password using SHA256\"\"\"\n",
    "    return hashlib.sha256(password.encode()).hexdigest()\n",
    "\n",
    "def verify_password_hash(password: str, hash_value: str) -> bool:\n",
    "    \"\"\"Verify password against hash\"\"\"\n",
    "    return hash_password(password) == hash_value\n",
    "''',\n",
    "    \n",
    "    'utils.py': '''\n",
    "from typing import List, Any\n",
    "\n",
    "def validate_email(email: str) -> bool:\n",
    "    \"\"\"Validate email format\"\"\"\n",
    "    return \"@\" in email and \".\" in email\n",
    "\n",
    "def clean_data(data: List[Any]) -> List[Any]:\n",
    "    \"\"\"Clean data by removing None values\"\"\"\n",
    "    return [item for item in data if item is not None]\n",
    "\n",
    "def sanitize_input(text: str) -> str:\n",
    "    \"\"\"Sanitize user input\"\"\"\n",
    "    cleaned = clean_data([text.strip()])\n",
    "    return cleaned[0] if cleaned else \"\"\n",
    "''',\n",
    "    \n",
    "    'app.py': '''\n",
    "from auth import User, hash_password\n",
    "from utils import validate_email, sanitize_input\n",
    "\n",
    "class Application:\n",
    "    def __init__(self):\n",
    "        self.users = []\n",
    "    \n",
    "    def register_user(self, username: str, email: str, password: str) -> bool:\n",
    "        \"\"\"Register a new user\"\"\"\n",
    "        if not validate_email(email):\n",
    "            return False\n",
    "        \n",
    "        clean_username = sanitize_input(username)\n",
    "        user = User(clean_username, email)\n",
    "        user.set_password(password)\n",
    "        self.users.append(user)\n",
    "        return True\n",
    "    \n",
    "    def authenticate_user(self, username: str, password: str) -> bool:\n",
    "        \"\"\"Authenticate user credentials\"\"\"\n",
    "        for user in self.users:\n",
    "            if user.username == username:\n",
    "                return user.verify_password(password)\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main application entry point\"\"\"\n",
    "    app = Application()\n",
    "    \n",
    "    # Register demo user\n",
    "    success = app.register_user(\"demo\", \"demo@example.com\", \"password123\")\n",
    "    print(f\"Registration: {success}\")\n",
    "    \n",
    "    # Test authentication\n",
    "    auth_result = app.authenticate_user(\"demo\", \"password123\")\n",
    "    print(f\"Authentication: {auth_result}\")\n",
    "'''\n",
    "}\n",
    "\n",
    "print(\"\\nBuilding Complete DS-Code Graph...\")\n",
    "ds_graph = DSCodeGraph(semantic_builder)\n",
    "ds_graph.build_from_repository(complex_repository)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nGraph Statistics:\")\n",
    "print(f\"Nodes by type: {dict(ds_graph.stats['nodes_by_type'])}\")\n",
    "print(f\"Edges by type: {dict(ds_graph.stats['edges_by_type'])}\")\n",
    "print(f\"Density: {ds_graph.stats.get('density', 0):.3f}\")\n",
    "print(f\"Average degree: {ds_graph.stats.get('avg_degree', 0):.2f}\")\n",
    "\n",
    "# Visualize around a key function\n",
    "app_class_id = None\n",
    "for node_id, node in ds_graph.nodes.items():\n",
    "    if node.name == \"Application\" and node.node_type == \"Class\":\n",
    "        app_class_id = node_id\n",
    "        break\n",
    "\n",
    "if app_class_id:\n",
    "    print(f\"\\nVisualizing subgraph around Application class...\")\n",
    "    ds_graph.visualize_subgraph(app_class_id, max_depth=2)\n",
    "else:\n",
    "    print(\"Application class not found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Analysis v√† Quality Metrics\n",
    "\n",
    "### Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng c·ªßa DS-Code Graph construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ds_code_graph_quality(ds_graph: DSCodeGraph):\n",
    "    \"\"\"Comprehensive quality analysis of DS-Code Graph\"\"\"\n",
    "    \n",
    "    # Collect graph metrics\n",
    "    metrics = {\n",
    "        'graph_connectivity': {},\n",
    "        'relationship_quality': {},\n",
    "        'semantic_analysis': {},\n",
    "        'structural_properties': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Graph connectivity analysis\n",
    "    if ds_graph.graph.nodes():\n",
    "        # Basic connectivity\n",
    "        metrics['graph_connectivity'] = {\n",
    "            'num_nodes': len(ds_graph.graph.nodes()),\n",
    "            'num_edges': len(ds_graph.graph.edges()),\n",
    "            'density': nx.density(ds_graph.graph),\n",
    "            'is_connected': nx.is_weakly_connected(ds_graph.graph),\n",
    "            'num_components': nx.number_weakly_connected_components(ds_graph.graph)\n",
    "        }\n",
    "        \n",
    "        # Degree analysis\n",
    "        degrees = dict(ds_graph.graph.degree())\n",
    "        in_degrees = dict(ds_graph.graph.in_degree())\n",
    "        out_degrees = dict(ds_graph.graph.out_degree())\n",
    "        \n",
    "        metrics['graph_connectivity'].update({\n",
    "            'avg_degree': np.mean(list(degrees.values())),\n",
    "            'max_degree': max(degrees.values()),\n",
    "            'avg_in_degree': np.mean(list(in_degrees.values())),\n",
    "            'avg_out_degree': np.mean(list(out_degrees.values()))\n",
    "        })\n",
    "    \n",
    "    # 2. Relationship quality analysis\n",
    "    edge_types = defaultdict(list)\n",
    "    confidence_scores = []\n",
    "    \n",
    "    for source, target, data in ds_graph.graph.edges(data=True):\n",
    "        edge_type = data.get('edge_type', 'unknown')\n",
    "        confidence = data.get('confidence', 0)\n",
    "        \n",
    "        edge_types[edge_type].append(confidence)\n",
    "        confidence_scores.append(confidence)\n",
    "    \n",
    "    metrics['relationship_quality'] = {\n",
    "        'edge_type_counts': {et: len(confs) for et, confs in edge_types.items()},\n",
    "        'avg_confidence_by_type': {et: np.mean(confs) for et, confs in edge_types.items()},\n",
    "        'overall_avg_confidence': np.mean(confidence_scores) if confidence_scores else 0,\n",
    "        'high_confidence_ratio': sum(1 for c in confidence_scores if c >= 0.8) / len(confidence_scores) if confidence_scores else 0\n",
    "    }\n",
    "    \n",
    "    # 3. Semantic analysis\n",
    "    semantic_edges = [e for e in ds_graph.edges.values() if e.edge_type == 'similarity']\n",
    "    \n",
    "    if semantic_edges:\n",
    "        semantic_scores = [e.confidence for e in semantic_edges]\n",
    "        cross_file_semantic = sum(1 for e in semantic_edges \n",
    "                                 if e.source_id.split(':')[1] != e.target_id.split(':')[1])\n",
    "        \n",
    "        metrics['semantic_analysis'] = {\n",
    "            'num_semantic_relationships': len(semantic_edges),\n",
    "            'avg_semantic_confidence': np.mean(semantic_scores),\n",
    "            'cross_file_semantic_ratio': cross_file_semantic / len(semantic_edges),\n",
    "            'semantic_score_distribution': {\n",
    "                'min': min(semantic_scores),\n",
    "                'max': max(semantic_scores),\n",
    "                'std': np.std(semantic_scores)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # 4. Structural properties\n",
    "    node_type_connectivity = defaultdict(lambda: {'in': 0, 'out': 0, 'total': 0})\n",
    "    \n",
    "    for node_id, node in ds_graph.nodes.items():\n",
    "        in_deg = ds_graph.graph.in_degree(node_id)\n",
    "        out_deg = ds_graph.graph.out_degree(node_id)\n",
    "        \n",
    "        node_type_connectivity[node.node_type]['in'] += in_deg\n",
    "        node_type_connectivity[node.node_type]['out'] += out_deg\n",
    "        node_type_connectivity[node.node_type]['total'] += (in_deg + out_deg)\n",
    "    \n",
    "    # Normalize by node count\n",
    "    for node_type in node_type_connectivity:\n",
    "        count = ds_graph.stats['nodes_by_type'][node_type]\n",
    "        if count > 0:\n",
    "            node_type_connectivity[node_type] = {\n",
    "                k: v / count for k, v in node_type_connectivity[node_type].items()\n",
    "            }\n",
    "    \n",
    "    metrics['structural_properties'] = {\n",
    "        'node_type_connectivity': dict(node_type_connectivity),\n",
    "        'nodes_by_type': dict(ds_graph.stats['nodes_by_type']),\n",
    "        'edges_by_type': dict(ds_graph.stats['edges_by_type'])\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_graph_quality_metrics(metrics: Dict):\n",
    "    \"\"\"Visualize DS-Code Graph quality metrics\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Edge type distribution\n",
    "    edge_counts = metrics['relationship_quality']['edge_type_counts']\n",
    "    if edge_counts:\n",
    "        ax1.bar(edge_counts.keys(), edge_counts.values(), alpha=0.8)\n",
    "        ax1.set_title('Edge Type Distribution')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Confidence by edge type\n",
    "    conf_by_type = metrics['relationship_quality']['avg_confidence_by_type']\n",
    "    if conf_by_type:\n",
    "        colors = ['skyblue' if c >= 0.8 else 'lightcoral' for c in conf_by_type.values()]\n",
    "        bars = ax2.bar(conf_by_type.keys(), conf_by_type.values(), color=colors, alpha=0.8)\n",
    "        ax2.set_title('Average Confidence by Edge Type')\n",
    "        ax2.set_ylabel('Confidence Score')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='High Confidence Threshold')\n",
    "        ax2.legend()\n",
    "    \n",
    "    # 3. Node type connectivity\n",
    "    if 'node_type_connectivity' in metrics['structural_properties']:\n",
    "        connectivity = metrics['structural_properties']['node_type_connectivity']\n",
    "        node_types = list(connectivity.keys())\n",
    "        in_degrees = [connectivity[nt]['in'] for nt in node_types]\n",
    "        out_degrees = [connectivity[nt]['out'] for nt in node_types]\n",
    "        \n",
    "        x = np.arange(len(node_types))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax3.bar(x - width/2, in_degrees, width, label='In-degree', alpha=0.8)\n",
    "        ax3.bar(x + width/2, out_degrees, width, label='Out-degree', alpha=0.8)\n",
    "        ax3.set_title('Average Connectivity by Node Type')\n",
    "        ax3.set_ylabel('Average Degree')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels(node_types)\n",
    "        ax3.legend()\n",
    "    \n",
    "    # 4. Overall graph metrics\n",
    "    if 'graph_connectivity' in metrics:\n",
    "        conn_metrics = metrics['graph_connectivity']\n",
    "        \n",
    "        metric_names = ['Density', 'Avg Degree', 'Connectivity']\n",
    "        metric_values = [\n",
    "            conn_metrics.get('density', 0),\n",
    "            conn_metrics.get('avg_degree', 0) / 10,  # Normalized for visualization\n",
    "            1.0 if conn_metrics.get('is_connected', False) else 0.0\n",
    "        ]\n",
    "        \n",
    "        colors = ['green' if v >= 0.5 else 'orange' if v >= 0.2 else 'red' for v in metric_values]\n",
    "        ax4.bar(metric_names, metric_values, color=colors, alpha=0.8)\n",
    "        ax4.set_title('Overall Graph Quality Metrics')\n",
    "        ax4.set_ylabel('Normalized Score')\n",
    "        ax4.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DS-CODE GRAPH QUALITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'graph_connectivity' in metrics:\n",
    "        conn = metrics['graph_connectivity']\n",
    "        print(f\"\\nüìä Graph Connectivity:\")\n",
    "        print(f\"‚Ä¢ Nodes: {conn.get('num_nodes', 0)}\")\n",
    "        print(f\"‚Ä¢ Edges: {conn.get('num_edges', 0)}\")\n",
    "        print(f\"‚Ä¢ Density: {conn.get('density', 0):.3f}\")\n",
    "        print(f\"‚Ä¢ Connected: {conn.get('is_connected', False)}\")\n",
    "        print(f\"‚Ä¢ Components: {conn.get('num_components', 0)}\")\n",
    "    \n",
    "    if 'relationship_quality' in metrics:\n",
    "        rel = metrics['relationship_quality']\n",
    "        print(f\"\\nüîó Relationship Quality:\")\n",
    "        print(f\"‚Ä¢ Overall confidence: {rel.get('overall_avg_confidence', 0):.3f}\")\n",
    "        print(f\"‚Ä¢ High confidence ratio: {rel.get('high_confidence_ratio', 0):.3f}\")\n",
    "        print(f\"‚Ä¢ Edge types: {rel.get('edge_type_counts', {})}\")\n",
    "    \n",
    "    if 'semantic_analysis' in metrics:\n",
    "        sem = metrics['semantic_analysis']\n",
    "        print(f\"\\nüß† Semantic Analysis:\")\n",
    "        print(f\"‚Ä¢ Semantic relationships: {sem.get('num_semantic_relationships', 0)}\")\n",
    "        print(f\"‚Ä¢ Avg semantic confidence: {sem.get('avg_semantic_confidence', 0):.3f}\")\n",
    "        print(f\"‚Ä¢ Cross-file semantic ratio: {sem.get('cross_file_semantic_ratio', 0):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    \n",
    "    # Generate insights based on metrics\n",
    "    insights = []\n",
    "    \n",
    "    if metrics.get('relationship_quality', {}).get('high_confidence_ratio', 0) >= 0.7:\n",
    "        insights.append(\"High-quality relationships v·ªõi strong confidence scores\")\n",
    "    \n",
    "    if metrics.get('semantic_analysis', {}).get('cross_file_semantic_ratio', 0) >= 0.3:\n",
    "        insights.append(\"Good cross-file semantic relationships detected\")\n",
    "    \n",
    "    if metrics.get('graph_connectivity', {}).get('density', 0) >= 0.1:\n",
    "        insights.append(\"Well-connected graph structure\")\n",
    "    \n",
    "    edge_types = metrics.get('relationship_quality', {}).get('edge_type_counts', {})\n",
    "    if len(edge_types) >= 4:\n",
    "        insights.append(\"Comprehensive relationship coverage (4+ edge types)\")\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(f\"‚Ä¢ {insight}\")\n",
    "    \n",
    "    if not insights:\n",
    "        print(\"‚Ä¢ Graph structure c√≥ th·ªÉ c·∫ßn optimization\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Run comprehensive quality analysis\n",
    "print(\"\\nRunning DS-Code Graph quality analysis...\")\n",
    "quality_metrics = analyze_ds_code_graph_quality(ds_graph)\n",
    "visualize_graph_quality_metrics(quality_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Mock Data Testing v√† Independent Validation\n",
    "\n",
    "T·∫°o test cases ƒë·ªôc l·∫≠p ƒë·ªÉ validate DS-Code Graph construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_scenarios_for_ds_graph():\n",
    "    \"\"\"Create comprehensive test scenarios for DS-Code Graph\"\"\"\n",
    "    \n",
    "    scenarios = {\n",
    "        'inheritance_test': {\n",
    "            'description': 'Test inheritance relationship extraction',\n",
    "            'code': '''\n",
    "class Animal:\n",
    "    def speak(self):\n",
    "        pass\n",
    "\n",
    "class Dog(Animal):\n",
    "    def speak(self):\n",
    "        return \"Woof!\"\n",
    "\n",
    "class Cat(Animal):\n",
    "    def speak(self):\n",
    "        return \"Meow!\"\n",
    "''',\n",
    "            'expected_edges': [\n",
    "                ('inherit', 'Dog', 'Animal'),\n",
    "                ('inherit', 'Cat', 'Animal'),\n",
    "                ('similarity', 'Dog.speak', 'Cat.speak')\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'call_dependency_test': {\n",
    "            'description': 'Test function call dependency extraction',\n",
    "            'code': '''\n",
    "def utility_func(x):\n",
    "    return x * 2\n",
    "\n",
    "def helper_func(y):\n",
    "    return y + 1\n",
    "\n",
    "def main_func(data):\n",
    "    processed = utility_func(data)\n",
    "    result = helper_func(processed)\n",
    "    return result\n",
    "''',\n",
    "            'expected_edges': [\n",
    "                ('call', 'main_func', 'utility_func'),\n",
    "                ('call', 'main_func', 'helper_func'),\n",
    "                ('similarity', 'utility_func', 'helper_func')\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'containment_test': {\n",
    "            'description': 'Test containment relationship extraction',\n",
    "            'code': '''\n",
    "class Calculator:\n",
    "    def add(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def multiply(self, a, b):\n",
    "        return a * b\n",
    "    \n",
    "    def complex_operation(self, x, y):\n",
    "        sum_result = self.add(x, y)\n",
    "        return self.multiply(sum_result, 2)\n",
    "''',\n",
    "            'expected_edges': [\n",
    "                ('contain', 'Calculator', 'add'),\n",
    "                ('contain', 'Calculator', 'multiply'),\n",
    "                ('contain', 'Calculator', 'complex_operation'),\n",
    "                ('call', 'complex_operation', 'add'),\n",
    "                ('call', 'complex_operation', 'multiply')\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "def run_ds_graph_test_scenario(scenario_name: str, scenario_data: Dict, semantic_builder: SemanticRelationshipBuilder):\n",
    "    \"\"\"Run a single test scenario\"\"\"\n",
    "    \n",
    "    print(f\"\\nüß™ Testing: {scenario_name}\")\n",
    "    print(f\"Description: {scenario_data['description']}\")\n",
    "    \n",
    "    # Create test repository\n",
    "    test_repo = {f\"{scenario_name}.py\": scenario_data['code']}\n",
    "    \n",
    "    # Build DS-Code Graph\n",
    "    test_graph = DSCodeGraph(semantic_builder)\n",
    "    test_graph.build_from_repository(test_repo)\n",
    "    \n",
    "    # Extract actual relationships\n",
    "    actual_edges = []\n",
    "    for source, target, data in test_graph.graph.edges(data=True):\n",
    "        edge_type = data.get('edge_type')\n",
    "        source_name = test_graph.nodes[source].name\n",
    "        target_name = test_graph.nodes[target].name\n",
    "        \n",
    "        # For methods, include class name\n",
    "        if test_graph.nodes[source].node_type == 'Method':\n",
    "            source_name = f\"{test_graph.nodes[source].class_name}.{source_name}\"\n",
    "        if test_graph.nodes[target].node_type == 'Method':\n",
    "            target_name = f\"{test_graph.nodes[target].class_name}.{target_name}\"\n",
    "        \n",
    "        actual_edges.append((edge_type, source_name, target_name))\n",
    "    \n",
    "    # Compare with expected\n",
    "    expected_edges = scenario_data['expected_edges']\n",
    "    \n",
    "    print(f\"\\nExpected edges: {len(expected_edges)}\")\n",
    "    print(f\"Actual edges: {len(actual_edges)}\")\n",
    "    \n",
    "    # Find matches\n",
    "    matches = 0\n",
    "    for expected in expected_edges:\n",
    "        if expected in actual_edges:\n",
    "            matches += 1\n",
    "            print(f\"‚úì Found: {expected}\")\n",
    "        else:\n",
    "            print(f\"‚úó Missing: {expected}\")\n",
    "    \n",
    "    # Show unexpected edges\n",
    "    unexpected = [edge for edge in actual_edges if edge not in expected_edges]\n",
    "    if unexpected:\n",
    "        print(f\"\\nUnexpected edges found:\")\n",
    "        for edge in unexpected[:5]:  # Limit output\n",
    "            print(f\"  + {edge}\")\n",
    "    \n",
    "    # Calculate success rate\n",
    "    success_rate = matches / len(expected_edges) if expected_edges else 0\n",
    "    print(f\"\\nSuccess rate: {matches}/{len(expected_edges)} ({success_rate:.1%})\")\n",
    "    \n",
    "    return {\n",
    "        'scenario': scenario_name,\n",
    "        'expected': len(expected_edges),\n",
    "        'actual': len(actual_edges),\n",
    "        'matches': matches,\n",
    "        'success_rate': success_rate,\n",
    "        'graph': test_graph\n",
    "    }\n",
    "\n",
    "def run_comprehensive_ds_graph_tests():\n",
    "    \"\"\"Run all DS-Code Graph test scenarios\"\"\"\n",
    "    \n",
    "    scenarios = create_test_scenarios_for_ds_graph()\n",
    "    results = []\n",
    "    \n",
    "    # Use TF-IDF for faster testing\n",
    "    test_semantic_builder = SemanticRelationshipBuilder(embedding_model=\"tfidf\")\n",
    "    \n",
    "    for scenario_name, scenario_data in scenarios.items():\n",
    "        result = run_ds_graph_test_scenario(scenario_name, scenario_data, test_semantic_builder)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DS-CODE GRAPH TEST SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_expected = sum(r['expected'] for r in results)\n",
    "    total_matches = sum(r['matches'] for r in results)\n",
    "    overall_success = total_matches / total_expected if total_expected > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Overall Results:\")\n",
    "    print(f\"‚Ä¢ Total test scenarios: {len(results)}\")\n",
    "    print(f\"‚Ä¢ Total expected relationships: {total_expected}\")\n",
    "    print(f\"‚Ä¢ Total matches found: {total_matches}\")\n",
    "    print(f\"‚Ä¢ Overall success rate: {overall_success:.1%}\")\n",
    "    \n",
    "    print(f\"\\nüìã Individual Scenario Results:\")\n",
    "    for result in results:\n",
    "        print(f\"‚Ä¢ {result['scenario']}: {result['success_rate']:.1%} ({result['matches']}/{result['expected']})\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Success rates by scenario\n",
    "    scenario_names = [r['scenario'].replace('_test', '') for r in results]\n",
    "    success_rates = [r['success_rate'] for r in results]\n",
    "    \n",
    "    colors = ['green' if sr >= 0.8 else 'orange' if sr >= 0.6 else 'red' for sr in success_rates]\n",
    "    \n",
    "    plt.bar(scenario_names, success_rates, color=colors, alpha=0.8)\n",
    "    plt.title('DS-Code Graph Test Results by Scenario')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.xlabel('Test Scenario')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add success rate labels\n",
    "    for i, rate in enumerate(success_rates):\n",
    "        plt.text(i, rate + 0.02, f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Target (80%)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive tests\n",
    "test_results = run_comprehensive_ds_graph_tests()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DS-CODE GRAPH FOCUSED LEARNING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Key Learnings:\")\n",
    "print(\"1. Multi-type node schema enables rich relationship modeling\")\n",
    "print(\"2. Semantic relationships complement dependency relationships\")\n",
    "print(\"3. Embedding-based similarity detection works for code\")\n",
    "print(\"4. Graph visualization reveals complex code structures\")\n",
    "print(\"5. Quality metrics help validate graph construction\")\n",
    "print(\"6. Test scenarios ensure reliable relationship extraction\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}