# 2407.08275v1.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: D:\llm\notebooks\AI-Papers\2407.08275v1.pdf
# KÃ­ch thÆ°á»›c file: 1593972 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================


--- TRANG 1 ---
VÆ°á»£t Ra NgoÃ i CÃ¡c BÃ i Kiá»ƒm Tra: ÄÃ¡nh GiÃ¡ Sá»± TÆ°Æ¡ng Äá»“ng MÃ´ HÃ¬nh Embedding cho
Há»‡ Thá»‘ng TÄƒng CÆ°á»ng Truy Xuáº¥t Sinh VÄƒn Báº£n
Laura Caspari
laura.caspari@uni-passau.de
Äáº¡i há»c Passau
Passau, Äá»©cKanishka Ghosh Dastidar
kanishka.ghoshdastidar@uni-
passau.de
Äáº¡i há»c Passau
Passau, Äá»©cSaber Zerhoudi
saber.zerhoudi@uni-passau.de
Äáº¡i há»c Passau
Passau, Äá»©c
Jelena Mitrovic
jelena.mitrovic@uni-passau.de
Äáº¡i há»c Passau
Passau, Äá»©cMichael Granitzer
michael.granitzer@uni-passau.de
Äáº¡i há»c Passau
Passau, Äá»©c

TÃ“M Táº®T
Viá»‡c lá»±a chá»n mÃ´ hÃ¬nh embedding lÃ  má»™t bÆ°á»›c quan trá»ng trong thiáº¿t káº¿
há»‡ thá»‘ng TÄƒng cÆ°á»ng Truy xuáº¥t Sinh vÄƒn báº£n (RAG). Vá»›i sá»‘ lÆ°á»£ng lá»›n cÃ¡c
lá»±a chá»n cÃ³ sáºµn, viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c cá»¥m mÃ´ hÃ¬nh tÆ°Æ¡ng tá»± nhau sáº½ Ä‘Æ¡n giáº£n
hÃ³a quÃ¡ trÃ¬nh lá»±a chá»n mÃ´ hÃ¬nh nÃ y. Viá»‡c chá»‰ dá»±a vÃ o Ä‘iá»ƒm sá»‘ hiá»‡u suáº¥t tá»«
cÃ¡c bÃ i kiá»ƒm tra chá»‰ cho phÃ©p Ä‘Ã¡nh giÃ¡ yáº¿u vá» sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a mÃ´ hÃ¬nh.
Do Ä‘Ã³, trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c mÃ´
hÃ¬nh embedding trong bá»‘i cáº£nh há»‡ thá»‘ng RAG. ÄÃ¡nh giÃ¡ cá»§a chÃºng tÃ´i cÃ³
hai khÃ­a cáº¡nh: ChÃºng tÃ´i sá»­ dá»¥ng CÄƒn chá»‰nh NhÃ¢n tÃ¢m (Centered Kernel
Alignment) Ä‘á»ƒ so sÃ¡nh cÃ¡c embedding theo tá»«ng cáº·p. NgoÃ i ra, vÃ¬ Ä‘iá»u nÃ y
Ä‘áº·c biá»‡t liÃªn quan Ä‘áº¿n há»‡ thá»‘ng RAG, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a
káº¿t quáº£ truy xuáº¥t giá»¯a cÃ¡c mÃ´ hÃ¬nh nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng
Jaccard vÃ  rank. ChÃºng tÃ´i so sÃ¡nh cÃ¡c há» mÃ´ hÃ¬nh embedding khÃ¡c nhau,
bao gá»“m cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n, trÃªn nÄƒm bá»™ dá»¯ liá»‡u tá»« Benchmark
Information Retrieval (BEIR) ná»•i tiáº¿ng. ThÃ´ng qua cÃ¡c thÃ­ nghiá»‡m, chÃºng
tÃ´i xÃ¡c Ä‘á»‹nh cÃ¡c cá»¥m mÃ´ hÃ¬nh tÆ°Æ¡ng á»©ng vá»›i cÃ¡c há» mÃ´ hÃ¬nh, nhÆ°ng thÃº vá»‹
lÃ  cÅ©ng cÃ³ má»™t sá»‘ cá»¥m liÃªn há». HÆ¡n ná»¯a, phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i vá» sá»±
tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t top-ğ‘˜ cho tháº¥y cÃ³ phÆ°Æ¡ng sai cao á»Ÿ giÃ¡ trá»‹ ğ‘˜ tháº¥p.
ChÃºng tÃ´i cÅ©ng xÃ¡c Ä‘á»‹nh cÃ¡c thay tháº¿ mÃ£ nguá»“n má»Ÿ cÃ³ thá»ƒ cho cÃ¡c mÃ´ hÃ¬nh
Ä‘á»™c quyá»n, vá»›i Mistral thá»ƒ hiá»‡n sá»± tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh
OpenAI.

CÃC KHÃI NIá»†M CCS
â€¢Há»‡ thá»‘ng thÃ´ng tin â†’ÄÃ¡nh giÃ¡ káº¿t quáº£ truy xuáº¥t ;MÃ´ hÃ¬nh truy xuáº¥t
vÃ  xáº¿p háº¡ng ;MÃ´ hÃ¬nh ngÃ´n ngá»¯ .

Tá»ª KHÃ“A
MÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, TÄƒng cÆ°á»ng truy xuáº¥t sinh vÄƒn báº£n, Sá»± tÆ°Æ¡ng Ä‘á»“ng
mÃ´ hÃ¬nh

1 Äá»˜NG CÆ 
TÄƒng cÆ°á»ng Truy xuáº¥t Sinh vÄƒn báº£n (RAG) lÃ  má»™t mÃ´ hÃ¬nh má»›i ná»•i giÃºp
giáº£m thiá»ƒu cÃ¡c váº¥n Ä‘á» vá» áº£o giÃ¡c thá»±c táº¿ [13] vÃ  dá»¯ liá»‡u huáº¥n luyá»‡n lá»—i thá»i
[27] cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) báº±ng cÃ¡ch cung cáº¥p cho cÃ¡c mÃ´
hÃ¬nh nÃ y quyá»n truy cáº­p vÃ o má»™t nguá»“n kiáº¿n thá»©c ngoÃ i, khÃ´ng tham sá»‘
(vÃ­ dá»¥: má»™t kho tÃ i liá»‡u). Trá»ng tÃ¢m cá»§a hoáº¡t Ä‘á»™ng cá»§a cÃ¡c framework RAG
lÃ  bÆ°á»›c truy xuáº¥t, trong Ä‘Ã³ má»™t táº­p há»£p nhá» cÃ¡c tÃ i liá»‡u á»©ng viÃªn Ä‘Æ°á»£c truy
xuáº¥t tá»« kho tÃ i liá»‡u, cá»¥ thá»ƒ cho truy váº¥n Ä‘áº§u vÃ o hoáº·c prompt. QuÃ¡ trÃ¬nh
truy xuáº¥t nÃ y, Ä‘Æ°á»£c gá»i lÃ  truy xuáº¥t dÃ y Ä‘áº·c (dense-retrieval), dá»±a vÃ o cÃ¡c
embedding vÄƒn báº£n. ThÃ´ng thÆ°á»ng, viá»‡c táº¡o ra cÃ¡c embedding nÃ y Ä‘Æ°á»£c
giao cho má»™t LLM, mÃ  cÃ³ nhiá»u lá»±a chá»n do sá»± phÃ¡t triá»ƒn nhanh chÃ³ng
cá»§a lÄ©nh vá»±c nÃ y. Do Ä‘Ã³, viá»‡c lá»±a chá»n mÃ´ hÃ¬nh embedding phÃ¹ há»£p nháº¥t
tá»« má»™t loáº¡t cÃ¡c lá»±a chá»n cÃ³ sáºµn ná»•i lÃªn nhÆ° má»™t khÃ­a cáº¡nh quan trá»ng trong
viá»‡c phÃ¡t triá»ƒn há»‡ thá»‘ng RAG. ThÃ´ng tin Ä‘á»ƒ hÆ°á»›ng dáº«n lá»±a chá»n nÃ y hiá»‡n
táº¡i chá»§ yáº¿u bá»‹ giá»›i háº¡n á»Ÿ cÃ¡c chi tiáº¿t kiáº¿n trÃºc (cÃ³ thá»ƒ cÅ©ng khan hiáº¿m Ä‘Ã´i
khi do sá»± phá»• biáº¿n cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Ã³ng) vÃ  cÃ¡c benchmark hiá»‡u suáº¥t nhÆ°
Massive Text Embedding Benchmark (MTEB) [28].

ChÃºng tÃ´i cho ráº±ng má»™t phÃ¢n tÃ­ch vá» sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c embedding
Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c mÃ´ hÃ¬nh nÃ y sáº½ há»— trá»£ Ä‘Ã¡ng ká»ƒ quÃ¡ trÃ¬nh lá»±a chá»n mÃ´
hÃ¬nh nÃ y. Vá»›i sá»‘ lÆ°á»£ng lá»›n á»©ng viÃªn vÃ  quy mÃ´ ngÃ y cÃ ng tÄƒng cá»§a cÃ¡c mÃ´
hÃ¬nh, má»™t Ä‘Ã¡nh giÃ¡ thá»±c nghiá»‡m tá»« Ä‘áº§u vá» cháº¥t lÆ°á»£ng embedding cá»§a cÃ¡c
LLM nÃ y trÃªn má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ cÃ³ thá»ƒ phÃ¡t sinh chi phÃ­ Ä‘Ã¡ng ká»ƒ. ThÃ¡ch
thá»©c nÃ y trá»Ÿ nÃªn Ä‘áº·c biá»‡t ná»•i báº­t khi xá»­ lÃ½ cÃ¡c kho tÃ i liá»‡u quy mÃ´ lá»›n bao
gá»“m cÃ³ thá»ƒ hÃ ng triá»‡u tÃ i liá»‡u. Trong khi Ä‘iá»ƒm sá»‘ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘á»‘i cá»§a
cÃ¡c mÃ´ hÃ¬nh nÃ y trÃªn cÃ¡c bá»™ dá»¯ liá»‡u benchmark cung cáº¥p quan Ä‘iá»ƒm Ä‘Æ¡n
giáº£n hÃ³a vá» viá»‡c so sÃ¡nh má»™t giÃ¡ trá»‹ vÃ´ hÆ°á»›ng duy nháº¥t trÃªn má»™t loáº¡t cÃ¡c
nhiá»‡m vá»¥ xuÃ´i dÃ²ng, quan Ä‘iá»ƒm nhÆ° váº­y vá» sá»± tÆ°Æ¡ng Ä‘á»“ng mÃ´ hÃ¬nh cÃ³ thá»ƒ
bá» qua cÃ¡c sáº¯c thÃ¡i vá» hÃ nh vi tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c mÃ´ hÃ¬nh [15]. VÃ­ dá»¥, sá»±
khÃ¡c biá»‡t tuyá»‡t Ä‘á»‘i vá» precision@k giá»¯a hai há»‡ thá»‘ng truy xuáº¥t chá»‰ cung cáº¥p
chá»‰ bÃ¡o yáº¿u vá» sá»± chá»“ng chÃ©o cá»§a káº¿t quáº£ Ä‘Æ°á»£c truy xuáº¥t. ChÃºng tÃ´i cho
ráº±ng viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c cá»¥m mÃ´ hÃ¬nh cÃ³ hÃ nh vi tÆ°Æ¡ng tá»± sáº½ cho phÃ©p cÃ¡c
nhÃ  thá»±c hÃ nh xÃ¢y dá»±ng cÃ¡c nhÃ³m á»©ng viÃªn mÃ´ hÃ¬nh nhá» hÆ¡n nhÆ°ng Ä‘a
dáº¡ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡. NgoÃ i viá»‡c lá»±a chá»n mÃ´ hÃ¬nh, nhÆ° Ä‘Æ°á»£c nháº¥n máº¡nh bá»Ÿi
Klabunde et al., [14], phÃ¢n tÃ­ch nhÆ° váº­y cÅ©ng táº¡o Ä‘iá»u kiá»‡n thuáº­n lá»£i cho
viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c yáº¿u tá»‘ chung gÃ³p pháº§n vÃ o hiá»‡u suáº¥t máº¡nh máº½, viá»‡c káº¿t
há»£p mÃ´ hÃ¬nh dá»… dÃ ng hÆ¡n vÃ  phÃ¡t hiá»‡n cÃ¡c trÆ°á»ng há»£p tiá»m áº©n vá» viá»‡c tÃ¡i
sá»­ dá»¥ng mÃ´ hÃ¬nh trÃ¡i phÃ©p.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i phÃ¢n tÃ­ch cÃ¡c LLM khÃ¡c nhau vá» máº·t sá»±
tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c embedding mÃ  chÃºng táº¡o ra. PhÃ¢n tÃ­ch sá»± tÆ°Æ¡ng Ä‘á»“ng
cá»§a chÃºng tÃ´i phá»¥c vá»¥ nhÆ° má»™t framework Ä‘Ã¡nh giÃ¡ khÃ´ng giÃ¡m sÃ¡t cho
cÃ¡c mÃ´ hÃ¬nh embedding nÃ y, trÃ¡i ngÆ°á»£c vá»›i cÃ¡c benchmark hiá»‡u suáº¥t yÃªu
cáº§u dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n. ChÃºng tÃ´i thá»±c hiá»‡n Ä‘iá»u nÃ y tá»« gÃ³c Ä‘á»™ kÃ©p -
chÃºng tÃ´i trá»±c tiáº¿p so sÃ¡nh cÃ¡c embedding báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c thÆ°á»›c Ä‘o
sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n. NgoÃ i ra, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng mÃ´
hÃ¬nh cá»¥ thá»ƒ vá» tÃ¡c Ä‘á»™ng chá»©c nÄƒng cá»§a chÃºng Ä‘á»‘i vá»›i há»‡ thá»‘ng RAG tá»©c lÃ 
chÃºng tÃ´i xem xÃ©t má»©c Ä‘á»™ tÆ°Æ¡ng tá»± cá»§a káº¿t quáº£ Ä‘Æ°á»£c truy xuáº¥t. ÄÃ¡nh giÃ¡
cá»§a chÃºng tÃ´i táº­p trung vÃ o má»™t sá»‘ há» mÃ´ hÃ¬nh ná»•i báº­t, Ä‘á»ƒ phÃ¢n tÃ­ch sá»±
tÆ°Æ¡ng Ä‘á»“ng cáº£ trong vÃ  giá»¯a chÃºng. ChÃºng tÃ´i cÅ©ng so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh
Ä‘á»™c quyá»n (nhÆ° cá»§a arXiv:2407.08275v1  [cs.IR]  11 Jul 2024

--- TRANG 2 ---
Caspari et al.
OpenAI hoáº·c Cohere) vá»›i cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c thay
tháº¿ tÆ°Æ¡ng tá»± nháº¥t. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn nÄƒm
bá»™ dá»¯ liá»‡u benchmark phá»• biáº¿n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xem sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c
mÃ´ hÃ¬nh cÃ³ bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi viá»‡c lá»±a chá»n dá»¯ liá»‡u hay khÃ´ng. MÃ£ cá»§a
chÃºng tÃ´i cÃ³ sáºµn táº¡i https://github.com/casparil/embedding-model-similarity.

2 CÃ”NG TRÃŒNH LIÃŠN QUAN
CÃ¡c nghiÃªn cá»©u Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c máº¡ng nÆ¡-ron thuá»™c hai
loáº¡i chÃ­nh: loáº¡i Ä‘áº§u tiÃªn liÃªn quan Ä‘áº¿n viá»‡c so sÃ¡nh cÃ¡c kÃ­ch hoáº¡t cá»§a cÃ¡c
mÃ´ hÃ¬nh khÃ¡c nhau Ä‘Æ°á»£c táº¡o ra á»Ÿ báº¥t ká»³ cáº·p lá»›p nÃ o cho má»™t Ä‘áº§u vÃ o cá»¥
thá»ƒ (sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n), trong khi loáº¡i thá»© hai so sÃ¡nh cÃ¡c Ä‘áº§u ra cá»§a
mÃ´ hÃ¬nh (sá»± tÆ°Æ¡ng Ä‘á»“ng chá»©c nÄƒng). Raghu et al. [33] vÃ  Morcos et al. [26]
Ä‘á» xuáº¥t cÃ¡c thÆ°á»›c Ä‘o xÃ¢y dá»±ng trÃªn PhÃ¢n tÃ­ch TÆ°Æ¡ng quan ChÃ­nh táº¯c (CCA)
[11], má»™t ká»¹ thuáº­t thá»‘ng kÃª Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¬m má»‘i quan há»‡ tuyáº¿n tÃ­nh
giá»¯a hai táº­p há»£p biáº¿n báº±ng cÃ¡ch tá»‘i Ä‘a hÃ³a tÆ°Æ¡ng quan cá»§a chÃºng. CÃ¡c so
sÃ¡nh nhÆ° váº­y sá»­ dá»¥ng CCA hoáº·c cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y
trong má»™t sá»‘ cÃ´ng trÃ¬nh [6], [42], [4]. NgoÃ i cÃ¡c thÆ°á»›c Ä‘o dá»±a trÃªn CCA,
cÃ¡c cÃ´ng trÃ¬nh khÃ¡c cÅ©ng Ä‘Ã£ khÃ¡m phÃ¡ viá»‡c tÃ­nh toÃ¡n tÆ°Æ¡ng quan [21] vÃ 
thÃ´ng tin láº«n nhau [20] giá»¯a cÃ¡c nÆ¡-ron qua cÃ¡c máº¡ng. Kornblith et al.
[16] Ä‘á» xuáº¥t CÄƒn chá»‰nh NhÃ¢n tÃ¢m (CKA), mÃ  há» chá»©ng minh cáº£i thiá»‡n so
vá»›i má»™t sá»‘ thÆ°á»›c Ä‘o tÆ°Æ¡ng Ä‘á»“ng trong viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c lá»›p tÆ°Æ¡ng á»©ng cá»§a
cÃ¡c máº¡ng giá»‘ng há»‡t nhau vá»›i cÃ¡c khá»Ÿi táº¡o khÃ¡c nhau.

Má»™t loáº¡t Ä‘a dáº¡ng cÃ¡c Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng chá»©c nÄƒng cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c
khÃ¡m phÃ¡ trong tÃ i liá»‡u. Má»™t sá»‘ vÃ­ dá»¥ bao gá»“m model-stitching [2], [18],
[1], cÃ¡c thÆ°á»›c Ä‘o báº¥t Ä‘á»“ng giá»¯a cÃ¡c lá»›p Ä‘áº§u ra [25], [41], vÃ  Ä‘á»‹nh lÆ°á»£ng sá»±
tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c xÃ¡c suáº¥t Ä‘áº§u ra theo lá»›p [22]. ChÃºng tÃ´i hÆ°á»›ng ngÆ°á»i
Ä‘á»c Ä‘áº¿n bÃ i kháº£o sÃ¡t cá»§a Klabunde et al. [15] Ä‘á»ƒ cÃ³ cÃ¡i nhÃ¬n tá»•ng quan chi
tiáº¿t vá» cÃ¡c thÆ°á»›c Ä‘o sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n vÃ  chá»©c nÄƒng.

Gáº§n Ä‘Ã¢y, má»™t sá»‘ cÃ´ng trÃ¬nh cÅ©ng Ä‘Ã£ táº­p trung cá»¥ thá»ƒ vÃ o viá»‡c Ä‘Ã¡nh giÃ¡
sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c LLM. Trong khi Wu et al. [39] Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh
ngÃ´n ngá»¯ theo má»™t sá»‘ quan Ä‘iá»ƒm, nhÆ° sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n vÃ  cáº¥p Ä‘á»™
nÆ¡-ron cá»§a chÃºng, Ä‘Ã¡nh giÃ¡ cá»§a há» cÃ³ trÆ°á»›c khi giá»›i thiá»‡u lÃ n sÃ³ng gáº§n
Ä‘Ã¢y cá»§a cÃ¡c mÃ´ hÃ¬nh quy mÃ´ lá»›n. Freestone vÃ  Santu [9] xem xÃ©t sá»± tÆ°Æ¡ng
Ä‘á»“ng cá»§a embedding tá»«, vÃ  Ä‘Ã¡nh giÃ¡ xem cÃ¡c LLM cÃ³ khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ
vá»›i cÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a cá»• Ä‘iá»ƒn vá» máº·t biá»ƒu diá»…n cá»§a chÃºng hay khÃ´ng.
CÃ¡c cÃ´ng trÃ¬nh cá»§a Klabunde et al. [14] vÃ  Brown et al. [3] gáº§n Ä‘Ã¢y hÆ¡n,
vÃ  Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n cá»§a cÃ¡c LLM, vá»›i cÃ´ng trÃ¬nh sau cÅ©ng
xem xÃ©t sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c mÃ´ hÃ¬nh cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau trong
cÃ¹ng má»™t há» mÃ´ hÃ¬nh.

Pháº§n lá»›n tÃ i liá»‡u vá» Ä‘Ã¡nh giÃ¡ embedding LLM táº­p trung vÃ o hiá»‡u suáº¥t
cá»§a chÃºng trÃªn cÃ¡c nhiá»‡m vá»¥ xuÃ´i dÃ²ng, vá»›i cÃ¡c benchmark nhÆ° BEIR [35]
(cá»¥ thá»ƒ cho truy xuáº¥t) vÃ  MTEB [28] cung cáº¥p cÃ¡i nhÃ¬n thá»‘ng nháº¥t vá» cháº¥t
lÆ°á»£ng embedding qua cÃ¡c chá»‰ sá»‘ vÃ  bá»™ dá»¯ liá»‡u. CÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ
Ä‘Ã¢y chá»§ yáº¿u bao gá»“m cÃ¡c chá»‰ sá»‘ truy xuáº¥t thÃ´ng tin Ä‘iá»ƒn hÃ¬nh nhÆ° precision,
recall, vÃ  mean reciprocal rank á»Ÿ cÃ¡c ngÆ°á»¡ng cáº¯t nháº¥t Ä‘á»‹nh. Má»™t sá»‘ cÃ´ng
trÃ¬nh cá»¥ thá»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c thÃ nh pháº§n truy xuáº¥t trong bá»‘i cáº£nh RAG, nÆ¡i há»
sá»­ dá»¥ng bá»™ dá»¯ liá»‡u ngoÃ i nhá»¯ng bá»™ Ä‘Æ°á»£c bao gá»“m trong cÃ¡c benchmark [8]
hoáº·c nÆ¡i Ä‘Ã¡nh giÃ¡ bao gá»“m cÃ¡c khÃ­a cáº¡nh khÃ¡c cá»§a bá»™ truy xuáº¥t ngoÃ i mÃ´
hÃ¬nh embedding Ä‘Æ°á»£c sá»­ dá»¥ng [34]. Má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c, khÃ´ng dá»±a
vÃ o nhÃ£n ground-truth, Ä‘Æ°á»£c Ä‘Æ°a ra bá»Ÿi framework Retrieval Augmented
Generation Assessment (RAGAS), sá»­ dá»¥ng má»™t LLM Ä‘á»ƒ xÃ¡c Ä‘á»‹nh tá»· lá»‡ cÃ¡c
cÃ¢u trong ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u tráº£ lá»i Ä‘Æ°á»£c táº¡o
ra [7]. Theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, khÃ´ng cÃ³

Báº£ng 1: CÃ¡c bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o embedding vá»›i
sá»‘ lÆ°á»£ng truy váº¥n vÃ  kÃ­ch thÆ°á»›c kho tÃ i liá»‡u cá»§a chÃºng.
TÃªn Bá»™ dá»¯ liá»‡u Truy váº¥n Kho tÃ i liá»‡u
TREC-COVID 50 171k
NFCorpus 323 3.6k
FiQA-2018 648 57k
ArguAna 1406 8.67k
SciFact 300 5k

cÃ´ng trÃ¬nh nÃ o Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c mÃ´ hÃ¬nh embedding tá»« gÃ³c
Ä‘á»™ truy xuáº¥t.

3 PHÆ¯Æ NG PHÃP
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng mÃ´ hÃ¬nh embedding báº±ng hai cÃ¡ch tiáº¿p
cáº­n. CÃ¡ch Ä‘áº§u tiÃªn trá»±c tiáº¿p so sÃ¡nh cÃ¡c embedding cá»§a cÃ¡c Ä‘oáº¡n vÄƒn báº£n
Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c mÃ´ hÃ¬nh. CÃ¡ch tiáº¿p cáº­n thá»© hai cá»¥ thá»ƒ cho bá»‘i cáº£nh
RAG, nÆ¡i chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a káº¿t quáº£ Ä‘Æ°á»£c truy xuáº¥t
cho má»™t truy váº¥n nháº¥t Ä‘á»‹nh. CÃ¡c cÃ¡ch tiáº¿p cáº­n nÃ y Ä‘Æ°á»£c tháº£o luáº­n chi tiáº¿t
trong cÃ¡c pháº§n sau.

3.1 Sá»± TÆ°Æ¡ng Ä‘á»“ng Embedding Theo cáº·p
CÃ³ má»™t sá»‘ chá»‰ sá»‘ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong tÃ i liá»‡u Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng
biá»ƒu diá»…n [15]. Nhiá»u chá»‰ sá»‘ nÃ y yÃªu cáº§u cÃ¡c khÃ´ng gian biá»ƒu diá»…n cá»§a cÃ¡c
embedding Ä‘Æ°á»£c so sÃ¡nh pháº£i Ä‘Æ°á»£c cÄƒn chá»‰nh vÃ /hoáº·c tÃ­nh chiá»u cá»§a cÃ¡c
embedding qua cÃ¡c mÃ´ hÃ¬nh pháº£i giá»‘ng há»‡t nhau. Äá»ƒ trÃ¡nh cÃ¡c rÃ ng buá»™c
nÃ y, chÃºng tÃ´i chá»n CÄƒn chá»‰nh NhÃ¢n tÃ¢m (CKA) [16] vá»›i nhÃ¢n tuyáº¿n tÃ­nh
lÃ m thÆ°á»›c Ä‘o tÆ°Æ¡ng Ä‘á»“ng cá»§a chÃºng tÃ´i.

ThÆ°á»›c Ä‘o nÃ y tÃ­nh toÃ¡n sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a hai táº­p há»£p embedding trong
hai bÆ°á»›c. Äáº§u tiÃªn, cho má»™t táº­p há»£p embedding, Ä‘iá»ƒm sá»‘ tÆ°Æ¡ng Ä‘á»“ng theo
cáº·p giá»¯a táº¥t cáº£ cÃ¡c má»¥c trong táº­p há»£p nÃ y Ä‘Æ°á»£c tÃ­nh toÃ¡n báº±ng cÃ¡ch sá»­
dá»¥ng hÃ m nhÃ¢n. Do Ä‘Ã³, hÃ ng k cá»§a ma tráº­n tÆ°Æ¡ng Ä‘á»“ng káº¿t quáº£ chá»©a cÃ¡c
má»¥c Ä‘áº¡i diá»‡n cho sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a embedding k vÃ  táº¥t cáº£ cÃ¡c embedding
khÃ¡c, bao gá»“m cáº£ chÃ­nh nÃ³. Viá»‡c tÃ­nh toÃ¡n hai ma tráº­n tÆ°Æ¡ng Ä‘á»“ng embedding
nhÆ° váº­y cho cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau vá»›i cÃ¹ng sá»‘ lÆ°á»£ng embedding sau Ä‘Ã³
dáº«n Ä‘áº¿n hai ma tráº­n E vÃ  E' cÃ³ kÃ­ch thÆ°á»›c phÃ¹ há»£p. ChÃºng Ä‘Æ°á»£c so sÃ¡nh
trá»±c tiáº¿p trong bÆ°á»›c thá»© hai vá»›i TiÃªu chÃ­ Äá»™c láº­p Hilbert-Schmidt (HSIC)
[10] báº±ng cÃ´ng thá»©c sau:

ğ¶ğ¾ğ´(ğ¸,ğ¸â€²)=ğ»ğ‘†ğ¼ğ¶(ğ¸,ğ¸â€²)âˆšï¸
ğ»ğ‘†ğ¼ğ¶(ğ¸,ğ¸)ğ»ğ‘†ğ¼ğ¶(ğ¸â€²,ğ¸â€²)(1)

Äiá»ƒm sá»‘ tÆ°Æ¡ng Ä‘á»“ng káº¿t quáº£ Ä‘Æ°á»£c giá»›i háº¡n trong khoáº£ng [0, 1] vá»›i Ä‘iá»ƒm
sá»‘ 1 biá»ƒu thá»‹ cÃ¡c biá»ƒu diá»…n tÆ°Æ¡ng Ä‘Æ°Æ¡ng. CKA giáº£ Ä‘á»‹nh ráº±ng cÃ¡c biá»ƒu diá»…n
Ä‘Æ°á»£c cÄƒn giá»¯a theo trung bÃ¬nh.

3.2 Sá»± TÆ°Æ¡ng Ä‘á»“ng Truy xuáº¥t
Trong khi viá»‡c so sÃ¡nh theo cáº·p cÃ¡c embedding cung cáº¥p thÃ´ng tin chi tiáº¿t
vá» sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c há»c bá»Ÿi cÃ¡c mÃ´ hÃ¬nh nÃ y, nÃ³
khÃ´ng Ä‘á»§ Ä‘á»ƒ Ä‘á»‹nh lÆ°á»£ng sá»± tÆ°Æ¡ng Ä‘á»“ng trong káº¿t quáº£ khi cÃ¡c mÃ´ hÃ¬nh
embedding nÃ y Ä‘Æ°á»£c triá»ƒn khai cho cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ. Do Ä‘Ã³, trong
bá»‘i cáº£nh há»‡ thá»‘ng RAG, chÃºng tÃ´i xem xÃ©t sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c Ä‘oáº¡n
vÄƒn báº£n Ä‘Æ°á»£c truy xuáº¥t cho má»™t truy váº¥n nháº¥t Ä‘á»‹nh, khi cÃ¡c mÃ´ hÃ¬nh
embedding khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng. NhÆ° bÆ°á»›c Ä‘áº§u tiÃªn, cho má»™t bá»™ dá»¯
liá»‡u nháº¥t Ä‘á»‹nh, chÃºng tÃ´i táº¡o ra cÃ¡c embedding cá»§a

--- TRANG 3 ---
VÆ°á»£t Ra NgoÃ i CÃ¡c BÃ i Kiá»ƒm Tra: ÄÃ¡nh GiÃ¡ Sá»± TÆ°Æ¡ng Äá»“ng MÃ´ HÃ¬nh Embedding cho Há»‡ Thá»‘ng TÄƒng CÆ°á»ng Truy Xuáº¥t Sinh VÄƒn Báº£n

Báº£ng 2: ChÃºng tÃ´i so sÃ¡nh má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ tá»« cÃ¡c há» khÃ¡c nhau cÅ©ng nhÆ° cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n vá»›i hiá»‡u suáº¥t khÃ¡c nhau trÃªn MTEB.

MÃ´ hÃ¬nh | Chiá»u embedding | Token tá»‘i Ä‘a | Trung bÃ¬nh MTEB | MÃ£ nguá»“n má»Ÿ
SFR-Embedding-Mistral | 4096 | 32768 | 67.56 | âœ“
mxbai-embed-large-v1 | 1024 | 512 | 64.68 | âœ“
UAE-Large-V1 | 1024 | 512 | 64.64 | âœ“
text-embedding-3-large | 3072 | 8191 | 64.59 | âœ—
Cohere embed-english-v3.0 | 1024 | 512 | 64.47 | âœ—
bge-large-en-v1.5 | 1024 | 512 | 64.23 | âœ“
bge-base-en-v1.5 | 768 | 512 | 63.55 | âœ“
gte-large | 1024 | 512 | 63.13 | âœ“
gte-base | 768 | 512 | 62.39 | âœ“
text-embedding-3-small | 1536 | 8191 | 62.26 | âœ—
e5-large-v2 | 1024 | 512 | 62.25 | âœ“
bge-small-en-v1.5 | 384 | 512 | 62.17 | âœ“
e5-base-v2 | 768 | 512 | 61.5 | âœ“
gte-small | 384 | 512 | 61.36 | âœ“
e5-small-v2 | 384 | 512 | 59.93 | âœ“
gtr-t5-large | 768 | 512 | 58.28 | âœ“
sentence-t5-large | 768 | 512 | 57.06 | âœ“
gtr-t5-base | 768 | 512 | 56.19 | âœ“
sentence-t5-base | 768 | 512 | 55.27 | âœ“

cÃ¡c truy váº¥n vÃ  Ä‘oáº¡n tÃ i liá»‡u vá»›i má»—i mÃ´ hÃ¬nh embedding. Sau Ä‘Ã³ chÃºng
tÃ´i truy xuáº¥t ğ‘˜ embedding tÆ°Æ¡ng tá»± nháº¥t vá» máº·t tÆ°Æ¡ng tá»± cosine cho má»™t
truy váº¥n cá»¥ thá»ƒ. VÃ¬ cÃ¡c embedding nÃ y tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘oáº¡n vÄƒn báº£n
cá»¥ thá»ƒ, chÃºng tÃ´i rÃºt ra cÃ¡c táº­p há»£p Ä‘oáº¡n Ä‘Æ°á»£c truy xuáº¥t C vÃ  C' cho má»™t
cáº·p mÃ´ hÃ¬nh. Äá»ƒ Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c táº­p há»£p nÃ y, chÃºng tÃ´i
sá»­ dá»¥ng há»‡ sá»‘ tÆ°Æ¡ng Ä‘á»“ng Jaccard nhÆ° sau:

ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘(ğ¶,ğ¶â€²)=|ğ¶âˆ©ğ¶â€²|/|ğ¶âˆªğ¶â€²|(2)

á» Ä‘Ã¢y, |ğ¶âˆ©ğ¶â€²| tÆ°Æ¡ng á»©ng vá»›i sá»± chá»“ng chÃ©o trong cÃ¡c Ä‘oáº¡n vÄƒn báº£n báº±ng
cÃ¡ch Ä‘áº¿m táº§n sá»‘ hai mÃ´ hÃ¬nh truy xuáº¥t cÃ¹ng cÃ¡c Ä‘oáº¡n. TÆ°Æ¡ng tá»±, chÃºng
ta cÃ³ thá»ƒ tÃ­nh toÃ¡n há»£p |ğ¶âˆªğ¶â€²|, tÆ°Æ¡ng á»©ng vá»›i táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn báº£n
Ä‘Æ°á»£c truy xuáº¥t, chá»‰ Ä‘áº¿m cÃ¡c Ä‘oáº¡n cÃ³ máº·t trong cáº£ hai táº­p há»£p má»™t láº§n. Äiá»ƒm
sá»‘ káº¿t quáº£ Ä‘Æ°á»£c giá»›i háº¡n trong khoáº£ng [0, 1] vá»›i 1 biá»ƒu thá»‹ ráº±ng cáº£ hai mÃ´
hÃ¬nh Ä‘á»u truy xuáº¥t cÃ¹ng táº­p há»£p cÃ¡c Ä‘oáº¡n vÄƒn báº£n.

Trong khi tÆ°Æ¡ng Ä‘á»“ng Jaccard tÃ­nh toÃ¡n tá»· lá»‡ pháº§n trÄƒm mÃ  hai táº­p há»£p
chá»“ng chÃ©o, nÃ³ bá» qua thá»© tá»± trong cÃ¡c táº­p há»£p. Máº·t khÃ¡c, tÆ°Æ¡ng Ä‘á»“ng
rank [36] xem xÃ©t thá»© tá»± cá»§a cÃ¡c pháº§n tá»­ chung, vá»›i cÃ¡c pháº§n tá»­ gáº§n nhau
hÆ¡n cÃ³ tÃ¡c Ä‘á»™ng cao hÆ¡n Ä‘áº¿n Ä‘iá»ƒm sá»‘. ThÆ°á»›c Ä‘o gÃ¡n rank cho cÃ¡c Ä‘oáº¡n
vÄƒn báº£n chung theo Ä‘á»™ tÆ°Æ¡ng tá»± cá»§a chÃºng vá»›i truy váº¥n, tá»©c lÃ  ğ‘Ÿğ¶(ğ‘—)=ğ‘›
náº¿u Ä‘oáº¡n ğ‘— lÃ  káº¿t quáº£ truy xuáº¥t top-ğ‘› cho truy váº¥n. CÃ¡c rank sau Ä‘Ã³ Ä‘Æ°á»£c
so sÃ¡nh báº±ng:

ğ‘…ğ‘ğ‘›ğ‘˜(ğ‘Ÿğ¶(ğ‘—),ğ‘Ÿğ¶â€²(ğ‘—))=2/((1+|ğ‘Ÿğ¶(ğ‘—)âˆ’ğ‘Ÿğ¶â€²(ğ‘—)|)(ğ‘Ÿğ¶(ğ‘—)+ğ‘Ÿğ¶â€²(ğ‘—)))(3)

Vá»›i Ä‘iá»u nÃ y, tÆ°Æ¡ng Ä‘á»“ng rank cho hai táº­p há»£p Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c truy
xuáº¥t C, C' Ä‘Æ°á»£c tÃ­nh nhÆ°:

ğ‘…ğ‘ğ‘›ğ‘˜ğ‘†ğ‘–ğ‘š(ğ¶,ğ¶â€²)=1/ğ»(|ğ¶âˆ©ğ¶â€²|)âˆ‘ï¸_{ğ‘—âˆˆ|ğ¶âˆ©ğ¶â€²|}ğ‘…ğ‘ğ‘›ğ‘˜(ğ‘Ÿğ¶(ğ‘—),ğ‘Ÿğ¶â€²(ğ‘—))(4)

vá»›i ğ»(|ğ¶âˆ©ğ¶â€²|)=Î£_{ğ‘˜=1}^{|ğ¶âˆ©ğ¶â€²|}1/ğ‘˜ biá»ƒu thá»‹ sá»‘ Ä‘iá»u hÃ²a thá»© K, chuáº©n hÃ³a
Ä‘iá»ƒm sá»‘. Giá»‘ng nhÆ° cÃ¡c thÆ°á»›c Ä‘o khÃ¡c, tÆ°Æ¡ng Ä‘á»“ng rank Ä‘Æ°á»£c giá»›i háº¡n
trong khoáº£ng [0, 1] vá»›i 1 biá»ƒu thá»‹ ráº±ng táº¥t cáº£ cÃ¡c rank Ä‘á»u giá»‘ng há»‡t nhau.

4 THIáº¾T Láº¬P THÃ NGHIá»†M
CÃ¡c Ä‘oáº¡n sau mÃ´ táº£ viá»‡c lá»±a chá»n bá»™ dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i,
cÃ¹ng vá»›i cÃ¡c chi tiáº¿t vá» viá»‡c thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m.

VÃ¬ chÃºng tÃ´i táº­p trung vÃ o thÃ nh pháº§n truy xuáº¥t cá»§a há»‡ thá»‘ng RAG, chÃºng
tÃ´i chá»n nÄƒm bá»™ dá»¯ liá»‡u cÃ³ sáºµn cÃ´ng khai tá»« benchmark BEIR [35]. VÃ¬ viá»‡c
táº¡o embedding cho cÃ¡c bá»™ dá»¯ liá»‡u lá»›n lÃ  má»™t quÃ¡ trÃ¬nh tá»‘n thá»i gian, Ä‘áº·c
biá»‡t lÃ  Ä‘á»‘i vá»›i má»™t sá»‘ lÆ°á»£ng lá»›n mÃ´ hÃ¬nh, chÃºng tÃ´i chá»n nÄƒm bá»™ dá»¯ liá»‡u
nhá» hÆ¡n tá»« benchmark. CÃ¡ch tiáº¿p cáº­n nÃ y cho phÃ©p chÃºng tÃ´i so sÃ¡nh cÃ¡c
embedding Ä‘Æ°á»£c táº¡o ra bá»Ÿi nhiá»u mÃ´ hÃ¬nh khÃ¡c nhau trong khi Ä‘á»“ng thá»i
cho phÃ©p chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng Ä‘á»“ng embedding qua cÃ¡c bá»™ dá»¯ liá»‡u.
Tá»•ng quan vá» cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1. Äá»‘i vá»›i má»—i bá»™
dá»¯ liá»‡u, chÃºng tÃ´i táº¡o embedding báº±ng cÃ¡ch chia tÃ i liá»‡u thÃ nh cÃ¡c Ä‘oáº¡n
vÄƒn báº£n sao cho má»—i Ä‘oáº¡n chá»©a 256 token. CÃ¡c vector embedding Ä‘Æ°á»£c
lÆ°u trá»¯ vá»›i Chroma DB [12], má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u embedding mÃ£ nguá»“n má»Ÿ.
Äá»‘i vá»›i má»—i vector, chÃºng tÃ´i cÅ©ng lÆ°u trá»¯ thÃ´ng tin vá» tÃ i liá»‡u vÃ  ID Ä‘oáº¡n
vÄƒn báº£n mÃ  nÃ³ mÃ£ hÃ³a Ä‘á»ƒ cÃ³ thá»ƒ khá»›p cÃ¡c embedding Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c
mÃ´ hÃ¬nh khÃ¡c nhau Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.

Äá»‘i vá»›i viá»‡c lá»±a chá»n mÃ´ hÃ¬nh, chÃºng tÃ´i chá»§ yáº¿u sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh
cÃ³ sáºµn cÃ´ng khai tá»« báº£ng xáº¿p háº¡ng MTEB [28]. ChÃºng tÃ´i khÃ´ng chá»‰ Ä‘Æ¡n
giáº£n chá»n cÃ¡c mÃ´ hÃ¬nh cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t trÃªn báº£ng xáº¿p háº¡ng; thay vÃ o
Ä‘Ã³, cÃ¡c lá»±a chá»n cá»§a chÃºng tÃ´i bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi má»™t sá»‘ yáº¿u tá»‘. Thá»© nháº¥t,
chÃºng tÃ´i táº­p trung vÃ o viá»‡c phÃ¢n tÃ­ch sá»± tÆ°Æ¡ng Ä‘á»“ng trong vÃ  giá»¯a cÃ¡c
há» mÃ´ hÃ¬nh vÃ  chá»n cÃ¡c mÃ´ hÃ¬nh thuá»™c cÃ¡c há» e5 [37], t5 [29,30], bge [40],
vÃ  gte [23]. Thá»© hai, chÃºng tÃ´i nháº­n ra ráº±ng ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ quan tÃ¢m
Ä‘áº¿n viá»‡c trÃ¡nh cÃ¡c chÃ­nh sÃ¡ch tráº£ phÃ­ theo token cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n
báº±ng cÃ¡ch xÃ¡c Ä‘á»‹nh cÃ¡c thay tháº¿ mÃ£ nguá»“n má»Ÿ tÆ°Æ¡ng tá»±. Do Ä‘Ã³, chÃºng
tÃ´i chá»n cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n cÃ³ hiá»‡u suáº¥t cao,

--- TRANG 4 ---
Caspari et al.

[HÃ¬nh áº£nh ma tráº­n tÆ°Æ¡ng Ä‘á»“ng CKA vá»›i cÃ¡c sá»‘ liá»‡u tá»« 0.61 Ä‘áº¿n 1.00]

HÃ¬nh 1: TÆ°Æ¡ng Ä‘á»“ng CKA trung bÃ¬nh trÃªn táº¥t cáº£ nÄƒm bá»™ dá»¯ liá»‡u.
CÃ¡c mÃ´ hÃ¬nh cÃ³ xu hÆ°á»›ng tÆ°Æ¡ng tá»± nháº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh thuá»™c
há» cá»§a chÃ­nh chÃºng, máº·c dÃ¹ má»™t sá»‘ máº«u liÃªn há» thÃº vá»‹ cÅ©ng
cÃ³ thá»ƒ nhÃ¬n tháº¥y.

hai tá»« OpenAI (text-embedding-3-large vÃ  -small) [31] vÃ  má»™t tá»« Cohere
(Cohere embed-english-v3.0) [5]. ChÃºng tÃ´i cÅ©ng so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh
mxbai-embed-large-v1 (mxbai) [17] vÃ  UAE-Large-V1 (UAE) [19], khÃ´ng
chá»‰ bÃ¡o cÃ¡o hiá»‡u suáº¥t ráº¥t tÆ°Æ¡ng tá»± trÃªn MTEB mÃ  cÃ²n cÃ³ chiá»u embedding,
kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  sá»­ dá»¥ng bá»™ nhá»› giá»‘ng há»‡t nhau. Cuá»‘i cÃ¹ng, chÃºng
tÃ´i bao gá»“m SFR-Embedding-Mistral (Mistral) [24] nhÆ° mÃ´ hÃ¬nh cÃ³ hiá»‡u
suáº¥t tá»‘t nháº¥t trÃªn báº£ng xáº¿p háº¡ng táº¡i thá»i Ä‘iá»ƒm thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i.
Tá»•ng quan chi tiáº¿t vá» táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c chá»n cÃ³ thá»ƒ tháº¥y trong Báº£ng 2.

Äá»ƒ so sÃ¡nh sá»± tÆ°Æ¡ng Ä‘á»“ng embedding qua cÃ¡c mÃ´ hÃ¬nh vÃ  bá»™ dá»¯ liá»‡u,
chÃºng tÃ´i Ã¡p dá»¥ng cÃ¡c chiáº¿n lÆ°á»£c khÃ¡c nhau tÃ¹y thuá»™c vÃ o thÆ°á»›c Ä‘o tÆ°Æ¡ng
Ä‘á»“ng. ChÃºng tÃ´i Ã¡p dá»¥ng CKA báº±ng cÃ¡ch truy xuáº¥t táº¥t cáº£ cÃ¡c embedding
Ä‘Æ°á»£c táº¡o bá»Ÿi má»™t mÃ´ hÃ¬nh, khá»›p cÃ¡c embedding báº±ng ID tÃ i liá»‡u vÃ  Ä‘oáº¡n
vÄƒn báº£n cá»§a chÃºng vÃ  sau Ä‘Ã³ tÃ­nh toÃ¡n sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a chÃºng cho má»—i
bá»™ dá»¯ liá»‡u trong sá»‘ nÄƒm bá»™. Äá»‘i vá»›i tÆ°Æ¡ng Ä‘á»“ng Jaccard vÃ  rank, chÃºng
tÃ´i sá»­ dá»¥ng lá»›p NearestNeighbor cá»§a sklearn [32] Ä‘á»ƒ xÃ¡c Ä‘á»‹nh káº¿t quáº£ truy
xuáº¥t top-ğ‘˜. ChÃºng tÃ´i tÃ­nh toÃ¡n Ä‘iá»ƒm Jaccard vÃ  rank cho má»—i bá»™ dá»¯ liá»‡u,
láº¥y trung bÃ¬nh trÃªn 25 truy váº¥n. Äá»‘i vá»›i bá»™ dá»¯ liá»‡u NFCorpus, chÃºng tÃ´i
tÃ­nh toÃ¡n sá»± tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t cho táº¥t cáº£ ğ‘˜ cÃ³ thá»ƒ, tá»©c lÃ  sá»­ dá»¥ng táº¥t
cáº£ cÃ¡c embedding Ä‘Æ°á»£c táº¡o ra cho bá»™ dá»¯ liá»‡u. VÃ¬ viá»‡c tÃ­nh toÃ¡n sá»± tÆ°Æ¡ng
Ä‘á»“ng cho má»—i ğ‘˜ cÃ³ thá»ƒ lÃ  tÃ­nh toÃ¡n Ä‘áº¯t Ä‘á», chÃºng tÃ´i khÃ´ng láº·p láº¡i Ä‘iá»u
nÃ y cho cÃ¡c bá»™ dá»¯ liá»‡u cÃ²n láº¡i vÃ  thay vÃ o Ä‘Ã³ chá»n giÃ¡ trá»‹ ğ‘˜ nhá» hÆ¡n. HÆ¡n
ná»¯a, vÃ¬ chá»‰ má»™t sá»‘ lÆ°á»£ng háº¡n cháº¿ káº¿t quáº£ Ä‘Æ°á»£c cung cáº¥p lÃ m ngá»¯ cáº£nh cho
mÃ´ hÃ¬nh sinh, viá»‡c phÃ¢n tÃ­ch sá»± tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t á»Ÿ giÃ¡ trá»‹ ğ‘˜ tháº¥p, vÃ­
dá»¥ top-10, lÃ  quan trá»ng nháº¥t.

VÃ¬ chÃºng tÃ´i quan tÃ¢m Ä‘áº¿n viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c cá»¥m mÃ´ hÃ¬nh tÆ°Æ¡ng tá»±, chÃºng
tÃ´i cÅ©ng thá»±c hiá»‡n phÃ¢n cá»¥m phÃ¢n cáº¥p trÃªn cÃ¡c giÃ¡ trá»‹ heatmap báº±ng cÃ¡ch
sá»­ dá»¥ng Seaborn [38]. Pháº§n sau mÃ´ táº£ káº¿t quáº£ Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i cho
cÃ¡c thÆ°á»›c Ä‘o khÃ¡c nhau.

[HÃ¬nh áº£nh biá»ƒu Ä‘á»“ so sÃ¡nh rank similarity]

HÃ¬nh 2: TÆ°Æ¡ng Ä‘á»“ng rank trÃªn táº¥t cáº£ ğ‘˜ trÃªn NFCorpus, so sÃ¡nh
gte-large vá»›i táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh khÃ¡c. Äiá»ƒm sá»‘ cao nháº¥t vÃ 
biáº¿n Ä‘á»•i nhiá»u nháº¥t cho ğ‘˜ nhá», nhÆ°ng sau Ä‘Ã³ giáº£m nhanh
trÆ°á»›c khi á»•n Ä‘á»‹nh cho ğ‘˜ lá»›n hÆ¡n.

5 Káº¾T QUáº¢
Äá»ƒ Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ tÆ°Æ¡ng tá»± cá»§a cÃ¡c embedding Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c mÃ´
hÃ¬nh khÃ¡c nhau, Ä‘áº§u tiÃªn chÃºng tÃ´i sáº½ xem xÃ©t cÃ¡c há» mÃ´ hÃ¬nh, kiá»ƒm tra
xem Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng theo cáº·p vÃ  top-k cá»§a chÃºng cÃ³ cao nháº¥t trong há»
cá»§a chÃºng hay khÃ´ng. Tiáº¿p theo, chÃºng tÃ´i sáº½ xÃ¡c Ä‘á»‹nh cÃ¡c mÃ´ hÃ¬nh mÃ£
nguá»“n má»Ÿ tÆ°Æ¡ng tá»± nháº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n Ä‘Æ°á»£c chá»n cá»§a chÃºng
tÃ´i.

5.1 CÃ¡c Cá»¥m Ná»™i vÃ  LiÃªn Há»
Viá»‡c so sÃ¡nh trá»±c tiáº¿p cÃ¡c embedding vá»›i CKA cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng
cao qua háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh, máº·c dÃ¹ cÃ³ má»™t sá»‘ phÆ°Æ¡ng sai. CÃ¡c Ä‘iá»ƒm sá»‘
nÃ y cho phÃ©p chÃºng tÃ´i xÃ¡c Ä‘á»‹nh má»™t sá»‘ cá»¥m mÃ´ hÃ¬nh nháº¥t Ä‘á»‹nh. HÃ¬nh 1
hiá»ƒn thá»‹ Ä‘iá»ƒm sá»‘ CKA theo cáº·p cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tÃ­nh trung bÃ¬nh
trÃªn nÄƒm bá»™ dá»¯ liá»‡u. NhÆ° mong Ä‘á»£i, Ä‘iá»ƒm sá»‘ cho háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh cao
nháº¥t trong há» cá»§a chÃ­nh chÃºng. Äiá»u nÃ y Ä‘Ãºng cho cÃ¡c mÃ´ hÃ¬nh gtr-t5,
sentence-t5 vÃ  text-embedding-3 (OpenAI). Máº·c dÃ¹ cÃ¡c mÃ´ hÃ¬nh sentence-
t5 vÃ  gtr-t5 cÃ³ liÃªn quan cháº·t cháº½, chÃºng khÃ´ng thá»ƒ hiá»‡n sá»± tÆ°Æ¡ng Ä‘á»“ng
cao hÆ¡n Ä‘Ã¡ng ká»ƒ vá»›i nhau so vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ²n láº¡i.

Tá»« gÃ³c Ä‘á»™ liÃªn há», chÃºng tÃ´i quan sÃ¡t tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng cao giá»¯a cÃ¡c
mÃ´ hÃ¬nh bge vÃ  gte. Äá»‘i vá»›i má»™t sá»‘ mÃ´ hÃ¬nh trong hai há» nÃ y, thÃº vá»‹ lÃ 
Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t láº¡i tÆ°Æ¡ng á»©ng vá»›i cÃ¡c Ä‘á»‘i tÃ¡c liÃªn há» cÃ³ chiá»u
embedding phÃ¹ há»£p hÆ¡n lÃ  vá»›i cÃ¡c mÃ´ hÃ¬nh trong cÃ¹ng há». Cá»¥ thá»ƒ, gte-
small bÃ¡o cÃ¡o sá»± tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t vá»›i bge-small vÃ  gte-base vá»›i bge-
base. Máº·t khÃ¡c, gte-large cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng cao hÆ¡n má»™t chÃºt vá»›i
bge-base hÆ¡n bge-large vÃ  do Ä‘Ã³ vá»›i má»™t mÃ´ hÃ¬nh cÃ³ chiá»u embedding
tháº¥p hÆ¡n. Má»™t cá»¥m liÃªn há» khÃ¡c Ä‘Æ°á»£c hÃ¬nh thÃ nh bá»Ÿi ba mÃ´ hÃ¬nh cÃ³ Ä‘iá»ƒm
CKA cao nháº¥t tá»•ng thá»ƒ, cá»¥ thá»ƒ lÃ  UAE, mxbai vÃ  bge-large, cÃ³ Ä‘iá»ƒm sá»‘
cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng embedding gáº§n nhÆ° hoÃ n háº£o. Trong

--- TRANG 5 ---
VÆ°á»£t Ra NgoÃ i CÃ¡c BÃ i Kiá»ƒm Tra: ÄÃ¡nh GiÃ¡ Sá»± TÆ°Æ¡ng Äá»“ng MÃ´ HÃ¬nh Embedding cho Há»‡ Thá»‘ng TÄƒng CÆ°á»ng Truy Xuáº¥t Sinh VÄƒn Báº£n

[HÃ¬nh áº£nh biá»ƒu Ä‘á»“ Jaccard similarity cho bge-large vÃ  gte-large]

HÃ¬nh 3: TÆ°Æ¡ng Ä‘á»“ng Jaccard trÃªn táº¥t cáº£ ğ‘˜ trÃªn NFCorpus, so sÃ¡nh bge-large (a) vÃ  gte-large (b) vá»›i táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh khÃ¡c.
Trong khi bge-large cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng cao vá»›i UAE-Large-v1 vÃ  mxbai-embed-large-v1, Ä‘iá»ƒm sá»‘ cho gte-large Ä‘Æ°á»£c
nhÃ³m gáº§n nhau hÆ¡n nhiá»u. TÆ°Æ¡ng Ä‘á»“ng Jaccard dÆ°á»ng nhÆ° khÃ´ng á»•n Ä‘á»‹nh nháº¥t Ä‘á»‘i vá»›i cÃ¡c giÃ¡ trá»‹ nhá» cá»§a ğ‘˜, thÆ°á»ng Ä‘Æ°á»£c
chá»n cho cÃ¡c nhiá»‡m vá»¥ truy xuáº¥t.

thá»±c táº¿, Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cá»§a bge-large vá»›i hai mÃ´ hÃ¬nh nÃ y cao hÆ¡n nhiá»u
so vá»›i vá»›i cÃ¡c mÃ´ hÃ¬nh bge khÃ¡c.

Chuyá»ƒn sá»± chÃº Ã½ cá»§a chÃºng tÃ´i Ä‘áº¿n sá»± tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t top-ğ‘˜, cÃ¡c
cá»¥m thay Ä‘á»•i tÃ¹y thuá»™c vÃ o giÃ¡ trá»‹ ğ‘˜. HÃ¬nh 3 minh há»a cÃ¡ch tÆ°Æ¡ng Ä‘á»“ng
Jaccard phÃ¡t triá»ƒn theo ğ‘˜ trÃªn NFCorpus. Biá»ƒu Ä‘á»“ Ä‘áº§u tiÃªn hiá»ƒn thá»‹ Ä‘iá»ƒm
Jaccard giá»¯a bge-large vÃ  táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh khÃ¡c, trong khi biá»ƒu Ä‘á»“ thá»©
hai minh há»a Ä‘iá»ƒm sá»‘ cho gte-large. Äá»‘i vá»›i ğ‘˜ cá»±c tháº¥p, chÃºng tÃ´i quan
sÃ¡t má»™t sá»‘ Ä‘á»‰nh cho gáº§n nhÆ° táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh, theo sau lÃ  sá»± giáº£m Ä‘Ã¡ng
chÃº Ã½ vá» tÆ°Æ¡ng Ä‘á»“ng. Táº¥t nhiÃªn, Ä‘á»‘i vá»›i ğ‘˜ lá»›n hÆ¡n, cÃ¡c Ä‘iá»ƒm sá»‘ há»™i tá»¥ vá»
má»™t. Kháº³ng Ä‘á»‹nh láº¡i cÃ¡c quan sÃ¡t trÆ°á»›c Ä‘Ã³ vá»›i chá»‰ sá»‘ CKA, bge-large thá»ƒ
hiá»‡n sá»± tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t cao vá»›i UAE vÃ  mxbai. Sá»± tÆ°Æ¡ng Ä‘á»“ng vá»›i
cÃ¡c mÃ´ hÃ¬nh cÃ²n láº¡i tháº¥p hÆ¡n nhiá»u, vá»›i Ä‘iá»ƒm sá»‘ cao nháº¥t cho bge-base
vÃ  bge-small Ä‘á»‘i vá»›i ğ‘˜ lá»›n hÆ¡n. Tuy nhiÃªn, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i ğ‘˜ nhá», cÃ³
phÆ°Æ¡ng sai cao trong Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng, vá»›i cÃ¡c mÃ´ hÃ¬nh tá»« cÃ¡c há» khÃ¡c,
vÃ­ dá»¥ Mistral hoáº·c gte-large Ä‘Ã´i khi Ä‘áº¡t Ä‘iá»ƒm cao hÆ¡n cÃ¡c mÃ´ hÃ¬nh bge.
Má»™t máº«u tÆ°Æ¡ng tá»± cÅ©ng cÃ³ thá»ƒ quan sÃ¡t tháº¥y trong biá»ƒu Ä‘á»“ thá»© hai, nÆ¡i
tÆ°Æ¡ng Ä‘á»“ng Jaccard cho gte-large cao nháº¥t trong há» cá»§a nÃ³ Ä‘á»‘i vá»›i ğ‘˜ lá»›n
hÆ¡n, nhÆ°ng cÃ¡c mÃ´ hÃ¬nh nhÆ° mxbai hoáº·c bge-base Ä‘Ã´i khi bÃ¡o cÃ¡o sá»± tÆ°Æ¡ng
Ä‘á»“ng cao hÆ¡n cho ğ‘˜ nhá». Do Ä‘Ã³, cÃ¡c cá»¥m chÃºng tÃ´i xÃ¡c Ä‘á»‹nh thÃ´ng qua
phÃ¢n tÃ­ch CKA chá»‰ thá»±c sá»± Ä‘Æ°á»£c pháº£n Ã¡nh trong cÃ¡c biá»ƒu Ä‘á»“ nÃ y Ä‘á»‘i vá»›i
cÃ¡c giÃ¡ trá»‹ lá»›n cá»§a ğ‘˜. Äiá»u nÃ y cho tháº¥y ráº±ng trong cÃ¡c trÆ°á»ng há»£p sá»­
dá»¥ng thá»±c táº¿, nÆ¡i top-ğ‘˜ lÃ  quan trá»ng, sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n nhÆ° váº­y

cÃ³ thá»ƒ khÃ´ng dá»± Ä‘oÃ¡n Ä‘Æ°á»£c sá»± tÆ°Æ¡ng Ä‘á»“ng chá»©c nÄƒng. ChÃºng tÃ´i cÅ©ng tÃ­nh
toÃ¡n tÆ°Æ¡ng Ä‘á»“ng rank cho táº¥t cáº£ ğ‘˜ cÃ³ thá»ƒ trÃªn NFCorpus, nhÆ° Ä‘Æ°á»£c hiá»ƒn
thá»‹ trong HÃ¬nh 2. TÆ°Æ¡ng tá»± nhÆ° vá»›i tÆ°Æ¡ng Ä‘á»“ng Jaccard, chÃºng tÃ´i quan
sÃ¡t phÆ°Æ¡ng sai cao cho ğ‘˜ nhá», nhÆ°ng sá»± tÆ°Æ¡ng Ä‘á»“ng sau Ä‘Ã³ giáº£m vÃ  á»•n
Ä‘á»‹nh á»Ÿ má»©c Ä‘á»™ tháº¥p hÆ¡n cho ğ‘˜ lá»›n hÆ¡n.

Äá»ƒ Ä‘Æ°a ra má»™t quan Ä‘iá»ƒm tá»•ng quÃ¡t hÆ¡n, chÃºng tÃ´i tÃ­nh toÃ¡n tÆ°Æ¡ng Ä‘á»“ng
Jaccard vÃ  rank trung bÃ¬nh cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh vÃ  bá»™ dá»¯ liá»‡u á»Ÿ ğ‘˜ = 10.
Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4 vÃ  5. Äá»‘i vá»›i tÆ°Æ¡ng Ä‘á»“ng Jaccard
(HÃ¬nh 4), chÃºng tÃ´i cÃ³ thá»ƒ quan sÃ¡t cÃ¡c cá»¥m rÃµ rÃ ng cho cÃ¡c mÃ´ hÃ¬nh text-
embedding-3, sentence-t5 vÃ  gtr-t5. HÆ¡n ná»¯a, UAE vÃ  mxbai má»™t láº§n ná»¯a
cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng ráº¥t cao vá»›i bge-large. Äiá»u thÃº vá»‹ lÃ  nhá»¯ng mÃ´
hÃ¬nh nÃ y cÅ©ng bÃ¡o cÃ¡o sá»± tÆ°Æ¡ng Ä‘á»“ng cao vá»›i nhau, phÃ¹ há»£p vá»›i cÃ¡c quan
sÃ¡t CKA trÆ°á»›c Ä‘Ã³ cá»§a chÃºng tÃ´i. Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ²n láº¡i, khÃ´ng cÃ³
sá»± phÃ¢n cá»¥m rÃµ rÃ ng. TÆ°Æ¡ng tá»± rank (HÃ¬nh 5) cho tháº¥y cÃ¡c cá»¥m tÆ°Æ¡ng tá»±
nhÆ°ng Ã­t rÃµ rÃ ng hÆ¡n. Má»™t láº§n ná»¯a, UAE, mxbai vÃ  bge-large hÃ¬nh thÃ nh
má»™t cá»¥m, vÃ  cÃ¡c mÃ´ hÃ¬nh OpenAI vÃ  t5 cÃ³ xu hÆ°á»›ng tÆ°Æ¡ng tá»± vá»›i nhau
hÆ¡n. Tuy nhiÃªn, nhÃ¬n chung, Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng rank tháº¥p hÆ¡n so vá»›i tÆ°Æ¡ng
Ä‘á»“ng Jaccard vÃ  CKA. PhÃ¹ há»£p vá»›i cÃ¡c quan sÃ¡t tá»« HÃ¬nh 2 vÃ  3, Ä‘iá»u nÃ y
cÃ³ thá»ƒ lÃ  do tÃ­nh biáº¿n Ä‘á»•i trong sá»± tÆ°Æ¡ng Ä‘á»“ng rank á»Ÿ giÃ¡ trá»‹ ğ‘˜ tháº¥p.

Äá»ƒ hiá»ƒu rÃµ hÆ¡n vá» áº£nh hÆ°á»Ÿng cá»§a viá»‡c lá»±a chá»n bá»™ dá»¯ liá»‡u Ä‘áº¿n sá»± tÆ°Æ¡ng
Ä‘á»“ng mÃ´ hÃ¬nh, chÃºng tÃ´i tÃ­nh toÃ¡n tÆ°Æ¡ng Ä‘á»“ng Jaccard trung bÃ¬nh cho tá»«ng
bá»™ dá»¯ liá»‡u riÃªng biá»‡t á»Ÿ ğ‘˜ = 10. Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6. ChÃºng
tÃ´i quan sÃ¡t ráº±ng cÃ¡c máº«u tÆ°Æ¡ng Ä‘á»“ng nháº¥t quÃ¡n qua cÃ¡c bá»™ dá»¯ liá»‡u, vá»›i
UAE, mxbai vÃ  bge-large luÃ´n táº¡o thÃ nh má»™t cá»¥m cháº·t cháº½. CÃ¡c mÃ´ hÃ¬nh
OpenAI vÃ  t5 cÅ©ng cÃ³ xu hÆ°á»›ng tÆ°Æ¡ng tá»± vá»›i nhau qua cÃ¡c bá»™ dá»¯ liá»‡u.
Tuy nhiÃªn, cÃ³ má»™t sá»‘ thay Ä‘á»•i vá» Ä‘á»™ lá»›n cá»§a sá»± tÆ°Æ¡ng Ä‘á»“ng qua cÃ¡c bá»™ dá»¯
liá»‡u. VÃ­ dá»¥, ArguAna cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng trung bÃ¬nh cao hÆ¡n so vá»›i
cÃ¡c bá»™ dá»¯ liá»‡u khÃ¡c.

5.2 CÃ¡c Thay tháº¿ MÃ£ nguá»“n Má»Ÿ cho CÃ¡c MÃ´ hÃ¬nh Äá»™c quyá»n
Má»™t trong nhá»¯ng má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  xÃ¡c Ä‘á»‹nh cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n
má»Ÿ tÆ°Æ¡ng tá»± nháº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n Ä‘Æ°á»£c chá»n. Tá»« phÃ¢n tÃ­ch CKA
(HÃ¬nh 1), chÃºng tÃ´i cÃ³ thá»ƒ quan sÃ¡t ráº±ng Mistral cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng cao
nháº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh OpenAI, vá»›i Ä‘iá»ƒm CKA láº§n lÆ°á»£t lÃ  0.84 vÃ  0.90 Ä‘á»‘i
vá»›i text-embedding-3-large vÃ  text-embedding-3-small. Äiá»u nÃ y Ä‘Æ°á»£c
theo sau bá»Ÿi cÃ¡c mÃ´ hÃ¬nh e5, vá»›i e5-large-v2 vÃ  e5-base-v2 cÃ³ Ä‘iá»ƒm CKA
láº§n lÆ°á»£t lÃ  0.78 vÃ  0.77 Ä‘á»‘i vá»›i text-embedding-3-large. Äá»‘i vá»›i Cohere
embed-english-v3.0, cÃ¡c mÃ´ hÃ¬nh e5 cÅ©ng cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng cao
nháº¥t, vá»›i e5-large-v2 cÃ³ Ä‘iá»ƒm CKA lÃ  0.93.

Khi xem xÃ©t tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t (HÃ¬nh 4 vÃ  5), cÃ¡c quan há»‡ nÃ y Ã­t rÃµ
rÃ ng hÆ¡n. Mistral váº«n cho tháº¥y sá»± tÆ°Æ¡ng Ä‘á»“ng cao vá»›i cÃ¡c mÃ´ hÃ¬nh OpenAI,
nhÆ°ng sá»± khÃ¡c biá»‡t Ã­t rÃµ rÃ ng hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ¡c. Äiá»u nÃ y phÃ¹
há»£p vá»›i quan sÃ¡t cá»§a chÃºng tÃ´i ráº±ng sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n khÃ´ng nháº¥t
thiáº¿t chuyá»ƒn thÃ nh sá»± tÆ°Æ¡ng Ä‘á»“ng chá»©c nÄƒng, Ä‘áº·c biá»‡t á»Ÿ giÃ¡ trá»‹ ğ‘˜ tháº¥p.

6 THáº¢O LUáº¬N
Káº¿t quáº£ cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng máº·c dÃ¹ cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ thá»ƒ hiá»‡n
sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n cao thÃ´ng qua phÃ¢n tÃ­ch CKA, hÃ nh vi truy xuáº¥t
cá»§a chÃºng cÃ³ thá»ƒ khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i cÃ¡c giÃ¡ trá»‹ ğ‘˜ nhá»
thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong há»‡ thá»‘ng RAG. Äiá»u nÃ y cÃ³ nhá»¯ng hÃ m Ã½ quan
trá»ng Ä‘á»‘i vá»›i cÃ¡c nhÃ  thá»±c hÃ nh chá»n mÃ´ hÃ¬nh embedding cho á»©ng dá»¥ng
RAG.

Äáº§u tiÃªn, phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng viá»‡c chá»‰ dá»±a vÃ o sá»± tÆ°Æ¡ng
Ä‘á»“ng biá»ƒu diá»…n cÃ³ thá»ƒ khÃ´ng Ä‘á»§ Ä‘á»ƒ dá»± Ä‘oÃ¡n hiá»‡u suáº¥t trong cÃ¡c nhiá»‡m vá»¥
xuÃ´i dÃ²ng. Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng trong bá»‘i cáº£nh RAG, nÆ¡i tÃ­nh
chÃ­nh xÃ¡c cá»§a quÃ¡ trÃ¬nh truy xuáº¥t áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n cháº¥t lÆ°á»£ng cá»§a
vÄƒn báº£n Ä‘Æ°á»£c táº¡o ra. PhÆ°Æ¡ng sai cao trong sá»± tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t á»Ÿ giÃ¡
trá»‹ ğ‘˜ tháº¥p cho tháº¥y ráº±ng ngay cáº£ cÃ¡c mÃ´ hÃ¬nh vá»›i biá»ƒu diá»…n tÆ°Æ¡ng tá»± cÅ©ng
cÃ³ thá»ƒ truy xuáº¥t cÃ¡c táº­p há»£p tÃ i liá»‡u khÃ¡c nhau cho cÃ¹ng má»™t truy váº¥n.

Thá»© hai, viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c cá»¥m liÃªn há», Ä‘áº·c biá»‡t lÃ  nhÃ³m UAE, mxbai
vÃ  bge-large, cho tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh tá»« cÃ¡c nguá»“n khÃ¡c nhau cÃ³ thá»ƒ cÃ³
hÃ nh vi tÆ°Æ¡ng tá»±. Äiá»u nÃ y cÃ³ thá»ƒ cÃ³ Ã½ nghÄ©a thá»±c táº¿ Ä‘á»‘i vá»›i viá»‡c lá»±a chá»n
mÃ´ hÃ¬nh, vÃ¬ nÃ³ cho tháº¥y ráº±ng chuyá»ƒn Ä‘á»•i giá»¯a má»™t sá»‘ mÃ´ hÃ¬nh cÃ³ thá»ƒ cÃ³
tÃ¡c Ä‘á»™ng tá»‘i thiá»ƒu Ä‘áº¿n hiá»‡u suáº¥t há»‡ thá»‘ng.

Thá»© ba, káº¿t quáº£ cá»§a chÃºng tÃ´i vá» cÃ¡c thay tháº¿ mÃ£ nguá»“n má»Ÿ cho cÃ¡c mÃ´
hÃ¬nh Ä‘á»™c quyá»n cung cáº¥p hÆ°á»›ng dáº«n cÃ³ giÃ¡ trá»‹ cho cÃ¡c nhÃ  thá»±c hÃ nh muá»‘n
trÃ¡nh cÃ¡c mÃ´ hÃ¬nh tráº£ phÃ­. Viá»‡c Mistral thá»ƒ hiá»‡n sá»± tÆ°Æ¡ng Ä‘á»“ng cao vá»›i cÃ¡c
mÃ´ hÃ¬nh OpenAI Ä‘áº·c biá»‡t Ä‘Ã¡ng chÃº Ã½, vÃ¬ nÃ³ cung cáº¥p má»™t thay tháº¿ mÃ£
nguá»“n má»Ÿ cÃ³ kháº£ nÄƒng cho cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n hÃ ng Ä‘áº§u.

Tuy nhiÃªn, chÃºng tÃ´i cÅ©ng lÆ°u Ã½ má»™t sá»‘ háº¡n cháº¿ cá»§a nghiÃªn cá»©u. Äáº§u
tiÃªn, phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i Ä‘Æ°á»£c giá»›i háº¡n á»Ÿ nÄƒm bá»™ dá»¯ liá»‡u tá»« BEIR,
cÃ³ thá»ƒ khÃ´ng Ä‘áº¡i diá»‡n Ä‘áº§y Ä‘á»§ cho táº¥t cáº£ cÃ¡c á»©ng dá»¥ng RAG cÃ³ thá»ƒ. Thá»©
hai, chÃºng tÃ´i táº­p trung vÃ o tÆ°Æ¡ng tá»± cosine cho truy xuáº¥t, trong khi cÃ¡c
há»‡ thá»‘ng RAG thá»±c táº¿ cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c thÆ°á»›c Ä‘o tÆ°Æ¡ng tá»± hoáº·c chiáº¿n
lÆ°á»£c truy xuáº¥t khÃ¡c. Thá»© ba, phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i khÃ´ng xem xÃ©t hiá»‡u
suáº¥t tÃ­nh toÃ¡n hoáº·c yÃªu cáº§u bá»™ nhá»› cá»§a cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau, lÃ  nhá»¯ng
yáº¿u tá»‘ quan trá»ng trong triá»ƒn khai thá»±c táº¿.

Báº¥t cháº¥p nhá»¯ng háº¡n cháº¿ nÃ y, nghiÃªn cá»©u cá»§a chÃºng tÃ´i cung cáº¥p nhá»¯ng
hiá»ƒu biáº¿t cÃ³ giÃ¡ trá»‹ vá» sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a mÃ´ hÃ¬nh embedding trong bá»‘i
cáº£nh RAG vÃ  Ä‘á» xuáº¥t cÃ¡c hÆ°á»›ng cho nghiÃªn cá»©u tÆ°Æ¡ng lai. Cá»¥ thá»ƒ, chÃºng
tÃ´i khuyáº¿n khÃ­ch nghiÃªn cá»©u tÆ°Æ¡ng lai khÃ¡m phÃ¡:

1. áº¢nh hÆ°á»Ÿng cá»§a cÃ¡c chiáº¿n lÆ°á»£c truy xuáº¥t khÃ¡c nhau Ä‘áº¿n sá»± tÆ°Æ¡ng Ä‘á»“ng
mÃ´ hÃ¬nh
2. Má»‘i quan há»‡ giá»¯a sá»± tÆ°Æ¡ng Ä‘á»“ng mÃ´ hÃ¬nh vÃ  hiá»‡u suáº¥t xuÃ´i dÃ²ng trong
cÃ¡c nhiá»‡m vá»¥ RAG cá»¥ thá»ƒ
3. PhÃ¡t triá»ƒn cÃ¡c thÆ°á»›c Ä‘o sá»± tÆ°Æ¡ng Ä‘á»“ng má»›i cÃ³ thá»ƒ dá»± Ä‘oÃ¡n tá»‘t hÆ¡n hiá»‡u
suáº¥t chá»©c nÄƒng
4. PhÃ¢n tÃ­ch cÃ¡c mÃ´ hÃ¬nh vÃ  bá»™ dá»¯ liá»‡u rá»™ng hÆ¡n Ä‘á»ƒ xÃ¡c thá»±c cÃ¡c phÃ¡t hiá»‡n
cá»§a chÃºng tÃ´i

7 Káº¾T LUáº¬N
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘Ã£ trÃ¬nh bÃ y má»™t phÃ¢n tÃ­ch toÃ n diá»‡n vá» sá»±
tÆ°Æ¡ng Ä‘á»“ng mÃ´ hÃ¬nh embedding trong bá»‘i cáº£nh há»‡ thá»‘ng TÄƒng cÆ°á»ng Truy
xuáº¥t Sinh vÄƒn báº£n. ThÃ´ng qua viá»‡c sá»­ dá»¥ng cáº£ thÆ°á»›c Ä‘o sá»± tÆ°Æ¡ng Ä‘á»“ng
biá»ƒu diá»…n (CKA) vÃ  chá»©c nÄƒng (Jaccard vÃ  rank similarity), chÃºng tÃ´i Ä‘Ã£
cung cáº¥p má»™t cÃ¡i nhÃ¬n sÃ¢u sáº¯c vá» hÃ nh vi cá»§a cÃ¡c mÃ´ hÃ¬nh embedding khÃ¡c
nhau.

CÃ¡c phÃ¡t hiá»‡n chÃ­nh cá»§a chÃºng tÃ´i bao gá»“m:

1. **CÃ¡c cá»¥m ná»™i vÃ  liÃªn há»**: Trong khi cÃ¡c mÃ´ hÃ¬nh thÆ°á»ng tÆ°Æ¡ng tá»±
nháº¥t vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ¡c trong cÃ¹ng há», chÃºng tÃ´i cÅ©ng xÃ¡c Ä‘á»‹nh cÃ¡c
cá»¥m liÃªn há» thÃº vá»‹, Ä‘áº·c biá»‡t lÃ  nhÃ³m UAE, mxbai vÃ  bge-large.

2. **PhÆ°Æ¡ng sai cao á»Ÿ giÃ¡ trá»‹ k tháº¥p**: Sá»± tÆ°Æ¡ng Ä‘á»“ng truy xuáº¥t thá»ƒ hiá»‡n
phÆ°Æ¡ng sai Ä‘Ã¡ng ká»ƒ á»Ÿ giÃ¡ trá»‹ ğ‘˜ tháº¥p, Ä‘iá»u nÃ y Ä‘áº·c biá»‡t liÃªn quan Ä‘áº¿n cÃ¡c
á»©ng dá»¥ng RAG thá»±c táº¿.

3. **Sá»± khÃ´ng phÃ¹ há»£p giá»¯a sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n vÃ  chá»©c nÄƒng**: CÃ¡c
mÃ´ hÃ¬nh cÃ³ sá»± tÆ°Æ¡ng Ä‘á»“ng biá»ƒu diá»…n cao khÃ´ng nháº¥t thiáº¿t thá»ƒ hiá»‡n hÃ nh vi
truy xuáº¥t tÆ°Æ¡ng tá»±, Ä‘áº·c biá»‡t á»Ÿ giÃ¡ trá»‹ ğ‘˜ tháº¥p.

4. **CÃ¡c thay tháº¿ mÃ£ nguá»“n má»Ÿ**: Mistral thá»ƒ hiá»‡n sá»± tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t
vá»›i cÃ¡c mÃ´ hÃ¬nh OpenAI, cung cáº¥p má»™t thay tháº¿ mÃ£ nguá»“n má»Ÿ tiá»m nÄƒng
cho cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c quyá»n.

Nhá»¯ng phÃ¡t hiá»‡n nÃ y cÃ³ nhá»¯ng hÃ m Ã½ quan trá»ng Ä‘á»‘i vá»›i cÃ¡c nhÃ  thá»±c
hÃ nh phÃ¡t triá»ƒn há»‡ thá»‘ng RAG. ChÃºng cho tháº¥y ráº±ng viá»‡c lá»±a chá»n mÃ´ hÃ¬nh
embedding khÃ´ng nÃªn chá»‰ dá»±a trÃªn hiá»‡u suáº¥t benchmark hoáº·c sá»± tÆ°Æ¡ng
Ä‘á»“ng biá»ƒu diá»…n mÃ  cÅ©ng cáº§n xem xÃ©t hÃ nh vi truy xuáº¥t cá»¥ thá»ƒ trong bá»‘i
cáº£nh á»©ng dá»¥ng Ä‘Ã­ch.

NghiÃªn cá»©u tÆ°Æ¡ng lai cÃ³ thá»ƒ má»Ÿ rá»™ng phÃ¢n tÃ­ch nÃ y Ä‘á»ƒ bao gá»“m cÃ¡c bá»™
dá»¯ liá»‡u vÃ  chiáº¿n lÆ°á»£c truy xuáº¥t Ä‘a dáº¡ng hÆ¡n, cÅ©ng nhÆ° khÃ¡m phÃ¡ má»‘i quan
há»‡ giá»¯a sá»± tÆ°Æ¡ng Ä‘á»“ng mÃ´ hÃ¬nh vÃ  hiá»‡u suáº¥t xuÃ´i dÃ²ng trong cÃ¡c nhiá»‡m
vá»¥ RAG cá»¥ thá»ƒ. HÆ¡n ná»¯a, viá»‡c phÃ¡t triá»ƒn cÃ¡c thÆ°á»›c Ä‘o sá»± tÆ°Æ¡ng Ä‘á»“ng má»›i
cÃ³ thá»ƒ dá»± Ä‘oÃ¡n tá»‘t hÆ¡n hiá»‡u suáº¥t chá»©c nÄƒng sáº½ lÃ  má»™t Ä‘Ã³ng gÃ³p cÃ³ giÃ¡ trá»‹
cho lÄ©nh vá»±c nÃ y.

Lá»œI Cáº¢M Æ N
NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c tÃ i trá»£ bá»Ÿi chÆ°Æ¡ng trÃ¬nh Horizon Europe cá»§a LiÃªn
minh ChÃ¢u Ã‚u theo thá»a thuáº­n tÃ i trá»£ sá»‘ 101070014.

TÃ€I LIá»†U THAM KHáº¢O
[1] Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik
Cho, Carlo C Del Mundo, Mohammad Rastegari, vÃ  Mehrdad Farajtabar. 2023.
LLM surgery: Efficient knowledge unlearning via layer-wise model editing.
arXiv preprint arXiv:2308.15267 (2023).

[2] Rana Ali Amjad vÃ  Bernhard C Geiger. 2020. Learning representations for neural
network-based classification using the information bottleneck principle. IEEE
transactions on pattern analysis and machine intelligence 42, 9 (2020), 2225â€“2239.

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877â€“1897.

[4] Ari S Morcos, Maithra Raghu, vÃ  Samy Bengio. 2018. Insights on representational
similarity in neural networks with canonical correlation analysis. Advances in
neural information processing systems 31 (2018).

[5] Cohere. 2024. Embed English v3.0. https://docs.cohere.com/reference/embed.
Accessed: 2024-01-15.

[6] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
Laskin, Pieter Abbeel, Aravind Srinivas, vÃ  Igor Mordatch. 2021. Decision
transformer: Reinforcement learning via sequence modeling. Advances in neural
information processing systems 34 (2021), 15084â€“15097.

[7] Shahul Es, Jithin James, Luis Espinosa-Anke, vÃ  Steven Schockaert. 2023. RAGAS:
Automated evaluation of retrieval augmented generation. arXiv preprint
arXiv:2309.15217 (2023).

[8] Luyu Gao, Xueguang Ma, Jimmy Lin, vÃ  Jamie Callan. 2023. Tevatron: An efficient
and flexible toolkit for dense retrieval. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 3: System Demonstrations).
Association for Computational Linguistics, Toronto, Canada, 56â€“69.

[9] Benjamin Freestone vÃ  Samual Santu. 2023. Understanding language model
representations through their lexical similarities. arXiv preprint arXiv:2308.03730
(2023).

[10] Arthur Gretton, Olivier Bousquet, Alex Smola, vÃ  Bernhard SchÃ¶lkopf. 2005.
Measuring statistical dependence with Hilbert-Schmidt norms. In International
conference on algorithmic learning theory. Springer, 63â€“77.

[11] Harold Hotelling. 1936. Relations between two sets of variates. Biometrika 28,
3/4 (1936), 321â€“377.

[12] Jeff Huber, Nikhil Thorat, vÃ  Anton Troynikov. 2023. Chroma: The open-source
embedding database. https://github.com/chroma-core/chroma.

[13] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
Ye Jin Bang, Andrea Madotto, vÃ  Pascale Fung. 2023. Survey of hallucination in
natural language generation. ACM Computing Surveys 55, 12 (2023), 1â€“38.

[14] Max Klabunde, Tobias Schumacher, Markus Strohmaier, vÃ  Florian Lemmerich.
2023. Similarity of neural network representations: Do wider networks help? In
International Conference on Machine Learning. PMLR, 17134â€“17151.

[15] Max Klabunde, Tobias Schumacher, Markus Strohmaier, vÃ  Florian Lemmerich.
2023. Similarity of neural network models: A survey of functional and representational
measures. arXiv preprint arXiv:2305.06329 (2023).

[16] Simon Kornblith, Mohammad Norouzi, Honglak Lee, vÃ  Geoffrey Hinton. 2019.
Similarity of neural network representations revisited. In International conference
on machine learning. PMLR, 3519â€“3529.

[17] MixedBread AI. 2024. mxbai-embed-large-v1. https://huggingface.co/mixedbread-
ai/mxbai-embed-large-v1. Accessed: 2024-01-15.

[18] Ari S Morcos, Maithra Raghu, vÃ  Samy Bengio. 2018. Insights on representational
similarity in neural networks with canonical correlation analysis. Advances in
neural information processing systems 31 (2018).

[19] Mohammad Nikhil vÃ  Kashif Sheikh. 2024. UAE-Large-V1. https://huggingface.co/
WhereIsAI/UAE-Large-V1. Accessed: 2024-01-15.

[20] Ari S Morcos, Maithra Raghu, vÃ  Samy Bengio. 2018. Insights on representational
similarity in neural networks with canonical correlation analysis. Advances in
neural information processing systems 31 (2018).

[21] Ari S Morcos, Maithra Raghu, vÃ  Samy Bengio. 2018. Insights on representational
similarity in neural networks with canonical correlation analysis. Advances in
neural information processing systems 31 (2018).

[22] Ari S Morcos, Maithra Raghu, vÃ  Samy Bengio. 2018. Insights on representational
similarity in neural networks with canonical correlation analysis. Advances in
neural information processing systems 31 (2018).

[23] Zehan Li vÃ  Xin Zhang. 2023. GTE: General Text Embeddings. https://huggingface.co/
thenlper/gte-large. Accessed: 2024-01-15.

[24] Salesforce Research. 2024. SFR-Embedding-Mistral. https://huggingface.co/
Salesforce/SFR-Embedding-Mistral. Accessed: 2024-01-15.

[25] Ari S Morcos, Maithra Raghu, vÃ  Samy Bengio. 2018. Insights on representational
similarity in neural networks with canonical correlation analysis. Advances in
neural information processing systems 31 (2018).

[26] Ari S Morcos, Maithra Raghu, vÃ  Samy Bengio. 2018. Insights on representational
similarity in neural networks with canonical correlation analysis. Advances in
neural information processing systems 31 (2018).

[27] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, vÃ  Tushar Khot. 2023. Complexity-
based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720 (2023).

[28] Niklas Muennighoff, Nouamane Tazi, LoÃ¯c Magne, vÃ  Nils Reimers. 2022. MTEB:
Massive text embedding benchmark. arXiv preprint arXiv:2210.07316 (2022).

[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, vÃ  Peter J Liu. 2020. Exploring the limits of transfer
learning with a unified text-to-text transformer. The Journal of Machine Learning
Research 21, 1 (2020), 5485â€“5551.

[30] Nils Reimers vÃ  Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings
using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP). Association for Computational
Linguistics, Hong Kong, China, 3982â€“3992.

[31] OpenAI. 2024. text-embedding-3-large vÃ  text-embedding-3-small. https://openai.com/
blog/new-embedding-models-and-api-updates. Accessed: 2024-01-15.

[32] Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand
Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent
Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. the Journal of
machine Learning research 12 (2011), 2825â€“2830.

[33] Maithra Raghu, Justin Gilmer, Jason Yosinski, vÃ  Jascha Sohl-Dickstein. 2017.
SVCCA: Singular vector canonical correlation analysis for deep learning dynamics
and interpretability. Advances in neural information processing systems 30 (2017).

[34] Chen Qu, Gottumukkala V Ravi Kiran, Nikita Bhutani, Lucas Woltmann, Anastasia
Krithara, Sarvnaz Karimi, Manuela SchÃ¼tze, vÃ  Georgios Paliouras. 2023. RM3:
A multi-domain, multi-lingual, multi-modal retrieval model for the biomedical
domain. arXiv preprint arXiv:2304.14200 (2023).

[35] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, vÃ  Iryna
Gurevych. 2021. BEIR: A heterogeneous benchmark for zero-shot evaluation of
information retrieval models. In Thirty-fifth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track (Round 1).

[36] Charles L Webber Jr vÃ  Joseph P Zbilut. 2005. Recurrence quantification analysis
of nonlinear dynamical systems. Tutorials in contemporary nonlinear methods
for the behavioral sciences 94, 2005 (2005), 26â€“94.

[37] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,
Rangan Majumder, vÃ  Furu Wei. 2024. Text embeddings by weakly-supervised
contrastive pre-training. arXiv preprint arXiv:2212.03533 (2024).

[38] Michael Waskom. 2021. seaborn: statistical data visualization. Journal of Open
Source Software 6, 60 (2021), 3021.

[39] Zhenqin Wu, Bharath Ramsundar, Edward N Feinberg, Joseph Gomes, Caleb
Geniesse, Aneesh Pappu, Karl Leswing, vÃ  Vijay Pande. 2018. MoleculeNet: a
benchmark for molecular machine learning. Chemical science 9, 2 (2018), 513â€“530.

[40] Shitao Xiao, Zheng Liu, Peitian Zhang, vÃ  Niklas Muennighoff. 2023. C-Pack:
Packaged resources to advance general Chinese embedding. arXiv preprint
arXiv:2309.07597 (2023).

[41] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, vÃ  Christian Szegedy. 2022.
Memorizing transformers. arXiv preprint arXiv:2203.08913 (2022).

[42] Jason Yosinski, Jeff Clune, Yoshua Bengio, vÃ  Hoan Lipson. 2014. How transferable
are features in deep neural networks? Advances in neural information processing
systems 27 (2014).