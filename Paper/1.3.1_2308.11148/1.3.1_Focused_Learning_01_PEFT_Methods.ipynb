{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning 01: Parameter-Efficient Fine-Tuning (PEFT) Methods\n",
    "\n",
    "## üéØ Objective\n",
    "Notebook n√†y gi·∫£i th√≠ch s√¢u v·ªÅ **Parameter-Efficient Fine-Tuning (PEFT)** methods ƒë∆∞·ª£c s·ª≠ d·ª•ng trong LLaMA-Reviewer, ƒë·∫∑c bi·ªát l√† **LoRA** v√† **Prefix-tuning**.\n",
    "\n",
    "## üìç Paper Reference\n",
    "- **Section III.C**: Zero-init Attention Prefix-tuning\n",
    "- **Section III.D**: Low-Rank Adaptation (LoRA)\n",
    "- **Figure 4**: Details of prefix-tuning on LLaMA\n",
    "- **Figure 5**: Core component of Low-Rank Adaptation\n",
    "- **Equations 1-6**: Mathematical formulations\n",
    "\n",
    "## üîç Core Problem\n",
    "Fine-tuning large language models nh∆∞ LLaMA (6.7B+ parameters) ƒë√≤i h·ªèi:\n",
    "- **Computational Resources**: C·∫ßn GPU memory l·ªõn\n",
    "- **Storage Space**: M·ªói fine-tuned model c·∫ßn ~13GB storage\n",
    "- **Training Time**: R·∫•t l√¢u ƒë·ªÉ update to√†n b·ªô parameters\n",
    "\n",
    "**PEFT Solution**: Ch·ªâ fine-tune m·ªôt s·ªë nh·ªè parameters (<1%) trong khi freeze base model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Mathematical Foundations\n",
    "\n",
    "### Core Hypothesis\n",
    "PEFT d·ª±a tr√™n gi·∫£ thuy·∫øt r·∫±ng qu√° tr√¨nh adaptation c√≥ **\"intrinsic rank\"** th·∫•p, nghƒ©a l√† c√≥ th·ªÉ bi·ªÉu di·ªÖn thay ƒë·ªïi weights th√¥ng qua low-rank matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üîß Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Method 1: Low-Rank Adaptation (LoRA)\n",
    "\n",
    "### Mathematical Foundation (Equation 5-6 from paper)\n",
    "\n",
    "LoRA approximates weight updates th√¥ng qua low-rank decomposition:\n",
    "\n",
    "$$W_0 + \\Delta W = W_0 + W_{down} W_{up}$$\n",
    "\n",
    "Where:\n",
    "- $W_0 \\in \\mathbb{R}^{d \\times k}$: Pre-trained weight matrix (frozen)\n",
    "- $W_{down} \\in \\mathbb{R}^{d \\times r}$: Trainable down-projection\n",
    "- $W_{up} \\in \\mathbb{R}^{r \\times k}$: Trainable up-projection  \n",
    "- $r \\ll \\min(d, k)$: Low rank\n",
    "\n",
    "Output computation:\n",
    "$$\\bar{h} = W_0 x + \\Delta W x = h + W_{down} W_{up} x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Implementation of LoRA layer based on paper equations\"\"\"\n",
    "    \n",
    "    def __init__(self, original_layer: nn.Linear, rank: int, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Freeze original weights (W_0)\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices (Equation 5)\n",
    "        self.lora_A = nn.Linear(original_layer.in_features, rank, bias=False)  # W_down\n",
    "        self.lora_B = nn.Linear(rank, original_layer.out_features, bias=False)  # W_up\n",
    "        \n",
    "        # Initialize following paper recommendations\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "        # Scaling factor\n",
    "        self.scaling = alpha / rank\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass implementing Equation 6\"\"\"\n",
    "        # Original output: h = W_0 * x\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA output: W_down * W_up * x\n",
    "        lora_output = self.lora_B(self.lora_A(x)) * self.scaling\n",
    "        \n",
    "        # Combined output: h + ŒîW * x (Equation 6)\n",
    "        return original_output + lora_output\n",
    "    \n",
    "    def get_delta_weights(self) -> torch.Tensor:\n",
    "        \"\"\"Get the ŒîW matrix for analysis\"\"\"\n",
    "        return (self.lora_B.weight @ self.lora_A.weight) * self.scaling\n",
    "\n",
    "print(\"‚úÖ LoRA implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate LoRA with mock data\n",
    "def demonstrate_lora():\n",
    "    \"\"\"Demonstrate LoRA layer behavior\"\"\"\n",
    "    \n",
    "    # Create original layer (simulating part of transformer)\n",
    "    d, k = 768, 768  # Typical transformer dimensions\n",
    "    original_layer = nn.Linear(d, k)\n",
    "    \n",
    "    # Create LoRA layers with different ranks\n",
    "    ranks = [4, 8, 16, 32]\n",
    "    lora_layers = {}\n",
    "    \n",
    "    for rank in ranks:\n",
    "        lora_layers[rank] = LoRALayer(original_layer, rank=rank, alpha=16)\n",
    "    \n",
    "    # Test input\n",
    "    batch_size, seq_len = 4, 128\n",
    "    x = torch.randn(batch_size, seq_len, d)\n",
    "    \n",
    "    print(\"üîç LoRA Analysis:\")\n",
    "    print(f\"Original layer parameters: {sum(p.numel() for p in original_layer.parameters()):,}\")\n",
    "    \n",
    "    results = {}\n",
    "    for rank in ranks:\n",
    "        layer = lora_layers[rank]\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in layer.parameters())\n",
    "        \n",
    "        # Forward pass\n",
    "        output = layer(x)\n",
    "        \n",
    "        # Analysis\n",
    "        delta_w = layer.get_delta_weights()\n",
    "        \n",
    "        results[rank] = {\n",
    "            'trainable_params': trainable_params,\n",
    "            'total_params': total_params,\n",
    "            'percentage': trainable_params / total_params * 100,\n",
    "            'delta_norm': torch.norm(delta_w).item(),\n",
    "            'output_shape': output.shape\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nRank {rank}:\")\n",
    "        print(f\"  Trainable: {trainable_params:,} ({results[rank]['percentage']:.3f}%)\")\n",
    "        print(f\"  ŒîW norm: {results[rank]['delta_norm']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run demonstration\n",
    "lora_results = demonstrate_lora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Method 2: Zero-init Attention Prefix-tuning\n",
    "\n",
    "### Mathematical Foundation (Equations 1-4 from paper)\n",
    "\n",
    "Prefix-tuning th√™m K learnable prefix tokens v√†o L top layers c·ªßa transformer.\n",
    "\n",
    "#### Attention Computation (Equation 1):\n",
    "$$Q_l = Linear_q(t_{M+1})$$\n",
    "$$K_l = Linear_k([P_l; T_l; t_{M+1}])$$\n",
    "$$V_l = Linear_v([P_l; T_l; t_{M+1}])$$\n",
    "\n",
    "Where $P_l \\in \\mathbb{R}^{K \\times C}$ l√† prefix tokens, $T_l \\in \\mathbb{R}^{M \\times C}$ l√† original tokens.\n",
    "\n",
    "#### Attention Scores (Equation 2):\n",
    "$$S_l^K = Q_l (K_l^K)^T / \\sqrt{C}$$\n",
    "$$S_l^{M+1} = Q_l (K_l^{M+1})^T / \\sqrt{C}$$\n",
    "\n",
    "#### Zero-init Gating (Equation 3):\n",
    "$$S_l^g = [Softmax(S_l^K) \\cdot g_l; Softmax(S_l^{M+1})]$$\n",
    "\n",
    "Where $g_l$ is learnable gating factor, initialized to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroInitPrefixTuning(nn.Module):\n",
    "    \"\"\"Implementation of Zero-init Attention Prefix-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_size: int,\n",
    "                 num_heads: int,\n",
    "                 prefix_length: int,\n",
    "                 num_layers: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.prefix_length = prefix_length  # K in paper\n",
    "        self.num_layers = num_layers  # L in paper\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Prefix tokens for each layer (P_l in Equation 1)\n",
    "        self.prefix_tokens = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(prefix_length, hidden_size))\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Zero-init gating factors (g_l in Equation 3)\n",
    "        self.gating_factors = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(num_heads))  # Zero initialization\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Forward pass implementing Equations 1-4\"\"\"\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        \n",
    "        # Get prefix tokens for this layer\n",
    "        prefix = self.prefix_tokens[layer_idx].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Concatenate prefix with original tokens (Equation 1)\n",
    "        # [P_l; T_l; t_{M+1}] -> [prefix_len + seq_len, hidden_size]\n",
    "        extended_input = torch.cat([prefix, x], dim=1)\n",
    "        \n",
    "        # Apply projections (Equation 1)\n",
    "        Q = self.q_proj(x)  # Only query from original tokens\n",
    "        K = self.k_proj(extended_input)  # Keys from prefix + original\n",
    "        V = self.v_proj(extended_input)  # Values from prefix + original\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores (Equation 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Split scores for prefix and original tokens\n",
    "        prefix_scores = scores[:, :, :, :self.prefix_length]  # S_l^K\n",
    "        original_scores = scores[:, :, :, self.prefix_length:]  # S_l^{M+1}\n",
    "        \n",
    "        # Apply zero-init gating (Equation 3)\n",
    "        gating = self.gating_factors[layer_idx].view(1, self.num_heads, 1, 1)\n",
    "        \n",
    "        prefix_attn = torch.softmax(prefix_scores, dim=-1) * gating\n",
    "        original_attn = torch.softmax(original_scores, dim=-1)\n",
    "        \n",
    "        # Combine attention weights\n",
    "        combined_attn = torch.cat([prefix_attn, original_attn], dim=-1)\n",
    "        \n",
    "        # Apply attention to values (Equation 4)\n",
    "        output = torch.matmul(combined_attn, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        output = output.transpose(1, 2).reshape(batch_size, seq_len, hidden_size)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_trainable_params(self) -> int:\n",
    "        \"\"\"Count trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"‚úÖ Zero-init Prefix-tuning implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Prefix-tuning\n",
    "def demonstrate_prefix_tuning():\n",
    "    \"\"\"Demonstrate prefix-tuning behavior\"\"\"\n",
    "    \n",
    "    # Configuration (similar to paper)\n",
    "    hidden_size = 768\n",
    "    num_heads = 12\n",
    "    num_layers = 12\n",
    "    prefix_lengths = [5, 10, 20]  # Different prefix lengths to test\n",
    "    \n",
    "    # Test input\n",
    "    batch_size, seq_len = 4, 64\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    print(\"üîç Prefix-tuning Analysis:\")\n",
    "    \n",
    "    results = {}\n",
    "    for prefix_len in prefix_lengths:\n",
    "        model = ZeroInitPrefixTuning(\n",
    "            hidden_size=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            prefix_length=prefix_len,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Forward pass through first layer\n",
    "        output = model(x, layer_idx=0)\n",
    "        \n",
    "        # Analysis\n",
    "        trainable_params = model.get_trainable_params()\n",
    "        \n",
    "        # Check gating factor (should be close to 0 initially)\n",
    "        initial_gating = model.gating_factors[0].mean().item()\n",
    "        \n",
    "        results[prefix_len] = {\n",
    "            'trainable_params': trainable_params,\n",
    "            'initial_gating': initial_gating,\n",
    "            'output_shape': output.shape,\n",
    "            'prefix_contribution': torch.norm(output - x).item()  # How much prefix changes output\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nPrefix Length {prefix_len}:\")\n",
    "        print(f\"  Trainable params: {trainable_params:,}\")\n",
    "        print(f\"  Initial gating: {initial_gating:.6f}\")\n",
    "        print(f\"  Output change norm: {results[prefix_len]['prefix_contribution']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run demonstration\n",
    "prefix_results = demonstrate_prefix_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comparative Analysis\n",
    "\n",
    "So s√°nh LoRA vs Prefix-tuning theo findings t·ª´ paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis visualization\n",
    "def compare_peft_methods():\n",
    "    \"\"\"Compare LoRA vs Prefix-tuning characteristics\"\"\"\n",
    "    \n",
    "    # Paper results reproduction (Table IX)\n",
    "    paper_comparison = {\n",
    "        'Method': ['Prefix-tuning', 'LoRA (r=8)', 'LoRA (r=16)'],\n",
    "        'Trainable_Params_M': [1.2, 4.2, 8.4],  # Million parameters\n",
    "        'Storage_MB': [2.4, 8, 16],\n",
    "        'RNP_F1': [None, 69.34, 70.49],  # Prefix-tuning couldn't do classification\n",
    "        'RCG_BLEU': [5.16, 5.64, 5.70],\n",
    "        'CR_BLEU': [76.71, 81.59, 82.27]\n",
    "    }\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('PEFT Methods Comparison: LoRA vs Prefix-tuning', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Parameter Efficiency\n",
    "    ax1 = axes[0, 0]\n",
    "    methods = paper_comparison['Method']\n",
    "    params = paper_comparison['Trainable_Params_M']\n",
    "    \n",
    "    bars = ax1.bar(methods, params, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "    ax1.set_title('Trainable Parameters (Millions)')\n",
    "    ax1.set_ylabel('Parameters (M)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, params):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{value}M', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Storage Requirements\n",
    "    ax2 = axes[0, 1]\n",
    "    storage = paper_comparison['Storage_MB']\n",
    "    \n",
    "    bars = ax2.bar(methods, storage, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "    ax2.set_title('Storage Requirements (MB)')\n",
    "    ax2.set_ylabel('Storage (MB)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, storage):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                f'{value}MB', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Performance - Review Comment Generation\n",
    "    ax3 = axes[1, 0]\n",
    "    rcg_scores = paper_comparison['RCG_BLEU']\n",
    "    \n",
    "    bars = ax3.bar(methods, rcg_scores, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "    ax3.set_title('Review Comment Generation (BLEU-4)')\n",
    "    ax3.set_ylabel('BLEU-4 Score')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, rcg_scores):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                f'{value:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Performance - Code Refinement\n",
    "    ax4 = axes[1, 1]\n",
    "    cr_scores = paper_comparison['CR_BLEU']\n",
    "    \n",
    "    bars = ax4.bar(methods, cr_scores, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "    ax4.set_title('Code Refinement (BLEU-4)')\n",
    "    ax4.set_ylabel('BLEU-4 Score')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, cr_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key insights\n",
    "    print(\"\\nüîç Key Insights from Paper (RQ4):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. LoRA consistently outperforms Prefix-tuning across all tasks\")\n",
    "    print(\"2. Higher LoRA rank (r=16) achieves better performance than r=8\")\n",
    "    print(\"3. Prefix-tuning struggles with classification tasks (RNP)\")\n",
    "    print(\"4. LoRA provides better approximation to full-parameter tuning\")\n",
    "    print(\"5. Trade-off: Higher rank = more parameters but better performance\")\n",
    "    \n",
    "    return paper_comparison\n",
    "\n",
    "# Run comparison\n",
    "comparison_data = compare_peft_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Hands-on Experimentation\n",
    "\n",
    "Th·ª±c nghi·ªám v·ªõi mock data ƒë·ªÉ hi·ªÉu behavior c·ªßa t·ª´ng method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands-on experimentation\n",
    "def experiment_with_peft():\n",
    "    \"\"\"Experiment to understand PEFT behavior\"\"\"\n",
    "    \n",
    "    print(\"üß™ PEFT Experimentation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a simple \"pre-trained\" layer\n",
    "    original_layer = nn.Linear(256, 256)\n",
    "    \n",
    "    # Create mock task: transform input in a specific way\n",
    "    # Target: multiply input by 2 (simple adaptation task)\n",
    "    target_transform = lambda x: x * 2\n",
    "    \n",
    "    # Test data\n",
    "    test_input = torch.randn(10, 256)\n",
    "    target_output = target_transform(test_input)\n",
    "    \n",
    "    # Method 1: LoRA adaptation\n",
    "    print(\"\\nüéØ Testing LoRA adaptation:\")\n",
    "    lora_layer = LoRALayer(original_layer, rank=8, alpha=16)\n",
    "    \n",
    "    # Simple \"training\" - just to show adaptation\n",
    "    optimizer = torch.optim.AdamW(lora_layer.parameters(), lr=0.01)\n",
    "    \n",
    "    initial_loss = None\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = lora_layer(test_input)\n",
    "        loss = nn.MSELoss()(output, target_output)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            initial_loss = loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"  Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    final_loss = loss.item()\n",
    "    improvement = (initial_loss - final_loss) / initial_loss * 100\n",
    "    \n",
    "    print(f\"  LoRA adaptation: {improvement:.1f}% loss reduction\")\n",
    "    print(f\"  Trainable params: {sum(p.numel() for p in lora_layer.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Method 2: Full fine-tuning comparison\n",
    "    print(\"\\nüéØ Testing full fine-tuning (baseline):\")\n",
    "    full_layer = nn.Linear(256, 256)\n",
    "    full_layer.load_state_dict(original_layer.state_dict())  # Start from same weights\n",
    "    \n",
    "    optimizer_full = torch.optim.AdamW(full_layer.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        optimizer_full.zero_grad()\n",
    "        output = full_layer(test_input)\n",
    "        loss = nn.MSELoss()(output, target_output)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_full.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"  Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"  Full fine-tuning final loss: {loss.item():.4f}\")\n",
    "    print(f\"  Trainable params: {sum(p.numel() for p in full_layer.parameters()):,}\")\n",
    "    \n",
    "    # Analysis\n",
    "    lora_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "    full_params = sum(p.numel() for p in full_layer.parameters())\n",
    "    \n",
    "    print(f\"\\nüìä Efficiency Analysis:\")\n",
    "    print(f\"  Parameter reduction: {(1 - lora_params/full_params)*100:.1f}%\")\n",
    "    print(f\"  LoRA final loss: {final_loss:.4f}\")\n",
    "    print(f\"  Full fine-tuning final loss: {loss.item():.4f}\")\n",
    "    print(f\"  Performance gap: {abs(final_loss - loss.item()):.4f}\")\n",
    "\n",
    "# Run experiment\n",
    "experiment_with_peft()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### From Paper Analysis (Section V.D):\n",
    "\n",
    "1. **LoRA Superior Performance**: LoRA outperforms prefix-tuning across all code review tasks\n",
    "   - Better approximation to full-parameter tuning\n",
    "   - More flexible adaptation mechanism\n",
    "\n",
    "2. **Rank Selection Impact**: Higher LoRA rank improves performance\n",
    "   - r=16 > r=8 consistently\n",
    "   - Trade-off between efficiency and performance\n",
    "\n",
    "3. **Task-Specific Behavior**: \n",
    "   - Prefix-tuning struggles with classification (RNP)\n",
    "   - Both methods effective for generation tasks (RCG, CR)\n",
    "\n",
    "4. **Efficiency Gains**: Both methods achieve <1% trainable parameters\n",
    "   - 99%+ parameter reduction vs full fine-tuning\n",
    "   - Significant storage savings (13GB ‚Üí <20MB)\n",
    "\n",
    "### Implementation Insights:\n",
    "\n",
    "- **LoRA**: Low-rank approximation captures essential adaptations\n",
    "- **Prefix-tuning**: Zero-init gating provides stable training\n",
    "- **Both**: Enable efficient deployment of specialized models\n",
    "\n",
    "### Practical Applications:\n",
    "\n",
    "- **Multi-task deployment**: Different PEFT adapters for different tasks\n",
    "- **Resource constraints**: Minimal memory and storage requirements\n",
    "- **Rapid adaptation**: Quick fine-tuning for new domains/tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}