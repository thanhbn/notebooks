{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– DeepEval - Evaluating AI Agents and Chain-of-Thought with LangGraph\n",
    "\n",
    "ChÃ o má»«ng Ä‘áº¿n vá»›i **Notebook 4** trong series DeepEval framework!\n",
    "\n",
    "## ðŸŽ¯ Má»¥c tiÃªu cá»§a Notebook nÃ y\n",
    "\n",
    "1. **XÃ¢y dá»±ng LangGraph Agents**: Multi-tool agents vá»›i complex reasoning\n",
    "2. **Chain-of-Thought Evaluation**: ÄÃ¡nh giÃ¡ quÃ¡ trÃ¬nh suy luáº­n tá»«ng bÆ°á»›c\n",
    "3. **Agent-Specific Metrics**: LogicalFlow, ToolUsage, PlanExecution, Adaptability\n",
    "4. **Multi-Step Analysis**: Capture vÃ  evaluate intermediate reasoning steps\n",
    "5. **Comprehensive Agent Benchmarking**: End-to-end agent performance assessment\n",
    "\n",
    "## ðŸ“– Táº¡i sao Agent Evaluation phá»©c táº¡p?\n",
    "\n",
    "AI Agents vá»›i Chain-of-Thought reasoning Ä‘áº·t ra nhá»¯ng thÃ¡ch thá»©c evaluation hoÃ n toÃ n má»›i:\n",
    "\n",
    "### ðŸ” Unique Challenges:\n",
    "- **Multi-Step Reasoning**: Pháº£i evaluate cáº£ process láº«n final result\n",
    "- **Tool Usage**: Agents sá»­ dá»¥ng multiple tools - cáº§n assess appropriateness\n",
    "- **Dynamic Planning**: Plans change based on intermediate results\n",
    "- **Context Awareness**: Agents maintain state across multiple interactions\n",
    "- **Error Recovery**: How well agents handle failures vÃ  adapt\n",
    "\n",
    "### âœ… DeepEval Approach:\n",
    "- **Intermediate Step Capture**: LLMTestCase supports reasoning steps\n",
    "- **Custom G-Eval Metrics**: Specialized cho agent evaluation\n",
    "- **Multi-dimensional Assessment**: Logic, tool usage, planning, execution\n",
    "- **Temporal Analysis**: Track performance over interaction sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Pháº§n 1: Setup vÃ  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple, TypedDict\n",
    "import warnings\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DeepEval imports\n",
    "import deepeval\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Custom tools\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "plt.style.use('default')\n",
    "\n",
    "print(f\"âœ… DeepEval version: {deepeval.__version__}\")\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Check API keys\n",
    "api_keys_status = {\n",
    "    \"OpenAI\": \"âœ… Configured\" if os.getenv(\"OPENAI_API_KEY\") else \"âŒ Missing\",\n",
    "    \"Anthropic\": \"âœ… Configured\" if os.getenv(\"ANTHROPIC_API_KEY\") else \"âŒ Missing\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ”‘ API Keys Status:\")\n",
    "for provider, status in api_keys_status.items():\n",
    "    print(f\"  {provider}: {status}\")\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\nâš ï¸  Cáº§n OPENAI_API_KEY Ä‘á»ƒ cháº¡y agent evaluation!\")\n",
    "    print(\"   Táº¡o file .env vá»›i: OPENAI_API_KEY=your_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Pháº§n 2: XÃ¢y dá»±ng Custom Tools cho Agents\n",
    "\n",
    "### 2.1 Mathematical Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_math_tools():\n",
    "    \"\"\"\n",
    "    Táº¡o mathematical tools cho agents\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate(expression: str) -> str:\n",
    "        \"\"\"Calculate mathematical expressions safely\"\"\"\n",
    "        try:\n",
    "            # Safe evaluation - only allow basic math operations\n",
    "            allowed_chars = set('0123456789+-*/().^ ')\n",
    "            if not set(expression.replace(' ', '')) <= allowed_chars:\n",
    "                return \"Error: Invalid characters in expression\"\n",
    "            \n",
    "            # Replace ^ with ** for Python power operator\n",
    "            expression = expression.replace('^', '**')\n",
    "            \n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def factorial(n: str) -> str:\n",
    "        \"\"\"Calculate factorial of a number\"\"\"\n",
    "        try:\n",
    "            num = int(n)\n",
    "            if num < 0:\n",
    "                return \"Error: Factorial khÃ´ng defined cho sá»‘ Ã¢m\"\n",
    "            if num > 20:\n",
    "                return \"Error: Number quÃ¡ lá»›n (max 20)\"\n",
    "            \n",
    "            result = math.factorial(num)\n",
    "            return f\"Factorial of {num} is {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def prime_check(n: str) -> str:\n",
    "        \"\"\"Check if a number is prime\"\"\"\n",
    "        try:\n",
    "            num = int(n)\n",
    "            if num < 2:\n",
    "                return f\"{num} is not prime\"\n",
    "            \n",
    "            for i in range(2, int(num ** 0.5) + 1):\n",
    "                if num % i == 0:\n",
    "                    return f\"{num} is not prime (divisible by {i})\"\n",
    "            \n",
    "            return f\"{num} is prime\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Create LangChain tools\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"calculator\",\n",
    "            description=\"Calculate mathematical expressions. Input: mathematical expression as string (e.g., '2+3*4', '10^2')\",\n",
    "            func=calculate\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"factorial\",\n",
    "            description=\"Calculate factorial of a number. Input: positive integer as string (e.g., '5')\",\n",
    "            func=factorial\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"prime_checker\",\n",
    "            description=\"Check if a number is prime. Input: positive integer as string (e.g., '17')\",\n",
    "            func=prime_check\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return tools\n",
    "\n",
    "# Create math tools\n",
    "math_tools = create_math_tools()\n",
    "print(f\"âœ… Created {len(math_tools)} math tools:\")\n",
    "for tool in math_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Information Retrieval Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_info_tools():\n",
    "    \"\"\"\n",
    "    Táº¡o information retrieval tools\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mock knowledge base\n",
    "    knowledge_base = {\n",
    "        \"python\": \"Python lÃ  programming language high-level, interpreted, vá»›i dynamic typing vÃ  automatic memory management.\",\n",
    "        \"ai\": \"Artificial Intelligence (AI) lÃ  simulation cá»§a human intelligence trong machines Ä‘á»ƒ perform tasks requiring human cognition.\",\n",
    "        \"machine learning\": \"Machine Learning lÃ  subset cá»§a AI cho phÃ©p systems learn vÃ  improve tá»« experience mÃ  khÃ´ng cáº§n explicit programming.\",\n",
    "        \"deep learning\": \"Deep Learning sá»­ dá»¥ng neural networks vá»›i multiple layers Ä‘á»ƒ learn complex patterns trong large amounts of data.\",\n",
    "        \"langchain\": \"LangChain lÃ  framework Ä‘á»ƒ develop applications powered by language models, vá»›i components cho chains, agents, vÃ  memory.\",\n",
    "        \"langgraph\": \"LangGraph lÃ  library Ä‘á»ƒ build stateful, multi-actor applications vá»›i LLMs, using graph-based approach.\"\n",
    "    }\n",
    "    \n",
    "    def search_knowledge(query: str) -> str:\n",
    "        \"\"\"Search knowledge base for information\"\"\"\n",
    "        query_lower = query.lower().strip()\n",
    "        \n",
    "        # Exact match\n",
    "        if query_lower in knowledge_base:\n",
    "            return f\"Found: {knowledge_base[query_lower]}\"\n",
    "        \n",
    "        # Partial match\n",
    "        matches = []\n",
    "        for key, value in knowledge_base.items():\n",
    "            if query_lower in key or any(word in key for word in query_lower.split()):\n",
    "                matches.append(f\"{key}: {value}\")\n",
    "        \n",
    "        if matches:\n",
    "            return f\"Partial matches found:\\n\" + \"\\n\".join(matches)\n",
    "        \n",
    "        return f\"No information found for '{query}'. Available topics: {', '.join(knowledge_base.keys())}\"\n",
    "    \n",
    "    def get_current_time() -> str:\n",
    "        \"\"\"Get current time and date\"\"\"\n",
    "        now = datetime.now()\n",
    "        return f\"Current time: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    \n",
    "    def generate_random_number(range_str: str) -> str:\n",
    "        \"\"\"Generate random number in specified range\"\"\"\n",
    "        try:\n",
    "            if '-' in range_str:\n",
    "                min_val, max_val = map(int, range_str.split('-'))\n",
    "            else:\n",
    "                min_val, max_val = 1, int(range_str)\n",
    "            \n",
    "            result = random.randint(min_val, max_val)\n",
    "            return f\"Random number between {min_val} and {max_val}: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}. Format: 'min-max' or just 'max'\"\n",
    "    \n",
    "    # Create tools\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"knowledge_search\",\n",
    "            description=\"Search knowledge base for information about topics. Input: topic name or keyword (e.g., 'python', 'ai', 'machine learning')\",\n",
    "            func=search_knowledge\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"get_time\",\n",
    "            description=\"Get current date and time. No input required.\",\n",
    "            func=lambda x: get_current_time()\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"random_number\",\n",
    "            description=\"Generate random number. Input: range as 'min-max' (e.g., '1-100') or just max value (e.g., '50')\",\n",
    "            func=generate_random_number\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return tools\n",
    "\n",
    "# Create info tools\n",
    "info_tools = create_info_tools()\n",
    "print(f\"âœ… Created {len(info_tools)} information tools:\")\n",
    "for tool in info_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Pháº§n 3: XÃ¢y dá»±ng LangGraph Agent\n",
    "\n",
    "### 3.1 Agent State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[dict]\n",
    "    current_task: str\n",
    "    reasoning_steps: List[str]\n",
    "    tool_calls: List[dict]\n",
    "    intermediate_results: List[str]\n",
    "    final_answer: str\n",
    "    execution_status: str  # 'running', 'completed', 'error'\n",
    "    error_message: str\n",
    "\n",
    "def create_initial_state(user_input: str) -> AgentState:\n",
    "    \"\"\"\n",
    "    Create initial agent state\n",
    "    \"\"\"\n",
    "    return AgentState(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "        current_task=user_input,\n",
    "        reasoning_steps=[],\n",
    "        tool_calls=[],\n",
    "        intermediate_results=[],\n",
    "        final_answer=\"\",\n",
    "        execution_status=\"running\",\n",
    "        error_message=\"\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Agent state structure defined\")\n",
    "print(\"State components:\", list(AgentState.__annotations__.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Agent Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_nodes():\n",
    "    \"\"\"\n",
    "    Create LangGraph agent node functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine all tools\n",
    "    all_tools = math_tools + info_tools\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"âŒ Cannot create agent nodes without OpenAI API key\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Create LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    def planning_node(state: AgentState) -> AgentState:\n",
    "        \"\"\"\n",
    "        Planning node - analyze task vÃ  create execution plan\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task = state[\"current_task\"]\n",
    "            \n",
    "            planning_prompt = f\"\"\"\n",
    "            PhÃ¢n tÃ­ch task sau vÃ  táº¡o detailed execution plan:\n",
    "            Task: {task}\n",
    "            \n",
    "            Available tools:\n",
    "            {chr(10).join([f\"- {tool.name}: {tool.description}\" for tool in all_tools])}\n",
    "            \n",
    "            HÃ£y:\n",
    "            1. PhÃ¢n tÃ­ch task requirements\n",
    "            2. Identify cáº§n tools nÃ o\n",
    "            3. Táº¡o step-by-step plan\n",
    "            4. Explain reasoning cho má»—i step\n",
    "            \n",
    "            Format: Detailed plan vá»›i clear reasoning steps.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = llm.invoke([HumanMessage(content=planning_prompt)])\n",
    "            \n",
    "            reasoning_step = f\"PLANNING: {response.content}\"\n",
    "            \n",
    "            state[\"reasoning_steps\"].append(reasoning_step)\n",
    "            state[\"messages\"].append({\"role\": \"assistant\", \"content\": f\"Plan created: {response.content[:200]}...\"})\n",
    "            \n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"execution_status\"] = \"error\"\n",
    "            state[\"error_message\"] = f\"Planning error: {str(e)}\"\n",
    "            return state\n",
    "    \n",
    "    def execution_node(state: AgentState) -> AgentState:\n",
    "        \"\"\"\n",
    "        Execution node - execute tools based on plan\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task = state[\"current_task\"]\n",
    "            \n",
    "            # Simple execution logic - analyze task vÃ  decide tools\n",
    "            execution_prompt = f\"\"\"\n",
    "            Execute this task step by step: {task}\n",
    "            \n",
    "            Based on the task, determine which tools to use vÃ  in what order.\n",
    "            For each tool call, explain why you're using it.\n",
    "            \n",
    "            Available tools: {[tool.name for tool in all_tools]}\n",
    "            \n",
    "            Provide tool calls in this format:\n",
    "            TOOL_CALL: tool_name(input)\n",
    "            REASONING: why you're using this tool\n",
    "            \"\"\"\n",
    "            \n",
    "            response = llm.invoke([HumanMessage(content=execution_prompt)])\n",
    "            \n",
    "            # Mock tool execution (simplified for demo)\n",
    "            tool_results = []\n",
    "            \n",
    "            # Check for math operations\n",
    "            if any(op in task.lower() for op in ['calculate', 'compute', '+', '-', '*', '/', 'factorial', 'prime']):\n",
    "                if 'factorial' in task.lower():\n",
    "                    # Extract number\n",
    "                    import re\n",
    "                    numbers = re.findall(r'\\d+', task)\n",
    "                    if numbers:\n",
    "                        result = math_tools[1].func(numbers[0])  # factorial tool\n",
    "                        tool_results.append(f\"factorial({numbers[0]}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"factorial\", \"input\": numbers[0], \"output\": result})\n",
    "                \n",
    "                elif 'prime' in task.lower():\n",
    "                    import re\n",
    "                    numbers = re.findall(r'\\d+', task)\n",
    "                    if numbers:\n",
    "                        result = math_tools[2].func(numbers[0])  # prime checker\n",
    "                        tool_results.append(f\"prime_check({numbers[0]}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"prime_checker\", \"input\": numbers[0], \"output\": result})\n",
    "                \n",
    "                else:\n",
    "                    # Extract expression\n",
    "                    import re\n",
    "                    math_expr = re.search(r'[\\d+\\-*/()\\s]+', task)\n",
    "                    if math_expr:\n",
    "                        expr = math_expr.group().strip()\n",
    "                        result = math_tools[0].func(expr)  # calculator\n",
    "                        tool_results.append(f\"calculate({expr}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"calculator\", \"input\": expr, \"output\": result})\n",
    "            \n",
    "            # Check for knowledge queries\n",
    "            elif any(keyword in task.lower() for keyword in ['what is', 'explain', 'about', 'python', 'ai', 'machine learning']):\n",
    "                # Extract topic\n",
    "                for topic in ['python', 'ai', 'machine learning', 'deep learning', 'langchain', 'langgraph']:\n",
    "                    if topic in task.lower():\n",
    "                        result = info_tools[0].func(topic)  # knowledge search\n",
    "                        tool_results.append(f\"knowledge_search({topic}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"knowledge_search\", \"input\": topic, \"output\": result})\n",
    "                        break\n",
    "            \n",
    "            # Check for time request\n",
    "            elif 'time' in task.lower() or 'date' in task.lower():\n",
    "                result = info_tools[1].func(\"\")  # get_time\n",
    "                tool_results.append(f\"get_time() = {result}\")\n",
    "                state[\"tool_calls\"].append({\"tool\": \"get_time\", \"input\": \"\", \"output\": result})\n",
    "            \n",
    "            state[\"intermediate_results\"].extend(tool_results)\n",
    "            state[\"reasoning_steps\"].append(f\"EXECUTION: {response.content}\")\n",
    "            \n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"execution_status\"] = \"error\"\n",
    "            state[\"error_message\"] = f\"Execution error: {str(e)}\"\n",
    "            return state\n",
    "    \n",
    "    def synthesis_node(state: AgentState) -> AgentState:\n",
    "        \"\"\"\n",
    "        Synthesis node - combine results vÃ  generate final answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task = state[\"current_task\"]\n",
    "            intermediate_results = state[\"intermediate_results\"]\n",
    "            \n",
    "            synthesis_prompt = f\"\"\"\n",
    "            Synthesize final answer tá»« tool results:\n",
    "            \n",
    "            Original task: {task}\n",
    "            \n",
    "            Tool results:\n",
    "            {chr(10).join(intermediate_results) if intermediate_results else 'No tool results'}\n",
    "            \n",
    "            Provide a comprehensive, well-structured answer that:\n",
    "            1. Directly addresses the original question\n",
    "            2. Incorporates relevant tool results\n",
    "            3. Explains the reasoning process\n",
    "            4. Provides clear conclusions\n",
    "            \"\"\"\n",
    "            \n",
    "            response = llm.invoke([HumanMessage(content=synthesis_prompt)])\n",
    "            \n",
    "            state[\"final_answer\"] = response.content\n",
    "            state[\"execution_status\"] = \"completed\"\n",
    "            state[\"reasoning_steps\"].append(f\"SYNTHESIS: Created final answer from {len(intermediate_results)} tool results\")\n",
    "            \n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"execution_status\"] = \"error\"\n",
    "            state[\"error_message\"] = f\"Synthesis error: {str(e)}\"\n",
    "            return state\n",
    "    \n",
    "    def should_continue(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Decide whether to continue or end\n",
    "        \"\"\"\n",
    "        if state[\"execution_status\"] == \"error\":\n",
    "            return \"end\"\n",
    "        elif state[\"execution_status\"] == \"completed\":\n",
    "            return \"end\"\n",
    "        elif len(state[\"reasoning_steps\"]) >= 10:  # Max steps\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "    \n",
    "    return planning_node, execution_node, synthesis_node, should_continue\n",
    "\n",
    "# Create agent nodes\n",
    "planning_node, execution_node, synthesis_node, should_continue = create_agent_nodes()\n",
    "\n",
    "if planning_node:\n",
    "    print(\"âœ… Agent nodes created successfully\")\n",
    "    print(\"Available nodes: planning, execution, synthesis\")\nelse:\n",
    "    print(\"âŒ Failed to create agent nodes - check API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build LangGraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langgraph_agent():\n",
    "    \"\"\"\n",
    "    Create complete LangGraph agent vá»›i state management\n",
    "    \"\"\"\n",
    "    \n",
    "    if not planning_node:\n",
    "        print(\"âŒ Cannot create LangGraph agent without node functions\")\n",
    "        return None\n",
    "    \n",
    "    # Create the graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"planning\", planning_node)\n",
    "    workflow.add_node(\"execution\", execution_node)\n",
    "    workflow.add_node(\"synthesis\", synthesis_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.set_entry_point(\"planning\")\n",
    "    workflow.add_edge(\"planning\", \"execution\")\n",
    "    workflow.add_edge(\"execution\", \"synthesis\")\n",
    "    workflow.add_edge(\"synthesis\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    agent = workflow.compile()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Create the agent\n",
    "langgraph_agent = create_langgraph_agent()\n",
    "\n",
    "if langgraph_agent:\n",
    "    print(\"âœ… LangGraph agent created successfully!\")\n",
    "    print(\"Agent workflow: planning â†’ execution â†’ synthesis â†’ end\")\nelse:\n",
    "    print(\"âŒ Failed to create LangGraph agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test Agent vá»›i Sample Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_execution(agent, test_queries: List[str]):\n",
    "    \"\"\"\n",
    "    Test agent vá»›i different types of queries\n",
    "    \"\"\"\n",
    "    \n",
    "    if not agent:\n",
    "        print(\"âŒ No agent available for testing\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"ðŸ§ª Testing LangGraph Agent vá»›i Multiple Queries\\n\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"ðŸ” Test {i}: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Create initial state\n",
    "            initial_state = create_initial_state(query)\n",
    "            \n",
    "            # Execute agent\n",
    "            start_time = time.time()\n",
    "            final_state = agent.invoke(initial_state)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Extract results\n",
    "            result = {\n",
    "                \"query\": query,\n",
    "                \"final_answer\": final_state.get(\"final_answer\", \"No answer generated\"),\n",
    "                \"reasoning_steps\": final_state.get(\"reasoning_steps\", []),\n",
    "                \"tool_calls\": final_state.get(\"tool_calls\", []),\n",
    "                \"intermediate_results\": final_state.get(\"intermediate_results\", []),\n",
    "                \"execution_status\": final_state.get(\"execution_status\", \"unknown\"),\n",
    "                \"execution_time\": execution_time,\n",
    "                \"error_message\": final_state.get(\"error_message\", \"\")\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Print summary\n",
    "            status_emoji = \"âœ…\" if result[\"execution_status\"] == \"completed\" else \"âŒ\"\n",
    "            print(f\"  {status_emoji} Status: {result['execution_status']}\")\n",
    "            print(f\"  â±ï¸  Execution time: {execution_time:.2f}s\")\n",
    "            print(f\"  ðŸ”§ Tools used: {len(result['tool_calls'])}\")\n",
    "            print(f\"  ðŸ§  Reasoning steps: {len(result['reasoning_steps'])}\")\n",
    "            \n",
    "            if result[\"final_answer\"]:\n",
    "                print(f\"  ðŸ’¡ Answer: {result['final_answer'][:150]}...\")\n",
    "            \n",
    "            if result[\"error_message\"]:\n",
    "                print(f\"  âš ï¸  Error: {result['error_message']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Execution failed: {e}\")\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"execution_status\": \"failed\",\n",
    "                \"error_message\": str(e)\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the factorial of 8?\",\n",
    "    \"Is 17 a prime number?\",\n",
    "    \"Calculate 15 + 25 * 3\",\n",
    "    \"What is machine learning?\",\n",
    "    \"What time is it now?\"\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "agent_test_results = test_agent_execution(langgraph_agent, test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Pháº§n 4: Agent-Specific Evaluation Metrics\n",
    "\n",
    "### 4.1 LogicalFlowMetric - ÄÃ¡nh giÃ¡ Chuá»—i Suy luáº­n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logical_flow_metric():\n",
    "    \"\"\"\n",
    "    Táº¡o G-Eval metric Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ logical flow cá»§a agent reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    Báº¡n sáº½ Ä‘Ã¡nh giÃ¡ logical flow vÃ  reasoning quality cá»§a AI agent.\n",
    "    \n",
    "    Criteria Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:\n",
    "    1. LOGICAL COHERENCE (30%):\n",
    "       - Reasoning steps follow logical sequence\n",
    "       - No contradictions between steps\n",
    "       - Clear cause-and-effect relationships\n",
    "    \n",
    "    2. PROBLEM DECOMPOSITION (25%):\n",
    "       - Complex problems broken down appropriately\n",
    "       - Sub-problems identified correctly\n",
    "       - Hierarchical thinking demonstrated\n",
    "    \n",
    "    3. STEP CLARITY (25%):\n",
    "       - Each reasoning step clearly explained\n",
    "       - Purpose of each step evident\n",
    "       - Transitions between steps smooth\n",
    "    \n",
    "    4. COMPLETENESS (20%):\n",
    "       - All necessary steps included\n",
    "       - No critical reasoning gaps\n",
    "       - Thorough analysis of the problem\n",
    "    \n",
    "    Scoring Guide:\n",
    "    - 9-10: Exceptional logical flow, crystal clear reasoning\n",
    "    - 7-8: Good logical structure, minor gaps\n",
    "    - 5-6: Adequate reasoning but some unclear steps\n",
    "    - 3-4: Poor logical flow, significant gaps\n",
    "    - 1-2: Incoherent or illogical reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Analyze the sequence of reasoning steps\",\n",
    "        \"Check for logical consistency and coherence\",\n",
    "        \"Evaluate problem decomposition approach\",\n",
    "        \"Assess clarity and explanation quality\",\n",
    "        \"Identify any reasoning gaps or issues\",\n",
    "        \"Score overall logical flow quality\"\n",
    "    ]\n",
    "    \n",
    "    logical_flow_metric = GEval(\n",
    "        name=\"Logical Flow\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Original query\n",
    "            LLMTestCase.actual_output,  # Final answer\n",
    "            \"reasoning_steps\"  # Custom field for reasoning steps\n",
    "        ],\n",
    "        threshold=7.0,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return logical_flow_metric\n",
    "\n",
    "# Create logical flow metric\n",
    "logical_flow_metric = create_logical_flow_metric()\n",
    "print(\"âœ… Logical Flow Metric created\")\n",
    "print(f\"Threshold: {logical_flow_metric.threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ToolUsageMetric - ÄÃ¡nh giÃ¡ Hiá»‡u quáº£ Sá»­ dá»¥ng Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tool_usage_metric():\n",
    "    \"\"\"\n",
    "    Táº¡o metric Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tool usage efficiency\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    ÄÃ¡nh giÃ¡ cÃ¡ch AI agent sá»­ dá»¥ng tools Ä‘á»ƒ solve problems.\n",
    "    \n",
    "    Tool Usage Criteria:\n",
    "    1. TOOL SELECTION APPROPRIATENESS (35%):\n",
    "       - Correct tools chosen for the task\n",
    "       - No unnecessary tool calls\n",
    "       - Optimal tool sequence\n",
    "    \n",
    "    2. INPUT QUALITY (25%):\n",
    "       - Tool inputs properly formatted\n",
    "       - Relevant parameters provided\n",
    "       - No malformed inputs\n",
    "    \n",
    "    3. EFFICIENCY (25%):\n",
    "       - Minimal number of tool calls needed\n",
    "       - No redundant operations\n",
    "       - Direct path to solution\n",
    "    \n",
    "    4. ERROR HANDLING (15%):\n",
    "       - Graceful handling of tool errors\n",
    "       - Appropriate fallback strategies\n",
    "       - Recovery from failures\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Perfect tool usage, optimal efficiency\n",
    "    - 7-8: Good tool selection and usage\n",
    "    - 5-6: Acceptable but suboptimal usage\n",
    "    - 3-4: Poor tool choices or inefficient usage\n",
    "    - 1-2: Inappropriate or failed tool usage\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Analyze tool selection appropriateness for the task\",\n",
    "        \"Evaluate tool input quality and formatting\",\n",
    "        \"Assess efficiency of tool usage pattern\",\n",
    "        \"Check error handling and recovery\",\n",
    "        \"Calculate overall tool usage effectiveness\"\n",
    "    ]\n",
    "    \n",
    "    tool_usage_metric = GEval(\n",
    "        name=\"Tool Usage Efficiency\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Task description\n",
    "            \"tool_calls\",  # Tool calls made\n",
    "            \"intermediate_results\"  # Tool outputs\n",
    "        ],\n",
    "        threshold=6.5,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return tool_usage_metric\n",
    "\n",
    "# Create tool usage metric\n",
    "tool_usage_metric = create_tool_usage_metric()\n",
    "print(\"âœ… Tool Usage Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PlanExecutionMetric - ÄÃ¡nh giÃ¡ Kháº£ nÄƒng Thá»±c hiá»‡n Káº¿ hoáº¡ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan_execution_metric():\n",
    "    \"\"\"\n",
    "    Táº¡o metric Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ plan execution capability\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    ÄÃ¡nh giÃ¡ kháº£ nÄƒng planning vÃ  execution cá»§a AI agent.\n",
    "    \n",
    "    Plan Execution Criteria:\n",
    "    1. PLAN QUALITY (30%):\n",
    "       - Comprehensive and well-structured plan\n",
    "       - Realistic and achievable steps\n",
    "       - Proper task decomposition\n",
    "    \n",
    "    2. EXECUTION FIDELITY (30%):\n",
    "       - Plan followed accurately\n",
    "       - Steps executed in logical order\n",
    "       - Minimal deviation from plan\n",
    "    \n",
    "    3. ADAPTABILITY (25%):\n",
    "       - Adjustments made when needed\n",
    "       - Handling of unexpected results\n",
    "       - Plan refinement during execution\n",
    "    \n",
    "    4. GOAL ACHIEVEMENT (15%):\n",
    "       - Original objective met\n",
    "       - Complete task completion\n",
    "       - Quality of final outcome\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Excellent planning and flawless execution\n",
    "    - 7-8: Good plan with solid execution\n",
    "    - 5-6: Adequate planning, some execution issues\n",
    "    - 3-4: Poor planning or significant execution problems\n",
    "    - 1-2: Failed planning or execution\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Evaluate initial plan quality and structure\",\n",
    "        \"Assess execution fidelity to the plan\",\n",
    "        \"Check adaptability and plan adjustments\",\n",
    "        \"Measure goal achievement and completion\",\n",
    "        \"Score overall plan execution performance\"\n",
    "    ]\n",
    "    \n",
    "    plan_execution_metric = GEval(\n",
    "        name=\"Plan Execution\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Original task\n",
    "            LLMTestCase.actual_output,  # Final result\n",
    "            \"reasoning_steps\",  # Planning and execution steps\n",
    "            \"execution_status\"  # Final status\n",
    "        ],\n",
    "        threshold=7.0,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return plan_execution_metric\n",
    "\n",
    "# Create plan execution metric\n",
    "plan_execution_metric = create_plan_execution_metric()\n",
    "print(\"âœ… Plan Execution Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 AdaptabilityMetric - ÄÃ¡nh giÃ¡ Kháº£ nÄƒng ThÃ­ch á»©ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adaptability_metric():\n",
    "    \"\"\"\n",
    "    Táº¡o metric Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ adaptability cá»§a agent\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    ÄÃ¡nh giÃ¡ kháº£ nÄƒng thÃ­ch á»©ng vÃ  flexibility cá»§a AI agent.\n",
    "    \n",
    "    Adaptability Criteria:\n",
    "    1. CONTEXT AWARENESS (30%):\n",
    "       - Understanding of changing context\n",
    "       - Recognition of new information\n",
    "       - Appropriate response to context shifts\n",
    "    \n",
    "    2. STRATEGY ADJUSTMENT (25%):\n",
    "       - Modification of approach when needed\n",
    "       - Alternative strategy exploration\n",
    "       - Dynamic problem-solving\n",
    "    \n",
    "    3. ERROR RECOVERY (25%):\n",
    "       - Grace handling of errors\n",
    "       - Learning from mistakes\n",
    "       - Resilient continuation\n",
    "    \n",
    "    4. FLEXIBLE THINKING (20%):\n",
    "       - Creative problem-solving approaches\n",
    "       - Multiple solution pathways\n",
    "       - Open-minded reasoning\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Highly adaptable, excellent flexibility\n",
    "    - 7-8: Good adaptability, handles changes well\n",
    "    - 5-6: Moderate adaptability, some rigidity\n",
    "    - 3-4: Poor adaptability, struggles with changes\n",
    "    - 1-2: Inflexible, cannot adapt to new situations\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Assess context awareness and recognition\",\n",
    "        \"Evaluate strategy adjustment capabilities\",\n",
    "        \"Check error recovery and resilience\",\n",
    "        \"Analyze flexible thinking patterns\",\n",
    "        \"Score overall adaptability performance\"\n",
    "    ]\n",
    "    \n",
    "    adaptability_metric = GEval(\n",
    "        name=\"Adaptability\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Task/scenario\n",
    "            \"reasoning_steps\",  # Decision making process\n",
    "            \"tool_calls\",  # Adaptive tool usage\n",
    "            \"error_message\"  # Error handling\n",
    "        ],\n",
    "        threshold=6.5,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return adaptability_metric\n",
    "\n",
    "# Create adaptability metric\n",
    "adaptability_metric = create_adaptability_metric()\n",
    "print(\"âœ… Adaptability Metric created\")\n",
    "print(\"\\nðŸŽ¯ All Agent Metrics Created:\")\n",
    "print(\"  1. Logical Flow - Reasoning quality\")\n",
    "print(\"  2. Tool Usage - Tool efficiency\")\n",
    "print(\"  3. Plan Execution - Planning & execution\")\n",
    "print(\"  4. Adaptability - Flexibility & recovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Pháº§n 5: Comprehensive Agent Evaluation\n",
    "\n",
    "### 5.1 Agent Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEvaluationPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation pipeline cho AI agents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        self.model = model\n",
    "        self.evaluation_history = []\n",
    "        \n",
    "        # Initialize agent-specific metrics\n",
    "        self.metrics = {\n",
    "            \"logical_flow\": logical_flow_metric,\n",
    "            \"tool_usage\": tool_usage_metric,\n",
    "            \"plan_execution\": plan_execution_metric,\n",
    "            \"adaptability\": adaptability_metric\n",
    "        }\n",
    "    \n",
    "    def evaluate_agent_execution(self, agent_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate single agent execution result\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create specialized test case for agent evaluation\n",
    "        test_case = LLMTestCase(\n",
    "            input=agent_result[\"query\"],\n",
    "            actual_output=agent_result.get(\"final_answer\", \"No answer\"),\n",
    "            # Add custom fields for agent evaluation\n",
    "            context=agent_result.get(\"reasoning_steps\", []),\n",
    "            retrieval_context=agent_result.get(\"intermediate_results\", [])\n",
    "        )\n",
    "        \n",
    "        # Add agent-specific data as attributes\n",
    "        test_case.reasoning_steps = \"\\n\".join(agent_result.get(\"reasoning_steps\", []))\n",
    "        test_case.tool_calls = json.dumps(agent_result.get(\"tool_calls\", []), indent=2)\n",
    "        test_case.intermediate_results = \"\\n\".join(agent_result.get(\"intermediate_results\", []))\n",
    "        test_case.execution_status = agent_result.get(\"execution_status\", \"unknown\")\n",
    "        test_case.error_message = agent_result.get(\"error_message\", \"\")\n",
    "        \n",
    "        results = {\n",
    "            \"query\": agent_result[\"query\"],\n",
    "            \"execution_time\": agent_result.get(\"execution_time\", 0),\n",
    "            \"execution_status\": agent_result.get(\"execution_status\", \"unknown\"),\n",
    "            \"tool_calls_count\": len(agent_result.get(\"tool_calls\", [])),\n",
    "            \"reasoning_steps_count\": len(agent_result.get(\"reasoning_steps\", [])),\n",
    "            \"metrics\": {},\n",
    "            \"overall_score\": 0,\n",
    "            \"pass_count\": 0,\n",
    "            \"total_metrics\": len(self.metrics)\n",
    "        }\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        metric_scores = []\n",
    "        \n",
    "        for metric_name, metric in self.metrics.items():\n",
    "            try:\n",
    "                # Create fresh metric instance\n",
    "                fresh_metric = GEval(\n",
    "                    name=metric.name,\n",
    "                    criteria=metric.criteria,\n",
    "                    evaluation_steps=metric.evaluation_steps,\n",
    "                    evaluation_params=metric.evaluation_params,\n",
    "                    threshold=metric.threshold,\n",
    "                    model=metric.model,\n",
    "                    include_reason=True\n",
    "                )\n",
    "                \n",
    "                fresh_metric.measure(test_case)\n",
    "                \n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": fresh_metric.score,\n",
    "                    \"passed\": fresh_metric.is_successful(),\n",
    "                    \"reason\": fresh_metric.reason,\n",
    "                    \"threshold\": fresh_metric.threshold\n",
    "                }\n",
    "                \n",
    "                metric_scores.append(fresh_metric.score)\n",
    "                if fresh_metric.is_successful():\n",
    "                    results[\"pass_count\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": 0,\n",
    "                    \"passed\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                metric_scores.append(0)\n",
    "        \n",
    "        # Calculate overall score\n",
    "        if metric_scores:\n",
    "            results[\"overall_score\"] = np.mean(metric_scores)\n",
    "        \n",
    "        # Store in history\n",
    "        self.evaluation_history.append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_evaluate(self, agent_results: List[Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Batch evaluation cá»§a multiple agent executions\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ðŸš€ Evaluating {len(agent_results)} Agent Executions\\n\")\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for i, agent_result in enumerate(agent_results):\n",
    "            if \"query\" not in agent_result:\n",
    "                continue\n",
    "                \n",
    "            print(f\"ðŸ” Evaluating {i+1}/{len(agent_results)}: {agent_result['query'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                result = self.evaluate_agent_execution(agent_result)\n",
    "                evaluation_results.append(result)\n",
    "                \n",
    "                # Quick summary\n",
    "                print(f\"  âœ… Overall Score: {result['overall_score']:.1f}/10\")\n",
    "                print(f\"  ðŸ“Š Passed: {result['pass_count']}/{result['total_metrics']} metrics\")\n",
    "                print(f\"  â±ï¸  Execution: {result['execution_time']:.2f}s\")\n",
    "                print(f\"  ðŸ”§ Tools: {result['tool_calls_count']}, Steps: {result['reasoning_steps_count']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Evaluation error: {e}\")\n",
    "                evaluation_results.append({\n",
    "                    \"query\": agent_result[\"query\"],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive performance summary\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.evaluation_history:\n",
    "            return {\"message\": \"No evaluations performed yet\"}\n",
    "        \n",
    "        # Collect metrics data\n",
    "        metric_summaries = {}\n",
    "        overall_scores = []\n",
    "        execution_times = []\n",
    "        tool_usage_counts = []\n",
    "        \n",
    "        for result in self.evaluation_history:\n",
    "            if \"overall_score\" in result:\n",
    "                overall_scores.append(result[\"overall_score\"])\n",
    "            if \"execution_time\" in result:\n",
    "                execution_times.append(result[\"execution_time\"])\n",
    "            if \"tool_calls_count\" in result:\n",
    "                tool_usage_counts.append(result[\"tool_calls_count\"])\n",
    "            \n",
    "            if \"metrics\" in result:\n",
    "                for metric_name, metric_data in result[\"metrics\"].items():\n",
    "                    if \"score\" in metric_data:\n",
    "                        if metric_name not in metric_summaries:\n",
    "                            metric_summaries[metric_name] = []\n",
    "                        metric_summaries[metric_name].append(metric_data[\"score\"])\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            \"total_evaluations\": len(self.evaluation_history),\n",
    "            \"overall_performance\": {\n",
    "                \"average_score\": round(np.mean(overall_scores), 2) if overall_scores else 0,\n",
    "                \"score_std\": round(np.std(overall_scores), 2) if overall_scores else 0,\n",
    "                \"min_score\": round(min(overall_scores), 2) if overall_scores else 0,\n",
    "                \"max_score\": round(max(overall_scores), 2) if overall_scores else 0\n",
    "            },\n",
    "            \"performance_metrics\": {\n",
    "                \"average_execution_time\": round(np.mean(execution_times), 2) if execution_times else 0,\n",
    "                \"average_tool_usage\": round(np.mean(tool_usage_counts), 1) if tool_usage_counts else 0\n",
    "            },\n",
    "            \"metric_performance\": {}\n",
    "        }\n",
    "        \n",
    "        for metric_name, scores in metric_summaries.items():\n",
    "            summary[\"metric_performance\"][metric_name] = {\n",
    "                \"average_score\": round(np.mean(scores), 2),\n",
    "                \"min_score\": round(min(scores), 2),\n",
    "                \"max_score\": round(max(scores), 2),\n",
    "                \"std_dev\": round(np.std(scores), 2)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create agent evaluation pipeline\n",
    "agent_eval_pipeline = AgentEvaluationPipeline()\n",
    "print(\"âœ… Agent Evaluation Pipeline created successfully!\")\n",
    "print(f\"Available metrics: {list(agent_eval_pipeline.metrics.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Comprehensive Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation on agent test results\n",
    "if agent_test_results:\n",
    "    comprehensive_agent_results = agent_eval_pipeline.batch_evaluate(agent_test_results)\n",
    "    agent_performance_summary = agent_eval_pipeline.get_performance_summary()\nelse:\n",
    "    print(\"âŒ No agent test results available for evaluation\")\n",
    "    comprehensive_agent_results = []\n",
    "    agent_performance_summary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive agent evaluation results\n",
    "def display_agent_evaluation_results(results, summary):\n",
    "    \"\"\"\n",
    "    Display vÃ  analyze agent evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âŒ No agent evaluation results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"ðŸ“Š Comprehensive Agent Evaluation Results\\n\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"overall_score\" in result:\n",
    "            row = {\n",
    "                \"Query\": result[\"query\"][:30] + \"...\",\n",
    "                \"Overall_Score\": result[\"overall_score\"],\n",
    "                \"Pass_Rate\": f\"{result['pass_count']}/{result['total_metrics']}\",\n",
    "                \"Exec_Time\": result[\"execution_time\"],\n",
    "                \"Tools_Used\": result[\"tool_calls_count\"],\n",
    "                \"Reasoning_Steps\": result[\"reasoning_steps_count\"]\n",
    "            }\n",
    "            \n",
    "            # Add individual metric scores\n",
    "            for metric_name, metric_data in result.get(\"metrics\", {}).items():\n",
    "                if \"score\" in metric_data:\n",
    "                    row[f\"{metric_name.replace('_', ' ').title()}\"] = metric_data[\"score\"]\n",
    "            \n",
    "            summary_data.append(row)\n",
    "    \n",
    "    if summary_data:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.round(1).to_string(index=False))\n",
    "    \n",
    "    # Display performance summary\n",
    "    print(f\"\\nðŸ“ˆ Agent Performance Summary:\")\n",
    "    if \"overall_performance\" in summary:\n",
    "        perf = summary[\"overall_performance\"]\n",
    "        print(f\"  Total Evaluations: {summary['total_evaluations']}\")\n",
    "        print(f\"  Average Overall Score: {perf['average_score']}/10\")\n",
    "        print(f\"  Score Range: {perf['min_score']} - {perf['max_score']}\")\n",
    "        print(f\"  Score Std Dev: {perf['score_std']}\")\n",
    "    \n",
    "    if \"performance_metrics\" in summary:\n",
    "        perf_metrics = summary[\"performance_metrics\"]\n",
    "        print(f\"\\nâš¡ Performance Metrics:\")\n",
    "        print(f\"  Average Execution Time: {perf_metrics['average_execution_time']}s\")\n",
    "        print(f\"  Average Tool Usage: {perf_metrics['average_tool_usage']} tools/query\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Agent-Specific Metric Performance:\")\n",
    "    if \"metric_performance\" in summary:\n",
    "        for metric_name, stats in summary[\"metric_performance\"].items():\n",
    "            print(f\"  {metric_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"    Average: {stats['average_score']}/10\")\n",
    "            print(f\"    Range: {stats['min_score']} - {stats['max_score']}\")\n",
    "            print(f\"    Std Dev: {stats['std_dev']}\")\n",
    "    \n",
    "    # Agent-specific insights\n",
    "    print(f\"\\nðŸ’¡ Agent Performance Insights:\")\n",
    "    \n",
    "    if \"metric_performance\" in summary:\n",
    "        metric_avgs = {name: stats['average_score'] for name, stats in summary[\"metric_performance\"].items()}\n",
    "        \n",
    "        if metric_avgs:\n",
    "            best_metric = max(metric_avgs.keys(), key=lambda k: metric_avgs[k])\n",
    "            worst_metric = min(metric_avgs.keys(), key=lambda k: metric_avgs[k])\n",
    "            \n",
    "            print(f\"  â€¢ Strongest capability: {best_metric.replace('_', ' ').title()} ({metric_avgs[best_metric]:.1f}/10)\")\n",
    "            print(f\"  â€¢ Improvement area: {worst_metric.replace('_', ' ').title()} ({metric_avgs[worst_metric]:.1f}/10)\")\n",
    "            \n",
    "            if metric_avgs[worst_metric] < 6.0:\n",
    "                print(f\"  â€¢ âš ï¸  {worst_metric.replace('_', ' ').title()} needs significant improvement\")\n",
    "    \n",
    "    if \"performance_metrics\" in summary:\n",
    "        perf_metrics = summary[\"performance_metrics\"]\n",
    "        if perf_metrics[\"average_execution_time\"] > 5.0:\n",
    "            print(f\"  â€¢ âš ï¸  Long execution times - consider optimization\")\n",
    "        if perf_metrics[\"average_tool_usage\"] > 3.0:\n",
    "            print(f\"  â€¢ âš ï¸  High tool usage - may indicate inefficiency\")\n",
    "        elif perf_metrics[\"average_tool_usage\"] < 1.0:\n",
    "            print(f\"  â€¢ âš ï¸  Low tool usage - agent may not be leveraging available tools\")\n",
    "    \n",
    "    return df if 'df' in locals() else None\n",
    "\n",
    "# Display agent evaluation results\n",
    "agent_results_df = display_agent_evaluation_results(comprehensive_agent_results, agent_performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Pháº§n 6: Agent Performance Visualization\n",
    "\n",
    "### 6.1 Agent Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_agent_performance(results, summary):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations cho agent performance\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results or not summary.get(\"metric_performance\"):\n",
    "        print(\"âŒ Insufficient data for agent visualization\")\n",
    "        return\n",
    "    \n",
    "    # Setup plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Agent Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Agent Metric Performance Radar Chart\n",
    "    metric_names = list(summary[\"metric_performance\"].keys())\n",
    "    metric_scores = [summary[\"metric_performance\"][name][\"average_score\"] for name in metric_names]\n",
    "    \n",
    "    # Convert to radar chart data\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metric_names), endpoint=False)\n",
    "    metric_scores += metric_scores[:1]  # Complete the circle\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 3, 1, projection='polar')\n",
    "    ax_radar.plot(angles, metric_scores, 'o-', linewidth=2, color='blue')\n",
    "    ax_radar.fill(angles, metric_scores, alpha=0.25, color='blue')\n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels([name.replace('_', '\\n').title() for name in metric_names])\n",
    "    ax_radar.set_ylim(0, 10)\n",
    "    ax_radar.set_title('Agent Capability Radar', pad=20)\n",
    "    ax_radar.grid(True)\n",
    "    \n",
    "    # 2. Execution Time vs Performance\n",
    "    exec_times = []\n",
    "    overall_scores = []\n",
    "    tool_counts = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"overall_score\" in result and \"execution_time\" in result:\n",
    "            exec_times.append(result[\"execution_time\"])\n",
    "            overall_scores.append(result[\"overall_score\"])\n",
    "            tool_counts.append(result.get(\"tool_calls_count\", 0))\n",
    "    \n",
    "    if exec_times and overall_scores:\n",
    "        scatter = axes[0,1].scatter(exec_times, overall_scores, c=tool_counts, \n",
    "                                  cmap='viridis', alpha=0.7, s=100)\n",
    "        axes[0,1].set_xlabel('Execution Time (seconds)')\n",
    "        axes[0,1].set_ylabel('Overall Score')\n",
    "        axes[0,1].set_title('Execution Time vs Performance')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[0,1])\n",
    "        cbar.set_label('Tool Usage Count')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(exec_times) > 1:\n",
    "            z = np.polyfit(exec_times, overall_scores, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[0,1].plot(exec_times, p(exec_times), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # 3. Metric Score Distribution\n",
    "    all_metric_scores = []\n",
    "    all_metric_names = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"metrics\" in result:\n",
    "            for metric_name, metric_data in result[\"metrics\"].items():\n",
    "                if \"score\" in metric_data:\n",
    "                    all_metric_scores.append(metric_data[\"score\"])\n",
    "                    all_metric_names.append(metric_name)\n",
    "    \n",
    "    if all_metric_scores:\n",
    "        axes[0,2].hist(all_metric_scores, bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[0,2].set_xlabel('Metric Scores')\n",
    "        axes[0,2].set_ylabel('Frequency')\n",
    "        axes[0,2].set_title('Agent Metric Score Distribution')\n",
    "        axes[0,2].axvline(x=np.mean(all_metric_scores), color='red', linestyle='--', \n",
    "                         label=f'Mean: {np.mean(all_metric_scores):.1f}')\n",
    "        axes[0,2].legend()\n",
    "    \n",
    "    # 4. Tool Usage Efficiency\n",
    "    tool_efficiency = []\n",
    "    query_names = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"tool_calls_count\" in result and \"overall_score\" in result:\n",
    "            tools_used = result[\"tool_calls_count\"]\n",
    "            score = result[\"overall_score\"]\n",
    "            # Efficiency = score per tool used (with minimum 1 to avoid division by zero)\n",
    "            efficiency = score / max(tools_used, 1)\n",
    "            tool_efficiency.append(efficiency)\n",
    "            query_names.append(result[\"query\"][:15] + \"...\")\n",
    "    \n",
    "    if tool_efficiency:\n",
    "        bars = axes[1,0].bar(range(len(tool_efficiency)), tool_efficiency, \n",
    "                            color=['green' if eff >= 5 else 'orange' if eff >= 3 else 'red' for eff in tool_efficiency])\n",
    "        axes[1,0].set_xlabel('Queries')\n",
    "        axes[1,0].set_ylabel('Score per Tool Used')\n",
    "        axes[1,0].set_title('Tool Usage Efficiency')\n",
    "        axes[1,0].set_xticks(range(len(query_names)))\n",
    "        axes[1,0].set_xticklabels(query_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, eff) in enumerate(zip(bars, tool_efficiency)):\n",
    "            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                          f'{eff:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 5. Reasoning Steps vs Success Rate\n",
    "    reasoning_steps = []\n",
    "    success_rates = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"reasoning_steps_count\" in result and \"pass_count\" in result and \"total_metrics\" in result:\n",
    "            steps = result[\"reasoning_steps_count\"]\n",
    "            success_rate = result[\"pass_count\"] / result[\"total_metrics\"] * 100\n",
    "            reasoning_steps.append(steps)\n",
    "            success_rates.append(success_rate)\n",
    "    \n",
    "    if reasoning_steps and success_rates:\n",
    "        axes[1,1].scatter(reasoning_steps, success_rates, alpha=0.7, s=100, color='purple')\n",
    "        axes[1,1].set_xlabel('Number of Reasoning Steps')\n",
    "        axes[1,1].set_ylabel('Success Rate (%)')\n",
    "        axes[1,1].set_title('Reasoning Depth vs Success')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(reasoning_steps) > 1:\n",
    "            z = np.polyfit(reasoning_steps, success_rates, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1,1].plot(reasoning_steps, p(reasoning_steps), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # 6. Agent Performance Heatmap\n",
    "    if len(metric_names) > 1 and len(results) > 1:\n",
    "        # Create performance matrix\n",
    "        performance_matrix = []\n",
    "        \n",
    "        for result in results:\n",
    "            if \"metrics\" in result:\n",
    "                row = []\n",
    "                for metric_name in metric_names:\n",
    "                    if metric_name in result[\"metrics\"] and \"score\" in result[\"metrics\"][metric_name]:\n",
    "                        row.append(result[\"metrics\"][metric_name][\"score\"])\n",
    "                    else:\n",
    "                        row.append(0)\n",
    "                if len(row) == len(metric_names):\n",
    "                    performance_matrix.append(row)\n",
    "        \n",
    "        if len(performance_matrix) > 1:\n",
    "            performance_df = pd.DataFrame(performance_matrix, \n",
    "                                        columns=[name.replace('_', ' ').title() for name in metric_names],\n",
    "                                        index=[f\"Query {i+1}\" for i in range(len(performance_matrix))])\n",
    "            \n",
    "            sns.heatmap(performance_df, annot=True, cmap='RdYlGn', center=5, \n",
    "                       square=False, ax=axes[1,2], cbar_kws={'shrink': 0.8})\n",
    "            axes[1,2].set_title('Performance Heatmap by Query')\n",
    "        else:\n",
    "            axes[1,2].text(0.5, 0.5, 'Insufficient data\\nfor heatmap', \n",
    "                          ha='center', va='center', transform=axes[1,2].transAxes, fontsize=12)\n",
    "            axes[1,2].set_title('Performance Heatmap by Query')\n",
    "    else:\n",
    "        axes[1,2].text(0.5, 0.5, 'Need multiple metrics\\nand queries for heatmap', \n",
    "                      ha='center', va='center', transform=axes[1,2].transAxes, fontsize=12)\n",
    "        axes[1,2].set_title('Performance Heatmap by Query')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\nðŸ” Agent Performance Insights:\")\n",
    "    \n",
    "    if metric_scores:\n",
    "        strongest_capability = metric_names[metric_scores[:-1].index(max(metric_scores[:-1]))]\n",
    "        weakest_capability = metric_names[metric_scores[:-1].index(min(metric_scores[:-1]))]\n",
    "        \n",
    "        print(f\"  â€¢ Strongest capability: {strongest_capability.replace('_', ' ').title()}\")\n",
    "        print(f\"  â€¢ Weakest capability: {weakest_capability.replace('_', ' ').title()}\")\n",
    "        \n",
    "        capability_range = max(metric_scores[:-1]) - min(metric_scores[:-1])\n",
    "        if capability_range > 3.0:\n",
    "            print(f\"  â€¢ âš ï¸  Large capability variance ({capability_range:.1f}) - uneven agent development\")\n",
    "    \n",
    "    if exec_times and overall_scores:\n",
    "        correlation = np.corrcoef(exec_times, overall_scores)[0, 1]\n",
    "        if correlation < -0.3:\n",
    "            print(f\"  â€¢ âš ï¸  Negative correlation between time and performance - efficiency issues\")\n",
    "        elif correlation > 0.3:\n",
    "            print(f\"  â€¢ âœ… Positive correlation - more time leads to better results\")\n",
    "    \n",
    "    if tool_efficiency:\n",
    "        avg_efficiency = np.mean(tool_efficiency)\n",
    "        print(f\"  â€¢ Average tool efficiency: {avg_efficiency:.1f} score per tool\")\n",
    "        \n",
    "        if avg_efficiency < 3.0:\n",
    "            print(f\"  â€¢ âš ï¸  Low tool efficiency - agent may be overusing tools\")\n",
    "        elif avg_efficiency > 7.0:\n",
    "            print(f\"  â€¢ âœ… High tool efficiency - agent uses tools effectively\")\n",
    "\n",
    "# Create agent performance visualizations\n",
    "visualize_agent_performance(comprehensive_agent_results, agent_performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Pháº§n 7: Advanced Agent Scenarios\n",
    "\n",
    "### 7.1 Complex Multi-Step Reasoning Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complex_agent_scenarios():\n",
    "    \"\"\"\n",
    "    Táº¡o complex scenarios Ä‘á»ƒ test agent reasoning capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    complex_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Mathematical Problem Solving\",\n",
    "            \"query\": \"I need to find all prime numbers between 10 and 30, then calculate the factorial of the largest prime found, and finally determine if the result is divisible by 12.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Identify prime numbers between 10 and 30\",\n",
    "                \"Find the largest prime\",\n",
    "                \"Calculate factorial of largest prime\",\n",
    "                \"Check divisibility by 12\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"prime_checker\", \"factorial\", \"calculator\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Information Synthesis\",\n",
    "            \"query\": \"What is the relationship between machine learning and deep learning? After explaining this, generate a random number between 1 and 100 and tell me if that number is prime.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Search for machine learning information\",\n",
    "                \"Search for deep learning information\",\n",
    "                \"Synthesize relationship\",\n",
    "                \"Generate random number\",\n",
    "                \"Check if number is prime\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"knowledge_search\", \"random_number\", \"prime_checker\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Time-Based Calculation\",\n",
    "            \"query\": \"What's the current time? Based on the current hour, calculate the factorial of that hour number. If the current minute is even, add 50 to the factorial result.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Get current time\",\n",
    "                \"Extract hour from time\",\n",
    "                \"Calculate factorial of hour\",\n",
    "                \"Check if minute is even\",\n",
    "                \"Conditionally add 50\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"get_time\", \"factorial\", \"calculator\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Error Recovery Scenario\",\n",
    "            \"query\": \"Calculate the factorial of 25, then find the square root of that result. If there are any issues, try alternative approaches.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Calculate factorial of 25\",\n",
    "                \"Attempt square root calculation\",\n",
    "                \"Handle potential errors\",\n",
    "                \"Provide alternative solution\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"factorial\", \"calculator\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return complex_scenarios\n",
    "\n",
    "# Create complex scenarios\n",
    "complex_scenarios = create_complex_agent_scenarios()\n",
    "\n",
    "print(\"ðŸ§© Created Complex Agent Scenarios:\")\n",
    "for i, scenario in enumerate(complex_scenarios, 1):\n",
    "    print(f\"  {i}. {scenario['name']}\")\n",
    "    print(f\"     Query: {scenario['query'][:60]}...\")\n",
    "    print(f\"     Expected tools: {', '.join(scenario['expected_tools'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Test Complex Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_complex_agent_scenarios(agent, scenarios):\n",
    "    \"\"\"\n",
    "    Test agent vá»›i complex scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    if not agent:\n",
    "        print(\"âŒ No agent available for complex scenario testing\")\n",
    "        return []\n",
    "    \n",
    "    print(\"ðŸš€ Testing Complex Agent Scenarios\\n\")\n",
    "    \n",
    "    complex_results = []\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"ðŸ§© Complex Scenario {i}: {scenario['name']}\")\n",
    "        print(f\"Query: {scenario['query']}\")\n",
    "        \n",
    "        try:\n",
    "            # Create initial state\n",
    "            initial_state = create_initial_state(scenario['query'])\n",
    "            \n",
    "            # Execute agent\n",
    "            start_time = time.time()\n",
    "            final_state = agent.invoke(initial_state)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Extract results with scenario analysis\n",
    "            result = {\n",
    "                \"scenario_name\": scenario['name'],\n",
    "                \"query\": scenario['query'],\n",
    "                \"final_answer\": final_state.get(\"final_answer\", \"No answer generated\"),\n",
    "                \"reasoning_steps\": final_state.get(\"reasoning_steps\", []),\n",
    "                \"tool_calls\": final_state.get(\"tool_calls\", []),\n",
    "                \"intermediate_results\": final_state.get(\"intermediate_results\", []),\n",
    "                \"execution_status\": final_state.get(\"execution_status\", \"unknown\"),\n",
    "                \"execution_time\": execution_time,\n",
    "                \"error_message\": final_state.get(\"error_message\", \"\"),\n",
    "                \n",
    "                # Scenario-specific analysis\n",
    "                \"expected_steps\": scenario['expected_steps'],\n",
    "                \"expected_tools\": scenario['expected_tools'],\n",
    "                \"tools_used\": [call['tool'] for call in final_state.get(\"tool_calls\", [])],\n",
    "                \"step_coverage\": 0,  # Will calculate below\n",
    "                \"tool_coverage\": 0   # Will calculate below\n",
    "            }\n",
    "            \n",
    "            # Calculate step coverage (how many expected steps were addressed)\n",
    "            reasoning_text = \" \".join(result[\"reasoning_steps\"]).lower()\n",
    "            covered_steps = 0\n",
    "            for step in scenario['expected_steps']:\n",
    "                if any(keyword.lower() in reasoning_text for keyword in step.split()):\n",
    "                    covered_steps += 1\n",
    "            result[\"step_coverage\"] = covered_steps / len(scenario['expected_steps']) if scenario['expected_steps'] else 0\n",
    "            \n",
    "            # Calculate tool coverage (how many expected tools were used)\n",
    "            used_expected_tools = set(result[\"tools_used\"]) & set(scenario['expected_tools'])\n",
    "            result[\"tool_coverage\"] = len(used_expected_tools) / len(scenario['expected_tools']) if scenario['expected_tools'] else 0\n",
    "            \n",
    "            complex_results.append(result)\n",
    "            \n",
    "            # Print detailed analysis\n",
    "            status_emoji = \"âœ…\" if result[\"execution_status\"] == \"completed\" else \"âŒ\"\n",
    "            print(f\"  {status_emoji} Status: {result['execution_status']}\")\n",
    "            print(f\"  â±ï¸  Execution time: {execution_time:.2f}s\")\n",
    "            print(f\"  ðŸŽ¯ Step coverage: {result['step_coverage']:.1%} ({covered_steps}/{len(scenario['expected_steps'])})\")\n",
    "            print(f\"  ðŸ”§ Tool coverage: {result['tool_coverage']:.1%} ({len(used_expected_tools)}/{len(scenario['expected_tools'])})\")\n",
    "            print(f\"  ðŸ“Š Tools used: {', '.join(result['tools_used']) if result['tools_used'] else 'None'}\")\n",
    "            print(f\"  ðŸ§  Reasoning steps: {len(result['reasoning_steps'])}\")\n",
    "            \n",
    "            if result[\"final_answer\"]:\n",
    "                print(f\"  ðŸ’¡ Answer: {result['final_answer'][:100]}...\")\n",
    "            \n",
    "            if result[\"error_message\"]:\n",
    "                print(f\"  âš ï¸  Error: {result['error_message']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Execution failed: {e}\")\n",
    "            complex_results.append({\n",
    "                \"scenario_name\": scenario['name'],\n",
    "                \"query\": scenario['query'],\n",
    "                \"execution_status\": \"failed\",\n",
    "                \"error_message\": str(e)\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Overall complex scenario analysis\n",
    "    if complex_results:\n",
    "        successful_scenarios = [r for r in complex_results if r.get(\"execution_status\") == \"completed\"]\n",
    "        \n",
    "        if successful_scenarios:\n",
    "            avg_step_coverage = np.mean([r.get(\"step_coverage\", 0) for r in successful_scenarios])\n",
    "            avg_tool_coverage = np.mean([r.get(\"tool_coverage\", 0) for r in successful_scenarios])\n",
    "            avg_execution_time = np.mean([r.get(\"execution_time\", 0) for r in successful_scenarios])\n",
    "            \n",
    "            print(f\"ðŸ“Š Complex Scenario Summary:\")\n",
    "            print(f\"  Successful scenarios: {len(successful_scenarios)}/{len(complex_results)}\")\n",
    "            print(f\"  Average step coverage: {avg_step_coverage:.1%}\")\n",
    "            print(f\"  Average tool coverage: {avg_tool_coverage:.1%}\")\n",
    "            print(f\"  Average execution time: {avg_execution_time:.2f}s\")\n",
    "            \n",
    "            if avg_step_coverage < 0.7:\n",
    "                print(f\"  âš ï¸  Low step coverage - agent may miss important reasoning steps\")\n",
    "            if avg_tool_coverage < 0.8:\n",
    "                print(f\"  âš ï¸  Low tool coverage - agent may not be using available tools effectively\")\n",
    "    \n",
    "    return complex_results\n",
    "\n",
    "# Test complex scenarios\n",
    "complex_scenario_results = test_complex_agent_scenarios(langgraph_agent, complex_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Evaluate Complex Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate complex scenarios vá»›i agent metrics\n",
    "if complex_scenario_results:\n",
    "    print(\"ðŸ” Evaluating Complex Scenarios vá»›i Agent Metrics\\n\")\n",
    "    \n",
    "    complex_evaluation_results = agent_eval_pipeline.batch_evaluate(complex_scenario_results)\n",
    "    complex_performance_summary = agent_eval_pipeline.get_performance_summary()\n",
    "    \n",
    "    # Compare simple vs complex performance\n",
    "    print(\"\\nðŸ“Š Simple vs Complex Scenario Comparison:\")\n",
    "    \n",
    "    # Get performance from both evaluations\n",
    "    simple_scores = [r.get(\"overall_score\", 0) for r in comprehensive_agent_results if \"overall_score\" in r]\n",
    "    complex_scores = [r.get(\"overall_score\", 0) for r in complex_evaluation_results if \"overall_score\" in r]\n",
    "    \n",
    "    if simple_scores and complex_scores:\n",
    "        simple_avg = np.mean(simple_scores)\n",
    "        complex_avg = np.mean(complex_scores)\n",
    "        \n",
    "        print(f\"  Simple scenarios average: {simple_avg:.1f}/10\")\n",
    "        print(f\"  Complex scenarios average: {complex_avg:.1f}/10\")\n",
    "        print(f\"  Performance difference: {simple_avg - complex_avg:+.1f}\")\n",
    "        \n",
    "        if complex_avg < simple_avg - 1.0:\n",
    "            print(f\"  âš ï¸  Significant performance drop on complex tasks - agent struggles with complexity\")\n",
    "        elif abs(complex_avg - simple_avg) < 0.5:\n",
    "            print(f\"  âœ… Consistent performance across complexity levels\")\n",
    "        else:\n",
    "            print(f\"  ðŸ“ˆ Agent handles complexity well\")\n",
    "\nelse:\n",
    "    print(\"âŒ No complex scenario results to evaluate\")\n",
    "    complex_evaluation_results = []\n",
    "    complex_performance_summary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Pháº§n 8: Exercises vÃ  Thá»±c hÃ nh\n",
    "\n",
    "### Exercise 1: Custom Agent Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Táº¡o custom metric cho agent creativity\n",
    "def exercise_1_creativity_metric():\n",
    "    \"\"\"\n",
    "    TODO: Táº¡o metric Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ creativity vÃ  innovation trong agent reasoning\n",
    "    YÃªu cáº§u:\n",
    "    1. Evaluate creative problem-solving approaches\n",
    "    2. Assess innovative use of available tools\n",
    "    3. Measure originality in reasoning steps\n",
    "    4. Test vá»›i various creative scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Define creativity evaluation criteria\n",
    "    creativity_criteria = \"\"\"\n",
    "    Your creativity evaluation criteria here...\n",
    "    Focus on:\n",
    "    - Novel problem-solving approaches\n",
    "    - Innovative tool combinations\n",
    "    - Original reasoning patterns\n",
    "    - Creative solution pathways\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create G-Eval metric\n",
    "    \n",
    "    # TODO: Test vá»›i creative scenarios\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"ðŸ’¡ Exercise 1 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Focus on non-standard approaches to problem solving\")\n",
    "print(\"- Consider tool usage patterns vÃ  combinations\")\n",
    "print(\"- Evaluate reasoning originality vÃ  innovation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Agent Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Optimize agent performance based on evaluation results\n",
    "def exercise_2_agent_optimization():\n",
    "    \"\"\"\n",
    "    TODO: Analyze evaluation results vÃ  propose agent improvements\n",
    "    YÃªu cáº§u:\n",
    "    1. Identify performance bottlenecks from evaluation data\n",
    "    2. Propose specific improvements cho agent architecture\n",
    "    3. Design enhanced reasoning prompts\n",
    "    4. Create optimized tool selection strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Analyze current performance data\n",
    "    performance_analysis = {\n",
    "        \"weak_areas\": [],\n",
    "        \"strong_areas\": [],\n",
    "        \"improvement_opportunities\": []\n",
    "    }\n",
    "    \n",
    "    # TODO: Design improvements\n",
    "    \n",
    "    # TODO: Create enhanced agent version\n",
    "    \n",
    "    # TODO: Compare performance improvements\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"ðŸ’¡ Exercise 2 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Analyze metric scores Ä‘á»ƒ identify improvement areas\")\n",
    "print(\"- Consider prompt engineering improvements\")\n",
    "print(\"- Design better tool selection logic\")\n",
    "print(\"- Implement enhanced error recovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Multi-Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create multi-agent evaluation framework\n",
    "def exercise_3_multi_agent_evaluation():\n",
    "    \"\"\"\n",
    "    TODO: Build framework Ä‘á»ƒ compare multiple agents\n",
    "    YÃªu cáº§u:\n",
    "    1. Create different agent configurations\n",
    "    2. Design comparative evaluation metrics\n",
    "    3. Run head-to-head agent comparisons\n",
    "    4. Analyze strengths/weaknesses cá»§a each agent\n",
    "    5. Determine optimal agent configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    class MultiAgentEvaluator:\n",
    "        def __init__(self):\n",
    "            # TODO: Initialize multiple agent configurations\n",
    "            pass\n",
    "        \n",
    "        def create_agent_variants(self):\n",
    "            # TODO: Create different agent configurations\n",
    "            # - Different reasoning strategies\n",
    "            # - Different tool sets\n",
    "            # - Different prompt templates\n",
    "            pass\n",
    "        \n",
    "        def comparative_evaluation(self, test_scenarios):\n",
    "            # TODO: Run same scenarios across all agents\n",
    "            # TODO: Compare performance metrics\n",
    "            # TODO: Identify best agent for each scenario type\n",
    "            pass\n",
    "        \n",
    "        def generate_recommendations(self):\n",
    "            # TODO: Recommend optimal agent configuration\n",
    "            pass\n",
    "    \n",
    "    # TODO: Implement and test\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"ðŸ’¡ Exercise 3 Template created. Complete the class above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Create agents vá»›i different capabilities\")\n",
    "print(\"- Use consistent evaluation criteria\")\n",
    "print(\"- Compare both performance vÃ  efficiency\")\n",
    "print(\"- Consider task-specific agent selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Tá»•ng káº¿t vÃ  Next Steps\n",
    "\n",
    "### ðŸ† Nhá»¯ng gÃ¬ Ä‘Ã£ há»c trong Notebook nÃ y:\n",
    "\n",
    "1. **âœ… LangGraph Agent Construction**\n",
    "   - Multi-node agent architecture vá»›i StateGraph\n",
    "   - Custom tool creation vÃ  integration\n",
    "   - Agent state management vÃ  workflow control\n",
    "   - Planning â†’ Execution â†’ Synthesis pipeline\n",
    "\n",
    "2. **âœ… Agent-Specific Evaluation Metrics**\n",
    "   - **LogicalFlowMetric**: Reasoning coherence vÃ  clarity\n",
    "   - **ToolUsageMetric**: Tool selection vÃ  efficiency\n",
    "   - **PlanExecutionMetric**: Planning vÃ  execution fidelity\n",
    "   - **AdaptabilityMetric**: Flexibility vÃ  error recovery\n",
    "\n",
    "3. **âœ… Chain-of-Thought Evaluation**\n",
    "   - Intermediate step capture vÃ  analysis\n",
    "   - Multi-dimensional reasoning assessment\n",
    "   - Reasoning quality measurement\n",
    "   - Step-by-step logical flow evaluation\n",
    "\n",
    "4. **âœ… Comprehensive Agent Assessment**\n",
    "   - End-to-end agent evaluation pipeline\n",
    "   - Complex scenario testing\n",
    "   - Performance benchmarking\n",
    "   - Comparative analysis frameworks\n",
    "\n",
    "5. **âœ… Advanced Analysis Techniques**\n",
    "   - Agent performance visualization\n",
    "   - Tool usage efficiency analysis\n",
    "   - Reasoning depth vs success correlation\n",
    "   - Multi-scenario performance tracking\n",
    "\n",
    "### ðŸš€ Next Steps - Notebook 5: Feedback Loop & Regenerating CoT\n",
    "\n",
    "Trong notebook cuá»‘i cÃ¹ng, chÃºng ta sáº½ há»c:\n",
    "\n",
    "- ðŸ”„ **Automated Feedback Loops**: Self-improving agent systems\n",
    "- ðŸ§  **CoT Regeneration**: Dynamic reasoning improvement\n",
    "- ðŸ“ˆ **Performance Optimization**: Iterative agent enhancement\n",
    "- ðŸŽ¯ **Production Deployment**: Real-world agent evaluation\n",
    "- ðŸ” **Continuous Monitoring**: Long-term performance tracking\n",
    "\n",
    "### ðŸ’¡ Key Insights tá»« Agent Evaluation:\n",
    "\n",
    "- **Multi-Step Reasoning**: Cáº§n evaluate cáº£ process láº«n outcome\n",
    "- **Tool Usage Patterns**: Efficiency matters more than quantity\n",
    "- **Adaptability**: Critical for real-world deployment\n",
    "- **Context Awareness**: Essential for complex scenarios\n",
    "- **Error Recovery**: Separates good agents from great ones\n",
    "\n",
    "### ðŸŽ¯ Agent Evaluation Best Practices:\n",
    "\n",
    "1. **Capture Intermediate Steps** cho detailed analysis\n",
    "2. **Test vá»›i Complex Scenarios** beyond simple queries\n",
    "3. **Measure Multiple Dimensions** of agent performance\n",
    "4. **Compare Agent Variants** Ä‘á»ƒ optimize configuration\n",
    "5. **Monitor Tool Usage Efficiency** Ä‘á»ƒ avoid waste\n",
    "6. **Evaluate Reasoning Quality** not just final answers\n",
    "\n",
    "### ðŸ“Š Agent vs Traditional System Evaluation:\n",
    "\n",
    "| Aspect | Traditional Systems | AI Agents vá»›i CoT |\n",
    "|--------|--------------------|-----------------|\n",
    "| **Evaluation Focus** | Final output only | Process + outcome |\n",
    "| **Reasoning** | Black box | Transparent steps |\n",
    "| **Tool Usage** | Fixed workflow | Dynamic selection |\n",
    "| **Adaptability** | Static rules | Learning capability |\n",
    "| **Error Handling** | Predefined paths | Creative recovery |\n",
    "| **Complexity** | Linear scaling | Emergent behavior |\n",
    "\n",
    "### ðŸ” Performance Insights Summary:\n",
    "\n",
    "Tá»« evaluation results, chÃºng ta tháº¥y:\n",
    "- Agents perform better vá»›i **well-defined tools**\n",
    "- **Planning quality** directly impacts execution success\n",
    "- **Tool efficiency** lÃ  key differentiator\n",
    "- **Complex scenarios** reveal agent limitations\n",
    "- **Reasoning transparency** enables better debugging\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Exceptional Achievement!\n",
    "\n",
    "Báº¡n Ä‘Ã£ mastered advanced AI agent evaluation vá»›i LangGraph vÃ  Chain-of-Thought reasoning! \n",
    "\n",
    "Ready for the final challenge: **Notebook 5: Feedback Loop & Regenerating CoT**? ðŸš€ðŸ”„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}