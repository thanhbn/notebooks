{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Iterative Code Re-generation with Agent-Based Approach\n",
    "\n",
    "## Mục tiêu học tập\n",
    "\n",
    "Notebook này tập trung vào khái niệm **Iterative Code Re-generation** (Section 3.2) - cốt lõi của CoderGen framework. Chúng ta sẽ:\n",
    "\n",
    "1. Hiểu cách agent-based system hoạt động trong code generation\n",
    "2. Implement iterative refinement loop với feedback\n",
    "3. Xây dựng state management cho multi-iteration process\n",
    "4. Tạo complete agent workflow từ generation đến validation\n",
    "\n",
    "## Trích xuất từ paper\n",
    "\n",
    "**Section 3.2: Iterative Code Re-generation** (trang 6)\n",
    "- \"The erroneous code snippet, along with the suggestions and the original instruction, are fed back into the language model\"\n",
    "- \"This iterative cycle ensures that the generated code not only resolves the immediate issues but also improves in quality and robustness with each iteration\"\n",
    "- \"The framework's ability to learn from its mistakes and adapt its code generation strategy based on real-time feedback\"\n",
    "- Figure 2 shows complete CoderGen architecture với feedback loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lý thuyết: Agent-Based Iterative Refinement\n",
    "\n",
    "### 1.1. Agent Architecture cho Code Generation\n",
    "\n",
    "Agent-based approach trong CoderGen có các components:\n",
    "\n",
    "1. **State**: Current code, error history, iteration count\n",
    "2. **Actions**: Generate, Execute, Analyze, Regenerate\n",
    "3. **Transitions**: Based on execution results\n",
    "4. **Memory**: Track improvements across iterations\n",
    "\n",
    "### 1.2. Iterative Improvement Formula\n",
    "\n",
    "$$Code_{n+1} = f(Code_n, Error_n, Feedback_n, Context)$$\n",
    "\n",
    "Trong đó:\n",
    "- $Code_n$: Code ở iteration thứ n\n",
    "- $Error_n$: Error analysis từ execution\n",
    "- $Feedback_n$: Suggestions và fixes\n",
    "- $Context$: Task description và history\n",
    "\n",
    "### 1.3. Convergence Strategy\n",
    "\n",
    "Agent cần balance giữa:\n",
    "- **Exploration**: Try different approaches\n",
    "- **Exploitation**: Refine current approach\n",
    "- **Termination**: Know when to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple, Any, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum, auto\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentAction(Enum):\n",
    "    \"\"\"Possible actions for the code generation agent\"\"\"\n",
    "    GENERATE = auto()\n",
    "    EXECUTE = auto()\n",
    "    ANALYZE = auto()\n",
    "    REGENERATE = auto()\n",
    "    TERMINATE = auto()\n",
    "\n",
    "class ExecutionStatus(Enum):\n",
    "    \"\"\"Status of code execution\"\"\"\n",
    "    SUCCESS = \"success\"\n",
    "    SYNTAX_ERROR = \"syntax_error\"\n",
    "    RUNTIME_ERROR = \"runtime_error\"\n",
    "    PARTIAL_SUCCESS = \"partial_success\"\n",
    "    TIMEOUT = \"timeout\"\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Complete state of the code generation agent\"\"\"\n",
    "    # Task information\n",
    "    task_description: str\n",
    "    task_requirements: List[str]\n",
    "    \n",
    "    # Current code state\n",
    "    current_code: str = \"\"\n",
    "    iteration: int = 0\n",
    "    \n",
    "    # Execution results\n",
    "    last_execution_status: Optional[ExecutionStatus] = None\n",
    "    last_error: Optional[str] = None\n",
    "    test_results: Dict[str, bool] = field(default_factory=dict)\n",
    "    \n",
    "    # History tracking\n",
    "    code_history: List[str] = field(default_factory=list)\n",
    "    error_history: List[str] = field(default_factory=list)\n",
    "    improvement_metrics: List[float] = field(default_factory=list)\n",
    "    \n",
    "    # Agent memory\n",
    "    successful_patterns: List[str] = field(default_factory=list)\n",
    "    failed_approaches: List[str] = field(default_factory=list)\n",
    "    \n",
    "    # Control parameters\n",
    "    max_iterations: int = 5\n",
    "    confidence_threshold: float = 0.8\n",
    "    current_confidence: float = 0.0\n",
    "    \n",
    "    def update_iteration(self):\n",
    "        \"\"\"Update iteration and save current state to history\"\"\"\n",
    "        self.iteration += 1\n",
    "        if self.current_code:\n",
    "            self.code_history.append(self.current_code)\n",
    "        if self.last_error:\n",
    "            self.error_history.append(self.last_error)\n",
    "    \n",
    "    def calculate_improvement(self) -> float:\n",
    "        \"\"\"Calculate improvement score based on test results\"\"\"\n",
    "        if not self.test_results:\n",
    "            return 0.0\n",
    "        \n",
    "        passed = sum(1 for result in self.test_results.values() if result)\n",
    "        total = len(self.test_results)\n",
    "        \n",
    "        return passed / total if total > 0 else 0.0\n",
    "    \n",
    "    def should_terminate(self) -> bool:\n",
    "        \"\"\"Decide if agent should stop iterating\"\"\"\n",
    "        # Success condition\n",
    "        if self.last_execution_status == ExecutionStatus.SUCCESS:\n",
    "            return True\n",
    "        \n",
    "        # Max iterations reached\n",
    "        if self.iteration >= self.max_iterations:\n",
    "            return True\n",
    "        \n",
    "        # High confidence achieved\n",
    "        if self.current_confidence >= self.confidence_threshold:\n",
    "            return True\n",
    "        \n",
    "        # No improvement in last 3 iterations\n",
    "        if len(self.improvement_metrics) >= 3:\n",
    "            recent_improvements = self.improvement_metrics[-3:]\n",
    "            if all(imp == recent_improvements[0] for imp in recent_improvements):\n",
    "                return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Execution Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeExecutor:\n",
    "    \"\"\"Safe code execution environment with timeout and error capture\"\"\"\n",
    "    \n",
    "    def __init__(self, timeout: int = 30):\n",
    "        self.timeout = timeout\n",
    "        self.execution_count = 0\n",
    "    \n",
    "    def execute_code(self, code: str, test_cases: List[Dict[str, Any]]) -> Tuple[ExecutionStatus, Dict[str, Any]]:\n",
    "        \"\"\"Execute code with test cases and return results\"\"\"\n",
    "        self.execution_count += 1\n",
    "        \n",
    "        # Create temporary file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            temp_file = f.name\n",
    "        \n",
    "        try:\n",
    "            # First check syntax\n",
    "            compile(code, temp_file, 'exec')\n",
    "        except SyntaxError as e:\n",
    "            os.unlink(temp_file)\n",
    "            return ExecutionStatus.SYNTAX_ERROR, {\n",
    "                \"error\": str(e),\n",
    "                \"line\": e.lineno,\n",
    "                \"offset\": e.offset\n",
    "            }\n",
    "        \n",
    "        # Execute with timeout (simulated)\n",
    "        results = self._simulate_execution(code, test_cases)\n",
    "        \n",
    "        os.unlink(temp_file)\n",
    "        return results\n",
    "    \n",
    "    def _simulate_execution(self, code: str, test_cases: List[Dict[str, Any]]) -> Tuple[ExecutionStatus, Dict[str, Any]]:\n",
    "        \"\"\"Simulate code execution with test cases\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # Simulate different execution scenarios\n",
    "        if \"import\" not in code:\n",
    "            return ExecutionStatus.RUNTIME_ERROR, {\n",
    "                \"error\": \"ImportError: No imports found\",\n",
    "                \"test_results\": {f\"test_{i}\": False for i in range(len(test_cases))}\n",
    "            }\n",
    "        \n",
    "        if \"def\" not in code:\n",
    "            return ExecutionStatus.RUNTIME_ERROR, {\n",
    "                \"error\": \"No function definition found\",\n",
    "                \"test_results\": {f\"test_{i}\": False for i in range(len(test_cases))}\n",
    "            }\n",
    "        \n",
    "        # Simulate test execution\n",
    "        test_results = {}\n",
    "        passed_count = 0\n",
    "        \n",
    "        for i, test in enumerate(test_cases):\n",
    "            # Improve success rate with iterations\n",
    "            success_probability = min(0.3 + (self.execution_count * 0.15), 0.9)\n",
    "            test_passed = random.random() < success_probability\n",
    "            test_results[f\"test_{i}\"] = test_passed\n",
    "            if test_passed:\n",
    "                passed_count += 1\n",
    "        \n",
    "        # Determine overall status\n",
    "        if passed_count == len(test_cases):\n",
    "            status = ExecutionStatus.SUCCESS\n",
    "        elif passed_count > 0:\n",
    "            status = ExecutionStatus.PARTIAL_SUCCESS\n",
    "        else:\n",
    "            status = ExecutionStatus.RUNTIME_ERROR\n",
    "        \n",
    "        return status, {\n",
    "            \"test_results\": test_results,\n",
    "            \"passed\": passed_count,\n",
    "            \"total\": len(test_cases),\n",
    "            \"execution_time\": random.uniform(0.1, 2.0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feedback Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackGenerator:\n",
    "    \"\"\"Generate actionable feedback from execution results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feedback_templates = self._init_templates()\n",
    "    \n",
    "    def generate_feedback(self, state: AgentState, execution_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate structured feedback for code improvement\"\"\"\n",
    "        feedback = {\n",
    "            \"iteration\": state.iteration,\n",
    "            \"status\": state.last_execution_status.value if state.last_execution_status else \"unknown\",\n",
    "            \"improvements\": [],\n",
    "            \"specific_fixes\": [],\n",
    "            \"confidence_boost\": 0.0\n",
    "        }\n",
    "        \n",
    "        # Analyze based on execution status\n",
    "        if state.last_execution_status == ExecutionStatus.SYNTAX_ERROR:\n",
    "            feedback[\"improvements\"] = self._syntax_error_feedback(state.last_error)\n",
    "            feedback[\"confidence_boost\"] = -0.2\n",
    "        \n",
    "        elif state.last_execution_status == ExecutionStatus.RUNTIME_ERROR:\n",
    "            feedback[\"improvements\"] = self._runtime_error_feedback(state.last_error)\n",
    "            feedback[\"confidence_boost\"] = -0.1\n",
    "        \n",
    "        elif state.last_execution_status == ExecutionStatus.PARTIAL_SUCCESS:\n",
    "            feedback[\"improvements\"] = self._partial_success_feedback(execution_result)\n",
    "            feedback[\"confidence_boost\"] = 0.2\n",
    "        \n",
    "        # Add specific fixes based on patterns\n",
    "        feedback[\"specific_fixes\"] = self._generate_specific_fixes(state)\n",
    "        \n",
    "        # Calculate improvement direction\n",
    "        feedback[\"improvement_direction\"] = self._calculate_improvement_direction(state)\n",
    "        \n",
    "        return feedback\n",
    "    \n",
    "    def _init_templates(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Initialize feedback templates\"\"\"\n",
    "        return {\n",
    "            \"syntax\": [\n",
    "                \"Check indentation and brackets\",\n",
    "                \"Ensure all strings are properly closed\",\n",
    "                \"Verify function definitions have colons\"\n",
    "            ],\n",
    "            \"import\": [\n",
    "                \"Add missing import statements\",\n",
    "                \"Use 'from module import function' syntax\",\n",
    "                \"Check module names are correct\"\n",
    "            ],\n",
    "            \"logic\": [\n",
    "                \"Review the algorithm logic\",\n",
    "                \"Add input validation\",\n",
    "                \"Handle edge cases\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def _syntax_error_feedback(self, error: str) -> List[str]:\n",
    "        \"\"\"Generate feedback for syntax errors\"\"\"\n",
    "        feedback = [\"Fix syntax error: \" + error]\n",
    "        \n",
    "        if \"indent\" in error.lower():\n",
    "            feedback.append(\"Check indentation - use 4 spaces consistently\")\n",
    "        if \"invalid syntax\" in error.lower():\n",
    "            feedback.append(\"Check for missing colons, brackets, or quotes\")\n",
    "        \n",
    "        return feedback\n",
    "    \n",
    "    def _runtime_error_feedback(self, error: str) -> List[str]:\n",
    "        \"\"\"Generate feedback for runtime errors\"\"\"\n",
    "        feedback = []\n",
    "        \n",
    "        if \"ImportError\" in error:\n",
    "            feedback.append(\"Add missing import statements at the beginning\")\n",
    "            feedback.append(\"Check if the module name is correct\")\n",
    "        elif \"NameError\" in error:\n",
    "            feedback.append(\"Define all variables and functions before use\")\n",
    "            feedback.append(\"Check for typos in variable names\")\n",
    "        elif \"TypeError\" in error:\n",
    "            feedback.append(\"Check data types match function expectations\")\n",
    "            feedback.append(\"Add type conversion if needed\")\n",
    "        \n",
    "        return feedback\n",
    "    \n",
    "    def _partial_success_feedback(self, results: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate feedback for partial success\"\"\"\n",
    "        feedback = []\n",
    "        \n",
    "        test_results = results.get(\"test_results\", {})\n",
    "        failed_tests = [k for k, v in test_results.items() if not v]\n",
    "        \n",
    "        if failed_tests:\n",
    "            feedback.append(f\"Failed tests: {', '.join(failed_tests)}\")\n",
    "            feedback.append(\"Review logic for edge cases\")\n",
    "            feedback.append(\"Add more comprehensive error handling\")\n",
    "        \n",
    "        return feedback\n",
    "    \n",
    "    def _generate_specific_fixes(self, state: AgentState) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate specific code fixes based on state\"\"\"\n",
    "        fixes = []\n",
    "        \n",
    "        # Check for common patterns in code\n",
    "        if state.current_code:\n",
    "            if \"transformers\" in state.task_description and \"import transformers\" not in state.current_code:\n",
    "                fixes.append({\n",
    "                    \"issue\": \"Missing transformers import\",\n",
    "                    \"fix\": \"Add 'from transformers import pipeline' at the top\"\n",
    "                })\n",
    "            \n",
    "            if \"try:\" not in state.current_code:\n",
    "                fixes.append({\n",
    "                    \"issue\": \"No error handling\",\n",
    "                    \"fix\": \"Wrap main logic in try-except block\"\n",
    "                })\n",
    "        \n",
    "        return fixes\n",
    "    \n",
    "    def _calculate_improvement_direction(self, state: AgentState) -> str:\n",
    "        \"\"\"Suggest improvement direction\"\"\"\n",
    "        if state.iteration == 1:\n",
    "            return \"Focus on getting basic structure working\"\n",
    "        elif state.last_execution_status == ExecutionStatus.PARTIAL_SUCCESS:\n",
    "            return \"Refine edge case handling\"\n",
    "        elif len(state.error_history) > 2:\n",
    "            return \"Consider alternative approach\"\n",
    "        else:\n",
    "            return \"Continue iterative refinement\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Iterative Code Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeCodeGenerator:\n",
    "    \"\"\"Generate and refine code based on feedback\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.generation_strategies = [\n",
    "            self._basic_generation,\n",
    "            self._error_aware_generation,\n",
    "            self._pattern_based_generation\n",
    "        ]\n",
    "    \n",
    "    def generate_initial_code(self, task_description: str, requirements: List[str]) -> str:\n",
    "        \"\"\"Generate initial code attempt\"\"\"\n",
    "        # Simplified code generation\n",
    "        code_template = '''\n",
    "# Task: {task}\n",
    "# Requirements: {requirements}\n",
    "\n",
    "def solve_task(input_data):\n",
    "    \"\"\"Solve the given task\"\"\"\n",
    "    # TODO: Implement solution\n",
    "    result = process_input(input_data)\n",
    "    return result\n",
    "\n",
    "def process_input(data):\n",
    "    \"\"\"Process input data\"\"\"\n",
    "    return data\n",
    "\n",
    "# Test the solution\n",
    "if __name__ == \"__main__\":\n",
    "    test_input = \"sample\"\n",
    "    print(solve_task(test_input))\n",
    "'''\n",
    "        \n",
    "        return code_template.format(\n",
    "            task=task_description,\n",
    "            requirements=\", \".join(requirements)\n",
    "        )\n",
    "    \n",
    "    def regenerate_code(self, state: AgentState, feedback: Dict[str, Any]) -> str:\n",
    "        \"\"\"Regenerate code based on state and feedback\"\"\"\n",
    "        # Choose strategy based on iteration\n",
    "        strategy_index = min(state.iteration - 1, len(self.generation_strategies) - 1)\n",
    "        strategy = self.generation_strategies[strategy_index]\n",
    "        \n",
    "        return strategy(state, feedback)\n",
    "    \n",
    "    def _basic_generation(self, state: AgentState, feedback: Dict[str, Any]) -> str:\n",
    "        \"\"\"Basic improvement strategy\"\"\"\n",
    "        improved_code = state.current_code\n",
    "        \n",
    "        # Add imports if missing\n",
    "        if \"import\" in str(feedback.get(\"improvements\", [])):\n",
    "            imports = self._generate_imports(state.task_description)\n",
    "            improved_code = imports + \"\\n\\n\" + improved_code\n",
    "        \n",
    "        # Add error handling\n",
    "        if \"error handling\" in str(feedback.get(\"improvements\", [])):\n",
    "            improved_code = self._add_error_handling(improved_code)\n",
    "        \n",
    "        return improved_code\n",
    "    \n",
    "    def _error_aware_generation(self, state: AgentState, feedback: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate code aware of previous errors\"\"\"\n",
    "        # Analyze error patterns\n",
    "        error_types = self._analyze_error_patterns(state.error_history)\n",
    "        \n",
    "        # Generate code avoiding known errors\n",
    "        if \"syntax\" in error_types:\n",
    "            return self._generate_syntax_safe_code(state)\n",
    "        elif \"import\" in error_types:\n",
    "            return self._generate_with_proper_imports(state)\n",
    "        else:\n",
    "            return self._generate_robust_code(state)\n",
    "    \n",
    "    def _pattern_based_generation(self, state: AgentState, feedback: Dict[str, Any]) -> str:\n",
    "        \"\"\"Use successful patterns from history\"\"\"\n",
    "        # Find successful patterns\n",
    "        if state.successful_patterns:\n",
    "            base_pattern = state.successful_patterns[-1]\n",
    "            return self._adapt_pattern(base_pattern, state.task_description)\n",
    "        \n",
    "        # Generate new approach\n",
    "        return self._generate_alternative_approach(state)\n",
    "    \n",
    "    def _generate_imports(self, task_description: str) -> str:\n",
    "        \"\"\"Generate appropriate imports based on task\"\"\"\n",
    "        imports = []\n",
    "        \n",
    "        if \"classification\" in task_description:\n",
    "            imports.append(\"from transformers import pipeline\")\n",
    "        if \"image\" in task_description:\n",
    "            imports.append(\"from PIL import Image\")\n",
    "            imports.append(\"import torchvision.transforms as transforms\")\n",
    "        if \"pytorch\" in task_description.lower():\n",
    "            imports.append(\"import torch\")\n",
    "        \n",
    "        return \"\\n\".join(imports)\n",
    "    \n",
    "    def _add_error_handling(self, code: str) -> str:\n",
    "        \"\"\"Add error handling to code\"\"\"\n",
    "        # Simple wrapper for demo\n",
    "        wrapped = f'''try:\n",
    "{chr(10).join(\"    \" + line for line in code.split(chr(10)))}\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {{e}}\")\n",
    "    raise\n",
    "'''\n",
    "        return wrapped\n",
    "    \n",
    "    def _analyze_error_patterns(self, error_history: List[str]) -> List[str]:\n",
    "        \"\"\"Analyze patterns in error history\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        for error in error_history:\n",
    "            if \"syntax\" in error.lower():\n",
    "                patterns.append(\"syntax\")\n",
    "            elif \"import\" in error.lower():\n",
    "                patterns.append(\"import\")\n",
    "            elif \"type\" in error.lower():\n",
    "                patterns.append(\"type\")\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _generate_syntax_safe_code(self, state: AgentState) -> str:\n",
    "        \"\"\"Generate code with careful syntax\"\"\"\n",
    "        template = '''# {task}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with proper syntax\"\"\"\n",
    "    try:\n",
    "        # Implementation\n",
    "        result = None\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error: {{e}}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(main())\n",
    "'''\n",
    "        return template.format(task=state.task_description)\n",
    "    \n",
    "    def _generate_with_proper_imports(self, state: AgentState) -> str:\n",
    "        \"\"\"Generate code with comprehensive imports\"\"\"\n",
    "        imports = self._generate_imports(state.task_description)\n",
    "        base_code = self._generate_syntax_safe_code(state)\n",
    "        return imports + \"\\n\\n\" + base_code\n",
    "    \n",
    "    def _generate_robust_code(self, state: AgentState) -> str:\n",
    "        \"\"\"Generate robust code with validation\"\"\"\n",
    "        return self._generate_with_proper_imports(state)\n",
    "    \n",
    "    def _adapt_pattern(self, pattern: str, task: str) -> str:\n",
    "        \"\"\"Adapt successful pattern to new task\"\"\"\n",
    "        # Simple adaptation for demo\n",
    "        return pattern.replace(\"# Task:\", f\"# Task: {task}\")\n",
    "    \n",
    "    def _generate_alternative_approach(self, state: AgentState) -> str:\n",
    "        \"\"\"Generate completely different approach\"\"\"\n",
    "        return self._generate_with_proper_imports(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Iterative Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeCodeAgent:\n",
    "    \"\"\"Complete agent implementing iterative code generation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.executor = CodeExecutor()\n",
    "        self.feedback_generator = FeedbackGenerator()\n",
    "        self.code_generator = IterativeCodeGenerator()\n",
    "        self.state_history = []\n",
    "    \n",
    "    async def generate_code(self, task_description: str, \n",
    "                          requirements: List[str], \n",
    "                          test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Main agent loop for code generation\"\"\"\n",
    "        \n",
    "        # Initialize state\n",
    "        state = AgentState(\n",
    "            task_description=task_description,\n",
    "            task_requirements=requirements\n",
    "        )\n",
    "        \n",
    "        # Generate initial code\n",
    "        state.current_code = self.code_generator.generate_initial_code(\n",
    "            task_description, requirements\n",
    "        )\n",
    "        \n",
    "        # Main iteration loop\n",
    "        while not state.should_terminate():\n",
    "            state.update_iteration()\n",
    "            print(f\"\\n=== Iteration {state.iteration} ===\")\n",
    "            \n",
    "            # Execute code\n",
    "            status, results = self.executor.execute_code(state.current_code, test_cases)\n",
    "            state.last_execution_status = status\n",
    "            state.test_results = results.get(\"test_results\", {})\n",
    "            \n",
    "            if \"error\" in results:\n",
    "                state.last_error = results[\"error\"]\n",
    "            \n",
    "            # Calculate improvement\n",
    "            improvement = state.calculate_improvement()\n",
    "            state.improvement_metrics.append(improvement)\n",
    "            \n",
    "            print(f\"Execution Status: {status.value}\")\n",
    "            print(f\"Test Results: {results.get('passed', 0)}/{results.get('total', 0)} passed\")\n",
    "            print(f\"Improvement Score: {improvement:.2f}\")\n",
    "            \n",
    "            # Check if we should stop\n",
    "            if state.should_terminate():\n",
    "                break\n",
    "            \n",
    "            # Generate feedback\n",
    "            feedback = self.feedback_generator.generate_feedback(state, results)\n",
    "            state.current_confidence += feedback[\"confidence_boost\"]\n",
    "            \n",
    "            print(f\"Feedback: {feedback['improvements'][:2]}\")\n",
    "            \n",
    "            # Regenerate code\n",
    "            state.current_code = self.code_generator.regenerate_code(state, feedback)\n",
    "            \n",
    "            # Save state\n",
    "            self.state_history.append(state.__dict__.copy())\n",
    "            \n",
    "            # Async delay to simulate processing\n",
    "            await asyncio.sleep(0.1)\n",
    "        \n",
    "        # Final result\n",
    "        return self._prepare_final_result(state)\n",
    "    \n",
    "    def _prepare_final_result(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Prepare final result summary\"\"\"\n",
    "        return {\n",
    "            \"success\": state.last_execution_status == ExecutionStatus.SUCCESS,\n",
    "            \"final_code\": state.current_code,\n",
    "            \"iterations\": state.iteration,\n",
    "            \"improvement_trajectory\": state.improvement_metrics,\n",
    "            \"final_test_results\": state.test_results,\n",
    "            \"confidence\": state.current_confidence,\n",
    "            \"error_count\": len(state.error_history)\n",
    "        }\n",
    "    \n",
    "    def visualize_iteration_process(self):\n",
    "        \"\"\"Visualize the iteration process\"\"\"\n",
    "        if not self.state_history:\n",
    "            print(\"No iteration history available\")\n",
    "            return\n",
    "        \n",
    "        iterations = [s[\"iteration\"] for s in self.state_history]\n",
    "        improvements = [s[\"improvement_metrics\"][-1] if s[\"improvement_metrics\"] else 0 \n",
    "                       for s in self.state_history]\n",
    "        confidence = [s[\"current_confidence\"] for s in self.state_history]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "        \n",
    "        # Improvement over iterations\n",
    "        ax1.plot(iterations, improvements, 'b-o', label='Test Success Rate')\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        ax1.set_ylabel('Success Rate')\n",
    "        ax1.set_title('Code Improvement Over Iterations')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Confidence over iterations\n",
    "        ax2.plot(iterations, confidence, 'g-s', label='Agent Confidence')\n",
    "        ax2.axhline(y=0.8, color='r', linestyle='--', label='Confidence Threshold')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Confidence')\n",
    "        ax2.set_title('Agent Confidence Evolution')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agent Workflow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_agent_workflow():\n",
    "    \"\"\"Create visual representation of agent workflow\"\"\"\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    nodes = [\n",
    "        (\"START\", {\"color\": \"green\", \"size\": 1000}),\n",
    "        (\"GENERATE\", {\"color\": \"lightblue\", \"size\": 1200}),\n",
    "        (\"EXECUTE\", {\"color\": \"yellow\", \"size\": 1200}),\n",
    "        (\"ANALYZE\", {\"color\": \"orange\", \"size\": 1200}),\n",
    "        (\"FEEDBACK\", {\"color\": \"pink\", \"size\": 1000}),\n",
    "        (\"REGENERATE\", {\"color\": \"lightcoral\", \"size\": 1200}),\n",
    "        (\"SUCCESS\", {\"color\": \"lightgreen\", \"size\": 1000}),\n",
    "        (\"TERMINATE\", {\"color\": \"red\", \"size\": 1000})\n",
    "    ]\n",
    "    \n",
    "    for node, attrs in nodes:\n",
    "        G.add_node(node, **attrs)\n",
    "    \n",
    "    # Add edges\n",
    "    edges = [\n",
    "        (\"START\", \"GENERATE\", \"Initial\"),\n",
    "        (\"GENERATE\", \"EXECUTE\", \"Run\"),\n",
    "        (\"EXECUTE\", \"ANALYZE\", \"Check\"),\n",
    "        (\"ANALYZE\", \"SUCCESS\", \"All Pass\"),\n",
    "        (\"ANALYZE\", \"FEEDBACK\", \"Errors\"),\n",
    "        (\"FEEDBACK\", \"REGENERATE\", \"Improve\"),\n",
    "        (\"REGENERATE\", \"EXECUTE\", \"Retry\"),\n",
    "        (\"ANALYZE\", \"TERMINATE\", \"Max Iter\"),\n",
    "        (\"SUCCESS\", \"TERMINATE\", \"Done\")\n",
    "    ]\n",
    "    \n",
    "    for src, dst, label in edges:\n",
    "        G.add_edge(src, dst, label=label)\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # Draw\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_colors = [G.nodes[node][\"color\"] for node in G.nodes()]\n",
    "    node_sizes = [G.nodes[node][\"size\"] for node in G.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                          node_size=node_sizes, alpha=0.9)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
    "                          arrows=True, arrowsize=20, width=2)\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "    \n",
    "    # Draw edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8)\n",
    "    \n",
    "    plt.title(\"Iterative Code Generation Agent Workflow\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the workflow\n",
    "visualize_agent_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demo: Complete Agent Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo execution\n",
    "async def demo_agent_execution():\n",
    "    \"\"\"Demonstrate the iterative agent in action\"\"\"\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = IterativeCodeAgent()\n",
    "    \n",
    "    # Define task\n",
    "    task = \"Create a text classification function using transformers pipeline\"\n",
    "    requirements = [\n",
    "        \"Use HuggingFace transformers\",\n",
    "        \"Handle empty input gracefully\",\n",
    "        \"Return confidence scores\"\n",
    "    ]\n",
    "    \n",
    "    # Define test cases\n",
    "    test_cases = [\n",
    "        {\"input\": \"This is great!\", \"expected\": \"positive\"},\n",
    "        {\"input\": \"\", \"expected\": \"error\"},\n",
    "        {\"input\": \"Terrible experience\", \"expected\": \"negative\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"Starting Iterative Code Generation Agent\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Requirements: {requirements}\")\n",
    "    print(f\"Test Cases: {len(test_cases)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run agent\n",
    "    result = await agent.generate_code(task, requirements, test_cases)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Success: {result['success']}\")\n",
    "    print(f\"Total Iterations: {result['iterations']}\")\n",
    "    print(f\"Final Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"Error Count: {result['error_count']}\")\n",
    "    print(f\"\\nImprovement Trajectory: {[f'{x:.2f}' for x in result['improvement_trajectory']]}\")\n",
    "    \n",
    "    print(\"\\nFinal Code:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(result['final_code'])\n",
    "    \n",
    "    # Visualize iteration process\n",
    "    agent.visualize_iteration_process()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the demo\n",
    "await demo_agent_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis: Convergence Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence patterns across multiple runs\n",
    "async def analyze_convergence_patterns():\n",
    "    \"\"\"Run multiple experiments to analyze convergence\"\"\"\n",
    "    \n",
    "    tasks = [\n",
    "        \"Text classification with transformers\",\n",
    "        \"Image classification with PyTorch\",\n",
    "        \"Speech recognition pipeline\",\n",
    "        \"Sentiment analysis function\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for task in tasks:\n",
    "        agent = IterativeCodeAgent()\n",
    "        result = await agent.generate_code(\n",
    "            task, \n",
    "            [\"Use appropriate library\", \"Add error handling\"],\n",
    "            [{\"test\": i} for i in range(3)]\n",
    "        )\n",
    "        results.append({\n",
    "            \"task\": task,\n",
    "            \"iterations\": result[\"iterations\"],\n",
    "            \"success\": result[\"success\"],\n",
    "            \"final_score\": result[\"improvement_trajectory\"][-1] if result[\"improvement_trajectory\"] else 0\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Iterations by task\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.bar(range(len(df)), df['iterations'])\n",
    "    ax1.set_xticks(range(len(df)))\n",
    "    ax1.set_xticklabels([t[:15] + \"...\" for t in df['task']], rotation=45)\n",
    "    ax1.set_ylabel('Iterations')\n",
    "    ax1.set_title('Iterations Required by Task')\n",
    "    \n",
    "    # Success rate\n",
    "    ax2 = axes[0, 1]\n",
    "    success_rate = df['success'].sum() / len(df) * 100\n",
    "    ax2.pie([success_rate, 100-success_rate], \n",
    "            labels=['Success', 'Failed'],\n",
    "            colors=['lightgreen', 'lightcoral'],\n",
    "            autopct='%1.1f%%')\n",
    "    ax2.set_title('Overall Success Rate')\n",
    "    \n",
    "    # Final scores\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.scatter(df['iterations'], df['final_score'], s=100, alpha=0.6)\n",
    "    ax3.set_xlabel('Iterations')\n",
    "    ax3.set_ylabel('Final Score')\n",
    "    ax3.set_title('Iterations vs Final Score')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average convergence\n",
    "    ax4 = axes[1, 1]\n",
    "    avg_iterations = df.groupby('success')['iterations'].mean()\n",
    "    ax4.bar(['Failed', 'Success'], avg_iterations)\n",
    "    ax4.set_ylabel('Average Iterations')\n",
    "    ax4.set_title('Average Iterations by Outcome')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run analysis\n",
    "convergence_data = await analyze_convergence_patterns()\n",
    "print(\"\\nConvergence Analysis Summary:\")\n",
    "print(convergence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Iterative Refinement Works**:\n",
    "   - Each iteration improves code quality\n",
    "   - Error feedback drives targeted improvements\n",
    "   - Success rate increases with iterations\n",
    "\n",
    "2. **Agent Architecture Benefits**:\n",
    "   - State tracking enables learning from history\n",
    "   - Multiple strategies prevent getting stuck\n",
    "   - Confidence tracking helps termination decisions\n",
    "\n",
    "3. **Feedback Loop Importance**:\n",
    "   - Specific error analysis → Better fixes\n",
    "   - Pattern recognition → Faster convergence\n",
    "   - Memory of successes → Reusable solutions\n",
    "\n",
    "4. **Practical Implementation Insights**:\n",
    "   - Start simple, refine incrementally\n",
    "   - Track metrics for improvement visibility\n",
    "   - Balance iteration count vs quality\n",
    "   - Use domain knowledge in feedback\n",
    "\n",
    "5. **Extensions and Applications**:\n",
    "   - Can adapt to any code generation domain\n",
    "   - Feedback mechanism is customizable\n",
    "   - Agent learns task-specific patterns\n",
    "   - Enables automated code improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}