{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Training Stabilization Techniques (Z-loss, AdamW)\n",
    "\n",
    "## Learning Objective\n",
    "Deep dive into the advanced training stabilization techniques used in the Kotlin ML Pack paper, focusing on Z-loss, modified AdamW parameters, and other stability methods from Section V.C.\n",
    "\n",
    "## Paper Reference\n",
    "- **Section**: V.C - Training setup\n",
    "- **Key Techniques**: Z-loss, decreased AdamW epsilon, gradient clipping, extended warm-up\n",
    "- **Goal**: Prevent training instabilities and logit divergence in large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Training Instabilities\n",
    "\n",
    "### 1.1 The Problem: Logit Divergence\n",
    "\n",
    "From the paper: \"The instability may happen closer to the end of the training, when the logits become very negative.\"\n",
    "\n",
    "This is a critical issue in training large language models where:\n",
    "- Output logits can grow unbounded\n",
    "- Softmax denominators explode\n",
    "- Gradients become unstable\n",
    "- Model performance degrades catastrophically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch numpy matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set up visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Z-Loss: The Core Innovation\n",
    "\n",
    "### 2.1 Mathematical Foundation\n",
    "\n",
    "The Z-loss is defined as:\n",
    "\n",
    "$$\\mathcal{L}_z = \\log^2(Z)$$\n",
    "\n",
    "where $Z = \\sum_j \\exp(y_j)$ is the partition function (softmax denominator).\n",
    "\n",
    "The key insight: By penalizing large values of $\\log(Z)$, we prevent logits from growing too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZLoss(nn.Module):\n",
    "    \"\"\"Z-loss implementation from Section V.C of the paper.\n",
    "    \n",
    "    This loss prevents logit divergence by penalizing large partition functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_history = []  # Track loss values for analysis\n",
    "    \n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate Z-loss for given logits.\n",
    "        \n",
    "        Args:\n",
    "            logits: Model output logits [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        Returns:\n",
    "            Z-loss value\n",
    "        \"\"\"\n",
    "        # Calculate log(sum(exp(logits))) using logsumexp for numerical stability\n",
    "        log_z = torch.logsumexp(logits, dim=-1)\n",
    "        \n",
    "        # Z-loss is log^2(Z)\n",
    "        z_loss = log_z ** 2\n",
    "        \n",
    "        # Store for analysis\n",
    "        self.loss_history.append(z_loss.mean().item())\n",
    "        \n",
    "        return z_loss.mean()\n",
    "    \n",
    "    def analyze_impact(self, logits: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"Analyze the impact of Z-loss on logit distribution\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Calculate various statistics\n",
    "            log_z = torch.logsumexp(logits, dim=-1)\n",
    "            max_logit = logits.max(dim=-1)[0]\n",
    "            min_logit = logits.min(dim=-1)[0]\n",
    "            logit_range = max_logit - min_logit\n",
    "            \n",
    "            return {\n",
    "                'mean_log_z': log_z.mean().item(),\n",
    "                'max_log_z': log_z.max().item(),\n",
    "                'mean_max_logit': max_logit.mean().item(),\n",
    "                'mean_logit_range': logit_range.mean().item(),\n",
    "                'z_loss': (log_z ** 2).mean().item()\n",
    "            }\n",
    "\n",
    "# Demonstrate Z-loss behavior\n",
    "z_loss_fn = ZLoss()\n",
    "\n",
    "# Create example logits with different scales\n",
    "batch_size, seq_len, vocab_size = 8, 128, 32000\n",
    "scales = [0.1, 1.0, 10.0, 100.0]\n",
    "results = []\n",
    "\n",
    "for scale in scales:\n",
    "    logits = torch.randn(batch_size, seq_len, vocab_size) * scale\n",
    "    z_loss = z_loss_fn(logits)\n",
    "    stats = z_loss_fn.analyze_impact(logits)\n",
    "    stats['scale'] = scale\n",
    "    results.append(stats)\n",
    "\n",
    "# Visualize results\n",
    "df = pd.DataFrame(results)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Z-loss vs scale\n",
    "axes[0, 0].plot(df['scale'], df['z_loss'], 'o-', color='red', markersize=8)\n",
    "axes[0, 0].set_xlabel('Logit Scale')\n",
    "axes[0, 0].set_ylabel('Z-loss')\n",
    "axes[0, 0].set_title('Z-loss vs Logit Scale')\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Log(Z) vs scale\n",
    "axes[0, 1].plot(df['scale'], df['mean_log_z'], 'o-', color='blue', markersize=8)\n",
    "axes[0, 1].set_xlabel('Logit Scale')\n",
    "axes[0, 1].set_ylabel('Mean log(Z)')\n",
    "axes[0, 1].set_title('Partition Function Growth')\n",
    "axes[0, 1].set_xscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Max logit vs scale\n",
    "axes[1, 0].plot(df['scale'], df['mean_max_logit'], 'o-', color='green', markersize=8)\n",
    "axes[1, 0].set_xlabel('Logit Scale')\n",
    "axes[1, 0].set_ylabel('Mean Max Logit')\n",
    "axes[1, 0].set_title('Maximum Logit Growth')\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Logit range vs scale\n",
    "axes[1, 1].plot(df['scale'], df['mean_logit_range'], 'o-', color='purple', markersize=8)\n",
    "axes[1, 1].set_xlabel('Logit Scale')\n",
    "axes[1, 1].set_ylabel('Mean Logit Range')\n",
    "axes[1, 1].set_title('Logit Range (max - min)')\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Z-loss grows quadratically with logit scale, providing strong regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modified AdamW: Decreased Epsilon\n",
    "\n",
    "### 3.1 The Paper's Finding\n",
    "\n",
    "From Section V.C: \"We find that in our case, the setting of ε = 10^-16 slightly improves both train loss and downstream benchmark scores at no extra costs.\"\n",
    "\n",
    "This is a significant deviation from the default ε = 10^-8. Let's understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedAdamW:\n",
    "    \"\"\"Demonstrates the impact of different epsilon values in AdamW\"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = {}\n",
    "        for p in self.params:\n",
    "            self.state[p] = {\n",
    "                'm': torch.zeros_like(p.data),  # First moment\n",
    "                'v': torch.zeros_like(p.data),  # Second moment\n",
    "                't': 0  # Time step\n",
    "            }\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Perform a single optimization step\"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            \n",
    "            grad = p.grad.data\n",
    "            state = self.state[p]\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            state['m'] = self.beta1 * state['m'] + (1 - self.beta1) * grad\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            state['v'] = self.beta2 * state['v'] + (1 - self.beta2) * grad**2\n",
    "            \n",
    "            # Update time step\n",
    "            state['t'] += 1\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = state['m'] / (1 - self.beta1**state['t'])\n",
    "            v_hat = state['v'] / (1 - self.beta2**state['t'])\n",
    "            \n",
    "            # AdamW update with weight decay\n",
    "            p.data = p.data - self.lr * self.weight_decay * p.data\n",
    "            \n",
    "            # Adam update\n",
    "            # This is where epsilon matters!\n",
    "            update = self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "            p.data = p.data - update\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def analyze_update_magnitude(self, grad, second_moment):\n",
    "        \"\"\"Analyze how epsilon affects update magnitude\"\"\"\n",
    "        # Simulate the denominator calculation\n",
    "        denominators = {}\n",
    "        epsilons = [1e-8, 1e-12, 1e-16]  # Different epsilon values\n",
    "        \n",
    "        for eps in epsilons:\n",
    "            denom = torch.sqrt(second_moment) + eps\n",
    "            update_scale = 1.0 / denom\n",
    "            denominators[eps] = {\n",
    "                'min': update_scale.min().item(),\n",
    "                'max': update_scale.max().item(),\n",
    "                'mean': update_scale.mean().item()\n",
    "            }\n",
    "        \n",
    "        return denominators\n",
    "\n",
    "# Demonstrate the impact of epsilon\n",
    "def demonstrate_epsilon_impact():\n",
    "    \"\"\"Show how different epsilon values affect gradient updates\"\"\"\n",
    "    \n",
    "    # Create example gradients and second moments\n",
    "    size = (1000,)\n",
    "    \n",
    "    # Case 1: Very small second moments (common in later training)\n",
    "    small_second_moments = torch.tensor(np.random.exponential(1e-10, size))\n",
    "    grad = torch.randn(size) * 0.01\n",
    "    \n",
    "    # Analyze update scales\n",
    "    epsilons = [1e-8, 1e-12, 1e-16]\n",
    "    results = []\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for i, eps in enumerate(epsilons):\n",
    "        denom = torch.sqrt(small_second_moments) + eps\n",
    "        update_scale = 1.0 / denom\n",
    "        actual_update = grad * update_scale\n",
    "        \n",
    "        # Plot distribution\n",
    "        ax = axes[i]\n",
    "        ax.hist(np.log10(update_scale.numpy()), bins=50, alpha=0.7, color='blue')\n",
    "        ax.set_xlabel('log10(Update Scale)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(f'ε = {eps}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        ax.text(0.05, 0.95, f'Max: {update_scale.max():.2e}\\nMin: {update_scale.min():.2e}', \n",
    "                transform=ax.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        results.append({\n",
    "            'epsilon': eps,\n",
    "            'max_scale': update_scale.max().item(),\n",
    "            'min_scale': update_scale.min().item(),\n",
    "            'mean_scale': update_scale.mean().item(),\n",
    "            'max_update': actual_update.abs().max().item()\n",
    "        })\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nImpact of Epsilon on Update Magnitudes:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nKey Insight: Smaller epsilon allows larger updates when second moments are tiny\")\n",
    "    print(\"This prevents 'dead' parameters that stop updating in later training stages.\")\n",
    "\n",
    "demonstrate_epsilon_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Norm Clipping\n",
    "\n",
    "### 4.1 The Balance\n",
    "\n",
    "From the paper: \"We choose gradient clipping so that very few gradients are clipped, avoiding the effective decrease of the learning rate caused by aggressive gradient clipping.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClippingAnalyzer:\n",
    "    \"\"\"Analyzes the impact of gradient clipping on training\"\"\"\n",
    "    \n",
    "    def __init__(self, max_norm=1.0):\n",
    "        self.max_norm = max_norm\n",
    "        self.clip_history = []\n",
    "    \n",
    "    def clip_grad_norm_(self, parameters, max_norm=None):\n",
    "        \"\"\"Custom gradient clipping with detailed tracking\"\"\"\n",
    "        if max_norm is None:\n",
    "            max_norm = self.max_norm\n",
    "        \n",
    "        # Calculate total gradient norm\n",
    "        total_norm = 0.0\n",
    "        for p in parameters:\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        \n",
    "        # Calculate clipping coefficient\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        clip_coef = min(clip_coef, 1.0)\n",
    "        \n",
    "        # Apply clipping\n",
    "        if clip_coef < 1.0:\n",
    "            for p in parameters:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.mul_(clip_coef)\n",
    "        \n",
    "        # Track statistics\n",
    "        self.clip_history.append({\n",
    "            'total_norm': total_norm,\n",
    "            'clip_coef': clip_coef,\n",
    "            'clipped': clip_coef < 1.0\n",
    "        })\n",
    "        \n",
    "        return total_norm\n",
    "    \n",
    "    def analyze_clipping_impact(self, gradient_norms: List[float], clip_values: List[float]):\n",
    "        \"\"\"Analyze how different clipping values affect gradients\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Convert to numpy for easier manipulation\n",
    "        gradient_norms = np.array(gradient_norms)\n",
    "        \n",
    "        for i, clip_value in enumerate(clip_values):\n",
    "            ax = axes[i // 2, i % 2]\n",
    "            \n",
    "            # Calculate clipped norms\n",
    "            clipped_norms = np.minimum(gradient_norms, clip_value)\n",
    "            clip_ratio = np.sum(gradient_norms > clip_value) / len(gradient_norms)\n",
    "            \n",
    "            # Plot histogram\n",
    "            ax.hist(gradient_norms, bins=50, alpha=0.5, label='Original', color='blue')\n",
    "            ax.hist(clipped_norms, bins=50, alpha=0.5, label='Clipped', color='red')\n",
    "            ax.axvline(clip_value, color='black', linestyle='--', label=f'Clip threshold')\n",
    "            \n",
    "            ax.set_xlabel('Gradient Norm')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Clip Value: {clip_value} (Clipped: {clip_ratio*100:.1f}%)')\n",
    "            ax.legend()\n",
    "            ax.set_xlim(0, max(gradient_norms) * 1.1)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Simulate gradient norms during training\n",
    "def simulate_training_gradients(num_steps=1000):\n",
    "    \"\"\"Simulate gradient norms during training\"\"\"\n",
    "    # Early training: high variance, occasional spikes\n",
    "    early_grads = np.random.lognormal(mean=0.0, sigma=1.0, size=num_steps//3)\n",
    "    \n",
    "    # Mid training: more stable\n",
    "    mid_grads = np.random.lognormal(mean=-0.5, sigma=0.5, size=num_steps//3)\n",
    "    \n",
    "    # Late training: very stable, small gradients\n",
    "    late_grads = np.random.lognormal(mean=-1.0, sigma=0.3, size=num_steps//3)\n",
    "    \n",
    "    # Add occasional spikes\n",
    "    all_grads = np.concatenate([early_grads, mid_grads, late_grads])\n",
    "    spike_indices = np.random.choice(len(all_grads), size=20, replace=False)\n",
    "    all_grads[spike_indices] *= np.random.uniform(5, 20, size=20)\n",
    "    \n",
    "    return all_grads\n",
    "\n",
    "# Analyze gradient clipping\n",
    "gradient_norms = simulate_training_gradients()\n",
    "analyzer = GradientClippingAnalyzer()\n",
    "\n",
    "# Test different clipping values\n",
    "clip_values = [0.5, 1.0, 5.0, 10.0]\n",
    "analyzer.analyze_clipping_impact(gradient_norms, clip_values)\n",
    "\n",
    "# Recommendation based on paper\n",
    "print(\"\\nPaper's Approach: Choose clipping threshold where <5% of gradients are clipped\")\n",
    "percentiles = [90, 95, 99, 99.9]\n",
    "print(\"\\nGradient Norm Percentiles:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(gradient_norms, p)\n",
    "    print(f\"  {p}th percentile: {value:.2f}\")\n",
    "print(\"\\nRecommendation: Set clip_value around 95th-99th percentile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extended Warm-up Period\n",
    "\n",
    "### 5.1 The Paper's Finding\n",
    "\n",
    "\"Using warm-up period length of up to 10% of the train dataset allows training models at a higher learning rate without facing instabilities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupScheduler:\n",
    "    \"\"\"Implements various warm-up strategies for learning rate scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_steps, max_lr, schedule_type='linear'):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_lr = max_lr\n",
    "        self.schedule_type = schedule_type\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def get_lr(self, step=None):\n",
    "        \"\"\"Calculate learning rate for given step\"\"\"\n",
    "        if step is None:\n",
    "            step = self.current_step\n",
    "        \n",
    "        if step >= self.warmup_steps:\n",
    "            return self.max_lr\n",
    "        \n",
    "        warmup_ratio = step / self.warmup_steps\n",
    "        \n",
    "        if self.schedule_type == 'linear':\n",
    "            return self.max_lr * warmup_ratio\n",
    "        elif self.schedule_type == 'exponential':\n",
    "            return self.max_lr * (warmup_ratio ** 2)\n",
    "        elif self.schedule_type == 'cosine':\n",
    "            return self.max_lr * 0.5 * (1 + np.cos(np.pi * (1 - warmup_ratio)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule type: {self.schedule_type}\")\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update learning rate\"\"\"\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "def visualize_warmup_impact():\n",
    "    \"\"\"Visualize the impact of different warm-up strategies\"\"\"\n",
    "    total_steps = 10000\n",
    "    warmup_percentages = [1, 5, 10, 20]  # Percentage of total steps\n",
    "    max_lr = 1e-3\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, warmup_pct in enumerate(warmup_percentages):\n",
    "        ax = axes[i]\n",
    "        warmup_steps = int(total_steps * warmup_pct / 100)\n",
    "        \n",
    "        # Create dummy optimizer\n",
    "        param = torch.nn.Parameter(torch.randn(1))\n",
    "        optimizer = torch.optim.Adam([param], lr=0)\n",
    "        \n",
    "        # Test different schedules\n",
    "        schedules = ['linear', 'exponential', 'cosine']\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        \n",
    "        for schedule, color in zip(schedules, colors):\n",
    "            scheduler = WarmupScheduler(optimizer, warmup_steps, max_lr, schedule)\n",
    "            lrs = [scheduler.get_lr(step) for step in range(total_steps)]\n",
    "            \n",
    "            # Plot learning rate schedule\n",
    "            ax.plot(lrs[:warmup_steps*2], label=schedule, color=color, linewidth=2)\n",
    "        \n",
    "        ax.axvline(warmup_steps, color='black', linestyle='--', alpha=0.5, label='End of warmup')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Learning Rate')\n",
    "        ax.set_title(f'Warmup: {warmup_pct}% of training ({warmup_steps} steps)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(0, warmup_steps * 2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Demonstrate impact on gradient variance\n",
    "    print(\"\\nImpact of Warm-up on Training Stability:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Without warm-up: High initial LR can cause gradient explosion\")\n",
    "    print(\"With 10% warm-up: Gradual increase prevents instabilities\")\n",
    "    print(\"Paper finding: Enables higher max LR → faster convergence\")\n",
    "\n",
    "visualize_warmup_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Training Configuration\n",
    "\n",
    "Let's implement the complete training setup from the paper, combining all techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StabilizedTrainer:\n",
    "    \"\"\"Complete implementation of training stabilization techniques from the paper\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self.z_loss = ZLoss()\n",
    "        \n",
    "        # Optimizer with decreased epsilon\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            eps=config['adam_epsilon']  # 1e-16 from paper\n",
    "        )\n",
    "        \n",
    "        # Warmup scheduler\n",
    "        self.warmup_steps = int(config['total_steps'] * config['warmup_ratio'])\n",
    "        self.scheduler = WarmupScheduler(\n",
    "            self.optimizer,\n",
    "            self.warmup_steps,\n",
    "            config['learning_rate'],\n",
    "            'linear'\n",
    "        )\n",
    "        \n",
    "        # Tracking\n",
    "        self.training_history = {\n",
    "            'loss': [],\n",
    "            'z_loss': [],\n",
    "            'grad_norm': [],\n",
    "            'learning_rate': [],\n",
    "            'clipped': []\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, logits, labels):\n",
    "        \"\"\"Compute combined loss with Z-loss regularization\"\"\"\n",
    "        # Cross-entropy loss\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        # Z-loss\n",
    "        z_loss = self.z_loss(logits)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = ce_loss + self.config['z_loss_weight'] * z_loss\n",
    "        \n",
    "        return total_loss, ce_loss.item(), z_loss.item()\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        \"\"\"Single training step with all stabilization techniques\"\"\"\n",
    "        # Forward pass\n",
    "        logits = self.model(batch['input_ids'])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, ce_loss, z_loss = self.compute_loss(logits, batch['labels'])\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            self.model.parameters(),\n",
    "            self.config['max_grad_norm']\n",
    "        )\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        current_lr = self.scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        self.training_history['loss'].append(ce_loss)\n",
    "        self.training_history['z_loss'].append(z_loss)\n",
    "        self.training_history['grad_norm'].append(grad_norm.item())\n",
    "        self.training_history['learning_rate'].append(current_lr)\n",
    "        self.training_history['clipped'].append(grad_norm > self.config['max_grad_norm'])\n",
    "        \n",
    "        return {\n",
    "            'loss': ce_loss,\n",
    "            'z_loss': z_loss,\n",
    "            'grad_norm': grad_norm.item(),\n",
    "            'lr': current_lr\n",
    "        }\n",
    "\n",
    "# Configuration based on the paper\n",
    "training_config = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.1,  # \"quite large\" as per paper\n",
    "    'adam_epsilon': 1e-16,  # Decreased from default\n",
    "    'z_loss_weight': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'warmup_ratio': 0.1,  # 10% of training\n",
    "    'total_steps': 10000\n",
    "}\n",
    "\n",
    "print(\"Training Configuration (from paper):\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in training_config.items():\n",
    "    print(f\"{key:<20}: {value}\")\n",
    "\n",
    "# Visualize the complete training dynamics\n",
    "def simulate_training_run():\n",
    "    \"\"\"Simulate a training run with stabilization techniques\"\"\"\n",
    "    # Create dummy model\n",
    "    class DummyModel(nn.Module):\n",
    "        def __init__(self, vocab_size=32000, hidden_size=768):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "            self.output = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        def forward(self, input_ids):\n",
    "            embeds = self.embedding(input_ids)\n",
    "            return self.output(embeds)\n",
    "    \n",
    "    model = DummyModel()\n",
    "    trainer = StabilizedTrainer(model, training_config)\n",
    "    \n",
    "    # Simulate training steps\n",
    "    for step in range(1000):\n",
    "        # Create dummy batch\n",
    "        batch = {\n",
    "            'input_ids': torch.randint(0, 32000, (8, 128)),\n",
    "            'labels': torch.randint(0, 32000, (8, 128))\n",
    "        }\n",
    "        \n",
    "        metrics = trainer.training_step(batch)\n",
    "    \n",
    "    return trainer.training_history\n",
    "\n",
    "# Run simulation\n",
    "history = simulate_training_run()\n",
    "\n",
    "# Visualize training dynamics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0, 0].plot(history['loss'], label='CE Loss', alpha=0.8)\n",
    "axes[0, 0].plot(history['z_loss'], label='Z-Loss', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Losses')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gradient norms\n",
    "axes[0, 1].plot(history['grad_norm'], alpha=0.8, color='green')\n",
    "axes[0, 1].axhline(training_config['max_grad_norm'], color='red', linestyle='--', label='Clip threshold')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Gradient Norm')\n",
    "axes[0, 1].set_title('Gradient Norms')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning rate schedule\n",
    "axes[1, 0].plot(history['learning_rate'], color='purple')\n",
    "axes[1, 0].axvline(training_config['total_steps'] * training_config['warmup_ratio'],\n",
    "                   color='black', linestyle='--', label='End of warmup')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Clipping frequency\n",
    "window_size = 50\n",
    "clipping_rate = pd.Series(history['clipped']).rolling(window_size).mean() * 100\n",
    "axes[1, 1].plot(clipping_rate, color='orange')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Clipping Rate (%)')\n",
    "axes[1, 1].set_title(f'Gradient Clipping Rate ({window_size}-step window)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"Average gradient clipping rate: {np.mean(history['clipped']) * 100:.2f}%\")\n",
    "print(f\"Max gradient norm: {max(history['grad_norm']):.2f}\")\n",
    "print(f\"Final Z-loss: {history['z_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dynamic Beta (Failed Experiment)\n",
    "\n",
    "The paper mentions trying dynamic β₂ but finding it doesn't help. Let's understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dynamic_beta():\n",
    "    \"\"\"Analyze the dynamic beta approach mentioned in the paper\"\"\"\n",
    "    \n",
    "    # Formula from paper: β₂ = 1 - k^(-0.8) where k is step number\n",
    "    steps = np.arange(1, 10000)\n",
    "    dynamic_beta2 = 1 - steps**(-0.8)\n",
    "    static_beta2 = 0.999  # Standard AdamW value\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot beta2 values\n",
    "    ax1.plot(steps, dynamic_beta2, label='Dynamic β₂', color='blue')\n",
    "    ax1.axhline(static_beta2, color='red', linestyle='--', label='Static β₂ = 0.999')\n",
    "    ax1.set_xlabel('Training Step')\n",
    "    ax1.set_ylabel('β₂ Value')\n",
    "    ax1.set_title('Dynamic vs Static β₂')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log')\n",
    "    \n",
    "    # Plot effective window size (1/(1-β₂))\n",
    "    dynamic_window = 1 / (1 - dynamic_beta2)\n",
    "    static_window = 1 / (1 - static_beta2)\n",
    "    \n",
    "    ax2.plot(steps, dynamic_window, label='Dynamic β₂', color='blue')\n",
    "    ax2.axhline(static_window, color='red', linestyle='--', label='Static β₂')\n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Effective Window Size')\n",
    "    ax2.set_title('Second Moment Estimation Window')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Analysis of Dynamic β₂:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Early training (step 10): β₂ = {:.4f}, window = {:.0f} steps\".format(\n",
    "        1 - 10**(-0.8), 1 / (1 - (1 - 10**(-0.8)))\n",
    "    ))\n",
    "    print(\"Mid training (step 1000): β₂ = {:.4f}, window = {:.0f} steps\".format(\n",
    "        1 - 1000**(-0.8), 1 / (1 - (1 - 1000**(-0.8)))\n",
    "    ))\n",
    "    print(\"\\nPaper's finding: No improvement over static β₂\")\n",
    "    print(\"Likely reason: Kotlin fine-tuning doesn't have the rare token issue that PaLM faced\")\n",
    "\n",
    "analyze_dynamic_beta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary: Best Practices for Stable Training\n",
    "\n",
    "Based on the paper's findings, here's a complete recipe for stable LLM fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Define techniques and their impacts\n",
    "techniques = [\n",
    "    \"Z-Loss (weight=0.01)\",\n",
    "    \"AdamW ε=1e-16\",\n",
    "    \"Weight Decay=0.1\",\n",
    "    \"Gradient Clipping (conservative)\",\n",
    "    \"10% Warmup Period\",\n",
    "    \"BF16 Precision\"\n",
    "]\n",
    "\n",
    "impacts = [\n",
    "    \"Prevents logit divergence\",\n",
    "    \"Enables updates for small gradients\",\n",
    "    \"Additional regularization\",\n",
    "    \"Handles gradient spikes\",\n",
    "    \"Allows higher learning rates\",\n",
    "    \"Reduces memory, maintains stability\"\n",
    "]\n",
    "\n",
    "benefits = [\n",
    "    \"Stable training to completion\",\n",
    "    \"Better final performance\",\n",
    "    \"Prevents overfitting\",\n",
    "    \"Robustness to outliers\",\n",
    "    \"Faster convergence\",\n",
    "    \"Efficient GPU utilization\"\n",
    "]\n",
    "\n",
    "# Create table\n",
    "table_data = []\n",
    "for tech, impact, benefit in zip(techniques, impacts, benefits):\n",
    "    table_data.append([tech, impact, benefit])\n",
    "\n",
    "# Hide axes\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create table\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Technique', 'Primary Impact', 'Key Benefit'],\n",
    "                cellLoc='left',\n",
    "                loc='center',\n",
    "                colWidths=[0.35, 0.35, 0.3])\n",
    "\n",
    "# Style the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Color header\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(techniques) + 1):\n",
    "    for j in range(3):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "plt.title('Training Stabilization Techniques Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Z-loss is crucial for preventing catastrophic divergence\")\n",
    "print(\"2. Small changes (like ε=1e-16) can have significant impacts\")\n",
    "print(\"3. Conservative gradient clipping is better than aggressive\")\n",
    "print(\"4. Extended warmup enables more aggressive learning rates\")\n",
    "print(\"5. These techniques enabled successful fine-tuning on Kotlin datasets\")\n",
    "print(\"\\nResult: Up to 16-point improvement on HumanEval benchmark!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}