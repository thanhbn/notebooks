{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚖️ Load Balancing Loss & Mutual Information trong MoL\n",
    "\n",
    "## 🎯 Mục tiêu Học tập\n",
    "\n",
    "Hiểu sâu về:\n",
    "1. **Load Balancing Problem** trong Mixture-of-Experts systems\n",
    "2. **Mutual Information-based Loss** để cải thiện component utilization\n",
    "3. **Component Collapse** và cách ngăn chặn\n",
    "4. **Conditional Computation** trong MoL\n",
    "5. **Mathematical Foundations** của Information Theory trong ML\n",
    "\n",
    "## 📖 Trích xuất từ Paper\n",
    "\n",
    "### Section 2.2 - Load Balancing Loss:\n",
    "\n",
    "> *\"We propose techniques to retrieve the approximate top-k results using MoL with tight error bounds... enhanced by our proposed mutual information-based load balancing loss\"*\n",
    "\n",
    "> *\"Our approximate top-k algorithms outperform baselines by up to 66× in latency while achieving >.99 recall rate compared to exact algorithms.\"*\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Component Collapse**: Khi một vài components dominate, others become unused\n",
    "**Load Balancing**: Ensure equal utilization across components\n",
    "**Mutual Information**: Measure dependence between gating decisions and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 Device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Phần 1: Understanding Component Collapse\n",
    "\n",
    "### 📊 Vấn đề:\n",
    "\n",
    "Trong Mixture-of-Experts systems, **component collapse** xảy ra khi:\n",
    "- Một vài components được sử dụng heavily\n",
    "- Majority components ít được sử dụng hoặc unused\n",
    "- Model capacity bị waste, performance giảm\n",
    "\n",
    "### 🎯 Giải pháp:\n",
    "**Load Balancing Loss** khuyến khích:\n",
    "- Uniform distribution của gating weights\n",
    "- Equal utilization across components\n",
    "- Better model capacity utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_component_collapse():\n",
    "    \"\"\"\n",
    "    Demonstrate component collapse phenomenon\n",
    "    \"\"\"\n",
    "    print(\"🚨 Component Collapse Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    num_components = 8\n",
    "    num_samples = 1000\n",
    "    \n",
    "    # Simulate different scenarios\n",
    "    scenarios = {\n",
    "        'uniform': np.random.dirichlet(np.ones(num_components), num_samples),\n",
    "        'slightly_biased': np.random.dirichlet(np.array([2, 2, 2, 1, 1, 1, 1, 1]), num_samples),\n",
    "        'heavily_biased': np.random.dirichlet(np.array([10, 5, 1, 1, 1, 1, 1, 1]), num_samples),\n",
    "        'collapsed': np.random.dirichlet(np.array([100, 1, 1, 1, 1, 1, 1, 1]), num_samples)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for scenario_name, weights in scenarios.items():\n",
    "        # Compute utilization statistics\n",
    "        mean_weights = np.mean(weights, axis=0)\n",
    "        std_weights = np.std(weights, axis=0)\n",
    "        \n",
    "        # Effective number of components (Perplexity)\n",
    "        entropy = -np.sum(mean_weights * np.log(mean_weights + 1e-8))\n",
    "        perplexity = np.exp(entropy)\n",
    "        \n",
    "        # Gini coefficient (inequality measure)\n",
    "        sorted_weights = np.sort(mean_weights)\n",
    "        n = len(sorted_weights)\n",
    "        index = np.arange(1, n + 1)\n",
    "        gini = (2 * np.sum(index * sorted_weights)) / (n * np.sum(sorted_weights)) - (n + 1) / n\n",
    "        \n",
    "        results[scenario_name] = {\n",
    "            'mean_weights': mean_weights,\n",
    "            'std_weights': std_weights,\n",
    "            'perplexity': perplexity,\n",
    "            'gini': gini,\n",
    "            'entropy': entropy\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📊 {scenario_name.upper()}:\")\n",
    "        print(f\"   Effective components: {perplexity:.2f} / {num_components}\")\n",
    "        print(f\"   Gini coefficient: {gini:.3f} (0=equal, 1=collapsed)\")\n",
    "        print(f\"   Entropy: {entropy:.3f} (max={np.log(num_components):.3f})\")\n",
    "        print(f\"   Weight distribution: {mean_weights}\")\n",
    "    \n",
    "    return results, scenarios\n",
    "\n",
    "collapse_results, collapse_scenarios = demonstrate_component_collapse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧮 Phần 2: Information Theory Foundations\n",
    "\n",
    "### 📖 Lý thuyết:\n",
    "\n",
    "**Entropy** (Shannon Entropy):\n",
    "$$H(X) = -\\sum_{i} p(x_i) \\log p(x_i)$$\n",
    "\n",
    "**Mutual Information**:\n",
    "$$I(X; Y) = H(X) - H(X|Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$$\n",
    "\n",
    "**KL Divergence**:\n",
    "$$D_{KL}(P||Q) = \\sum_{i} p_i \\log \\frac{p_i}{q_i}$$\n",
    "\n",
    "### 🎯 Application to MoL:\n",
    "- **Z**: Latent representations (queries/items)\n",
    "- **G**: Gating decisions (component assignments)\n",
    "- **Goal**: Minimize I(Z; G) → components independent of input patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationTheoryUtils:\n",
    "    \"\"\"\n",
    "    Utility functions for Information Theory calculations\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy(probabilities: np.ndarray, base: float = 2) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy\n",
    "        \"\"\"\n",
    "        p = probabilities + 1e-12  # Avoid log(0)\n",
    "        return -np.sum(p * np.log(p) / np.log(base))\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutual_information_discrete(X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate mutual information for discrete variables\n",
    "        \"\"\"\n",
    "        return mutual_info_score(X, Y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence(P: np.ndarray, Q: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate KL divergence D_KL(P||Q)\n",
    "        \"\"\"\n",
    "        P = P + 1e-12\n",
    "        Q = Q + 1e-12\n",
    "        return np.sum(P * np.log(P / Q))\n",
    "    \n",
    "    @staticmethod\n",
    "    def perplexity(probabilities: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate perplexity (effective number of components)\n",
    "        \"\"\"\n",
    "        entropy = InformationTheoryUtils.entropy(probabilities, base=np.e)\n",
    "        return np.exp(entropy)\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_mutual_information_continuous(X: torch.Tensor, Y: torch.Tensor, \n",
    "                                             bins: int = 20) -> float:\n",
    "        \"\"\"\n",
    "        Estimate mutual information for continuous variables using binning\n",
    "        \"\"\"\n",
    "        # Convert to numpy\n",
    "        X_np = X.detach().cpu().numpy().flatten()\n",
    "        Y_np = Y.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        # Discretize using histograms\n",
    "        X_discrete = np.digitize(X_np, np.histogram(X_np, bins=bins)[1][:-1])\n",
    "        Y_discrete = np.digitize(Y_np, np.histogram(Y_np, bins=bins)[1][:-1])\n",
    "        \n",
    "        return mutual_info_score(X_discrete, Y_discrete)\n",
    "\n",
    "# Demonstrate information theory concepts\n",
    "print(\"📊 Information Theory Demonstrations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "utils = InformationTheoryUtils()\n",
    "\n",
    "# Entropy examples\n",
    "distributions = {\n",
    "    'uniform': np.ones(8) / 8,\n",
    "    'peaked': np.array([0.7, 0.1, 0.05, 0.05, 0.03, 0.03, 0.02, 0.02]),\n",
    "    'bimodal': np.array([0.4, 0.4, 0.05, 0.05, 0.05, 0.05, 0, 0]),\n",
    "    'delta': np.array([1.0, 0, 0, 0, 0, 0, 0, 0])\n",
    "}\n",
    "\n",
    "print(\"\\n🔢 Entropy Analysis:\")\n",
    "for name, dist in distributions.items():\n",
    "    entropy = utils.entropy(dist)\n",
    "    perplexity = utils.perplexity(dist)\n",
    "    print(f\"   {name:8s}: H = {entropy:.3f}, Perplexity = {perplexity:.2f}\")\n",
    "\n",
    "# Mutual Information demonstration\n",
    "print(\"\\n🔗 Mutual Information Examples:\")\n",
    "\n",
    "# Independent variables\n",
    "X_indep = np.random.randint(0, 4, 1000)\n",
    "Y_indep = np.random.randint(0, 4, 1000)\n",
    "mi_indep = utils.mutual_information_discrete(X_indep, Y_indep)\n",
    "print(f\"   Independent variables: I(X;Y) = {mi_indep:.3f}\")\n",
    "\n",
    "# Dependent variables\n",
    "X_dep = np.random.randint(0, 4, 1000)\n",
    "Y_dep = (X_dep + np.random.randint(0, 2, 1000)) % 4  # Partial dependence\n",
    "mi_dep = utils.mutual_information_discrete(X_dep, Y_dep)\n",
    "print(f\"   Dependent variables: I(X;Y) = {mi_dep:.3f}\")\n",
    "\n",
    "# Perfectly correlated\n",
    "X_corr = np.random.randint(0, 4, 1000)\n",
    "Y_corr = X_corr.copy()  # Perfect correlation\n",
    "mi_corr = utils.mutual_information_discrete(X_corr, Y_corr)\n",
    "print(f\"   Perfectly correlated: I(X;Y) = {mi_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚖️ Phần 3: Load Balancing Loss Implementation\n",
    "\n",
    "### 🎯 Mục tiêu:\n",
    "Thiết kế loss function để:\n",
    "1. **Encourage uniform component usage**\n",
    "2. **Minimize mutual information** between inputs và gating decisions\n",
    "3. **Prevent component collapse**\n",
    "4. **Maintain model expressiveness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLoadBalancingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Load Balancing Loss with multiple components:\n",
    "    1. Uniform Distribution Loss (KL divergence)\n",
    "    2. Entropy Maximization Loss\n",
    "    3. Mutual Information Minimization Loss\n",
    "    4. Variance Minimization Loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_components: int,\n",
    "                 lambda_uniform: float = 0.01,\n",
    "                 lambda_entropy: float = 0.01,\n",
    "                 lambda_mi: float = 0.005,\n",
    "                 lambda_variance: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.num_components = num_components\n",
    "        self.lambda_uniform = lambda_uniform\n",
    "        self.lambda_entropy = lambda_entropy\n",
    "        self.lambda_mi = lambda_mi\n",
    "        self.lambda_variance = lambda_variance\n",
    "        \n",
    "        # Target uniform distribution\n",
    "        self.register_buffer('uniform_target', \n",
    "                           torch.ones(num_components) / num_components)\n",
    "    \n",
    "    def forward(self, \n",
    "               gating_weights: torch.Tensor,\n",
    "               input_features: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute comprehensive load balancing loss\n",
    "        \n",
    "        Args:\n",
    "            gating_weights: [batch_size, num_components] - gating decisions\n",
    "            input_features: [batch_size, feature_dim] - input representations\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of loss components\n",
    "        \"\"\"\n",
    "        batch_size = gating_weights.size(0)\n",
    "        \n",
    "        # Reshape if needed\n",
    "        if gating_weights.dim() > 2:\n",
    "            gating_weights = gating_weights.view(batch_size, -1)\n",
    "        \n",
    "        # Component usage statistics\n",
    "        component_usage = gating_weights.mean(dim=0)  # [num_components]\n",
    "        \n",
    "        losses = {}\n",
    "        \n",
    "        # 1. Uniform Distribution Loss (KL Divergence)\n",
    "        uniform_loss = F.kl_div(\n",
    "            (component_usage + 1e-8).log(),\n",
    "            self.uniform_target,\n",
    "            reduction='sum'\n",
    "        )\n",
    "        losses['uniform'] = self.lambda_uniform * uniform_loss\n",
    "        \n",
    "        # 2. Entropy Maximization Loss\n",
    "        entropy = -torch.sum(component_usage * torch.log(component_usage + 1e-8))\n",
    "        max_entropy = math.log(self.num_components)  # Maximum possible entropy\n",
    "        entropy_loss = max_entropy - entropy  # Minimize negative entropy\n",
    "        losses['entropy'] = self.lambda_entropy * entropy_loss\n",
    "        \n",
    "        # 3. Variance Minimization Loss (encourage equal usage)\n",
    "        variance_loss = torch.var(component_usage)\n",
    "        losses['variance'] = self.lambda_variance * variance_loss\n",
    "        \n",
    "        # 4. Mutual Information Minimization (if input features available)\n",
    "        if input_features is not None:\n",
    "            mi_loss = self._approximate_mutual_information_loss(\n",
    "                input_features, gating_weights\n",
    "            )\n",
    "            losses['mutual_info'] = self.lambda_mi * mi_loss\n",
    "        else:\n",
    "            losses['mutual_info'] = torch.tensor(0.0, device=gating_weights.device)\n",
    "        \n",
    "        # Total loss\n",
    "        losses['total'] = sum(losses.values())\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _approximate_mutual_information_loss(self, \n",
    "                                           inputs: torch.Tensor, \n",
    "                                           gating_weights: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Approximate mutual information loss using neural estimation\n",
    "        \n",
    "        This is a simplified version - in practice, you might use:\n",
    "        - MINE (Mutual Information Neural Estimation)\n",
    "        - InfoNCE\n",
    "        - Histogram-based estimation\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        # Simple approximation: encourage independence\n",
    "        # by minimizing correlation between input patterns and gating decisions\n",
    "        \n",
    "        # Compute pairwise similarities in input space\n",
    "        input_norm = F.normalize(inputs, dim=1)\n",
    "        input_sim = torch.mm(input_norm, input_norm.t())  # [batch, batch]\n",
    "        \n",
    "        # Compute pairwise similarities in gating space\n",
    "        gating_norm = F.normalize(gating_weights, dim=1)\n",
    "        gating_sim = torch.mm(gating_norm, gating_norm.t())  # [batch, batch]\n",
    "        \n",
    "        # Minimize correlation between input and gating similarities\n",
    "        # This encourages gating decisions to be independent of input patterns\n",
    "        correlation = torch.mean(input_sim * gating_sim)\n",
    "        \n",
    "        return correlation\n",
    "\n",
    "# Demonstration\n",
    "print(\"⚖️ Advanced Load Balancing Loss Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "num_components = 8\n",
    "batch_size = 100\n",
    "feature_dim = 64\n",
    "\n",
    "# Create load balancing loss\n",
    "lb_loss = AdvancedLoadBalancingLoss(num_components)\n",
    "\n",
    "# Test different gating weight scenarios\n",
    "scenarios = {\n",
    "    'uniform': torch.ones(batch_size, num_components) / num_components,\n",
    "    'biased': F.softmax(torch.tensor([3., 2., 1., 1., 0.5, 0.5, 0.2, 0.2]).unsqueeze(0).expand(batch_size, -1), dim=1),\n",
    "    'collapsed': F.softmax(torch.tensor([10., 1., 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]).unsqueeze(0).expand(batch_size, -1), dim=1)\n",
    "}\n",
    "\n",
    "input_features = torch.randn(batch_size, feature_dim)\n",
    "\n",
    "print(\"\\n📊 Load Balancing Loss Analysis:\")\n",
    "for scenario_name, weights in scenarios.items():\n",
    "    with torch.no_grad():\n",
    "        losses = lb_loss(weights, input_features)\n",
    "        \n",
    "        component_usage = weights.mean(dim=0)\n",
    "        entropy = -torch.sum(component_usage * torch.log(component_usage + 1e-8))\n",
    "        \n",
    "        print(f\"\\n   {scenario_name.upper()}:\")\n",
    "        print(f\"     Total Loss: {losses['total'].item():.4f}\")\n",
    "        print(f\"     Uniform Loss: {losses['uniform'].item():.4f}\")\n",
    "        print(f\"     Entropy Loss: {losses['entropy'].item():.4f}\")\n",
    "        print(f\"     Variance Loss: {losses['variance'].item():.4f}\")\n",
    "        print(f\"     MI Loss: {losses['mutual_info'].item():.4f}\")\n",
    "        print(f\"     Component Entropy: {entropy.item():.3f} / {math.log(num_components):.3f}\")\n",
    "        print(f\"     Usage: {component_usage.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Phần 4: Training với Load Balancing\n",
    "\n",
    "### 🎯 Experiment Design:\n",
    "So sánh training với và không có load balancing loss để thấy impact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMoLWithLoadBalancing(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified MoL model with integrated load balancing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, num_components: int = 6, component_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.num_components = num_components\n",
    "        self.component_dim = component_dim\n",
    "        \n",
    "        # Component embeddings\n",
    "        self.query_embeddings = nn.ModuleList([\n",
    "            nn.Linear(input_dim, component_dim) for _ in range(num_components)\n",
    "        ])\n",
    "        self.item_embeddings = nn.ModuleList([\n",
    "            nn.Linear(input_dim, component_dim) for _ in range(num_components)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_components),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Load balancing\n",
    "        self.load_balancer = AdvancedLoadBalancingLoss(num_components)\n",
    "    \n",
    "    def forward(self, queries: torch.Tensor, items: torch.Tensor, \n",
    "               return_gating_info: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Forward pass with optional gating information\n",
    "        \"\"\"\n",
    "        batch_q, batch_i = queries.size(0), items.size(0)\n",
    "        \n",
    "        similarities = torch.zeros(batch_q, batch_i, device=queries.device)\n",
    "        all_gating_weights = []\n",
    "        all_input_features = []\n",
    "        \n",
    "        # Compute component embeddings\n",
    "        q_components = [F.normalize(emb(queries), dim=-1) for emb in self.query_embeddings]\n",
    "        i_components = [F.normalize(emb(items), dim=-1) for emb in self.item_embeddings]\n",
    "        \n",
    "        # Compute similarities for all pairs\n",
    "        for i in range(batch_q):\n",
    "            for j in range(batch_i):\n",
    "                # Combined features for gating\n",
    "                combined_features = torch.cat([queries[i], items[j]], dim=0)\n",
    "                gating_weights = self.gating_network(combined_features.unsqueeze(0)).squeeze(0)\n",
    "                \n",
    "                if return_gating_info:\n",
    "                    all_gating_weights.append(gating_weights)\n",
    "                    all_input_features.append(combined_features)\n",
    "                \n",
    "                # Compute weighted similarity\n",
    "                similarity = 0.0\n",
    "                for p in range(self.num_components):\n",
    "                    component_sim = torch.dot(q_components[p][i], i_components[p][j])\n",
    "                    similarity += gating_weights[p] * component_sim\n",
    "                \n",
    "                similarities[i, j] = similarity\n",
    "        \n",
    "        result = {'similarities': similarities}\n",
    "        \n",
    "        if return_gating_info and all_gating_weights:\n",
    "            result['gating_weights'] = torch.stack(all_gating_weights)\n",
    "            result['input_features'] = torch.stack(all_input_features)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def compute_load_balancing_loss(self, gating_weights: torch.Tensor, \n",
    "                                  input_features: torch.Tensor) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute load balancing loss\n",
    "        \"\"\"\n",
    "        return self.load_balancer(gating_weights, input_features)\n",
    "\n",
    "def train_with_load_balancing_comparison(num_epochs: int = 100):\n",
    "    \"\"\"\n",
    "    Compare training with and without load balancing\n",
    "    \"\"\"\n",
    "    print(\"\\n🧪 Load Balancing Training Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    num_queries, num_items = 30, 50\n",
    "    input_dim = 32\n",
    "    \n",
    "    queries = torch.randn(num_queries, input_dim)\n",
    "    items = torch.randn(num_items, input_dim)\n",
    "    \n",
    "    # Create relevance labels (simplified)\n",
    "    labels = torch.sigmoid(torch.mm(queries, items.t()) + 0.5 * torch.randn(num_queries, num_items))\n",
    "    \n",
    "    # Two models: with and without load balancing\n",
    "    model_with_lb = SimpleMoLWithLoadBalancing(input_dim, num_components=6)\n",
    "    model_without_lb = SimpleMoLWithLoadBalancing(input_dim, num_components=6)\n",
    "    \n",
    "    # Optimizers\n",
    "    opt_with_lb = torch.optim.Adam(model_with_lb.parameters(), lr=0.01)\n",
    "    opt_without_lb = torch.optim.Adam(model_without_lb.parameters(), lr=0.01)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'with_lb': {'main_loss': [], 'lb_loss': [], 'entropy': [], 'gini': []},\n",
    "        'without_lb': {'main_loss': [], 'lb_loss': [], 'entropy': [], 'gini': []}\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train model WITH load balancing\n",
    "        opt_with_lb.zero_grad()\n",
    "        \n",
    "        output_with_lb = model_with_lb(queries, items, return_gating_info=True)\n",
    "        similarities_with_lb = output_with_lb['similarities']\n",
    "        \n",
    "        main_loss_with_lb = F.mse_loss(similarities_with_lb, labels)\n",
    "        \n",
    "        if 'gating_weights' in output_with_lb:\n",
    "            lb_losses = model_with_lb.compute_load_balancing_loss(\n",
    "                output_with_lb['gating_weights'], \n",
    "                output_with_lb['input_features']\n",
    "            )\n",
    "            total_loss_with_lb = main_loss_with_lb + lb_losses['total']\n",
    "        else:\n",
    "            lb_losses = {'total': torch.tensor(0.0)}\n",
    "            total_loss_with_lb = main_loss_with_lb\n",
    "        \n",
    "        total_loss_with_lb.backward()\n",
    "        opt_with_lb.step()\n",
    "        \n",
    "        # Train model WITHOUT load balancing\n",
    "        opt_without_lb.zero_grad()\n",
    "        \n",
    "        output_without_lb = model_without_lb(queries, items, return_gating_info=True)\n",
    "        similarities_without_lb = output_without_lb['similarities']\n",
    "        \n",
    "        main_loss_without_lb = F.mse_loss(similarities_without_lb, labels)\n",
    "        main_loss_without_lb.backward()\n",
    "        opt_without_lb.step()\n",
    "        \n",
    "        # Compute statistics\n",
    "        with torch.no_grad():\n",
    "            for model_name, output in [('with_lb', output_with_lb), ('without_lb', output_without_lb)]:\n",
    "                if 'gating_weights' in output:\n",
    "                    gating_weights = output['gating_weights']\n",
    "                    component_usage = gating_weights.mean(dim=0)\n",
    "                    \n",
    "                    # Compute entropy\n",
    "                    entropy = -torch.sum(component_usage * torch.log(component_usage + 1e-8))\n",
    "                    \n",
    "                    # Compute Gini coefficient\n",
    "                    sorted_weights = torch.sort(component_usage)[0]\n",
    "                    n = len(sorted_weights)\n",
    "                    index = torch.arange(1, n + 1, dtype=torch.float32)\n",
    "                    gini = (2 * torch.sum(index * sorted_weights)) / (n * torch.sum(sorted_weights)) - (n + 1) / n\n",
    "                    \n",
    "                    history[model_name]['entropy'].append(entropy.item())\n",
    "                    history[model_name]['gini'].append(gini.item())\n",
    "                else:\n",
    "                    history[model_name]['entropy'].append(0)\n",
    "                    history[model_name]['gini'].append(1.0)\n",
    "        \n",
    "        # Record losses\n",
    "        history['with_lb']['main_loss'].append(main_loss_with_lb.item())\n",
    "        history['with_lb']['lb_loss'].append(lb_losses['total'].item())\n",
    "        history['without_lb']['main_loss'].append(main_loss_without_lb.item())\n",
    "        history['without_lb']['lb_loss'].append(0.0)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: WITH LB - Main: {main_loss_with_lb.item():.4f}, LB: {lb_losses['total'].item():.4f}\")\n",
    "            print(f\"           WITHOUT LB - Main: {main_loss_without_lb.item():.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run training comparison\n",
    "training_history = train_with_load_balancing_comparison(num_epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Phần 5: Visualization và Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# 1. Component Collapse Scenarios\n",
    "scenario_names = list(collapse_results.keys())\n",
    "perplexities = [collapse_results[name]['perplexity'] for name in scenario_names]\n",
    "ginis = [collapse_results[name]['gini'] for name in scenario_names]\n",
    "\n",
    "axes[0, 0].bar(scenario_names, perplexities, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].axhline(y=8, color='red', linestyle='--', alpha=0.7, label='Max Perplexity')\n",
    "axes[0, 0].set_title('Component Collapse: Effective Components')\n",
    "axes[0, 0].set_ylabel('Perplexity (Effective Components)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Gini Coefficient\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "bars = axes[0, 1].bar(scenario_names, ginis, alpha=0.7, color=colors)\n",
    "axes[0, 1].set_title('Component Collapse: Inequality (Gini)')\n",
    "axes[0, 1].set_ylabel('Gini Coefficient')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add text annotations\n",
    "for bar, gini in zip(bars, ginis):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{gini:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Weight Distributions\n",
    "for i, (name, weights) in enumerate(collapse_scenarios.items()):\n",
    "    if i < 4:  # Show first 4 scenarios\n",
    "        mean_weights = np.mean(weights, axis=0)\n",
    "        axes[0, 2].plot(mean_weights, 'o-', label=name, alpha=0.8, linewidth=2)\n",
    "\n",
    "axes[0, 2].set_title('Component Weight Distributions')\n",
    "axes[0, 2].set_xlabel('Component Index')\n",
    "axes[0, 2].set_ylabel('Average Weight')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Loss Comparison\n",
    "epochs = range(len(training_history['with_lb']['main_loss']))\n",
    "axes[1, 0].plot(epochs, training_history['with_lb']['main_loss'], 'b-', label='With Load Balancing', linewidth=2)\n",
    "axes[1, 0].plot(epochs, training_history['without_lb']['main_loss'], 'r-', label='Without Load Balancing', linewidth=2)\n",
    "axes[1, 0].set_title('Training: Main Loss Comparison')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Main Loss (MSE)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Load Balancing Loss\n",
    "axes[1, 1].plot(epochs, training_history['with_lb']['lb_loss'], 'g-', linewidth=2)\n",
    "axes[1, 1].set_title('Load Balancing Loss Over Time')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Load Balancing Loss')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Entropy Evolution\n",
    "max_entropy = math.log(6)  # For 6 components\n",
    "axes[1, 2].plot(epochs, training_history['with_lb']['entropy'], 'b-', label='With LB', linewidth=2)\n",
    "axes[1, 2].plot(epochs, training_history['without_lb']['entropy'], 'r-', label='Without LB', linewidth=2)\n",
    "axes[1, 2].axhline(y=max_entropy, color='green', linestyle='--', alpha=0.7, label='Max Entropy')\n",
    "axes[1, 2].set_title('Component Usage Entropy')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Entropy')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Gini Evolution\n",
    "axes[2, 0].plot(epochs, training_history['with_lb']['gini'], 'b-', label='With LB', linewidth=2)\n",
    "axes[2, 0].plot(epochs, training_history['without_lb']['gini'], 'r-', label='Without LB', linewidth=2)\n",
    "axes[2, 0].set_title('Component Usage Inequality (Gini)')\n",
    "axes[2, 0].set_xlabel('Epoch')\n",
    "axes[2, 0].set_ylabel('Gini Coefficient')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Information Theory Concepts Visualization\n",
    "entropy_values = []\n",
    "perplexity_values = []\n",
    "prob_values = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "for p in prob_values:\n",
    "    # Binary distribution\n",
    "    dist = np.array([p, 1-p])\n",
    "    entropy = utils.entropy(dist, base=2)\n",
    "    perplexity = utils.perplexity(dist)\n",
    "    entropy_values.append(entropy)\n",
    "    perplexity_values.append(perplexity)\n",
    "\n",
    "axes[2, 1].plot(prob_values, entropy_values, 'go-', label='Entropy', linewidth=2)\n",
    "axes[2, 1].set_title('Binary Distribution: Entropy vs Probability')\n",
    "axes[2, 1].set_xlabel('P(X=1)')\n",
    "axes[2, 1].set_ylabel('Entropy (bits)')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Perplexity vs Probability\n",
    "axes[2, 2].plot(prob_values, perplexity_values, 'mo-', linewidth=2)\n",
    "axes[2, 2].set_title('Binary Distribution: Perplexity vs Probability')\n",
    "axes[2, 2].set_xlabel('P(X=1)')\n",
    "axes[2, 2].set_ylabel('Perplexity')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Comprehensive visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Key Insights và Practical Guidelines\n",
    "\n",
    "### 🔍 Quan sát từ Experiments:\n",
    "\n",
    "1. **Component Collapse Impact**:\n",
    "   - Uniform distribution: Perplexity = 8/8 (full utilization)\n",
    "   - Collapsed distribution: Perplexity << 8 (wasted capacity)\n",
    "   - Gini coefficient tăng → inequality tăng\n",
    "\n",
    "2. **Load Balancing Benefits**:\n",
    "   - Higher entropy → better component utilization\n",
    "   - Lower Gini coefficient → more equal distribution\n",
    "   - Potentially better generalization\n",
    "\n",
    "3. **Training Dynamics**:\n",
    "   - Load balancing loss initially high, decreases over time\n",
    "   - Main task performance không bị hurt significantly\n",
    "   - Component usage becomes more balanced\n",
    "\n",
    "### 📖 Theoretical Insights:\n",
    "\n",
    "**Information Theory Perspective**:\n",
    "- **High Entropy** → Uniform component usage → Better capacity utilization\n",
    "- **Low Mutual Information** → Gating decisions independent of input patterns\n",
    "- **Perplexity** → Effective number of components being used\n",
    "\n",
    "**Mathematical Foundation**:\n",
    "```\n",
    "Optimal Load Balancing:\n",
    "- Maximize H(G) (entropy of gating decisions)\n",
    "- Minimize I(Z; G) (mutual information with inputs)\n",
    "- Minimize Var(usage) (variance in component usage)\n",
    "```\n",
    "\n",
    "### 🚀 Practical Implementation Guidelines:\n",
    "\n",
    "1. **Loss Function Design**:\n",
    "   ```python\n",
    "   total_loss = main_loss + λ₁ * uniform_loss + λ₂ * entropy_loss + λ₃ * mi_loss\n",
    "   ```\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Start with small λ values (0.001 - 0.01)\n",
    "   - Monitor component usage statistics\n",
    "   - Adjust based on Gini coefficient and perplexity\n",
    "\n",
    "3. **Monitoring Metrics**:\n",
    "   - **Perplexity**: Should be close to num_components\n",
    "   - **Gini Coefficient**: Should be close to 0\n",
    "   - **Entropy**: Should be close to log(num_components)\n",
    "\n",
    "4. **When to Use Load Balancing**:\n",
    "   - Large number of components (P > 4)\n",
    "   - Complex datasets with diverse patterns\n",
    "   - When component collapse is observed\n",
    "   - In production systems where efficiency matters\n",
    "\n",
    "### ⚠️ Common Pitfalls:\n",
    "\n",
    "1. **Over-regularization**: Too strong load balancing → worse main task performance\n",
    "2. **Under-regularization**: Component collapse → wasted model capacity\n",
    "3. **Ignoring MI term**: Components may still correlate with input patterns\n",
    "4. **Fixed λ values**: May need adaptive scheduling during training\n",
    "\n",
    "### 🎯 Advanced Techniques:\n",
    "\n",
    "1. **Adaptive Load Balancing**: Adjust λ based on current component usage\n",
    "2. **Curriculum Learning**: Gradually increase load balancing strength\n",
    "3. **Temperature Annealing**: Start with high temperature, decrease over time\n",
    "4. **Component-specific Penalties**: Different penalties for different components\n",
    "\n",
    "### 📚 Research Directions:\n",
    "\n",
    "1. **Better MI Estimation**: MINE, InfoNCE, contrastive methods\n",
    "2. **Dynamic Component Count**: Adaptive number of components\n",
    "3. **Hierarchical Load Balancing**: Multi-level component organization\n",
    "4. **Hardware-aware Load Balancing**: Consider GPU memory and compute constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}