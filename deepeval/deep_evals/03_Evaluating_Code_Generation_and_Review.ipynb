{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1588c7b",
   "metadata": {},
   "source": [
    "# 03 – Đánh giá sinh & review mã nguồn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ed7c0",
   "metadata": {},
   "source": [
    "Tạo GEval metric tuỳ chỉnh để đánh giá sinh mã và review mã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18246a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from deepeval import assert_test, LLMTestCase\n",
    "from deepeval.metrics.geval import GEval\n",
    "\n",
    "with open('data/code_samples.json', encoding='utf-8') as f:\n",
    "    samples = json.load(f)['samples']\n",
    "\n",
    "# Metric CodeCorrectness\n",
    "code_correctness_prompt = \"\"\"Bạn là trình thông dịch Python. Thực thi hàm trong {actual_output}\n",
    "với input mẫu để xác định đúng logic (1) hay sai (0).\"\"\"\n",
    "CodeCorrectness = GEval(name=\"CodeCorrectness\", prompt=code_correctness_prompt, threshold=0.7)\n",
    "\n",
    "# Metric Readability\n",
    "readability_prompt = \"\"\"Đánh giá độ dễ đọc của đoạn mã sau (0-1):\\n{actual_output}\"\"\"\n",
    "Readability = GEval(name=\"Readability\", prompt=readability_prompt, threshold=0.5)\n",
    "\n",
    "case = LLMTestCase(input=samples[0]['problem'], actual_output=samples[0]['correct_code'])\n",
    "assert_test(case, metrics=[CodeCorrectness, Readability])\n",
    "\n",
    "# Metric ReviewCompleteness\n",
    "review_prompt = \"\"\"Đánh giá mức độ đầy đủ của phần review dưới đây so với lỗi trong context (0-1):\\nReview: {actual_output}\\nLỗi: {context}\"\"\"\n",
    "ReviewCompleteness = GEval(name=\"ReviewCompleteness\", prompt=review_prompt, threshold=0.6)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
