{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning 4: Tied Ranking Metrics (MTRR & TMHits@K)\n",
    "\n",
    "## Paper: Don't Forget to Connect! Improving RAG with Graph-based Reranking\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "This notebook provides a deep dive into the novel evaluation metrics introduced in the G-RAG paper:\n",
    "\n",
    "1. **Mean Tied Reciprocal Rank (MTRR)**: A ranking metric that handles tied scores by averaging reciprocal ranks\n",
    "2. **Tied Mean Hits@K (TMHits@K)**: An extension of Hits@K that accounts for tied rankings\n",
    "3. **Implementation and Analysis**: Complete implementation with visualization and comparison to standard metrics\n",
    "4. **Real-world Applications**: Understanding when and why these metrics are crucial for LLM-based reranking\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Why Tied Ranking Metrics Matter\n",
    "\n",
    "### The Problem with Traditional Metrics\n",
    "\n",
    "When Large Language Models (LLMs) are used as rerankers, they often produce **tied scores** for multiple documents. Traditional ranking metrics like Mean Reciprocal Rank (MRR) and Hits@K don't handle ties appropriately, leading to:\n",
    "\n",
    "- **Unfair evaluation**: Tied documents get arbitrary rankings\n",
    "- **Inconsistent results**: Same model performance varies based on tie-breaking strategy\n",
    "- **Misleading comparisons**: Models with different tie patterns appear incomparable\n",
    "\n",
    "### The G-RAG Solution\n",
    "\n",
    "The paper introduces metrics that:\n",
    "- **Average over all possible tie-breaking arrangements**\n",
    "- **Provide fair evaluation** regardless of tie-breaking strategy\n",
    "- **Enable consistent model comparison** across different LLM rerankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“Š Environment setup complete!\")\n",
    "print(\"ðŸŽ¯ Ready to explore tied ranking metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Mathematical Foundation\n",
    "\n",
    "### Traditional Mean Reciprocal Rank (MRR)\n",
    "\n",
    "For a set of queries $Q$ and their first relevant document at rank $r_i$:\n",
    "\n",
    "$$MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{r_i}$$\n",
    "\n",
    "### Mean Tied Reciprocal Rank (MTRR)\n",
    "\n",
    "When documents have tied scores, we consider all possible rankings and average:\n",
    "\n",
    "$$MTRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{|\\Pi_i|} \\sum_{\\pi \\in \\Pi_i} \\frac{1}{r_i^\\pi}$$\n",
    "\n",
    "Where:\n",
    "- $\\Pi_i$ is the set of all possible rankings for query $i$\n",
    "- $r_i^\\pi$ is the rank of first relevant document in permutation $\\pi$\n",
    "\n",
    "### Tied Mean Hits@K (TMHits@K)\n",
    "\n",
    "$$TMHits@K = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{|\\Pi_i|} \\sum_{\\pi \\in \\Pi_i} \\mathbb{1}[r_i^\\pi \\leq K]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RankingResult:\n",
    "    \"\"\"Represents a document ranking with potential ties\"\"\"\n",
    "    doc_ids: List[str]\n",
    "    scores: List[float]\n",
    "    relevance: List[bool]  # True if document is relevant\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert len(self.doc_ids) == len(self.scores) == len(self.relevance)\n",
    "        \n",
    "    def get_tied_groups(self) -> List[List[int]]:\n",
    "        \"\"\"Group document indices by their scores (tied documents)\"\"\"\n",
    "        score_to_indices = defaultdict(list)\n",
    "        for i, score in enumerate(self.scores):\n",
    "            score_to_indices[score].append(i)\n",
    "        \n",
    "        # Sort by score (descending) and return groups\n",
    "        sorted_scores = sorted(score_to_indices.keys(), reverse=True)\n",
    "        return [score_to_indices[score] for score in sorted_scores]\n",
    "\n",
    "class TiedRankingMetrics:\n",
    "    \"\"\"Implementation of tied ranking metrics from G-RAG paper\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_all_rankings(tied_groups: List[List[int]]) -> List[List[int]]:\n",
    "        \"\"\"Generate all possible rankings given tied groups\"\"\"\n",
    "        # Generate all permutations within each tied group\n",
    "        group_permutations = []\n",
    "        for group in tied_groups:\n",
    "            group_permutations.append(list(itertools.permutations(group)))\n",
    "        \n",
    "        # Combine all group permutations\n",
    "        all_rankings = []\n",
    "        for combination in itertools.product(*group_permutations):\n",
    "            ranking = []\n",
    "            for group_perm in combination:\n",
    "                ranking.extend(group_perm)\n",
    "            all_rankings.append(ranking)\n",
    "            \n",
    "        return all_rankings\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_mrr_single(ranking: List[int], relevance: List[bool]) -> float:\n",
    "        \"\"\"Compute MRR for a single ranking\"\"\"\n",
    "        for rank, doc_idx in enumerate(ranking, 1):\n",
    "            if relevance[doc_idx]:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_hits_k_single(ranking: List[int], relevance: List[bool], k: int) -> float:\n",
    "        \"\"\"Compute Hits@K for a single ranking\"\"\"\n",
    "        for rank, doc_idx in enumerate(ranking, 1):\n",
    "            if relevance[doc_idx] and rank <= k:\n",
    "                return 1.0\n",
    "        return 0.0\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_mtrr(cls, result: RankingResult) -> float:\n",
    "        \"\"\"Compute Mean Tied Reciprocal Rank\"\"\"\n",
    "        tied_groups = result.get_tied_groups()\n",
    "        all_rankings = cls.generate_all_rankings(tied_groups)\n",
    "        \n",
    "        if not all_rankings:\n",
    "            return 0.0\n",
    "        \n",
    "        total_rr = 0.0\n",
    "        for ranking in all_rankings:\n",
    "            total_rr += cls.compute_mrr_single(ranking, result.relevance)\n",
    "        \n",
    "        return total_rr / len(all_rankings)\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_tmhits_k(cls, result: RankingResult, k: int) -> float:\n",
    "        \"\"\"Compute Tied Mean Hits@K\"\"\"\n",
    "        tied_groups = result.get_tied_groups()\n",
    "        all_rankings = cls.generate_all_rankings(tied_groups)\n",
    "        \n",
    "        if not all_rankings:\n",
    "            return 0.0\n",
    "        \n",
    "        total_hits = 0.0\n",
    "        for ranking in all_rankings:\n",
    "            total_hits += cls.compute_hits_k_single(ranking, result.relevance, k)\n",
    "        \n",
    "        return total_hits / len(all_rankings)\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_traditional_mrr(cls, result: RankingResult) -> float:\n",
    "        \"\"\"Compute traditional MRR (arbitrary tie-breaking)\"\"\"\n",
    "        # Sort by score descending, then by original index for tie-breaking\n",
    "        sorted_indices = sorted(range(len(result.scores)), \n",
    "                              key=lambda i: (-result.scores[i], i))\n",
    "        return cls.compute_mrr_single(sorted_indices, result.relevance)\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_traditional_hits_k(cls, result: RankingResult, k: int) -> float:\n",
    "        \"\"\"Compute traditional Hits@K (arbitrary tie-breaking)\"\"\"\n",
    "        sorted_indices = sorted(range(len(result.scores)), \n",
    "                              key=lambda i: (-result.scores[i], i))\n",
    "        return cls.compute_hits_k_single(sorted_indices, result.relevance, k)\n",
    "\n",
    "print(\"ðŸ“ Tied ranking metrics implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Demonstrating the Problem\n",
    "\n",
    "Let's create scenarios where tied scores lead to different traditional metric values but consistent tied metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demo_scenarios():\n",
    "    \"\"\"Create demonstration scenarios showing tied ranking issues\"\"\"\n",
    "    \n",
    "    scenarios = {\n",
    "        \"No Ties\": RankingResult(\n",
    "            doc_ids=[\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"],\n",
    "            scores=[0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "            relevance=[False, True, False, False, False]  # Relevant doc at rank 2\n",
    "        ),\n",
    "        \n",
    "        \"Simple Tie\": RankingResult(\n",
    "            doc_ids=[\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"],\n",
    "            scores=[0.9, 0.8, 0.8, 0.6, 0.5],  # doc2 and doc3 tied\n",
    "            relevance=[False, True, False, False, False]  # Relevant doc tied at rank 2-3\n",
    "        ),\n",
    "        \n",
    "        \"Complex Tie\": RankingResult(\n",
    "            doc_ids=[\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"],\n",
    "            scores=[0.8, 0.8, 0.8, 0.6, 0.5],  # Three docs tied at top\n",
    "            relevance=[False, True, False, False, False]  # Relevant doc in top tie\n",
    "        ),\n",
    "        \n",
    "        \"Multiple Ties\": RankingResult(\n",
    "            doc_ids=[\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"],\n",
    "            scores=[0.9, 0.8, 0.8, 0.6, 0.6],  # Two separate tie groups\n",
    "            relevance=[False, False, True, False, False]  # Relevant doc in first tie\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# Analyze each scenario\n",
    "scenarios = create_demo_scenarios()\n",
    "metrics = TiedRankingMetrics()\n",
    "\n",
    "results_df = []\n",
    "\n",
    "for name, scenario in scenarios.items():\n",
    "    # Compute traditional metrics\n",
    "    trad_mrr = metrics.compute_traditional_mrr(scenario)\n",
    "    trad_hits5 = metrics.compute_traditional_hits_k(scenario, 5)\n",
    "    trad_hits3 = metrics.compute_traditional_hits_k(scenario, 3)\n",
    "    \n",
    "    # Compute tied metrics\n",
    "    mtrr = metrics.compute_mtrr(scenario)\n",
    "    tmhits5 = metrics.compute_tmhits_k(scenario, 5)\n",
    "    tmhits3 = metrics.compute_tmhits_k(scenario, 3)\n",
    "    \n",
    "    # Count tied groups\n",
    "    tied_groups = scenario.get_tied_groups()\n",
    "    num_ties = sum(1 for group in tied_groups if len(group) > 1)\n",
    "    max_tie_size = max(len(group) for group in tied_groups)\n",
    "    \n",
    "    results_df.append({\n",
    "        'Scenario': name,\n",
    "        'Scores': str(scenario.scores),\n",
    "        'Relevance': str(scenario.relevance),\n",
    "        'Num_Ties': num_ties,\n",
    "        'Max_Tie_Size': max_tie_size,\n",
    "        'Traditional_MRR': f\"{trad_mrr:.3f}\",\n",
    "        'MTRR': f\"{mtrr:.3f}\",\n",
    "        'Traditional_Hits@3': f\"{trad_hits3:.3f}\",\n",
    "        'TMHits@3': f\"{tmhits3:.3f}\",\n",
    "        'Traditional_Hits@5': f\"{trad_hits5:.3f}\",\n",
    "        'TMHits@5': f\"{tmhits5:.3f}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_df)\n",
    "print(\"ðŸ“Š Scenario Analysis Results:\")\n",
    "print(\"=\" * 80)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Detailed Analysis: Understanding the Differences\n",
    "\n",
    "Let's dive deeper into how tied rankings affect the metrics and why the tied versions provide fairer evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ranking_permutations(scenario: RankingResult, scenario_name: str):\n",
    "    \"\"\"Analyze all possible rankings for a tied scenario\"\"\"\n",
    "    print(f\"\\nðŸ” Detailed Analysis: {scenario_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show original data\n",
    "    print(f\"ðŸ“Š Scores: {scenario.scores}\")\n",
    "    print(f\"ðŸŽ¯ Relevance: {scenario.relevance}\")\n",
    "    \n",
    "    # Get tied groups\n",
    "    tied_groups = scenario.get_tied_groups()\n",
    "    print(f\"\\nðŸ”— Tied Groups:\")\n",
    "    for i, group in enumerate(tied_groups):\n",
    "        if len(group) > 1:\n",
    "            group_docs = [scenario.doc_ids[idx] for idx in group]\n",
    "            group_scores = [scenario.scores[idx] for idx in group]\n",
    "            group_rel = [scenario.relevance[idx] for idx in group]\n",
    "            print(f\"   Group {i+1}: {group_docs} (score={group_scores[0]}, relevant={group_rel})\")\n",
    "        else:\n",
    "            doc_idx = group[0]\n",
    "            print(f\"   Single: {scenario.doc_ids[doc_idx]} (score={scenario.scores[doc_idx]}, relevant={scenario.relevance[doc_idx]})\")\n",
    "    \n",
    "    # Generate all rankings\n",
    "    all_rankings = metrics.generate_all_rankings(tied_groups)\n",
    "    print(f\"\\nðŸ“ˆ Total possible rankings: {len(all_rankings)}\")\n",
    "    \n",
    "    if len(all_rankings) <= 10:  # Only show details for manageable number\n",
    "        print(\"\\nðŸŽ² All possible rankings:\")\n",
    "        ranking_mrrs = []\n",
    "        ranking_hits3 = []\n",
    "        \n",
    "        for i, ranking in enumerate(all_rankings):\n",
    "            ranking_docs = [scenario.doc_ids[idx] for idx in ranking]\n",
    "            ranking_rel = [scenario.relevance[idx] for idx in ranking]\n",
    "            \n",
    "            # Find first relevant doc position\n",
    "            first_rel_pos = next((pos + 1 for pos, rel in enumerate(ranking_rel) if rel), None)\n",
    "            \n",
    "            mrr = metrics.compute_mrr_single(ranking, scenario.relevance)\n",
    "            hits3 = metrics.compute_hits_k_single(ranking, scenario.relevance, 3)\n",
    "            \n",
    "            ranking_mrrs.append(mrr)\n",
    "            ranking_hits3.append(hits3)\n",
    "            \n",
    "            print(f\"   Ranking {i+1}: {ranking_docs}\")\n",
    "            print(f\"     First relevant at position: {first_rel_pos}\")\n",
    "            print(f\"     MRR: {mrr:.3f}, Hits@3: {hits3:.3f}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Summary:\")\n",
    "        print(f\"   MRR range: {min(ranking_mrrs):.3f} - {max(ranking_mrrs):.3f}\")\n",
    "        print(f\"   Average MRR (MTRR): {np.mean(ranking_mrrs):.3f}\")\n",
    "        print(f\"   Hits@3 range: {min(ranking_hits3):.3f} - {max(ranking_hits3):.3f}\")\n",
    "        print(f\"   Average Hits@3 (TMHits@3): {np.mean(ranking_hits3):.3f}\")\n",
    "    else:\n",
    "        print(f\"   (Too many rankings to show details - {len(all_rankings)} total)\")\n",
    "\n",
    "# Analyze the most interesting scenarios\n",
    "interesting_scenarios = [\"Simple Tie\", \"Complex Tie\"]\n",
    "for name in interesting_scenarios:\n",
    "    analyze_ranking_permutations(scenarios[name], name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Visualization: Impact of Ties on Evaluation\n",
    "\n",
    "Let's visualize how the number and size of ties affect the difference between traditional and tied metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_scenarios(n_scenarios: int = 100) -> List[RankingResult]:\n",
    "    \"\"\"Generate synthetic scenarios with varying tie patterns\"\"\"\n",
    "    np.random.seed(42)\n",
    "    scenarios = []\n",
    "    \n",
    "    for i in range(n_scenarios):\n",
    "        n_docs = np.random.randint(5, 15)  # 5-14 documents\n",
    "        \n",
    "        # Generate scores with some probability of ties\n",
    "        unique_scores = np.random.uniform(0.1, 1.0, size=np.random.randint(3, n_docs+1))\n",
    "        scores = np.random.choice(unique_scores, size=n_docs, replace=True)\n",
    "        \n",
    "        # Add some noise to reduce ties\n",
    "        if np.random.random() > 0.3:  # 70% chance to add noise\n",
    "            noise = np.random.normal(0, 0.01, size=n_docs)\n",
    "            scores = scores + noise\n",
    "        \n",
    "        # Generate relevance (1-3 relevant docs)\n",
    "        n_relevant = np.random.randint(1, min(4, n_docs+1))\n",
    "        relevance = [False] * n_docs\n",
    "        relevant_indices = np.random.choice(n_docs, size=n_relevant, replace=False)\n",
    "        for idx in relevant_indices:\n",
    "            relevance[idx] = True\n",
    "        \n",
    "        doc_ids = [f\"doc_{i}_{j}\" for j in range(n_docs)]\n",
    "        \n",
    "        scenarios.append(RankingResult(\n",
    "            doc_ids=doc_ids,\n",
    "            scores=scores.tolist(),\n",
    "            relevance=relevance\n",
    "        ))\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# Generate and analyze synthetic scenarios\n",
    "synthetic_scenarios = generate_synthetic_scenarios(200)\n",
    "analysis_results = []\n",
    "\n",
    "for i, scenario in enumerate(synthetic_scenarios):\n",
    "    # Count ties\n",
    "    tied_groups = scenario.get_tied_groups()\n",
    "    num_ties = sum(1 for group in tied_groups if len(group) > 1)\n",
    "    max_tie_size = max(len(group) for group in tied_groups) if tied_groups else 1\n",
    "    total_tied_docs = sum(len(group) for group in tied_groups if len(group) > 1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    trad_mrr = metrics.compute_traditional_mrr(scenario)\n",
    "    mtrr = metrics.compute_mtrr(scenario)\n",
    "    trad_hits3 = metrics.compute_traditional_hits_k(scenario, 3)\n",
    "    tmhits3 = metrics.compute_tmhits_k(scenario, 3)\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'scenario_id': i,\n",
    "        'n_docs': len(scenario.doc_ids),\n",
    "        'n_relevant': sum(scenario.relevance),\n",
    "        'num_ties': num_ties,\n",
    "        'max_tie_size': max_tie_size,\n",
    "        'total_tied_docs': total_tied_docs,\n",
    "        'tie_ratio': total_tied_docs / len(scenario.doc_ids),\n",
    "        'traditional_mrr': trad_mrr,\n",
    "        'mtrr': mtrr,\n",
    "        'mrr_diff': abs(trad_mrr - mtrr),\n",
    "        'traditional_hits3': trad_hits3,\n",
    "        'tmhits3': tmhits3,\n",
    "        'hits3_diff': abs(trad_hits3 - tmhits3)\n",
    "    })\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "print(f\"ðŸ“Š Analyzed {len(synthetic_scenarios)} synthetic scenarios\")\n",
    "print(f\"ðŸ“ˆ Scenarios with ties: {(analysis_df['num_ties'] > 0).sum()} ({(analysis_df['num_ties'] > 0).mean()*100:.1f}%)\")\n",
    "print(f\"ðŸ“Š Average tie ratio: {analysis_df['tie_ratio'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Impact of Tied Rankings on Evaluation Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Tie ratio vs MRR difference\n",
    "axes[0, 0].scatter(analysis_df['tie_ratio'], analysis_df['mrr_diff'], alpha=0.6, s=50)\n",
    "axes[0, 0].set_xlabel('Tie Ratio (Fraction of Tied Documents)')\n",
    "axes[0, 0].set_ylabel('|Traditional MRR - MTRR|')\n",
    "axes[0, 0].set_title('MRR Difference vs Tie Ratio')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Max tie size vs MRR difference\n",
    "axes[0, 1].scatter(analysis_df['max_tie_size'], analysis_df['mrr_diff'], alpha=0.6, s=50)\n",
    "axes[0, 1].set_xlabel('Maximum Tie Size')\n",
    "axes[0, 1].set_ylabel('|Traditional MRR - MTRR|')\n",
    "axes[0, 1].set_title('MRR Difference vs Max Tie Size')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribution of MRR differences\n",
    "axes[0, 2].hist(analysis_df['mrr_diff'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_xlabel('|Traditional MRR - MTRR|')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title('Distribution of MRR Differences')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Tie ratio vs Hits@3 difference\n",
    "axes[1, 0].scatter(analysis_df['tie_ratio'], analysis_df['hits3_diff'], alpha=0.6, s=50, color='orange')\n",
    "axes[1, 0].set_xlabel('Tie Ratio (Fraction of Tied Documents)')\n",
    "axes[1, 0].set_ylabel('|Traditional Hits@3 - TMHits@3|')\n",
    "axes[1, 0].set_title('Hits@3 Difference vs Tie Ratio')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Max tie size vs Hits@3 difference\n",
    "axes[1, 1].scatter(analysis_df['max_tie_size'], analysis_df['hits3_diff'], alpha=0.6, s=50, color='orange')\n",
    "axes[1, 1].set_xlabel('Maximum Tie Size')\n",
    "axes[1, 1].set_ylabel('|Traditional Hits@3 - TMHits@3|')\n",
    "axes[1, 1].set_title('Hits@3 Difference vs Max Tie Size')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Correlation between MRR and Hits@3 differences\n",
    "axes[1, 2].scatter(analysis_df['mrr_diff'], analysis_df['hits3_diff'], alpha=0.6, s=50, color='green')\n",
    "axes[1, 2].set_xlabel('|Traditional MRR - MTRR|')\n",
    "axes[1, 2].set_ylabel('|Traditional Hits@3 - TMHits@3|')\n",
    "axes[1, 2].set_title('MRR vs Hits@3 Difference Correlation')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nðŸ“Š Summary Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean MRR difference: {analysis_df['mrr_diff'].mean():.4f} (Â±{analysis_df['mrr_diff'].std():.4f})\")\n",
    "print(f\"Max MRR difference: {analysis_df['mrr_diff'].max():.4f}\")\n",
    "print(f\"Mean Hits@3 difference: {analysis_df['hits3_diff'].mean():.4f} (Â±{analysis_df['hits3_diff'].std():.4f})\")\n",
    "print(f\"Max Hits@3 difference: {analysis_df['hits3_diff'].max():.4f}\")\n",
    "print(f\"\\nCorrelation between tie ratio and MRR diff: {analysis_df['tie_ratio'].corr(analysis_df['mrr_diff']):.3f}\")\n",
    "print(f\"Correlation between max tie size and MRR diff: {analysis_df['max_tie_size'].corr(analysis_df['mrr_diff']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– LLM Reranking Simulation\n",
    "\n",
    "Let's simulate how LLM-based rerankers might produce tied scores and why tied metrics are crucial for fair evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMRerankerSimulator:\n",
    "    \"\"\"Simulates different LLM reranking behaviors\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, tie_probability: float = 0.3, score_discretization: int = 10):\n",
    "        self.model_name = model_name\n",
    "        self.tie_probability = tie_probability\n",
    "        self.score_discretization = score_discretization\n",
    "    \n",
    "    def rerank_documents(self, query: str, documents: List[str], true_relevance: List[bool]) -> RankingResult:\n",
    "        \"\"\"Simulate LLM reranking with potential ties\"\"\"\n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        # Simulate base relevance scores (with some correlation to true relevance)\n",
    "        base_scores = np.random.uniform(0.1, 0.9, n_docs)\n",
    "        \n",
    "        # Boost relevant documents (but not perfectly)\n",
    "        for i, is_relevant in enumerate(true_relevance):\n",
    "            if is_relevant:\n",
    "                if np.random.random() > 0.1:  # 90% chance to boost relevant docs\n",
    "                    base_scores[i] += np.random.uniform(0.1, 0.3)\n",
    "        \n",
    "        # Clip scores to [0, 1]\n",
    "        base_scores = np.clip(base_scores, 0, 1)\n",
    "        \n",
    "        # Discretize scores to simulate LLM output patterns\n",
    "        if self.score_discretization > 0:\n",
    "            score_levels = np.linspace(0, 1, self.score_discretization)\n",
    "            discretized_scores = []\n",
    "            for score in base_scores:\n",
    "                # Find closest level\n",
    "                closest_level = score_levels[np.argmin(np.abs(score_levels - score))]\n",
    "                discretized_scores.append(closest_level)\n",
    "            base_scores = np.array(discretized_scores)\n",
    "        \n",
    "        # Introduce additional ties based on tie_probability\n",
    "        if self.tie_probability > 0:\n",
    "            for i in range(n_docs):\n",
    "                if np.random.random() < self.tie_probability:\n",
    "                    # Find another document to tie with\n",
    "                    other_idx = np.random.choice([j for j in range(n_docs) if j != i])\n",
    "                    base_scores[other_idx] = base_scores[i]\n",
    "        \n",
    "        doc_ids = [f\"doc_{i}\" for i in range(n_docs)]\n",
    "        \n",
    "        return RankingResult(\n",
    "            doc_ids=doc_ids,\n",
    "            scores=base_scores.tolist(),\n",
    "            relevance=true_relevance\n",
    "        )\n",
    "\n",
    "# Simulate different LLM rerankers\n",
    "rerankers = {\n",
    "    \"GPT-4 (Low Ties)\": LLMRerankerSimulator(\"GPT-4\", tie_probability=0.1, score_discretization=20),\n",
    "    \"PaLM-2 (Medium Ties)\": LLMRerankerSimulator(\"PaLM-2\", tie_probability=0.3, score_discretization=10),\n",
    "    \"Claude (High Ties)\": LLMRerankerSimulator(\"Claude\", tie_probability=0.5, score_discretization=5),\n",
    "    \"Local LLM (Very High Ties)\": LLMRerankerSimulator(\"Local\", tie_probability=0.7, score_discretization=3)\n",
    "}\n",
    "\n",
    "def evaluate_llm_rerankers(n_queries: int = 50):\n",
    "    \"\"\"Evaluate different LLM rerankers on synthetic queries\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for query_id in range(n_queries):\n",
    "        # Generate synthetic query scenario\n",
    "        n_docs = np.random.randint(8, 20)\n",
    "        n_relevant = np.random.randint(1, min(5, n_docs))\n",
    "        \n",
    "        documents = [f\"Document {i} about topic X\" for i in range(n_docs)]\n",
    "        true_relevance = [False] * n_docs\n",
    "        relevant_indices = np.random.choice(n_docs, size=n_relevant, replace=False)\n",
    "        for idx in relevant_indices:\n",
    "            true_relevance[idx] = True\n",
    "        \n",
    "        query = f\"Query {query_id} about topic X\"\n",
    "        \n",
    "        # Evaluate each reranker\n",
    "        for reranker_name, reranker in rerankers.items():\n",
    "            ranking_result = reranker.rerank_documents(query, documents, true_relevance)\n",
    "            \n",
    "            # Compute metrics\n",
    "            trad_mrr = metrics.compute_traditional_mrr(ranking_result)\n",
    "            mtrr = metrics.compute_mtrr(ranking_result)\n",
    "            trad_hits5 = metrics.compute_traditional_hits_k(ranking_result, 5)\n",
    "            tmhits5 = metrics.compute_tmhits_k(ranking_result, 5)\n",
    "            \n",
    "            # Analyze ties\n",
    "            tied_groups = ranking_result.get_tied_groups()\n",
    "            num_ties = sum(1 for group in tied_groups if len(group) > 1)\n",
    "            tie_ratio = sum(len(group) for group in tied_groups if len(group) > 1) / len(documents)\n",
    "            \n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'reranker': reranker_name,\n",
    "                'n_docs': n_docs,\n",
    "                'n_relevant': n_relevant,\n",
    "                'num_ties': num_ties,\n",
    "                'tie_ratio': tie_ratio,\n",
    "                'traditional_mrr': trad_mrr,\n",
    "                'mtrr': mtrr,\n",
    "                'mrr_diff': abs(trad_mrr - mtrr),\n",
    "                'traditional_hits5': trad_hits5,\n",
    "                'tmhits5': tmhits5,\n",
    "                'hits5_diff': abs(trad_hits5 - tmhits5)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "llm_results = evaluate_llm_rerankers(100)\n",
    "\n",
    "print(\"ðŸ¤– LLM Reranker Evaluation Complete!\")\n",
    "print(f\"ðŸ“Š Evaluated {len(rerankers)} rerankers on {llm_results['query_id'].nunique()} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze LLM reranker performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('LLM Reranker Performance: Traditional vs Tied Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Mean performance by reranker (MRR)\n",
    "mrr_summary = llm_results.groupby('reranker').agg({\n",
    "    'traditional_mrr': 'mean',\n",
    "    'mtrr': 'mean',\n",
    "    'tie_ratio': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "x = np.arange(len(mrr_summary))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, mrr_summary['traditional_mrr'], width, label='Traditional MRR', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, mrr_summary['mtrr'], width, label='MTRR', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('LLM Reranker')\n",
    "axes[0, 0].set_ylabel('Mean MRR')\n",
    "axes[0, 0].set_title('Mean MRR Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(mrr_summary['reranker'], rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Tie ratio by reranker\n",
    "tie_summary = llm_results.groupby('reranker')['tie_ratio'].mean()\n",
    "axes[0, 1].bar(range(len(tie_summary)), tie_summary.values, color='orange', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('LLM Reranker')\n",
    "axes[0, 1].set_ylabel('Mean Tie Ratio')\n",
    "axes[0, 1].set_title('Average Tie Ratio by Reranker')\n",
    "axes[0, 1].set_xticks(range(len(tie_summary)))\n",
    "axes[0, 1].set_xticklabels(tie_summary.index, rotation=45, ha='right')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. MRR difference vs tie ratio\n",
    "for reranker in rerankers.keys():\n",
    "    subset = llm_results[llm_results['reranker'] == reranker]\n",
    "    axes[1, 0].scatter(subset['tie_ratio'], subset['mrr_diff'], \n",
    "                      label=reranker, alpha=0.6, s=30)\n",
    "\n",
    "axes[1, 0].set_xlabel('Tie Ratio')\n",
    "axes[1, 0].set_ylabel('|Traditional MRR - MTRR|')\n",
    "axes[1, 0].set_title('MRR Difference vs Tie Ratio by Reranker')\n",
    "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Distribution of metric differences\n",
    "for reranker in rerankers.keys():\n",
    "    subset = llm_results[llm_results['reranker'] == reranker]\n",
    "    axes[1, 1].hist(subset['mrr_diff'], bins=20, alpha=0.5, label=reranker, density=True)\n",
    "\n",
    "axes[1, 1].set_xlabel('|Traditional MRR - MTRR|')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Distribution of MRR Differences')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "summary_stats = llm_results.groupby('reranker').agg({\n",
    "    'tie_ratio': ['mean', 'std'],\n",
    "    'traditional_mrr': ['mean', 'std'],\n",
    "    'mtrr': ['mean', 'std'],\n",
    "    'mrr_diff': ['mean', 'max'],\n",
    "    'traditional_hits5': ['mean', 'std'],\n",
    "    'tmhits5': ['mean', 'std'],\n",
    "    'hits5_diff': ['mean', 'max']\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nðŸ“Š LLM Reranker Summary Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Implementation Guidelines & Best Practices\n",
    "\n",
    "### When to Use Tied Metrics\n",
    "\n",
    "1. **LLM-based rerankers**: When using large language models for document reranking\n",
    "2. **Discrete scoring systems**: When rerankers produce scores from a limited set of values\n",
    "3. **Model comparison**: When comparing different reranking approaches with varying tie patterns\n",
    "4. **Production systems**: When evaluation consistency is crucial for system monitoring\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "1. **Computational complexity**: Tied metrics can be computationally expensive for large tie groups\n",
    "2. **Approximation methods**: For very large tie groups, consider sampling-based approximations\n",
    "3. **Reporting**: Always report both traditional and tied metrics for comprehensive evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedTiedMetrics:\n",
    "    \"\"\"Optimized implementation for large-scale evaluation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_mtrr_efficient(result: RankingResult, max_permutations: int = 10000) -> float:\n",
    "        \"\"\"Compute MTRR with optimization for large tie groups\"\"\"\n",
    "        tied_groups = result.get_tied_groups()\n",
    "        \n",
    "        # Calculate total permutations\n",
    "        total_perms = 1\n",
    "        for group in tied_groups:\n",
    "            if len(group) > 1:\n",
    "                total_perms *= np.math.factorial(len(group))\n",
    "        \n",
    "        if total_perms <= max_permutations:\n",
    "            # Use exact computation\n",
    "            return TiedRankingMetrics.compute_mtrr(result)\n",
    "        else:\n",
    "            # Use sampling approximation\n",
    "            return OptimizedTiedMetrics._compute_mtrr_sampling(\n",
    "                result, tied_groups, max_permutations\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_mtrr_sampling(result: RankingResult, tied_groups: List[List[int]], \n",
    "                              n_samples: int) -> float:\n",
    "        \"\"\"Approximate MTRR using random sampling of permutations\"\"\"\n",
    "        total_rr = 0.0\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Generate random ranking\n",
    "            ranking = []\n",
    "            for group in tied_groups:\n",
    "                if len(group) > 1:\n",
    "                    # Random permutation of tied group\n",
    "                    shuffled_group = list(group)\n",
    "                    np.random.shuffle(shuffled_group)\n",
    "                    ranking.extend(shuffled_group)\n",
    "                else:\n",
    "                    ranking.extend(group)\n",
    "            \n",
    "            # Compute MRR for this ranking\n",
    "            total_rr += TiedRankingMetrics.compute_mrr_single(ranking, result.relevance)\n",
    "        \n",
    "        return total_rr / n_samples\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_computational_complexity(tied_groups: List[List[int]]) -> Dict:\n",
    "        \"\"\"Analyze computational complexity of tied ranking evaluation\"\"\"\n",
    "        total_perms = 1\n",
    "        tie_sizes = []\n",
    "        \n",
    "        for group in tied_groups:\n",
    "            if len(group) > 1:\n",
    "                tie_sizes.append(len(group))\n",
    "                total_perms *= np.math.factorial(len(group))\n",
    "        \n",
    "        return {\n",
    "            'total_permutations': total_perms,\n",
    "            'num_tie_groups': len(tie_sizes),\n",
    "            'tie_sizes': tie_sizes,\n",
    "            'max_tie_size': max(tie_sizes) if tie_sizes else 0,\n",
    "            'is_tractable': total_perms <= 10000,\n",
    "            'recommended_sampling': max(1000, min(10000, total_perms // 10))\n",
    "        }\n",
    "\n",
    "# Test computational efficiency\n",
    "def test_computational_efficiency():\n",
    "    \"\"\"Test computational efficiency of tied metrics\"\"\"\n",
    "    test_cases = [\n",
    "        {\"name\": \"Small ties\", \"tie_sizes\": [2, 2]},\n",
    "        {\"name\": \"Medium ties\", \"tie_sizes\": [3, 3, 2]},\n",
    "        {\"name\": \"Large tie\", \"tie_sizes\": [5]},\n",
    "        {\"name\": \"Very large tie\", \"tie_sizes\": [8]},\n",
    "        {\"name\": \"Multiple large ties\", \"tie_sizes\": [4, 4, 3]}\n",
    "    ]\n",
    "    \n",
    "    print(\"âš¡ Computational Complexity Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for case in test_cases:\n",
    "        # Create mock tied groups\n",
    "        tied_groups = []\n",
    "        current_idx = 0\n",
    "        \n",
    "        for tie_size in case[\"tie_sizes\"]:\n",
    "            tied_groups.append(list(range(current_idx, current_idx + tie_size)))\n",
    "            current_idx += tie_size\n",
    "        \n",
    "        # Add remaining single documents\n",
    "        for i in range(current_idx, current_idx + 3):\n",
    "            tied_groups.append([i])\n",
    "        \n",
    "        complexity = OptimizedTiedMetrics.analyze_computational_complexity(tied_groups)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š {case['name']}:\")\n",
    "        print(f\"   Tie sizes: {case['tie_sizes']}\")\n",
    "        print(f\"   Total permutations: {complexity['total_permutations']:,}\")\n",
    "        print(f\"   Tractable: {'âœ…' if complexity['is_tractable'] else 'âŒ'}\")\n",
    "        if not complexity['is_tractable']:\n",
    "            print(f\"   Recommended sampling: {complexity['recommended_sampling']:,}\")\n",
    "\n",
    "test_computational_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Practical Implementation for G-RAG\n",
    "\n",
    "Let's implement a complete evaluation framework that integrates tied metrics into the G-RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRAGEvaluator:\n",
    "    \"\"\"Complete evaluation framework for G-RAG with tied metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, use_tied_metrics: bool = True, max_permutations: int = 10000):\n",
    "        self.use_tied_metrics = use_tied_metrics\n",
    "        self.max_permutations = max_permutations\n",
    "        self.tied_metrics = TiedRankingMetrics()\n",
    "        self.optimized_metrics = OptimizedTiedMetrics()\n",
    "    \n",
    "    def evaluate_single_query(self, ranking_result: RankingResult) -> Dict:\n",
    "        \"\"\"Evaluate a single query ranking\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Traditional metrics\n",
    "        results['traditional_mrr'] = self.tied_metrics.compute_traditional_mrr(ranking_result)\n",
    "        results['traditional_hits_1'] = self.tied_metrics.compute_traditional_hits_k(ranking_result, 1)\n",
    "        results['traditional_hits_3'] = self.tied_metrics.compute_traditional_hits_k(ranking_result, 3)\n",
    "        results['traditional_hits_5'] = self.tied_metrics.compute_traditional_hits_k(ranking_result, 5)\n",
    "        results['traditional_hits_10'] = self.tied_metrics.compute_traditional_hits_k(ranking_result, 10)\n",
    "        \n",
    "        if self.use_tied_metrics:\n",
    "            # Tied metrics\n",
    "            results['mtrr'] = self.optimized_metrics.compute_mtrr_efficient(\n",
    "                ranking_result, self.max_permutations\n",
    "            )\n",
    "            results['tmhits_1'] = self.tied_metrics.compute_tmhits_k(ranking_result, 1)\n",
    "            results['tmhits_3'] = self.tied_metrics.compute_tmhits_k(ranking_result, 3)\n",
    "            results['tmhits_5'] = self.tied_metrics.compute_tmhits_k(ranking_result, 5)\n",
    "            results['tmhits_10'] = self.tied_metrics.compute_tmhits_k(ranking_result, 10)\n",
    "            \n",
    "            # Tie analysis\n",
    "            tied_groups = ranking_result.get_tied_groups()\n",
    "            complexity = self.optimized_metrics.analyze_computational_complexity(tied_groups)\n",
    "            \n",
    "            results['num_tie_groups'] = complexity['num_tie_groups']\n",
    "            results['max_tie_size'] = complexity['max_tie_size']\n",
    "            results['total_permutations'] = complexity['total_permutations']\n",
    "            results['is_exact_computation'] = complexity['is_tractable']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_query_set(self, ranking_results: List[RankingResult]) -> Dict:\n",
    "        \"\"\"Evaluate a set of query rankings\"\"\"\n",
    "        individual_results = []\n",
    "        \n",
    "        for result in ranking_results:\n",
    "            individual_results.append(self.evaluate_single_query(result))\n",
    "        \n",
    "        # Aggregate results\n",
    "        aggregated = {}\n",
    "        \n",
    "        # Calculate means for all metrics\n",
    "        for key in individual_results[0].keys():\n",
    "            if isinstance(individual_results[0][key], (int, float)):\n",
    "                values = [r[key] for r in individual_results]\n",
    "                aggregated[f'mean_{key}'] = np.mean(values)\n",
    "                aggregated[f'std_{key}'] = np.std(values)\n",
    "        \n",
    "        # Add summary statistics\n",
    "        aggregated['num_queries'] = len(ranking_results)\n",
    "        \n",
    "        if self.use_tied_metrics:\n",
    "            # Percentage of queries with ties\n",
    "            queries_with_ties = sum(1 for r in individual_results if r['num_tie_groups'] > 0)\n",
    "            aggregated['queries_with_ties_pct'] = queries_with_ties / len(ranking_results) * 100\n",
    "            \n",
    "            # Average differences between traditional and tied metrics\n",
    "            mrr_diffs = [abs(r['traditional_mrr'] - r['mtrr']) for r in individual_results]\n",
    "            aggregated['mean_mrr_difference'] = np.mean(mrr_diffs)\n",
    "            aggregated['max_mrr_difference'] = np.max(mrr_diffs)\n",
    "        \n",
    "        return aggregated, individual_results\n",
    "    \n",
    "    def generate_evaluation_report(self, ranking_results: List[RankingResult], \n",
    "                                 model_name: str = \"G-RAG\") -> str:\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        aggregated, individual = self.evaluate_query_set(ranking_results)\n",
    "        \n",
    "        report = f\"\"\"\n",
    "ðŸŽ¯ G-RAG Evaluation Report: {model_name}\n",
    "{'=' * 60}\n",
    "\n",
    "ðŸ“Š Dataset Summary:\n",
    "   Number of queries: {aggregated['num_queries']}\n",
    "\"\"\"\n",
    "        \n",
    "        if self.use_tied_metrics:\n",
    "            report += f\"\"\"\n",
    "   Queries with ties: {aggregated['queries_with_ties_pct']:.1f}%\n",
    "   Avg tie groups per query: {aggregated['mean_num_tie_groups']:.1f}\n",
    "   Max tie size observed: {aggregated['mean_max_tie_size']:.1f}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "\n",
    "ðŸ“ˆ Traditional Metrics:\n",
    "   MRR: {aggregated['mean_traditional_mrr']:.4f} (Â±{aggregated['std_traditional_mrr']:.4f})\n",
    "   Hits@1: {aggregated['mean_traditional_hits_1']:.4f} (Â±{aggregated['std_traditional_hits_1']:.4f})\n",
    "   Hits@3: {aggregated['mean_traditional_hits_3']:.4f} (Â±{aggregated['std_traditional_hits_3']:.4f})\n",
    "   Hits@5: {aggregated['mean_traditional_hits_5']:.4f} (Â±{aggregated['std_traditional_hits_5']:.4f})\n",
    "   Hits@10: {aggregated['mean_traditional_hits_10']:.4f} (Â±{aggregated['std_traditional_hits_10']:.4f})\n",
    "\"\"\"\n",
    "        \n",
    "        if self.use_tied_metrics:\n",
    "            report += f\"\"\"\n",
    "\n",
    "ðŸ”„ Tied Metrics (Fair Evaluation):\n",
    "   MTRR: {aggregated['mean_mtrr']:.4f} (Â±{aggregated['std_mtrr']:.4f})\n",
    "   TMHits@1: {aggregated['mean_tmhits_1']:.4f} (Â±{aggregated['std_tmhits_1']:.4f})\n",
    "   TMHits@3: {aggregated['mean_tmhits_3']:.4f} (Â±{aggregated['std_tmhits_3']:.4f})\n",
    "   TMHits@5: {aggregated['mean_tmhits_5']:.4f} (Â±{aggregated['std_tmhits_5']:.4f})\n",
    "   TMHits@10: {aggregated['mean_tmhits_10']:.4f} (Â±{aggregated['std_tmhits_10']:.4f})\n",
    "\n",
    "ðŸ” Metric Stability Analysis:\n",
    "   Mean |MRR - MTRR|: {aggregated['mean_mrr_difference']:.4f}\n",
    "   Max |MRR - MTRR|: {aggregated['max_mrr_difference']:.4f}\n",
    "   \n",
    "âœ… Recommendation: {'Use tied metrics for fair comparison' if aggregated['queries_with_ties_pct'] > 10 else 'Traditional metrics sufficient'}\n",
    "\"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Demonstrate with G-RAG evaluation\n",
    "def demo_grag_evaluation():\n",
    "    \"\"\"Demonstrate G-RAG evaluation with tied metrics\"\"\"\n",
    "    \n",
    "    # Create synthetic G-RAG results (simulating different reranker behaviors)\n",
    "    grag_scenarios = []\n",
    "    \n",
    "    # Simulate G-RAG results on various query types\n",
    "    query_types = [\n",
    "        {\"name\": \"Simple factual\", \"tie_prob\": 0.2, \"performance\": 0.8},\n",
    "        {\"name\": \"Complex reasoning\", \"tie_prob\": 0.4, \"performance\": 0.6},\n",
    "        {\"name\": \"Multi-hop\", \"tie_prob\": 0.5, \"performance\": 0.7},\n",
    "        {\"name\": \"Ambiguous\", \"tie_prob\": 0.6, \"performance\": 0.5}\n",
    "    ]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for query_type in query_types:\n",
    "        for _ in range(25):  # 25 queries per type\n",
    "            n_docs = np.random.randint(10, 20)\n",
    "            n_relevant = np.random.randint(1, 4)\n",
    "            \n",
    "            # Simulate G-RAG scores with ties\n",
    "            scores = np.random.uniform(0.1, 1.0, n_docs)\n",
    "            \n",
    "            # Boost relevant documents based on performance\n",
    "            relevance = [False] * n_docs\n",
    "            relevant_indices = np.random.choice(n_docs, size=n_relevant, replace=False)\n",
    "            for idx in relevant_indices:\n",
    "                relevance[idx] = True\n",
    "                if np.random.random() < query_type[\"performance\"]:\n",
    "                    scores[idx] += np.random.uniform(0.2, 0.4)\n",
    "            \n",
    "            # Introduce ties\n",
    "            if np.random.random() < query_type[\"tie_prob\"]:\n",
    "                # Create some tied scores\n",
    "                tie_value = np.random.choice(scores)\n",
    "                n_tied = np.random.randint(2, min(5, n_docs))\n",
    "                tied_indices = np.random.choice(n_docs, size=n_tied, replace=False)\n",
    "                for idx in tied_indices:\n",
    "                    scores[idx] = tie_value\n",
    "            \n",
    "            scores = np.clip(scores, 0, 1)\n",
    "            doc_ids = [f\"doc_{i}\" for i in range(n_docs)]\n",
    "            \n",
    "            grag_scenarios.append(RankingResult(\n",
    "                doc_ids=doc_ids,\n",
    "                scores=scores.tolist(),\n",
    "                relevance=relevance\n",
    "            ))\n",
    "    \n",
    "    # Evaluate with tied metrics\n",
    "    evaluator = GRAGEvaluator(use_tied_metrics=True)\n",
    "    report = evaluator.generate_evaluation_report(grag_scenarios, \"G-RAG\")\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    return grag_scenarios, evaluator\n",
    "\n",
    "# Run demonstration\n",
    "grag_scenarios, evaluator = demo_grag_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Insights and Takeaways\n",
    "\n",
    "### 1. **The Tied Ranking Problem**\n",
    "- LLM-based rerankers frequently produce tied scores\n",
    "- Traditional metrics give inconsistent results depending on tie-breaking\n",
    "- This inconsistency makes model comparison unreliable\n",
    "\n",
    "### 2. **Mathematical Foundation**\n",
    "- **MTRR**: Averages reciprocal ranks across all possible tie arrangements\n",
    "- **TMHits@K**: Extends Hits@K to handle tied rankings fairly\n",
    "- Both metrics provide stable, tie-breaking-independent evaluation\n",
    "\n",
    "### 3. **Computational Considerations**\n",
    "- Exact computation becomes intractable for large tie groups\n",
    "- Sampling-based approximation maintains accuracy while reducing cost\n",
    "- Trade-off between precision and computational efficiency\n",
    "\n",
    "### 4. **Practical Impact**\n",
    "- Tied metrics provide fairer comparison between different LLM rerankers\n",
    "- Essential for production systems where consistent evaluation is crucial\n",
    "- Help identify which models are truly better vs. appearing better due to tie-breaking\n",
    "\n",
    "### 5. **Integration with G-RAG**\n",
    "- Tied metrics complement the G-RAG architecture\n",
    "- Enable fair evaluation of the graph-based reranking approach\n",
    "- Support better model selection and hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Research Extensions\n",
    "\n",
    "1. **Adaptive Tie Handling**: Develop metrics that weight ties based on confidence scores\n",
    "2. **Hierarchical Ties**: Handle nested tie structures in complex ranking scenarios\n",
    "3. **Multi-objective Ties**: Extend to scenarios with multiple ranking criteria\n",
    "4. **Online Evaluation**: Adapt tied metrics for streaming/online evaluation settings\n",
    "\n",
    "## ðŸŽ¯ Conclusion\n",
    "\n",
    "The introduction of **Mean Tied Reciprocal Rank (MTRR)** and **Tied Mean Hits@K** represents a significant advancement in fair evaluation of retrieval systems, particularly those using LLMs as rerankers. These metrics:\n",
    "\n",
    "- âœ… **Eliminate tie-breaking bias** in evaluation\n",
    "- âœ… **Enable consistent model comparison** across different systems\n",
    "- âœ… **Provide theoretical foundation** for fair ranking evaluation\n",
    "- âœ… **Support practical implementation** with computational optimizations\n",
    "\n",
    "For the G-RAG system and other modern retrieval-augmented generation approaches, tied metrics are essential for accurate performance assessment and meaningful progress measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Focused Learning 4: Tied Ranking Metrics - Complete!\")\n",
    "print(\"ðŸ“š Key concepts covered:\")\n",
    "print(\"   âœ… Mean Tied Reciprocal Rank (MTRR)\")\n",
    "print(\"   âœ… Tied Mean Hits@K (TMHits@K)\")\n",
    "print(\"   âœ… Computational optimization strategies\")\n",
    "print(\"   âœ… LLM reranker evaluation\")\n",
    "print(\"   âœ… G-RAG integration framework\")\n",
    "print(\"\\nðŸš€ Ready to apply fair evaluation metrics in your RAG systems!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}