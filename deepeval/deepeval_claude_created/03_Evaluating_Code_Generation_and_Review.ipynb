{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíª DeepEval - Evaluating Code Generation and Review\n",
    "\n",
    "Ch√†o m·ª´ng ƒë·∫øn v·ªõi **Notebook 3** trong series DeepEval framework!\n",
    "\n",
    "## üéØ M·ª•c ti√™u c·ªßa Notebook n√†y\n",
    "\n",
    "1. **Custom Metrics v·ªõi G-Eval**: T·∫°o evaluation criteria t√πy ch·ªânh cho code\n",
    "2. **Code Generation Metrics**: Correctness, Readability, Efficiency\n",
    "3. **Code Review Metrics**: Completeness, Security, Best Practices\n",
    "4. **Automated Code Review**: End-to-end evaluation pipeline\n",
    "5. **Security Assessment**: Ph√°t hi·ªán vulnerabilities trong code\n",
    "\n",
    "## üìñ T·∫°i sao Code Evaluation quan tr·ªçng?\n",
    "\n",
    "Code generation ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng ·ª©ng d·ª•ng quan tr·ªçng nh·∫•t c·ªßa LLM, nh∆∞ng vi·ªác ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng code t·ª± ƒë·ªông l√† c·ª±c k·ª≥ th√°ch th·ª©c:\n",
    "\n",
    "### üîç Th√°ch th·ª©c c·ªßa Code Evaluation:\n",
    "- **Functional Correctness**: Code c√≥ ch·∫°y ƒë√∫ng kh√¥ng?\n",
    "- **Code Quality**: Readable, maintainable, efficient\n",
    "- **Security**: C√≥ vulnerabilities kh√¥ng?\n",
    "- **Best Practices**: Follow coding standards\n",
    "- **Context Awareness**: Code ph√π h·ª£p v·ªõi requirements\n",
    "\n",
    "### ‚úÖ DeepEval gi·∫£i quy·∫øt nh∆∞ th·∫ø n√†o:\n",
    "- **G-Eval Framework**: Custom metrics v·ªõi LLM-based evaluation\n",
    "- **Multi-dimensional Assessment**: ƒê√°nh gi√° nhi·ªÅu aspects c√πng l√∫c\n",
    "- **Automated Code Review**: Scale code review process\n",
    "- **Security-focused Metrics**: Specialized security evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Ph·∫ßn 1: Setup v√† Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DeepEval imports\n",
    "import deepeval\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.metrics.utils import trimAndLoadJson\n",
    "\n",
    "# Code analysis libraries\n",
    "import ast\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "\n",
    "print(f\"‚úÖ DeepEval version: {deepeval.__version__}\")\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Check API keys\n",
    "api_keys_status = {\n",
    "    \"OpenAI\": \"‚úÖ Configured\" if os.getenv(\"OPENAI_API_KEY\") else \"‚ùå Missing\",\n",
    "    \"Anthropic\": \"‚úÖ Configured\" if os.getenv(\"ANTHROPIC_API_KEY\") else \"‚ùå Missing\"\n",
    "}\n",
    "\n",
    "print(\"üîë API Keys Status:\")\n",
    "for provider, status in api_keys_status.items():\n",
    "    print(f\"  {provider}: {status}\")\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\n‚ö†Ô∏è  C·∫ßn OPENAI_API_KEY ƒë·ªÉ ch·∫°y code evaluation!\")\n",
    "    print(\"   T·∫°o file .env v·ªõi: OPENAI_API_KEY=your_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Ph·∫ßn 2: Load Code Samples Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code_samples():\n",
    "    \"\"\"\n",
    "    Load code samples t·ª´ data folder\n",
    "    \"\"\"\n",
    "    \n",
    "    code_samples_path = \"data/code_samples.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(code_samples_path, 'r', encoding='utf-8') as f:\n",
    "            code_data = json.load(f)\n",
    "        \n",
    "        print(f\"üìÑ Loaded code samples data\")\n",
    "        print(f\"  Code Generation Problems: {len(code_data.get('code_generation_problems', []))}\")\n",
    "        print(f\"  Buggy Code Samples: {len(code_data.get('buggy_code_samples', []))}\")\n",
    "        print(f\"  Code Review Scenarios: {len(code_data.get('code_review_scenarios', []))}\")\n",
    "        \n",
    "        return code_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File kh√¥ng t√¨m th·∫•y: {code_samples_path}\")\n",
    "        print(\"üí° ƒê·∫£m b·∫£o ƒë√£ ch·∫°y notebook trong ƒë√∫ng directory\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading code samples: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load code samples\n",
    "code_data = load_code_samples()\n",
    "\n",
    "# Preview data structure\n",
    "if code_data:\n",
    "    print(\"\\nüîç Preview Data Structure:\")\n",
    "    for category, items in code_data.items():\n",
    "        if items and len(items) > 0:\n",
    "            print(f\"\\n{category}:\")\n",
    "            first_item = items[0]\n",
    "            for key in first_item.keys():\n",
    "                print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ph·∫ßn 3: Custom Metrics v·ªõi G-Eval\n",
    "\n",
    "### 3.1 Hi·ªÉu v·ªÅ G-Eval Framework\n",
    "\n",
    "G-Eval l√† m·ªôt framework m·∫°nh m·∫Ω cho ph√©p t·∫°o custom evaluation metrics s·ª≠ d·ª•ng LLM. Thay v√¨ d·ª±a v√†o predefined metrics, G-Eval cho ph√©p ƒë·ªãnh nghƒ©a evaluation criteria b·∫±ng natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_code_correctness_metric():\n",
    "    \"\"\"\n",
    "    T·∫°o G-Eval metric ƒë·ªÉ ƒë√°nh gi√° t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa code\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define evaluation criteria\n",
    "    evaluation_criteria = \"\"\"\n",
    "    B·∫°n s·∫Ω ƒë√°nh gi√° t√≠nh ƒë√∫ng ƒë·∫Øn (correctness) c·ªßa code Python ƒë∆∞·ª£c generate.\n",
    "    \n",
    "    Ti√™u ch√≠ ƒë√°nh gi√°:\n",
    "    1. LOGIC CORRECTNESS (40%):\n",
    "       - Code implement ƒë√∫ng algorithm/logic ƒë∆∞·ª£c y√™u c·∫ßu\n",
    "       - Handle c√°c edge cases appropriately\n",
    "       - Kh√¥ng c√≥ logical errors\n",
    "    \n",
    "    2. SYNTAX & RUNTIME (30%):\n",
    "       - Code syntactically correct (kh√¥ng syntax errors)\n",
    "       - Kh√¥ng c√≥ runtime errors v·ªõi valid inputs\n",
    "       - Proper variable declarations v√† scope\n",
    "    \n",
    "    3. FUNCTIONAL REQUIREMENTS (30%):\n",
    "       - Output matches expected results\n",
    "       - Function signature ƒë√∫ng nh∆∞ y√™u c·∫ßu\n",
    "       - Handle input validation appropriately\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Ho√†n to√†n ƒë√∫ng, handle t·∫•t c·∫£ cases\n",
    "    - 7-8: Mostly correct, minor issues\n",
    "    - 5-6: C√≥ m·ªôt s·ªë l·ªói logic ho·∫∑c edge cases\n",
    "    - 3-4: Major logic errors ho·∫∑c nhi·ªÅu bugs\n",
    "    - 1-2: Fundamentally incorrect ho·∫∑c kh√¥ng ch·∫°y ƒë∆∞·ª£c\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define evaluation steps\n",
    "    evaluation_steps = [\n",
    "        \"Ph√¢n t√≠ch problem requirements v√† expected functionality\",\n",
    "        \"Ki·ªÉm tra syntax correctness v√† potential runtime errors\",\n",
    "        \"Trace through algorithm logic v·ªõi sample inputs\",\n",
    "        \"Identify potential edge cases v√† c√°ch code handle ch√∫ng\",\n",
    "        \"Assess overall correctness v√† assign score\"\n",
    "    ]\n",
    "    \n",
    "    # Create G-Eval metric\n",
    "    correctness_metric = GEval(\n",
    "        name=\"Code Correctness\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Problem description\n",
    "            LLMTestCase.actual_output,  # Generated code\n",
    "            LLMTestCase.expected_output  # Expected solution (if available)\n",
    "        ],\n",
    "        threshold=7.0,  # Threshold for pass/fail\n",
    "        model=\"gpt-4\",  # Use GPT-4 for better code understanding\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return correctness_metric\n",
    "\n",
    "# Create correctness metric\n",
    "code_correctness_metric = create_code_correctness_metric()\n",
    "print(\"‚úÖ Code Correctness Metric created\")\n",
    "print(f\"Name: {code_correctness_metric.name}\")\n",
    "print(f\"Threshold: {code_correctness_metric.threshold}\")\n",
    "print(f\"Model: {code_correctness_metric.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Code Readability Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_code_readability_metric():\n",
    "    \"\"\"\n",
    "    T·∫°o G-Eval metric ƒë·ªÉ ƒë√°nh gi√° readability c·ªßa code\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    B·∫°n s·∫Ω ƒë√°nh gi√° ƒë·ªô d·ªÖ ƒë·ªçc (readability) c·ªßa Python code.\n",
    "    \n",
    "    Ti√™u ch√≠ ƒë√°nh gi√°:\n",
    "    1. NAMING & CLARITY (35%):\n",
    "       - Variable names descriptive v√† meaningful\n",
    "       - Function names clear v√† verb-based\n",
    "       - Avoid abbreviations v√† cryptic names\n",
    "    \n",
    "    2. CODE STRUCTURE (25%):\n",
    "       - Proper indentation v√† formatting\n",
    "       - Logical organization c·ªßa code blocks\n",
    "       - Appropriate use of whitespace\n",
    "    \n",
    "    3. COMMENTS & DOCUMENTATION (25%):\n",
    "       - Docstrings cho functions\n",
    "       - Inline comments cho complex logic\n",
    "       - Clear explanation of algorithm steps\n",
    "    \n",
    "    4. SIMPLICITY & ELEGANCE (15%):\n",
    "       - Code concise nh∆∞ng kh√¥ng cryptic\n",
    "       - Avoid unnecessary complexity\n",
    "       - Use appropriate Python idioms\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Exceptionally readable, professional quality\n",
    "    - 7-8: Well-written, easy to understand\n",
    "    - 5-6: Acceptable but c√≥ th·ªÉ improve readability\n",
    "    - 3-4: Hard to read, poor naming/structure\n",
    "    - 1-2: Very difficult to understand\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Ki·ªÉm tra naming conventions c·ªßa variables v√† functions\",\n",
    "        \"Assess code structure, indentation, v√† formatting\",\n",
    "        \"Review comments, docstrings, v√† documentation quality\",\n",
    "        \"Evaluate overall clarity v√† ease of understanding\",\n",
    "        \"Assign readability score based on criteria\"\n",
    "    ]\n",
    "    \n",
    "    readability_metric = GEval(\n",
    "        name=\"Code Readability\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,\n",
    "            LLMTestCase.actual_output\n",
    "        ],\n",
    "        threshold=6.0,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return readability_metric\n",
    "\n",
    "# Create readability metric\n",
    "code_readability_metric = create_code_readability_metric()\n",
    "print(\"‚úÖ Code Readability Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Code Efficiency Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_code_efficiency_metric():\n",
    "    \"\"\"\n",
    "    T·∫°o G-Eval metric ƒë·ªÉ ƒë√°nh gi√° efficiency c·ªßa code\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    B·∫°n s·∫Ω ƒë√°nh gi√° hi·ªáu su·∫•t (efficiency) c·ªßa Python code v·ªÅ time v√† space complexity.\n",
    "    \n",
    "    Ti√™u ch√≠ ƒë√°nh gi√°:\n",
    "    1. TIME COMPLEXITY (40%):\n",
    "       - Algorithm c√≥ optimal time complexity kh√¥ng?\n",
    "       - Avoid unnecessary nested loops\n",
    "       - Use efficient data structures (dict vs list lookup)\n",
    "    \n",
    "    2. SPACE COMPLEXITY (30%):\n",
    "       - Minimize memory usage\n",
    "       - Avoid unnecessary data copies\n",
    "       - Consider in-place operations khi possible\n",
    "    \n",
    "    3. ALGORITHMIC APPROACH (20%):\n",
    "       - Choose appropriate algorithm cho problem\n",
    "       - Consider trade-offs between time v√† space\n",
    "       - Use built-in optimized functions\n",
    "    \n",
    "    4. IMPLEMENTATION EFFICIENCY (10%):\n",
    "       - Avoid redundant calculations\n",
    "       - Minimize function call overhead\n",
    "       - Early termination conditions\n",
    "    \n",
    "    Scoring Guide:\n",
    "    - 9-10: Optimal complexity, highly efficient\n",
    "    - 7-8: Good efficiency, minor optimizations possible\n",
    "    - 5-6: Acceptable but not optimal\n",
    "    - 3-4: Inefficient approach, needs optimization\n",
    "    - 1-2: Very poor efficiency, major issues\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Analyze algorithm approach v√† complexity\",\n",
    "        \"Calculate time complexity (Big O notation)\",\n",
    "        \"Assess space complexity v√† memory usage\",\n",
    "        \"Identify potential optimizations\",\n",
    "        \"Compare v·ªõi alternative approaches n·∫øu c√≥\",\n",
    "        \"Assign efficiency score\"\n",
    "    ]\n",
    "    \n",
    "    efficiency_metric = GEval(\n",
    "        name=\"Code Efficiency\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,\n",
    "            LLMTestCase.actual_output\n",
    "        ],\n",
    "        threshold=6.0,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return efficiency_metric\n",
    "\n",
    "# Create efficiency metric\n",
    "code_efficiency_metric = create_code_efficiency_metric()\n",
    "print(\"‚úÖ Code Efficiency Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Ph·∫ßn 4: Test Code Generation Metrics\n",
    "\n",
    "### 4.1 Evaluate Bubble Sort Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bubble_sort_evaluation():\n",
    "    \"\"\"\n",
    "    Test code generation metrics v·ªõi bubble sort example\n",
    "    \"\"\"\n",
    "    \n",
    "    if not code_data or 'code_generation_problems' not in code_data:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ code generation data ƒë·ªÉ test\")\n",
    "        return\n",
    "    \n",
    "    # L·∫•y bubble sort problem\n",
    "    bubble_sort_problem = None\n",
    "    for problem in code_data['code_generation_problems']:\n",
    "        if problem['id'] == 'bubble_sort':\n",
    "            bubble_sort_problem = problem\n",
    "            break\n",
    "    \n",
    "    if not bubble_sort_problem:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y bubble sort problem\")\n",
    "        return\n",
    "    \n",
    "    print(\"üß™ Testing Code Generation Metrics v·ªõi Bubble Sort\")\n",
    "    print(f\"Problem: {bubble_sort_problem['problem'][:100]}...\")\n",
    "    \n",
    "    # T·∫°o test case\n",
    "    test_case = LLMTestCase(\n",
    "        input=bubble_sort_problem['problem'],\n",
    "        actual_output=bubble_sort_problem['correct_solution'],\n",
    "        expected_output=bubble_sort_problem['correct_solution']  # Using same solution as reference\n",
    "    )\n",
    "    \n",
    "    # Test metrics\n",
    "    metrics = {\n",
    "        \"Correctness\": code_correctness_metric,\n",
    "        \"Readability\": code_readability_metric,\n",
    "        \"Efficiency\": code_efficiency_metric\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for metric_name, metric in metrics.items():\n",
    "        print(f\"\\nüîç Testing {metric_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create fresh metric instance ƒë·ªÉ avoid state conflicts\n",
    "            fresh_metric = type(metric)(\n",
    "                name=metric.name,\n",
    "                criteria=metric.criteria,\n",
    "                evaluation_steps=metric.evaluation_steps,\n",
    "                evaluation_params=metric.evaluation_params,\n",
    "                threshold=metric.threshold,\n",
    "                model=metric.model,\n",
    "                include_reason=True\n",
    "            )\n",
    "            \n",
    "            fresh_metric.measure(test_case)\n",
    "            \n",
    "            results[metric_name] = {\n",
    "                \"score\": fresh_metric.score,\n",
    "                \"passed\": fresh_metric.is_successful(),\n",
    "                \"reason\": fresh_metric.reason\n",
    "            }\n",
    "            \n",
    "            status = \"‚úÖ\" if fresh_metric.is_successful() else \"‚ùå\"\n",
    "            print(f\"  {status} Score: {fresh_metric.score:.1f}/10\")\n",
    "            print(f\"  Reason: {fresh_metric.reason[:150]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "            results[metric_name] = {\n",
    "                \"score\": 0,\n",
    "                \"passed\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    return results, test_case\n",
    "\n",
    "# Run bubble sort evaluation\n",
    "bubble_sort_results, bubble_sort_test_case = test_bubble_sort_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test v·ªõi Generated Code (Mock LLM Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generated_code_samples():\n",
    "    \"\"\"\n",
    "    Test metrics v·ªõi various quality levels c·ªßa generated code\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mock generated code samples v·ªõi different quality levels\n",
    "    generated_samples = [\n",
    "        {\n",
    "            \"name\": \"High Quality Implementation\",\n",
    "            \"problem\": \"Implement binary search algorithm\",\n",
    "            \"code\": '''def binary_search(arr, target):\n",
    "    \"\"\"\n",
    "    Perform binary search on a sorted array.\n",
    "    \n",
    "    Args:\n",
    "        arr: Sorted list of integers\n",
    "        target: Integer to search for\n",
    "    \n",
    "    Returns:\n",
    "        Index of target if found, -1 otherwise\n",
    "    \"\"\"\n",
    "    left, right = 0, len(arr) - 1\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return -1'''\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Poor Quality Implementation\", \n",
    "            \"problem\": \"Implement binary search algorithm\",\n",
    "            \"code\": '''def search(a, x):\n",
    "    # search for x\n",
    "    i=0\n",
    "    j=len(a)-1\n",
    "    while i<=j:\n",
    "        m=(i+j)/2  # Bug: should use // for integer division\n",
    "        if a[m]==x:\n",
    "            return m\n",
    "        if a[m]<x:\n",
    "            i=m+1\n",
    "        else:\n",
    "            j=m-1\n",
    "    return -1'''\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Inefficient Implementation\",\n",
    "            \"problem\": \"Find maximum element in array\",\n",
    "            \"code\": '''def find_max(arr):\n",
    "    # O(n^2) approach - very inefficient\n",
    "    max_val = arr[0]\n",
    "    for i in range(len(arr)):\n",
    "        is_max = True\n",
    "        for j in range(len(arr)):\n",
    "            if arr[j] > arr[i]:\n",
    "                is_max = False\n",
    "                break\n",
    "        if is_max:\n",
    "            max_val = arr[i]\n",
    "    return max_val'''\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ Testing Generated Code Samples v·ªõi Different Quality Levels\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for sample in generated_samples:\n",
    "        print(f\"üìù Testing: {sample['name']}\")\n",
    "        print(f\"Problem: {sample['problem']}\")\n",
    "        \n",
    "        # Create test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=sample['problem'],\n",
    "            actual_output=sample['code']\n",
    "        )\n",
    "        \n",
    "        # Test v·ªõi all metrics\n",
    "        sample_results = {\n",
    "            \"name\": sample['name'],\n",
    "            \"problem\": sample['problem'],\n",
    "            \"metrics\": {}\n",
    "        }\n",
    "        \n",
    "        metrics = {\n",
    "            \"Correctness\": code_correctness_metric,\n",
    "            \"Readability\": code_readability_metric,\n",
    "            \"Efficiency\": code_efficiency_metric\n",
    "        }\n",
    "        \n",
    "        for metric_name, metric in metrics.items():\n",
    "            try:\n",
    "                # Create fresh instance\n",
    "                fresh_metric = type(metric)(\n",
    "                    name=metric.name,\n",
    "                    criteria=metric.criteria,\n",
    "                    evaluation_steps=metric.evaluation_steps,\n",
    "                    evaluation_params=metric.evaluation_params,\n",
    "                    threshold=metric.threshold,\n",
    "                    model=metric.model,\n",
    "                    include_reason=True\n",
    "                )\n",
    "                \n",
    "                fresh_metric.measure(test_case)\n",
    "                \n",
    "                sample_results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": fresh_metric.score,\n",
    "                    \"passed\": fresh_metric.is_successful()\n",
    "                }\n",
    "                \n",
    "                status = \"‚úÖ\" if fresh_metric.is_successful() else \"‚ùå\"\n",
    "                print(f\"  {metric_name}: {status} {fresh_metric.score:.1f}/10\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {metric_name}: ‚ùå Error - {e}\")\n",
    "                sample_results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": 0,\n",
    "                    \"passed\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "        \n",
    "        all_results.append(sample_results)\n",
    "        print()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Test generated code samples\n",
    "generated_code_results = test_generated_code_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Ph·∫ßn 5: Security Review Metrics\n",
    "\n",
    "### 5.1 Security Vulnerability Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_security_review_metric():\n",
    "    \"\"\"\n",
    "    T·∫°o G-Eval metric ƒë·ªÉ detect security vulnerabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    B·∫°n s·∫Ω ƒë√°nh gi√° code Python ƒë·ªÉ identify security vulnerabilities v√† issues.\n",
    "    \n",
    "    Security Issues c·∫ßn check:\n",
    "    1. INJECTION VULNERABILITIES (30%):\n",
    "       - SQL injection (raw SQL queries)\n",
    "       - Command injection (subprocess, os.system)\n",
    "       - Code injection (eval, exec v·ªõi user input)\n",
    "    \n",
    "    2. INPUT VALIDATION (25%):\n",
    "       - Lack of input sanitization\n",
    "       - Missing bounds checking\n",
    "       - Improper data type validation\n",
    "    \n",
    "    3. AUTHENTICATION & AUTHORIZATION (20%):\n",
    "       - Hard-coded credentials\n",
    "       - Weak authentication mechanisms\n",
    "       - Missing access controls\n",
    "    \n",
    "    4. DATA EXPOSURE (15%):\n",
    "       - Sensitive data in logs\n",
    "       - Unencrypted sensitive data\n",
    "       - Information leakage in error messages\n",
    "    \n",
    "    5. OTHER SECURITY ISSUES (10%):\n",
    "       - Insecure random number generation\n",
    "       - Improper error handling\n",
    "       - Race conditions\n",
    "    \n",
    "    Scoring (Security Score - higher is better):\n",
    "    - 9-10: No security issues detected\n",
    "    - 7-8: Minor security concerns\n",
    "    - 5-6: Some security issues present\n",
    "    - 3-4: Multiple security vulnerabilities\n",
    "    - 1-2: Critical security flaws\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Scan code cho injection vulnerabilities (SQL, command, code)\",\n",
    "        \"Check input validation v√† sanitization\",\n",
    "        \"Look for hard-coded credentials ho·∫∑c secrets\",\n",
    "        \"Identify potential data exposure issues\",\n",
    "        \"Check error handling v√† information leakage\",\n",
    "        \"Assess overall security posture v√† assign score\"\n",
    "    ]\n",
    "    \n",
    "    security_metric = GEval(\n",
    "        name=\"Security Review\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.actual_output  # Code to review\n",
    "        ],\n",
    "        threshold=7.0,  # High threshold for security\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return security_metric\n",
    "\n",
    "# Create security metric\n",
    "security_review_metric = create_security_review_metric()\n",
    "print(\"‚úÖ Security Review Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Test Security Metrics v·ªõi Vulnerable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_security_vulnerabilities():\n",
    "    \"\"\"\n",
    "    Test security metrics v·ªõi code c√≥ vulnerabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    if not code_data or 'buggy_code_samples' not in code_data:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ buggy code data ƒë·ªÉ test security\")\n",
    "        return\n",
    "    \n",
    "    print(\"üõ°Ô∏è Testing Security Review Metrics\\n\")\n",
    "    \n",
    "    security_results = []\n",
    "    \n",
    "    # Test v·ªõi buggy code samples c√≥ security issues\n",
    "    for sample in code_data['buggy_code_samples']:\n",
    "        if sample.get('security_issues'):\n",
    "            print(f\"üîç Testing: {sample['description']}\")\n",
    "            print(f\"Expected Issues: {', '.join(sample['security_issues'])}\")\n",
    "            \n",
    "            # Create test case\n",
    "            test_case = LLMTestCase(\n",
    "                input=f\"Review this code for security vulnerabilities: {sample['description']}\",\n",
    "                actual_output=sample['buggy_code']\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Create fresh metric instance\n",
    "                fresh_security_metric = GEval(\n",
    "                    name=security_review_metric.name,\n",
    "                    criteria=security_review_metric.criteria,\n",
    "                    evaluation_steps=security_review_metric.evaluation_steps,\n",
    "                    evaluation_params=security_review_metric.evaluation_params,\n",
    "                    threshold=security_review_metric.threshold,\n",
    "                    model=security_review_metric.model,\n",
    "                    include_reason=True\n",
    "                )\n",
    "                \n",
    "                fresh_security_metric.measure(test_case)\n",
    "                \n",
    "                result = {\n",
    "                    \"description\": sample['description'],\n",
    "                    \"expected_issues\": sample['security_issues'],\n",
    "                    \"score\": fresh_security_metric.score,\n",
    "                    \"passed\": fresh_security_metric.is_successful(),\n",
    "                    \"reason\": fresh_security_metric.reason,\n",
    "                    \"detected_correctly\": fresh_security_metric.score < 7.0  # Low score = detected issues\n",
    "                }\n",
    "                \n",
    "                security_results.append(result)\n",
    "                \n",
    "                status = \"‚úÖ Detected\" if not fresh_security_metric.is_successful() else \"‚ùå Missed\"\n",
    "                print(f\"  {status} Security Score: {fresh_security_metric.score:.1f}/10\")\n",
    "                print(f\"  Analysis: {fresh_security_metric.reason[:200]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                security_results.append({\n",
    "                    \"description\": sample['description'],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    # Summary\n",
    "    if security_results:\n",
    "        detected_count = sum(1 for r in security_results if r.get('detected_correctly', False))\n",
    "        total_count = len([r for r in security_results if 'detected_correctly' in r])\n",
    "        \n",
    "        print(f\"üìä Security Detection Summary:\")\n",
    "        print(f\"  Vulnerabilities Detected: {detected_count}/{total_count}\")\n",
    "        print(f\"  Detection Rate: {detected_count/total_count*100:.1f}%\" if total_count > 0 else \"  Detection Rate: N/A\")\n",
    "    \n",
    "    return security_results\n",
    "\n",
    "# Test security vulnerabilities\n",
    "security_test_results = test_security_vulnerabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Ph·∫ßn 6: Code Review Automation\n",
    "\n",
    "### 6.1 Comprehensive Code Review Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_code_review_completeness_metric():\n",
    "    \"\"\"\n",
    "    T·∫°o metric ƒë·ªÉ ƒë√°nh gi√° completeness c·ªßa code review\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    B·∫°n s·∫Ω ƒë√°nh gi√° m·ª©c ƒë·ªô ho√†n thi·ªán (completeness) c·ªßa m·ªôt code review.\n",
    "    \n",
    "    Aspects c·∫ßn review:\n",
    "    1. FUNCTIONALITY REVIEW (25%):\n",
    "       - Logic correctness ƒë∆∞·ª£c check\n",
    "       - Edge cases ƒë∆∞·ª£c identify\n",
    "       - Error handling ƒë∆∞·ª£c evaluate\n",
    "    \n",
    "    2. CODE QUALITY REVIEW (25%):\n",
    "       - Readability v√† naming conventions\n",
    "       - Code structure v√† organization\n",
    "       - Documentation quality\n",
    "    \n",
    "    3. PERFORMANCE REVIEW (20%):\n",
    "       - Algorithm efficiency\n",
    "       - Time/space complexity analysis\n",
    "       - Optimization opportunities\n",
    "    \n",
    "    4. SECURITY REVIEW (20%):\n",
    "       - Security vulnerabilities check\n",
    "       - Input validation review\n",
    "       - Authentication/authorization issues\n",
    "    \n",
    "    5. BEST PRACTICES REVIEW (10%):\n",
    "       - Coding standards compliance\n",
    "       - Design patterns usage\n",
    "       - Maintainability considerations\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Comprehensive review covering all aspects\n",
    "    - 7-8: Good review but missing some areas\n",
    "    - 5-6: Basic review, several gaps\n",
    "    - 3-4: Incomplete review, major aspects missed\n",
    "    - 1-2: Very superficial or inadequate review\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Check if functionality v√† logic ƒë∆∞·ª£c thoroughly reviewed\",\n",
    "        \"Verify code quality aspects ƒë∆∞·ª£c addressed\",\n",
    "        \"Assess if performance considerations ƒë∆∞·ª£c discussed\",\n",
    "        \"Confirm security implications ƒë∆∞·ª£c reviewed\",\n",
    "        \"Evaluate coverage c·ªßa best practices\",\n",
    "        \"Assess overall completeness c·ªßa review\"\n",
    "    ]\n",
    "    \n",
    "    review_completeness_metric = GEval(\n",
    "        name=\"Code Review Completeness\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Original code\n",
    "            LLMTestCase.actual_output  # Review comments\n",
    "        ],\n",
    "        threshold=7.0,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return review_completeness_metric\n",
    "\n",
    "# Create code review metric\n",
    "code_review_completeness_metric = create_code_review_completeness_metric()\n",
    "print(\"‚úÖ Code Review Completeness Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Test Code Review Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_code_review_scenarios():\n",
    "    \"\"\"\n",
    "    Test code review metrics v·ªõi realistic review scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    if not code_data or 'code_review_scenarios' not in code_data:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ code review scenarios ƒë·ªÉ test\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìù Testing Code Review Scenarios\\n\")\n",
    "    \n",
    "    review_results = []\n",
    "    \n",
    "    for scenario in code_data['code_review_scenarios']:\n",
    "        print(f\"üîç Scenario: {scenario['title']}\")\n",
    "        \n",
    "        # Generate mock review comments based on review points\n",
    "        review_comments = []\n",
    "        for point in scenario['review_points']:\n",
    "            severity = point['severity'].upper()\n",
    "            comment = f\"[{severity}] {point['description']} - {point['suggestion']}\"\n",
    "            review_comments.append(comment)\n",
    "        \n",
    "        mock_review = \"\\n\".join(review_comments)\n",
    "        \n",
    "        print(f\"Code length: {len(scenario['code_to_review'])} characters\")\n",
    "        print(f\"Review points: {len(scenario['review_points'])}\")\n",
    "        \n",
    "        # Create test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=scenario['code_to_review'],\n",
    "            actual_output=mock_review\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Test review completeness\n",
    "            fresh_review_metric = GEval(\n",
    "                name=code_review_completeness_metric.name,\n",
    "                criteria=code_review_completeness_metric.criteria,\n",
    "                evaluation_steps=code_review_completeness_metric.evaluation_steps,\n",
    "                evaluation_params=code_review_completeness_metric.evaluation_params,\n",
    "                threshold=code_review_completeness_metric.threshold,\n",
    "                model=code_review_completeness_metric.model,\n",
    "                include_reason=True\n",
    "            )\n",
    "            \n",
    "            fresh_review_metric.measure(test_case)\n",
    "            \n",
    "            result = {\n",
    "                \"scenario\": scenario['title'],\n",
    "                \"review_points_count\": len(scenario['review_points']),\n",
    "                \"completeness_score\": fresh_review_metric.score,\n",
    "                \"passed\": fresh_review_metric.is_successful(),\n",
    "                \"reason\": fresh_review_metric.reason\n",
    "            }\n",
    "            \n",
    "            review_results.append(result)\n",
    "            \n",
    "            status = \"‚úÖ\" if fresh_review_metric.is_successful() else \"‚ùå\"\n",
    "            print(f\"  {status} Completeness Score: {fresh_review_metric.score:.1f}/10\")\n",
    "            print(f\"  Assessment: {fresh_review_metric.reason[:150]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "            review_results.append({\n",
    "                \"scenario\": scenario['title'],\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Analyze results\n",
    "    if review_results:\n",
    "        valid_results = [r for r in review_results if 'completeness_score' in r]\n",
    "        \n",
    "        if valid_results:\n",
    "            avg_score = np.mean([r['completeness_score'] for r in valid_results])\n",
    "            pass_rate = np.mean([r['passed'] for r in valid_results]) * 100\n",
    "            \n",
    "            print(f\"üìä Code Review Analysis Summary:\")\n",
    "            print(f\"  Scenarios Tested: {len(valid_results)}\")\n",
    "            print(f\"  Average Completeness Score: {avg_score:.1f}/10\")\n",
    "            print(f\"  Pass Rate: {pass_rate:.1f}%\")\n",
    "    \n",
    "    return review_results\n",
    "\n",
    "# Test code review scenarios\n",
    "review_scenario_results = test_code_review_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ph·∫ßn 7: Comprehensive Code Evaluation Pipeline\n",
    "\n",
    "### 7.1 End-to-End Code Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeEvaluationPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive pipeline cho code evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        self.model = model\n",
    "        self.evaluation_history = []\n",
    "        \n",
    "        # Initialize all metrics\n",
    "        self.metrics = {\n",
    "            \"correctness\": self._create_correctness_metric(),\n",
    "            \"readability\": self._create_readability_metric(),\n",
    "            \"efficiency\": self._create_efficiency_metric(),\n",
    "            \"security\": self._create_security_metric()\n",
    "        }\n",
    "    \n",
    "    def _create_correctness_metric(self):\n",
    "        return create_code_correctness_metric()\n",
    "    \n",
    "    def _create_readability_metric(self):\n",
    "        return create_code_readability_metric()\n",
    "    \n",
    "    def _create_efficiency_metric(self):\n",
    "        return create_code_efficiency_metric()\n",
    "    \n",
    "    def _create_security_metric(self):\n",
    "        return create_security_review_metric()\n",
    "    \n",
    "    def evaluate_code(self, problem_description: str, generated_code: str, \n",
    "                     expected_solution: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation c·ªßa generated code\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=problem_description,\n",
    "            actual_output=generated_code,\n",
    "            expected_output=expected_solution\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            \"problem\": problem_description,\n",
    "            \"code\": generated_code,\n",
    "            \"code_length\": len(generated_code),\n",
    "            \"metrics\": {},\n",
    "            \"overall_score\": 0,\n",
    "            \"pass_count\": 0,\n",
    "            \"total_metrics\": len(self.metrics)\n",
    "        }\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        metric_scores = []\n",
    "        \n",
    "        for metric_name, metric in self.metrics.items():\n",
    "            try:\n",
    "                # Create fresh metric instance\n",
    "                fresh_metric = type(metric)(\n",
    "                    name=metric.name,\n",
    "                    criteria=metric.criteria,\n",
    "                    evaluation_steps=metric.evaluation_steps,\n",
    "                    evaluation_params=metric.evaluation_params,\n",
    "                    threshold=metric.threshold,\n",
    "                    model=metric.model,\n",
    "                    include_reason=True\n",
    "                )\n",
    "                \n",
    "                fresh_metric.measure(test_case)\n",
    "                \n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": fresh_metric.score,\n",
    "                    \"passed\": fresh_metric.is_successful(),\n",
    "                    \"reason\": fresh_metric.reason,\n",
    "                    \"threshold\": fresh_metric.threshold\n",
    "                }\n",
    "                \n",
    "                metric_scores.append(fresh_metric.score)\n",
    "                if fresh_metric.is_successful():\n",
    "                    results[\"pass_count\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": 0,\n",
    "                    \"passed\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                metric_scores.append(0)\n",
    "        \n",
    "        # Calculate overall score\n",
    "        if metric_scores:\n",
    "            results[\"overall_score\"] = np.mean(metric_scores)\n",
    "        \n",
    "        # Store in history\n",
    "        self.evaluation_history.append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_evaluate(self, code_samples: List[Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Batch evaluation c·ªßa multiple code samples\n",
    "        \"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, sample in enumerate(code_samples):\n",
    "            print(f\"üîç Evaluating {i+1}/{len(code_samples)}: {sample.get('name', f'Sample {i+1}')}\")\n",
    "            \n",
    "            try:\n",
    "                result = self.evaluate_code(\n",
    "                    problem_description=sample.get('problem', 'Code evaluation'),\n",
    "                    generated_code=sample.get('code', ''),\n",
    "                    expected_solution=sample.get('expected_solution')\n",
    "                )\n",
    "                \n",
    "                result[\"sample_name\"] = sample.get('name', f'Sample {i+1}')\n",
    "                results.append(result)\n",
    "                \n",
    "                # Quick summary\n",
    "                print(f\"  ‚úÖ Overall Score: {result['overall_score']:.1f}/10\")\n",
    "                print(f\"  üìä Passed: {result['pass_count']}/{result['total_metrics']} metrics\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                results.append({\n",
    "                    \"sample_name\": sample.get('name', f'Sample {i+1}'),\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive performance summary\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.evaluation_history:\n",
    "            return {\"message\": \"No evaluations performed yet\"}\n",
    "        \n",
    "        # Collect metrics data\n",
    "        metric_summaries = {}\n",
    "        overall_scores = []\n",
    "        pass_rates = []\n",
    "        \n",
    "        for result in self.evaluation_history:\n",
    "            if \"overall_score\" in result:\n",
    "                overall_scores.append(result[\"overall_score\"])\n",
    "                pass_rates.append(result[\"pass_count\"] / result[\"total_metrics\"])\n",
    "            \n",
    "            if \"metrics\" in result:\n",
    "                for metric_name, metric_data in result[\"metrics\"].items():\n",
    "                    if \"score\" in metric_data:\n",
    "                        if metric_name not in metric_summaries:\n",
    "                            metric_summaries[metric_name] = []\n",
    "                        metric_summaries[metric_name].append(metric_data[\"score\"])\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            \"total_evaluations\": len(self.evaluation_history),\n",
    "            \"overall_performance\": {\n",
    "                \"average_score\": round(np.mean(overall_scores), 2) if overall_scores else 0,\n",
    "                \"average_pass_rate\": round(np.mean(pass_rates) * 100, 1) if pass_rates else 0,\n",
    "                \"score_std\": round(np.std(overall_scores), 2) if overall_scores else 0\n",
    "            },\n",
    "            \"metric_performance\": {}\n",
    "        }\n",
    "        \n",
    "        for metric_name, scores in metric_summaries.items():\n",
    "            summary[\"metric_performance\"][metric_name] = {\n",
    "                \"average_score\": round(np.mean(scores), 2),\n",
    "                \"min_score\": round(min(scores), 2),\n",
    "                \"max_score\": round(max(scores), 2),\n",
    "                \"std_dev\": round(np.std(scores), 2)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create evaluation pipeline\n",
    "code_eval_pipeline = CodeEvaluationPipeline()\n",
    "print(\"‚úÖ Code Evaluation Pipeline created successfully!\")\n",
    "print(f\"Available metrics: {list(code_eval_pipeline.metrics.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_code_evaluation():\n",
    "    \"\"\"\n",
    "    Run comprehensive evaluation pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    if not code_data or 'code_generation_problems' not in code_data:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ code generation data ƒë·ªÉ comprehensive evaluation\")\n",
    "        return\n",
    "    \n",
    "    print(\"üöÄ Running Comprehensive Code Evaluation Pipeline\\n\")\n",
    "    \n",
    "    # Prepare evaluation samples t·ª´ code data\n",
    "    evaluation_samples = []\n",
    "    \n",
    "    # Add code generation problems\n",
    "    for problem in code_data['code_generation_problems']:\n",
    "        evaluation_samples.append({\n",
    "            \"name\": f\"Problem: {problem['id']}\",\n",
    "            \"problem\": problem['problem'],\n",
    "            \"code\": problem['correct_solution'],\n",
    "            \"expected_solution\": problem['correct_solution']\n",
    "        })\n",
    "    \n",
    "    # Add some buggy code samples\n",
    "    for sample in code_data['buggy_code_samples'][:2]:  # Just first 2\n",
    "        evaluation_samples.append({\n",
    "            \"name\": f\"Buggy: {sample['id']}\",\n",
    "            \"problem\": sample['description'],\n",
    "            \"code\": sample['buggy_code']\n",
    "        })\n",
    "    \n",
    "    print(f\"üìù Evaluating {len(evaluation_samples)} code samples\")\n",
    "    \n",
    "    # Run batch evaluation\n",
    "    comprehensive_results = code_eval_pipeline.batch_evaluate(evaluation_samples)\n",
    "    \n",
    "    # Get performance summary\n",
    "    performance_summary = code_eval_pipeline.get_performance_summary()\n",
    "    \n",
    "    return comprehensive_results, performance_summary\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "comprehensive_results, performance_summary = run_comprehensive_code_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results\n",
    "def display_comprehensive_results(results, summary):\n",
    "    \"\"\"\n",
    "    Display v√† analyze comprehensive evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ results ƒë·ªÉ display\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Comprehensive Code Evaluation Results\\n\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"overall_score\" in result:\n",
    "            row = {\n",
    "                \"Sample\": result.get(\"sample_name\", \"Unknown\"),\n",
    "                \"Overall_Score\": result[\"overall_score\"],\n",
    "                \"Pass_Rate\": f\"{result['pass_count']}/{result['total_metrics']}\",\n",
    "                \"Code_Length\": result.get(\"code_length\", 0)\n",
    "            }\n",
    "            \n",
    "            # Add individual metric scores\n",
    "            for metric_name, metric_data in result.get(\"metrics\", {}).items():\n",
    "                if \"score\" in metric_data:\n",
    "                    row[f\"{metric_name.title()}\"] = metric_data[\"score\"]\n",
    "            \n",
    "            summary_data.append(row)\n",
    "    \n",
    "    if summary_data:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.round(1).to_string(index=False))\n",
    "    \n",
    "    # Display performance summary\n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    if \"overall_performance\" in summary:\n",
    "        perf = summary[\"overall_performance\"]\n",
    "        print(f\"  Total Evaluations: {summary['total_evaluations']}\")\n",
    "        print(f\"  Average Overall Score: {perf['average_score']}/10\")\n",
    "        print(f\"  Average Pass Rate: {perf['average_pass_rate']}%\")\n",
    "        print(f\"  Score Std Dev: {perf['score_std']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Metric Performance:\")\n",
    "    if \"metric_performance\" in summary:\n",
    "        for metric_name, stats in summary[\"metric_performance\"].items():\n",
    "            print(f\"  {metric_name.title()}:\")\n",
    "            print(f\"    Average: {stats['average_score']}/10\")\n",
    "            print(f\"    Range: {stats['min_score']} - {stats['max_score']}\")\n",
    "            print(f\"    Std Dev: {stats['std_dev']}\")\n",
    "    \n",
    "    # Insights\n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    \n",
    "    if \"metric_performance\" in summary:\n",
    "        metric_avgs = {name: stats['average_score'] for name, stats in summary[\"metric_performance\"].items()}\n",
    "        \n",
    "        if metric_avgs:\n",
    "            best_metric = max(metric_avgs.keys(), key=lambda k: metric_avgs[k])\n",
    "            worst_metric = min(metric_avgs.keys(), key=lambda k: metric_avgs[k])\n",
    "            \n",
    "            print(f\"  ‚Ä¢ Strongest area: {best_metric.title()} ({metric_avgs[best_metric]:.1f}/10)\")\n",
    "            print(f\"  ‚Ä¢ Weakest area: {worst_metric.title()} ({metric_avgs[worst_metric]:.1f}/10)\")\n",
    "            \n",
    "            if metric_avgs[worst_metric] < 6.0:\n",
    "                print(f\"  ‚Ä¢ ‚ö†Ô∏è  {worst_metric.title()} needs improvement (below threshold)\")\n",
    "    \n",
    "    return df if 'df' in locals() else None\n",
    "\n",
    "# Display results\n",
    "results_df = display_comprehensive_results(comprehensive_results, performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ph·∫ßn 8: Visualization v√† Analysis\n",
    "\n",
    "### 8.1 Code Evaluation Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_code_evaluation_results(results, summary):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations cho code evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results or not summary.get(\"metric_performance\"):\n",
    "        print(\"‚ùå Insufficient data for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Setup plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Code Evaluation Results Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Metric Performance Comparison\n",
    "    metric_names = list(summary[\"metric_performance\"].keys())\n",
    "    metric_scores = [summary[\"metric_performance\"][name][\"average_score\"] for name in metric_names]\n",
    "    \n",
    "    bars = axes[0,0].bar(metric_names, metric_scores, \n",
    "                        color=['skyblue', 'lightgreen', 'coral', 'lightpink'])\n",
    "    axes[0,0].set_title('Average Metric Scores')\n",
    "    axes[0,0].set_ylabel('Score (0-10)')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].axhline(y=7.0, color='red', linestyle='--', alpha=0.7, label='Typical Threshold')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, metric_scores):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                      f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Score Distribution\n",
    "    all_scores = []\n",
    "    all_metric_labels = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"metrics\" in result:\n",
    "            for metric_name, metric_data in result[\"metrics\"].items():\n",
    "                if \"score\" in metric_data:\n",
    "                    all_scores.append(metric_data[\"score\"])\n",
    "                    all_metric_labels.append(metric_name)\n",
    "    \n",
    "    if all_scores:\n",
    "        axes[0,1].hist(all_scores, bins=10, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "        axes[0,1].set_title('Score Distribution')\n",
    "        axes[0,1].set_xlabel('Score')\n",
    "        axes[0,1].set_ylabel('Frequency')\n",
    "        axes[0,1].axvline(x=np.mean(all_scores), color='red', linestyle='--', \n",
    "                         label=f'Mean: {np.mean(all_scores):.1f}')\n",
    "        axes[0,1].legend()\n",
    "    \n",
    "    # 3. Pass Rate Analysis\n",
    "    pass_rates = []\n",
    "    sample_names = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"pass_count\" in result and \"total_metrics\" in result:\n",
    "            pass_rate = result[\"pass_count\"] / result[\"total_metrics\"] * 100\n",
    "            pass_rates.append(pass_rate)\n",
    "            sample_names.append(result.get(\"sample_name\", \"Unknown\")[:20])\n",
    "    \n",
    "    if pass_rates:\n",
    "        bars = axes[1,0].bar(range(len(pass_rates)), pass_rates, \n",
    "                            color=['green' if pr >= 75 else 'orange' if pr >= 50 else 'red' for pr in pass_rates])\n",
    "        axes[1,0].set_title('Pass Rate by Sample')\n",
    "        axes[1,0].set_ylabel('Pass Rate (%)')\n",
    "        axes[1,0].set_xlabel('Samples')\n",
    "        axes[1,0].set_xticks(range(len(sample_names)))\n",
    "        axes[1,0].set_xticklabels(sample_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, rate) in enumerate(zip(bars, pass_rates)):\n",
    "            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                          f'{rate:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Metric Correlation Heatmap\n",
    "    if len(metric_names) > 1:\n",
    "        # Create correlation matrix\n",
    "        metric_data_matrix = []\n",
    "        \n",
    "        for result in results:\n",
    "            if \"metrics\" in result:\n",
    "                row = []\n",
    "                for metric_name in metric_names:\n",
    "                    if metric_name in result[\"metrics\"] and \"score\" in result[\"metrics\"][metric_name]:\n",
    "                        row.append(result[\"metrics\"][metric_name][\"score\"])\n",
    "                    else:\n",
    "                        row.append(0)\n",
    "                if len(row) == len(metric_names):\n",
    "                    metric_data_matrix.append(row)\n",
    "        \n",
    "        if len(metric_data_matrix) > 1:\n",
    "            correlation_df = pd.DataFrame(metric_data_matrix, columns=metric_names)\n",
    "            correlation_matrix = correlation_df.corr()\n",
    "            \n",
    "            im = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                           square=True, ax=axes[1,1], cbar_kws={'shrink': 0.8})\n",
    "            axes[1,1].set_title('Metric Score Correlations')\n",
    "        else:\n",
    "            axes[1,1].text(0.5, 0.5, 'Insufficient data\\nfor correlation', \n",
    "                          ha='center', va='center', transform=axes[1,1].transAxes, fontsize=12)\n",
    "            axes[1,1].set_title('Metric Score Correlations')\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'Need multiple metrics\\nfor correlation', \n",
    "                      ha='center', va='center', transform=axes[1,1].transAxes, fontsize=12)\n",
    "        axes[1,1].set_title('Metric Score Correlations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print additional insights\n",
    "    print(\"\\nüîç Visualization Insights:\")\n",
    "    \n",
    "    if metric_scores:\n",
    "        highest_metric = metric_names[metric_scores.index(max(metric_scores))]\n",
    "        lowest_metric = metric_names[metric_scores.index(min(metric_scores))]\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Highest performing metric: {highest_metric.title()} ({max(metric_scores):.1f})\")\n",
    "        print(f\"  ‚Ä¢ Lowest performing metric: {lowest_metric.title()} ({min(metric_scores):.1f})\")\n",
    "        \n",
    "        score_range = max(metric_scores) - min(metric_scores)\n",
    "        if score_range > 3.0:\n",
    "            print(f\"  ‚Ä¢ ‚ö†Ô∏è  Large score variance ({score_range:.1f}) indicates inconsistent quality\")\n",
    "        \n",
    "    if pass_rates:\n",
    "        avg_pass_rate = np.mean(pass_rates)\n",
    "        print(f\"  ‚Ä¢ Average pass rate: {avg_pass_rate:.1f}%\")\n",
    "        \n",
    "        if avg_pass_rate < 50:\n",
    "            print(f\"  ‚Ä¢ ‚ö†Ô∏è  Low pass rate suggests code quality issues\")\n",
    "        elif avg_pass_rate > 80:\n",
    "            print(f\"  ‚Ä¢ ‚úÖ High pass rate indicates good code quality\")\n",
    "\n",
    "# Create visualizations\n",
    "visualize_code_evaluation_results(comprehensive_results, performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Ph·∫ßn 9: Exercises v√† Th·ª±c h√†nh\n",
    "\n",
    "### Exercise 1: Custom Security Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: T·∫°o custom security metric cho specific vulnerability type\n",
    "def exercise_1_custom_security_metric():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o specialized security metric cho SQL injection detection\n",
    "    Y√™u c·∫ßu:\n",
    "    1. Focus specifically on SQL injection patterns\n",
    "    2. Include common SQL injection techniques\n",
    "    3. Test v·ªõi vulnerable code samples\n",
    "    4. Compare v·ªõi general security metric\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Define SQL injection specific criteria\n",
    "    sql_injection_criteria = \"\"\"\n",
    "    Your SQL injection detection criteria here...\n",
    "    Focus on:\n",
    "    - String concatenation in SQL queries\n",
    "    - Missing parameterized queries\n",
    "    - User input directly in SQL\n",
    "    - Dynamic query construction\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create G-Eval metric\n",
    "    \n",
    "    # TODO: Test v·ªõi vulnerable samples\n",
    "    \n",
    "    # TODO: Compare results\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üí° Exercise 1 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Use specific SQL injection patterns in criteria\")\n",
    "print(\"- Test v·ªõi code_data['buggy_code_samples'] c√≥ SQL injection\")\n",
    "print(\"- Compare specificity vs general security metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Code Style Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: T·∫°o code style compliance metric\n",
    "def exercise_2_code_style_metric():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o metric ƒë·ªÉ check PEP 8 compliance v√† Python best practices\n",
    "    Y√™u c·∫ßu:\n",
    "    1. Check naming conventions (snake_case, PascalCase)\n",
    "    2. Line length v√† formatting\n",
    "    3. Import organization\n",
    "    4. Comment style v√† docstrings\n",
    "    5. Python idioms usage\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Define PEP 8 criteria\n",
    "    pep8_criteria = \"\"\"\n",
    "    Your PEP 8 compliance criteria here...\n",
    "    Check for:\n",
    "    - Variable naming (snake_case)\n",
    "    - Function naming conventions\n",
    "    - Line length (79 characters)\n",
    "    - Proper spacing and indentation\n",
    "    - Import statements organization\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create metric v√† test\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üí° Exercise 2 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Reference PEP 8 guidelines\")\n",
    "print(\"- Create test cases v·ªõi good/bad style examples\")\n",
    "print(\"- Consider using ast module ƒë·ªÉ analyze code structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Automated Code Review Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build automated code review system\n",
    "def exercise_3_automated_code_review():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o complete automated code review system\n",
    "    Y√™u c·∫ßu:\n",
    "    1. Integrate t·∫•t c·∫£ metrics (correctness, style, security, performance)\n",
    "    2. Generate comprehensive review report\n",
    "    3. Provide specific recommendations\n",
    "    4. Rank issues by severity\n",
    "    5. Output structured review format\n",
    "    \"\"\"\n",
    "    \n",
    "    class AutomatedCodeReviewer:\n",
    "        def __init__(self):\n",
    "            # TODO: Initialize all metrics\n",
    "            pass\n",
    "        \n",
    "        def review_code(self, code: str, context: str = \"\") -> Dict:\n",
    "            # TODO: Run all evaluations\n",
    "            # TODO: Generate recommendations\n",
    "            # TODO: Format review report\n",
    "            pass\n",
    "        \n",
    "        def generate_review_report(self, results: Dict) -> str:\n",
    "            # TODO: Create structured review report\n",
    "            pass\n",
    "    \n",
    "    # TODO: Test v·ªõi various code samples\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üí° Exercise 3 Template created. Complete the class above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Combine all metrics t·ª´ previous sections\")\n",
    "print(\"- Use severity levels: Critical, High, Medium, Low\")\n",
    "print(\"- Generate actionable recommendations\")\n",
    "print(\"- Consider code context trong recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ T·ªïng k·∫øt v√† Next Steps\n",
    "\n",
    "### üèÜ Nh·ªØng g√¨ ƒë√£ h·ªçc trong Notebook n√†y:\n",
    "\n",
    "1. **‚úÖ G-Eval Framework Mastery**\n",
    "   - T·∫°o custom metrics v·ªõi natural language criteria\n",
    "   - Define evaluation steps v√† parameters\n",
    "   - Flexible threshold setting\n",
    "\n",
    "2. **‚úÖ Code Generation Metrics**\n",
    "   - **CodeCorrectnessMetric**: Logic, syntax, functional requirements\n",
    "   - **ReadabilityMetric**: Naming, structure, documentation\n",
    "   - **EfficiencyMetric**: Time/space complexity, algorithmic approach\n",
    "\n",
    "3. **‚úÖ Security Evaluation**\n",
    "   - **SecurityReviewMetric**: Vulnerability detection\n",
    "   - Injection attacks, input validation, authentication issues\n",
    "   - Automated security assessment\n",
    "\n",
    "4. **‚úÖ Code Review Automation**\n",
    "   - **ReviewCompletenessMetric**: Comprehensive review coverage\n",
    "   - Multi-dimensional code assessment\n",
    "   - Automated review pipeline\n",
    "\n",
    "5. **‚úÖ Comprehensive Evaluation Pipeline**\n",
    "   - End-to-end code evaluation system\n",
    "   - Batch processing capabilities\n",
    "   - Performance analytics v√† insights\n",
    "\n",
    "### üöÄ Next Steps - Notebook 4: AI Agents & Chain-of-Thought\n",
    "\n",
    "Trong notebook ti·∫øp theo, ch√∫ng ta s·∫Ω h·ªçc:\n",
    "\n",
    "- ü§ñ **LangGraph Agent Construction**: Multi-tool agents\n",
    "- üß† **Chain-of-Thought Evaluation**: Reasoning process assessment\n",
    "- üõ†Ô∏è **Agent-Specific Metrics**: LogicalFlow, ToolUsage, PlanExecution\n",
    "- üîÑ **Multi-step Evaluation**: Intermediate step analysis\n",
    "- üìä **Agent Performance Benchmarking**: Comprehensive agent assessment\n",
    "\n",
    "### üí° Key Insights t·ª´ Code Evaluation:\n",
    "\n",
    "- **Multi-dimensional Assessment**: Code quality kh√¥ng ch·ªâ v·ªÅ correctness\n",
    "- **Security First**: Security evaluation c·∫ßn ƒë∆∞·ª£c prioritized\n",
    "- **Custom Metrics Power**: G-Eval cho ph√©p domain-specific evaluation\n",
    "- **Automated Review**: Scale code review process with consistent standards\n",
    "- **Comprehensive Pipeline**: Holistic approach t·ªët h∆°n individual metrics\n",
    "\n",
    "### üéØ Best Practices Summary:\n",
    "\n",
    "1. **Use multiple metrics** cho comprehensive assessment\n",
    "2. **Customize evaluation criteria** cho specific use cases\n",
    "3. **Include security metrics** trong all code evaluations\n",
    "4. **Test v·ªõi various code quality levels** ƒë·ªÉ validate metrics\n",
    "5. **Visualize results** ƒë·ªÉ identify patterns v√† insights\n",
    "6. **Automate evaluation pipeline** cho consistent assessment\n",
    "\n",
    "### üìä Evaluation Framework Comparison:\n",
    "\n",
    "| Aspect | Traditional Testing | DeepEval Code Metrics |\n",
    "|--------|--------------------|-----------------------|\n",
    "| **Scope** | Functionality only | Multi-dimensional |\n",
    "| **Automation** | Limited | Comprehensive |\n",
    "| **Customization** | Rigid | Highly flexible |\n",
    "| **Security** | Manual review | Automated detection |\n",
    "| **Scalability** | Manual effort | Automated pipeline |\n",
    "| **Consistency** | Reviewer dependent | Standardized criteria |\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Outstanding Progress!\n",
    "\n",
    "B·∫°n ƒë√£ mastered code generation evaluation v·ªõi DeepEval! \n",
    "\n",
    "Ready to tackle **Notebook 4: Evaluating AI Agents v√† CoT with LangGraph**? üöÄü§ñ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}