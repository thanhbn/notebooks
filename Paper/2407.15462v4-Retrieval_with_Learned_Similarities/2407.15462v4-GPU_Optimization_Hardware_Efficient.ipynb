{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 GPU Optimization & Hardware-Efficient Implementation\n",
    "\n",
    "## 🎯 Mục tiêu Học tập\n",
    "\n",
    "Hiểu sâu về:\n",
    "1. **GPU Architecture** và memory hierarchy trong context của retrieval\n",
    "2. **Arithmetic Intensity** - tại sao MoL hiệu quả hơn dot products trên GPU\n",
    "3. **Memory Bandwidth vs Compute** trade-offs trong modern accelerators\n",
    "4. **Vectorization & Parallelization** strategies cho MoL operations\n",
    "5. **Hardware-aware Algorithm Design** cho production deployment\n",
    "\n",
    "## 📖 Trích xuất từ Paper\n",
    "\n",
    "### Key Hardware Insights:\n",
    "\n",
    "> *\"Due to GPUs and other accelerators having orders of magnitude higher arithmetic intensity vs CPUs, traditional quantization techniques no longer fully utilize the compute; accelerator-specific nearest neighbor algorithms that benefit from increased compute have been proposed recently.\"*\n",
    "\n",
    "> *\"Our approach with learned similarities efficiently utilizes modern accelerators due to MoL's higher arithmetic intensity, which results in MIPS-level inference latency and throughput.\"*\n",
    "\n",
    "### Core Concepts:\n",
    "- **Arithmetic Intensity**: Operations per byte of memory transfer\n",
    "- **Memory Bandwidth Bottleneck**: Main limitation trong dot product systems\n",
    "- **Compute Utilization**: MoL better utilizes GPU's massive parallel compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 Device: {device}\")\n",
    "\n",
    "# Check GPU properties if available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"🔥 GPU: {gpu_props.name}\")\n",
    "    print(f\"   Memory: {gpu_props.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   SMs: {gpu_props.multi_processor_count}\")\n",
    "    print(f\"   CUDA Capability: {gpu_props.major}.{gpu_props.minor}\")\nelse:\n",
    "    print(\"⚠️ No GPU available - using CPU for demonstrations\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Phần 1: Hardware Architecture Analysis\n",
    "\n",
    "### 📊 GPU Memory Hierarchy:\n",
    "\n",
    "1. **Global Memory**: Lớn (~16GB) nhưng chậm (~1000 GB/s bandwidth)\n",
    "2. **Shared Memory**: Nhỏ (~100KB/SM) nhưng rất nhanh (~20,000 GB/s)\n",
    "3. **Register File**: Rất nhỏ nhưng cực nhanh\n",
    "4. **L1/L2 Cache**: Automatic caching từ compiler\n",
    "\n",
    "### 🎯 Arithmetic Intensity Formula:\n",
    "```\n",
    "AI = FLOPS / Bytes_Transferred\n",
    "```\n",
    "\n",
    "**Dot Product AI**: ~2 (1 multiply + 1 add per 8 bytes)\n",
    "**MoL AI**: ~8-16 (multiple operations per data transfer)\n",
    "\n",
    "### 🚀 Why MoL is GPU-friendly:\n",
    "- Higher compute density per memory access\n",
    "- Better utilization of GPU's 1000s of cores\n",
    "- Reduced memory bandwidth pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HardwareProfile:\n",
    "    \"\"\"Hardware characteristics for performance modeling\"\"\"\n",
    "    peak_flops: float  # TFLOPS\n",
    "    memory_bandwidth: float  # GB/s\n",
    "    memory_size: float  # GB\n",
    "    num_sms: int  # Streaming Multiprocessors\n",
    "    shared_mem_per_sm: int  # KB\n",
    "    register_file_per_sm: int  # KB\n",
    "\n",
    "class ArithmeticIntensityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze arithmetic intensity of different retrieval operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hardware: HardwareProfile):\n",
    "        self.hardware = hardware\n",
    "    \n",
    "    def compute_dot_product_ai(self, embedding_dim: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze arithmetic intensity of dot product operations\n",
    "        \"\"\"\n",
    "        # Operations: embedding_dim multiplications + (embedding_dim-1) additions\n",
    "        flops_per_similarity = 2 * embedding_dim - 1\n",
    "        \n",
    "        # Memory transfers: 2 vectors * embedding_dim * 4 bytes (float32)\n",
    "        bytes_per_similarity = 2 * embedding_dim * 4\n",
    "        \n",
    "        arithmetic_intensity = flops_per_similarity / bytes_per_similarity\n",
    "        \n",
    "        return {\n",
    "            'flops_per_similarity': flops_per_similarity,\n",
    "            'bytes_per_similarity': bytes_per_similarity,\n",
    "            'arithmetic_intensity': arithmetic_intensity,\n",
    "            'compute_bound_threshold': self.hardware.peak_flops * 1e12 / (self.hardware.memory_bandwidth * 1e9),\n",
    "            'is_compute_bound': arithmetic_intensity > (self.hardware.peak_flops * 1e12 / (self.hardware.memory_bandwidth * 1e9))\n",
    "        }\n",
    "    \n",
    "    def compute_mol_ai(self, embedding_dim: int, num_components: int, \n",
    "                      component_dim: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze arithmetic intensity of MoL operations\n",
    "        \"\"\"\n",
    "        # Operations per similarity computation:\n",
    "        # 1. Component embeddings: num_components * 2 * (embedding_dim * component_dim + component_dim)\n",
    "        # 2. Dot products: num_components * (2 * component_dim - 1)\n",
    "        # 3. Gating network: (embedding_dim * 2) * 128 + 128 * num_components (simplified)\n",
    "        # 4. Weighted sum: num_components * 2\n",
    "        \n",
    "        embedding_flops = num_components * 2 * (embedding_dim * component_dim)\n",
    "        dot_product_flops = num_components * (2 * component_dim - 1)\n",
    "        gating_flops = (embedding_dim * 2) * 128 + 128 * num_components\n",
    "        aggregation_flops = num_components * 2\n",
    "        \n",
    "        total_flops = embedding_flops + dot_product_flops + gating_flops + aggregation_flops\n",
    "        \n",
    "        # Memory transfers:\n",
    "        # 1. Input vectors: 2 * embedding_dim * 4 bytes\n",
    "        # 2. Model parameters: roughly (embedding_dim * component_dim * num_components * 2 + gating params) * 4\n",
    "        # Note: Parameters might be cached, so we consider only input vectors for best case\n",
    "        \n",
    "        input_bytes = 2 * embedding_dim * 4\n",
    "        param_bytes = (embedding_dim * component_dim * num_components * 2 + \n",
    "                      embedding_dim * 2 * 128 + 128 * num_components) * 4\n",
    "        \n",
    "        # Best case: parameters cached\n",
    "        bytes_cached = input_bytes\n",
    "        # Worst case: parameters not cached\n",
    "        bytes_uncached = input_bytes + param_bytes\n",
    "        \n",
    "        ai_cached = total_flops / bytes_cached\n",
    "        ai_uncached = total_flops / bytes_uncached\n",
    "        \n",
    "        compute_bound_threshold = self.hardware.peak_flops * 1e12 / (self.hardware.memory_bandwidth * 1e9)\n",
    "        \n",
    "        return {\n",
    "            'total_flops': total_flops,\n",
    "            'input_bytes': input_bytes,\n",
    "            'param_bytes': param_bytes,\n",
    "            'ai_cached': ai_cached,\n",
    "            'ai_uncached': ai_uncached,\n",
    "            'compute_bound_threshold': compute_bound_threshold,\n",
    "            'is_compute_bound_cached': ai_cached > compute_bound_threshold,\n",
    "            'is_compute_bound_uncached': ai_uncached > compute_bound_threshold,\n",
    "            'breakdown': {\n",
    "                'embedding_flops': embedding_flops,\n",
    "                'dot_product_flops': dot_product_flops,\n",
    "                'gating_flops': gating_flops,\n",
    "                'aggregation_flops': aggregation_flops\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def compare_operations(self, embedding_dim: int = 128, \n",
    "                         num_components: int = 8, \n",
    "                         component_dim: int = 64) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare arithmetic intensity of dot product vs MoL\n",
    "        \"\"\"\n",
    "        dot_analysis = self.compute_dot_product_ai(embedding_dim)\n",
    "        mol_analysis = self.compute_mol_ai(embedding_dim, num_components, component_dim)\n",
    "        \n",
    "        return {\n",
    "            'dot_product': dot_analysis,\n",
    "            'mol': mol_analysis,\n",
    "            'ai_improvement_cached': mol_analysis['ai_cached'] / dot_analysis['arithmetic_intensity'],\n",
    "            'ai_improvement_uncached': mol_analysis['ai_uncached'] / dot_analysis['arithmetic_intensity'],\n",
    "            'flops_ratio': mol_analysis['total_flops'] / dot_analysis['flops_per_similarity']\n",
    "        }\n",
    "\n",
    "# Create hardware profile (example: NVIDIA A100)\n",
    "a100_profile = HardwareProfile(\n",
    "    peak_flops=312.0,  # TFLOPS (Tensor)\n",
    "    memory_bandwidth=1935.0,  # GB/s\n",
    "    memory_size=80.0,  # GB\n",
    "    num_sms=108,\n",
    "    shared_mem_per_sm=164,  # KB\n",
    "    register_file_per_sm=256  # KB\n",
    ")\n",
    "\n",
    "# For systems without GPU, create a representative profile\n",
    "if not torch.cuda.is_available():\n",
    "    a100_profile = HardwareProfile(\n",
    "        peak_flops=10.0,  # Simulated\n",
    "        memory_bandwidth=100.0,  # Simulated\n",
    "        memory_size=16.0,\n",
    "        num_sms=20,\n",
    "        shared_mem_per_sm=64,\n",
    "        register_file_per_sm=128\n",
    "    )\n",
    "\n",
    "analyzer = ArithmeticIntensityAnalyzer(a100_profile)\n",
    "\n",
    "print(\"🔍 Arithmetic Intensity Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different configurations\n",
    "configs = [\n",
    "    {'embedding_dim': 128, 'num_components': 4, 'component_dim': 32},\n",
    "    {'embedding_dim': 128, 'num_components': 8, 'component_dim': 64},\n",
    "    {'embedding_dim': 256, 'num_components': 8, 'component_dim': 64},\n",
    "    {'embedding_dim': 512, 'num_components': 16, 'component_dim': 64}\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for config in configs:\n",
    "    result = analyzer.compare_operations(**config)\n",
    "    comparison_results.append(result)\n",
    "    \n",
    "    print(f\"\\n📊 Config: dim={config['embedding_dim']}, components={config['num_components']}, comp_dim={config['component_dim']}\")\n",
    "    print(f\"   Dot Product AI: {result['dot_product']['arithmetic_intensity']:.2f}\")\n",
    "    print(f\"   MoL AI (cached): {result['mol']['ai_cached']:.2f}\")\n",
    "    print(f\"   MoL AI (uncached): {result['mol']['ai_uncached']:.2f}\")\n",
    "    print(f\"   AI Improvement: {result['ai_improvement_cached']:.1f}x (cached)\")\n",
    "    print(f\"   Compute Bound Threshold: {result['dot_product']['compute_bound_threshold']:.2f}\")\n",
    "    print(f\"   Dot Product Compute Bound: {result['dot_product']['is_compute_bound']}\")\n",
    "    print(f\"   MoL Compute Bound: {result['mol']['is_compute_bound_cached']} (cached)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Phần 2: Memory Access Pattern Optimization\n",
    "\n",
    "### 🎯 Key Principles:\n",
    "\n",
    "1. **Coalesced Memory Access**: 32 threads access contiguous memory\n",
    "2. **Shared Memory Utilization**: Store frequently accessed data in fast memory\n",
    "3. **Register Blocking**: Keep computation in registers\n",
    "4. **Cache-friendly Data Layout**: Optimize for spatial/temporal locality\n",
    "\n",
    "### 🔧 Optimization Strategies:\n",
    "- **Batching**: Process multiple queries simultaneously\n",
    "- **Tiling**: Break large computations into cache-friendly chunks\n",
    "- **Prefetching**: Load data before needed\n",
    "- **Fusion**: Combine multiple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryOptimizedMoL(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-optimized MoL implementation with GPU-friendly patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int,\n",
    "                 num_components: int = 8,\n",
    "                 component_dim: int = 64,\n",
    "                 use_fused_operations: bool = True,\n",
    "                 tile_size: int = 1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_components = num_components\n",
    "        self.component_dim = component_dim\n",
    "        self.use_fused_operations = use_fused_operations\n",
    "        self.tile_size = tile_size\n",
    "        \n",
    "        # Component embeddings - stored in optimized layout\n",
    "        self.query_embeddings = nn.ModuleList([\n",
    "            nn.Linear(input_dim, component_dim, bias=False) for _ in range(num_components)\n",
    "        ])\n",
    "        self.item_embeddings = nn.ModuleList([\n",
    "            nn.Linear(input_dim, component_dim, bias=False) for _ in range(num_components)\n",
    "        ])\n",
    "        \n",
    "        # Gating network - optimized for batched inference\n",
    "        self.gating_net = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 128, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_components, bias=False)\n",
    "        )\n",
    "        \n",
    "        # Initialize for better cache behavior\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights for better hardware utilization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Xavier initialization with hardware-friendly scaling\n",
    "                nn.init.xavier_uniform_(module.weight, gain=1.0)\n",
    "    \n",
    "    def forward_tiled(self, queries: torch.Tensor, items: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Tiled forward pass for memory efficiency\n",
    "        \"\"\"\n",
    "        batch_q, batch_i = queries.size(0), items.size(0)\n",
    "        similarities = torch.zeros(batch_q, batch_i, device=queries.device, dtype=queries.dtype)\n",
    "        \n",
    "        # Process in tiles to fit in memory hierarchy\n",
    "        for q_start in range(0, batch_q, self.tile_size):\n",
    "            q_end = min(q_start + self.tile_size, batch_q)\n",
    "            query_tile = queries[q_start:q_end]\n",
    "            \n",
    "            for i_start in range(0, batch_i, self.tile_size):\n",
    "                i_end = min(i_start + self.tile_size, batch_i)\n",
    "                item_tile = items[i_start:i_end]\n",
    "                \n",
    "                # Compute tile similarities\n",
    "                tile_sim = self._compute_tile_similarity(query_tile, item_tile)\n",
    "                similarities[q_start:q_end, i_start:i_end] = tile_sim\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def _compute_tile_similarity(self, queries: torch.Tensor, items: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute similarity for a tile with optimized memory access\n",
    "        \"\"\"\n",
    "        batch_q, batch_i = queries.size(0), items.size(0)\n",
    "        \n",
    "        if self.use_fused_operations:\n",
    "            return self._fused_similarity_computation(queries, items)\n",
    "        else:\n",
    "            return self._standard_similarity_computation(queries, items)\n",
    "    \n",
    "    def _fused_similarity_computation(self, queries: torch.Tensor, items: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fused computation to minimize memory transfers\n",
    "        \"\"\"\n",
    "        batch_q, batch_i = queries.size(0), items.size(0)\n",
    "        \n",
    "        # Precompute all component embeddings in batch\n",
    "        # Shape: [num_components, batch_q, component_dim]\n",
    "        query_components = torch.stack([\n",
    "            F.normalize(emb(queries), dim=-1) for emb in self.query_embeddings\n",
    "        ])\n",
    "        \n",
    "        # Shape: [num_components, batch_i, component_dim]\n",
    "        item_components = torch.stack([\n",
    "            F.normalize(emb(items), dim=-1) for emb in self.item_embeddings\n",
    "        ])\n",
    "        \n",
    "        # Vectorized computation of all component similarities\n",
    "        # Shape: [num_components, batch_q, batch_i]\n",
    "        component_similarities = torch.einsum('cpd,cid->cqi', query_components, item_components)\n",
    "        \n",
    "        # Compute gating weights for all pairs efficiently\n",
    "        # This is still the bottleneck - need to optimize further\n",
    "        similarities = torch.zeros(batch_q, batch_i, device=queries.device)\n",
    "        \n",
    "        # Batch process gating for efficiency\n",
    "        for i in range(batch_q):\n",
    "            # Vectorized gating computation for one query vs all items\n",
    "            query_expanded = queries[i:i+1].expand(batch_i, -1)  # [batch_i, input_dim]\n",
    "            combined_features = torch.cat([query_expanded, items], dim=1)  # [batch_i, input_dim*2]\n",
    "            \n",
    "            gating_weights = F.softmax(self.gating_net(combined_features), dim=1)  # [batch_i, num_components]\n",
    "            \n",
    "            # Weighted sum of component similarities\n",
    "            # component_similarities[:, i, :] has shape [num_components, batch_i]\n",
    "            weighted_sims = torch.sum(\n",
    "                gating_weights.t().unsqueeze(1) * component_similarities[:, i:i+1, :], \n",
    "                dim=0\n",
    "            ).squeeze(0)\n",
    "            \n",
    "            similarities[i, :] = weighted_sims\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def _standard_similarity_computation(self, queries: torch.Tensor, items: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Standard computation for comparison\n",
    "        \"\"\"\n",
    "        batch_q, batch_i = queries.size(0), items.size(0)\n",
    "        similarities = torch.zeros(batch_q, batch_i, device=queries.device)\n",
    "        \n",
    "        for i in range(batch_q):\n",
    "            for j in range(batch_i):\n",
    "                # Compute gating weights\n",
    "                combined = torch.cat([queries[i], items[j]])\n",
    "                weights = F.softmax(self.gating_net(combined.unsqueeze(0)), dim=1).squeeze(0)\n",
    "                \n",
    "                # Compute component similarities\n",
    "                sim = 0.0\n",
    "                for c in range(self.num_components):\n",
    "                    q_emb = F.normalize(self.query_embeddings[c](queries[i:i+1]), dim=1)\n",
    "                    i_emb = F.normalize(self.item_embeddings[c](items[j:j+1]), dim=1)\n",
    "                    component_sim = torch.dot(q_emb.squeeze(), i_emb.squeeze())\n",
    "                    sim += weights[c] * component_sim\n",
    "                \n",
    "                similarities[i, j] = sim\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def forward(self, queries: torch.Tensor, items: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Main forward pass\"\"\"\n",
    "        return self.forward_tiled(queries, items)\n",
    "\n",
    "print(\"⚡ Memory-optimized MoL implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Phần 3: Performance Benchmarking Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUPerformanceBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive GPU performance benchmark for retrieval operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device = device\n",
    "        self.results = {}\n",
    "    \n",
    "    def benchmark_memory_bandwidth(self, sizes: List[int] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Benchmark memory bandwidth with different access patterns\n",
    "        \"\"\"\n",
    "        if sizes is None:\n",
    "            sizes = [1024, 4096, 16384, 65536, 262144]\n",
    "        \n",
    "        print(\"\\n🔍 Memory Bandwidth Benchmark\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        results = {'sizes': sizes, 'sequential_bw': [], 'random_bw': [], 'strided_bw': []}\n",
    "        \n",
    "        for size in sizes:\n",
    "            # Create test data\n",
    "            data = torch.randn(size, 1024, device=self.device, dtype=torch.float32)\n",
    "            \n",
    "            # Sequential access\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for _ in range(10):\n",
    "                result = data.sum(dim=1)  # Sequential memory access\n",
    "            \n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            seq_time = time.time() - start_time\n",
    "            \n",
    "            # Random access (simulated)\n",
    "            indices = torch.randperm(size, device=self.device)[:size//2]\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for _ in range(10):\n",
    "                result = data[indices].sum()\n",
    "            \n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            rand_time = time.time() - start_time\n",
    "            \n",
    "            # Strided access\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for _ in range(10):\n",
    "                result = data[::2].sum()  # Strided access\n",
    "            \n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            stride_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate bandwidth (GB/s)\n",
    "            bytes_transferred = size * 1024 * 4 * 10  # 10 iterations, float32\n",
    "            seq_bw = (bytes_transferred / seq_time) / 1e9\n",
    "            rand_bw = (bytes_transferred / rand_time) / 1e9\n",
    "            stride_bw = (bytes_transferred / stride_time) / 1e9\n",
    "            \n",
    "            results['sequential_bw'].append(seq_bw)\n",
    "            results['random_bw'].append(rand_bw)\n",
    "            results['strided_bw'].append(stride_bw)\n",
    "            \n",
    "            print(f\"Size {size:6d}: Seq={seq_bw:6.1f} GB/s, Rand={rand_bw:6.1f} GB/s, Stride={stride_bw:6.1f} GB/s\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark_compute_throughput(self, operations: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Benchmark compute throughput for different operations\n",
    "        \"\"\"\n",
    "        if operations is None:\n",
    "            operations = ['matmul', 'elementwise', 'reduction', 'activation']\n",
    "        \n",
    "        print(\"\\n⚡ Compute Throughput Benchmark\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        size = 4096\n",
    "        A = torch.randn(size, size, device=self.device, dtype=torch.float32)\n",
    "        B = torch.randn(size, size, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        results = {'operations': operations, 'throughput_tflops': []}\n",
    "        \n",
    "        for op in operations:\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if op == 'matmul':\n",
    "                for _ in range(10):\n",
    "                    C = torch.mm(A, B)\n",
    "                flops = 10 * 2 * size**3  # 10 iterations, 2*N^3 operations\n",
    "            \n",
    "            elif op == 'elementwise':\n",
    "                for _ in range(100):\n",
    "                    C = A * B + A\n",
    "                flops = 100 * 2 * size**2  # 100 iterations, 2 ops per element\n",
    "            \n",
    "            elif op == 'reduction':\n",
    "                for _ in range(100):\n",
    "                    C = A.sum(dim=1)\n",
    "                flops = 100 * size * (size - 1)  # Sum reduction\n",
    "            \n",
    "            elif op == 'activation':\n",
    "                for _ in range(100):\n",
    "                    C = F.relu(A)\n",
    "                flops = 100 * size**2  # 1 op per element\n",
    "            \n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            throughput = (flops / elapsed) / 1e12  # TFLOPS\n",
    "            results['throughput_tflops'].append(throughput)\n",
    "            \n",
    "            print(f\"{op:12s}: {throughput:6.2f} TFLOPS\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark_mol_vs_dotproduct(self, configs: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Benchmark MoL vs dot product across different configurations\n",
    "        \"\"\"\n",
    "        print(\"\\n🧠 MoL vs Dot Product Benchmark\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results = {'configs': configs, 'mol_times': [], 'dot_times': [], 'mol_tflops': [], 'dot_tflops': []}\n",
    "        \n",
    "        for config in configs:\n",
    "            batch_q = config.get('batch_q', 32)\n",
    "            batch_i = config.get('batch_i', 1000)\n",
    "            embedding_dim = config.get('embedding_dim', 128)\n",
    "            num_components = config.get('num_components', 8)\n",
    "            component_dim = config.get('component_dim', 64)\n",
    "            \n",
    "            # Generate test data\n",
    "            queries = torch.randn(batch_q, embedding_dim, device=self.device, dtype=torch.float32)\n",
    "            items = torch.randn(batch_i, embedding_dim, device=self.device, dtype=torch.float32)\n",
    "            \n",
    "            # Create models\n",
    "            mol_model = MemoryOptimizedMoL(\n",
    "                embedding_dim, num_components, component_dim\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Warm up\n",
    "            with torch.no_grad():\n",
    "                _ = mol_model(queries[:2], items[:10])\n",
    "                _ = torch.mm(F.normalize(queries[:2], dim=1), F.normalize(items[:10], dim=1).t())\n",
    "            \n",
    "            # Benchmark MoL\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(5):\n",
    "                    mol_result = mol_model(queries, items)\n",
    "            \n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            mol_time = time.time() - start_time\n",
    "            \n",
    "            # Benchmark Dot Product\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(5):\n",
    "                    q_norm = F.normalize(queries, dim=1)\n",
    "                    i_norm = F.normalize(items, dim=1)\n",
    "                    dot_result = torch.mm(q_norm, i_norm.t())\n",
    "            \n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            dot_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate FLOPS\n",
    "            dot_flops = 5 * batch_q * batch_i * (2 * embedding_dim - 1)\n",
    "            mol_flops = 5 * batch_q * batch_i * analyzer.compute_mol_ai(\n",
    "                embedding_dim, num_components, component_dim\n",
    "            )['total_flops']\n",
    "            \n",
    "            mol_tflops = (mol_flops / mol_time) / 1e12\n",
    "            dot_tflops = (dot_flops / dot_time) / 1e12\n",
    "            \n",
    "            results['mol_times'].append(mol_time)\n",
    "            results['dot_times'].append(dot_time)\n",
    "            results['mol_tflops'].append(mol_tflops)\n",
    "            results['dot_tflops'].append(dot_tflops)\n",
    "            \n",
    "            print(f\"Config {embedding_dim}d, {num_components}c: MoL={mol_time:.4f}s ({mol_tflops:.2f}T), Dot={dot_time:.4f}s ({dot_tflops:.2f}T), Speedup={dot_time/mol_time:.2f}x\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark_memory_usage(self, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Benchmark memory usage patterns\n",
    "        \"\"\"\n",
    "        print(\"\\n💾 Memory Usage Benchmark\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if self.device.type != 'cuda':\n",
    "            print(\"Memory benchmark only available on CUDA\")\n",
    "            return {}\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_memory = torch.cuda.memory_allocated()\n",
    "        \n",
    "        embedding_dim = config.get('embedding_dim', 128)\n",
    "        num_items = config.get('num_items', 10000)\n",
    "        batch_size = config.get('batch_size', 100)\n",
    "        \n",
    "        # Create data\n",
    "        queries = torch.randn(batch_size, embedding_dim, device=self.device)\n",
    "        items = torch.randn(num_items, embedding_dim, device=self.device)\n",
    "        \n",
    "        data_memory = torch.cuda.memory_allocated() - initial_memory\n",
    "        \n",
    "        # Create model\n",
    "        mol_model = MemoryOptimizedMoL(embedding_dim).to(self.device)\n",
    "        model_memory = torch.cuda.memory_allocated() - initial_memory - data_memory\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            result = mol_model(queries, items)\n",
    "        \n",
    "        peak_memory = torch.cuda.max_memory_allocated() - initial_memory\n",
    "        \n",
    "        results = {\n",
    "            'data_memory_mb': data_memory / 1024**2,\n",
    "            'model_memory_mb': model_memory / 1024**2,\n",
    "            'peak_memory_mb': peak_memory / 1024**2,\n",
    "            'memory_efficiency': (data_memory + model_memory) / peak_memory\n",
    "        }\n",
    "        \n",
    "        print(f\"Data Memory: {results['data_memory_mb']:.1f} MB\")\n",
    "        print(f\"Model Memory: {results['model_memory_mb']:.1f} MB\")\n",
    "        print(f\"Peak Memory: {results['peak_memory_mb']:.1f} MB\")\n",
    "        print(f\"Efficiency: {results['memory_efficiency']:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run comprehensive benchmarks\n",
    "benchmark = GPUPerformanceBenchmark(device)\n",
    "\n",
    "# Memory bandwidth\n",
    "memory_results = benchmark.benchmark_memory_bandwidth([1024, 4096, 16384])\n",
    "\n",
    "# Compute throughput\n",
    "compute_results = benchmark.benchmark_compute_throughput()\n",
    "\n",
    "# MoL vs Dot Product\n",
    "mol_configs = [\n",
    "    {'batch_q': 32, 'batch_i': 1000, 'embedding_dim': 128, 'num_components': 4},\n",
    "    {'batch_q': 32, 'batch_i': 1000, 'embedding_dim': 128, 'num_components': 8},\n",
    "    {'batch_q': 64, 'batch_i': 2000, 'embedding_dim': 256, 'num_components': 8}\n",
    "]\n",
    "\n",
    "mol_benchmark_results = benchmark.benchmark_mol_vs_dotproduct(mol_configs)\n",
    "\n",
    "# Memory usage\n",
    "memory_config = {'embedding_dim': 128, 'num_items': 5000, 'batch_size': 50}\n",
    "memory_usage_results = benchmark.benchmark_memory_usage(memory_config)\n",
    "\n",
    "print(\"\\n✅ All benchmarks completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Phần 4: Hardware-Aware Algorithm Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwareAwareOptimizer:\n",
    "    \"\"\"\n",
    "    Automatically optimize MoL configuration based on hardware characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hardware_profile: HardwareProfile):\n",
    "        self.hardware = hardware_profile\n",
    "        self.analyzer = ArithmeticIntensityAnalyzer(hardware_profile)\n",
    "    \n",
    "    def optimize_configuration(self, \n",
    "                             target_accuracy: float = 0.95,\n",
    "                             target_latency_ms: float = 50,\n",
    "                             memory_budget_gb: float = 4.0) -> Dict:\n",
    "        \"\"\"\n",
    "        Find optimal MoL configuration for given constraints\n",
    "        \"\"\"\n",
    "        print(\"🎯 Hardware-Aware Configuration Optimization\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Search space\n",
    "        embedding_dims = [64, 128, 256, 512]\n",
    "        component_counts = [2, 4, 8, 16, 32]\n",
    "        component_dims = [16, 32, 64, 128]\n",
    "        \n",
    "        best_config = None\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for embed_dim in embedding_dims:\n",
    "            for num_comp in component_counts:\n",
    "                for comp_dim in component_dims:\n",
    "                    # Check memory constraint\n",
    "                    memory_usage = self._estimate_memory_usage(\n",
    "                        embed_dim, num_comp, comp_dim\n",
    "                    )\n",
    "                    \n",
    "                    if memory_usage > memory_budget_gb:\n",
    "                        continue\n",
    "                    \n",
    "                    # Estimate performance\n",
    "                    performance = self._estimate_performance(\n",
    "                        embed_dim, num_comp, comp_dim\n",
    "                    )\n",
    "                    \n",
    "                    # Estimate accuracy (simplified)\n",
    "                    accuracy = self._estimate_accuracy(\n",
    "                        embed_dim, num_comp, comp_dim\n",
    "                    )\n",
    "                    \n",
    "                    # Check constraints\n",
    "                    if (performance['latency_ms'] <= target_latency_ms and \n",
    "                        accuracy >= target_accuracy):\n",
    "                        \n",
    "                        # Score function (higher is better)\n",
    "                        score = (accuracy * performance['throughput'] / \n",
    "                                performance['latency_ms'] / memory_usage)\n",
    "                        \n",
    "                        config = {\n",
    "                            'embedding_dim': embed_dim,\n",
    "                            'num_components': num_comp,\n",
    "                            'component_dim': comp_dim,\n",
    "                            'memory_usage_gb': memory_usage,\n",
    "                            'estimated_accuracy': accuracy,\n",
    "                            'estimated_latency_ms': performance['latency_ms'],\n",
    "                            'estimated_throughput': performance['throughput'],\n",
    "                            'score': score\n",
    "                        }\n",
    "                        \n",
    "                        results.append(config)\n",
    "                        \n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_config = config\n",
    "        \n",
    "        # Sort results by score\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        print(f\"\\n🏆 Best Configuration:\")\n",
    "        if best_config:\n",
    "            for key, value in best_config.items():\n",
    "                print(f\"   {key}: {value}\")\n",
    "        else:\n",
    "            print(\"   No configuration meets the constraints\")\n",
    "        \n",
    "        print(f\"\\n📊 Top 5 Configurations:\")\n",
    "        for i, config in enumerate(results[:5]):\n",
    "            print(f\"   {i+1}. Score={config['score']:.3f}, \"\n",
    "                  f\"Dim={config['embedding_dim']}, \"\n",
    "                  f\"Comp={config['num_components']}, \"\n",
    "                  f\"CompDim={config['component_dim']}, \"\n",
    "                  f\"Acc={config['estimated_accuracy']:.3f}, \"\n",
    "                  f\"Lat={config['estimated_latency_ms']:.1f}ms\")\n",
    "        \n",
    "        return {\n",
    "            'best_config': best_config,\n",
    "            'all_configs': results[:10],  # Top 10\n",
    "            'optimization_summary': {\n",
    "                'total_evaluated': len(results),\n",
    "                'target_accuracy': target_accuracy,\n",
    "                'target_latency_ms': target_latency_ms,\n",
    "                'memory_budget_gb': memory_budget_gb\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _estimate_memory_usage(self, embedding_dim: int, \n",
    "                              num_components: int, component_dim: int) -> float:\n",
    "        \"\"\"\n",
    "        Estimate memory usage in GB\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        params_per_component = 2 * embedding_dim * component_dim  # Query + item embeddings\n",
    "        gating_params = embedding_dim * 2 * 128 + 128 * num_components\n",
    "        total_params = num_components * params_per_component + gating_params\n",
    "        \n",
    "        # Assume float32 (4 bytes per parameter)\n",
    "        model_memory = total_params * 4\n",
    "        \n",
    "        # Add typical activation memory (estimated)\n",
    "        activation_memory = num_components * component_dim * 1000 * 4  # Rough estimate\n",
    "        \n",
    "        total_memory = model_memory + activation_memory\n",
    "        return total_memory / (1024**3)  # Convert to GB\n",
    "    \n",
    "    def _estimate_performance(self, embedding_dim: int, \n",
    "                            num_components: int, component_dim: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Estimate performance characteristics\n",
    "        \"\"\"\n",
    "        analysis = self.analyzer.compute_mol_ai(embedding_dim, num_components, component_dim)\n",
    "        \n",
    "        # Simplified performance model\n",
    "        # Assumes typical batch size and item count\n",
    "        batch_size = 32\n",
    "        num_items = 1000\n",
    "        \n",
    "        total_flops = batch_size * num_items * analysis['total_flops']\n",
    "        \n",
    "        # Estimate latency based on whether compute or memory bound\n",
    "        if analysis['is_compute_bound_cached']:\n",
    "            # Compute bound: limited by FLOPS\n",
    "            latency_s = total_flops / (self.hardware.peak_flops * 1e12 * 0.7)  # 70% efficiency\n",
    "        else:\n",
    "            # Memory bound: limited by bandwidth\n",
    "            bytes_needed = batch_size * num_items * analysis['input_bytes']\n",
    "            latency_s = bytes_needed / (self.hardware.memory_bandwidth * 1e9 * 0.8)  # 80% efficiency\n",
    "        \n",
    "        latency_ms = latency_s * 1000\n",
    "        throughput = batch_size / latency_s  # queries per second\n",
    "        \n",
    "        return {\n",
    "            'latency_ms': latency_ms,\n",
    "            'throughput': throughput,\n",
    "            'is_compute_bound': analysis['is_compute_bound_cached']\n",
    "        }\n",
    "    \n",
    "    def _estimate_accuracy(self, embedding_dim: int, \n",
    "                          num_components: int, component_dim: int) -> float:\n",
    "        \"\"\"\n",
    "        Estimate accuracy based on model capacity (simplified)\n",
    "        \"\"\"\n",
    "        # Simplified model: accuracy increases with capacity but saturates\n",
    "        total_params = num_components * 2 * embedding_dim * component_dim\n",
    "        \n",
    "        # Sigmoid-like function for accuracy\n",
    "        base_accuracy = 0.7\n",
    "        max_improvement = 0.28\n",
    "        saturation_point = 1e6  # Parameters\n",
    "        \n",
    "        improvement = max_improvement * (1 - math.exp(-total_params / saturation_point))\n",
    "        accuracy = base_accuracy + improvement\n",
    "        \n",
    "        return min(accuracy, 0.99)  # Cap at 99%\n",
    "    \n",
    "    def generate_deployment_recommendations(self, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate deployment recommendations for the optimized configuration\n",
    "        \"\"\"\n",
    "        recommendations = {\n",
    "            'batch_size_recommendations': self._recommend_batch_size(config),\n",
    "            'memory_optimizations': self._recommend_memory_optimizations(config),\n",
    "            'compute_optimizations': self._recommend_compute_optimizations(config),\n",
    "            'monitoring_metrics': self._recommend_monitoring_metrics(config)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n📋 Deployment Recommendations:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for category, recs in recommendations.items():\n",
    "            print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "            for rec in recs:\n",
    "                print(f\"  • {rec}\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _recommend_batch_size(self, config: Dict) -> List[str]:\n",
    "        return [\n",
    "            f\"Optimal batch size: 32-64 for {config['embedding_dim']}d embeddings\",\n",
    "            \"Use dynamic batching to maintain consistent latency\",\n",
    "            \"Monitor memory usage to avoid OOM errors\"\n",
    "        ]\n",
    "    \n",
    "    def _recommend_memory_optimizations(self, config: Dict) -> List[str]:\n",
    "        return [\n",
    "            \"Use mixed precision (float16) to reduce memory usage\",\n",
    "            \"Enable gradient checkpointing if training\",\n",
    "            \"Use model parallel processing for large models\",\n",
    "            f\"Reserve {config['memory_usage_gb']*1.5:.1f}GB GPU memory\"\n",
    "        ]\n",
    "    \n",
    "    def _recommend_compute_optimizations(self, config: Dict) -> List[str]:\n",
    "        return [\n",
    "            \"Use torch.compile() for JIT optimization\",\n",
    "            \"Enable tensor fusion where possible\",\n",
    "            \"Use CUDA streams for overlapping computation\",\n",
    "            f\"Optimize for {config['num_components']} component parallelism\"\n",
    "        ]\n",
    "    \n",
    "    def _recommend_monitoring_metrics(self, config: Dict) -> List[str]:\n",
    "        return [\n",
    "            f\"Target latency: <{config['estimated_latency_ms']:.0f}ms (95th percentile)\",\n",
    "            f\"Target throughput: >{config['estimated_throughput']:.0f} queries/sec\",\n",
    "            \"Monitor GPU utilization (target: >80%)\",\n",
    "            \"Track memory usage and fragmentation\"\n",
    "        ]\n",
    "\n",
    "# Run hardware-aware optimization\n",
    "optimizer = HardwareAwareOptimizer(a100_profile)\n",
    "\n",
    "optimization_results = optimizer.optimize_configuration(\n",
    "    target_accuracy=0.93,\n",
    "    target_latency_ms=30,\n",
    "    memory_budget_gb=2.0\n",
    ")\n",
    "\n",
    "if optimization_results['best_config']:\n",
    "    deployment_recs = optimizer.generate_deployment_recommendations(\n",
    "        optimization_results['best_config']\n",
    "    )\nelse:\n",
    "    print(\"\\n⚠️ No optimal configuration found with given constraints\")\n",
    "\n",
    "print(\"\\n🎯 Hardware-aware optimization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Phần 5: Comprehensive Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive hardware performance visualization\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 20))\n",
    "\n",
    "# 1. Arithmetic Intensity Comparison\n",
    "ai_configs = [f\"D{r['dot_product']['flops_per_similarity']//128}C{i}\" for i, r in enumerate(comparison_results)]\n",
    "dot_ais = [r['dot_product']['arithmetic_intensity'] for r in comparison_results]\n",
    "mol_ais = [r['mol']['ai_cached'] for r in comparison_results]\n",
    "\n",
    "x = np.arange(len(ai_configs))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0, 0].bar(x - width/2, dot_ais, width, label='Dot Product', alpha=0.7, color='orange')\n",
    "bars2 = axes[0, 0].bar(x + width/2, mol_ais, width, label='MoL (Cached)', alpha=0.7, color='green')\n",
    "\n",
    "axes[0, 0].set_title('Arithmetic Intensity Comparison')\n",
    "axes[0, 0].set_xlabel('Configuration')\n",
    "axes[0, 0].set_ylabel('Operations/Byte')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(ai_configs, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add compute bound threshold line\n",
    "if comparison_results:\n",
    "    threshold = comparison_results[0]['dot_product']['compute_bound_threshold']\n",
    "    axes[0, 0].axhline(y=threshold, color='red', linestyle='--', \n",
    "                      label=f'Compute Bound Threshold ({threshold:.1f})', alpha=0.7)\n",
    "\n",
    "# 2. Memory Bandwidth Results (if available)\n",
    "if memory_results and 'sizes' in memory_results:\n",
    "    sizes = memory_results['sizes']\n",
    "    seq_bw = memory_results['sequential_bw']\n",
    "    rand_bw = memory_results['random_bw']\n",
    "    \n",
    "    axes[0, 1].plot(sizes, seq_bw, 'o-', label='Sequential', linewidth=2)\n",
    "    axes[0, 1].plot(sizes, rand_bw, 's-', label='Random', linewidth=2)\n",
    "    axes[0, 1].set_title('Memory Bandwidth vs Access Pattern')\n",
    "    axes[0, 1].set_xlabel('Data Size')\n",
    "    axes[0, 1].set_ylabel('Bandwidth (GB/s)')\n",
    "    axes[0, 1].set_xscale('log')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\nelse:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Memory bandwidth\\ndata not available', \n",
    "                   ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "\n",
    "# 3. Compute Throughput Results\n",
    "if compute_results and 'operations' in compute_results:\n",
    "    ops = compute_results['operations']\n",
    "    throughputs = compute_results['throughput_tflops']\n",
    "    \n",
    "    bars = axes[0, 2].bar(ops, throughputs, alpha=0.7, color='purple')\n",
    "    axes[0, 2].set_title('Compute Throughput by Operation')\n",
    "    axes[0, 2].set_ylabel('TFLOPS')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, val in zip(bars, throughputs):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                       f'{val:.1f}', ha='center', va='bottom')\n",
    "else:\n",
    "    axes[0, 2].text(0.5, 0.5, 'Compute throughput\\ndata not available', \n",
    "                   ha='center', va='center', transform=axes[0, 2].transAxes)\n",
    "\n",
    "# 4. MoL vs Dot Product Performance\n",
    "if mol_benchmark_results and 'configs' in mol_benchmark_results:\n",
    "    config_labels = [f\"B{c['batch_q']}I{c['batch_i']}D{c['embedding_dim']}C{c['num_components']}\" \n",
    "                    for c in mol_benchmark_results['configs']]\n",
    "    mol_times = mol_benchmark_results['mol_times']\n",
    "    dot_times = mol_benchmark_results['dot_times']\n",
    "    \n",
    "    x = np.arange(len(config_labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, mol_times, width, label='MoL', alpha=0.7)\n",
    "    axes[1, 0].bar(x + width/2, dot_times, width, label='Dot Product', alpha=0.7)\n",
    "    axes[1, 0].set_title('Execution Time Comparison')\n",
    "    axes[1, 0].set_ylabel('Time (seconds)')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(config_labels, rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\nelse:\n",
    "    axes[1, 0].text(0.5, 0.5, 'MoL benchmark\\ndata not available', \n",
    "                   ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# 5. FLOPS Comparison\n",
    "if mol_benchmark_results and 'mol_tflops' in mol_benchmark_results:\n",
    "    mol_tflops = mol_benchmark_results['mol_tflops']\n",
    "    dot_tflops = mol_benchmark_results['dot_tflops']\n",
    "    \n",
    "    axes[1, 1].bar(x - width/2, mol_tflops, width, label='MoL', alpha=0.7)\n",
    "    axes[1, 1].bar(x + width/2, dot_tflops, width, label='Dot Product', alpha=0.7)\n",
    "    axes[1, 1].set_title('Computational Throughput (TFLOPS)')\n",
    "    axes[1, 1].set_ylabel('TFLOPS')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(config_labels, rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\nelse:\n",
    "    axes[1, 1].text(0.5, 0.5, 'FLOPS data\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "\n",
    "# 6. Memory Usage Breakdown\n",
    "if memory_usage_results:\n",
    "    memory_categories = ['Data', 'Model', 'Peak']\n",
    "    memory_values = [\n",
    "        memory_usage_results.get('data_memory_mb', 0),\n",
    "        memory_usage_results.get('model_memory_mb', 0),\n",
    "        memory_usage_results.get('peak_memory_mb', 0)\n",
    "    ]\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen', 'salmon']\n",
    "    bars = axes[1, 2].bar(memory_categories, memory_values, color=colors, alpha=0.7)\n",
    "    axes[1, 2].set_title('Memory Usage Breakdown')\n",
    "    axes[1, 2].set_ylabel('Memory (MB)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add efficiency annotation\n",
    "    efficiency = memory_usage_results.get('memory_efficiency', 0)\n",
    "    axes[1, 2].text(0.5, 0.95, f'Efficiency: {efficiency:.2f}', \n",
    "                   transform=axes[1, 2].transAxes, ha='center', \n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\nelse:\n",
    "    axes[1, 2].text(0.5, 0.5, 'Memory usage\\ndata not available', \n",
    "                   ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "\n",
    "# 7. Hardware Utilization Analysis\n",
    "utilization_metrics = ['Compute', 'Memory BW', 'Cache Hit', 'Parallelism']\n",
    "dot_utilization = [30, 85, 60, 40]  # Estimated values\n",
    "mol_utilization = [75, 45, 80, 85]  # Estimated values\n",
    "\n",
    "x = np.arange(len(utilization_metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[2, 0].bar(x - width/2, dot_utilization, width, label='Dot Product', alpha=0.7)\n",
    "axes[2, 0].bar(x + width/2, mol_utilization, width, label='MoL', alpha=0.7)\n",
    "axes[2, 0].set_title('Hardware Resource Utilization')\n",
    "axes[2, 0].set_ylabel('Utilization (%)')\n",
    "axes[2, 0].set_xticks(x)\n",
    "axes[2, 0].set_xticklabels(utilization_metrics)\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "axes[2, 0].set_ylim(0, 100)\n",
    "\n",
    "# 8. Optimization Results (if available)\n",
    "if optimization_results and optimization_results['best_config']:\n",
    "    top_configs = optimization_results['all_configs'][:5]\n",
    "    scores = [c['score'] for c in top_configs]\n",
    "    config_names = [f\"D{c['embedding_dim']}C{c['num_components']}\" for c in top_configs]\n",
    "    \n",
    "    bars = axes[2, 1].bar(config_names, scores, alpha=0.7, color='gold')\n",
    "    axes[2, 1].set_title('Configuration Optimization Scores')\n",
    "    axes[2, 1].set_ylabel('Score')\n",
    "    axes[2, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[2, 1].grid(True, alpha=0.3)\nelse:\n",
    "    axes[2, 1].text(0.5, 0.5, 'Optimization results\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "\n",
    "# 9. Performance vs Accuracy Trade-off\n",
    "if optimization_results and optimization_results['all_configs']:\n",
    "    configs = optimization_results['all_configs'][:10]\n",
    "    latencies = [c['estimated_latency_ms'] for c in configs]\n",
    "    accuracies = [c['estimated_accuracy'] for c in configs]\n",
    "    \n",
    "    scatter = axes[2, 2].scatter(latencies, accuracies, \n",
    "                               c=range(len(latencies)), \n",
    "                               cmap='viridis', s=100, alpha=0.7)\n",
    "    \n",
    "    axes[2, 2].set_title('Performance vs Accuracy Trade-off')\n",
    "    axes[2, 2].set_xlabel('Latency (ms)')\n",
    "    axes[2, 2].set_ylabel('Estimated Accuracy')\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight best configuration\n",
    "    best_config = optimization_results['best_config']\n",
    "    if best_config:\n",
    "        axes[2, 2].scatter([best_config['estimated_latency_ms']], \n",
    "                          [best_config['estimated_accuracy']], \n",
    "                          c='red', s=200, marker='*', \n",
    "                          label='Best Config')\n",
    "        axes[2, 2].legend()\nelse:\n",
    "    axes[2, 2].text(0.5, 0.5, 'Trade-off analysis\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[2, 2].transAxes)\n",
    "\n",
    "# 10. FLOPS Breakdown for MoL\n",
    "if comparison_results:\n",
    "    mol_breakdown = comparison_results[1]['mol']['breakdown']  # Use second config\n",
    "    operations = list(mol_breakdown.keys())\n",
    "    flops = list(mol_breakdown.values())\n",
    "    \n",
    "    # Create pie chart\n",
    "    axes[3, 0].pie(flops, labels=operations, autopct='%1.1f%%', startangle=90)\n",
    "    axes[3, 0].set_title('MoL FLOPS Breakdown')\nelse:\n",
    "    axes[3, 0].text(0.5, 0.5, 'FLOPS breakdown\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[3, 0].transAxes)\n",
    "\n",
    "# 11. Scaling Analysis\n",
    "embedding_dims = [64, 128, 256, 512, 1024]\n",
    "dot_complexity = [d**2 for d in embedding_dims]\n",
    "mol_complexity = [d**2 * 8 for d in embedding_dims]  # Assume 8 components\n",
    "\n",
    "axes[3, 1].plot(embedding_dims, dot_complexity, 'o-', label='Dot Product', linewidth=2)\n",
    "axes[3, 1].plot(embedding_dims, mol_complexity, 's-', label='MoL (8 comp)', linewidth=2)\n",
    "axes[3, 1].set_title('Computational Complexity Scaling')\n",
    "axes[3, 1].set_xlabel('Embedding Dimension')\n",
    "axes[3, 1].set_ylabel('Relative FLOPS')\n",
    "axes[3, 1].set_yscale('log')\n",
    "axes[3, 1].legend()\n",
    "axes[3, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 12. Hardware Efficiency Summary\n",
    "efficiency_categories = ['AI Improvement', 'Memory Efficiency', 'Compute Utilization', 'Overall Score']\n",
    "efficiency_values = [5.2, 0.85, 0.75, 4.4]  # Example values\n",
    "\n",
    "colors = ['green' if v > 1 else 'orange' if v > 0.5 else 'red' for v in efficiency_values]\n",
    "bars = axes[3, 2].barh(efficiency_categories, efficiency_values, color=colors, alpha=0.7)\n",
    "axes[3, 2].set_title('Hardware Efficiency Summary')\n",
    "axes[3, 2].set_xlabel('Score/Ratio')\n",
    "axes[3, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, efficiency_values)):\n",
    "    width = bar.get_width()\n",
    "    axes[3, 2].text(width + 0.05, bar.get_y() + bar.get_height()/2,\n",
    "                   f'{val:.2f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Comprehensive hardware analysis visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Key Insights và Production Guidelines\n",
    "\n",
    "### 🔍 Hardware Performance Insights:\n",
    "\n",
    "1. **Arithmetic Intensity Advantage**:\n",
    "   - MoL: 8-16 operations/byte vs Dot Product: 2-4 operations/byte\n",
    "   - Higher AI → better GPU utilization → higher throughput\n",
    "   - Most beneficial on compute-heavy accelerators (A100, H100)\n",
    "\n",
    "2. **Memory Hierarchy Optimization**:\n",
    "   - Sequential access: ~1000 GB/s (good)\n",
    "   - Random access: ~300 GB/s (poor)\n",
    "   - **Key**: Design algorithms for coalesced memory access\n",
    "\n",
    "3. **Compute vs Memory Bound Transition**:\n",
    "   - Small models: Memory bound (limited by bandwidth)\n",
    "   - Large models: Compute bound (limited by FLOPS)\n",
    "   - **Sweet spot**: MoL becomes advantageous in compute-bound regime\n",
    "\n",
    "### 📖 Mathematical Foundation:\n",
    "\n",
    "**Roofline Model Analysis**:\n",
    "```\n",
    "Performance = min(Peak_FLOPS, AI × Memory_Bandwidth)\n",
    "```\n",
    "\n",
    "**MoL Advantage Condition**:\n",
    "```\n",
    "AI_MoL > Peak_FLOPS / Memory_Bandwidth\n",
    "```\n",
    "\n",
    "**Optimal Configuration**:\n",
    "```\n",
    "Components = f(Memory_Budget, Target_Latency, Hardware_Specs)\n",
    "```\n",
    "\n",
    "### 🚀 Production Optimization Strategies:\n",
    "\n",
    "1. **Model Architecture**:\n",
    "   ```python\n",
    "   # Optimal for A100-class GPUs\n",
    "   optimal_config = {\n",
    "       'embedding_dim': 256,\n",
    "       'num_components': 8,\n",
    "       'component_dim': 64,\n",
    "       'batch_size': 64\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Memory Layout Optimization**:\n",
    "   ```python\n",
    "   # Use tensor cores for mixed precision\n",
    "   model = model.half()  # FP16\n",
    "   \n",
    "   # Optimize memory access patterns\n",
    "   torch.backends.cudnn.benchmark = True\n",
    "   \n",
    "   # Use memory mapping for large datasets\n",
    "   dataset = torch.utils.data.DataLoader(\n",
    "       ..., pin_memory=True, num_workers=4\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Kernel Fusion**:\n",
    "   ```python\n",
    "   # Use torch.compile for automatic optimization\n",
    "   model = torch.compile(model, mode='max-autotune')\n",
    "   \n",
    "   # Manual fusion for critical paths\n",
    "   @torch.jit.script\n",
    "   def fused_mol_component(q, i, weight):\n",
    "       return weight * torch.sum(q * i, dim=-1)\n",
    "   ```\n",
    "\n",
    "4. **Multi-GPU Scaling**:\n",
    "   ```python\n",
    "   # Model parallelism for large models\n",
    "   model = torch.nn.DataParallel(model)\n",
    "   \n",
    "   # Pipeline parallelism for batch processing\n",
    "   # Split computation across multiple GPUs\n",
    "   ```\n",
    "\n",
    "### ⚡ Performance Monitoring:\n",
    "\n",
    "1. **Key Metrics**:\n",
    "   ```python\n",
    "   metrics = {\n",
    "       'gpu_utilization': '>80%',\n",
    "       'memory_utilization': '<90%',\n",
    "       'arithmetic_intensity': '>8.0',\n",
    "       'cache_hit_rate': '>70%',\n",
    "       'latency_p99': '<50ms'\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Profiling Tools**:\n",
    "   ```bash\n",
    "   # NVIDIA profiling\n",
    "   nsys profile python train.py\n",
    "   ncu --set full python inference.py\n",
    "   \n",
    "   # PyTorch profiling\n",
    "   torch.profiler.profile(with_stack=True)\n",
    "   ```\n",
    "\n",
    "### 🎯 Hardware-Specific Recommendations:\n",
    "\n",
    "1. **A100/H100 (High-end)**:\n",
    "   - Use large models (8-16 components)\n",
    "   - Enable tensor core utilization\n",
    "   - Optimize for compute-bound regime\n",
    "\n",
    "2. **RTX 4090 (Mid-range)**:\n",
    "   - Moderate model size (4-8 components)\n",
    "   - Balance compute and memory optimization\n",
    "   - Use mixed precision\n",
    "\n",
    "3. **T4/V100 (Budget)**:\n",
    "   - Small models (2-4 components)\n",
    "   - Focus on memory efficiency\n",
    "   - Use quantization techniques\n",
    "\n",
    "### ⚠️ Common Performance Pitfalls:\n",
    "\n",
    "1. **Memory Bandwidth Bottlenecks**:\n",
    "   - Symptom: Low GPU utilization (<50%)\n",
    "   - Solution: Increase arithmetic intensity, reduce data transfers\n",
    "\n",
    "2. **Poor Memory Access Patterns**:\n",
    "   - Symptom: Low memory bandwidth utilization\n",
    "   - Solution: Coalesce memory accesses, use tiling\n",
    "\n",
    "3. **Suboptimal Batch Sizes**:\n",
    "   - Symptom: Underutilized compute units\n",
    "   - Solution: Profile different batch sizes, use dynamic batching\n",
    "\n",
    "4. **Synchronization Overhead**:\n",
    "   - Symptom: Frequent CPU-GPU sync points\n",
    "   - Solution: Use async operations, overlap computation\n",
    "\n",
    "### 🏆 Production Checklist:\n",
    "\n",
    "- ✅ **Model Configuration**: Optimized for target hardware\n",
    "- ✅ **Memory Layout**: Coalesced access patterns\n",
    "- ✅ **Precision**: Mixed precision where appropriate\n",
    "- ✅ **Batching**: Optimal batch sizes determined\n",
    "- ✅ **Profiling**: Performance bottlenecks identified\n",
    "- ✅ **Monitoring**: Real-time metrics in place\n",
    "- ✅ **Scaling**: Multi-GPU strategy defined\n",
    "- ✅ **Fallbacks**: CPU/smaller model alternatives ready\n",
    "\n",
    "### 📚 Advanced Topics:\n",
    "\n",
    "1. **Custom CUDA Kernels**: For ultimate performance\n",
    "2. **Graph Optimization**: TensorRT, torch.fx\n",
    "3. **Dynamic Shapes**: Handle variable input sizes\n",
    "4. **Quantization**: INT8/INT4 for inference\n",
    "5. **Sparsity**: Leverage structured sparsity patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}