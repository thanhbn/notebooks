{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Topic Modeling for Quality Evaluation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook explores the innovative semi-automated approach for evaluating comment quality at scale using topic modeling. We'll dive deep into how BERTopic and semantic clustering enable quality assessment of thousands of generated comments without exhaustive manual review.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to use topic modeling to cluster similar code review comments\n",
    "2. The theory behind BERTopic and why it outperforms traditional methods\n",
    "3. How to evaluate information and relevance scores efficiently\n",
    "4. Practical implementation of semi-automated quality assessment\n",
    "\n",
    "**Paper Reference**: Section VI-C - Overall Evaluation using Topic Modeling (RQ3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Foundation\n",
    "\n",
    "### 1.1 The Challenge of Quality Evaluation at Scale\n",
    "\n",
    "The paper identifies a critical challenge: manually evaluating thousands of generated comments is infeasible. Traditional metrics like BLEU only measure lexical similarity, not actual quality.\n",
    "\n",
    "**Quality Dimensions** (Section VI-A):\n",
    "1. **Information Score (1-5)**: How informative is the comment for code revision?\n",
    "   - 5: Explicitly points out issues with concrete suggestions\n",
    "   - 1: Purely seeks clarification without feedback\n",
    "\n",
    "2. **Relevance Score (1-3)**: How related is the comment to the code change?\n",
    "   - 3: Explicitly references specific code elements\n",
    "   - 1: Implicitly related or off-topic\n",
    "\n",
    "### 1.2 Why Topic Modeling?\n",
    "\n",
    "Topic modeling enables:\n",
    "- **Clustering similar comments** → Evaluate representatives instead of all\n",
    "- **Semantic grouping** → Comments with similar intent grouped together\n",
    "- **Scalable evaluation** → 50 clusters instead of 10,000+ individual comments\n",
    "\n",
    "### 1.3 BERTopic vs Traditional Methods\n",
    "\n",
    "The paper chooses BERTopic over LDA because:\n",
    "1. **Contextual embeddings**: Uses transformer models for better semantic understanding\n",
    "2. **Dynamic clustering**: Doesn't require pre-specifying number of topics\n",
    "3. **Code-aware**: Can use code-specific embedding models (CodeT5+)\n",
    "4. **High coherence**: Produces more interpretable topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Topic modeling\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# For embeddings and clustering\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For coherence calculation\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Visualization\n",
    "from wordcloud import WordCloud\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Quality Metrics\n",
    "\n",
    "Let's implement the information and relevance scoring systems from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityMetrics:\n",
    "    \"\"\"Implementation of quality metrics from Section VI-A\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def information_score_rubric() -> Dict[int, Dict]:\n",
    "        \"\"\"Information score rubric (1-5 scale)\"\"\"\n",
    "        return {\n",
    "            5: {\n",
    "                \"description\": \"Very informative with concrete suggestions\",\n",
    "                \"examples\": [\n",
    "                    \"This method violates SRP. Extract validation logic into validateUser() method\",\n",
    "                    \"Replace magic number 86400 with constant SECONDS_PER_DAY for clarity\",\n",
    "                    \"Add try-catch to handle potential NullPointerException when user is null\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"Identifies specific issue\",\n",
    "                    \"Provides concrete solution\",\n",
    "                    \"Explains reasoning\"\n",
    "                ]\n",
    "            },\n",
    "            4: {\n",
    "                \"description\": \"Informative with clear direction\",\n",
    "                \"examples\": [\n",
    "                    \"Consider using dependency injection here\",\n",
    "                    \"This could be simplified with a ternary operator\",\n",
    "                    \"Add error handling for edge cases\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"Clear suggestion\",\n",
    "                    \"Actionable feedback\",\n",
    "                    \"May lack specifics\"\n",
    "                ]\n",
    "            },\n",
    "            3: {\n",
    "                \"description\": \"Moderately informative\",\n",
    "                \"examples\": [\n",
    "                    \"This method is too long\",\n",
    "                    \"Consider refactoring this\",\n",
    "                    \"Add documentation\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"General guidance\",\n",
    "                    \"Lacks specific solution\",\n",
    "                    \"Still actionable\"\n",
    "                ]\n",
    "            },\n",
    "            2: {\n",
    "                \"description\": \"Minimally informative\",\n",
    "                \"examples\": [\n",
    "                    \"This looks complex\",\n",
    "                    \"Not sure about this approach\",\n",
    "                    \"Could be better\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"Vague feedback\",\n",
    "                    \"No clear action\",\n",
    "                    \"Opinion without solution\"\n",
    "                ]\n",
    "            },\n",
    "            1: {\n",
    "                \"description\": \"Not informative (seeks clarification)\",\n",
    "                \"examples\": [\n",
    "                    \"Why do we need this?\",\n",
    "                    \"What does this do?\",\n",
    "                    \"???\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"Questions without suggestions\",\n",
    "                    \"No actionable feedback\",\n",
    "                    \"Purely clarification\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def relevance_score_rubric() -> Dict[int, Dict]:\n",
    "        \"\"\"Relevance score rubric (1-3 scale)\"\"\"\n",
    "        return {\n",
    "            3: {\n",
    "                \"description\": \"Highly relevant - explicitly references code\",\n",
    "                \"examples\": [\n",
    "                    \"The variable 'userId' should be renamed to 'userIdentifier'\",\n",
    "                    \"Line 45: Add null check before calling user.getName()\",\n",
    "                    \"The getUserData() method needs error handling\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"References specific code elements\",\n",
    "                    \"Mentions variable/method names\",\n",
    "                    \"May include line numbers\"\n",
    "                ]\n",
    "            },\n",
    "            2: {\n",
    "                \"description\": \"Moderately relevant - implicitly related\",\n",
    "                \"examples\": [\n",
    "                    \"Add error handling here\",\n",
    "                    \"This needs documentation\",\n",
    "                    \"Consider performance impact\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"General reference to code area\",\n",
    "                    \"No specific code elements\",\n",
    "                    \"Still contextually appropriate\"\n",
    "                ]\n",
    "            },\n",
    "            1: {\n",
    "                \"description\": \"Low relevance - generic or off-topic\",\n",
    "                \"examples\": [\n",
    "                    \"Follow coding standards\",\n",
    "                    \"This could be improved\",\n",
    "                    \"LGTM\"\n",
    "                ],\n",
    "                \"characteristics\": [\n",
    "                    \"Generic feedback\",\n",
    "                    \"Could apply to any code\",\n",
    "                    \"No clear connection to diff\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Display rubrics\n",
    "metrics = QualityMetrics()\n",
    "\n",
    "print(\"=== Information Score Rubric ===\")\n",
    "for score, details in metrics.information_score_rubric().items():\n",
    "    print(f\"\\nScore {score}: {details['description']}\")\n",
    "    print(f\"Example: '{details['examples'][0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Quality Scorer\n",
    "\n",
    "Let's build an automated scorer that approximates manual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatedQualityScorer:\n",
    "    \"\"\"Automated scoring based on comment characteristics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Keywords indicating different quality levels\n",
    "        self.action_keywords = [\n",
    "            'should', 'consider', 'suggest', 'recommend', 'must',\n",
    "            'add', 'remove', 'extract', 'refactor', 'rename',\n",
    "            'replace', 'use', 'implement', 'fix', 'update'\n",
    "        ]\n",
    "        \n",
    "        self.concrete_keywords = [\n",
    "            'method', 'function', 'variable', 'class', 'constant',\n",
    "            'parameter', 'return', 'exception', 'import', 'package'\n",
    "        ]\n",
    "        \n",
    "        self.vague_keywords = [\n",
    "            'maybe', 'perhaps', 'not sure', 'hmm', 'weird',\n",
    "            'strange', 'confusing', 'unclear', 'complicated'\n",
    "        ]\n",
    "        \n",
    "        self.question_keywords = [\n",
    "            'why', 'what', 'how', 'when', 'where', 'which',\n",
    "            '?', 'clarify', 'explain'\n",
    "        ]\n",
    "    \n",
    "    def score_information(self, comment: str) -> int:\n",
    "        \"\"\"Score comment informativeness (1-5)\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "        score = 1  # Base score\n",
    "        \n",
    "        # Check for action words (+1)\n",
    "        if any(word in comment_lower for word in self.action_keywords):\n",
    "            score += 1\n",
    "        \n",
    "        # Check for concrete technical terms (+1)\n",
    "        if any(word in comment_lower for word in self.concrete_keywords):\n",
    "            score += 1\n",
    "        \n",
    "        # Check for specific suggestions (+1)\n",
    "        if ('instead' in comment_lower or 'rather than' in comment_lower or \n",
    "            '->' in comment or '=>' in comment):\n",
    "            score += 1\n",
    "        \n",
    "        # Check for reasoning (+1)\n",
    "        if any(word in comment_lower for word in ['because', 'since', 'improve', 'better', 'cleaner']):\n",
    "            score += 1\n",
    "        \n",
    "        # Penalize vague language (-1)\n",
    "        if any(word in comment_lower for word in self.vague_keywords):\n",
    "            score = max(1, score - 1)\n",
    "        \n",
    "        # Penalize pure questions (-1)\n",
    "        if comment.strip().endswith('?') and len(comment.split()) < 10:\n",
    "            score = max(1, score - 1)\n",
    "        \n",
    "        return min(5, score)\n",
    "    \n",
    "    def score_relevance(self, comment: str, code_diff: Optional[str] = None) -> int:\n",
    "        \"\"\"Score comment relevance to code (1-3)\"\"\"\n",
    "        score = 1  # Base score\n",
    "        \n",
    "        if not code_diff:\n",
    "            # Without code diff, use heuristics\n",
    "            if any(word in comment for word in ['line', 'Line', 'L']):\n",
    "                score = 3\n",
    "            elif re.search(r'\\b\\w+\\(\\)', comment):  # Method calls\n",
    "                score = 3\n",
    "            elif re.search(r'\\$\\w+|\\w+\\$', comment):  # Variable references\n",
    "                score = 3\n",
    "            elif any(word in comment.lower() for word in self.concrete_keywords):\n",
    "                score = 2\n",
    "        else:\n",
    "            # With code diff, check overlap\n",
    "            code_tokens = set(re.findall(r'\\b\\w+\\b', code_diff))\n",
    "            comment_tokens = set(re.findall(r'\\b\\w+\\b', comment))\n",
    "            \n",
    "            overlap = len(code_tokens.intersection(comment_tokens))\n",
    "            if overlap >= 3:\n",
    "                score = 3\n",
    "            elif overlap >= 1:\n",
    "                score = 2\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def score_batch(self, comments: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Score a batch of comments\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for item in comments:\n",
    "            comment = item['comment']\n",
    "            code_diff = item.get('code_diff', None)\n",
    "            \n",
    "            info_score = self.score_information(comment)\n",
    "            rel_score = self.score_relevance(comment, code_diff)\n",
    "            \n",
    "            results.append({\n",
    "                'comment': comment,\n",
    "                'information_score': info_score,\n",
    "                'relevance_score': rel_score,\n",
    "                'quality_score': (info_score / 5 * 0.7) + (rel_score / 3 * 0.3)  # Weighted\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Test the scorer\n",
    "import re\n",
    "scorer = AutomatedQualityScorer()\n",
    "\n",
    "test_comments = [\n",
    "    {\"comment\": \"Why do we have this flag?\"},\n",
    "    {\"comment\": \"Consider extracting this logic into a separate validateUser() method for better modularity\"},\n",
    "    {\"comment\": \"The variable userId should be renamed to userIdentifier for consistency\"},\n",
    "    {\"comment\": \"What does this do?\"},\n",
    "    {\"comment\": \"Add null check before accessing user.getName() to prevent NPE\"}\n",
    "]\n",
    "\n",
    "scores_df = scorer.score_batch(test_comments)\n",
    "print(\"Sample Scoring Results:\")\n",
    "print(scores_df.to_string(index=False, max_colwidth=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating Mock Generated Comments Dataset\n",
    "\n",
    "Let's create datasets simulating comments from original and cleaned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_comments(model_type: str, n_comments: int = 1000) -> List[Dict]:\n",
    "    \"\"\"Generate mock comments simulating different model outputs\"\"\"\n",
    "    \n",
    "    # Templates based on model type (reflecting paper's findings)\n",
    "    if model_type == \"original\":\n",
    "        # Original model: mix of valid and noisy patterns\n",
    "        templates = [\n",
    "            # Noisy (40%)\n",
    "            \"Why this change?\",\n",
    "            \"What is the purpose of this?\",\n",
    "            \"I don't understand this\",\n",
    "            \"Is this necessary?\",\n",
    "            \"???\",\n",
    "            \"Not sure about this\",\n",
    "            \"This looks weird\",\n",
    "            \"hmm\",\n",
    "            \n",
    "            # Semi-valid (30%)\n",
    "            \"Consider refactoring\",\n",
    "            \"This could be improved\",\n",
    "            \"Add documentation\",\n",
    "            \"Check error handling\",\n",
    "            \"Review this logic\",\n",
    "            \n",
    "            # Valid (30%)\n",
    "            \"Extract this into a separate method\",\n",
    "            \"Use a constant instead of magic number {}\",\n",
    "            \"Add null check for {}\",\n",
    "            \"Rename {} to {} for clarity\",\n",
    "            \"This violates single responsibility principle\"\n",
    "        ]\n",
    "        weights = [0.05] * 8 + [0.06] * 5 + [0.06] * 5\n",
    "        \n",
    "    elif model_type == \"cleaned\":\n",
    "        # Cleaned model: mostly valid, specific suggestions\n",
    "        templates = [\n",
    "            # High quality (60%)\n",
    "            \"Extract {} logic into separate {} method for better modularity\",\n",
    "            \"Replace magic number {} with constant {} for maintainability\",\n",
    "            \"Add try-catch to handle {} when {} is null\",\n",
    "            \"Rename variable {} to {} to follow naming conventions\",\n",
    "            \"This method violates SRP. Split into {} and {}\",\n",
    "            \"Use dependency injection instead of creating {} directly\",\n",
    "            \"Consider using {} pattern here for better extensibility\",\n",
    "            \"Add unit tests to cover the edge case when {} is {}\",\n",
    "            \n",
    "            # Medium quality (30%)\n",
    "            \"This could be simplified using {}\",\n",
    "            \"Consider caching {} for performance\",\n",
    "            \"Add validation for {} parameter\",\n",
    "            \"Document the purpose of this {}\",\n",
    "            \n",
    "            # Low quality (10%)\n",
    "            \"Refactor this method\",\n",
    "            \"Improve naming\"\n",
    "        ]\n",
    "        weights = [0.075] * 8 + [0.075] * 4 + [0.05] * 2\n",
    "    \n",
    "    # Generate comments\n",
    "    comments = []\n",
    "    \n",
    "    # Common fill values\n",
    "    variables = ['userId', 'userData', 'config', 'result', 'response']\n",
    "    methods = ['validate', 'process', 'calculate', 'fetch', 'update']\n",
    "    patterns = ['Factory', 'Observer', 'Strategy', 'Singleton']\n",
    "    numbers = ['3600', '86400', '1000', '256', '42']\n",
    "    \n",
    "    for i in range(n_comments):\n",
    "        template = np.random.choice(templates, p=weights)\n",
    "        \n",
    "        # Fill template with random values\n",
    "        if '{}' in template:\n",
    "            n_placeholders = template.count('{}')\n",
    "            if 'magic number' in template:\n",
    "                values = [np.random.choice(numbers), \n",
    "                         f\"{np.random.choice(['MAX_', 'DEFAULT_', ''])}{np.random.choice(['TIMEOUT', 'SIZE', 'COUNT'])}\"]\n",
    "            elif 'variable' in template or 'Rename' in template:\n",
    "                values = [np.random.choice(variables), \n",
    "                         np.random.choice(variables) + np.random.choice(['Id', 'Data', 'Info'])]  \n",
    "            else:\n",
    "                values = [np.random.choice(variables + methods + patterns) \n",
    "                         for _ in range(n_placeholders)]\n",
    "            \n",
    "            comment = template.format(*values[:n_placeholders])\n",
    "        else:\n",
    "            comment = template\n",
    "        \n",
    "        # Add mock code diff\n",
    "        code_diff = f\"+ {np.random.choice(variables)} = {np.random.choice(methods)}();\"\n",
    "        \n",
    "        comments.append({\n",
    "            'id': f'comment_{i}',\n",
    "            'comment': comment,\n",
    "            'code_diff': code_diff,\n",
    "            'model': model_type\n",
    "        })\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Generate datasets\n",
    "original_comments = generate_mock_comments('original', 500)\n",
    "cleaned_comments = generate_mock_comments('cleaned', 500)\n",
    "\n",
    "print(f\"Generated {len(original_comments)} comments from original model\")\n",
    "print(f\"Generated {len(cleaned_comments)} comments from cleaned model\")\n",
    "\n",
    "# Sample comments\n",
    "print(\"\\nSample from original model:\")\n",
    "for c in original_comments[:3]:\n",
    "    print(f\"  - {c['comment']}\")\n",
    "    \n",
    "print(\"\\nSample from cleaned model:\")  \n",
    "for c in cleaned_comments[:3]:\n",
    "    print(f\"  - {c['comment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing BERTopic for Comment Clustering\n",
    "\n",
    "Now let's implement the topic modeling approach from Section VI-C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentTopicModeler:\n",
    "    \"\"\"Topic modeling for code review comments using BERTopic\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_model: str = 'all-MiniLM-L6-v2',\n",
    "                 n_topics: int = 50,\n",
    "                 min_topic_size: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize topic modeler.\n",
    "        \n",
    "        Paper uses:\n",
    "        - CodeT5+ for embeddings (we'll use all-MiniLM-L6-v2 as substitute)\n",
    "        - Agglomerative clustering\n",
    "        - 50 topics for ~10k comments\n",
    "        \"\"\"\n",
    "        \n",
    "        # Embedding model\n",
    "        self.sentence_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        # Clustering model (paper uses agglomerative)\n",
    "        self.cluster_model = AgglomerativeClustering(\n",
    "            n_clusters=n_topics,\n",
    "            linkage='ward'\n",
    "        )\n",
    "        \n",
    "        # Initialize BERTopic\n",
    "        self.topic_model = BERTopic(\n",
    "            embedding_model=self.sentence_model,\n",
    "            hdbscan_model=self.cluster_model,\n",
    "            nr_topics=n_topics,\n",
    "            min_topic_size=min_topic_size,\n",
    "            calculate_probabilities=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        self.topics = None\n",
    "        self.probs = None\n",
    "        self.topic_info = None\n",
    "        \n",
    "    def fit_transform(self, comments: List[str]) -> Tuple:\n",
    "        \"\"\"Fit topic model and transform comments\"\"\"\n",
    "        \n",
    "        print(f\"Fitting topic model on {len(comments)} comments...\")\n",
    "        self.topics, self.probs = self.topic_model.fit_transform(comments)\n",
    "        \n",
    "        # Get topic information\n",
    "        self.topic_info = self.topic_model.get_topic_info()\n",
    "        \n",
    "        print(f\"Found {len(self.topic_info) - 1} topics (excluding outliers)\")\n",
    "        \n",
    "        return self.topics, self.probs\n",
    "    \n",
    "    def calculate_coherence(self, comments: List[str]) -> float:\n",
    "        \"\"\"Calculate topic coherence score\"\"\"\n",
    "        \n",
    "        # Get topics and their words\n",
    "        topics_words = []\n",
    "        for topic_id in range(len(self.topic_info) - 1):\n",
    "            if topic_id != -1:  # Skip outlier topic\n",
    "                words = self.topic_model.get_topic(topic_id)\n",
    "                topics_words.append([word[0] for word in words[:10]])\n",
    "        \n",
    "        # Tokenize documents\n",
    "        tokenized_docs = [doc.lower().split() for doc in comments]\n",
    "        \n",
    "        # Create dictionary and corpus\n",
    "        dictionary = Dictionary(tokenized_docs)\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "        \n",
    "        # Calculate coherence\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_words,\n",
    "            texts=tokenized_docs,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        \n",
    "        return coherence_model.get_coherence()\n",
    "    \n",
    "    def get_representative_comments(self, comments: List[str], n_per_topic: int = 3) -> Dict:\n",
    "        \"\"\"Get representative comments for each topic\"\"\"\n",
    "        \n",
    "        representatives = {}\n",
    "        \n",
    "        for topic_id in range(len(self.topic_info) - 1):\n",
    "            if topic_id == -1:\n",
    "                continue\n",
    "                \n",
    "            # Get indices of comments in this topic\n",
    "            topic_indices = [i for i, t in enumerate(self.topics) if t == topic_id]\n",
    "            \n",
    "            if not topic_indices:\n",
    "                continue\n",
    "            \n",
    "            # Get probabilities for these comments\n",
    "            topic_probs = [self.probs[i][topic_id] if topic_id < len(self.probs[i]) else 0 \n",
    "                          for i in topic_indices]\n",
    "            \n",
    "            # Sort by probability and get top N\n",
    "            sorted_indices = sorted(zip(topic_indices, topic_probs), \n",
    "                                  key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Get representative comments\n",
    "            reps = []\n",
    "            for idx, prob in sorted_indices[:n_per_topic]:\n",
    "                reps.append({\n",
    "                    'comment': comments[idx],\n",
    "                    'probability': prob,\n",
    "                    'index': idx\n",
    "                })\n",
    "            \n",
    "            representatives[topic_id] = reps\n",
    "        \n",
    "        return representatives\n",
    "    \n",
    "    def visualize_topics(self, comments: List[str]):\n",
    "        \"\"\"Create visualizations of topic distribution\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Topic size distribution\n",
    "        topic_sizes = self.topic_info[self.topic_info.Topic != -1]['Count'].values\n",
    "        axes[0, 0].bar(range(len(topic_sizes)), topic_sizes)\n",
    "        axes[0, 0].set_xlabel('Topic ID')\n",
    "        axes[0, 0].set_ylabel('Number of Comments')\n",
    "        axes[0, 0].set_title('Topic Size Distribution')\n",
    "        \n",
    "        # 2. Top topics\n",
    "        top_topics = self.topic_info[self.topic_info.Topic != -1].nlargest(10, 'Count')\n",
    "        axes[0, 1].barh(top_topics['Name'], top_topics['Count'])\n",
    "        axes[0, 1].set_xlabel('Number of Comments')\n",
    "        axes[0, 1].set_title('Top 10 Topics')\n",
    "        \n",
    "        # 3. Word cloud of top topic\n",
    "        if len(self.topic_info) > 1:\n",
    "            top_topic_words = self.topic_model.get_topic(0)\n",
    "            word_freq = {word: score for word, score in top_topic_words[:20]}\n",
    "            \n",
    "            wordcloud = WordCloud(width=400, height=400, \n",
    "                                 background_color='white').generate_from_frequencies(word_freq)\n",
    "            \n",
    "            axes[1, 0].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[1, 0].axis('off')\n",
    "            axes[1, 0].set_title('Top Topic Word Cloud')\n",
    "        \n",
    "        # 4. Coherence by topic\n",
    "        # Simplified visualization - in practice, calculate per-topic coherence\n",
    "        axes[1, 1].text(0.5, 0.5, f'Overall Coherence\\n{self.calculate_coherence(comments):.3f}', \n",
    "                       ha='center', va='center', fontsize=24)\n",
    "        axes[1, 1].set_xlim(0, 1)\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].axis('off')\n",
    "        axes[1, 1].set_title('Model Coherence Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Apply topic modeling to our mock comments\n",
    "all_comments_text = [c['comment'] for c in original_comments + cleaned_comments]\n",
    "\n",
    "# Initialize and fit model\n",
    "topic_modeler = CommentTopicModeler(n_topics=20)  # Fewer topics for demo\n",
    "topics, probs = topic_modeler.fit_transform(all_comments_text[:200])  # Subset for demo\n",
    "\n",
    "# Visualize results\n",
    "topic_modeler.visualize_topics(all_comments_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quality Evaluation via Topic Representatives\n",
    "\n",
    "Now let's implement the semi-automated quality evaluation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicBasedQualityEvaluator:\n",
    "    \"\"\"Evaluate quality using topic modeling approach from paper\"\"\"\n",
    "    \n",
    "    def __init__(self, topic_modeler: CommentTopicModeler, quality_scorer: AutomatedQualityScorer):\n",
    "        self.topic_modeler = topic_modeler\n",
    "        self.quality_scorer = quality_scorer\n",
    "        \n",
    "    def evaluate_representatives(self, \n",
    "                               comments_data: List[Dict],\n",
    "                               n_representatives: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate quality by scoring topic representatives.\n",
    "        \n",
    "        This implements the approach from Section VI-C:\n",
    "        1. Get top 3 representatives per topic\n",
    "        2. Score them for information and relevance\n",
    "        3. Use average as topic score\n",
    "        4. Apply to all comments in topic\n",
    "        \"\"\"\n",
    "        \n",
    "        comments_text = [c['comment'] for c in comments_data]\n",
    "        \n",
    "        # Get representatives\n",
    "        representatives = self.topic_modeler.get_representative_comments(\n",
    "            comments_text, \n",
    "            n_per_topic=n_representatives\n",
    "        )\n",
    "        \n",
    "        # Score representatives and calculate topic scores\n",
    "        topic_scores = {}\n",
    "        \n",
    "        for topic_id, reps in representatives.items():\n",
    "            info_scores = []\n",
    "            rel_scores = []\n",
    "            \n",
    "            for rep in reps:\n",
    "                idx = rep['index']\n",
    "                comment_data = comments_data[idx]\n",
    "                \n",
    "                info_score = self.quality_scorer.score_information(comment_data['comment'])\n",
    "                rel_score = self.quality_scorer.score_relevance(\n",
    "                    comment_data['comment'], \n",
    "                    comment_data.get('code_diff')\n",
    "                )\n",
    "                \n",
    "                info_scores.append(info_score)\n",
    "                rel_scores.append(rel_score)\n",
    "            \n",
    "            # Average scores for topic\n",
    "            topic_scores[topic_id] = {\n",
    "                'avg_information': np.mean(info_scores),\n",
    "                'avg_relevance': np.mean(rel_scores),\n",
    "                'n_comments': sum(1 for t in self.topic_modeler.topics if t == topic_id)\n",
    "            }\n",
    "        \n",
    "        # Apply topic scores to all comments\n",
    "        results = []\n",
    "        \n",
    "        for i, (comment_data, topic) in enumerate(zip(comments_data, self.topic_modeler.topics)):\n",
    "            if topic in topic_scores:\n",
    "                info_score = topic_scores[topic]['avg_information']\n",
    "                rel_score = topic_scores[topic]['avg_relevance']\n",
    "            else:\n",
    "                # Outliers - score individually\n",
    "                info_score = self.quality_scorer.score_information(comment_data['comment'])\n",
    "                rel_score = self.quality_scorer.score_relevance(\n",
    "                    comment_data['comment'],\n",
    "                    comment_data.get('code_diff')\n",
    "                )\n",
    "            \n",
    "            results.append({\n",
    "                'comment': comment_data['comment'][:50] + '...',\n",
    "                'model': comment_data.get('model', 'unknown'),\n",
    "                'topic': topic,\n",
    "                'information_score': info_score,\n",
    "                'relevance_score': rel_score,\n",
    "                'quality_score': (info_score / 5 * 0.7) + (rel_score / 3 * 0.3)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def compare_model_quality(self, results_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Compare quality between models\"\"\"\n",
    "        \n",
    "        comparison = {}\n",
    "        \n",
    "        for model in results_df['model'].unique():\n",
    "            model_data = results_df[results_df['model'] == model]\n",
    "            \n",
    "            comparison[model] = {\n",
    "                'avg_information': model_data['information_score'].mean(),\n",
    "                'avg_relevance': model_data['relevance_score'].mean(),\n",
    "                'avg_quality': model_data['quality_score'].mean(),\n",
    "                \n",
    "                # Distribution of scores\n",
    "                'info_distribution': model_data['information_score'].value_counts().to_dict(),\n",
    "                'rel_distribution': model_data['relevance_score'].value_counts().to_dict(),\n",
    "                \n",
    "                # Low quality percentage (info <= 2 or rel == 1)\n",
    "                'low_info_pct': (model_data['information_score'] <= 2).mean() * 100,\n",
    "                'low_rel_pct': (model_data['relevance_score'] == 1).mean() * 100\n",
    "            }\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "# Evaluate quality using topic modeling\n",
    "evaluator = TopicBasedQualityEvaluator(topic_modeler, scorer)\n",
    "\n",
    "# Combine datasets for evaluation\n",
    "all_comments_data = original_comments[:100] + cleaned_comments[:100]\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating comment quality via topic representatives...\")\n",
    "quality_results = evaluator.evaluate_representatives(all_comments_data)\n",
    "\n",
    "# Compare models\n",
    "model_comparison = evaluator.compare_model_quality(quality_results)\n",
    "\n",
    "print(\"\\n=== Model Quality Comparison ===\")\n",
    "for model, metrics in model_comparison.items():\n",
    "    print(f\"\\n{model.upper()} Model:\")\n",
    "    print(f\"  Average Information Score: {metrics['avg_information']:.2f}/5\")\n",
    "    print(f\"  Average Relevance Score: {metrics['avg_relevance']:.2f}/3\")\n",
    "    print(f\"  Low Quality Comments: {metrics['low_info_pct']:.1f}% (info), {metrics['low_rel_pct']:.1f}% (rel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Quality Improvements\n",
    "\n",
    "Let's create visualizations matching Figure 4 from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quality_improvements(quality_results: pd.DataFrame, model_comparison: Dict):\n",
    "    \"\"\"Create visualizations similar to Figure 4 in the paper\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Colors for models\n",
    "    colors = {'original': '#e74c3c', 'cleaned': '#2ecc71'}\n",
    "    \n",
    "    # 1. Information score distribution\n",
    "    for model in ['original', 'cleaned']:\n",
    "        model_data = quality_results[quality_results['model'] == model]\n",
    "        info_counts = model_data['information_score'].value_counts().sort_index()\n",
    "        \n",
    "        x = info_counts.index\n",
    "        y = info_counts.values\n",
    "        \n",
    "        axes[0, 0].bar(x + (0.2 if model == 'cleaned' else -0.2), y, \n",
    "                      width=0.4, label=model.capitalize(), \n",
    "                      color=colors[model], alpha=0.7)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Information Score')\n",
    "    axes[0, 0].set_ylabel('Number of Comments')\n",
    "    axes[0, 0].set_title('Information Score Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_xticks(range(1, 6))\n",
    "    \n",
    "    # 2. Relevance score distribution\n",
    "    for model in ['original', 'cleaned']:\n",
    "        model_data = quality_results[quality_results['model'] == model]\n",
    "        rel_counts = model_data['relevance_score'].value_counts().sort_index()\n",
    "        \n",
    "        x = rel_counts.index\n",
    "        y = rel_counts.values\n",
    "        \n",
    "        axes[0, 1].bar(x + (0.1 if model == 'cleaned' else -0.1), y,\n",
    "                      width=0.2, label=model.capitalize(),\n",
    "                      color=colors[model], alpha=0.7)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Relevance Score')\n",
    "    axes[0, 1].set_ylabel('Number of Comments')\n",
    "    axes[0, 1].set_title('Relevance Score Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].set_xticks(range(1, 4))\n",
    "    \n",
    "    # 3. Average scores comparison\n",
    "    models = list(model_comparison.keys())\n",
    "    info_scores = [model_comparison[m]['avg_information'] for m in models]\n",
    "    rel_scores = [model_comparison[m]['avg_relevance'] for m in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[1, 0].bar(x - width/2, info_scores, width, \n",
    "                           label='Information', color='#3498db')\n",
    "    bars2 = axes[1, 0].bar(x + width/2, [r * 5/3 for r in rel_scores], width,\n",
    "                           label='Relevance (scaled)', color='#9b59b6')\n",
    "    \n",
    "    axes[1, 0].set_ylabel('Average Score')\n",
    "    axes[1, 0].set_title('Average Quality Scores by Model')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels([m.capitalize() for m in models])\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_ylim(0, 5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars, scores in [(bars1, info_scores), (bars2, [r * 5/3 for r in rel_scores])]:\n",
    "        for bar, score in zip(bars, scores):\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                           f'{score:.2f}', ha='center', fontsize=10)\n",
    "    \n",
    "    # 4. Improvement percentages (matching paper's findings)\n",
    "    improvements = {\n",
    "        'Information': 24,  # From paper\n",
    "        'Relevance': 11,    # From paper  \n",
    "        'Low Info Reduction': 73,  # From paper\n",
    "        'Low Rel Reduction': 61    # From paper\n",
    "    }\n",
    "    \n",
    "    categories = list(improvements.keys())\n",
    "    values = list(improvements.values())\n",
    "    \n",
    "    bars = axes[1, 1].bar(categories, values, color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'])\n",
    "    axes[1, 1].set_ylabel('Improvement (%)')\n",
    "    axes[1, 1].set_title('Quality Improvements (Paper Results)')\n",
    "    axes[1, 1].set_xticklabels(categories, rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                       f'{val}%', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "visualize_quality_improvements(quality_results, model_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Topic Explorer\n",
    "\n",
    "Let's create an interactive way to explore topics and their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_quality_dashboard(topic_modeler: CommentTopicModeler, \n",
    "                                 quality_results: pd.DataFrame,\n",
    "                                 comments_data: List[Dict]):\n",
    "    \"\"\"Create interactive dashboard for topic exploration\"\"\"\n",
    "    \n",
    "    # Calculate topic-level quality metrics\n",
    "    topic_quality = quality_results.groupby('topic').agg({\n",
    "        'information_score': ['mean', 'std', 'count'],\n",
    "        'relevance_score': ['mean', 'std'],\n",
    "        'quality_score': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    topic_quality.columns = ['_'.join(col).strip() for col in topic_quality.columns.values]\n",
    "    topic_quality = topic_quality.reset_index()\n",
    "    \n",
    "    # Add topic names\n",
    "    topic_names = []\n",
    "    for topic_id in topic_quality['topic']:\n",
    "        if topic_id == -1:\n",
    "            topic_names.append('Outliers')\n",
    "        else:\n",
    "            # Get top words for topic\n",
    "            words = topic_modeler.topic_model.get_topic(topic_id)[:3]\n",
    "            topic_name = ', '.join([w[0] for w in words])\n",
    "            topic_names.append(f\"Topic {topic_id}: {topic_name}\")\n",
    "    \n",
    "    topic_quality['topic_name'] = topic_names\n",
    "    \n",
    "    # Display top and bottom quality topics\n",
    "    print(\"=== Topic Quality Analysis ===\")\n",
    "    print(\"\\nTop 5 Highest Quality Topics:\")\n",
    "    top_topics = topic_quality.nlargest(5, 'quality_score_mean')\n",
    "    print(top_topics[['topic_name', 'information_score_mean', \n",
    "                     'relevance_score_mean', 'information_score_count']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nBottom 5 Lowest Quality Topics:\")\n",
    "    bottom_topics = topic_quality.nsmallest(5, 'quality_score_mean')\n",
    "    print(bottom_topics[['topic_name', 'information_score_mean',\n",
    "                        'relevance_score_mean', 'information_score_count']].to_string(index=False))\n",
    "    \n",
    "    # Example comments from best and worst topics\n",
    "    print(\"\\n=== Example Comments ===\")\n",
    "    \n",
    "    best_topic = top_topics.iloc[0]['topic']\n",
    "    worst_topic = bottom_topics.iloc[0]['topic']\n",
    "    \n",
    "    print(f\"\\nBest Topic ({top_topics.iloc[0]['topic_name']}):\")\n",
    "    best_examples = [comments_data[i] for i, t in enumerate(topic_modeler.topics[:len(comments_data)]) \n",
    "                    if t == best_topic][:3]\n",
    "    for ex in best_examples:\n",
    "        print(f\"  - {ex['comment']}\")\n",
    "    \n",
    "    print(f\"\\nWorst Topic ({bottom_topics.iloc[0]['topic_name']}):\")\n",
    "    worst_examples = [comments_data[i] for i, t in enumerate(topic_modeler.topics[:len(comments_data)])\n",
    "                     if t == worst_topic][:3]\n",
    "    for ex in worst_examples:\n",
    "        print(f\"  - {ex['comment']}\")\n",
    "    \n",
    "    return topic_quality\n",
    "\n",
    "# Create dashboard\n",
    "topic_quality_df = create_topic_quality_dashboard(\n",
    "    topic_modeler,\n",
    "    quality_results,\n",
    "    all_comments_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Implementing the Complete Evaluation Pipeline\n",
    "\n",
    "Let's create a production-ready pipeline that combines all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionQualityEvaluator:\n",
    "    \"\"\"Complete pipeline for quality evaluation using topic modeling\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_model: str = 'all-MiniLM-L6-v2',\n",
    "                 n_topics: int = 50,\n",
    "                 n_representatives: int = 3):\n",
    "        \n",
    "        self.embedding_model = embedding_model\n",
    "        self.n_topics = n_topics\n",
    "        self.n_representatives = n_representatives\n",
    "        \n",
    "        # Initialize components\n",
    "        self.scorer = AutomatedQualityScorer()\n",
    "        self.topic_modeler = None\n",
    "        self.evaluator = None\n",
    "        \n",
    "    def evaluate_model_outputs(self,\n",
    "                             original_comments: List[Dict],\n",
    "                             cleaned_comments: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete evaluation pipeline:\n",
    "        1. Combine datasets\n",
    "        2. Fit topic model\n",
    "        3. Evaluate representatives\n",
    "        4. Calculate improvements\n",
    "        5. Generate report\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"=== Starting Quality Evaluation Pipeline ===\")\n",
    "        \n",
    "        # Step 1: Prepare data\n",
    "        print(\"\\n1. Preparing datasets...\")\n",
    "        all_comments = original_comments + cleaned_comments\n",
    "        all_text = [c['comment'] for c in all_comments]\n",
    "        \n",
    "        print(f\"   Total comments: {len(all_comments)}\")\n",
    "        print(f\"   Original model: {len(original_comments)}\")\n",
    "        print(f\"   Cleaned model: {len(cleaned_comments)}\")\n",
    "        \n",
    "        # Step 2: Topic modeling\n",
    "        print(\"\\n2. Fitting topic model...\")\n",
    "        self.topic_modeler = CommentTopicModeler(\n",
    "            embedding_model=self.embedding_model,\n",
    "            n_topics=self.n_topics\n",
    "        )\n",
    "        topics, probs = self.topic_modeler.fit_transform(all_text)\n",
    "        \n",
    "        # Calculate coherence\n",
    "        coherence = self.topic_modeler.calculate_coherence(all_text)\n",
    "        print(f\"   Topic coherence: {coherence:.3f}\")\n",
    "        \n",
    "        # Step 3: Quality evaluation\n",
    "        print(\"\\n3. Evaluating quality via representatives...\")\n",
    "        self.evaluator = TopicBasedQualityEvaluator(\n",
    "            self.topic_modeler,\n",
    "            self.scorer\n",
    "        )\n",
    "        \n",
    "        quality_results = self.evaluator.evaluate_representatives(\n",
    "            all_comments,\n",
    "            n_representatives=self.n_representatives\n",
    "        )\n",
    "        \n",
    "        # Step 4: Calculate improvements\n",
    "        print(\"\\n4. Calculating improvements...\")\n",
    "        model_comparison = self.evaluator.compare_model_quality(quality_results)\n",
    "        \n",
    "        # Calculate percentage improvements\n",
    "        orig_metrics = model_comparison['original']\n",
    "        clean_metrics = model_comparison['cleaned']\n",
    "        \n",
    "        improvements = {\n",
    "            'information_improvement': (\n",
    "                (clean_metrics['avg_information'] - orig_metrics['avg_information']) / \n",
    "                orig_metrics['avg_information'] * 100\n",
    "            ),\n",
    "            'relevance_improvement': (\n",
    "                (clean_metrics['avg_relevance'] - orig_metrics['avg_relevance']) /\n",
    "                orig_metrics['avg_relevance'] * 100\n",
    "            ),\n",
    "            'low_info_reduction': (\n",
    "                (orig_metrics['low_info_pct'] - clean_metrics['low_info_pct']) /\n",
    "                orig_metrics['low_info_pct'] * 100\n",
    "            ),\n",
    "            'low_rel_reduction': (\n",
    "                (orig_metrics['low_rel_pct'] - clean_metrics['low_rel_pct']) /\n",
    "                orig_metrics['low_rel_pct'] * 100 if orig_metrics['low_rel_pct'] > 0 else 0\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Step 5: Generate report\n",
    "        print(\"\\n5. Generating evaluation report...\")\n",
    "        report = self._generate_report(\n",
    "            model_comparison,\n",
    "            improvements,\n",
    "            coherence,\n",
    "            quality_results\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'quality_results': quality_results,\n",
    "            'model_comparison': model_comparison,\n",
    "            'improvements': improvements,\n",
    "            'coherence': coherence,\n",
    "            'report': report\n",
    "        }\n",
    "    \n",
    "    def _generate_report(self, \n",
    "                        model_comparison: Dict,\n",
    "                        improvements: Dict,\n",
    "                        coherence: float,\n",
    "                        quality_results: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"=\" * 60,\n",
    "            \"QUALITY EVALUATION REPORT\",\n",
    "            \"=\" * 60,\n",
    "            f\"\\nEvaluation Method: Topic Modeling with {self.n_topics} topics\",\n",
    "            f\"Embedding Model: {self.embedding_model}\",\n",
    "            f\"Representatives per Topic: {self.n_representatives}\",\n",
    "            f\"Topic Coherence Score: {coherence:.3f}\",\n",
    "            \n",
    "            \"\\n\" + \"=\" * 60,\n",
    "            \"MODEL COMPARISON\",\n",
    "            \"=\" * 60,\n",
    "        ]\n",
    "        \n",
    "        for model, metrics in model_comparison.items():\n",
    "            report_lines.extend([\n",
    "                f\"\\n{model.upper()} MODEL:\",\n",
    "                f\"  Average Information Score: {metrics['avg_information']:.2f}/5.0\",\n",
    "                f\"  Average Relevance Score: {metrics['avg_relevance']:.2f}/3.0\",\n",
    "                f\"  Overall Quality Score: {metrics['avg_quality']:.3f}\",\n",
    "                f\"  Low Quality Comments:\",\n",
    "                f\"    - Low Information (≤2): {metrics['low_info_pct']:.1f}%\",\n",
    "                f\"    - Low Relevance (=1): {metrics['low_rel_pct']:.1f}%\"\n",
    "            ])\n",
    "        \n",
    "        report_lines.extend([\n",
    "            \"\\n\" + \"=\" * 60,\n",
    "            \"IMPROVEMENTS\",\n",
    "            \"=\" * 60,\n",
    "            f\"\\nInformation Score: {improvements['information_improvement']:+.1f}%\",\n",
    "            f\"Relevance Score: {improvements['relevance_improvement']:+.1f}%\",\n",
    "            f\"Low Information Reduction: {improvements['low_info_reduction']:.1f}%\",\n",
    "            f\"Low Relevance Reduction: {improvements['low_rel_reduction']:.1f}%\",\n",
    "            \n",
    "            \"\\n\" + \"=\" * 60,\n",
    "            \"COMPARISON WITH PAPER RESULTS\",\n",
    "            \"=\" * 60,\n",
    "            \"\\nPaper reported improvements:\",\n",
    "            \"  - Information: +24%\",\n",
    "            \"  - Relevance: +11%\",\n",
    "            \"  - Low Info Reduction: 73-80%\",\n",
    "            \"  - Low Rel Reduction: 61-72%\",\n",
    "            \n",
    "            \"\\n\" + \"=\" * 60,\n",
    "            \"CONCLUSION\",\n",
    "            \"=\" * 60,\n",
    "            \"\\nThe cleaned model demonstrates substantial improvements in\",\n",
    "            \"comment quality across all metrics, validating the effectiveness\",\n",
    "            \"of semantic data cleaning for code review comment generation.\"\n",
    "        ])\n",
    "        \n",
    "        report = \"\\n\".join(report_lines)\n",
    "        print(\"\\n\" + report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Run complete evaluation\n",
    "production_evaluator = ProductionQualityEvaluator(n_topics=10)  # Fewer topics for demo\n",
    "\n",
    "evaluation_results = production_evaluator.evaluate_model_outputs(\n",
    "    original_comments[:100],\n",
    "    cleaned_comments[:100]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights and Best Practices\n",
    "\n",
    "### Main Findings from the Paper:\n",
    "\n",
    "1. **Topic Modeling Enables Scale**: Evaluate 50 topics instead of 10,000+ comments\n",
    "2. **High Coherence (0.67+)**: Well-formed clusters enable reliable evaluation\n",
    "3. **Quality Improvements**:\n",
    "   - Information: +24%\n",
    "   - Relevance: +11%\n",
    "   - Low quality reduction: 73-80%\n",
    "\n",
    "### Best Practices for Implementation:\n",
    "\n",
    "1. **Choose Right Embedding Model**:\n",
    "   - Use code-aware models (CodeT5+, CodeBERT)\n",
    "   - Fine-tune on your domain if possible\n",
    "\n",
    "2. **Optimize Topic Count**:\n",
    "   - ~50 topics per 10k comments\n",
    "   - Higher coherence > more topics\n",
    "\n",
    "3. **Representative Selection**:\n",
    "   - Top 3 by probability\n",
    "   - Manual verify borderline topics\n",
    "\n",
    "4. **Quality Metrics**:\n",
    "   - Weight information higher (70%)\n",
    "   - Consider domain-specific rubrics\n",
    "\n",
    "### Limitations and Considerations:\n",
    "\n",
    "1. **Approximation**: Topic average may not capture outliers\n",
    "2. **Coherence Dependency**: Low coherence → unreliable evaluation  \n",
    "3. **Manual Validation**: Still need spot checks on representatives\n",
    "4. **Domain Specificity**: Rubrics may need adjustment per project\n",
    "\n",
    "This approach provides a scalable, semi-automated method for evaluating the quality of generated code review comments, demonstrating that cleaned datasets produce significantly higher quality outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}