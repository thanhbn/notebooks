{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-RAG Focused Learning 2: Document Graph Construction\n",
    "\n",
    "## Learning Objective\n",
    "Master the document graph construction methodology in G-RAG, where documents become nodes and shared AMR concepts create edges for improved reranking.\n",
    "\n",
    "## Paper Context\n",
    "\n",
    "### Key Paper Sections:\n",
    "- **Section 3.1**: Establishing Document Graphs via AMR\n",
    "- **Section 3.2.2**: Edge Features\n",
    "- **Figure 1**: G-RAG framework illustration\n",
    "\n",
    "### Paper Quote (Section 3.1):\n",
    "> *\"For each node vi ∈ V, it corresponds to the document pi. For vi, vj ∈ V, i ≠ j, if the corresponding AMR Gqpi and Gqpj have common nodes, there will be an undirected edge between vi and vj denoted as eij = (vi, vj) ∈ E. We remove isolated nodes in Gq.\"*\n",
    "\n",
    "### Innovation:\n",
    "1. **Document-Level Graphs**: Unlike traditional approaches that work with text-level relations, G-RAG creates document-level connections\n",
    "2. **AMR-Based Edges**: Edges represent shared semantic concepts, not just keyword overlap\n",
    "3. **Cross-Document Reasoning**: Enables discovering relevant documents through their connections to other relevant documents\n",
    "\n",
    "### Why This Matters:\n",
    "- **Weak Connection Problem**: Documents with valuable information but weak direct connection to queries\n",
    "- **Transitive Relevance**: Document A might be relevant to query Q through its connection to clearly relevant document B\n",
    "- **Semantic Clustering**: Groups semantically related documents for better ranking decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Foundation\n",
    "\n",
    "### Document Graph Definition\n",
    "\n",
    "Given a question $q$ and retrieved documents $\\{p_1, p_2, ..., p_n\\}$:\n",
    "\n",
    "**Document Graph**: $G_q = \\{V, E\\}$ where:\n",
    "- **Vertices (V)**: Each $v_i \\in V$ corresponds to document $p_i$\n",
    "- **Edges (E)**: $e_{ij} = (v_i, v_j) \\in E$ if AMR graphs $G_{qp_i}$ and $G_{qp_j}$ share common nodes\n",
    "\n",
    "### Edge Feature Computation (Section 3.2.2)\n",
    "\n",
    "For edge between documents $i$ and $j$:\n",
    "$$\\hat{E}_{ij} = \\begin{cases}\n",
    "0 & \\text{if no connection between } G_{qp_i} \\text{ and } G_{qp_j} \\\\\n",
    "\\begin{bmatrix}\n",
    "\\text{# common nodes} \\\\\n",
    "\\text{# common edges}\n",
    "\\end{bmatrix} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Key Insights:\n",
    "1. **Semantic Similarity**: Shared AMR concepts indicate semantic similarity\n",
    "2. **Transitive Information**: Information can flow through the graph\n",
    "3. **Contextual Relevance**: Documents gain relevance through their neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment for document graph construction\n",
    "!pip install networkx matplotlib seaborn numpy pandas\n",
    "!pip install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Set, Optional\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready for document graph construction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMR Graph Simulator for Document Graphs\n",
    "\n",
    "Building on the AMR processing from the previous notebook, we create a specialized simulator for document graph construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAMRSimulator:\n",
    "    \"\"\"\n",
    "    Specialized AMR simulator for document graph construction.\n",
    "    \n",
    "    Focuses on creating realistic AMR graphs that enable meaningful\n",
    "    document-level connections as described in the G-RAG paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Domain-specific concept mappings for better document connections\n",
    "        self.concept_domains = {\n",
    "            'music': {\n",
    "                'concepts': ['sing-01', 'compose-01', 'perform-01', 'album', 'song', 'music', 'artist', 'stage'],\n",
    "                'entities': ['sinatra', 'williams', 'orchestra', 'symphony']\n",
    "            },\n",
    "            'geography': {\n",
    "                'concepts': ['capital', 'city', 'country', 'location', 'government', 'parliament'],\n",
    "                'entities': ['australia', 'canberra', 'sydney', 'melbourne']\n",
    "            },\n",
    "            'technology': {\n",
    "                'concepts': ['release-01', 'create-01', 'invent-01', 'product', 'company', 'innovation'],\n",
    "                'entities': ['iphone', 'apple', 'smartphone', 'technology']\n",
    "            },\n",
    "            'science': {\n",
    "                'concepts': ['cause-01', 'phenomenon', 'particle', 'magnetic-field', 'interact-01'],\n",
    "                'entities': ['aurora', 'sun', 'earth', 'atmosphere']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Common AMR relations for more realistic graphs\n",
    "        self.relations = [\n",
    "            ':ARG0', ':ARG1', ':ARG2',  # Core arguments\n",
    "            ':mod', ':domain', ':poss',  # Modifiers\n",
    "            ':location', ':time', ':manner',  # Adjuncts\n",
    "            ':name', ':wiki'  # Named entity relations\n",
    "        ]\n",
    "    \n",
    "    def identify_document_domain(self, text: str) -> str:\n",
    "        \"\"\"Identify the primary domain of a document for targeted concept extraction.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        domain_scores = {}\n",
    "        \n",
    "        for domain, domain_data in self.concept_domains.items():\n",
    "            score = 0\n",
    "            for concept in domain_data['concepts']:\n",
    "                if concept.replace('-01', '') in text_lower:\n",
    "                    score += 2\n",
    "            for entity in domain_data['entities']:\n",
    "                if entity in text_lower:\n",
    "                    score += 3\n",
    "            domain_scores[domain] = score\n",
    "        \n",
    "        return max(domain_scores, key=domain_scores.get) if max(domain_scores.values()) > 0 else 'general'\n",
    "    \n",
    "    def extract_domain_concepts(self, text: str, domain: str) -> List[str]:\n",
    "        \"\"\"Extract concepts relevant to the identified domain.\"\"\"\n",
    "        concepts = ['question']  # Always include question node\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if domain in self.concept_domains:\n",
    "            domain_data = self.concept_domains[domain]\n",
    "            \n",
    "            # Add domain-specific concepts\n",
    "            for concept in domain_data['concepts']:\n",
    "                if concept.replace('-01', '') in text_lower:\n",
    "                    concepts.append(concept)\n",
    "            \n",
    "            # Add domain-specific entities\n",
    "            for entity in domain_data['entities']:\n",
    "                if entity in text_lower:\n",
    "                    concepts.append(entity)\n",
    "        \n",
    "        # Add general concepts from text\n",
    "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', text_lower)\n",
    "        for word in words[:10]:  # Limit to avoid too large graphs\n",
    "            if word not in ['question', 'document', 'text'] and len(word) > 3:\n",
    "                concepts.append(word)\n",
    "        \n",
    "        return list(set(concepts))\n",
    "    \n",
    "    def create_document_amr(self, question: str, document: str) -> nx.DiGraph:\n",
    "        \"\"\"Create AMR graph for a question-document pair.\"\"\"\n",
    "        combined_text = f\"question: {question} {document}\"\n",
    "        \n",
    "        # Identify domain and extract relevant concepts\n",
    "        domain = self.identify_document_domain(combined_text)\n",
    "        concepts = self.extract_domain_concepts(combined_text, domain)\n",
    "        \n",
    "        # Create directed graph\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(concepts)\n",
    "        \n",
    "        # Add metadata\n",
    "        G.graph['domain'] = domain\n",
    "        G.graph['question'] = question\n",
    "        G.graph['document_id'] = hash(document) % 10000\n",
    "        \n",
    "        # Create edges based on semantic relationships\n",
    "        self._add_semantic_edges(G, question, document)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def _add_semantic_edges(self, G: nx.DiGraph, question: str, document: str):\n",
    "        \"\"\"Add edges based on semantic relationships.\"\"\"\n",
    "        nodes = list(G.nodes())\n",
    "        \n",
    "        # Connect question node to relevant concepts\n",
    "        if 'question' in nodes:\n",
    "            question_words = set(question.lower().split())\n",
    "            for node in nodes:\n",
    "                if node != 'question' and (node in question_words or \n",
    "                                         any(qw in node for qw in question_words)):\n",
    "                    G.add_edge('question', node, relation=':ARG1')\n",
    "        \n",
    "        # Connect co-occurring concepts in sentences\n",
    "        sentences = re.split(r'[.!?]+', document)\n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            sentence_nodes = [node for node in nodes if node in sentence_lower]\n",
    "            \n",
    "            # Connect concepts that appear together\n",
    "            for i, node1 in enumerate(sentence_nodes):\n",
    "                for node2 in sentence_nodes[i+1:]:\n",
    "                    if not G.has_edge(node1, node2):\n",
    "                        relation = np.random.choice(self.relations)\n",
    "                        G.add_edge(node1, node2, relation=relation)\n",
    "    \n",
    "    def get_graph_statistics(self, G: nx.DiGraph) -> Dict:\n",
    "        \"\"\"Get statistics for an AMR graph.\"\"\"\n",
    "        return {\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'domain': G.graph.get('domain', 'unknown'),\n",
    "            'has_question': 'question' in G.nodes(),\n",
    "            'density': nx.density(G),\n",
    "            'avg_degree': sum(dict(G.degree()).values()) / G.number_of_nodes() if G.number_of_nodes() > 0 else 0\n",
    "        }\n",
    "\n",
    "# Test the document AMR simulator\n",
    "doc_amr_sim = DocumentAMRSimulator()\n",
    "\n",
    "# Sample question and documents\n",
    "sample_question = \"What is the nickname of Frank Sinatra?\"\n",
    "sample_documents = [\n",
    "    \"Frank Sinatra was an American singer known for his blue eyes, earning him the nickname 'Ol' Blue Eyes'.\",\n",
    "    \"The famous musician Sinatra performed on stage with his distinctive blue eyes captivating audiences.\",\n",
    "    \"Many singers in the music industry have unique characteristics that make them memorable.\",\n",
    "    \"Blue eyes are a common physical feature among many people in the entertainment world.\"\n",
    "]\n",
    "\n",
    "# Create AMR graphs for each document\n",
    "amr_graphs = []\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    amr = doc_amr_sim.create_document_amr(sample_question, doc)\n",
    "    amr_graphs.append(amr)\n",
    "    \n",
    "    stats = doc_amr_sim.get_graph_statistics(amr)\n",
    "    print(f\"Document {i+1} AMR Stats: {stats}\")\n",
    "    print(f\"  Text: {doc[:60]}...\")\n",
    "    print(f\"  Concepts: {list(amr.nodes())[:8]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"Created {len(amr_graphs)} AMR graphs for document graph construction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Graph Construction - Core Algorithm\n",
    "\n",
    "### Implementation of Paper's Methodology\n",
    "Following Section 3.1 and 3.2.2, we implement the exact algorithm for creating document graphs from AMR representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentGraphBuilder:\n",
    "    \"\"\"\n",
    "    Implements the document graph construction algorithm from G-RAG paper.\n",
    "    \n",
    "    Key Features:\n",
    "    - Creates document-level graphs from AMR representations\n",
    "    - Computes edge features based on shared concepts and relations\n",
    "    - Handles isolated node removal as specified in paper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_shared_concepts: int = 1):\n",
    "        self.min_shared_concepts = min_shared_concepts\n",
    "    \n",
    "    def compute_amr_similarity(self, amr1: nx.DiGraph, amr2: nx.DiGraph) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Compute shared concepts and relations between two AMR graphs.\n",
    "        \n",
    "        Returns:\n",
    "            (common_nodes, common_edges) as specified in paper Section 3.2.2\n",
    "        \"\"\"\n",
    "        # Get nodes (concepts) from both graphs\n",
    "        nodes1 = set(amr1.nodes())\n",
    "        nodes2 = set(amr2.nodes())\n",
    "        \n",
    "        # Count common nodes (excluding 'question' node for meaningful comparison)\n",
    "        common_nodes = len(nodes1 & nodes2)\n",
    "        if 'question' in (nodes1 & nodes2):\n",
    "            common_nodes = max(0, common_nodes - 1)  # Don't count question node\n",
    "        \n",
    "        # Get edges (relations) from both graphs\n",
    "        edges1 = set(amr1.edges())\n",
    "        edges2 = set(amr2.edges())\n",
    "        \n",
    "        # Count common edges\n",
    "        common_edges = len(edges1 & edges2)\n",
    "        \n",
    "        return common_nodes, common_edges\n",
    "    \n",
    "    def build_document_graph(self, \n",
    "                           question: str, \n",
    "                           documents: List[str], \n",
    "                           amr_graphs: List[nx.DiGraph]) -> Dict:\n",
    "        \"\"\"\n",
    "        Build document graph following the exact methodology from G-RAG paper.\n",
    "        \n",
    "        Args:\n",
    "            question: The input question\n",
    "            documents: List of retrieved documents\n",
    "            amr_graphs: Corresponding AMR graphs for each document\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing graph data ready for GNN processing\n",
    "        \"\"\"\n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        # Initialize adjacency matrix and edge features\n",
    "        adjacency = np.zeros((n_docs, n_docs), dtype=int)\n",
    "        edge_features = np.zeros((n_docs, n_docs, 2))  # [common_nodes, common_edges]\n",
    "        \n",
    "        # Compute pairwise similarities and build adjacency matrix\n",
    "        connections = []\n",
    "        for i in range(n_docs):\n",
    "            for j in range(i + 1, n_docs):\n",
    "                common_nodes, common_edges = self.compute_amr_similarity(\n",
    "                    amr_graphs[i], amr_graphs[j]\n",
    "                )\n",
    "                \n",
    "                # Create edge if documents share sufficient concepts\n",
    "                if common_nodes >= self.min_shared_concepts:\n",
    "                    adjacency[i, j] = adjacency[j, i] = 1\n",
    "                    edge_features[i, j] = edge_features[j, i] = [common_nodes, common_edges]\n",
    "                    \n",
    "                    connections.append({\n",
    "                        'doc_i': i,\n",
    "                        'doc_j': j,\n",
    "                        'common_nodes': common_nodes,\n",
    "                        'common_edges': common_edges,\n",
    "                        'strength': common_nodes + 0.5 * common_edges\n",
    "                    })\n",
    "        \n",
    "        # Remove isolated nodes as specified in paper\n",
    "        connected_docs = set()\n",
    "        for i in range(n_docs):\n",
    "            if np.sum(adjacency[i, :]) > 0:  # Node has at least one connection\n",
    "                connected_docs.add(i)\n",
    "        \n",
    "        # Normalize edge features to prevent explosive scaling (as mentioned in paper)\n",
    "        normalized_edge_features = edge_features.copy()\n",
    "        if edge_features.max() > 0:\n",
    "            normalized_edge_features[:, :, 0] = edge_features[:, :, 0] / edge_features[:, :, 0].max()\n",
    "            if edge_features[:, :, 1].max() > 0:\n",
    "                normalized_edge_features[:, :, 1] = edge_features[:, :, 1] / edge_features[:, :, 1].max()\n",
    "        \n",
    "        return {\n",
    "            'adjacency': adjacency,\n",
    "            'edge_features': edge_features,\n",
    "            'normalized_edge_features': normalized_edge_features,\n",
    "            'connections': connections,\n",
    "            'connected_docs': connected_docs,\n",
    "            'isolated_docs': set(range(n_docs)) - connected_docs,\n",
    "            'n_documents': n_docs,\n",
    "            'question': question,\n",
    "            'documents': documents,\n",
    "            'amr_graphs': amr_graphs\n",
    "        }\n",
    "    \n",
    "    def analyze_graph_structure(self, graph_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze the structure of the created document graph.\n",
    "        \"\"\"\n",
    "        adjacency = graph_data['adjacency']\n",
    "        n_docs = graph_data['n_documents']\n",
    "        connections = graph_data['connections']\n",
    "        \n",
    "        # Create NetworkX graph for analysis\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(n_docs))\n",
    "        \n",
    "        for conn in connections:\n",
    "            G.add_edge(conn['doc_i'], conn['doc_j'], \n",
    "                      weight=conn['strength'])\n",
    "        \n",
    "        # Compute graph metrics\n",
    "        analysis = {\n",
    "            'total_documents': n_docs,\n",
    "            'total_connections': len(connections),\n",
    "            'connected_documents': len(graph_data['connected_docs']),\n",
    "            'isolated_documents': len(graph_data['isolated_docs']),\n",
    "            'graph_density': nx.density(G),\n",
    "            'average_degree': sum(dict(G.degree()).values()) / n_docs if n_docs > 0 else 0,\n",
    "            'clustering_coefficient': nx.average_clustering(G) if G.number_of_edges() > 0 else 0,\n",
    "            'connected_components': nx.number_connected_components(G),\n",
    "            'largest_component_size': len(max(nx.connected_components(G), key=len)) if G.number_of_edges() > 0 else 1\n",
    "        }\n",
    "        \n",
    "        # Connection strength statistics\n",
    "        if connections:\n",
    "            strengths = [conn['strength'] for conn in connections]\n",
    "            analysis.update({\n",
    "                'avg_connection_strength': np.mean(strengths),\n",
    "                'max_connection_strength': np.max(strengths),\n",
    "                'min_connection_strength': np.min(strengths)\n",
    "            })\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def visualize_document_graph(self, graph_data: Dict, \n",
    "                                positive_docs: Optional[List[int]] = None,\n",
    "                                title: str = \"Document Graph\",\n",
    "                                figsize: Tuple[int, int] = (14, 10)):\n",
    "        \"\"\"\n",
    "        Visualize the document graph with enhanced information display.\n",
    "        \"\"\"\n",
    "        adjacency = graph_data['adjacency']\n",
    "        connections = graph_data['connections']\n",
    "        documents = graph_data['documents']\n",
    "        n_docs = graph_data['n_documents']\n",
    "        \n",
    "        # Create NetworkX graph\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(n_docs))\n",
    "        \n",
    "        for conn in connections:\n",
    "            G.add_edge(conn['doc_i'], conn['doc_j'], \n",
    "                      weight=conn['strength'],\n",
    "                      common_nodes=conn['common_nodes'],\n",
    "                      common_edges=conn['common_edges'])\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Use spring layout with good spacing\n",
    "        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
    "        \n",
    "        # Color nodes based on positive/negative status\n",
    "        node_colors = []\n",
    "        for i in range(n_docs):\n",
    "            if positive_docs and i in positive_docs:\n",
    "                node_colors.append('lightgreen')  # Positive documents\n",
    "            elif i in graph_data['isolated_docs']:\n",
    "                node_colors.append('lightgray')   # Isolated documents\n",
    "            else:\n",
    "                node_colors.append('lightblue')   # Connected negative documents\n",
    "        \n",
    "        # Draw nodes with size based on degree\n",
    "        node_sizes = [1000 + 200 * G.degree(node) for node in G.nodes()]\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                              node_size=node_sizes, alpha=0.8)\n",
    "        \n",
    "        # Draw edges with thickness based on connection strength\n",
    "        for (u, v, d) in G.edges(data=True):\n",
    "            weight = d['weight']\n",
    "            nx.draw_networkx_edges(G, pos, [(u, v)], \n",
    "                                  width=max(1, weight * 2), alpha=0.6)\n",
    "        \n",
    "        # Draw node labels\n",
    "        labels = {i: f\"D{i}\" for i in range(n_docs)}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=12, font_weight='bold')\n",
    "        \n",
    "        # Draw edge labels with connection strength\n",
    "        edge_labels = {}\n",
    "        for (u, v, d) in G.edges(data=True):\n",
    "            edge_labels[(u, v)] = f\"{d['common_nodes']}\"\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8)\n",
    "        \n",
    "        plt.title(title, fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', \n",
    "                      markersize=15, label='Positive Documents'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', \n",
    "                      markersize=15, label='Connected Documents'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgray', \n",
    "                      markersize=15, label='Isolated Documents')\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed connection information\n",
    "        print(f\"\\nDocument Graph Details:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            status = \"[POSITIVE]\" if positive_docs and i in positive_docs else \"[NEGATIVE]\"\n",
    "            isolated = \"[ISOLATED]\" if i in graph_data['isolated_docs'] else \"\"\n",
    "            print(f\"Document {i} {status}{isolated}: {doc[:80]}...\")\n",
    "        \n",
    "        print(f\"\\nConnections:\")\n",
    "        for conn in connections:\n",
    "            i, j = conn['doc_i'], conn['doc_j']\n",
    "            strength = conn['strength']\n",
    "            nodes = conn['common_nodes']\n",
    "            edges = conn['common_edges']\n",
    "            print(f\"  D{i} ↔ D{j}: {nodes} shared concepts, {edges} shared relations (strength: {strength:.2f})\")\n",
    "\n",
    "# Test the document graph builder\n",
    "graph_builder = DocumentGraphBuilder(min_shared_concepts=1)\n",
    "\n",
    "# Build document graph\n",
    "graph_data = graph_builder.build_document_graph(\n",
    "    sample_question, sample_documents, amr_graphs\n",
    ")\n",
    "\n",
    "# Analyze graph structure\n",
    "analysis = graph_builder.analyze_graph_structure(graph_data)\n",
    "\n",
    "print(\"DOCUMENT GRAPH CONSTRUCTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Documents: {len(sample_documents)}\")\n",
    "print()\n",
    "\n",
    "print(\"Graph Analysis:\")\n",
    "for key, value in analysis.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "# Visualize with positive document marking\n",
    "positive_docs = [0, 1]  # First two documents are positive\n",
    "graph_builder.visualize_document_graph(\n",
    "    graph_data, positive_docs, \n",
    "    \"Document Graph: Frank Sinatra Nickname Question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Feature Analysis\n",
    "\n",
    "### Deep Dive into Edge Features (Section 3.2.2)\n",
    "Understanding how edge features capture semantic relationships between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeFeatureAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes edge features in document graphs to understand \n",
    "    semantic relationships between documents.\n",
    "    \n",
    "    Implements detailed analysis of the edge feature computation\n",
    "    described in Section 3.2.2 of the G-RAG paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def analyze_edge_features(self, graph_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform comprehensive analysis of edge features.\n",
    "        \"\"\"\n",
    "        edge_features = graph_data['edge_features']\n",
    "        connections = graph_data['connections']\n",
    "        n_docs = graph_data['n_documents']\n",
    "        \n",
    "        analysis = {\n",
    "            'total_edges': len(connections),\n",
    "            'edge_density': len(connections) / (n_docs * (n_docs - 1) / 2) if n_docs > 1 else 0\n",
    "        }\n",
    "        \n",
    "        if connections:\n",
    "            # Analyze shared concepts (first dimension)\n",
    "            shared_concepts = [conn['common_nodes'] for conn in connections]\n",
    "            analysis.update({\n",
    "                'avg_shared_concepts': np.mean(shared_concepts),\n",
    "                'max_shared_concepts': np.max(shared_concepts),\n",
    "                'min_shared_concepts': np.min(shared_concepts),\n",
    "                'std_shared_concepts': np.std(shared_concepts)\n",
    "            })\n",
    "            \n",
    "            # Analyze shared relations (second dimension)\n",
    "            shared_relations = [conn['common_edges'] for conn in connections]\n",
    "            analysis.update({\n",
    "                'avg_shared_relations': np.mean(shared_relations),\n",
    "                'max_shared_relations': np.max(shared_relations),\n",
    "                'min_shared_relations': np.min(shared_relations),\n",
    "                'std_shared_relations': np.std(shared_relations)\n",
    "            })\n",
    "            \n",
    "            # Correlation analysis\n",
    "            if len(shared_concepts) > 1:\n",
    "                correlation = np.corrcoef(shared_concepts, shared_relations)[0, 1]\n",
    "                analysis['concept_relation_correlation'] = correlation if not np.isnan(correlation) else 0\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def visualize_edge_features(self, graph_data: Dict, figsize: Tuple[int, int] = (15, 5)):\n",
    "        \"\"\"\n",
    "        Visualize edge feature distributions and relationships.\n",
    "        \"\"\"\n",
    "        connections = graph_data['connections']\n",
    "        \n",
    "        if not connections:\n",
    "            print(\"No connections found in the graph.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        \n",
    "        # Extract data\n",
    "        shared_concepts = [conn['common_nodes'] for conn in connections]\n",
    "        shared_relations = [conn['common_edges'] for conn in connections]\n",
    "        strengths = [conn['strength'] for conn in connections]\n",
    "        \n",
    "        # Plot 1: Shared concepts distribution\n",
    "        axes[0].hist(shared_concepts, bins=max(1, len(set(shared_concepts))), \n",
    "                    alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('Shared Concepts Distribution')\n",
    "        axes[0].set_xlabel('Number of Shared Concepts')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Shared relations distribution\n",
    "        axes[1].hist(shared_relations, bins=max(1, len(set(shared_relations))), \n",
    "                    alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[1].set_title('Shared Relations Distribution')\n",
    "        axes[1].set_xlabel('Number of Shared Relations')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Relationship between concepts and relations\n",
    "        scatter = axes[2].scatter(shared_concepts, shared_relations, \n",
    "                                 c=strengths, cmap='viridis', alpha=0.7, s=100)\n",
    "        axes[2].set_title('Concepts vs Relations')\n",
    "        axes[2].set_xlabel('Shared Concepts')\n",
    "        axes[2].set_ylabel('Shared Relations')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar for strength\n",
    "        cbar = plt.colorbar(scatter, ax=axes[2])\n",
    "        cbar.set_label('Connection Strength')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_edge_feature_heatmap(self, graph_data: Dict, feature_type: str = 'concepts'):\n",
    "        \"\"\"\n",
    "        Create heatmap showing edge features between all document pairs.\n",
    "        \n",
    "        Args:\n",
    "            feature_type: 'concepts' or 'relations'\n",
    "        \"\"\"\n",
    "        edge_features = graph_data['edge_features']\n",
    "        documents = graph_data['documents']\n",
    "        n_docs = graph_data['n_documents']\n",
    "        \n",
    "        # Select feature dimension\n",
    "        feature_idx = 0 if feature_type == 'concepts' else 1\n",
    "        feature_matrix = edge_features[:, :, feature_idx]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create heatmap\n",
    "        mask = np.triu(np.ones_like(feature_matrix, dtype=bool), k=1)\n",
    "        sns.heatmap(feature_matrix, \n",
    "                   mask=mask,\n",
    "                   annot=True, \n",
    "                   cmap='YlOrRd', \n",
    "                   square=True,\n",
    "                   fmt='.0f',\n",
    "                   cbar_kws={'label': f'Shared {feature_type.title()}'})\n",
    "        \n",
    "        plt.title(f'Document Similarity: Shared {feature_type.title()}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Document Index')\n",
    "        plt.ylabel('Document Index')\n",
    "        \n",
    "        # Add document labels on the side\n",
    "        plt.figtext(0.02, 0.5, \n",
    "                   '\\n'.join([f\"D{i}: {doc[:40]}...\" for i, doc in enumerate(documents)]), \n",
    "                   fontsize=8, verticalalignment='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_positive_negative_connections(self, graph_data: Dict, positive_docs: List[int]):\n",
    "        \"\"\"\n",
    "        Compare edge features between positive and negative documents.\n",
    "        \"\"\"\n",
    "        connections = graph_data['connections']\n",
    "        \n",
    "        # Categorize connections\n",
    "        pos_pos_connections = []  # Positive to positive\n",
    "        pos_neg_connections = []  # Positive to negative\n",
    "        neg_neg_connections = []  # Negative to negative\n",
    "        \n",
    "        for conn in connections:\n",
    "            i, j = conn['doc_i'], conn['doc_j']\n",
    "            \n",
    "            if i in positive_docs and j in positive_docs:\n",
    "                pos_pos_connections.append(conn)\n",
    "            elif (i in positive_docs and j not in positive_docs) or (i not in positive_docs and j in positive_docs):\n",
    "                pos_neg_connections.append(conn)\n",
    "            else:\n",
    "                neg_neg_connections.append(conn)\n",
    "        \n",
    "        # Analyze each category\n",
    "        categories = {\n",
    "            'Positive-Positive': pos_pos_connections,\n",
    "            'Positive-Negative': pos_neg_connections,\n",
    "            'Negative-Negative': neg_neg_connections\n",
    "        }\n",
    "        \n",
    "        print(\"CONNECTION TYPE ANALYSIS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for category, conns in categories.items():\n",
    "            if conns:\n",
    "                avg_concepts = np.mean([c['common_nodes'] for c in conns])\n",
    "                avg_relations = np.mean([c['common_edges'] for c in conns])\n",
    "                avg_strength = np.mean([c['strength'] for c in conns])\n",
    "                \n",
    "                print(f\"{category}: {len(conns)} connections\")\n",
    "                print(f\"  Avg shared concepts: {avg_concepts:.2f}\")\n",
    "                print(f\"  Avg shared relations: {avg_relations:.2f}\")\n",
    "                print(f\"  Avg strength: {avg_strength:.2f}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"{category}: No connections\")\n",
    "                print()\n",
    "        \n",
    "        # Visualize comparison\n",
    "        if any(categories.values()):\n",
    "            self._plot_connection_comparison(categories)\n",
    "    \n",
    "    def _plot_connection_comparison(self, categories: Dict[str, List[Dict]]):\n",
    "        \"\"\"Plot comparison of connection types.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        metrics = ['common_nodes', 'common_edges', 'strength']\n",
    "        titles = ['Shared Concepts', 'Shared Relations', 'Connection Strength']\n",
    "        \n",
    "        for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            data = []\n",
    "            labels = []\n",
    "            \n",
    "            for category, conns in categories.items():\n",
    "                if conns:\n",
    "                    values = [conn[metric] for conn in conns]\n",
    "                    data.append(values)\n",
    "                    labels.append(f\"{category}\\n(n={len(conns)})\")\n",
    "            \n",
    "            if data:\n",
    "                axes[idx].boxplot(data, labels=labels)\n",
    "                axes[idx].set_title(title)\n",
    "                axes[idx].set_ylabel('Value')\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Analyze edge features\n",
    "edge_analyzer = EdgeFeatureAnalyzer()\n",
    "\n",
    "# Perform edge feature analysis\n",
    "edge_analysis = edge_analyzer.analyze_edge_features(graph_data)\n",
    "\n",
    "print(\"EDGE FEATURE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "for key, value in edge_analysis.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print()\n",
    "\n",
    "# Visualize edge features\n",
    "edge_analyzer.visualize_edge_features(graph_data)\n",
    "\n",
    "# Create heatmaps\n",
    "edge_analyzer.create_edge_feature_heatmap(graph_data, 'concepts')\n",
    "edge_analyzer.create_edge_feature_heatmap(graph_data, 'relations')\n",
    "\n",
    "# Compare positive vs negative document connections\n",
    "positive_docs = [0, 1]  # First two documents are positive\n",
    "edge_analyzer.compare_positive_negative_connections(graph_data, positive_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis: Different Graph Construction Strategies\n",
    "\n",
    "### Comparing AMR-based vs Alternative Approaches\n",
    "Understanding why AMR-based document graphs outperform simpler alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConstructionComparator:\n",
    "    \"\"\"\n",
    "    Compares different document graph construction strategies:\n",
    "    1. AMR-based (G-RAG approach)\n",
    "    2. TF-IDF similarity\n",
    "    3. Keyword overlap\n",
    "    4. Embedding similarity\n",
    "    \n",
    "    This helps understand why the AMR-based approach is superior.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    \n",
    "    def build_tfidf_graph(self, documents: List[str], threshold: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build document graph using TF-IDF similarity.\n",
    "        \"\"\"\n",
    "        # Compute TF-IDF vectors\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Create adjacency matrix with threshold\n",
    "        adjacency = (similarity_matrix > threshold).astype(int)\n",
    "        \n",
    "        # Remove self-connections\n",
    "        np.fill_diagonal(adjacency, 0)\n",
    "        \n",
    "        return adjacency\n",
    "    \n",
    "    def build_keyword_overlap_graph(self, documents: List[str], min_overlap: int = 2) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build document graph using keyword overlap.\n",
    "        \"\"\"\n",
    "        n_docs = len(documents)\n",
    "        adjacency = np.zeros((n_docs, n_docs), dtype=int)\n",
    "        \n",
    "        # Extract keywords from each document\n",
    "        doc_keywords = []\n",
    "        for doc in documents:\n",
    "            # Simple keyword extraction (words > 3 chars, excluding common words)\n",
    "            words = re.findall(r'\\b[a-zA-Z]{4,}\\b', doc.lower())\n",
    "            stop_words = {'that', 'with', 'have', 'this', 'will', 'your', 'from', 'they', 'know', \n",
    "                         'want', 'been', 'good', 'much', 'some', 'time', 'very', 'when', 'come',\n",
    "                         'here', 'just', 'like', 'long', 'make', 'many', 'over', 'such', 'take',\n",
    "                         'than', 'them', 'well', 'were'}\n",
    "            keywords = set([w for w in words if w not in stop_words])\n",
    "            doc_keywords.append(keywords)\n",
    "        \n",
    "        # Compute overlap\n",
    "        for i in range(n_docs):\n",
    "            for j in range(i + 1, n_docs):\n",
    "                overlap = len(doc_keywords[i] & doc_keywords[j])\n",
    "                if overlap >= min_overlap:\n",
    "                    adjacency[i, j] = adjacency[j, i] = 1\n",
    "        \n",
    "        return adjacency\n",
    "    \n",
    "    def build_embedding_similarity_graph(self, documents: List[str], threshold: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build document graph using simple embedding similarity.\n",
    "        (Simplified version for demonstration)\n",
    "        \"\"\"\n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        # Simple bag-of-words embeddings\n",
    "        vectorizer = TfidfVectorizer(max_features=100)\n",
    "        embeddings = vectorizer.fit_transform(documents).toarray()\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Create adjacency matrix\n",
    "        adjacency = (similarity_matrix > threshold).astype(int)\n",
    "        np.fill_diagonal(adjacency, 0)\n",
    "        \n",
    "        return adjacency\n",
    "    \n",
    "    def compare_graph_methods(self, question: str, documents: List[str], \n",
    "                            positive_docs: List[int], amr_graphs: List[nx.DiGraph]):\n",
    "        \"\"\"\n",
    "        Compare different graph construction methods.\n",
    "        \"\"\"\n",
    "        print(\"GRAPH CONSTRUCTION COMPARISON\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Documents: {len(documents)}\")\n",
    "        print(f\"Positive documents: {positive_docs}\")\n",
    "        print()\n",
    "        \n",
    "        # 1. AMR-based graph (G-RAG approach)\n",
    "        graph_builder = DocumentGraphBuilder(min_shared_concepts=1)\n",
    "        amr_graph_data = graph_builder.build_document_graph(question, documents, amr_graphs)\n",
    "        amr_adjacency = amr_graph_data['adjacency']\n",
    "        \n",
    "        # 2. TF-IDF based graph\n",
    "        tfidf_adjacency = self.build_tfidf_graph(documents, threshold=0.1)\n",
    "        \n",
    "        # 3. Keyword overlap graph\n",
    "        keyword_adjacency = self.build_keyword_overlap_graph(documents, min_overlap=2)\n",
    "        \n",
    "        # 4. Embedding similarity graph\n",
    "        embedding_adjacency = self.build_embedding_similarity_graph(documents, threshold=0.3)\n",
    "        \n",
    "        # Analyze each method\n",
    "        methods = {\n",
    "            'AMR-based (G-RAG)': amr_adjacency,\n",
    "            'TF-IDF Similarity': tfidf_adjacency,\n",
    "            'Keyword Overlap': keyword_adjacency,\n",
    "            'Embedding Similarity': embedding_adjacency\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for method_name, adjacency in methods.items():\n",
    "            analysis = self._analyze_adjacency_matrix(adjacency, positive_docs)\n",
    "            results[method_name] = analysis\n",
    "            \n",
    "            print(f\"{method_name}:\")\n",
    "            for key, value in analysis.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            print()\n",
    "        \n",
    "        # Visualize comparison\n",
    "        self._visualize_method_comparison(methods, positive_docs, documents)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _analyze_adjacency_matrix(self, adjacency: np.ndarray, positive_docs: List[int]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze properties of an adjacency matrix.\n",
    "        \"\"\"\n",
    "        n_docs = adjacency.shape[0]\n",
    "        \n",
    "        # Basic graph properties\n",
    "        total_edges = np.sum(adjacency) // 2  # Undirected graph\n",
    "        density = total_edges / (n_docs * (n_docs - 1) / 2) if n_docs > 1 else 0\n",
    "        \n",
    "        # Connected components\n",
    "        G = nx.from_numpy_array(adjacency)\n",
    "        components = list(nx.connected_components(G))\n",
    "        largest_component = max(components, key=len) if components else set()\n",
    "        \n",
    "        # Positive document connectivity\n",
    "        positive_connections = 0\n",
    "        for i in positive_docs:\n",
    "            for j in positive_docs:\n",
    "                if i != j and adjacency[i, j] == 1:\n",
    "                    positive_connections += 1\n",
    "        positive_connections //= 2  # Undirected\n",
    "        \n",
    "        # Mixed connections (positive to negative)\n",
    "        mixed_connections = 0\n",
    "        for i in positive_docs:\n",
    "            for j in range(n_docs):\n",
    "                if j not in positive_docs and adjacency[i, j] == 1:\n",
    "                    mixed_connections += 1\n",
    "        \n",
    "        return {\n",
    "            'total_edges': total_edges,\n",
    "            'density': round(density, 3),\n",
    "            'connected_components': len(components),\n",
    "            'largest_component_size': len(largest_component),\n",
    "            'positive_connections': positive_connections,\n",
    "            'mixed_connections': mixed_connections,\n",
    "            'avg_degree': round(2 * total_edges / n_docs, 2) if n_docs > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def _visualize_method_comparison(self, methods: Dict[str, np.ndarray], \n",
    "                                   positive_docs: List[int], documents: List[str]):\n",
    "        \"\"\"\n",
    "        Visualize adjacency matrices for different methods.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, (method_name, adjacency) in enumerate(methods.items()):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Create heatmap\n",
    "            im = ax.imshow(adjacency, cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "            \n",
    "            # Highlight positive documents\n",
    "            for pos_doc in positive_docs:\n",
    "                # Add border around positive documents\n",
    "                rect = plt.Rectangle((pos_doc-0.5, pos_doc-0.5), 1, 1, \n",
    "                                   fill=False, edgecolor='red', linewidth=3)\n",
    "                ax.add_patch(rect)\n",
    "            \n",
    "            ax.set_title(f'{method_name}\\n({np.sum(adjacency)//2} connections)')\n",
    "            ax.set_xlabel('Document Index')\n",
    "            ax.set_ylabel('Document Index')\n",
    "            \n",
    "            # Add grid\n",
    "            ax.set_xticks(range(len(documents)))\n",
    "            ax.set_yticks(range(len(documents)))\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print document reference\n",
    "        print(\"Document Reference:\")\n",
    "        for i, doc in enumerate(documents):\n",
    "            status = \"[POS]\" if i in positive_docs else \"[NEG]\"\n",
    "            print(f\"  D{i} {status}: {doc[:60]}...\")\n",
    "\n",
    "# Run comparison\n",
    "comparator = GraphConstructionComparator()\n",
    "\n",
    "# Compare different graph construction methods\n",
    "comparison_results = comparator.compare_graph_methods(\n",
    "    sample_question, sample_documents, positive_docs, amr_graphs\n",
    ")\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"• AMR-based graphs capture semantic relationships better than surface-level similarity\")\n",
    "print(\"• TF-IDF may miss semantic connections that AMR captures\")\n",
    "print(\"• Keyword overlap is too simplistic for complex semantic relationships\")\n",
    "print(\"• AMR provides structured representation that enables better document connections\")\n",
    "print(\"• The graph structure directly impacts the GNN's ability to propagate information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability Analysis\n",
    "\n",
    "### Understanding Performance with Different Document Set Sizes\n",
    "Analyzing how document graph construction scales and performs with varying numbers of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalabilityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes scalability of document graph construction.\n",
    "    \n",
    "    Tests performance with different document set sizes\n",
    "    and provides insights for large-scale deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.doc_amr_sim = DocumentAMRSimulator()\n",
    "        self.graph_builder = DocumentGraphBuilder()\n",
    "    \n",
    "    def generate_test_documents(self, base_question: str, n_docs: int) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Generate test documents for scalability analysis.\n",
    "        \n",
    "        Returns:\n",
    "            (documents, positive_indices)\n",
    "        \"\"\"\n",
    "        # Base documents with known positive examples\n",
    "        base_docs = [\n",
    "            \"Frank Sinatra was known for his bright blue eyes, earning him the nickname 'Ol' Blue Eyes'.\",\n",
    "            \"The famous singer Sinatra performed with distinctive blue eyes that captivated audiences.\",\n",
    "            \"His blue eyes made Frank Sinatra instantly recognizable to fans everywhere.\"\n",
    "        ]\n",
    "        \n",
    "        # Generate additional documents with varying relevance\n",
    "        templates = [\n",
    "            \"The musician {name} was known for {characteristic} in the entertainment industry.\",\n",
    "            \"Many singers like {name} have performed on stage with {characteristic}.\",\n",
    "            \"The artist {name} gained fame through {characteristic} and musical talent.\",\n",
    "            \"In the music world, {name} stood out due to {characteristic}.\",\n",
    "            \"Entertainment history remembers {name} for {characteristic} and performances.\"\n",
    "        ]\n",
    "        \n",
    "        names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Davis', 'Miller', 'Wilson', 'Moore']\n",
    "        characteristics = ['unique voice', 'stage presence', 'musical style', 'performance skills', \n",
    "                          'artistic vision', 'vocal range', 'charisma', 'talent']\n",
    "        \n",
    "        documents = base_docs.copy()\n",
    "        positive_indices = [0, 1, 2]  # Base positive documents\n",
    "        \n",
    "        # Add generated documents\n",
    "        for i in range(n_docs - len(base_docs)):\n",
    "            template = np.random.choice(templates)\n",
    "            name = np.random.choice(names)\n",
    "            char = np.random.choice(characteristics)\n",
    "            \n",
    "            # Occasionally add relevant documents\n",
    "            if np.random.random() < 0.2:  # 20% chance of relevance\n",
    "                char = 'blue eyes and distinctive appearance'\n",
    "                if np.random.random() < 0.5:\n",
    "                    name = 'Sinatra'\n",
    "                positive_indices.append(len(documents))\n",
    "            \n",
    "            doc = template.format(name=name, characteristic=char)\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents, positive_indices\n",
    "    \n",
    "    def measure_scalability(self, document_sizes: List[int]) -> Dict:\n",
    "        \"\"\"\n",
    "        Measure performance across different document set sizes.\n",
    "        \"\"\"\n",
    "        question = \"What is the nickname of Frank Sinatra?\"\n",
    "        results = []\n",
    "        \n",
    "        print(\"SCALABILITY ANALYSIS\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        for n_docs in document_sizes:\n",
    "            print(f\"\\nTesting with {n_docs} documents...\")\n",
    "            \n",
    "            # Generate test documents\n",
    "            documents, positive_indices = self.generate_test_documents(question, n_docs)\n",
    "            \n",
    "            # Create AMR graphs\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            amr_graphs = []\n",
    "            for doc in documents:\n",
    "                amr = self.doc_amr_sim.create_document_amr(question, doc)\n",
    "                amr_graphs.append(amr)\n",
    "            \n",
    "            amr_time = time.time() - start_time\n",
    "            \n",
    "            # Build document graph\n",
    "            start_time = time.time()\n",
    "            graph_data = self.graph_builder.build_document_graph(question, documents, amr_graphs)\n",
    "            graph_time = time.time() - start_time\n",
    "            \n",
    "            # Analyze results\n",
    "            analysis = self.graph_builder.analyze_graph_structure(graph_data)\n",
    "            \n",
    "            result = {\n",
    "                'n_documents': n_docs,\n",
    "                'positive_docs': len(positive_indices),\n",
    "                'amr_processing_time': amr_time,\n",
    "                'graph_construction_time': graph_time,\n",
    "                'total_time': amr_time + graph_time,\n",
    "                'total_connections': analysis['total_connections'],\n",
    "                'graph_density': analysis['graph_density'],\n",
    "                'connected_documents': analysis['connected_documents'],\n",
    "                'largest_component_size': analysis['largest_component_size']\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  AMR processing: {amr_time:.3f}s\")\n",
    "            print(f\"  Graph construction: {graph_time:.3f}s\")\n",
    "            print(f\"  Total connections: {analysis['total_connections']}\")\n",
    "            print(f\"  Graph density: {analysis['graph_density']:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_scalability_results(self, results: List[Dict]):\n",
    "        \"\"\"\n",
    "        Visualize scalability analysis results.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Plot 1: Processing time vs document count\n",
    "        axes[0].plot(df['n_documents'], df['amr_processing_time'], 'o-', label='AMR Processing')\n",
    "        axes[0].plot(df['n_documents'], df['graph_construction_time'], 's-', label='Graph Construction')\n",
    "        axes[0].plot(df['n_documents'], df['total_time'], '^-', label='Total Time')\n",
    "        axes[0].set_xlabel('Number of Documents')\n",
    "        axes[0].set_ylabel('Time (seconds)')\n",
    "        axes[0].set_title('Processing Time vs Document Count')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Time complexity analysis\n",
    "        axes[1].plot(df['n_documents'], df['total_time'] / (df['n_documents'] ** 2), 'o-')\n",
    "        axes[1].set_xlabel('Number of Documents')\n",
    "        axes[1].set_ylabel('Time / n²')\n",
    "        axes[1].set_title('Time Complexity Analysis')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Number of connections vs document count\n",
    "        axes[2].plot(df['n_documents'], df['total_connections'], 'o-', color='green')\n",
    "        axes[2].set_xlabel('Number of Documents')\n",
    "        axes[2].set_ylabel('Total Connections')\n",
    "        axes[2].set_title('Graph Connections vs Document Count')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Graph density vs document count\n",
    "        axes[3].plot(df['n_documents'], df['graph_density'], 's-', color='red')\n",
    "        axes[3].set_xlabel('Number of Documents')\n",
    "        axes[3].set_ylabel('Graph Density')\n",
    "        axes[3].set_title('Graph Density vs Document Count')\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Connected vs total documents\n",
    "        axes[4].plot(df['n_documents'], df['connected_documents'], 'o-', label='Connected')\n",
    "        axes[4].plot(df['n_documents'], df['n_documents'], '--', alpha=0.5, label='Total')\n",
    "        axes[4].set_xlabel('Total Documents')\n",
    "        axes[4].set_ylabel('Documents')\n",
    "        axes[4].set_title('Connected vs Total Documents')\n",
    "        axes[4].legend()\n",
    "        axes[4].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Largest component size\n",
    "        axes[5].plot(df['n_documents'], df['largest_component_size'], '^-', color='purple')\n",
    "        axes[5].set_xlabel('Number of Documents')\n",
    "        axes[5].set_ylabel('Largest Component Size')\n",
    "        axes[5].set_title('Largest Connected Component')\n",
    "        axes[5].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print scalability insights\n",
    "        print(\"\\nSCALABILITY INSIGHTS:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        if len(results) > 1:\n",
    "            time_growth_rate = (results[-1]['total_time'] / results[0]['total_time']) / (results[-1]['n_documents'] / results[0]['n_documents'])\n",
    "            print(f\"• Time growth rate: {time_growth_rate:.2f}x relative to document count\")\n",
    "            \n",
    "            avg_density = np.mean([r['graph_density'] for r in results])\n",
    "            print(f\"• Average graph density: {avg_density:.3f}\")\n",
    "            \n",
    "            connection_efficiency = results[-1]['total_connections'] / (results[-1]['n_documents'] * (results[-1]['n_documents'] - 1) / 2)\n",
    "            print(f\"• Connection efficiency at largest scale: {connection_efficiency:.3f}\")\n",
    "        \n",
    "        print(\"• AMR processing scales roughly linearly with document count\")\n",
    "        print(\"• Graph construction scales quadratically (expected for pairwise comparisons)\")\n",
    "        print(\"• Graph density typically decreases as document count increases\")\n",
    "        print(\"• Connected components provide natural clustering for large document sets\")\n",
    "\n",
    "# Run scalability analysis\n",
    "scalability_analyzer = ScalabilityAnalyzer()\n",
    "\n",
    "# Test with different document sizes\n",
    "test_sizes = [5, 10, 15, 20, 25]\n",
    "scalability_results = scalability_analyzer.measure_scalability(test_sizes)\n",
    "\n",
    "# Visualize results\n",
    "scalability_analyzer.visualize_scalability_results(scalability_results)\n",
    "\n",
    "print(\"\\nScalability analysis complete!\")\n",
    "print(\"This provides insights for deploying G-RAG at different scales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with GNN Architecture\n",
    "\n",
    "### Preparing Document Graph Data for Graph Neural Networks\n",
    "Bridge between document graph construction and the GNN-based reranking module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentGraphToGNNBridge:\n",
    "    \"\"\"\n",
    "    Prepares document graph data for consumption by Graph Neural Networks.\n",
    "    \n",
    "    Implements the data preparation pipeline that connects document graph\n",
    "    construction to the GNN-based reranking described in the paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def prepare_gnn_data(self, graph_data: Dict, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Prepare document graph data for GNN processing.\n",
    "        \n",
    "        Returns data in the format expected by PyTorch Geometric.\n",
    "        \"\"\"\n",
    "        adjacency = graph_data['adjacency']\n",
    "        normalized_edge_features = graph_data['normalized_edge_features']\n",
    "        documents = graph_data['documents']\n",
    "        amr_graphs = graph_data['amr_graphs']\n",
    "        n_docs = graph_data['n_documents']\n",
    "        \n",
    "        # Create edge index (COO format for PyTorch Geometric)\n",
    "        edge_indices = np.where(adjacency)\n",
    "        edge_index = np.vstack([edge_indices[0], edge_indices[1]])\n",
    "        \n",
    "        # Extract edge attributes\n",
    "        edge_attr = []\n",
    "        for i, j in zip(edge_indices[0], edge_indices[1]):\n",
    "            edge_attr.append(normalized_edge_features[i, j])\n",
    "        edge_attr = np.array(edge_attr) if edge_attr else np.empty((0, 2))\n",
    "        \n",
    "        # Prepare node features (to be combined with document embeddings)\n",
    "        node_features = []\n",
    "        for i in range(n_docs):\n",
    "            amr_stats = self._extract_amr_statistics(amr_graphs[i])\n",
    "            node_features.append({\n",
    "                'document': documents[i],\n",
    "                'amr_sequence': self._extract_amr_sequence(amr_graphs[i]),\n",
    "                'amr_stats': amr_stats,\n",
    "                'graph_position': i\n",
    "            })\n",
    "        \n",
    "        gnn_data = {\n",
    "            'edge_index': edge_index,\n",
    "            'edge_attr': edge_attr,\n",
    "            'node_features': node_features,\n",
    "            'num_nodes': n_docs,\n",
    "            'question': question,\n",
    "            'adjacency': adjacency,\n",
    "            'raw_graph_data': graph_data\n",
    "        }\n",
    "        \n",
    "        return gnn_data\n",
    "    \n",
    "    def _extract_amr_statistics(self, amr_graph: nx.DiGraph) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract statistical features from AMR graph for node features.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'num_nodes': amr_graph.number_of_nodes(),\n",
    "            'num_edges': amr_graph.number_of_edges(),\n",
    "            'has_question_node': 'question' in amr_graph.nodes(),\n",
    "            'avg_degree': np.mean(list(dict(amr_graph.degree()).values())) if amr_graph.number_of_nodes() > 0 else 0,\n",
    "            'density': nx.density(amr_graph),\n",
    "            'domain': amr_graph.graph.get('domain', 'unknown')\n",
    "        }\n",
    "    \n",
    "    def _extract_amr_sequence(self, amr_graph: nx.DiGraph) -> str:\n",
    "        \"\"\"\n",
    "        Extract AMR sequence using shortest paths from question node.\n",
    "        \n",
    "        This replicates the methodology from the previous notebook.\n",
    "        \"\"\"\n",
    "        if 'question' not in amr_graph.nodes():\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            # Find shortest paths from question node\n",
    "            paths = nx.single_source_shortest_path(amr_graph, 'question')\n",
    "            \n",
    "            # Extract concepts from paths (excluding question node)\n",
    "            concepts = []\n",
    "            for path in paths.values():\n",
    "                concepts.extend(path[1:])  # Skip question node\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            unique_concepts = []\n",
    "            seen = set()\n",
    "            for concept in concepts:\n",
    "                if concept not in seen:\n",
    "                    unique_concepts.append(concept)\n",
    "                    seen.add(concept)\n",
    "            \n",
    "            return ' '.join(unique_concepts)\n",
    "        except:\n",
    "            return ''\n",
    "    \n",
    "    def create_mock_embeddings(self, gnn_data: Dict, embedding_dim: int = 128) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create mock document embeddings for demonstration.\n",
    "        \n",
    "        In practice, these would come from a pre-trained language model\n",
    "        like BERT, RoBERTa, or sentence transformers.\n",
    "        \"\"\"\n",
    "        node_features = gnn_data['node_features']\n",
    "        num_nodes = gnn_data['num_nodes']\n",
    "        \n",
    "        # Create embeddings based on document and AMR content\n",
    "        embeddings = []\n",
    "        \n",
    "        for nf in node_features:\n",
    "            # Combine document text and AMR sequence\n",
    "            combined_text = f\"{nf['document']} {nf['amr_sequence']}\"\n",
    "            \n",
    "            # Simple hash-based embedding (for demonstration)\n",
    "            np.random.seed(hash(combined_text) % (2**32))\n",
    "            embedding = np.random.randn(embedding_dim)\n",
    "            \n",
    "            # Add AMR statistics as additional features\n",
    "            stats = nf['amr_stats']\n",
    "            stat_features = np.array([\n",
    "                stats['num_nodes'] / 20.0,  # Normalize\n",
    "                stats['num_edges'] / 20.0,\n",
    "                float(stats['has_question_node']),\n",
    "                stats['avg_degree'] / 5.0,\n",
    "                stats['density']\n",
    "            ])\n",
    "            \n",
    "            # Combine text embedding with AMR features\n",
    "            full_embedding = np.concatenate([embedding, stat_features])\n",
    "            embeddings.append(full_embedding)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def visualize_gnn_data_structure(self, gnn_data: Dict, embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Visualize the structure of data prepared for GNN processing.\n",
    "        \"\"\"\n",
    "        print(\"GNN DATA STRUCTURE\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        print(f\"Number of nodes (documents): {gnn_data['num_nodes']}\")\n",
    "        print(f\"Number of edges: {gnn_data['edge_index'].shape[1]}\")\n",
    "        print(f\"Edge feature dimension: {gnn_data['edge_attr'].shape[1] if gnn_data['edge_attr'].size > 0 else 0}\")\n",
    "        print(f\"Node embedding dimension: {embeddings.shape[1]}\")\n",
    "        print(f\"Question: {gnn_data['question']}\")\n",
    "        print()\n",
    "        \n",
    "        # Show sample node features\n",
    "        print(\"Sample Node Features:\")\n",
    "        for i, nf in enumerate(gnn_data['node_features'][:3]):\n",
    "            print(f\"\\nNode {i}:\")\n",
    "            print(f\"  Document: {nf['document'][:60]}...\")\n",
    "            print(f\"  AMR sequence: {nf['amr_sequence'][:60]}...\")\n",
    "            print(f\"  AMR stats: {nf['amr_stats']}\")\n",
    "            print(f\"  Embedding shape: {embeddings[i].shape}\")\n",
    "        \n",
    "        # Show edge information\n",
    "        if gnn_data['edge_index'].shape[1] > 0:\n",
    "            print(f\"\\nSample Edges:\")\n",
    "            for i in range(min(5, gnn_data['edge_index'].shape[1])):\n",
    "                src = gnn_data['edge_index'][0, i]\n",
    "                dst = gnn_data['edge_index'][1, i]\n",
    "                attr = gnn_data['edge_attr'][i] if gnn_data['edge_attr'].size > 0 else \"No attributes\"\n",
    "                print(f\"  Edge {i}: Node {src} ↔ Node {dst}, Attributes: {attr}\")\n",
    "        else:\n",
    "            print(\"\\nNo edges in the graph.\")\n",
    "        \n",
    "        # Visualize embedding space\n",
    "        self._plot_embedding_space(embeddings, gnn_data)\n",
    "    \n",
    "    def _plot_embedding_space(self, embeddings: np.ndarray, gnn_data: Dict):\n",
    "        \"\"\"\n",
    "        Plot 2D projection of embedding space.\n",
    "        \"\"\"\n",
    "        from sklearn.decomposition import PCA\n",
    "        \n",
    "        # Project to 2D using PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot nodes\n",
    "        for i, (x, y) in enumerate(embeddings_2d):\n",
    "            plt.scatter(x, y, s=200, alpha=0.7, label=f'Doc {i}')\n",
    "            plt.annotate(f'D{i}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        # Draw edges\n",
    "        edge_index = gnn_data['edge_index']\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            src, dst = edge_index[0, i], edge_index[1, i]\n",
    "            x1, y1 = embeddings_2d[src]\n",
    "            x2, y2 = embeddings_2d[dst]\n",
    "            plt.plot([x1, x2], [y1, y2], 'k-', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        plt.title('Document Embedding Space (2D Projection)\\nConnected documents in graph')\n",
    "        plt.xlabel(f'PC1 (explains {pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        plt.ylabel(f'PC2 (explains {pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate the bridge\n",
    "bridge = DocumentGraphToGNNBridge()\n",
    "\n",
    "# Prepare GNN data from our document graph\n",
    "gnn_data = bridge.prepare_gnn_data(graph_data, sample_question)\n",
    "\n",
    "# Create mock embeddings\n",
    "embeddings = bridge.create_mock_embeddings(gnn_data, embedding_dim=128)\n",
    "\n",
    "# Visualize the prepared data\n",
    "bridge.visualize_gnn_data_structure(gnn_data, embeddings)\n",
    "\n",
    "print(\"\\nData preparation complete!\")\n",
    "print(\"This data is now ready for the GNN-based reranking module.\")\n",
    "print(\"The next notebook will cover the GNN architecture and training process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights and Learning Summary\n",
    "\n",
    "### 🎯 What We've Mastered:\n",
    "\n",
    "#### 1. **Document Graph Construction Methodology**\n",
    "- **Node Definition**: Each document becomes a node in the graph\n",
    "- **Edge Creation**: Edges connect documents with shared AMR concepts\n",
    "- **Feature Computation**: Edge features capture both shared nodes and relations\n",
    "- **Normalization**: Essential to prevent explosive scaling in GNN operations\n",
    "\n",
    "#### 2. **AMR-Based Semantic Connections**\n",
    "- **Beyond Keywords**: AMR captures semantic relationships that keyword matching misses\n",
    "- **Structured Representation**: AMR provides more reliable connections than TF-IDF\n",
    "- **Domain Awareness**: Documents cluster naturally by semantic domains\n",
    "- **Transitive Relevance**: Documents gain relevance through their connections\n",
    "\n",
    "#### 3. **Edge Feature Analysis**\n",
    "- **Two-Dimensional Features**: [common_nodes, common_edges] as described in paper\n",
    "- **Connection Strength**: Combination of shared concepts and relations\n",
    "- **Positive Document Patterns**: Positive documents often have stronger connections\n",
    "- **Graph Topology**: Structure directly impacts information flow in GNN\n",
    "\n",
    "#### 4. **Scalability Considerations**\n",
    "- **Time Complexity**: O(n²) for pairwise comparisons, but manageable for typical retrieval sets\n",
    "- **Memory Efficiency**: Graph sparsity helps with large document collections\n",
    "- **Connected Components**: Natural clustering for very large sets\n",
    "- **Optimization Opportunities**: Caching and parallel processing\n",
    "\n",
    "### 🔬 Research Implications:\n",
    "\n",
    "1. **Graph-Based Information Retrieval**: Document graphs enable new approaches to relevance ranking\n",
    "2. **Semantic Clustering**: AMR-based connections reveal semantic document clusters\n",
    "3. **Weak Signal Amplification**: Marginally relevant documents benefit from strong neighbors\n",
    "4. **Cross-Document Reasoning**: Information flows through the graph structure\n",
    "\n",
    "### 🚀 Connection to GNN Processing:\n",
    "\n",
    "The document graphs we've constructed provide:\n",
    "- **Node Features**: Document embeddings + AMR statistics\n",
    "- **Edge Features**: Shared concept/relation counts\n",
    "- **Graph Structure**: Adjacency matrix for message passing\n",
    "- **Semantic Context**: Rich representation for neural processing\n",
    "\n",
    "### 💡 Practical Applications:\n",
    "\n",
    "- **Search Result Reranking**: Improve relevance through document connections\n",
    "- **Recommendation Systems**: Leverage item relationships for better recommendations\n",
    "- **Content Discovery**: Find related content through semantic graphs\n",
    "- **Knowledge Organization**: Automatic semantic clustering of documents\n",
    "\n",
    "### 🔍 Key Differences from Traditional Approaches:\n",
    "\n",
    "| Aspect | Traditional | G-RAG Document Graphs |\n",
    "|--------|-------------|----------------------|\n",
    "| **Connection Basis** | Keyword overlap | AMR semantic concepts |\n",
    "| **Relationship Type** | Surface similarity | Deep semantic relations |\n",
    "| **Information Flow** | Independent scoring | Graph-based propagation |\n",
    "| **Weak Connections** | Often missed | Amplified through neighbors |\n",
    "| **Computational** | Linear in documents | Quadratic but manageable |\n",
    "\n",
    "This foundation in document graph construction sets the stage for understanding how Graph Neural Networks can leverage these semantic connections for superior document reranking in the next focused learning notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}