{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Parameter-Efficient Fine-Tuning (PEFT) Methods\n",
    "## Deep Dive into LoRA and Zero-init Attention Prefix-tuning\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand the mathematical foundations of LoRA and Prefix-tuning\n",
    "- Implement both methods from scratch\n",
    "- Compare their effectiveness for code review tasks\n",
    "- Visualize how these methods modify model behavior\n",
    "\n",
    "### Paper References:\n",
    "- **Section III.C**: Zero-init Attention Prefix-tuning (Pages 3-4)\n",
    "- **Section III.D**: Low-Rank Adaptation (Page 4)\n",
    "- **Figure 4**: Details of prefix-tuning on LLaMA\n",
    "- **Figure 5**: Core component of LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding PEFT: Why Do We Need It?\n",
    "\n",
    "Large Language Models like LLaMA-7B have billions of parameters. Fine-tuning all parameters is:\n",
    "- **Computationally expensive**: Requires massive GPU memory\n",
    "- **Storage intensive**: Each fine-tuned model needs ~13GB storage\n",
    "- **Inefficient for multi-task**: Need separate copies for each task\n",
    "\n",
    "PEFT methods solve this by freezing most parameters and only training a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Helper function to count parameters\n",
    "def count_parameters(model: nn.Module) -> Tuple[int, int]:\n",
    "    \"\"\"Count total and trainable parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def print_parameter_stats(model: nn.Module, model_name: str):\n",
    "    \"\"\"Print parameter statistics\"\"\"\n",
    "    total, trainable = count_parameters(model)\n",
    "    percentage = (trainable / total) * 100 if total > 0 else 0\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Trainable percentage: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Low-Rank Adaptation (LoRA) - Mathematical Foundation\n",
    "\n",
    "### Core Idea:\n",
    "Instead of updating the full weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, LoRA approximates the weight update as:\n",
    "\n",
    "$$W_0 + \\Delta W = W_0 + W_{down} \\cdot W_{up}$$\n",
    "\n",
    "Where:\n",
    "- $W_{down} \\in \\mathbb{R}^{d \\times r}$\n",
    "- $W_{up} \\in \\mathbb{R}^{r \\times k}$\n",
    "- $r \\ll \\min(d, k)$ (rank)\n",
    "\n",
    "This reduces parameters from $d \\times k$ to $r \\times (d + k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Implementation of LoRA layer based on the paper\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 16, alpha: int = 16):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Frozen pretrained weight - not updated during training\n",
    "        self.pretrained_weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.pretrained_weight.requires_grad = False\n",
    "        \n",
    "        # LoRA decomposition matrices - these are trained\n",
    "        self.lora_down = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_up = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialize LoRA weights\n",
    "        nn.init.kaiming_uniform_(self.lora_down, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_up)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original computation\n",
    "        h = x @ self.pretrained_weight.t()\n",
    "        \n",
    "        # LoRA computation: h + scaling * x @ W_down^T @ W_up^T\n",
    "        lora_output = x @ self.lora_down.t() @ self.lora_up.t()\n",
    "        \n",
    "        return h + self.scaling * lora_output\n",
    "\n",
    "# Demonstrate LoRA layer\n",
    "input_dim, output_dim = 768, 768  # Typical transformer dimensions\n",
    "batch_size, seq_len = 4, 128\n",
    "\n",
    "# Create layers with different ranks\n",
    "lora_r8 = LoRALayer(input_dim, output_dim, rank=8)\n",
    "lora_r16 = LoRALayer(input_dim, output_dim, rank=16)\n",
    "full_linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "print(\"Parameter Comparison:\")\n",
    "print_parameter_stats(full_linear, \"Full Linear Layer\")\n",
    "print_parameter_stats(lora_r8, \"LoRA (r=8)\")\n",
    "print_parameter_stats(lora_r16, \"LoRA (r=16)\")\n",
    "\n",
    "# Visualize parameter reduction\n",
    "models = ['Full Fine-tuning', 'LoRA (r=16)', 'LoRA (r=8)']\n",
    "params = [input_dim * output_dim, 16 * (input_dim + output_dim), 8 * (input_dim + output_dim)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, params, color=['red', 'green', 'blue'])\n",
    "plt.ylabel('Number of Trainable Parameters')\n",
    "plt.title('LoRA Parameter Efficiency')\n",
    "for bar, param in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{param:,}\\n({param/params[0]*100:.1f}%)', \n",
    "             ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-init Attention Prefix-tuning - Mathematical Foundation\n",
    "\n",
    "### Core Idea:\n",
    "Prefix-tuning adds learnable \"soft prompts\" (prefix tokens) to the input, which influence the attention mechanism.\n",
    "\n",
    "The attention computation with prefix becomes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q[K_p; K]^T}{\\sqrt{d}}\\right)[V_p; V]$$\n",
    "\n",
    "Where $K_p, V_p$ are learnable prefix keys and values.\n",
    "\n",
    "The \"zero-init\" variant adds a learnable gating factor $g_l$ initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroInitPrefixAttention(nn.Module):\n",
    "    \"\"\"Implementation of Zero-init Attention Prefix-tuning based on the paper\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, prefix_length: int = 10):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.prefix_length = prefix_length\n",
    "        \n",
    "        # Learnable prefix tokens\n",
    "        self.prefix_tokens = nn.Parameter(torch.randn(prefix_length, embed_dim))\n",
    "        \n",
    "        # Projection layers for queries, keys, values (frozen in practice)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Freeze the projection layers\n",
    "        for param in [self.q_proj, self.k_proj, self.v_proj, self.out_proj]:\n",
    "            for p in param.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        # Zero-init gating factor (one per layer in practice)\n",
    "        self.gating = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Expand prefix tokens for batch\n",
    "        prefix = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Concatenate prefix with input\n",
    "        x_with_prefix = torch.cat([prefix, x], dim=1)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.q_proj(x)  # Only compute Q for original tokens\n",
    "        K = self.k_proj(x_with_prefix)\n",
    "        V = self.v_proj(x_with_prefix)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len + self.prefix_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len + self.prefix_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        # Split scores for prefix and original tokens\n",
    "        prefix_scores = scores[..., :self.prefix_length]\n",
    "        token_scores = scores[..., self.prefix_length:]\n",
    "        \n",
    "        # Apply gating to prefix scores\n",
    "        gated_prefix_scores = prefix_scores * torch.sigmoid(self.gating)\n",
    "        \n",
    "        # Combine and apply softmax\n",
    "        all_scores = torch.cat([gated_prefix_scores, token_scores], dim=-1)\n",
    "        attn_weights = torch.softmax(all_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Demonstrate prefix attention\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "prefix_attention = ZeroInitPrefixAttention(embed_dim, num_heads, prefix_length=10)\n",
    "\n",
    "print(\"\\nPrefix-tuning Parameter Analysis:\")\n",
    "print_parameter_stats(prefix_attention, \"Zero-init Prefix Attention\")\n",
    "\n",
    "# Compare with standard attention\n",
    "standard_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "print_parameter_stats(standard_attention, \"Standard Multi-head Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing How PEFT Methods Work\n",
    "\n",
    "Let's visualize how LoRA and Prefix-tuning modify the model's computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LoRA decomposition\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Generate example matrices\n",
    "d, k, r = 8, 8, 2\n",
    "W0 = np.random.randn(d, k)\n",
    "W_down = np.random.randn(d, r) * 0.1\n",
    "W_up = np.random.randn(r, k) * 0.1\n",
    "delta_W = W_down @ W_up\n",
    "\n",
    "# Plot original weight\n",
    "im1 = axes[0].imshow(W0, cmap='coolwarm', aspect='auto')\n",
    "axes[0].set_title('Original Weight W₀\\n(Frozen)', fontsize=14)\n",
    "axes[0].set_xlabel('Input dimension')\n",
    "axes[0].set_ylabel('Output dimension')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot LoRA decomposition\n",
    "im2 = axes[1].imshow(delta_W, cmap='coolwarm', aspect='auto')\n",
    "axes[1].set_title('LoRA Update ΔW = W_down × W_up\\n(Trainable)', fontsize=14)\n",
    "axes[1].set_xlabel('Input dimension')\n",
    "axes[1].set_ylabel('Output dimension')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Add annotation showing decomposition\n",
    "axes[1].text(k/2, d+1, f'Rank r={r}', ha='center', fontsize=12)\n",
    "\n",
    "# Plot final weight\n",
    "im3 = axes[2].imshow(W0 + delta_W, cmap='coolwarm', aspect='auto')\n",
    "axes[2].set_title('Final Weight W₀ + ΔW', fontsize=14)\n",
    "axes[2].set_xlabel('Input dimension')\n",
    "axes[2].set_ylabel('Output dimension')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize prefix-tuning attention pattern\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Standard attention pattern\n",
    "seq_len = 10\n",
    "attn_standard = np.random.softmax(np.random.randn(seq_len, seq_len), axis=-1)\n",
    "im1 = ax1.imshow(attn_standard, cmap='Blues', aspect='auto')\n",
    "ax1.set_title('Standard Attention', fontsize=14)\n",
    "ax1.set_xlabel('Key/Value positions')\n",
    "ax1.set_ylabel('Query positions')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Prefix-tuning attention pattern\n",
    "prefix_len = 3\n",
    "total_len = seq_len + prefix_len\n",
    "attn_prefix = np.random.softmax(np.random.randn(seq_len, total_len), axis=-1)\n",
    "# Highlight prefix region\n",
    "attn_prefix[:, :prefix_len] *= 0.5  # Gating effect\n",
    "\n",
    "im2 = ax2.imshow(attn_prefix, cmap='Blues', aspect='auto')\n",
    "ax2.set_title('Prefix-tuning Attention', fontsize=14)\n",
    "ax2.set_xlabel('Key/Value positions')\n",
    "ax2.set_ylabel('Query positions')\n",
    "ax2.axvline(x=prefix_len-0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax2.text(prefix_len/2, -0.5, 'Prefix', ha='center', color='red', fontsize=12)\n",
    "ax2.text(prefix_len + (seq_len-prefix_len)/2, -0.5, 'Original', ha='center', fontsize=12)\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing LoRA vs Prefix-tuning for Code Review\n",
    "\n",
    "Based on the paper's findings, let's analyze why LoRA outperforms Prefix-tuning for code review tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate performance comparison based on paper results\n",
    "comparison_data = {\n",
    "    'Method': ['Prefix-tuning', 'LoRA (r=8)', 'LoRA (r=16)'],\n",
    "    'Review Comment Generation': [5.16, 5.64, 5.70],\n",
    "    'Code Refinement': [76.71, 81.59, 82.27],\n",
    "    'Trainable Params (M)': [1.2, 4.2, 8.4],\n",
    "    'Storage (MB)': [2.4, 8.0, 16.0]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Performance comparison\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(df['Method']))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x - width/2, df['Review Comment Generation'], width, label='Comment Gen', color='skyblue')\n",
    "bars2 = ax1.bar(x + width/2, df['Code Refinement']/10, width, label='Code Refine (/10)', color='lightcoral')\n",
    "ax1.set_xlabel('Method')\n",
    "ax1.set_ylabel('BLEU-4 Score')\n",
    "ax1.set_title('Task Performance Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df['Method'])\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Parameter efficiency\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(df['Method'], df['Trainable Params (M)'], color='green', alpha=0.7)\n",
    "ax2.set_xlabel('Method')\n",
    "ax2.set_ylabel('Trainable Parameters (Millions)')\n",
    "ax2.set_title('Parameter Efficiency')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Analysis: Why LoRA performs better\n",
    "ax3 = axes[1, 0]\n",
    "ax3.axis('off')\n",
    "analysis_text = \"\"\"Why LoRA Outperforms Prefix-tuning:\n",
    "\n",
    "1. Better Approximation:\n",
    "   - LoRA directly modifies weight matrices\n",
    "   - Prefix only influences attention patterns\n",
    "\n",
    "2. Task Alignment:\n",
    "   - Code review needs precise token generation\n",
    "   - LoRA better captures fine-grained patterns\n",
    "\n",
    "3. Flexibility:\n",
    "   - LoRA rank can be adjusted (r=8, 16)\n",
    "   - More parameters = better performance\n",
    "\n",
    "4. Training Stability:\n",
    "   - LoRA has smoother optimization landscape\n",
    "   - Prefix-tuning's zero-init can be unstable\"\"\"\n",
    "ax3.text(0.1, 0.9, analysis_text, transform=ax3.transAxes, \n",
    "         fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "# Practical considerations\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "practical_text = \"\"\"Practical Considerations:\n",
    "\n",
    "Choose LoRA when:\n",
    "✓ Need best performance\n",
    "✓ Have moderate GPU memory\n",
    "✓ Multiple tasks to fine-tune\n",
    "\n",
    "Choose Prefix-tuning when:\n",
    "✓ Extremely limited memory\n",
    "✓ Need minimal storage\n",
    "✓ Simple classification tasks\n",
    "\n",
    "Recommended: LoRA with r=16\n",
    "- Best performance/efficiency balance\n",
    "- Still <1% of full parameters\"\"\"\n",
    "ax4.text(0.1, 0.9, practical_text, transform=ax4.transAxes,\n",
    "         fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing PEFT for Code Review Task\n",
    "\n",
    "Let's implement a complete example of using LoRA for the review comment generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewLoRAModel(nn.Module):\n",
    "    \"\"\"Simplified model demonstrating LoRA for code review\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 50000, embed_dim: int = 768, \n",
    "                 num_layers: int = 4, lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings (frozen in practice)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        \n",
    "        # Transformer layers with LoRA\n",
    "        self.layers = nn.ModuleList([\n",
    "            self._create_layer_with_lora(embed_dim, lora_rank) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection (with LoRA)\n",
    "        self.output_proj = LoRALayer(embed_dim, vocab_size, rank=lora_rank)\n",
    "    \n",
    "    def _create_layer_with_lora(self, embed_dim: int, lora_rank: int) -> nn.Module:\n",
    "        \"\"\"Create a transformer layer with LoRA adapters\"\"\"\n",
    "        return nn.ModuleDict({\n",
    "            'self_attn_q': LoRALayer(embed_dim, embed_dim, rank=lora_rank),\n",
    "            'self_attn_k': LoRALayer(embed_dim, embed_dim, rank=lora_rank),\n",
    "            'self_attn_v': LoRALayer(embed_dim, embed_dim, rank=lora_rank),\n",
    "            'self_attn_o': LoRALayer(embed_dim, embed_dim, rank=lora_rank),\n",
    "            'layer_norm1': nn.LayerNorm(embed_dim),\n",
    "            'layer_norm2': nn.LayerNorm(embed_dim),\n",
    "            # FFN would also have LoRA in practice\n",
    "        })\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # Simplified forward pass\n",
    "        x = self.embeddings(input_ids)\n",
    "        \n",
    "        # Pass through layers (simplified - no actual attention)\n",
    "        for layer in self.layers:\n",
    "            # This is simplified - real implementation would compute attention\n",
    "            q = layer['self_attn_q'](x)\n",
    "            k = layer['self_attn_k'](x)\n",
    "            v = layer['self_attn_v'](x)\n",
    "            # ... attention computation ...\n",
    "            x = layer['layer_norm1'](x)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Create model and analyze parameters\n",
    "model = CodeReviewLoRAModel(vocab_size=30000, embed_dim=768, num_layers=4, lora_rank=16)\n",
    "print_parameter_stats(model, \"Code Review Model with LoRA\")\n",
    "\n",
    "# Calculate memory savings\n",
    "full_model_params = 30000 * 768 + 4 * 4 * 768 * 768 + 768 * 30000  # Approximate\n",
    "lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "savings = (1 - lora_params / full_model_params) * 100\n",
    "\n",
    "print(f\"\\nMemory Savings: {savings:.1f}%\")\n",
    "print(f\"Storage per task: {lora_params * 4 / 1024 / 1024:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Takeaways\n",
    "\n",
    "### From the Paper's Experiments:\n",
    "\n",
    "1. **LoRA Performance**:\n",
    "   - Achieves 5.70 BLEU-4 on comment generation (best among all models)\n",
    "   - Uses only 8.4M parameters vs 220M for baseline models\n",
    "   - Storage: 16MB vs 850MB for full models\n",
    "\n",
    "2. **Prefix-tuning Limitations**:\n",
    "   - Lower performance (5.16 BLEU-4)\n",
    "   - Incompatible with classification tasks\n",
    "   - Less stable training with instruction tuning\n",
    "\n",
    "3. **Rank Selection**:\n",
    "   - r=16 provides best performance\n",
    "   - r=8 offers better efficiency with slight performance drop\n",
    "   - Higher ranks approach full fine-tuning performance\n",
    "\n",
    "### Practical Implementation Tips:\n",
    "\n",
    "1. **When to Use LoRA**:\n",
    "   - Multi-task scenarios (different adapters per task)\n",
    "   - Limited GPU memory\n",
    "   - Need to preserve base model\n",
    "\n",
    "2. **Optimization**:\n",
    "   - Learning rate: 3e-4 for LoRA (vs 5e-5 for full fine-tuning)\n",
    "   - Weight decay: 0.01\n",
    "   - Batch size can be larger due to memory savings\n",
    "\n",
    "3. **Integration**:\n",
    "   - LoRA weights can be merged into base model for inference\n",
    "   - Multiple LoRA adapters can be swapped dynamically\n",
    "   - Compatible with quantization (8-bit, 4-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Data from the paper\n",
    "methods = ['Full\\nFine-tuning', 'LoRA\\n(r=16)', 'LoRA\\n(r=8)', 'Prefix\\nTuning']\n",
    "performance = [100, 98, 95, 85]  # Relative performance\n",
    "efficiency = [0, 99.87, 99.93, 99.98]  # Parameter reduction %\n",
    "storage = [850, 16, 8, 2.4]  # MB\n",
    "\n",
    "# Create bubble chart\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "for i, (method, perf, eff, stor) in enumerate(zip(methods, performance, efficiency, storage)):\n",
    "    ax.scatter(eff, perf, s=stor*5, c=colors[i], alpha=0.6, edgecolors='black', linewidth=2)\n",
    "    ax.annotate(method, (eff, perf), ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Parameter Reduction (%)', fontsize=12)\n",
    "ax.set_ylabel('Relative Performance (%)', fontsize=12)\n",
    "ax.set_title('PEFT Methods: Performance vs Efficiency Trade-off\\n(Bubble size = Storage in MB)', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-5, 105)\n",
    "ax.set_ylim(80, 105)\n",
    "\n",
    "# Add recommendation box\n",
    "textstr = 'Paper Recommendation:\\nLoRA with r=16 for best\\nperformance-efficiency balance'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}