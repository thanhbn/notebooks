{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-Precision Quantization with Hardware Co-Design\n",
    "## Focused Learning Notebook 3/4\n",
    "\n",
    "**Paper Source**: Optimizing Edge AI: A Comprehensive Survey (2501.03265v1)  \n",
    "**Paper Sections**: Pages 13-14 (Model Quantization)  \n",
    "**Focus Concept**: Advanced Quantization with Hardware-Aware Optimization\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By completing this notebook, you will understand:\n",
    "\n",
    "1. **Mixed-precision quantization** strategies for optimal accuracy-efficiency trade-offs\n",
    "2. **Hardware-aware quantization** considering specific edge device constraints\n",
    "3. **Quantization-aware training (QAT)** vs post-training quantization techniques\n",
    "4. **Advanced bit-width selection** algorithms and sensitivity analysis\n",
    "5. **Gradient approximation** for non-differentiable quantization operations\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Theoretical Foundation\n",
    "\n",
    "### Mixed-Precision Quantization Mathematical Framework\n",
    "\n",
    "**Paper Quote** (Model Quantization Section):\n",
    "> *\"Mixed-precision quantization uses different bit-widths for different layers/operations, combined with hardware-software co-design to maximize efficiency on specific edge hardware architectures.\"*\n",
    "\n",
    "### Quantization Function\n",
    "\n",
    "The basic quantization operation maps continuous values to discrete levels:\n",
    "\n",
    "$$Q(x) = \\text{round}\\left(\\frac{x - z}{s}\\right) \\cdot s + z$$\n",
    "\n",
    "Where:\n",
    "- $s$: scale factor\n",
    "- $z$: zero-point offset\n",
    "- $\\text{round}()$: rounding function\n",
    "\n",
    "### Symmetric vs Asymmetric Quantization\n",
    "\n",
    "**Symmetric (zero-point = 0):**\n",
    "$$s = \\frac{2 \\cdot \\max(|x|)}{2^b - 1}$$\n",
    "\n",
    "**Asymmetric:**\n",
    "$$s = \\frac{\\max(x) - \\min(x)}{2^b - 1}, \\quad z = -\\text{round}\\left(\\frac{\\min(x)}{s}\\right)$$\n",
    "\n",
    "### Mixed-Precision Optimization Problem\n",
    "\n",
    "**Hardware-Aware Automated Quantization (HAQ) Framework:**\n",
    "\n",
    "$$\\min_{\\{b_i\\}} \\mathcal{L}(\\{b_i\\}) \\text{ subject to } \\sum_{i} C_i(b_i) \\leq C_{budget}$$\n",
    "\n",
    "Where:\n",
    "- $b_i$: bit-width for layer $i$\n",
    "- $\\mathcal{L}(\\{b_i\\})$: accuracy loss function\n",
    "- $C_i(b_i)$: hardware cost (latency, energy, memory) for layer $i$ with bit-width $b_i$\n",
    "- $C_{budget}$: total hardware budget\n",
    "\n",
    "### Straight-Through Estimator (STE)\n",
    "\n",
    "For gradient flow through non-differentiable quantization:\n",
    "\n",
    "$$\\frac{\\partial Q(x)}{\\partial x} = \\begin{cases}\n",
    "1 & \\text{if } |x| \\leq \\text{threshold} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Quantization utilities\n",
    "import copy\n",
    "from enum import Enum\n",
    "\n",
    "# Optimization and analysis\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"✅ Environment setup complete for Mixed-Precision Quantization\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔢 Quantization Building Blocks\n",
    "\n",
    "Implement fundamental quantization operations with different precision levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizationMode(Enum):\n",
    "    \"\"\"Quantization modes for different scenarios\"\"\"\n",
    "    SYMMETRIC = \"symmetric\"\n",
    "    ASYMMETRIC = \"asymmetric\"\n",
    "    DYNAMIC = \"dynamic\"\n",
    "\n",
    "class StraightThroughEstimator(torch.autograd.Function):\n",
    "    \"\"\"Straight-through estimator for gradient flow through quantization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_tensor, scale, zero_point, bit_width):\n",
    "        # Quantize forward pass\n",
    "        qmin = 0\n",
    "        qmax = 2 ** bit_width - 1\n",
    "        \n",
    "        # Scale and shift\n",
    "        scaled = input_tensor / scale + zero_point\n",
    "        \n",
    "        # Clamp and round\n",
    "        quantized = torch.clamp(torch.round(scaled), qmin, qmax)\n",
    "        \n",
    "        # Dequantize\n",
    "        dequantized = (quantized - zero_point) * scale\n",
    "        \n",
    "        return dequantized\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through: pass gradient as-is\n",
    "        return grad_output, None, None, None\n",
    "\n",
    "class QuantizationScheme:\n",
    "    \"\"\"Flexible quantization scheme supporting multiple bit-widths and modes\"\"\"\n",
    "    \n",
    "    def __init__(self, bit_width: int = 8, mode: QuantizationMode = QuantizationMode.SYMMETRIC, \n",
    "                 per_channel: bool = False):\n",
    "        self.bit_width = bit_width\n",
    "        self.mode = mode\n",
    "        self.per_channel = per_channel\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        \n",
    "    def calibrate(self, tensor: torch.Tensor):\n",
    "        \"\"\"Calibrate quantization parameters from tensor statistics\"\"\"\n",
    "        if self.per_channel and len(tensor.shape) >= 2:\n",
    "            # Per-channel quantization (typically for weights)\n",
    "            dims = list(range(1, len(tensor.shape)))\n",
    "            tensor_min = tensor.min(dim=dims, keepdim=True)[0]\n",
    "            tensor_max = tensor.max(dim=dims, keepdim=True)[0]\n",
    "        else:\n",
    "            # Per-tensor quantization\n",
    "            tensor_min = tensor.min()\n",
    "            tensor_max = tensor.max()\n",
    "        \n",
    "        if self.mode == QuantizationMode.SYMMETRIC:\n",
    "            # Symmetric quantization (zero-point = 0)\n",
    "            abs_max = torch.max(torch.abs(tensor_min), torch.abs(tensor_max))\n",
    "            self.scale = 2 * abs_max / (2 ** self.bit_width - 1)\n",
    "            self.zero_point = torch.zeros_like(self.scale)\n",
    "        else:\n",
    "            # Asymmetric quantization\n",
    "            self.scale = (tensor_max - tensor_min) / (2 ** self.bit_width - 1)\n",
    "            self.zero_point = -torch.round(tensor_min / self.scale)\n",
    "            \n",
    "        # Avoid division by zero\n",
    "        self.scale = torch.clamp(self.scale, min=1e-8)\n",
    "        \n",
    "    def quantize(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply quantization to tensor\"\"\"\n",
    "        if self.scale is None or self.zero_point is None:\n",
    "            self.calibrate(tensor)\n",
    "            \n",
    "        return StraightThroughEstimator.apply(tensor, self.scale, self.zero_point, self.bit_width)\n",
    "    \n",
    "    def get_quantization_error(self, tensor: torch.Tensor) -> float:\n",
    "        \"\"\"Calculate quantization error (MSE)\"\"\"\n",
    "        quantized = self.quantize(tensor)\n",
    "        return F.mse_loss(tensor, quantized).item()\n",
    "    \n",
    "    def get_compression_ratio(self) -> float:\n",
    "        \"\"\"Get compression ratio compared to FP32\"\"\"\n",
    "        return 32.0 / self.bit_width\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"QuantizationScheme(bit_width={self.bit_width}, \"\n",
    "                f\"mode={self.mode.value}, per_channel={self.per_channel})\")\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"Quantized linear layer with configurable bit-width\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, \n",
    "                 weight_bit_width: int = 8, activation_bit_width: int = 8,\n",
    "                 bias: bool = True):\n",
    "        super(QuantizedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Create standard linear layer\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        # Quantization schemes\n",
    "        self.weight_quantizer = QuantizationScheme(\n",
    "            bit_width=weight_bit_width, \n",
    "            mode=QuantizationMode.SYMMETRIC,\n",
    "            per_channel=True\n",
    "        )\n",
    "        self.activation_quantizer = QuantizationScheme(\n",
    "            bit_width=activation_bit_width,\n",
    "            mode=QuantizationMode.ASYMMETRIC,\n",
    "            per_channel=False\n",
    "        )\n",
    "        \n",
    "        self.weight_bit_width = weight_bit_width\n",
    "        self.activation_bit_width = activation_bit_width\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize input activations\n",
    "        x_quantized = self.activation_quantizer.quantize(x)\n",
    "        \n",
    "        # Quantize weights\n",
    "        weight_quantized = self.weight_quantizer.quantize(self.linear.weight)\n",
    "        \n",
    "        # Perform linear operation with quantized parameters\n",
    "        output = F.linear(x_quantized, weight_quantized, self.linear.bias)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_bit_widths(self):\n",
    "        return {\n",
    "            'weight': self.weight_bit_width,\n",
    "            'activation': self.activation_bit_width\n",
    "        }\n",
    "\n",
    "class QuantizedConv2d(nn.Module):\n",
    "    \"\"\"Quantized 2D convolution layer with configurable bit-width\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int,\n",
    "                 weight_bit_width: int = 8, activation_bit_width: int = 8,\n",
    "                 stride: int = 1, padding: int = 0, bias: bool = True):\n",
    "        super(QuantizedConv2d, self).__init__()\n",
    "        \n",
    "        # Create standard conv layer\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                             stride=stride, padding=padding, bias=bias)\n",
    "        \n",
    "        # Quantization schemes\n",
    "        self.weight_quantizer = QuantizationScheme(\n",
    "            bit_width=weight_bit_width,\n",
    "            mode=QuantizationMode.SYMMETRIC,\n",
    "            per_channel=True\n",
    "        )\n",
    "        self.activation_quantizer = QuantizationScheme(\n",
    "            bit_width=activation_bit_width,\n",
    "            mode=QuantizationMode.ASYMMETRIC,\n",
    "            per_channel=False\n",
    "        )\n",
    "        \n",
    "        self.weight_bit_width = weight_bit_width\n",
    "        self.activation_bit_width = activation_bit_width\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize input activations\n",
    "        x_quantized = self.activation_quantizer.quantize(x)\n",
    "        \n",
    "        # Quantize weights\n",
    "        weight_quantized = self.weight_quantizer.quantize(self.conv.weight)\n",
    "        \n",
    "        # Perform convolution with quantized parameters\n",
    "        output = F.conv2d(x_quantized, weight_quantized, self.conv.bias,\n",
    "                         self.conv.stride, self.conv.padding)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_bit_widths(self):\n",
    "        return {\n",
    "            'weight': self.weight_bit_width,\n",
    "            'activation': self.activation_bit_width\n",
    "        }\n",
    "\n",
    "# Test quantization building blocks\n",
    "print(\"✅ Quantization building blocks implemented\")\n",
    "\n",
    "# Test basic quantization\n",
    "test_tensor = torch.randn(10, 20)\n",
    "quant_schemes = [\n",
    "    QuantizationScheme(8, QuantizationMode.SYMMETRIC),\n",
    "    QuantizationScheme(4, QuantizationMode.ASYMMETRIC),\n",
    "    QuantizationScheme(2, QuantizationMode.SYMMETRIC)\n",
    "]\n",
    "\n",
    "print(\"\\n📊 Quantization Error Analysis:\")\n",
    "for scheme in quant_schemes:\n",
    "    error = scheme.get_quantization_error(test_tensor)\n",
    "    compression = scheme.get_compression_ratio()\n",
    "    print(f\"   {scheme.bit_width}-bit {scheme.mode.value}: \"\n",
    "          f\"MSE={error:.6f}, Compression={compression:.1f}x\")\n",
    "\n",
    "# Test quantized layers\n",
    "qlinear = QuantizedLinear(10, 5, weight_bit_width=8, activation_bit_width=8)\n",
    "qconv = QuantizedConv2d(3, 16, kernel_size=3, weight_bit_width=4, activation_bit_width=8)\n",
    "\n",
    "print(f\"\\n✅ Quantized layers created:\")\n",
    "print(f\"   Linear: {qlinear.get_bit_widths()}\")\n",
    "print(f\"   Conv2d: {qconv.get_bit_widths()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Mixed-Precision Neural Networks\n",
    "\n",
    "Create networks with different bit-widths for different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecisionCNN(nn.Module):\n",
    "    \"\"\"CNN with mixed-precision quantization across layers\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 10, precision_config: Dict[str, int] = None):\n",
    "        super(MixedPrecisionCNN, self).__init__()\n",
    "        \n",
    "        # Default precision configuration\n",
    "        if precision_config is None:\n",
    "            precision_config = {\n",
    "                'conv1_w': 8, 'conv1_a': 8,\n",
    "                'conv2_w': 8, 'conv2_a': 8,\n",
    "                'conv3_w': 8, 'conv3_a': 8,\n",
    "                'fc1_w': 8, 'fc1_a': 8,\n",
    "                'fc2_w': 8, 'fc2_a': 8\n",
    "            }\n",
    "        \n",
    "        self.precision_config = precision_config\n",
    "        \n",
    "        # Quantized convolutional layers\n",
    "        self.conv1 = QuantizedConv2d(\n",
    "            3, 32, kernel_size=3, padding=1,\n",
    "            weight_bit_width=precision_config['conv1_w'],\n",
    "            activation_bit_width=precision_config['conv1_a']\n",
    "        )\n",
    "        self.conv2 = QuantizedConv2d(\n",
    "            32, 64, kernel_size=3, padding=1,\n",
    "            weight_bit_width=precision_config['conv2_w'],\n",
    "            activation_bit_width=precision_config['conv2_a']\n",
    "        )\n",
    "        self.conv3 = QuantizedConv2d(\n",
    "            64, 128, kernel_size=3, padding=1,\n",
    "            weight_bit_width=precision_config['conv3_w'],\n",
    "            activation_bit_width=precision_config['conv3_a']\n",
    "        )\n",
    "        \n",
    "        # Standard layers (batch norm, pooling, activations)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        # Quantized fully connected layers\n",
    "        self.fc1 = QuantizedLinear(\n",
    "            128 * 4 * 4, 256,\n",
    "            weight_bit_width=precision_config['fc1_w'],\n",
    "            activation_bit_width=precision_config['fc1_a']\n",
    "        )\n",
    "        self.fc2 = QuantizedLinear(\n",
    "            256, num_classes,\n",
    "            weight_bit_width=precision_config['fc2_w'],\n",
    "            activation_bit_width=precision_config['fc2_a']\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_layer_bit_widths(self):\n",
    "        \"\"\"Get bit-width configuration for all layers\"\"\"\n",
    "        return {\n",
    "            'conv1': self.conv1.get_bit_widths(),\n",
    "            'conv2': self.conv2.get_bit_widths(),\n",
    "            'conv3': self.conv3.get_bit_widths(),\n",
    "            'fc1': self.fc1.get_bit_widths(),\n",
    "            'fc2': self.fc2.get_bit_widths()\n",
    "        }\n",
    "    \n",
    "    def get_total_compression_ratio(self):\n",
    "        \"\"\"Calculate overall compression ratio\"\"\"\n",
    "        total_bits = 0\n",
    "        total_params = 0\n",
    "        \n",
    "        layers = [self.conv1, self.conv2, self.conv3, self.fc1, self.fc2]\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'weight_bit_width'):\n",
    "                weight_params = layer.linear.weight.numel() if hasattr(layer, 'linear') else layer.conv.weight.numel()\n",
    "                total_bits += weight_params * layer.weight_bit_width\n",
    "                total_params += weight_params\n",
    "        \n",
    "        avg_bit_width = total_bits / total_params\n",
    "        return 32.0 / avg_bit_width\n",
    "    \n",
    "    def get_memory_footprint(self):\n",
    "        \"\"\"Estimate memory footprint in MB\"\"\"\n",
    "        total_bits = 0\n",
    "        \n",
    "        layers = [self.conv1, self.conv2, self.conv3, self.fc1, self.fc2]\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'weight_bit_width'):\n",
    "                weight_params = layer.linear.weight.numel() if hasattr(layer, 'linear') else layer.conv.weight.numel()\n",
    "                total_bits += weight_params * layer.weight_bit_width\n",
    "                \n",
    "                if hasattr(layer, 'linear') and layer.linear.bias is not None:\n",
    "                    total_bits += layer.linear.bias.numel() * 32  # Bias typically kept at FP32\n",
    "                elif hasattr(layer, 'conv') and layer.conv.bias is not None:\n",
    "                    total_bits += layer.conv.bias.numel() * 32\n",
    "        \n",
    "        # Convert to MB\n",
    "        return total_bits / (8 * 1024 * 1024)\n",
    "\n",
    "# Create different precision configurations\n",
    "precision_configs = {\n",
    "    'fp32': {\n",
    "        'conv1_w': 32, 'conv1_a': 32,\n",
    "        'conv2_w': 32, 'conv2_a': 32,\n",
    "        'conv3_w': 32, 'conv3_a': 32,\n",
    "        'fc1_w': 32, 'fc1_a': 32,\n",
    "        'fc2_w': 32, 'fc2_a': 32\n",
    "    },\n",
    "    'int8_uniform': {\n",
    "        'conv1_w': 8, 'conv1_a': 8,\n",
    "        'conv2_w': 8, 'conv2_a': 8,\n",
    "        'conv3_w': 8, 'conv3_a': 8,\n",
    "        'fc1_w': 8, 'fc1_a': 8,\n",
    "        'fc2_w': 8, 'fc2_a': 8\n",
    "    },\n",
    "    'mixed_conservative': {\n",
    "        'conv1_w': 8, 'conv1_a': 8,   # First layer higher precision\n",
    "        'conv2_w': 6, 'conv2_a': 8,   # Middle layers moderate\n",
    "        'conv3_w': 6, 'conv3_a': 8,\n",
    "        'fc1_w': 4, 'fc1_a': 8,       # FC layers lower precision\n",
    "        'fc2_w': 8, 'fc2_a': 8        # Last layer higher for accuracy\n",
    "    },\n",
    "    'mixed_aggressive': {\n",
    "        'conv1_w': 8, 'conv1_a': 8,   # First layer higher precision\n",
    "        'conv2_w': 4, 'conv2_a': 8,   # Aggressive middle layers\n",
    "        'conv3_w': 4, 'conv3_a': 8,\n",
    "        'fc1_w': 3, 'fc1_a': 8,       # Very low precision FC\n",
    "        'fc2_w': 6, 'fc2_a': 8        # Last layer moderate\n",
    "    },\n",
    "    'ultra_low': {\n",
    "        'conv1_w': 4, 'conv1_a': 8,   # Still keep activations higher\n",
    "        'conv2_w': 3, 'conv2_a': 8,\n",
    "        'conv3_w': 3, 'conv3_a': 8,\n",
    "        'fc1_w': 2, 'fc1_a': 8,       # Binary weights\n",
    "        'fc2_w': 4, 'fc2_a': 8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create models with different precision configurations\n",
    "models = {}\n",
    "for config_name, config in precision_configs.items():\n",
    "    models[config_name] = MixedPrecisionCNN(num_classes=10, precision_config=config).to(device)\n",
    "\n",
    "print(\"✅ Mixed-precision models created\")\n",
    "print(f\"   Number of configurations: {len(models)}\")\n",
    "\n",
    "# Analyze compression ratios\n",
    "print(\"\\n📊 Model Compression Analysis:\")\n",
    "for name, model in models.items():\n",
    "    compression = model.get_total_compression_ratio()\n",
    "    memory_mb = model.get_memory_footprint()\n",
    "    print(f\"   {name:<20}: {compression:.1f}x compression, {memory_mb:.2f} MB\")\n",
    "\n",
    "# Show detailed bit-width configuration for mixed models\n",
    "print(\"\\n🔍 Mixed-Precision Configuration Details:\")\n",
    "for name in ['mixed_conservative', 'mixed_aggressive']:\n",
    "    print(f\"\\n{name}:\")\n",
    "    bit_widths = models[name].get_layer_bit_widths()\n",
    "    for layer, widths in bit_widths.items():\n",
    "        print(f\"   {layer}: W={widths['weight']}bit, A={widths['activation']}bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Dataset and Baseline Training\n",
    "\n",
    "Prepare dataset and establish full-precision baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Create smaller datasets for demonstration\n",
    "train_subset = Subset(train_dataset, range(0, 8000))  # 8k samples\n",
    "test_subset = Subset(test_dataset, range(0, 1000))    # 1k samples\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"✅ Dataset prepared\")\n",
    "print(f\"   Training samples: {len(train_subset):,}\")\n",
    "print(f\"   Test samples: {len(test_subset):,}\")\n",
    "\n",
    "# Training utility functions\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, model_name=\"Model\"):\n",
    "    \"\"\"Train a model and return final validation accuracy\"\"\"\n",
    "    print(f\"🎓 Training {model_name}...\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            if batch_idx % 40 == 0:\n",
    "                print(f'   Epoch {epoch+1}/{epochs}, Batch {batch_idx}, '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        val_acc = evaluate_model(model, val_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        print(f'   Epoch {epoch+1} - Train Acc: {train_acc:.3f}, '\n",
    "              f'Val Acc: {val_acc:.3f}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    final_acc = evaluate_model(model, val_loader)\n",
    "    print(f\"✅ {model_name} training complete - Final accuracy: {final_acc:.3f}\")\n",
    "    return final_acc\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "print(\"\\n✅ Training utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Quantization-Aware Training (QAT)\n",
    "\n",
    "**Paper Reference**: *\"Quantization-aware training algorithms enable models to adapt to quantization during training, achieving better accuracy than post-training quantization.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different precision models\n",
    "print(\"🚀 QUANTIZATION-AWARE TRAINING EXPERIMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {}\n",
    "training_epochs = 8  # Reduced for demonstration\n",
    "\n",
    "# Train each configuration\n",
    "for config_name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create a fresh copy of the model\n",
    "    fresh_model = MixedPrecisionCNN(\n",
    "        num_classes=10, \n",
    "        precision_config=precision_configs[config_name]\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    accuracy = train_model(\n",
    "        fresh_model, train_loader, test_loader, \n",
    "        epochs=training_epochs, lr=0.001, model_name=config_name\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[config_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'compression_ratio': fresh_model.get_total_compression_ratio(),\n",
    "        'memory_mb': fresh_model.get_memory_footprint(),\n",
    "        'model': fresh_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n📊 {config_name} Results:\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   Compression: {results[config_name]['compression_ratio']:.1f}x\")\n",
    "    print(f\"   Memory: {results[config_name]['memory_mb']:.2f} MB\")\n",
    "\n",
    "print(f\"\\n\\n🏆 QUANTIZATION-AWARE TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "\n",
    "fp32_accuracy = results['fp32']['accuracy']\n",
    "fp32_memory = results['fp32']['memory_mb']\n",
    "\n",
    "print(f\"{'Configuration':<20} {'Accuracy':<10} {'Acc Drop':<10} {'Compression':<12} {'Memory':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for config_name, result in sorted_results:\n",
    "    acc = result['accuracy']\n",
    "    acc_drop = (fp32_accuracy - acc) / fp32_accuracy * 100\n",
    "    compression = result['compression_ratio']\n",
    "    memory = result['memory_mb']\n",
    "    \n",
    "    print(f\"{config_name:<20} {acc:<10.3f} {acc_drop:<10.1f}% \"\n",
    "          f\"{compression:<12.1f}x {memory:<10.2f}MB\")\n",
    "\n",
    "# Find best trade-offs\n",
    "print(f\"\\n🎯 OPTIMIZATION INSIGHTS:\")\n",
    "\n",
    "# Best accuracy with significant compression\n",
    "quantized_results = {k: v for k, v in results.items() if k != 'fp32'}\n",
    "best_quantized = max(quantized_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"   Best quantized model: {best_quantized[0]} ({best_quantized[1]['accuracy']:.3f} accuracy)\")\n",
    "\n",
    "# Best compression with reasonable accuracy\n",
    "best_compression = max(quantized_results.items(), key=lambda x: x[1]['compression_ratio'])\n",
    "print(f\"   Highest compression: {best_compression[0]} ({best_compression[1]['compression_ratio']:.1f}x)\")\n",
    "\n",
    "# Calculate efficiency score (accuracy / memory)\n",
    "efficiency_scores = {}\n",
    "for name, result in quantized_results.items():\n",
    "    efficiency_scores[name] = result['accuracy'] / result['memory_mb']\n",
    "\n",
    "best_efficiency = max(efficiency_scores.items(), key=lambda x: x[1])\n",
    "print(f\"   Best efficiency: {best_efficiency[0]} (score: {best_efficiency[1]:.3f})\")\n",
    "\n",
    "print(f\"\\n✅ Quantization-aware training experiments complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Mixed-Precision Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Mixed-Precision Quantization: Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract data for visualization\n",
    "config_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in config_names]\n",
    "compressions = [results[name]['compression_ratio'] for name in config_names]\n",
    "memories = [results[name]['memory_mb'] for name in config_names]\n",
    "\n",
    "# Define colors\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# 1. Accuracy vs Compression Trade-off\n",
    "scatter1 = ax1.scatter(compressions, accuracies, c=colors[:len(config_names)], s=150, alpha=0.8)\n",
    "ax1.set_xlabel('Compression Ratio (x)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy vs Compression Trade-off')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels\n",
    "for i, (name, comp, acc) in enumerate(zip(config_names, compressions, accuracies)):\n",
    "    ax1.annotate(name, (comp, acc), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Add Pareto frontier\n",
    "pareto_points = []\n",
    "for i, (comp_i, acc_i) in enumerate(zip(compressions, accuracies)):\n",
    "    is_pareto = True\n",
    "    for j, (comp_j, acc_j) in enumerate(zip(compressions, accuracies)):\n",
    "        if i != j and comp_j >= comp_i and acc_j >= acc_i and (comp_j > comp_i or acc_j > acc_i):\n",
    "            is_pareto = False\n",
    "            break\n",
    "    if is_pareto:\n",
    "        pareto_points.append((comp_i, acc_i))\n",
    "\n",
    "if len(pareto_points) > 1:\n",
    "    pareto_points.sort()\n",
    "    pareto_x, pareto_y = zip(*pareto_points)\n",
    "    ax1.plot(pareto_x, pareto_y, 'r--', alpha=0.7, linewidth=2, label='Pareto Frontier')\n",
    "    ax1.legend()\n",
    "\n",
    "# 2. Memory Footprint Analysis\n",
    "bars2 = ax2.bar(config_names, memories, color=colors[:len(config_names)], alpha=0.8)\n",
    "ax2.set_ylabel('Memory Footprint (MB)')\n",
    "ax2.set_title('Model Memory Requirements')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, mem in zip(bars2, memories):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{mem:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Accuracy Drop Analysis\n",
    "fp32_acc = results['fp32']['accuracy']\n",
    "acc_drops = [(fp32_acc - acc) / fp32_acc * 100 for acc in accuracies]\n",
    "quantized_names = [name for name in config_names if name != 'fp32']\n",
    "quantized_drops = [drop for name, drop in zip(config_names, acc_drops) if name != 'fp32']\n",
    "\n",
    "bars3 = ax3.bar(quantized_names, quantized_drops, \n",
    "                color=colors[1:len(quantized_names)+1], alpha=0.8)\n",
    "ax3.set_ylabel('Accuracy Drop (%)')\n",
    "ax3.set_title('Accuracy Degradation from FP32')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=5, color='red', linestyle='--', alpha=0.7, label='5% Threshold')\n",
    "ax3.axhline(y=10, color='orange', linestyle='--', alpha=0.7, label='10% Threshold')\n",
    "ax3.legend()\n",
    "plt.setp(ax3.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, drop in zip(bars3, quantized_drops):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "             f'{drop:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Efficiency Analysis (Accuracy per MB)\n",
    "efficiencies = [acc / mem for acc, mem in zip(accuracies, memories)]\n",
    "\n",
    "bars4 = ax4.bar(config_names, efficiencies, color=colors[:len(config_names)], alpha=0.8)\n",
    "ax4.set_ylabel('Efficiency (Accuracy / MB)')\n",
    "ax4.set_title('Memory Efficiency Analysis')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.setp(ax4.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, eff in zip(bars4, efficiencies):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{eff:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Mixed-precision analysis visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Advanced Quantization Techniques\n",
    "\n",
    "Implement and analyze advanced quantization methods from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedQuantizationTechniques:\n",
    "    \"\"\"Implementation of advanced quantization methods from the paper\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def layer_sensitivity_analysis(model, data_loader, num_batches=10):\n",
    "        \"\"\"Analyze sensitivity of different layers to quantization\"\"\"\n",
    "        print(\"🔍 Performing layer sensitivity analysis...\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Get baseline accuracy\n",
    "        baseline_acc = evaluate_model(model, data_loader)\n",
    "        \n",
    "        # Identify quantizable layers\n",
    "        quantizable_layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (QuantizedLinear, QuantizedConv2d)):\n",
    "                quantizable_layers.append((name, module))\n",
    "        \n",
    "        sensitivity_scores = {}\n",
    "        \n",
    "        for layer_name, layer in quantizable_layers:\n",
    "            # Temporarily reduce precision of this layer\n",
    "            original_w_bits = layer.weight_bit_width\n",
    "            original_a_bits = layer.activation_bit_width\n",
    "            \n",
    "            # Test with reduced precision\n",
    "            layer.weight_quantizer.bit_width = max(2, original_w_bits - 2)\n",
    "            layer.activation_quantizer.bit_width = max(2, original_a_bits - 2)\n",
    "            \n",
    "            # Recalibrate quantizers\n",
    "            layer.weight_quantizer.scale = None\n",
    "            layer.weight_quantizer.zero_point = None\n",
    "            layer.activation_quantizer.scale = None\n",
    "            layer.activation_quantizer.zero_point = None\n",
    "            \n",
    "            # Evaluate with reduced precision\n",
    "            reduced_acc = evaluate_model(model, data_loader)\n",
    "            sensitivity = baseline_acc - reduced_acc\n",
    "            sensitivity_scores[layer_name] = sensitivity\n",
    "            \n",
    "            # Restore original precision\n",
    "            layer.weight_quantizer.bit_width = original_w_bits\n",
    "            layer.activation_quantizer.bit_width = original_a_bits\n",
    "            layer.weight_quantizer.scale = None\n",
    "            layer.weight_quantizer.zero_point = None\n",
    "            layer.activation_quantizer.scale = None\n",
    "            layer.activation_quantizer.zero_point = None\n",
    "            \n",
    "            print(f\"   {layer_name}: sensitivity = {sensitivity:.4f}\")\n",
    "        \n",
    "        return sensitivity_scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimal_bit_allocation(sensitivity_scores, total_bit_budget, num_layers):\n",
    "        \"\"\"Allocate bit-widths based on sensitivity analysis\"\"\"\n",
    "        print(\"🎯 Computing optimal bit allocation...\")\n",
    "        \n",
    "        # Simple greedy allocation based on sensitivity\n",
    "        sorted_layers = sorted(sensitivity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        bit_allocation = {}\n",
    "        remaining_budget = total_bit_budget\n",
    "        \n",
    "        for layer_name, sensitivity in sorted_layers:\n",
    "            if remaining_budget >= 8:\n",
    "                # High sensitivity layers get more bits\n",
    "                if sensitivity > 0.05:\n",
    "                    bits = 8\n",
    "                elif sensitivity > 0.02:\n",
    "                    bits = 6\n",
    "                else:\n",
    "                    bits = 4\n",
    "            else:\n",
    "                bits = max(2, remaining_budget)\n",
    "            \n",
    "            bit_allocation[layer_name] = bits\n",
    "            remaining_budget -= bits\n",
    "            \n",
    "            if remaining_budget <= 0:\n",
    "                break\n",
    "        \n",
    "        return bit_allocation\n",
    "    \n",
    "    @staticmethod\n",
    "    def progressive_quantization(model, train_loader, val_loader, epochs=10):\n",
    "        \"\"\"Progressive quantization: gradually reduce precision during training\"\"\"\n",
    "        print(\"📈 Starting progressive quantization...\")\n",
    "        \n",
    "        # Initial configuration (high precision)\n",
    "        initial_bits = 8\n",
    "        final_bits = 4\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        progress_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Calculate current bit-width (linearly decrease)\n",
    "            progress = epoch / (epochs - 1) if epochs > 1 else 0\n",
    "            current_bits = int(initial_bits - (initial_bits - final_bits) * progress)\n",
    "            \n",
    "            # Update quantization schemes\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, (QuantizedLinear, QuantizedConv2d)):\n",
    "                    module.weight_quantizer.bit_width = current_bits\n",
    "                    module.activation_quantizer.bit_width = max(current_bits, 6)  # Keep activations higher\n",
    "                    # Reset calibration\n",
    "                    module.weight_quantizer.scale = None\n",
    "                    module.weight_quantizer.zero_point = None\n",
    "                    module.activation_quantizer.scale = None\n",
    "                    module.activation_quantizer.zero_point = None\n",
    "            \n",
    "            # Training epoch\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                if batch_idx > 20:  # Limit batches for demo\n",
    "                    break\n",
    "                    \n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            val_acc = evaluate_model(model, val_loader)\n",
    "            \n",
    "            progress_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'bit_width': current_bits,\n",
    "                'accuracy': val_acc,\n",
    "                'loss': total_loss / min(21, len(train_loader))\n",
    "            })\n",
    "            \n",
    "            print(f\"   Epoch {epoch+1}: {current_bits}-bit weights, \"\n",
    "                  f\"Accuracy: {val_acc:.3f}, Loss: {total_loss/21:.4f}\")\n",
    "        \n",
    "        return progress_history\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantization_error_analysis(model, data_loader, num_batches=5):\n",
    "        \"\"\"Analyze quantization error distribution across layers\"\"\"\n",
    "        print(\"📊 Analyzing quantization error distribution...\")\n",
    "        \n",
    "        model.eval()\n",
    "        layer_errors = defaultdict(list)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_count = 0\n",
    "            for data, _ in data_loader:\n",
    "                if batch_count >= num_batches:\n",
    "                    break\n",
    "                    \n",
    "                data = data.to(device)\n",
    "                \n",
    "                # Hook to capture layer outputs\n",
    "                layer_outputs = {}\n",
    "                \n",
    "                def hook_fn(name):\n",
    "                    def hook(module, input, output):\n",
    "                        if hasattr(module, 'weight_quantizer'):\n",
    "                            # Calculate quantization error for weights\n",
    "                            if hasattr(module, 'linear'):\n",
    "                                original_weight = module.linear.weight\n",
    "                            else:\n",
    "                                original_weight = module.conv.weight\n",
    "                            \n",
    "                            quantized_weight = module.weight_quantizer.quantize(original_weight)\n",
    "                            weight_error = F.mse_loss(original_weight, quantized_weight).item()\n",
    "                            layer_errors[name].append(weight_error)\n",
    "                    return hook\n",
    "                \n",
    "                # Register hooks\n",
    "                hooks = []\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, (QuantizedLinear, QuantizedConv2d)):\n",
    "                        hook = module.register_forward_hook(hook_fn(name))\n",
    "                        hooks.append(hook)\n",
    "                \n",
    "                # Forward pass\n",
    "                _ = model(data)\n",
    "                \n",
    "                # Remove hooks\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "                \n",
    "                batch_count += 1\n",
    "        \n",
    "        # Compute statistics\n",
    "        error_stats = {}\n",
    "        for layer_name, errors in layer_errors.items():\n",
    "            error_stats[layer_name] = {\n",
    "                'mean': np.mean(errors),\n",
    "                'std': np.std(errors),\n",
    "                'max': np.max(errors),\n",
    "                'min': np.min(errors)\n",
    "            }\n",
    "        \n",
    "        return error_stats\n",
    "\n",
    "# Initialize advanced techniques analyzer\n",
    "advanced_quant = AdvancedQuantizationTechniques()\n",
    "\n",
    "print(\"✅ Advanced quantization techniques implemented\")\n",
    "print(\"   Available methods:\")\n",
    "print(\"   - Layer sensitivity analysis\")\n",
    "print(\"   - Optimal bit allocation\")\n",
    "print(\"   - Progressive quantization\")\n",
    "print(\"   - Quantization error analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Advanced Quantization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔬 ADVANCED QUANTIZATION EXPERIMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the best performing mixed-precision model for advanced analysis\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'] if x[0] != 'fp32' else 0)[0]\n",
    "test_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Using model: {best_model_name} (Accuracy: {results[best_model_name]['accuracy']:.3f})\")\n",
    "\n",
    "# 1. Layer Sensitivity Analysis\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"1. LAYER SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "sensitivity_scores = advanced_quant.layer_sensitivity_analysis(test_model, test_loader)\n",
    "\n",
    "# Sort by sensitivity\n",
    "sorted_sensitivity = sorted(sensitivity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n📊 Layer Sensitivity Ranking:\")\n",
    "for i, (layer_name, sensitivity) in enumerate(sorted_sensitivity):\n",
    "    sensitivity_level = \"HIGH\" if sensitivity > 0.05 else \"MEDIUM\" if sensitivity > 0.02 else \"LOW\"\n",
    "    print(f\"   {i+1}. {layer_name:<15}: {sensitivity:.4f} ({sensitivity_level})\")\n",
    "\n",
    "# 2. Optimal Bit Allocation\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"2. OPTIMAL BIT ALLOCATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "total_budget = 40  # Total bits to allocate across layers\n",
    "optimal_allocation = advanced_quant.optimal_bit_allocation(\n",
    "    sensitivity_scores, total_budget, len(sensitivity_scores)\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 Optimal Bit Allocation:\")\n",
    "total_allocated = 0\n",
    "for layer_name, bits in optimal_allocation.items():\n",
    "    sensitivity = sensitivity_scores[layer_name]\n",
    "    print(f\"   {layer_name:<15}: {bits} bits (sensitivity: {sensitivity:.4f})\")\n",
    "    total_allocated += bits\n",
    "\n",
    "print(f\"\\n   Total allocated: {total_allocated} bits (budget: {total_budget})\")\n",
    "\n",
    "# 3. Quantization Error Analysis\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"3. QUANTIZATION ERROR ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "error_stats = advanced_quant.quantization_error_analysis(test_model, test_loader)\n",
    "\n",
    "print(\"\\n📈 Quantization Error Statistics:\")\n",
    "print(f\"{'Layer':<15} {'Mean Error':<12} {'Std Error':<12} {'Max Error':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for layer_name, stats in error_stats.items():\n",
    "    print(f\"{layer_name:<15} {stats['mean']:<12.6f} {stats['std']:<12.6f} {stats['max']:<12.6f}\")\n",
    "\n",
    "# 4. Hardware Cost Analysis\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"4. HARDWARE COST ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "def estimate_hardware_cost(model, bit_config):\n",
    "    \"\"\"Estimate hardware costs for different bit configurations\"\"\"\n",
    "    total_ops = 0\n",
    "    total_memory_accesses = 0\n",
    "    total_energy = 0\n",
    "    \n",
    "    # Simplified hardware cost model\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantizedConv2d):\n",
    "            # Convolution operations\n",
    "            input_size = 32 * 32  # Assume CIFAR-10 input\n",
    "            kernel_ops = module.conv.kernel_size[0] ** 2\n",
    "            output_channels = module.conv.out_channels\n",
    "            ops = input_size * kernel_ops * output_channels\n",
    "            \n",
    "            # Bit-width affects operation cost\n",
    "            w_bits = module.weight_bit_width\n",
    "            a_bits = module.activation_bit_width\n",
    "            \n",
    "            # Cost scales with bit-width complexity\n",
    "            cost_factor = (w_bits * a_bits) / (8 * 8)  # Normalized to 8-bit\n",
    "            total_ops += ops * cost_factor\n",
    "            \n",
    "            # Memory access cost\n",
    "            weight_memory = module.conv.weight.numel() * w_bits / 8  # bytes\n",
    "            total_memory_accesses += weight_memory\n",
    "            \n",
    "            # Energy cost (simplified)\n",
    "            total_energy += ops * cost_factor * 0.1  # pJ per operation\n",
    "        \n",
    "        elif isinstance(module, QuantizedLinear):\n",
    "            # Linear operations\n",
    "            ops = module.linear.in_features * module.linear.out_features\n",
    "            \n",
    "            w_bits = module.weight_bit_width\n",
    "            a_bits = module.activation_bit_width\n",
    "            \n",
    "            cost_factor = (w_bits * a_bits) / (8 * 8)\n",
    "            total_ops += ops * cost_factor\n",
    "            \n",
    "            weight_memory = module.linear.weight.numel() * w_bits / 8\n",
    "            total_memory_accesses += weight_memory\n",
    "            \n",
    "            total_energy += ops * cost_factor * 0.05  # Lower energy for FC\n",
    "    \n",
    "    return {\n",
    "        'total_ops': total_ops,\n",
    "        'memory_accesses_kb': total_memory_accesses / 1024,\n",
    "        'energy_nj': total_energy / 1000\n",
    "    }\n",
    "\n",
    "# Compare hardware costs across different configurations\n",
    "print(\"\\n💻 Hardware Cost Comparison:\")\n",
    "print(f\"{'Configuration':<20} {'Ops (M)':<12} {'Memory (KB)':<14} {'Energy (nJ)':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for config_name, result in results.items():\n",
    "    if config_name != 'fp32':  # Skip FP32 for hardware analysis\n",
    "        model = result['model']\n",
    "        costs = estimate_hardware_cost(model, None)\n",
    "        \n",
    "        print(f\"{config_name:<20} {costs['total_ops']/1e6:<12.1f} \"\n",
    "              f\"{costs['memory_accesses_kb']:<14.1f} {costs['energy_nj']:<12.1f}\")\n",
    "\n",
    "print(\"\\n✅ Advanced quantization experiments complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Research Extensions: Next-Generation Quantization\n",
    "\n",
    "Advanced research directions for quantization optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextGenQuantizationResearch:\n",
    "    \"\"\"Research framework for next-generation quantization techniques\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.research_directions = [\n",
    "            {\n",
    "                'name': 'Learnable Quantization Parameters',\n",
    "                'description': 'Make quantization scales and zero-points learnable parameters',\n",
    "                'paper_reference': 'LSQ: Learned Step Size Quantization',\n",
    "                'complexity': 'Medium',\n",
    "                'potential_impact': 'High'\n",
    "            },\n",
    "            {\n",
    "                'name': 'Adaptive Bit-Width Selection',\n",
    "                'description': 'Dynamically adjust bit-widths based on input characteristics',\n",
    "                'paper_reference': 'BitWidth-Adaptive Quantization',\n",
    "                'complexity': 'High',\n",
    "                'potential_impact': 'Very High'\n",
    "            },\n",
    "            {\n",
    "                'name': 'Non-Uniform Quantization',\n",
    "                'description': 'Use non-uniform quantization levels based on weight/activation distributions',\n",
    "                'paper_reference': 'PACT: Parameterized Clipping Activation',\n",
    "                'complexity': 'Medium',\n",
    "                'potential_impact': 'Medium'\n",
    "            },\n",
    "            {\n",
    "                'name': 'Hardware-Specific Quantization',\n",
    "                'description': 'Co-design quantization with specific hardware accelerators',\n",
    "                'paper_reference': 'Hardware-Aware Quantization (HAQ)',\n",
    "                'complexity': 'Very High',\n",
    "                'potential_impact': 'Very High'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def generate_implementation_template(self, research_idx: int) -> str:\n",
    "        \"\"\"Generate implementation template for research direction\"\"\"\n",
    "        if research_idx >= len(self.research_directions):\n",
    "            raise ValueError(\"Invalid research index\")\n",
    "        \n",
    "        direction = self.research_directions[research_idx]\n",
    "        \n",
    "        templates = {\n",
    "            'Learnable Quantization Parameters': '''\n",
    "class LearnableQuantization(nn.Module):\n",
    "    def __init__(self, bit_width=8, init_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.bit_width = bit_width\n",
    "        # Learnable parameters\n",
    "        self.scale = nn.Parameter(torch.tensor(init_scale))\n",
    "        self.zero_point = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize with learnable parameters\n",
    "        qmin, qmax = 0, 2**self.bit_width - 1\n",
    "        \n",
    "        # Gradients flow through scale and zero_point\n",
    "        scaled = x / self.scale + self.zero_point\n",
    "        quantized = torch.clamp(torch.round(scaled), qmin, qmax)\n",
    "        \n",
    "        # Straight-through estimator\n",
    "        output = (quantized - self.zero_point) * self.scale\n",
    "        return output + (x - output).detach()  # STE\n",
    "            ''',\n",
    "            'Adaptive Bit-Width Selection': '''\n",
    "class AdaptiveBitWidthQuantization(nn.Module):\n",
    "    def __init__(self, min_bits=2, max_bits=8):\n",
    "        super().__init__()\n",
    "        self.min_bits = min_bits\n",
    "        self.max_bits = max_bits\n",
    "        \n",
    "        # Bit-width predictor network\n",
    "        self.bit_predictor = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Predict optimal bit-width for this input\n",
    "        bit_score = self.bit_predictor(x)\n",
    "        bit_width = self.min_bits + bit_score * (self.max_bits - self.min_bits)\n",
    "        \n",
    "        # Quantize with predicted bit-width\n",
    "        return self.quantize_with_bits(x, bit_width)\n",
    "    \n",
    "    def quantize_with_bits(self, x, bit_width):\n",
    "        # Dynamic quantization based on predicted bit-width\n",
    "        levels = 2 ** bit_width.int()\n",
    "        scale = 2 * x.abs().max() / (levels - 1)\n",
    "        quantized = torch.round(x / scale) * scale\n",
    "        return quantized\n",
    "            ''',\n",
    "            'Non-Uniform Quantization': '''\n",
    "class NonUniformQuantization(nn.Module):\n",
    "    def __init__(self, num_levels=256):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_levels\n",
    "        # Learnable quantization levels\n",
    "        self.levels = nn.Parameter(torch.linspace(-1, 1, num_levels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Find closest quantization level for each value\n",
    "        x_expanded = x.unsqueeze(-1)  # [..., 1]\n",
    "        levels_expanded = self.levels.view(1, -1)  # [1, num_levels]\n",
    "        \n",
    "        # Compute distances to all levels\n",
    "        distances = torch.abs(x_expanded - levels_expanded)\n",
    "        \n",
    "        # Find closest level indices\n",
    "        closest_indices = torch.argmin(distances, dim=-1)\n",
    "        \n",
    "        # Quantize to closest levels\n",
    "        quantized = self.levels[closest_indices]\n",
    "        \n",
    "        # Straight-through estimator\n",
    "        return quantized + (x - quantized).detach()\n",
    "            ''',\n",
    "            'Hardware-Specific Quantization': '''\n",
    "class HardwareAwareQuantization(nn.Module):\n",
    "    def __init__(self, hardware_profile):\n",
    "        super().__init__()\n",
    "        self.hardware_profile = hardware_profile\n",
    "        \n",
    "        # Hardware-specific quantization parameters\n",
    "        if hardware_profile == 'mobile_cpu':\n",
    "            self.preferred_bits = [4, 8]  # INT4, INT8\n",
    "            self.asymmetric = True\n",
    "        elif hardware_profile == 'edge_tpu':\n",
    "            self.preferred_bits = [8]  # INT8 only\n",
    "            self.asymmetric = False\n",
    "        elif hardware_profile == 'gpu':\n",
    "            self.preferred_bits = [16]  # FP16\n",
    "            self.asymmetric = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Select bit-width based on hardware constraints\n",
    "        bit_width = self.select_optimal_bits(x)\n",
    "        return self.hardware_optimized_quantize(x, bit_width)\n",
    "    \n",
    "    def select_optimal_bits(self, x):\n",
    "        # Hardware-specific bit selection logic\n",
    "        if self.hardware_profile == 'mobile_cpu':\n",
    "            # Prefer lower bits for mobile efficiency\n",
    "            return 4 if x.numel() > 1000 else 8\n",
    "        else:\n",
    "            return self.preferred_bits[0]\n",
    "    \n",
    "    def hardware_optimized_quantize(self, x, bit_width):\n",
    "        # Hardware-specific quantization implementation\n",
    "        if self.hardware_profile == 'edge_tpu':\n",
    "            # TPU-optimized symmetric quantization\n",
    "            scale = 2 * x.abs().max() / (2**bit_width - 1)\n",
    "            return torch.round(x / scale) * scale\n",
    "        else:\n",
    "            # Standard quantization\n",
    "            return self.standard_quantize(x, bit_width)\n",
    "            '''\n",
    "        }\n",
    "        \n",
    "        return templates.get(direction['name'], 'Template not available')\n",
    "    \n",
    "    def generate_experiment_plan(self, research_idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Generate detailed experiment plan\"\"\"\n",
    "        direction = self.research_directions[research_idx]\n",
    "        \n",
    "        experiment_plans = {\n",
    "            'Learnable Quantization Parameters': {\n",
    "                'hypothesis': 'Learnable quantization parameters adapt better to data distribution',\n",
    "                'methodology': [\n",
    "                    'Replace fixed quantization with learnable scale/zero-point',\n",
    "                    'Train with gradient-based optimization of quantization parameters',\n",
    "                    'Compare with fixed uniform quantization',\n",
    "                    'Analyze learned parameter distributions'\n",
    "                ],\n",
    "                'metrics': ['Accuracy', 'Quantization error', 'Parameter convergence'],\n",
    "                'expected_improvement': '2-5% accuracy improvement over fixed quantization'\n",
    "            },\n",
    "            'Adaptive Bit-Width Selection': {\n",
    "                'hypothesis': 'Dynamic bit-width allocation improves efficiency-accuracy trade-off',\n",
    "                'methodology': [\n",
    "                    'Train bit-width predictor network',\n",
    "                    'Implement dynamic quantization based on input characteristics',\n",
    "                    'Compare with fixed mixed-precision approaches',\n",
    "                    'Analyze bit-width allocation patterns'\n",
    "                ],\n",
    "                'metrics': ['Average bit-width', 'Accuracy', 'Inference efficiency'],\n",
    "                'expected_improvement': '10-20% better efficiency at same accuracy'\n",
    "            },\n",
    "            'Non-Uniform Quantization': {\n",
    "                'hypothesis': 'Non-uniform levels better capture weight/activation distributions',\n",
    "                'methodology': [\n",
    "                    'Analyze weight/activation distributions',\n",
    "                    'Design optimal non-uniform quantization levels',\n",
    "                    'Compare with uniform quantization',\n",
    "                    'Evaluate hardware implementation feasibility'\n",
    "                ],\n",
    "                'metrics': ['Quantization error', 'Accuracy', 'Hardware cost'],\n",
    "                'expected_improvement': '3-7% better accuracy at same bit-width'\n",
    "            },\n",
    "            'Hardware-Specific Quantization': {\n",
    "                'hypothesis': 'Hardware co-design maximizes deployment efficiency',\n",
    "                'methodology': [\n",
    "                    'Profile target hardware characteristics',\n",
    "                    'Design hardware-aware quantization schemes',\n",
    "                    'Implement hardware-specific optimizations',\n",
    "                    'Validate on actual hardware platforms'\n",
    "                ],\n",
    "                'metrics': ['Real hardware latency', 'Energy consumption', 'Accuracy'],\n",
    "                'expected_improvement': '2-5x better hardware efficiency'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        base_info = direction\n",
    "        detailed_plan = experiment_plans[direction['name']]\n",
    "        \n",
    "        return {**base_info, **detailed_plan}\n",
    "\n",
    "# Initialize research framework\n",
    "next_gen_research = NextGenQuantizationResearch()\n",
    "\n",
    "print(\"🔬 NEXT-GENERATION QUANTIZATION RESEARCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, direction in enumerate(next_gen_research.research_directions):\n",
    "    print(f\"\\n{i+1}. {direction['name']}\")\n",
    "    print(f\"   📝 {direction['description']}\")\n",
    "    print(f\"   📚 Reference: {direction['paper_reference']}\")\n",
    "    print(f\"   🔧 Complexity: {direction['complexity']}\")\n",
    "    print(f\"   🎯 Impact: {direction['potential_impact']}\")\n",
    "\n",
    "# Generate detailed experiment plan\n",
    "example_plan = next_gen_research.generate_experiment_plan(1)  # Adaptive Bit-Width\n",
    "\n",
    "print(f\"\\n\\n🧪 DETAILED EXPERIMENT PLAN: {example_plan['name']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Hypothesis: {example_plan['hypothesis']}\")\n",
    "print(f\"\\nMethodology:\")\n",
    "for step in example_plan['methodology']:\n",
    "    print(f\"   • {step}\")\n",
    "print(f\"\\nMetrics: {example_plan['metrics']}\")\n",
    "print(f\"Expected Improvement: {example_plan['expected_improvement']}\")\n",
    "\n",
    "# Show implementation template\n",
    "print(f\"\\n\\n💻 IMPLEMENTATION TEMPLATE:\")\n",
    "print(\"=\" * 60)\n",
    "template = next_gen_research.generate_implementation_template(1)\n",
    "print(template)\n",
    "\n",
    "print(\"\\n✅ Next-generation quantization research framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Key Takeaways & Summary\n",
    "\n",
    "### 🎯 Concepts Mastered:\n",
    "\n",
    "1. **Mixed-Precision Quantization**: Successfully implemented different bit-widths across network layers for optimal accuracy-efficiency trade-offs\n",
    "\n",
    "2. **Quantization-Aware Training (QAT)**: Demonstrated how training with quantization simulation achieves better results than post-training quantization\n",
    "\n",
    "3. **Hardware-Aware Optimization**: Analyzed hardware costs (memory, operations, energy) for different quantization configurations\n",
    "\n",
    "4. **Sensitivity Analysis**: Identified which layers are most critical for accuracy and require higher precision\n",
    "\n",
    "5. **Advanced Bit Allocation**: Implemented optimal bit-width distribution based on layer sensitivity\n",
    "\n",
    "### 📊 Experimental Results:\n",
    "\n",
    "**Quantization Effectiveness:**\n",
    "- **FP32 Baseline**: ~0.750 accuracy, ~2.5 MB memory\n",
    "- **INT8 Uniform**: ~0.720 accuracy (4% drop), 4x compression\n",
    "- **Mixed Conservative**: ~0.735 accuracy (2% drop), 5.2x compression\n",
    "- **Mixed Aggressive**: ~0.710 accuracy (5.3% drop), 7.8x compression\n",
    "- **Ultra Low**: ~0.680 accuracy (9.3% drop), 12.0x compression\n",
    "\n",
    "**Key Insights:**\n",
    "- **Mixed-precision** consistently outperforms uniform quantization\n",
    "- **First and last layers** are most sensitive to quantization\n",
    "- **Activations** can be kept at higher precision with minimal cost\n",
    "- **10x+ compression** possible with <10% accuracy drop\n",
    "\n",
    "### 🔬 Advanced Techniques Implemented:\n",
    "\n",
    "1. **Layer Sensitivity Analysis**: Systematic identification of quantization-sensitive layers\n",
    "2. **Straight-Through Estimator**: Gradient flow through non-differentiable quantization\n",
    "3. **Progressive Quantization**: Gradual precision reduction during training\n",
    "4. **Hardware Cost Modeling**: Operation count, memory access, and energy estimation\n",
    "\n",
    "### 🎓 Paper Implementation Achievements:\n",
    "\n",
    "**Successfully implemented paper concepts:**\n",
    "- ✅ **Mixed-precision quantization** with layer-specific bit-widths\n",
    "- ✅ **Hardware-Aware Automated Quantization (HAQ)** framework\n",
    "- ✅ **Progressive fractional quantization** concepts\n",
    "- ✅ **Parametric quantization** with learnable parameters\n",
    "- ✅ **Hardware-software co-design** considerations\n",
    "\n",
    "### 🚀 Research Extensions Ready:\n",
    "\n",
    "1. **Learnable Quantization Parameters**: Making scales and zero-points trainable\n",
    "2. **Adaptive Bit-Width Selection**: Dynamic precision based on input characteristics\n",
    "3. **Non-Uniform Quantization**: Custom quantization levels based on data distribution\n",
    "4. **Hardware-Specific Optimization**: Co-design with specific accelerators (TPU, mobile CPU)\n",
    "\n",
    "### 🏆 Edge AI Impact:\n",
    "\n",
    "Mixed-precision quantization enables:\n",
    "- **Massive model compression** (5-12x parameter reduction)\n",
    "- **Preserved accuracy** (<5% degradation with careful design)\n",
    "- **Hardware efficiency** (reduced memory, operations, energy)\n",
    "- **Flexible deployment** (different precision configs for different devices)\n",
    "- **Real-time inference** on resource-constrained edge devices\n",
    "\n",
    "### 🔧 Practical Guidelines:\n",
    "\n",
    "1. **Start conservative**: Use 8-bit weights, 8-bit activations as baseline\n",
    "2. **Protect critical layers**: Keep first and last layers at higher precision\n",
    "3. **Use sensitivity analysis**: Guide bit allocation with systematic testing\n",
    "4. **Consider hardware**: Match quantization to target deployment platform\n",
    "5. **Train with quantization**: QAT consistently outperforms post-training methods\n",
    "\n",
    "---\n",
    "\n",
    "**📄 Paper Citation**: Wang, X., & Jia, W. (2025). *Optimizing Edge AI: A Comprehensive Survey on Data, Model, and System Strategies*. arXiv:2501.03265v1. **Sections 13-14**: Mixed-Precision Quantization with Hardware Co-Design.\n",
    "\n",
    "**🔗 Next**: Continue with **Focused Learning Notebook 4: Structured Pruning Strategies** to explore systematic network compression through structural removal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}