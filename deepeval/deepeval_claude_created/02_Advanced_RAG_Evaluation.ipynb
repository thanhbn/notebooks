{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç DeepEval - Advanced RAG Evaluation\n",
    "\n",
    "Ch√†o m·ª´ng ƒë·∫øn v·ªõi **Notebook 2** trong series DeepEval framework!\n",
    "\n",
    "## üéØ M·ª•c ti√™u c·ªßa Notebook n√†y\n",
    "\n",
    "1. **X√¢y d·ª±ng RAG Pipeline** ho√†n ch·ªânh v·ªõi LangChain\n",
    "2. **RAG-Specific Metrics**: ContextualPrecision, ContextualRecall, ContextualRelevancy, Faithfulness\n",
    "3. **T·ª± ƒë·ªông t·∫°o Dataset** v·ªõi deepeval.Synthesizer\n",
    "4. **Advanced Evaluation Techniques** cho retrieval systems\n",
    "5. **Performance Analysis** v√† optimization strategies\n",
    "\n",
    "## üìñ T·∫°i sao RAG Evaluation quan tr·ªçng?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) l√† m·ªôt trong nh·ªØng architecture ph·ªï bi·∫øn nh·∫•t cho LLM applications, nh∆∞ng vi·ªác ƒë√°nh gi√° RAG systems c√≥ nh·ªØng th√°ch th·ª©c ri√™ng:\n",
    "\n",
    "### üîç Th√°ch th·ª©c c·ªßa RAG Evaluation:\n",
    "- **Multi-stage process**: Retrieval ‚Üí Ranking ‚Üí Generation\n",
    "- **Context quality**: Li·ªáu context c√≥ relevant v√† sufficient?\n",
    "- **Faithfulness**: LLM c√≥ trung th·ª±c v·ªõi retrieved context?\n",
    "- **Completeness**: C√≥ thi·∫øu th√¥ng tin quan tr·ªçng?\n",
    "- **Redundancy**: Context c√≥ b·ªã duplicate kh√¥ng?\n",
    "\n",
    "### ‚úÖ DeepEval gi·∫£i quy·∫øt nh∆∞ th·∫ø n√†o:\n",
    "- **Contextual Metrics**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng retrieval\n",
    "- **Faithfulness Metrics**: Ki·ªÉm tra consistency v·ªõi context\n",
    "- **Automated Dataset Generation**: T·∫°o test cases t·ª´ documents\n",
    "- **End-to-end Evaluation**: ƒê√°nh gi√° to√†n b·ªô RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Ph·∫ßn 1: Setup v√† Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DeepEval imports\n",
    "import deepeval\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric, \n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    AnswerRelevancyMetric\n",
    ")\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# LangChain imports for RAG\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "\n",
    "print(f\"‚úÖ DeepEval version: {deepeval.__version__}\")\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Check API keys\n",
    "api_keys_status = {\n",
    "    \"OpenAI\": \"‚úÖ Configured\" if os.getenv(\"OPENAI_API_KEY\") else \"‚ùå Missing\",\n",
    "    \"Anthropic\": \"‚úÖ Configured\" if os.getenv(\"ANTHROPIC_API_KEY\") else \"‚ùå Missing\"\n",
    "}\n",
    "\n",
    "print(\"üîë API Keys Status:\")\n",
    "for provider, status in api_keys_status.items():\n",
    "    print(f\"  {provider}: {status}\")\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\n‚ö†Ô∏è  C·∫ßn OPENAI_API_KEY ƒë·ªÉ ch·∫°y RAG evaluation!\")\n",
    "    print(\"   T·∫°o file .env v·ªõi: OPENAI_API_KEY=your_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Ph·∫ßn 2: X√¢y d·ª±ng RAG Pipeline\n",
    "\n",
    "### 2.1 Load v√† Prepare Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_documents():\n",
    "    \"\"\"\n",
    "    Load document t·ª´ data folder v√† prepare cho RAG\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load document\n",
    "    doc_path = \"data/rag_document.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"üìÑ Loaded document: {len(content)} characters\")\n",
    "        print(f\"Preview: {content[:200]}...\")\n",
    "        \n",
    "        # Create Document object\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": doc_path, \"title\": \"AI v√† ML Guide\"}\n",
    "        )\n",
    "        \n",
    "        return [doc]\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File kh√¥ng t√¨m th·∫•y: {doc_path}\")\n",
    "        print(\"üí° ƒê·∫£m b·∫£o ƒë√£ ch·∫°y notebook trong ƒë√∫ng directory\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading document: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load documents\n",
    "documents = load_and_prepare_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents th√†nh chunks nh·ªè h∆°n cho retrieval\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "    \n",
    "    # T·∫°o text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # K√≠ch th∆∞·ªõc chunk\n",
    "        chunk_overlap=50,  # Overlap between chunks\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split documents\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"üìë Document split th√†nh {len(chunks)} chunks\")\n",
    "    print(f\"üìè Average chunk size: {np.mean([len(chunk.page_content) for chunk in chunks]):.0f} characters\")\n",
    "    \n",
    "    # Preview first few chunks\n",
    "    print(\"\\nüîç Preview chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"  Chunk {i+1}: {chunk.page_content[:100]}...\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split documents\n",
    "document_chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 T·∫°o Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks: List[Document]) -> Optional[FAISS]:\n",
    "    \"\"\"\n",
    "    T·∫°o FAISS vector store t·ª´ document chunks\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ chunks ƒë·ªÉ t·∫°o vector store\")\n",
    "        return None\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"‚ùå C·∫ßn OPENAI_API_KEY ƒë·ªÉ t·∫°o embeddings\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ ƒêang t·∫°o embeddings...\")\n",
    "        \n",
    "        # T·∫°o embeddings\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        \n",
    "        # T·∫°o FAISS vector store\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Vector store t·∫°o th√†nh c√¥ng v·ªõi {len(chunks)} documents\")\n",
    "        \n",
    "        # Test similarity search\n",
    "        test_query = \"Machine learning l√† g√¨?\"\n",
    "        similar_docs = vector_store.similarity_search(test_query, k=2)\n",
    "        \n",
    "        print(f\"\\nüîç Test similarity search cho '{test_query}':\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"  Result {i+1}: {doc.page_content[:100]}...\")\n",
    "        \n",
    "        return vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "# T·∫°o vector store\n",
    "vector_store = create_vector_store(document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 T·∫°o RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_chain(vector_store: FAISS) -> Optional[RetrievalQA]:\n",
    "    \"\"\"\n",
    "    T·∫°o RAG chain v·ªõi retrieval v√† generation\n",
    "    \"\"\"\n",
    "    if not vector_store:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ vector store ƒë·ªÉ t·∫°o RAG chain\")\n",
    "        return None\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"‚ùå C·∫ßn OPENAI_API_KEY ƒë·ªÉ t·∫°o LLM\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # T·∫°o LLM\n",
    "        llm = OpenAI(\n",
    "            model_name=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0.1,  # Low temperature cho factual responses\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # T·∫°o retriever\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}  # Retrieve top 3 relevant chunks\n",
    "        )\n",
    "        \n",
    "        # T·∫°o RAG chain\n",
    "        rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",  # Combine all retrieved docs\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,  # Return source docs for evaluation\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ RAG chain t·∫°o th√†nh c√¥ng!\")\n",
    "        \n",
    "        return rag_chain\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating RAG chain: {e}\")\n",
    "        return None\n",
    "\n",
    "# T·∫°o RAG chain\n",
    "rag_chain = create_rag_chain(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Test RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_pipeline(rag_chain: RetrievalQA) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Test RAG pipeline v·ªõi sample questions\n",
    "    \"\"\"\n",
    "    if not rag_chain:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ RAG chain ƒë·ªÉ test\")\n",
    "        return {}\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"Machine learning c√≥ nh·ªØng lo·∫°i ch√≠nh n√†o?\",\n",
    "        \"Deep learning kh√°c g√¨ v·ªõi machine learning th√¥ng th∆∞·ªùng?\",\n",
    "        \"AI ƒë∆∞·ª£c ·ª©ng d·ª•ng trong lƒ©nh v·ª±c n√†o?\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"üß™ Testing RAG Pipeline:\\n\")\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        try:\n",
    "            print(f\"‚ùì Question {i}: {question}\")\n",
    "            \n",
    "            # Get RAG response\n",
    "            response = rag_chain({\"query\": question})\n",
    "            \n",
    "            answer = response[\"result\"]\n",
    "            source_docs = response[\"source_documents\"]\n",
    "            \n",
    "            print(f\"üí° Answer: {answer[:200]}...\")\n",
    "            print(f\"üìö Sources: {len(source_docs)} documents retrieved\")\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_documents\": source_docs,\n",
    "                \"num_sources\": len(source_docs)\n",
    "            })\n",
    "            \n",
    "            print(\"\" + \"-\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error v·ªõi question {i}: {e}\")\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": None,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test RAG pipeline\n",
    "rag_test_results = test_rag_pipeline(rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ph·∫ßn 3: RAG-Specific Metrics\n",
    "\n",
    "### 3.1 ContextualRelevancyMetric\n",
    "\n",
    "ƒê√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa retrieved context v·ªõi query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_contextual_relevancy():\n",
    "    \"\"\"\n",
    "    Demo ContextualRelevancyMetric\n",
    "    \"\"\"\n",
    "    \n",
    "    if not rag_test_results or not rag_test_results[0].get(\"answer\"):\n",
    "        print(\"‚ùå C·∫ßn RAG results ƒë·ªÉ demo contextual relevancy\")\n",
    "        return\n",
    "    \n",
    "    # L·∫•y result ƒë·∫ßu ti√™n\n",
    "    test_result = rag_test_results[0]\n",
    "    \n",
    "    # T·∫°o test case cho DeepEval\n",
    "    test_case = LLMTestCase(\n",
    "        input=test_result[\"question\"],\n",
    "        actual_output=test_result[\"answer\"],\n",
    "        retrieval_context=[doc.page_content for doc in test_result[\"source_documents\"]]\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ ContextualRelevancyMetric Demo\")\n",
    "    print(f\"Query: {test_case.input}\")\n",
    "    print(f\"Retrieved contexts: {len(test_case.retrieval_context)}\")\n",
    "    \n",
    "    # Preview contexts\n",
    "    for i, context in enumerate(test_case.retrieval_context):\n",
    "        print(f\"  Context {i+1}: {context[:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        # T·∫°o metric\n",
    "        relevancy_metric = ContextualRelevancyMetric(\n",
    "            threshold=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            include_reason=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        relevancy_metric.measure(test_case)\n",
    "        \n",
    "        print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "        print(f\"  Score: {relevancy_metric.score:.3f}\")\n",
    "        print(f\"  Passed: {'‚úÖ' if relevancy_metric.is_successful() else '‚ùå'}\")\n",
    "        print(f\"  Reason: {relevancy_metric.reason}\")\n",
    "        \n",
    "        return relevancy_metric, test_case\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None, test_case\n",
    "\n",
    "# Run demo\n",
    "contextual_relevancy_result = demo_contextual_relevancy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ContextualPrecisionMetric\n",
    "\n",
    "ƒê√°nh gi√° precision c·ªßa retrieval - li·ªáu c√°c context c√≥ ƒë√∫ng th·ª© t·ª± relevance kh√¥ng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_contextual_precision():\n",
    "    \"\"\"\n",
    "    Demo ContextualPrecisionMetric\n",
    "    \"\"\"\n",
    "    \n",
    "    if not rag_test_results or not rag_test_results[0].get(\"answer\"):\n",
    "        print(\"‚ùå C·∫ßn RAG results ƒë·ªÉ demo contextual precision\")\n",
    "        return\n",
    "    \n",
    "    # S·ª≠ d·ª•ng test case t·ª´ result th·ª© 2\n",
    "    test_result = rag_test_results[1] if len(rag_test_results) > 1 else rag_test_results[0]\n",
    "    \n",
    "    # T·∫°o test case v·ªõi expected_output ƒë·ªÉ ƒë√°nh gi√° precision\n",
    "    test_case = LLMTestCase(\n",
    "        input=test_result[\"question\"],\n",
    "        actual_output=test_result[\"answer\"],\n",
    "        expected_output=\"Deep learning s·ª≠ d·ª•ng neural networks v·ªõi nhi·ªÅu layers ƒë·ªÉ h·ªçc complex patterns trong data, kh√°c v·ªõi ML truy·ªÅn th·ªëng.\",  # Expected answer\n",
    "        retrieval_context=[doc.page_content for doc in test_result[\"source_documents\"]]\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ ContextualPrecisionMetric Demo\")\n",
    "    print(f\"Query: {test_case.input}\")\n",
    "    print(f\"Expected: {test_case.expected_output}\")\n",
    "    print(f\"Actual: {test_case.actual_output[:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        # T·∫°o metric\n",
    "        precision_metric = ContextualPrecisionMetric(\n",
    "            threshold=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            include_reason=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        precision_metric.measure(test_case)\n",
    "        \n",
    "        print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "        print(f\"  Score: {precision_metric.score:.3f}\")\n",
    "        print(f\"  Passed: {'‚úÖ' if precision_metric.is_successful() else '‚ùå'}\")\n",
    "        print(f\"  Reason: {precision_metric.reason}\")\n",
    "        \n",
    "        return precision_metric, test_case\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None, test_case\n",
    "\n",
    "# Run demo\n",
    "contextual_precision_result = demo_contextual_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ContextualRecallMetric\n",
    "\n",
    "ƒê√°nh gi√° recall - li·ªáu t·∫•t c·∫£ th√¥ng tin c·∫ßn thi·∫øt c√≥ ƒë∆∞·ª£c retrieve kh√¥ng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_contextual_recall():\n",
    "    \"\"\"\n",
    "    Demo ContextualRecallMetric\n",
    "    \"\"\"\n",
    "    \n",
    "    if not rag_test_results or not rag_test_results[2].get(\"answer\"):\n",
    "        print(\"‚ùå C·∫ßn RAG results ƒë·ªÉ demo contextual recall\")\n",
    "        return\n",
    "    \n",
    "    # S·ª≠ d·ª•ng c√¢u h·ªèi v·ªÅ ·ª©ng d·ª•ng AI\n",
    "    test_result = rag_test_results[2] if len(rag_test_results) > 2 else rag_test_results[0]\n",
    "    \n",
    "    test_case = LLMTestCase(\n",
    "        input=test_result[\"question\"],\n",
    "        actual_output=test_result[\"answer\"],\n",
    "        expected_output=\"AI ƒë∆∞·ª£c ·ª©ng d·ª•ng trong y t·∫ø (ch·∫©n ƒëo√°n, ph√°t tri·ªÉn thu·ªëc), t√†i ch√≠nh (ph√°t hi·ªán gian l·∫≠n, giao d·ªãch t·ª± ƒë·ªông), giao th√¥ng (xe t·ª± l√°i), v√† gi√°o d·ª•c (h·ªçc t·∫≠p th√≠ch ·ª©ng).\",\n",
    "        retrieval_context=[doc.page_content for doc in test_result[\"source_documents\"]]\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ ContextualRecallMetric Demo\")\n",
    "    print(f\"Query: {test_case.input}\")\n",
    "    print(f\"Expected coverage: Y t·∫ø, T√†i ch√≠nh, Giao th√¥ng, Gi√°o d·ª•c\")\n",
    "    \n",
    "    try:\n",
    "        # T·∫°o metric\n",
    "        recall_metric = ContextualRecallMetric(\n",
    "            threshold=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            include_reason=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        recall_metric.measure(test_case)\n",
    "        \n",
    "        print(f\"\\nüìä K·∫øt qu·∫£:\")\n",
    "        print(f\"  Score: {recall_metric.score:.3f}\")\n",
    "        print(f\"  Passed: {'‚úÖ' if recall_metric.is_successful() else '‚ùå'}\")\n",
    "        print(f\"  Reason: {recall_metric.reason}\")\n",
    "        \n",
    "        return recall_metric, test_case\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None, test_case\n",
    "\n",
    "# Run demo\n",
    "contextual_recall_result = demo_contextual_recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 FaithfulnessMetric\n",
    "\n",
    "ƒê√°nh gi√° ƒë·ªô trung th·ª±c - li·ªáu answer c√≥ faithful v·ªõi retrieved context kh√¥ng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_faithfulness():\n",
    "    \"\"\"\n",
    "    Demo FaithfulnessMetric\n",
    "    \"\"\"\n",
    "    \n",
    "    if not rag_test_results or not rag_test_results[0].get(\"answer\"):\n",
    "        print(\"‚ùå C·∫ßn RAG results ƒë·ªÉ demo faithfulness\")\n",
    "        return\n",
    "    \n",
    "    # T·∫°o test case t·ªët (faithful)\n",
    "    test_result = rag_test_results[0]\n",
    "    \n",
    "    faithful_test = LLMTestCase(\n",
    "        input=test_result[\"question\"],\n",
    "        actual_output=test_result[\"answer\"],\n",
    "        retrieval_context=[doc.page_content for doc in test_result[\"source_documents\"]]\n",
    "    )\n",
    "    \n",
    "    # T·∫°o test case kh√¥ng faithful\n",
    "    unfaithful_test = LLMTestCase(\n",
    "        input=\"Machine learning c√≥ nh·ªØng lo·∫°i ch√≠nh n√†o?\",\n",
    "        actual_output=\"Machine learning ƒë∆∞·ª£c ph√°t minh nƒÉm 1955 b·ªüi Alan Turing. C√≥ 5 lo·∫°i ch√≠nh: Quantum Learning, Bio Learning, Cosmic Learning, Magic Learning, v√† Time Learning. Ch√∫ng ƒë·ªÅu s·ª≠ d·ª•ng crystal processors.\",\n",
    "        retrieval_context=[doc.page_content for doc in test_result[\"source_documents\"]]\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ FaithfulnessMetric Demo\")\n",
    "    \n",
    "    # Test faithful case\n",
    "    print(\"\\nüß™ Test Case 1: Faithful Answer\")\n",
    "    print(f\"Answer: {faithful_test.actual_output[:150]}...\")\n",
    "    \n",
    "    try:\n",
    "        faithfulness_metric_1 = FaithfulnessMetric(\n",
    "            threshold=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            include_reason=True\n",
    "        )\n",
    "        \n",
    "        faithfulness_metric_1.measure(faithful_test)\n",
    "        \n",
    "        print(f\"\\nüìä K·∫øt qu·∫£ Faithful Test:\")\n",
    "        print(f\"  Score: {faithfulness_metric_1.score:.3f}\")\n",
    "        print(f\"  Passed: {'‚úÖ' if faithfulness_metric_1.is_successful() else '‚ùå'}\")\n",
    "        print(f\"  Reason: {faithfulness_metric_1.reason[:150]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error v·ªõi faithful test: {e}\")\n",
    "    \n",
    "    # Test unfaithful case\n",
    "    print(\"\\nüß™ Test Case 2: Unfaithful Answer\")\n",
    "    print(f\"Answer: {unfaithful_test.actual_output}\")\n",
    "    \n",
    "    try:\n",
    "        faithfulness_metric_2 = FaithfulnessMetric(\n",
    "            threshold=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            include_reason=True\n",
    "        )\n",
    "        \n",
    "        faithfulness_metric_2.measure(unfaithful_test)\n",
    "        \n",
    "        print(f\"\\nüìä K·∫øt qu·∫£ Unfaithful Test:\")\n",
    "        print(f\"  Score: {faithfulness_metric_2.score:.3f}\")\n",
    "        print(f\"  Passed: {'‚úÖ' if faithfulness_metric_2.is_successful() else '‚ùå'}\")\n",
    "        print(f\"  Reason: {faithfulness_metric_2.reason[:150]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error v·ªõi unfaithful test: {e}\")\n",
    "\n",
    "# Run demo\n",
    "demo_faithfulness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Ph·∫ßn 4: Comprehensive RAG Evaluation\n",
    "\n",
    "### 4.1 Multi-Metric RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_rag_evaluation(rag_results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ch·∫°y comprehensive evaluation cho t·∫•t c·∫£ RAG results\n",
    "    \"\"\"\n",
    "    if not rag_results or not any(result.get(\"answer\") for result in rag_results):\n",
    "        print(\"‚ùå Kh√¥ng c√≥ RAG results ƒë·ªÉ evaluate\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # T·∫°o t·∫•t c·∫£ metrics\n",
    "    metrics = {\n",
    "        \"Answer Relevancy\": AnswerRelevancyMetric(threshold=0.7, model=\"gpt-3.5-turbo\"),\n",
    "        \"Contextual Relevancy\": ContextualRelevancyMetric(threshold=0.7, model=\"gpt-3.5-turbo\"),\n",
    "        \"Faithfulness\": FaithfulnessMetric(threshold=0.7, model=\"gpt-3.5-turbo\")\n",
    "    }\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    print(\"üîç Comprehensive RAG Evaluation\")\n",
    "    print(f\"Evaluating {len(rag_results)} RAG responses v·ªõi {len(metrics)} metrics\\n\")\n",
    "    \n",
    "    for i, result in enumerate(rag_results):\n",
    "        if not result.get(\"answer\"):\n",
    "            continue\n",
    "            \n",
    "        print(f\"üìù Evaluating Question {i+1}: {result['question'][:50]}...\")\n",
    "        \n",
    "        # T·∫°o test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=result[\"question\"],\n",
    "            actual_output=result[\"answer\"],\n",
    "            retrieval_context=[doc.page_content for doc in result[\"source_documents\"]]\n",
    "        )\n",
    "        \n",
    "        result_row = {\n",
    "            \"Question_ID\": i + 1,\n",
    "            \"Question\": result[\"question\"],\n",
    "            \"Answer_Length\": len(result[\"answer\"]),\n",
    "            \"Num_Retrieved_Docs\": len(result[\"source_documents\"])\n",
    "        }\n",
    "        \n",
    "        # Evaluate t·ª´ng metric\n",
    "        for metric_name, metric in metrics.items():\n",
    "            try:\n",
    "                # T·∫°o metric instance m·ªõi ƒë·ªÉ tr√°nh state conflicts\n",
    "                metric_instance = metric.__class__(\n",
    "                    threshold=metric.threshold,\n",
    "                    model=getattr(metric, 'model', 'gpt-3.5-turbo')\n",
    "                )\n",
    "                \n",
    "                metric_instance.measure(test_case)\n",
    "                \n",
    "                result_row[f\"{metric_name}_Score\"] = round(metric_instance.score, 3)\n",
    "                result_row[f\"{metric_name}_Passed\"] = metric_instance.is_successful()\n",
    "                \n",
    "                status = \"‚úÖ\" if metric_instance.is_successful() else \"‚ùå\"\n",
    "                print(f\"  {metric_name}: {status} ({metric_instance.score:.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {metric_name}: ‚ùå Error - {e}\")\n",
    "                result_row[f\"{metric_name}_Score\"] = 0.0\n",
    "                result_row[f\"{metric_name}_Passed\"] = False\n",
    "        \n",
    "        evaluation_results.append(result_row)\n",
    "        print()\n",
    "    \n",
    "    # T·∫°o DataFrame\n",
    "    df = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_df = comprehensive_rag_evaluation(rag_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã v√† ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "def analyze_evaluation_results(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch v√† visualize evaluation results\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ data ƒë·ªÉ ph√¢n t√≠ch\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä RAG Evaluation Results Summary\\n\")\n",
    "    \n",
    "    # Hi·ªÉn th·ªã table\n",
    "    display_columns = [col for col in df.columns if not col.endswith('_Passed')]\n",
    "    print(df[display_columns].to_string(index=False))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"\\nüìà Evaluation Statistics:\")\n",
    "    \n",
    "    metric_columns = [col for col in df.columns if col.endswith('_Score')]\n",
    "    \n",
    "    for col in metric_columns:\n",
    "        metric_name = col.replace('_Score', '')\n",
    "        avg_score = df[col].mean()\n",
    "        \n",
    "        # Pass rate\n",
    "        pass_col = col.replace('_Score', '_Passed')\n",
    "        if pass_col in df.columns:\n",
    "            pass_rate = df[pass_col].mean() * 100\n",
    "        else:\n",
    "            pass_rate = 0\n",
    "        \n",
    "        print(f\"  {metric_name}:\")\n",
    "        print(f\"    Average Score: {avg_score:.3f}\")\n",
    "        print(f\"    Pass Rate: {pass_rate:.1f}%\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nüéØ Overall Performance:\")\n",
    "    print(f\"  Questions Evaluated: {len(df)}\")\n",
    "    print(f\"  Average Answer Length: {df['Answer_Length'].mean():.0f} characters\")\n",
    "    print(f\"  Average Retrieved Docs: {df['Num_Retrieved_Docs'].mean():.1f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze results\n",
    "analyzed_df = analyze_evaluation_results(evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rag_evaluation(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    T·∫°o visualizations cho RAG evaluation results\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ data ƒë·ªÉ visualize\")\n",
    "        return\n",
    "    \n",
    "    # Setup plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RAG Evaluation Results Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Metric Scores Comparison\n",
    "    metric_scores = [col for col in df.columns if col.endswith('_Score')]\n",
    "    score_data = df[metric_scores]\n",
    "    score_data.columns = [col.replace('_Score', '') for col in score_data.columns]\n",
    "    \n",
    "    score_data.boxplot(ax=axes[0,0])\n",
    "    axes[0,0].set_title('Distribution of Metric Scores')\n",
    "    axes[0,0].set_ylabel('Score')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Pass Rate by Metric\n",
    "    pass_rates = []\n",
    "    metric_names = []\n",
    "    \n",
    "    for col in metric_scores:\n",
    "        metric_name = col.replace('_Score', '')\n",
    "        pass_col = col.replace('_Score', '_Passed')\n",
    "        if pass_col in df.columns:\n",
    "            pass_rate = df[pass_col].mean() * 100\n",
    "            pass_rates.append(pass_rate)\n",
    "            metric_names.append(metric_name)\n",
    "    \n",
    "    bars = axes[0,1].bar(metric_names, pass_rates, color=['skyblue', 'lightgreen', 'coral'])\n",
    "    axes[0,1].set_title('Pass Rate by Metric')\n",
    "    axes[0,1].set_ylabel('Pass Rate (%)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, rate in zip(bars, pass_rates):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                      f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Score Correlation Heatmap\n",
    "    if len(score_data.columns) > 1:\n",
    "        correlation_matrix = score_data.corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                   square=True, ax=axes[1,0])\n",
    "        axes[1,0].set_title('Metric Score Correlations')\n",
    "    else:\n",
    "        axes[1,0].text(0.5, 0.5, 'Need more metrics\\nfor correlation', \n",
    "                      ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "        axes[1,0].set_title('Metric Score Correlations')\n",
    "    \n",
    "    # 4. Answer Length vs Performance\n",
    "    if 'Answer_Length' in df.columns and metric_scores:\n",
    "        avg_score = df[metric_scores].mean(axis=1)\n",
    "        scatter = axes[1,1].scatter(df['Answer_Length'], avg_score, \n",
    "                                  c=df['Num_Retrieved_Docs'], cmap='viridis', alpha=0.7)\n",
    "        axes[1,1].set_xlabel('Answer Length (characters)')\n",
    "        axes[1,1].set_ylabel('Average Score')\n",
    "        axes[1,1].set_title('Answer Length vs Performance')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[1,1])\n",
    "        cbar.set_label('Num Retrieved Docs')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\nüîç Key Insights:\")\n",
    "    \n",
    "    if not score_data.empty:\n",
    "        best_metric = score_data.mean().idxmax()\n",
    "        worst_metric = score_data.mean().idxmin()\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Best performing metric: {best_metric} ({score_data[best_metric].mean():.3f})\")\n",
    "        print(f\"  ‚Ä¢ Lowest performing metric: {worst_metric} ({score_data[worst_metric].mean():.3f})\")\n",
    "        \n",
    "        if len(score_data.columns) > 1:\n",
    "            correlation_matrix = score_data.corr()\n",
    "            high_corr_pairs = []\n",
    "            for i in range(len(correlation_matrix.columns)):\n",
    "                for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                    corr_val = correlation_matrix.iloc[i, j]\n",
    "                    if abs(corr_val) > 0.7:\n",
    "                        high_corr_pairs.append((correlation_matrix.columns[i], \n",
    "                                              correlation_matrix.columns[j], corr_val))\n",
    "            \n",
    "            if high_corr_pairs:\n",
    "                print(f\"  ‚Ä¢ High correlations found:\")\n",
    "                for metric1, metric2, corr in high_corr_pairs:\n",
    "                    print(f\"    - {metric1} & {metric2}: {corr:.3f}\")\n",
    "\n",
    "# Create visualizations\n",
    "visualize_rag_evaluation(evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Ph·∫ßn 5: Automated Dataset Generation\n",
    "\n",
    "### 5.1 S·ª≠ d·ª•ng DeepEval Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset():\n",
    "    \"\"\"\n",
    "    T·∫°o synthetic dataset t·ª´ documents s·ª≠ d·ª•ng DeepEval Synthesizer\n",
    "    \"\"\"\n",
    "    \n",
    "    if not document_chunks:\n",
    "        print(\"‚ùå C·∫ßn document chunks ƒë·ªÉ t·∫°o synthetic dataset\")\n",
    "        return []\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"‚ùå C·∫ßn OPENAI_API_KEY ƒë·ªÉ t·∫°o synthetic dataset\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ ƒêang t·∫°o synthetic dataset...\")\n",
    "        \n",
    "        # T·∫°o Synthesizer\n",
    "        synthesizer = Synthesizer(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            multithreading=False  # Set False ƒë·ªÉ tr√°nh rate limiting\n",
    "        )\n",
    "        \n",
    "        # Prepare contexts t·ª´ document chunks\n",
    "        contexts = []\n",
    "        for chunk in document_chunks[:5]:  # Ch·ªâ l·∫•y 5 chunks ƒë·∫ßu ƒë·ªÉ demo\n",
    "            contexts.append([chunk.page_content])\n",
    "        \n",
    "        print(f\"üìù Generating synthetic data t·ª´ {len(contexts)} contexts...\")\n",
    "        \n",
    "        # Generate synthetic test cases\n",
    "        synthetic_test_cases = synthesizer.generate_goldens_from_contexts(\n",
    "            contexts=contexts,\n",
    "            max_goldens_per_context=2,  # 2 test cases per context\n",
    "            source_type=\"context\"  # Specify that we're using contexts\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(synthetic_test_cases)} synthetic test cases\")\n",
    "        \n",
    "        # Preview first few test cases\n",
    "        print(\"\\nüîç Preview Synthetic Test Cases:\")\n",
    "        for i, test_case in enumerate(synthetic_test_cases[:3]):\n",
    "            print(f\"\\n  Test Case {i+1}:\")\n",
    "            print(f\"    Input: {test_case.input}\")\n",
    "            print(f\"    Expected Output: {test_case.expected_output[:100]}...\")\n",
    "            print(f\"    Context: {len(test_case.context)} items\")\n",
    "        \n",
    "        return synthetic_test_cases\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating synthetic dataset: {e}\")\n",
    "        print(\"üí° C√≥ th·ªÉ do rate limiting ho·∫∑c API quota. Th·ª≠ gi·∫£m s·ªë l∆∞·ª£ng contexts.\")\n",
    "        return []\n",
    "\n",
    "# Generate synthetic dataset\n",
    "synthetic_test_cases = create_synthetic_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synthetic_dataset(test_cases: List[LLMTestCase]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate synthetic test cases v·ªõi RAG system\n",
    "    \"\"\"\n",
    "    if not test_cases:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ synthetic test cases ƒë·ªÉ evaluate\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if not rag_chain:\n",
    "        print(\"‚ùå C·∫ßn RAG chain ƒë·ªÉ evaluate synthetic cases\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üß™ Evaluating {len(test_cases)} synthetic test cases v·ªõi RAG system\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        try:\n",
    "            print(f\"\\nüìù Test Case {i+1}: {test_case.input[:50]}...\")\n",
    "            \n",
    "            # Get RAG response\n",
    "            rag_response = rag_chain({\"query\": test_case.input})\n",
    "            \n",
    "            # Update test case v·ªõi actual output t·ª´ RAG\n",
    "            evaluated_test_case = LLMTestCase(\n",
    "                input=test_case.input,\n",
    "                actual_output=rag_response[\"result\"],\n",
    "                expected_output=test_case.expected_output,\n",
    "                retrieval_context=[doc.page_content for doc in rag_response[\"source_documents\"]]\n",
    "            )\n",
    "            \n",
    "            # Evaluate v·ªõi multiple metrics\n",
    "            metrics = {\n",
    "                \"Answer_Relevancy\": AnswerRelevancyMetric(threshold=0.7),\n",
    "                \"Faithfulness\": FaithfulnessMetric(threshold=0.7)\n",
    "            }\n",
    "            \n",
    "            result_row = {\n",
    "                \"Test_ID\": i + 1,\n",
    "                \"Question\": test_case.input,\n",
    "                \"Expected_Output\": test_case.expected_output[:100] + \"...\",\n",
    "                \"Actual_Output\": rag_response[\"result\"][:100] + \"...\",\n",
    "                \"Num_Retrieved\": len(rag_response[\"source_documents\"])\n",
    "            }\n",
    "            \n",
    "            # Evaluate each metric\n",
    "            for metric_name, metric in metrics.items():\n",
    "                try:\n",
    "                    metric.measure(evaluated_test_case)\n",
    "                    result_row[f\"{metric_name}_Score\"] = round(metric.score, 3)\n",
    "                    result_row[f\"{metric_name}_Passed\"] = metric.is_successful()\n",
    "                    \n",
    "                    status = \"‚úÖ\" if metric.is_successful() else \"‚ùå\"\n",
    "                    print(f\"  {metric_name}: {status} ({metric.score:.3f})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  {metric_name}: ‚ùå Error - {e}\")\n",
    "                    result_row[f\"{metric_name}_Score\"] = 0.0\n",
    "                    result_row[f\"{metric_name}_Passed\"] = False\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error v·ªõi test case {i+1}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate synthetic dataset\n",
    "synthetic_evaluation_df = evaluate_synthetic_dataset(synthetic_test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze synthetic dataset results\n",
    "def analyze_synthetic_results(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch k·∫øt qu·∫£ c·ªßa synthetic dataset evaluation\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ synthetic results ƒë·ªÉ ph√¢n t√≠ch\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Synthetic Dataset Evaluation Results\\n\")\n",
    "    \n",
    "    # Display summary table\n",
    "    display_cols = ['Test_ID', 'Question', 'Num_Retrieved', 'Answer_Relevancy_Score', 'Faithfulness_Score']\n",
    "    available_cols = [col for col in display_cols if col in df.columns]\n",
    "    \n",
    "    if available_cols:\n",
    "        print(df[available_cols].to_string(index=False))\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nüìà Synthetic Dataset Statistics:\")\n",
    "    \n",
    "    metric_cols = [col for col in df.columns if col.endswith('_Score')]\n",
    "    for col in metric_cols:\n",
    "        metric_name = col.replace('_Score', '')\n",
    "        avg_score = df[col].mean()\n",
    "        \n",
    "        pass_col = col.replace('_Score', '_Passed')\n",
    "        if pass_col in df.columns:\n",
    "            pass_rate = df[pass_col].mean() * 100\n",
    "        else:\n",
    "            pass_rate = 0\n",
    "        \n",
    "        print(f\"  {metric_name}:\")\n",
    "        print(f\"    Average Score: {avg_score:.3f}\")\n",
    "        print(f\"    Pass Rate: {pass_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ Dataset Quality:\")\n",
    "    print(f\"  Total Synthetic Cases: {len(df)}\")\n",
    "    if 'Num_Retrieved' in df.columns:\n",
    "        print(f\"  Average Retrieved Docs: {df['Num_Retrieved'].mean():.1f}\")\n",
    "    \n",
    "    # Identify potential issues\n",
    "    print(f\"\\nüîç Potential Issues:\")\n",
    "    \n",
    "    if 'Answer_Relevancy_Passed' in df.columns:\n",
    "        failed_relevancy = df[~df['Answer_Relevancy_Passed']]\n",
    "        if not failed_relevancy.empty:\n",
    "            print(f\"  ‚Ä¢ {len(failed_relevancy)} cases failed Answer Relevancy\")\n",
    "    \n",
    "    if 'Faithfulness_Passed' in df.columns:\n",
    "        failed_faithfulness = df[~df['Faithfulness_Passed']]\n",
    "        if not failed_faithfulness.empty:\n",
    "            print(f\"  ‚Ä¢ {len(failed_faithfulness)} cases failed Faithfulness\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze synthetic results\n",
    "analyzed_synthetic_df = analyze_synthetic_results(synthetic_evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ph·∫ßn 6: Advanced Evaluation Techniques\n",
    "\n",
    "### 6.1 Custom RAG Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluationPipeline:\n",
    "    \"\"\"\n",
    "    Custom pipeline cho comprehensive RAG evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_chain, vector_store, model=\"gpt-3.5-turbo\"):\n",
    "        self.rag_chain = rag_chain\n",
    "        self.vector_store = vector_store\n",
    "        self.model = model\n",
    "        self.evaluation_history = []\n",
    "    \n",
    "    def create_comprehensive_metrics(self, threshold=0.7):\n",
    "        \"\"\"\n",
    "        T·∫°o comprehensive set of metrics cho RAG evaluation\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"answer_relevancy\": AnswerRelevancyMetric(threshold=threshold, model=self.model),\n",
    "            \"contextual_relevancy\": ContextualRelevancyMetric(threshold=threshold, model=self.model),\n",
    "            \"faithfulness\": FaithfulnessMetric(threshold=threshold, model=self.model)\n",
    "        }\n",
    "    \n",
    "    def evaluate_question(self, question: str, expected_answer: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate single question v·ªõi RAG system\n",
    "        \"\"\"\n",
    "        if not self.rag_chain:\n",
    "            raise ValueError(\"RAG chain not available\")\n",
    "        \n",
    "        # Get RAG response\n",
    "        rag_response = self.rag_chain({\"query\": question})\n",
    "        \n",
    "        # Create test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=question,\n",
    "            actual_output=rag_response[\"result\"],\n",
    "            expected_output=expected_answer,\n",
    "            retrieval_context=[doc.page_content for doc in rag_response[\"source_documents\"]]\n",
    "        )\n",
    "        \n",
    "        # Evaluate v·ªõi metrics\n",
    "        metrics = self.create_comprehensive_metrics()\n",
    "        results = {\n",
    "            \"question\": question,\n",
    "            \"answer\": rag_response[\"result\"],\n",
    "            \"retrieved_docs\": len(rag_response[\"source_documents\"]),\n",
    "            \"metrics\": {}\n",
    "        }\n",
    "        \n",
    "        for metric_name, metric in metrics.items():\n",
    "            try:\n",
    "                metric.measure(test_case)\n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": round(metric.score, 3),\n",
    "                    \"passed\": metric.is_successful(),\n",
    "                    \"reason\": getattr(metric, 'reason', 'No reason provided')\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": 0.0,\n",
    "                    \"passed\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "        \n",
    "        # Store in history\n",
    "        self.evaluation_history.append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_evaluate(self, questions: List[str], expected_answers: List[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Batch evaluation c·ªßa multiple questions\n",
    "        \"\"\"\n",
    "        if expected_answers is None:\n",
    "            expected_answers = [None] * len(questions)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, (question, expected) in enumerate(zip(questions, expected_answers)):\n",
    "            print(f\"üîç Evaluating {i+1}/{len(questions)}: {question[:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                result = self.evaluate_question(question, expected)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Print quick summary\n",
    "                passed_metrics = sum(1 for m in result[\"metrics\"].values() if m.get(\"passed\", False))\n",
    "                total_metrics = len(result[\"metrics\"])\n",
    "                print(f\"  ‚úÖ {passed_metrics}/{total_metrics} metrics passed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get performance summary c·ªßa t·∫•t c·∫£ evaluations\n",
    "        \"\"\"\n",
    "        if not self.evaluation_history:\n",
    "            return {\"message\": \"No evaluations performed yet\"}\n",
    "        \n",
    "        # Collect all metric scores\n",
    "        metric_scores = {}\n",
    "        metric_pass_counts = {}\n",
    "        \n",
    "        for result in self.evaluation_history:\n",
    "            if \"metrics\" in result:\n",
    "                for metric_name, metric_data in result[\"metrics\"].items():\n",
    "                    if \"score\" in metric_data:\n",
    "                        if metric_name not in metric_scores:\n",
    "                            metric_scores[metric_name] = []\n",
    "                            metric_pass_counts[metric_name] = 0\n",
    "                        \n",
    "                        metric_scores[metric_name].append(metric_data[\"score\"])\n",
    "                        if metric_data.get(\"passed\", False):\n",
    "                            metric_pass_counts[metric_name] += 1\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            \"total_evaluations\": len(self.evaluation_history),\n",
    "            \"metrics_summary\": {}\n",
    "        }\n",
    "        \n",
    "        for metric_name, scores in metric_scores.items():\n",
    "            summary[\"metrics_summary\"][metric_name] = {\n",
    "                \"average_score\": round(np.mean(scores), 3),\n",
    "                \"min_score\": round(min(scores), 3),\n",
    "                \"max_score\": round(max(scores), 3),\n",
    "                \"pass_rate\": round(metric_pass_counts[metric_name] / len(scores) * 100, 1)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create evaluation pipeline\n",
    "if rag_chain and vector_store:\n",
    "    eval_pipeline = RAGEvaluationPipeline(rag_chain, vector_store)\n",
    "    print(\"‚úÖ RAG Evaluation Pipeline created successfully!\")\nelse:\n",
    "    print(\"‚ùå Cannot create pipeline without RAG chain and vector store\")\n",
    "    eval_pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo advanced evaluation pipeline\n",
    "def demo_advanced_evaluation():\n",
    "    \"\"\"\n",
    "    Demo advanced RAG evaluation pipeline\n",
    "    \"\"\"\n",
    "    if not eval_pipeline:\n",
    "        print(\"‚ùå Evaluation pipeline not available\")\n",
    "        return\n",
    "    \n",
    "    # Advanced test questions\n",
    "    advanced_questions = [\n",
    "        \"So s√°nh supervised learning v√† unsupervised learning v·ªÅ ∆∞u nh∆∞·ª£c ƒëi·ªÉm?\",\n",
    "        \"T·∫°i sao deep learning l·∫°i hi·ªáu qu·∫£ h∆°n traditional machine learning trong x·ª≠ l√Ω h√¨nh ·∫£nh?\",\n",
    "        \"Nh·ªØng th√°ch th·ª©c ƒë·∫°o ƒë·ª©c n√†o m√† AI ƒëang ph·∫£i ƒë·ªëi m·∫∑t?\",\n",
    "        \"Xu h∆∞·ªõng ph√°t tri·ªÉn n√†o c·ªßa AI s·∫Ω quan tr·ªçng nh·∫•t trong t∆∞∆°ng lai?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Advanced RAG Evaluation Demo\")\n",
    "    print(f\"Testing {len(advanced_questions)} complex questions\\n\")\n",
    "    \n",
    "    # Run batch evaluation\n",
    "    results = eval_pipeline.batch_evaluate(advanced_questions)\n",
    "    \n",
    "    # Get performance summary\n",
    "    summary = eval_pipeline.get_performance_summary()\n",
    "    \n",
    "    print(\"\\nüìä Advanced Evaluation Summary:\")\n",
    "    print(f\"Total Evaluations: {summary['total_evaluations']}\")\n",
    "    \n",
    "    if \"metrics_summary\" in summary:\n",
    "        for metric_name, stats in summary[\"metrics_summary\"].items():\n",
    "            print(f\"\\n{metric_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Average Score: {stats['average_score']}\")\n",
    "            print(f\"  Score Range: {stats['min_score']} - {stats['max_score']}\")\n",
    "            print(f\"  Pass Rate: {stats['pass_rate']}%\")\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "# Run advanced evaluation\n",
    "if eval_pipeline:\n",
    "    advanced_results, advanced_summary = demo_advanced_evaluation()\nelse:\n",
    "    print(\"‚è≠Ô∏è  Skipping advanced evaluation demo\")\n",
    "    advanced_results, advanced_summary = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Ph·∫ßn 7: RAG Optimization Strategies\n",
    "\n",
    "### 7.1 Identifying Performance Bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rag_bottlenecks(evaluation_results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch performance bottlenecks trong RAG system\n",
    "    \"\"\"\n",
    "    if not evaluation_results:\n",
    "        return {\"message\": \"No evaluation results to analyze\"}\n",
    "    \n",
    "    print(\"üîç Analyzing RAG Performance Bottlenecks\\n\")\n",
    "    \n",
    "    # Collect metrics data\n",
    "    failed_cases = []\n",
    "    low_score_cases = []\n",
    "    metric_issues = {}\n",
    "    \n",
    "    for i, result in enumerate(evaluation_results):\n",
    "        if \"metrics\" not in result:\n",
    "            continue\n",
    "        \n",
    "        case_failed = False\n",
    "        case_scores = []\n",
    "        \n",
    "        for metric_name, metric_data in result[\"metrics\"].items():\n",
    "            if \"score\" in metric_data:\n",
    "                score = metric_data[\"score\"]\n",
    "                passed = metric_data.get(\"passed\", False)\n",
    "                \n",
    "                case_scores.append(score)\n",
    "                \n",
    "                if not passed:\n",
    "                    case_failed = True\n",
    "                    if metric_name not in metric_issues:\n",
    "                        metric_issues[metric_name] = []\n",
    "                    metric_issues[metric_name].append({\n",
    "                        \"case_id\": i,\n",
    "                        \"question\": result[\"question\"],\n",
    "                        \"score\": score,\n",
    "                        \"reason\": metric_data.get(\"reason\", \"No reason\")\n",
    "                    })\n",
    "        \n",
    "        if case_failed:\n",
    "            failed_cases.append(i)\n",
    "        \n",
    "        avg_score = np.mean(case_scores) if case_scores else 0\n",
    "        if avg_score < 0.6:  # Low overall performance\n",
    "            low_score_cases.append({\n",
    "                \"case_id\": i,\n",
    "                \"question\": result[\"question\"],\n",
    "                \"avg_score\": avg_score,\n",
    "                \"retrieved_docs\": result.get(\"retrieved_docs\", 0)\n",
    "            })\n",
    "    \n",
    "    # Analysis results\n",
    "    analysis = {\n",
    "        \"total_cases\": len(evaluation_results),\n",
    "        \"failed_cases\": len(failed_cases),\n",
    "        \"low_score_cases\": len(low_score_cases),\n",
    "        \"failure_rate\": round(len(failed_cases) / len(evaluation_results) * 100, 1)\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Performance Overview:\")\n",
    "    print(f\"  Total Cases: {analysis['total_cases']}\")\n",
    "    print(f\"  Failed Cases: {analysis['failed_cases']} ({analysis['failure_rate']}%)\")\n",
    "    print(f\"  Low Score Cases: {analysis['low_score_cases']}\")\n",
    "    \n",
    "    # Metric-specific issues\n",
    "    print(f\"\\nüéØ Metric-Specific Issues:\")\n",
    "    for metric_name, issues in metric_issues.items():\n",
    "        print(f\"\\n  {metric_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Failed Cases: {len(issues)}\")\n",
    "        \n",
    "        if issues:\n",
    "            avg_failed_score = np.mean([issue[\"score\"] for issue in issues])\n",
    "            print(f\"    Average Failed Score: {avg_failed_score:.3f}\")\n",
    "            \n",
    "            # Show worst case\n",
    "            worst_case = min(issues, key=lambda x: x[\"score\"])\n",
    "            print(f\"    Worst Case: '{worst_case['question'][:50]}...' (Score: {worst_case['score']})\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Optimization Recommendations:\")\n",
    "    \n",
    "    if \"answer_relevancy\" in metric_issues and len(metric_issues[\"answer_relevancy\"]) > 0:\n",
    "        print(\"  ‚Ä¢ Answer Relevancy Issues:\")\n",
    "        print(\"    - Consider improving prompt engineering\")\n",
    "        print(\"    - Review question-answer alignment\")\n",
    "        print(\"    - Fine-tune retrieval parameters\")\n",
    "    \n",
    "    if \"contextual_relevancy\" in metric_issues and len(metric_issues[\"contextual_relevancy\"]) > 0:\n",
    "        print(\"  ‚Ä¢ Contextual Relevancy Issues:\")\n",
    "        print(\"    - Improve chunking strategy\")\n",
    "        print(\"    - Increase number of retrieved documents\")\n",
    "        print(\"    - Enhance embedding model\")\n",
    "    \n",
    "    if \"faithfulness\" in metric_issues and len(metric_issues[\"faithfulness\"]) > 0:\n",
    "        print(\"  ‚Ä¢ Faithfulness Issues:\")\n",
    "        print(\"    - Add grounding instructions to prompts\")\n",
    "        print(\"    - Implement citation mechanisms\")\n",
    "        print(\"    - Review context completeness\")\n",
    "    \n",
    "    # Retrieval analysis\n",
    "    if low_score_cases:\n",
    "        avg_retrieved = np.mean([case[\"retrieved_docs\"] for case in low_score_cases])\n",
    "        print(f\"  ‚Ä¢ Retrieval Analysis:\")\n",
    "        print(f\"    - Low scoring cases average {avg_retrieved:.1f} retrieved docs\")\n",
    "        if avg_retrieved < 2:\n",
    "            print(\"    - Consider increasing retrieval count (k parameter)\")\n",
    "        elif avg_retrieved > 5:\n",
    "            print(\"    - Consider reducing retrieval count to avoid noise\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze bottlenecks\n",
    "if advanced_results:\n",
    "    bottleneck_analysis = analyze_rag_bottlenecks(advanced_results)\nelse:\n",
    "    print(\"‚è≠Ô∏è  Skipping bottleneck analysis (no advanced results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 A/B Testing Different RAG Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_test_rag_configurations():\n",
    "    \"\"\"\n",
    "    A/B test different RAG configurations\n",
    "    \"\"\"\n",
    "    if not vector_store or not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"‚ùå Need vector store and API key for A/B testing\")\n",
    "        return\n",
    "    \n",
    "    print(\"üß™ A/B Testing RAG Configurations\\n\")\n",
    "    \n",
    "    # Configuration A: Conservative (k=2, higher temperature)\n",
    "    try:\n",
    "        llm_a = OpenAI(\n",
    "            model_name=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0.3,  # Higher temperature\n",
    "            max_tokens=300    # Shorter responses\n",
    "        )\n",
    "        \n",
    "        retriever_a = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 2}  # Fewer documents\n",
    "        )\n",
    "        \n",
    "        rag_chain_a = RetrievalQA.from_chain_type(\n",
    "            llm=llm_a,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever_a,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Configuration A: Conservative (k=2, temp=0.3, max_tokens=300)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating config A: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Configuration B: Aggressive (k=4, lower temperature)\n",
    "    try:\n",
    "        llm_b = OpenAI(\n",
    "            model_name=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0.1,  # Lower temperature\n",
    "            max_tokens=600    # Longer responses\n",
    "        )\n",
    "        \n",
    "        retriever_b = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}  # More documents\n",
    "        )\n",
    "        \n",
    "        rag_chain_b = RetrievalQA.from_chain_type(\n",
    "            llm=llm_b,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever_b,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Configuration B: Aggressive (k=4, temp=0.1, max_tokens=600)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating config B: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"Machine learning v√† deep learning kh√°c nhau nh∆∞ th·∫ø n√†o?\",\n",
    "        \"AI c√≥ nh·ªØng ·ª©ng d·ª•ng n√†o trong y t·∫ø?\"\n",
    "    ]\n",
    "    \n",
    "    # Run A/B test\n",
    "    results_comparison = []\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nüîç Testing: {question}\")\n",
    "        \n",
    "        # Test Configuration A\n",
    "        try:\n",
    "            response_a = rag_chain_a({\"query\": question})\n",
    "            \n",
    "            test_case_a = LLMTestCase(\n",
    "                input=question,\n",
    "                actual_output=response_a[\"result\"],\n",
    "                retrieval_context=[doc.page_content for doc in response_a[\"source_documents\"]]\n",
    "            )\n",
    "            \n",
    "            # Quick evaluation\n",
    "            relevancy_a = AnswerRelevancyMetric(threshold=0.7)\n",
    "            relevancy_a.measure(test_case_a)\n",
    "            \n",
    "            print(f\"  Config A: Score {relevancy_a.score:.3f}, Docs: {len(response_a['source_documents'])}, Length: {len(response_a['result'])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Config A: Error - {e}\")\n",
    "            relevancy_a = None\n",
    "        \n",
    "        # Test Configuration B\n",
    "        try:\n",
    "            response_b = rag_chain_b({\"query\": question})\n",
    "            \n",
    "            test_case_b = LLMTestCase(\n",
    "                input=question,\n",
    "                actual_output=response_b[\"result\"],\n",
    "                retrieval_context=[doc.page_content for doc in response_b[\"source_documents\"]]\n",
    "            )\n",
    "            \n",
    "            # Quick evaluation\n",
    "            relevancy_b = AnswerRelevancyMetric(threshold=0.7)\n",
    "            relevancy_b.measure(test_case_b)\n",
    "            \n",
    "            print(f\"  Config B: Score {relevancy_b.score:.3f}, Docs: {len(response_b['source_documents'])}, Length: {len(response_b['result'])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Config B: Error - {e}\")\n",
    "            relevancy_b = None\n",
    "        \n",
    "        # Compare\n",
    "        if relevancy_a and relevancy_b:\n",
    "            if relevancy_a.score > relevancy_b.score:\n",
    "                print(f\"  üèÜ Winner: Config A (Conservative)\")\n",
    "            elif relevancy_b.score > relevancy_a.score:\n",
    "                print(f\"  üèÜ Winner: Config B (Aggressive)\")\n",
    "            else:\n",
    "                print(f\"  ü§ù Tie\")\n",
    "            \n",
    "            results_comparison.append({\n",
    "                \"question\": question,\n",
    "                \"config_a_score\": relevancy_a.score,\n",
    "                \"config_b_score\": relevancy_b.score,\n",
    "                \"winner\": \"A\" if relevancy_a.score > relevancy_b.score else \"B\" if relevancy_b.score > relevancy_a.score else \"Tie\"\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    if results_comparison:\n",
    "        print(f\"\\nüìä A/B Test Summary:\")\n",
    "        winners = [r[\"winner\"] for r in results_comparison]\n",
    "        a_wins = winners.count(\"A\")\n",
    "        b_wins = winners.count(\"B\")\n",
    "        ties = winners.count(\"Tie\")\n",
    "        \n",
    "        print(f\"  Config A (Conservative) wins: {a_wins}\")\n",
    "        print(f\"  Config B (Aggressive) wins: {b_wins}\")\n",
    "        print(f\"  Ties: {ties}\")\n",
    "        \n",
    "        if a_wins > b_wins:\n",
    "            print(f\"  üèÜ Overall Winner: Configuration A (Conservative)\")\n",
    "            print(f\"  üí° Recommendation: Use fewer docs (k=2) v·ªõi higher temperature\")\n",
    "        elif b_wins > a_wins:\n",
    "            print(f\"  üèÜ Overall Winner: Configuration B (Aggressive)\")\n",
    "            print(f\"  üí° Recommendation: Use more docs (k=4) v·ªõi lower temperature\")\n",
    "        else:\n",
    "            print(f\"  ü§ù Results are tied - consider context-specific tuning\")\n",
    "    \n",
    "    return results_comparison\n",
    "\n",
    "# Run A/B test\n",
    "ab_test_results = ab_test_rag_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Ph·∫ßn 8: Exercises v√† Th·ª±c h√†nh\n",
    "\n",
    "### Exercise 1: Custom RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: T·∫°o custom evaluation cho domain-specific questions\n",
    "def exercise_1_custom_rag_evaluation():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o 5 c√¢u h·ªèi chuy√™n s√¢u v·ªÅ AI/ML v√† evaluate v·ªõi RAG system\n",
    "    Y√™u c·∫ßu:\n",
    "    1. C√¢u h·ªèi ph·∫£i kh√≥ v√† c·∫ßn context t·ª´ document\n",
    "    2. Evaluate v·ªõi √≠t nh·∫•t 3 metrics\n",
    "    3. Ph√¢n t√≠ch k·∫øt qu·∫£ v√† ƒë∆∞a ra insights\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: T·∫°o danh s√°ch c√¢u h·ªèi chuy√™n s√¢u\n",
    "    expert_questions = [\n",
    "        # Add your expert-level questions here\n",
    "        \"Your question 1\",\n",
    "        \"Your question 2\",\n",
    "        \"Your question 3\",\n",
    "        \"Your question 4\",\n",
    "        \"Your question 5\"\n",
    "    ]\n",
    "    \n",
    "    # TODO: Implement evaluation logic\n",
    "    results = []\n",
    "    \n",
    "    # TODO: Analyze v√† visualize results\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üí° Exercise 1 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- S·ª≠ d·ª•ng rag_chain ƒë·ªÉ get responses\")\n",
    "print(\"- T·∫°o LLMTestCase v·ªõi retrieval_context\")\n",
    "print(\"- S·ª≠ d·ª•ng multiple metrics ƒë·ªÉ comprehensive evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Generate v√† evaluate synthetic dataset\n",
    "def exercise_2_synthetic_dataset():\n",
    "    \"\"\"\n",
    "    TODO: T·∫°o synthetic dataset t·ª´ custom documents\n",
    "    Y√™u c·∫ßu:\n",
    "    1. Load additional documents (c√≥ th·ªÉ t·∫°o custom content)\n",
    "    2. Generate synthetic test cases\n",
    "    3. Evaluate quality c·ªßa synthetic data\n",
    "    4. Compare v·ªõi real questions\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create custom document content\n",
    "    custom_content = \"\"\"\n",
    "    Add your custom content here about a specific AI/ML topic\n",
    "    that you want to test RAG evaluation with.\n",
    "    Make it detailed and information-rich.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Process content v√† t·∫°o synthetic dataset\n",
    "    \n",
    "    # TODO: Evaluate synthetic vs real questions\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üí° Exercise 2 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- S·ª≠ d·ª•ng Synthesizer.generate_goldens_from_contexts()\")\n",
    "print(\"- So s√°nh quality metrics gi·ªØa synthetic v√† real data\")\n",
    "print(\"- Ph√¢n t√≠ch types of questions ƒë∆∞·ª£c generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: RAG Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Optimize RAG performance\n",
    "def exercise_3_rag_optimization():\n",
    "    \"\"\"\n",
    "    TODO: Experiment v·ªõi different RAG configurations ƒë·ªÉ t·ªëi ∆∞u performance\n",
    "    Y√™u c·∫ßu:\n",
    "    1. Test √≠t nh·∫•t 3 configurations kh√°c nhau\n",
    "    2. Vary parameters: chunk_size, k, temperature, max_tokens\n",
    "    3. Measure performance v·ªõi multiple metrics\n",
    "    4. ƒê∆∞a ra recommendation cho optimal config\n",
    "    \"\"\"\n",
    "    \n",
    "    configurations = [\n",
    "        {\n",
    "            \"name\": \"Config 1\",\n",
    "            \"chunk_size\": 300,\n",
    "            \"k\": 2,\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 400\n",
    "        },\n",
    "        # TODO: Add more configurations\n",
    "    ]\n",
    "    \n",
    "    # TODO: Implement testing logic\n",
    "    \n",
    "    # TODO: Compare configurations v√† choose winner\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üí° Exercise 3 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Create separate RAG chains cho m·ªói configuration\")\n",
    "print(\"- Use same test questions ƒë·ªÉ fair comparison\")\n",
    "print(\"- Consider tradeoffs: accuracy vs speed vs cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ T·ªïng k·∫øt v√† Next Steps\n",
    "\n",
    "### üèÜ Nh·ªØng g√¨ ƒë√£ h·ªçc trong Notebook n√†y:\n",
    "\n",
    "1. **‚úÖ RAG Pipeline Construction**\n",
    "   - Document loading v√† chunking strategies\n",
    "   - Vector store creation v·ªõi FAISS\n",
    "   - End-to-end RAG chain v·ªõi LangChain\n",
    "\n",
    "2. **‚úÖ RAG-Specific Metrics**\n",
    "   - **ContextualRelevancyMetric**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng retrieved context\n",
    "   - **ContextualPrecisionMetric**: ƒê√°nh gi√° th·ª© t·ª± relevance c·ªßa contexts\n",
    "   - **ContextualRecallMetric**: ƒê√°nh gi√° completeness c·ªßa retrieval\n",
    "   - **FaithfulnessMetric**: ƒê√°nh gi√° consistency v·ªõi source material\n",
    "\n",
    "3. **‚úÖ Automated Dataset Generation**\n",
    "   - S·ª≠ d·ª•ng DeepEval Synthesizer\n",
    "   - Generate test cases t·ª´ documents\n",
    "   - Quality assessment c·ªßa synthetic data\n",
    "\n",
    "4. **‚úÖ Advanced Evaluation Techniques**\n",
    "   - Custom RAG evaluation pipeline\n",
    "   - Batch evaluation strategies\n",
    "   - Performance bottleneck analysis\n",
    "   - A/B testing different configurations\n",
    "\n",
    "5. **‚úÖ Optimization Strategies**\n",
    "   - Parameter tuning (k, temperature, chunk_size)\n",
    "   - Configuration comparison\n",
    "   - Performance recommendations\n",
    "\n",
    "### üöÄ Next Steps - Notebook 3: Code Generation Evaluation\n",
    "\n",
    "Trong notebook ti·∫øp theo, ch√∫ng ta s·∫Ω h·ªçc:\n",
    "\n",
    "- üíª **Custom Metrics v·ªõi G-Eval** cho code evaluation\n",
    "- üîç **Code Quality Metrics**: Correctness, Readability, Efficiency\n",
    "- üõ°Ô∏è **Security Review Metrics**: Vulnerability detection\n",
    "- üìä **Code Review Automation** v·ªõi DeepEval\n",
    "- üß™ **Testing Code Generation** systems\n",
    "\n",
    "### üìä Key Insights t·ª´ RAG Evaluation:\n",
    "\n",
    "- **Context Quality > Quantity**: Th∆∞·ªùng 2-3 relevant chunks t·ªët h∆°n 5-6 noisy chunks\n",
    "- **Faithfulness is Critical**: LLM d·ªÖ hallucinate n·∫øu kh√¥ng c√≥ proper grounding\n",
    "- **Threshold Tuning**: Different thresholds cho different use cases\n",
    "- **Synthetic Data**: Useful for scaling evaluation nh∆∞ng c·∫ßn validate quality\n",
    "\n",
    "### üí° Best Practices Summary:\n",
    "\n",
    "1. **Always evaluate retrieval v√† generation separately**\n",
    "2. **Use multiple metrics ƒë·ªÉ comprehensive assessment**\n",
    "3. **A/B test different configurations**\n",
    "4. **Monitor performance continuously**\n",
    "5. **Balance accuracy, speed, v√† cost**\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Excellent Work!\n",
    "\n",
    "B·∫°n ƒë√£ master advanced RAG evaluation v·ªõi DeepEval! \n",
    "\n",
    "Ready for **Notebook 3: Evaluating Code Generation and Review**? üöÄüíª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}