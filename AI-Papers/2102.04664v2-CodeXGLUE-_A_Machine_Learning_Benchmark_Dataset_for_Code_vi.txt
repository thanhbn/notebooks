# 2102.04664v2.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: D:\llm\notebooks\AI-Papers\2102.04664v2.pdf
# KÃ­ch thÆ°á»›c file: 1282683 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================


--- TRANG 1 ---
CodeXGLUE: Má»™t Benchmark Dataset cho Machine Learning trong viá»‡c Hiá»ƒu vÃ  Sinh Code

Shuai Luâˆ—
Äáº¡i há»c Báº¯c KinhDaya Guoâˆ—
Äáº¡i há»c Trung SÆ¡n (Sun Yat-sen)Shuo Renâˆ—
Äáº¡i há»c Báº¯c HÃ ng

Junjie Huangâˆ—
Äáº¡i há»c Báº¯c HÃ ngAlexey Svyatkovskiy
MicrosoftAmbrosio Blanco
Microsoft Research Asia

Colin Clement
MicrosoftDawn Drain
MicrosoftDaxin Jiang
Microsoft

Duyu Tang
Microsoft Research AsiaGe Li
Äáº¡i há»c Báº¯c KinhLidong Zhou
Microsoft Research Asia

Linjun Shou
MicrosoftLong Zhou
Microsoft Research AsiaMichele Tufano
Microsoft

Ming Gong
MicrosoftMing Zhou
Microsoft Research AsiaNan Duan
Microsoft Research Asia

Neel Sundaresan
MicrosoftShao Kun Deng
MicrosoftShengyu Fu
Microsoft

Shujie Liu
Microsoft Research Asia

TÃ“M Táº®T
CÃ¡c benchmark dataset cÃ³ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ trong viá»‡c thÃºc Ä‘áº©y nghiÃªn cá»©u vá» cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n ngÃ´n ngá»¯ láº­p trÃ¬nh. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giá»›i thiá»‡u CodeXGLUE, má»™t benchmark dataset nháº±m thÃºc Ä‘áº©y nghiÃªn cá»©u machine learning cho viá»‡c hiá»ƒu vÃ  sinh mÃ£ chÆ°Æ¡ng trÃ¬nh. CodeXGLUE bao gá»“m má»™t táº­p há»£p 10 tÃ¡c vá»¥ trÃªn 14 dataset vÃ  má»™t ná»n táº£ng cho viá»‡c Ä‘Ã¡nh giÃ¡ vÃ  so sÃ¡nh mÃ´ hÃ¬nh. CodeXGLUE cÅ©ng cÃ³ ba há»‡ thá»‘ng baseline, bao gá»“m cÃ¡c mÃ´ hÃ¬nh kiá»ƒu BERT, kiá»ƒu GPT vÃ  Encoder-Decoder, Ä‘á»ƒ giÃºp cÃ¡c nhÃ  nghiÃªn cá»©u dá»… dÃ ng sá»­ dá»¥ng ná»n táº£ng nÃ y. Viá»‡c cÃ³ sáºµn dá»¯ liá»‡u vÃ  baseline nhÆ° váº­y cÃ³ thá»ƒ giÃºp phÃ¡t triá»ƒn vÃ  xÃ¡c thá»±c cÃ¡c phÆ°Æ¡ng phÃ¡p má»›i cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c váº¥n Ä‘á» hiá»ƒu vÃ  sinh mÃ£ chÆ°Æ¡ng trÃ¬nh khÃ¡c nhauÂ¹.

Tá»ª KHÃ“A
hiá»ƒu chÆ°Æ¡ng trÃ¬nh, machine learning, tÃ­nh tá»± nhiÃªn cá»§a pháº§n má»m

1 GIá»šI THIá»†U
Evans Data CorporationÂ² Æ°á»›c tÃ­nh ráº±ng cÃ³ 23.9 triá»‡u láº­p trÃ¬nh viÃªn chuyÃªn nghiá»‡p vÃ o nÄƒm 2019 vÃ  con sá»‘ nÃ y dá»± kiáº¿n sáº½ Ä‘áº¡t 28.7 triá»‡u vÃ o nÄƒm 2024. Vá»›i dÃ¢n sá»‘ láº­p trÃ¬nh viÃªn Ä‘ang tÄƒng trÆ°á»Ÿng vá»›i tá»‘c Ä‘á»™ nhÆ° váº­y, code intelligence táº­n dá»¥ng trÃ­ tuá»‡ nhÃ¢n táº¡o (AI) Ä‘á»ƒ giÃºp cÃ¡c láº­p trÃ¬nh viÃªn pháº§n má»m cáº£i thiá»‡n nÄƒng suáº¥t cá»§a quÃ¡ trÃ¬nh phÃ¡t triá»ƒn Ä‘ang trá»Ÿ nÃªn ngÃ y cÃ ng quan trá»ng.

âˆ— cho biáº¿t Ä‘Ã³ng gÃ³p báº±ng nhau vÃ  thá»±c táº­p táº¡i Microsoft. CÃ¡c tÃ¡c giáº£ Ä‘Æ°á»£c liá»‡t kÃª theo thá»© tá»± alpha-beta. CÃ¡c tÃ¡c giáº£ liÃªn há»‡ lÃ  Duyu Tang vÃ  Shujie Liu.
Â¹ CodeXGLUE cÃ³ sáºµn cÃ´ng khai táº¡i https://github.com/microsoft/CodeXGLUE. NgÆ°á»i tham gia cÃ³ thá»ƒ gá»­i káº¿t quáº£ cá»§a há» báº±ng cÃ¡ch gá»­i email Ä‘áº¿n codexglue@microsoft.com.
Â² https://evansdata.com/press/viewRelease.php?pressID=278

Viá»‡c Ä‘Æ°á»£c cháº¥p nháº­n rá»™ng rÃ£i ráº±ng cÃ¡c benchmark cÃ³ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n sá»± phÃ¡t triá»ƒn cá»§a nghiÃªn cá»©u AI á»©ng dá»¥ng. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i táº­p trung vÃ o viá»‡c thiáº¿t láº­p má»™t benchmark dataset cho code intelligence.

Viá»‡c hiá»ƒu vÃ  sinh mÃ£ chÆ°Æ¡ng trÃ¬nh tá»± Ä‘á»™ng cÃ³ thá»ƒ tÄƒng nÄƒng suáº¥t cá»§a cÃ¡c láº­p trÃ¬nh viÃªn pháº§n má»m. Thá»±c táº¿, cÃ¡c láº­p trÃ¬nh viÃªn muá»‘n tÃ¬m mÃ£ Ä‘Æ°á»£c viáº¿t bá»Ÿi ngÆ°á»i khÃ¡c vá»›i cÃ¹ng Ã½ Ä‘á»‹nh cÃ³ thá»ƒ táº­n dá»¥ng cÃ¡c há»‡ thá»‘ng tÃ¬m kiáº¿m mÃ£ [23,35,58,85] Ä‘á»ƒ tá»± Ä‘á»™ng truy xuáº¥t cÃ¡c mÃ£ cÃ³ liÃªn quan vá» máº·t ngá»¯ nghÄ©a thÃ´ng qua cÃ¡c truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn. TÆ°Æ¡ng tá»±, cÃ¡c láº­p trÃ¬nh viÃªn bá»‘i rá»‘i vá» viá»‡c viáº¿t gÃ¬ tiáº¿p theo cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c há»‡ thá»‘ng hoÃ n thÃ nh mÃ£ [4,8,9,31,62,63,72,73] Ä‘á»ƒ tá»± Ä‘á»™ng hoÃ n thÃ nh cÃ¡c token tiáº¿p theo dá»±a trÃªn cÃ¡c chá»‰nh sá»­a Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn mÃ£. Cuá»‘i cÃ¹ng, khi cÃ¡c láº­p trÃ¬nh viÃªn muá»‘n triá»ƒn khai mÃ£ Java báº±ng Python, cÃ¡c há»‡ thá»‘ng dá»‹ch mÃ£ sang mÃ£ [11,41,46,54] cÃ³ thá»ƒ giÃºp dá»‹ch mÃ£ cá»§a há» tá»« má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh (Python) sang ngÃ´n ngá»¯ khÃ¡c (Java).

Trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ ngÃ y cÃ ng Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh thá»‘ng kÃª, bao gá»“m máº¡ng neural, cho cÃ¡c tÃ¡c vá»¥ code intelligence. Gáº§n Ä‘Ã¢y, viá»‡c á»©ng dá»¥ng cÃ¡c mÃ´ hÃ¬nh pretrained há»c tá»« dá»¯ liá»‡u ngÃ´n ngá»¯ láº­p trÃ¬nh lá»›n Ä‘Ã£ Ä‘Æ°á»£c truyá»n cáº£m há»©ng bá»Ÿi thÃ nh cÃ´ng lá»›n cá»§a cÃ¡c mÃ´ hÃ¬nh pretrained nhÆ° BERT [16] vÃ  GPT [69] trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP). CÃ¡c mÃ´ hÃ¬nh nÃ y, bao gá»“m CodeBERT [18] vÃ  IntelliCode Compose [72], Ä‘Ã£ dáº«n Ä‘áº¿n cÃ¡c cáº£i tiáº¿n thÃªm trong cÃ¡c váº¥n Ä‘á» hiá»ƒu vÃ  sinh mÃ£, nhÆ°ng chÃºng thiáº¿u má»™t bá»™ benchmark bao quÃ¡t nhiá»u tÃ¡c vá»¥. Viá»‡c sá»­ dá»¥ng ImageNet [15] cho computer vision vÃ  viá»‡c sá»­ dá»¥ng GLUE [81] cho NLP Ä‘Ã£ cho tháº¥y ráº±ng má»™t benchmark dataset Ä‘a dáº¡ng cÃ³ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n sá»± phÃ¡t triá»ƒn cá»§a nghiÃªn cá»©u AI á»©ng dá»¥ng.arXiv:2102.04664v2  [cs.SE]  16 Mar 2021

--- TRANG 2 ---
Lu, Guo, Ren vÃ  Huang, et al.

Báº£ng 1: TÃ³m táº¯t ngáº¯n gá»n vá» CodeXGLUE, bao gá»“m cÃ¡c tÃ¡c vá»¥, dataset, ngÃ´n ngá»¯, kÃ­ch thÆ°á»›c trong cÃ¡c tráº¡ng thÃ¡i khÃ¡c nhau vÃ  cÃ¡c há»‡ thá»‘ng baseline. CÃ¡c dataset Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u lÃ  má»›i Ä‘Æ°á»£c giá»›i thiá»‡u.

Danh má»¥c | TÃ¡c vá»¥ | TÃªn Dataset | NgÃ´n ngá»¯ | KÃ­ch thÆ°á»›c Train/Dev/Test | Baseline
Code-Code | Clone Detection | BigCloneBench [71] | Java | 900K/416K/416K | 
| | POJ-104 [52] | C/C++ | 32K/8K/12K | 
| Defect Detection | Devign [99] | C | 21K/2.7K/2.7K | 
| Cloze Test | CT-all | Python,Java,PHP, JavaScript,Ruby,Go | -/-/176K | 
| | CT-max/min [18] | Python,Java,PHP, JavaScript,Ruby,Go | -/-/2.6K | CodeBERT
| Code Completion | PY150 [62] | Python | 100K/5K/50K | 
| | Github Java Corpus[4] | Java | 13K/7K/8K | CodeGPT
| Code Repair | Bugs2Fix [75] | Java | 98K/12K/12K | Encoder-
| Code Translation | CodeTrans | Java-C# | 10K/0.5K/1K | Decoder
Text-Code | NL Code Search | CodeSearchNet [35], AdvTest | Python | 251K/9.6K/19K | 
| | CodeSearchNet [35], WebQueryTest | Python | 251K/9.6K/1K | CodeBERT
| Text-to-Code Generation | CONCODE [38] | Java | 100K/2K/2K | CodeGPT
Code-Text | Code Summarization | CodeSearchNet [35] | Python,Java,PHP, JavaScript,Ruby,Go | 908K/45K/53K | Encoder-Decoder
Text-Text | Documentation Translation | Microsoft Docs | English-Latvian/Danish/Norwegian/Chinese | 156K/4K/4K | 

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i giá»›i thiá»‡u CodeXGLUE, má»™t benchmark dataset machine learning cho nghiÃªn cá»©u hiá»ƒu vÃ  sinh mÃ£ chÆ°Æ¡ng trÃ¬nh bao gá»“m 14 datasetÂ³, má»™t táº­p há»£p 10 tÃ¡c vá»¥ hiá»ƒu vÃ  sinh ngÃ´n ngá»¯ láº­p trÃ¬nh Ä‘a dáº¡ng, vÃ  má»™t ná»n táº£ng cho viá»‡c Ä‘Ã¡nh giÃ¡ vÃ  so sÃ¡nh mÃ´ hÃ¬nh. CodeXGLUE há»— trá»£ cÃ¡c tÃ¡c vá»¥ sau:

â€¢ code-code (phÃ¡t hiá»‡n clone [10,52,71,84,89,93,97], phÃ¡t hiá»‡n lá»—i [47,57,61,82,83,99], cloze test [18], hoÃ n thÃ nh mÃ£ [4,8,9,31,62,63,72,73], sá»­a mÃ£ [2,28,30,75,76,78], vÃ  dá»‹ch code-to-code [11, 41, 46, 54])
â€¢ text-code (tÃ¬m kiáº¿m mÃ£ ngÃ´n ngá»¯ tá»± nhiÃªn [23,35,85], sinh mÃ£ tá»« text [12, 26, 36, 38, 87, 90, 94, 95])
â€¢ code-text (tÃ³m táº¯t mÃ£ [3,12,19,34,37,80,85â€“87])
â€¢ text-text (dá»‹ch tÃ i liá»‡u [40])

CodeXGLUE bao gá»“m tÃ¡m dataset Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t trÆ°á»›c Ä‘Ã¢y â€” BigCloneBench [71], POJ-104 [52], Devign [99], PY150 [62], Github Java Corpus [4], Bugs2Fix [75], CONCODE [38], vÃ  CodeSearchNet [35]â€” nhÆ°ng cÅ©ng cÃ³ cÃ¡c dataset má»›i Ä‘Æ°á»£c giá»›i thiá»‡u Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u trong Báº£ng 1. CÃ¡c dataset Ä‘Æ°á»£c chá»n hoáº·c táº¡o dá»±a trÃªn viá»‡c xem xÃ©t ráº±ng tÃ¡c vá»¥ cÃ³ Ä‘á»‹nh nghÄ©a rÃµ rÃ ng, vÃ  khá»‘i lÆ°á»£ng cá»§a dataset cÃ³ thá»ƒ há»— trá»£ phÃ¡t triá»ƒn vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p machine learning dá»±a trÃªn dá»¯ liá»‡u. CÃ¡c dataset Ä‘Æ°á»£c táº¡o bá»Ÿi chÃºng tÃ´i bao gá»“m (1) hai bá»™ test cloze test bao quÃ¡t 6 ngÃ´n ngá»¯ láº­p trÃ¬nh, (2) hai bá»™ test hoÃ n thÃ nh mÃ£ cáº¥p dÃ²ng trong Java vÃ  Python tÆ°Æ¡ng á»©ng, (3) má»™t dataset dá»‹ch code-to-code giá»¯a Java vÃ  C#, (4) hai bá»™ test tÃ¬m kiáº¿m mÃ£ ngÃ´n ngá»¯ tá»± nhiÃªn vá»›i cÃ¡c truy váº¥n web vÃ  tÃªn hÃ m vÃ  biáº¿n Ä‘Æ°á»£c chuáº©n hÃ³a tÆ°Æ¡ng á»©ng, vÃ  (5) má»™t dataset dá»‹ch tÃ i liá»‡u bao quÃ¡t nÄƒm ngÃ´n ngá»¯ tá»± nhiÃªn.

Â³ ChÃºng tÃ´i dá»± Ä‘á»‹nh phÃ¡t triá»ƒn benchmark theo thá»i gian báº±ng cÃ¡ch má»Ÿ rá»™ng thÃªm cÃ¡c tÃ¡c vá»¥.

Äá»ƒ giÃºp cho ngÆ°á»i tham gia dá»… dÃ ng, chÃºng tÃ´i cung cáº¥p ba mÃ´ hÃ¬nh baseline Ä‘á»ƒ giÃºp thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥, bao gá»“m má»™t mÃ´ hÃ¬nh pretrained kiá»ƒu BERT (trong trÆ°á»ng há»£p nÃ y lÃ  CodeBERT) Ä‘á»ƒ há»— trá»£ cÃ¡c váº¥n Ä‘á» hiá»ƒu mÃ£, má»™t mÃ´ hÃ¬nh pretrained kiá»ƒu GPT, mÃ  chÃºng tÃ´i gá»i lÃ  CodeGPT, Ä‘á»ƒ giÃºp giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» hoÃ n thÃ nh vÃ  sinh, vÃ  má»™t framework Encoder-Decoder giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» sinh sequence-to-sequence.

2 Tá»”NG QUAN CÃC TÃC Vá»¤
Trong pháº§n nÃ y, chÃºng tÃ´i cung cáº¥p Ä‘á»‹nh nghÄ©a cho tá»«ng tÃ¡c vá»¥.

Clone detection [52, 71]. TÃ¡c vá»¥ nÃ y nháº±m Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a cÃ¡c mÃ£. Äiá»u nÃ y bao gá»“m hai subtask: phÃ¢n loáº¡i nhá»‹ phÃ¢n giá»¯a má»™t cáº·p mÃ£ vÃ  truy xuáº¥t mÃ£, trong Ä‘Ã³ má»¥c tiÃªu lÃ  tÃ¬m cÃ¡c mÃ£ tÆ°Æ¡ng tá»± vá» máº·t ngá»¯ nghÄ©a.

Defect detection [99]. Má»¥c tiÃªu lÃ  xÃ¡c Ä‘á»‹nh liá»‡u má»™t Ä‘oáº¡n mÃ£ nguá»“n cÃ³ chá»©a lá»—i cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¥n cÃ´ng cÃ¡c há»‡ thá»‘ng pháº§n má»m hay khÃ´ng, cháº³ng háº¡n nhÆ° rÃ² rá»‰ tÃ i nguyÃªn, lá»— há»•ng use-after-free vÃ  táº¥n cÃ´ng DoS.

Cloze test [18]. Äiá»u nÃ y nháº±m dá»± Ä‘oÃ¡n token bá»‹ che cá»§a má»™t mÃ£ vÃ  bao gá»“m hai subtask. CÃ¡i Ä‘áº§u tiÃªn lÃ  Ä‘o Ä‘á»™ chÃ­nh xÃ¡c cá»§a viá»‡c dá»± Ä‘oÃ¡n token bá»‹ che tá»« toÃ n bá»™ tá»« vá»±ng. CÃ¡i khÃ¡c lÃ  kiá»ƒm tra kháº£ nÄƒng lÃ½ luáº­n ngá»¯ nghÄ©a báº±ng cÃ¡ch phÃ¢n biá»‡t giá»¯a "max" vÃ  "min".

Code completion [4, 62]. NÃ³ nháº±m dá»± Ä‘oÃ¡n cÃ¡c token tiáº¿p theo dá»±a trÃªn ngá»¯ cáº£nh mÃ£. CÃ¡c subtask cá»§a nÃ³ lÃ  hoÃ n thÃ nh cáº¥p token vÃ  hoÃ n thÃ nh cáº¥p dÃ²ng. CÃ¡i Ä‘áº§u tiÃªn kiá»ƒm tra xem token tiáº¿p theo cÃ³ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n Ä‘Ãºng hay khÃ´ng, trong khi cÃ¡i sau kiá»ƒm tra cháº¥t lÆ°á»£ng cá»§a dÃ²ng Ä‘Æ°á»£c sinh.

Code translation [54]. NÃ³ bao gá»“m viá»‡c dá»‹ch má»™t mÃ£ tá»« má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh sang má»™t ngÃ´n ngá»¯ khÃ¡c.

--- TRANG 3 ---
CodeXGLUE: Má»™t Benchmark Dataset cho Machine Learning trong viá»‡c Hiá»ƒu vÃ  Sinh Code

Code search [35]. NÃ³ Ä‘o lÆ°á»ng má»©c Ä‘á»™ liÃªn quan ngá»¯ nghÄ©a giá»¯a vÄƒn báº£n vÃ  mÃ£ vÃ  Ä‘Æ°á»£c cáº¥u thÃ nh tá»« hai subtask. CÃ¡i Ä‘áº§u tiÃªn lÃ  tÃ¬m mÃ£ liÃªn quan nháº¥t trong má»™t táº­p há»£p cÃ¡c mÃ£ theo má»™t truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn. Subtask thá»© hai bao gá»“m phÃ¢n tÃ­ch má»™t cáº·p truy váº¥n-mÃ£ Ä‘á»ƒ dá»± Ä‘oÃ¡n liá»‡u mÃ£ cÃ³ tráº£ lá»i truy váº¥n hay khÃ´ng.

Code repair [75]. Má»¥c tiÃªu cá»§a nÃ³ lÃ  tinh chá»‰nh mÃ£ báº±ng cÃ¡ch sá»­a lá»—i tá»± Ä‘á»™ng.

Text-to-code generation [38]. Äiá»u nÃ y nháº±m sinh má»™t mÃ£ thÃ´ng qua mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn.

Code summarization [37]. Má»¥c tiÃªu lÃ  sinh chÃº thÃ­ch ngÃ´n ngá»¯ tá»± nhiÃªn cho má»™t mÃ£.

Documentation translation [40]. NÃ³ nháº±m dá»‹ch tÃ i liá»‡u mÃ£ tá»« má»™t ngÃ´n ngá»¯ tá»± nhiÃªn sang ngÃ´n ngá»¯ khÃ¡c.

3 CÃC DATASET
Trong pháº§n nÃ y, chÃºng tÃ´i mÃ´ táº£ cÃ¡c dataset cÃ³ trong CodeXGLUE. CÃ¡c dataset Ä‘Æ°á»£c chá»n hoáº·c táº¡o dá»±a trÃªn tiÃªu chÃ­ ráº±ng khá»‘i lÆ°á»£ng cá»§a dataset cÃ³ thá»ƒ há»— trá»£ phÃ¡t triá»ƒn vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p machine learning dá»±a trÃªn dá»¯ liá»‡u.

3.1 Clone detection
Clone detection bao gá»“m hai subtask. Subtask Ä‘áº§u tiÃªn lÃ  dá»± Ä‘oÃ¡n liá»‡u hai mÃ£ cho trÆ°á»›c cÃ³ cÃ¹ng ngá»¯ nghÄ©a hay khÃ´ng. ChÃºng tÃ´i sá»­ dá»¥ng dataset BigCloneBench [71] cho subtask nÃ y. Subtask thá»© hai nháº±m truy xuáº¥t cÃ¡c mÃ£ tÆ°Æ¡ng tá»± vá» máº·t ngá»¯ nghÄ©a vá»›i má»™t mÃ£ lÃ m truy váº¥n vÃ  chÃºng tÃ´i sá»­ dá»¥ng dataset POJ-104 [52] Ä‘á»ƒ thá»±c hiá»‡n nÃ³.

BigCloneBench lÃ  má»™t benchmark clone mÃ£ lá»›n Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i chá»©a hÆ¡n 6,000,000 cáº·p clone Ä‘Ãºng vÃ  260,000 cáº·p clone sai tá»« 10 chá»©c nÄƒng khÃ¡c nhau. Dataset Ä‘Æ°á»£c cung cáº¥p bá»Ÿi Wang et al. [84] Ä‘Æ°á»£c lá»c báº±ng cÃ¡ch loáº¡i bá» cÃ¡c Ä‘oáº¡n mÃ£ khÃ´ng cÃ³ báº¥t ká»³ cáº·p clone Ä‘Ãºng hoáº·c sai nÃ o Ä‘Æ°á»£c gáº¯n tháº», Ä‘á»ƒ láº¡i nÃ³ vá»›i 9,134 Ä‘oáº¡n mÃ£ Java. Cuá»‘i cÃ¹ng, dataset bao gá»“m 901,028/415,416/415,416 vÃ­ dá»¥ cho training, validation vÃ  testing tÆ°Æ¡ng á»©ng.

Dataset POJ-104 [52] Ä‘áº¿n tá»« má»™t há»‡ thá»‘ng open judge (OJ) láº­p trÃ¬nh giÃ¡o dá»¥c tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ tÃ­nh há»£p lá»‡ cá»§a mÃ£ nguá»“n Ä‘Æ°á»£c gá»­i cho cÃ¡c váº¥n Ä‘á» cá»¥ thá»ƒ báº±ng cÃ¡ch cháº¡y mÃ£. ChÃºng tÃ´i sá»­ dá»¥ng dataset POJ-104, bao gá»“m 104 váº¥n Ä‘á» vÃ  bao gá»“m 500 chÆ°Æ¡ng trÃ¬nh C/C++ Ä‘Æ°á»£c viáº¿t bá»Ÿi sinh viÃªn cho má»—i váº¥n Ä‘á». KhÃ¡c vá»›i dataset BigCloneBench, tÃ¡c vá»¥ cá»§a POJ-104 nháº±m truy xuáº¥t cÃ¡c chÆ°Æ¡ng trÃ¬nh khÃ¡c giáº£i quyáº¿t cÃ¹ng váº¥n Ä‘á» vá»›i má»™t chÆ°Æ¡ng trÃ¬nh cho trÆ°á»›c. ChÃºng tÃ´i nhÃ³m cÃ¡c dataset thÃ nh ba táº­p con dá»±a trÃªn sá»‘ lÆ°á»£ng váº¥n Ä‘á» chÃºng Ä‘Æ°á»£c yÃªu cáº§u giáº£i quyáº¿t (64/16/24) cho training, validation vÃ  testing.

3.2 Defect detection
Cho tÃ¡c vá»¥ phÃ¡t hiá»‡n lá»—i, Zhou et al. [99] cung cáº¥p dataset Devign bao gá»“m 27,318 hÃ m Ä‘Æ°á»£c gáº¯n nhÃ£n thá»§ cÃ´ng Ä‘Æ°á»£c thu tháº­p tá»« hai dá»± Ã¡n mÃ£ nguá»“n má»Ÿ ngÃ´n ngá»¯ láº­p trÃ¬nh C lá»›n phá»• biáº¿n trong cÃ¡c láº­p trÃ¬nh viÃªn vÃ  Ä‘a dáº¡ng vá» chá»©c nÄƒng, tá»©c lÃ  QEMU vÃ  FFmpeg. Dataset Ä‘Æ°á»£c táº¡o báº±ng cÃ¡ch thu tháº­p cÃ¡c commit liÃªn quan Ä‘áº¿n báº£o máº­t vÃ  trÃ­ch xuáº¥t cÃ¡c hÃ m dá»… bá»‹ tá»•n thÆ°Æ¡ng hoáº·c khÃ´ng dá»… bá»‹ tá»•n thÆ°Æ¡ng tá»« cÃ¡c commit Ä‘Æ°á»£c gáº¯n nhÃ£n. VÃ¬ Zhou et al. [99] khÃ´ng cung cáº¥p cÃ¡c táº­p training/validation/testing chÃ­nh thá»©c cho hai dá»± Ã¡n, chÃºng tÃ´i xÃ¡o trá»™n ngáº«u nhiÃªn dataset vÃ  chia 80%/10%/10% cá»§a dataset cho training/validation/testing. TÃ¡c vá»¥ Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° má»™t phÃ¢n loáº¡i nhá»‹ phÃ¢n Ä‘á»ƒ dá»± Ä‘oÃ¡n liá»‡u má»™t hÃ m cÃ³ dá»… bá»‹ tá»•n thÆ°Æ¡ng hay khÃ´ng.

3.3 Cloze test
HÃ¬nh 1 cho tháº¥y hai vÃ­ dá»¥ vá» tÃ¡c vá»¥ cloze test (CT) trong domain mÃ£, nháº±m Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng cá»§a cÃ¡c mÃ´ hÃ¬nh hiá»ƒu má»™t mÃ£ báº±ng cÃ¡ch yÃªu cáº§u cÃ¡c mÃ´ hÃ¬nh Ä‘Ã³ dá»± Ä‘oÃ¡n mÃ£ bá»‹ che tá»« má»™t sá»‘ á»©ng viÃªn. ChÃºng tÃ´i táº­p trung vÃ o hai subtask: CT-all vá»›i cÃ¡c á»©ng viÃªn tá»« má»™t tá»« vá»±ng Ä‘Æ°á»£c lá»c vÃ  CT-maxmin vá»›i cÃ¡c á»©ng viÃªn "max" vÃ  "min".

Má»Ÿ há»™p tháº£. Doc.:
Code:def open(self): 
self.workingArea .<mask> ( )
self.runid_pkgidx_map = {}
self.runid_to_return = deque()
ÄÃ¡p Ã¡n: open

TÃ¬m giÃ¡ trá»‹ min vÃ  max cá»§a má»i feature. Doc.:
Code:deffit(self, X, y=None):
X = check_array (X)
self._ x_min = X.<mask> (axis=0)
self._ x_max = X.max (axis=0)
return self
ÄÃ¡p Ã¡n: min

Cloze Test -maxmin    Cloze Test-all

HÃ¬nh 1: Hai vÃ­ dá»¥ trong dataset cloze test.

ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c táº­p validation vÃ  testing cá»§a CodeSearchNet [35] Ä‘á»ƒ táº¡o cÃ¡c dataset CT-all vÃ  CT-maxmin cho sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh, tá»©c lÃ  Go, Java, JavaScript (JS), PHP, Python vÃ  Ruby.

CT-all. Äá»ƒ Ã­t giá»›i thiá»‡u cÃ¡c tÃªn biáº¿n dÃ i vÃ  trÃ¡nh váº¥n Ä‘á» gÃ¢y ra bá»Ÿi viá»‡c sá»­ dá»¥ng cÃ¡c tokenizer khÃ¡c nhau, chÃºng tÃ´i chá»n cÃ¡c tá»« cloze Ä‘Ã­ch báº±ng cÃ¡ch giá»¯ láº¡i cÃ¡c tá»« duy nháº¥t sau Byte Pair Encoding [67], vÃ  chÃºng tÃ´i loáº¡i bá» cÃ¡c token vÃ´ nghÄ©a nhÆ° dáº¥u cÃ¢u vá»›i cÃ¡c quy táº¯c thá»§ cÃ´ng. Cuá»‘i cÃ¹ng, 930 token Ä‘Æ°á»£c chá»n trong sÃ¡u ngÃ´n ngá»¯ tá»•ng cá»™ng. ChÃºng tÃ´i chá»n cÃ¡c mÃ£ chá»©a 930 token vÃ  thiáº¿t láº­p thá»§ cÃ´ng cÃ¡c giÃ¡ trá»‹ ngÆ°á»¡ng xuáº¥t hiá»‡n token Ä‘á»ƒ cÃ¢n báº±ng táº§n suáº¥t cá»§a 930 token trong CT-all.

CT-maxmin. Äá»ƒ Ä‘Ã¡nh giÃ¡ thÃªm kháº£ nÄƒng hiá»ƒu ngá»¯ nghÄ©a mÃ£ cá»§a cÃ¡c mÃ´ hÃ¬nh, chÃºng tÃ´i giá»›i thiá»‡u CT-maxmin Ä‘á»ƒ kiá»ƒm tra mÃ´ hÃ¬nh cÃ³ thá»ƒ phÃ¢n biá»‡t sá»± khÃ¡c biá»‡t giá»¯a max vÃ  min tá»‘t nhÆ° tháº¿ nÃ o. CT-maxmin Ä‘áº¿n tá»« dataset Ä‘Æ°á»£c sá»­ dá»¥ng cho tÃ¡c vá»¥ PL-Probing trong CodeBERT [18], bao gá»“m cÃ¡c mÃ£ chá»©a tá»« khÃ³a max hoáº·c min.

Thá»‘ng kÃª dá»¯ liá»‡u Ä‘Æ°á»£c liá»‡t kÃª trong Báº£ng 2.

3.4 Code completion
ChÃºng tÃ´i sá»­ dá»¥ng hai dataset cÃ³ áº£nh hÆ°á»Ÿng cho hoÃ n thÃ nh mÃ£, PY150 trong python vÃ  Github Java Corpus trong Java. Cáº£ hai dataset Ä‘á»u cÃ³ thá»ƒ giÃºp Ä‘áº¡t Ä‘Æ°á»£c hoÃ n thÃ nh mÃ£ cáº¥p token. ChÃºng tÃ´i tiáº¿n xa hÆ¡n báº±ng cÃ¡ch táº¡o hai bá»™ test cho tÃ¡c vá»¥ hoÃ n thÃ nh mÃ£ cáº¥p dÃ²ng tá»« hai corpus. TÃ¡c vá»¥ lÃ  hoÃ n thÃ nh má»™t dÃ²ng chÆ°a hoÃ n thÃ nh. CÃ¡c mÃ´ hÃ¬nh nÃªn

--- TRANG 4 ---
Lu, Guo, Ren vÃ  Huang, et al.

Báº£ng 2: Thá»‘ng kÃª dá»¯ liá»‡u vá» cÃ¡c dataset cloze test.

TÃ¡c vá»¥ | CT-all | CT-maxmin
Go | 25,282 | 152
Java | 40,492 | 482
JavaScript | 13,837 | 272
PHP | 51,930 | 407
Python | 40,137 | 1,264
Ruby | 4,437 | 38
Táº¥t cáº£ | 176,115 | 2,615

cÃ³ kháº£ nÄƒng dá»± Ä‘oÃ¡n cÃ¡c chuá»—i mÃ£ vá»›i cÃ¡c loáº¡i token tÃ¹y Ã½ vÃ  cáº¥u trÃºc mÃ£.

PY150 lÃ  má»™t dataset Python [62] chá»©a 150,000 file nguá»“n Python Ä‘Æ°á»£c thu tháº­p tá»« Github. ChÃºng tÃ´i theo phÃ¢n chia dá»¯ liá»‡u trong Raychev et al. [62], dáº«n Ä‘áº¿n 100,000 file cho training vÃ  50,000 file cho testing, bao gá»“m 76.3M token vÃ  37.2M token tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i tiá»n xá»­ lÃ½ cÃ¡c corpus báº±ng cÃ¡ch tokenize mÃ£ nguá»“n, loáº¡i bá» comment, thay tháº¿ chuá»—i cÃ³ Ä‘á»™ dÃ i hÆ¡n 15 kÃ½ tá»± báº±ng chuá»—i rá»—ng, vÃ  thÃªm token Ä‘áº·c biá»‡t âŸ¨EOLâŸ©(end-of-line) Ä‘á»ƒ Ä‘Ã¡nh dáº¥u káº¿t thÃºc cá»§a má»™t dÃ²ng má»™t cÃ¡ch rÃµ rÃ ng. Cho hoÃ n thÃ nh mÃ£ cáº¥p dÃ²ng, chÃºng tÃ´i táº¡o 10,000 vÃ­ dá»¥ tá»« cÃ¡c file khÃ¡c nhau trong táº­p test cá»§a PY150 Ä‘á»ƒ testing. VÃ¬ chÃºng tÃ´i cÃ³ Ã½ Ä‘á»‹nh kiá»ƒm tra kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh tá»± Ä‘á»™ng hoÃ n thÃ nh má»™t dÃ²ng tÃ¹y Ã½, chÃºng tÃ´i chá»n dÃ²ng Ä‘Æ°á»£c dá»± Ä‘oÃ¡n má»™t cÃ¡ch ngáº«u nhiÃªn. ChÃºng tÃ´i sinh má»™t test case báº±ng cÃ¡ch Ä‘áº£m báº£o ráº±ng cÃ³ Ä‘á»§ ngá»¯ cáº£nh, tá»©c lÃ  Ã­t nháº¥t 15% cá»§a toÃ n bá»™ file. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c ká»³ vá»ng sinh dÃ²ng tiáº¿p theo káº¿t thÃºc bá»Ÿi âŸ¨EOLâŸ© cho ngá»¯ cáº£nh. Sá»‘ lÆ°á»£ng token trung bÃ¬nh trong input vÃ  output láº§n lÆ°á»£t lÃ  489.11 vÃ  6.56. HÃ¬nh 2 cho tháº¥y má»™t vÃ­ dá»¥ vá» hoÃ n thÃ nh mÃ£ cáº¥p dÃ²ng.

HÃ¬nh 2: Má»™t vÃ­ dá»¥ trong dataset hoÃ n thÃ nh mÃ£ cáº¥p dÃ²ng.

Github Java Corpus lÃ  má»™t dataset Java Ä‘Æ°á»£c khai thÃ¡c bá»Ÿi Allamanis vÃ  Sutton [4], vÃ  nÃ³ thu tháº­p hÆ¡n 14 nghÃ¬n dá»± Ã¡n Java tá»« Github. ChÃºng tÃ´i theo cÃ¡c thiáº¿t láº­p Ä‘Æ°á»£c thiáº¿t láº­p bá»Ÿi Hellendoorn vÃ  Devanbu [29] cÅ©ng nhÆ° Karampatsis et al. [42], sá»­ dá»¥ng 1% cá»§a táº­p con trong corpus. ChÃºng tÃ´i cÃ³ 12,934/7,189/8,268 file cho training/validation/testing, bao gá»“m 15.8M/3.8M/5.3M token tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i thá»±c hiá»‡n cÃ¹ng tiá»n xá»­ lÃ½ Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn PY150, nhÆ°ng chÃºng tÃ´i khÃ´ng thÃªm token Ä‘áº·c biá»‡t âŸ¨EOLâŸ© vÃ¬ trong Java cÃ¡c kÃ½ hiá»‡u ; vÃ  } Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh dáº¥u káº¿t thÃºc cá»§a má»™t cÃ¢u lá»‡nh mÃ£. Cho hoÃ n thÃ nh mÃ£ cáº¥p dÃ²ng, chÃºng tÃ´i táº¡o 3,000 vÃ­ dá»¥ Ä‘á»ƒ testing tá»« cÃ¡c file khÃ¡c nhau trong táº­p test cá»§a corpus. TÆ°Æ¡ng tá»± nhÆ° quÃ¡ trÃ¬nh chÃºng tÃ´i theo cho Python, dÃ²ng Ä‘Æ°á»£c dá»± Ä‘oÃ¡n Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn tá»« test file. Sá»‘ lÆ°á»£ng token trung bÃ¬nh láº§n lÆ°á»£t lÃ  350.62 vÃ  10.49 trong input vÃ  output.

3.5 Code translation
Dá»¯ liá»‡u training cho dá»‹ch mÃ£ lÃ  cÃ¡c cáº·p mÃ£ vá»›i chá»©c nÄƒng tÆ°Æ¡ng Ä‘Æ°Æ¡ng trong hai ngÃ´n ngá»¯ láº­p trÃ¬nh. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i cung cáº¥p má»™t dataset bao gá»“m cÃ¡c mÃ£ song song giá»¯a Java vÃ  C#. ChÃºng tÃ´i khÃ´ng sá»­ dá»¥ng dataset cá»§a Lachaux et al. [46] vÃ¬ há» khÃ´ng cÃ³ dá»¯ liá»‡u cho training mÃ´ hÃ¬nh cÃ³ giÃ¡m sÃ¡t. Theo Nguyen et al. [54] vÃ  Chen et al. [11], chÃºng tÃ´i sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p tá»« má»™t sá»‘ dá»± Ã¡n mÃ£ nguá»“n má»Ÿ, tá»©c lÃ  Luceneâ´, POIâµ, JGitâ¶ vÃ  Antlrâ·. ChÃºng tÃ´i khÃ´ng sá»­ dá»¥ng Itextâ¸ vÃ  JTSâ¹ do váº¥n Ä‘á» báº£n quyá»n. Nhá»¯ng dá»± Ã¡n Ä‘Ã³ ban Ä‘áº§u Ä‘Æ°á»£c phÃ¡t triá»ƒn trong Java vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c chuyá»ƒn sang C#. ChÃºng lÃ  cÃ¡c há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t láº­p tá»‘t vá»›i lá»‹ch sá»­ phÃ¡t triá»ƒn dÃ i vÃ  vá»›i cáº£ phiÃªn báº£n Java vÃ  C# Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng.

BÆ°á»›c tiáº¿p theo lÃ  khai thÃ¡c cÃ¡c hÃ m hoáº·c phÆ°Æ¡ng thá»©c Ä‘Æ°á»£c ghÃ©p cáº·p tá»« nhá»¯ng dá»± Ã¡n Ä‘Ã³. Theo quan sÃ¡t cá»§a chÃºng tÃ´i, cáº¥u trÃºc thÆ° má»¥c vÃ  tÃªn hÃ m hoáº·c phÆ°Æ¡ng thá»©c cá»§a hai phiÃªn báº£n giá»‘ng nhau hoáº·c tÆ°Æ¡ng tá»± khi chÃºng Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¹ng má»™t dá»± Ã¡n. Do Ä‘Ã³, theo Nguyen et al. [54], chÃºng tÃ´i má»™t cÃ¡ch tháº­n trá»ng tÃ¬m kiáº¿m cÃ¡c hÃ m cÃ³ cÃ¹ng chá»¯ kÃ½ trong cÃ¡c lá»›p cÃ³ tÃªn giá»‘ng/tÆ°Æ¡ng tá»± vÃ  Ä‘Æ°á»£c bao gá»“m trong cÃ¡c cáº¥u trÃºc thÆ° má»¥c giá»‘ng/tÆ°Æ¡ng tá»± cá»§a cáº£ hai phiÃªn báº£n. ChÃºng tÃ´i loáº¡i bá» cÃ¡c cáº·p mÃ£ trÃ¹ng láº·p vÃ  cÃ¡c mÃ£ cÃ³ nhiá»u má»¥c tiÃªu Ä‘Æ°á»£c tÃ¬m kiáº¿m báº±ng phÆ°Æ¡ng phÃ¡p trÃªn. Sau bÆ°á»›c nÃ y, chÃºng tÃ´i loáº¡i bá» cÃ¡c cáº·p cÃ³ sá»‘ lÆ°á»£ng token chá»“ng chÃ©o Ã­t hÆ¡n 1/3 Ä‘á»™ dÃ i cÃ¢u. Äá»ƒ lÃ m cho dá»¯ liá»‡u cá»§a chÃºng tÃ´i cÃ³ thá»ƒ má»Ÿ rá»™ng hÆ¡n cho phÃ¢n tÃ­ch cÃº phÃ¡p vÃ  ngá»¯ nghÄ©a thÃªm, chÃºng tÃ´i cÅ©ng loáº¡i bá» cÃ¡c hÃ m vá»›i thÃ¢n hÃ m null theo abstract syntax tree (AST) cá»§a chÃºng. Sau Ä‘Ã³ chÃºng tÃ´i xÃ¢y dá»±ng Ä‘á»“ thá»‹ data-flow [25] cho má»—i hÃ m, biá»ƒu diá»…n sá»± phá»¥ thuá»™c giá»¯a hai biáº¿n vÃ  cung cáº¥p thÃ´ng tin ngá»¯ nghÄ©a cÃ³ giÃ¡ trá»‹ cho viá»‡c hiá»ƒu mÃ£. Cuá»‘i cÃ¹ng, má»™t hÃ m khÃ´ng cÃ³ data-flow Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« AST cá»§a má»™t hÃ m cá»¥ thá»ƒ cÅ©ng bá»‹ loáº¡i bá».

Cuá»‘i cÃ¹ng, tá»•ng sá»‘ hÃ m hoáº·c phÆ°Æ¡ng thá»©c Ä‘Æ°á»£c ghÃ©p cáº·p lÃ  11,800. ChÃºng tÃ´i chá»n ngáº«u nhiÃªn 500 cáº·p hÃ m cho táº­p development vÃ  1,000 cáº·p khÃ¡c cho táº­p test. Äá»™ dÃ i trung bÃ¬nh cá»§a cÃ¡c hÃ m Java vÃ  C# sau tokenization láº§n lÆ°á»£t lÃ  38.51 vÃ  46.16Â¹â°. Má»™t vÃ­ dá»¥ vá» cÃ¡c cáº·p dá»‹ch Ä‘Æ°á»£c khai thÃ¡c tá»« C# sang Java Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3.

3.6 Code search
Code search bao gá»“m hai subtask. CÃ¡i Ä‘áº§u tiÃªn lÃ  tÃ¬m mÃ£ liÃªn quan nháº¥t tá»« má»™t táº­p há»£p cÃ¡c á»©ng viÃªn cho má»™t truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn. ChÃºng tÃ´i táº¡o má»™t táº­p testing thÃ¡ch thá»©c, Ä‘Æ°á»£c gá»i lÃ  CodeSearchNet AdvTest, tá»« corpus CodeSearchNet [35] Ä‘á»ƒ thá»±c hiá»‡n tÃ¡c vá»¥ nÃ y. Má»™t vÃ­ dá»¥ vá» dataset nÃ y Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4. Subtask thá»© hai lÃ  dá»± Ä‘oÃ¡n liá»‡u má»™t mÃ£ cÃ³ tráº£ lá»i má»™t truy váº¥n cho trÆ°á»›c hay khÃ´ng. ChÃºng tÃ´i cung cáº¥p má»™t táº­p testing WebQueryTest cá»§a cÃ¡c truy váº¥n ngÆ°á»i dÃ¹ng thá»±c. Hai vÃ­ dá»¥ cá»§a dataset Ä‘Æ°á»£c minh há»a trong HÃ¬nh 5.

â´ http://lucene.apache.org/
âµ http://poi.apache.org/
â¶ https://github.com/eclipse/jgit/
â· https://github.com/antlr/
â¸ http://sourceforge.net/projects/itext/
â¹ http://sourceforge.net/projects/jts-topo-suite/
Â¹â° https://github.com/c2nes/javalang

--- TRANG 5 ---
CodeXGLUE: Má»™t Benchmark Dataset cho Machine Learning trong viá»‡c Hiá»ƒu vÃ  Sinh Code

HÃ¬nh 3: Má»™t vÃ­ dá»¥ trong dataset dá»‹ch mÃ£.

CodeSearchNet AdvTest lÃ  má»™t dataset Python tá»« corpus CodeSearchNet [35]. Má»—i vÃ­ dá»¥ bao gá»“m má»™t hÃ m Ä‘Æ°á»£c ghÃ©p cáº·p vá»›i má»™t tÃ i liá»‡u. ChÃºng tÃ´i theo Husain et al. [35] Ä‘á»ƒ láº¥y Ä‘oáº¡n Ä‘áº§u tiÃªn cá»§a tÃ i liá»‡u lÃ m truy váº¥n cho hÃ m tÆ°Æ¡ng á»©ng. Äá»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng cá»§a dataset, chÃºng tÃ´i lá»c nÃ³ báº±ng cÃ¡ch loáº¡i bá» cÃ¡c vÃ­ dá»¥ sau.

(1) CÃ¡c vÃ­ dá»¥ cÃ³ mÃ£ khÃ´ng thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ­ch thÃ nh abstract syntax tree.
(2) CÃ¡c vÃ­ dá»¥ cÃ³ sá»‘ token tÃ i liá»‡u ngáº¯n hÆ¡n 3 hoáº·c lá»›n hÆ¡n 256.
(3) CÃ¡c vÃ­ dá»¥ cÃ³ tÃ i liá»‡u chá»©a token Ä‘áº·c biá»‡t nhÆ° "http://".
(4) CÃ¡c vÃ­ dá»¥ cÃ³ tÃ i liá»‡u rá»—ng hoáº·c khÃ´ng Ä‘Æ°á»£c viáº¿t báº±ng tiáº¿ng Anh.

á» cuá»‘i quÃ¡ trÃ¬nh, chÃºng tÃ´i cÃ³ Ä‘Æ°á»£c má»™t dataset vá»›i 251,820 / 9,604 / 19,210 vÃ­ dá»¥ cho training/validation/testing. Sau khi chuáº©n hÃ³a tÃªn hÃ m hoáº·c biáº¿n vá»›i cÃ¡c token Ä‘áº·c biá»‡t, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng Ä‘iá»ƒm Mean Reciprocal Rank (MRR) cá»§a RoBERTa [50] vÃ  CodeBERT [18] cho tÃ¡c vá»¥ tÃ¬m kiáº¿m mÃ£ trÃªn dataset CodesearchNet [35] giáº£m tá»« 0.809 xuá»‘ng 0.419 vÃ  tá»« 0.869 xuá»‘ng 0.507 tÆ°Æ¡ng á»©ng, trong ngÃ´n ngá»¯ láº­p trÃ¬nh Python. Äá»ƒ kiá»ƒm tra tá»‘t hÆ¡n kháº£ nÄƒng hiá»ƒu vÃ  khÃ¡i quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh, chÃºng tÃ´i chuáº©n hÃ³a tÃªn hÃ m vÃ  biáº¿n trong cÃ¡c táº­p testing vÃ  development nhÆ° ğ‘“ğ‘¢ğ‘›ğ‘ cho tÃªn hÃ m vÃ  ğ‘ğ‘Ÿğ‘” ğ‘– cho tÃªn biáº¿n thá»© i. HÃ¬nh 4 hiá»ƒn thá»‹ má»™t vÃ­ dá»¥ trong dataset CodeSearchNet AdvTest. TÃ¡c vá»¥ nháº±m tÃ¬m kiáº¿m mÃ£ nguá»“n tá»« cÃ¡c á»©ng viÃªn cho má»™t truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn. TrÃ¡i ngÆ°á»£c vá»›i giai Ä‘oáº¡n testing cá»§a cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c [18,35] chá»‰ liÃªn quan Ä‘áº¿n 1,000 á»©ng viÃªn, chÃºng tÃ´i sá»­ dá»¥ng toÃ n bá»™ táº­p testing cho má»—i truy váº¥n, Ä‘iá»u nÃ y lÃ m cho dataset CodeSearchNet AdvTest khÃ³ hÆ¡n. Táº­p training cho tÃ¡c vá»¥ nÃ y Ä‘áº¿n tá»« dataset CodeSearchNet Ä‘Æ°á»£c lá»c [35].

WebQueryTest: Háº§u háº¿t cÃ¡c dataset tÃ¬m kiáº¿m mÃ£ sá»­ dá»¥ng tÃ i liá»‡u mÃ£ hoáº·c cÃ¢u há»i tá»« cÃ¡c cá»™ng Ä‘á»“ng trá»±c tuyáº¿n cho cÃ¡c láº­p trÃ¬nh viÃªn pháº§n má»m lÃ m truy váº¥n, nhÆ°ng chÃºng khÃ¡c vá»›i cÃ¡c truy váº¥n tÃ¬m kiáº¿m ngÆ°á»i dÃ¹ng thá»±c. Äá»ƒ sá»­a chá»¯a sá»± khÃ¡c biá»‡t nÃ y, chÃºng tÃ´i cung cáº¥p WebQueryTest, má»™t táº­p testing cá»§a

QuÃ©t qua má»™t chuá»—i Ä‘á»ƒ tÃ¬m cÃ¡c chuá»—i con khá»›p vá»›i má»™t sá»‘ máº«u. Truy váº¥n:

MÃ£ VÃ ng:
defmatchall(text, patterns):
ret = []
forpattern inpatterns:
match = re.findall(pattern, text)
ret += match
return ret

MÃ£ Chuáº©n hÃ³a:
deffunc (arg0, arg1):
arg2 = []
forarg3 inarg1:
arg4 = re.findall(arg3, arg0)
arg2 += arg4
return arg2

HÃ¬nh 4: Má»™t vÃ­ dá»¥ trong dataset CodeSearchNet AdvTest.

tÃ¬m kiáº¿m mÃ£ thá»±c cho Python. Váº¥n Ä‘á» Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° má»™t tÃ¡c vá»¥ phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  nhÆ° má»™t thiáº¿t láº­p bá»• sung cho tÃ¬nh huá»‘ng truy xuáº¥t. Cho má»™t cáº·p truy váº¥n vÃ  hÃ m mÃ£, má»™t mÃ´ hÃ¬nh cáº§n phÃ¢n loáº¡i liá»‡u hÃ m mÃ£ cÃ³ thá»ƒ tráº£ lá»i truy váº¥n hay khÃ´ng.

QuÃ¡ trÃ¬nh táº¡o dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh hai giai Ä‘oáº¡n: thu tháº­p dá»¯ liá»‡u vÃ  gÃ¡n nhÃ£n. Äáº§u tiÃªn chÃºng tÃ´i thu tháº­p cÃ¡c truy váº¥n ngÆ°á»i dÃ¹ng thá»±c tá»« nháº­t kÃ½ truy váº¥n web cá»§a má»™t cÃ´ng cá»¥ tÃ¬m kiáº¿m thÆ°Æ¡ng máº¡i vÃ  chÃºng tÃ´i giá»¯ cÃ¡c truy váº¥n cÃ³ "python". ÄÆ°á»£c truyá»n cáº£m há»©ng bá»Ÿi Yan et al. [91], chÃºng tÃ´i thiáº¿t káº¿ má»™t sá»‘ heuristic dá»±a trÃªn khá»›p chÃ­nh xÃ¡c tá»« khÃ³a Ä‘á»ƒ lá»c ra cÃ¡c truy váº¥n khÃ´ng cÃ³ Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m mÃ£. Sau Ä‘Ã³ chÃºng tÃ´i chá»n cÃ¡c mÃ£ á»©ng viÃªn cho má»—i truy váº¥n tá»« cÃ¡c táº­p validation vÃ  testing Python trong CodeSearchNet. Äá»ƒ thu háº¹p cÃ¡c á»©ng viÃªn Ä‘Æ°á»£c gÃ¡n nhÃ£n cho má»—i truy váº¥n, chÃºng tÃ´i chá»n hai hÃ m hÃ ng Ä‘áº§u vá»›i Ä‘á»™ tÆ°Æ¡ng tá»± truy váº¥n-mÃ£ cao nháº¥t Ä‘Æ°á»£c tÃ­nh bá»Ÿi má»™t mÃ´ hÃ¬nh truy xuáº¥t mÃ£ dá»±a trÃªn CodeBERT, Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn 148K Python Stack Overflow Question-Code (StaQC) tá»± Ä‘á»™ng [92] vá»›i cÃ¡c tham sá»‘ máº·c Ä‘á»‹nh Ä‘Æ°á»£c cung cáº¥p bá»Ÿi Feng et al. [18].

ChÃºng tÃ´i sá»­ dá»¥ng má»™t lÆ°á»£c Ä‘á»“ gÃ¡n nhÃ£n hai giai Ä‘oáº¡n Ä‘á»ƒ gáº¯n nhÃ£n má»—i thá»ƒ hiá»‡n. BÆ°á»›c Ä‘áº§u tiÃªn lÃ  Ä‘Ã¡nh giÃ¡ liá»‡u truy váº¥n cÃ³ Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m mÃ£ hay khÃ´ng. CÃ¡c thá»ƒ hiá»‡n Ä‘Æ°á»£c gáº¯n nhÃ£n "-1" lÃ  nhá»¯ng cÃ¡i khÃ´ng cÃ³ Ã½ Ä‘á»‹nh tÃ¬m kiáº¿m mÃ£. BÆ°á»›c thá»© hai lÃ  Ä‘Ã¡nh giÃ¡ liá»‡u mÃ£ (vá»›i tÃ i liá»‡u cá»§a nÃ³) cÃ³ thá»ƒ tráº£ lá»i truy váº¥n hay khÃ´ng. CÃ¡c thá»ƒ hiá»‡n Ä‘Æ°á»£c gáº¯n nhÃ£n "1" lÃ  nhá»¯ng cÃ¡i mÃ  mÃ£ cÃ³ thá»ƒ tráº£ lá»i truy váº¥n. Náº¿u khÃ´ng, chÃºng Ä‘Æ°á»£c gáº¯n nhÃ£n "0". Hai vÃ­ dá»¥ Ä‘Æ°á»£c minh há»a trong HÃ¬nh 5. ChÃºng tÃ´i má»i 13 láº­p trÃ¬nh viÃªn thÃ nh tháº¡o Python Ä‘á»ƒ gáº¯n nhÃ£n 1,300 thá»ƒ hiá»‡n, vá»›i má»—i ngÆ°á»i gÃ¡n nhÃ£n xá»­ lÃ½ 100 trong sá»‘ chÃºng. Tháº£o luáº­n Ä‘Æ°á»£c phÃ©p trong quÃ¡ trÃ¬nh gÃ¡n nhÃ£n. Cuá»‘i cÃ¹ng, sá»‘ lÆ°á»£ng thá»ƒ hiá»‡n vá»›i nhÃ£n -1, 0 vÃ  1 láº§n lÆ°á»£t lÃ  254, 642 vÃ  422. VÃ¬ chÃºng tÃ´i quan tÃ¢m hÆ¡n Ä‘áº¿n viá»‡c khá»›p truy váº¥n-mÃ£, chÃºng tÃ´i chá»‰ bao gá»“m cÃ¡c danh má»¥c 0 vÃ  1 trong táº­p test cuá»‘i cÃ¹ng cá»§a chÃºng tÃ´i. CÃ¡c táº­p training vÃ  validation chÃºng tÃ´i sá»­ dá»¥ng cho tÃ¡c vá»¥ nÃ y lÃ  tá»« dataset CodeSearchNet gá»‘c [35].

3.7 Code repair
Code repair nháº±m sá»­a lá»—i trong mÃ£ má»™t cÃ¡ch tá»± Ä‘á»™ng. ChÃºng tÃ´i sá»­ dá»¥ng dataset Ä‘Æ°á»£c phÃ¡t hÃ nh bá»Ÿi Tufano et al. [75]. Nguá»“n lÃ  cÃ¡c hÃ m Java bá»‹ lá»—i, trong khi Ä‘Ã­ch lÃ  cÃ¡c hÃ m Ä‘Æ°á»£c sá»­a tÆ°Æ¡ng á»©ng. Äá»ƒ xÃ¢y dá»±ng dataset nÃ y, Ä‘áº§u tiÃªn há» táº£i xuá»‘ng má»i sá»± kiá»‡n GitHub cÃ´ng khai

--- TRANG 6 ---
Lu, Guo, Ren vÃ  Huang, et al.

python Ä‘o khoáº£ng cÃ¡ch giá»¯a 2 Ä‘iá»ƒm Truy váº¥n:
Code:defvector_distance (a, b):
""" Khoáº£ng cÃ¡ch Euclidean giá»¯a hai vector """
a = np.array (a)
b = np.array (b)
return np.linalg.norm (a -b)
NhÃ£n: 1

cÃ¡ch thÃªm object vÃ o má»™t chá»‰ sá»‘ cá»¥ thá»ƒ trong list python Truy váº¥n:
Code:defappend(self, item):           
""" thÃªm item vÃ  in nÃ³ ra stdout """           
print(item)           
super( MyList , self).append(item)
NhÃ£n: 0

HÃ¬nh 5: Hai vÃ­ dá»¥ trong dataset WebQueryTest.

giá»¯a thÃ¡ng 3 nÄƒm 2011 vÃ  thÃ¡ng 10 nÄƒm 2017 tá»« GitHub ArchiveÂ¹Â¹ vÃ  sá»­ dá»¥ng API Google BigQuery Ä‘á»ƒ xÃ¡c Ä‘á»‹nh táº¥t cáº£ cÃ¡c commit file Java cÃ³ thÃ´ng Ä‘iá»‡p chá»©a cÃ¡c máº«u [21]: ("fix" hoáº·c "solve") vÃ  ("bug" hoáº·c "issue" hoáº·c "problem" hoáº·c "error"). Cho má»—i commit sá»­a lá»—i, há» trÃ­ch xuáº¥t mÃ£ nguá»“n trÆ°á»›c vÃ  sau quÃ¡ trÃ¬nh sá»­a báº±ng cÃ¡ch sá»­ dá»¥ng GitHub Compare APIÂ¹Â² Ä‘á»ƒ thu tháº­p cÃ¡c mÃ£ bá»‹ lá»—i (pre-commit) vÃ  Ä‘Ã£ sá»­a (post-commit). Sau Ä‘Ã³, há» chuáº©n hÃ³a táº¥t cáº£ tÃªn cá»§a cÃ¡c biáº¿n vÃ  phÆ°Æ¡ng thá»©c tÃ¹y chá»‰nh, Ä‘iá»u nÃ y háº¡n cháº¿ Ä‘Ã¡ng ká»ƒ kÃ­ch thÆ°á»›c tá»« vá»±ng vÃ  cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o viá»‡c há»c cÃ¡c máº«u sá»­a lá»—i. Sau Ä‘Ã³, há» lá»c ra cÃ¡c cáº·p chá»©a lá»—i tá»« vá»±ng hoáº·c cÃº phÃ¡p trong mÃ£ bá»‹ lá»—i hoáº·c Ä‘Ã£ sá»­a, cÅ©ng nhÆ° cÃ¡c cáº·p cÃ³ hÆ¡n 100 hÃ nh Ä‘á»™ng sá»­a Ä‘á»•i AST nguyÃªn tá»­ giá»¯a phiÃªn báº£n bá»‹ lá»—i vÃ  Ä‘Ã£ sá»­a. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, há» sá»­ dá»¥ng cÃ´ng cá»¥ GumTree Spoon AST Diff [17]. Cuá»‘i cÃ¹ng, há» chia toÃ n bá»™ dataset thÃ nh hai táº­p con (small vá»›i tokenâ‰¤50 vÃ  medium vá»›i token >50 vÃ  â‰¤100) dá»±a trÃªn Ä‘á»™ dÃ i mÃ£. Cho táº­p con small, sá»‘ lÆ°á»£ng máº«u training, development vÃ  test láº§n lÆ°á»£t lÃ  46,680, 5,835 vÃ  5,835. Cho táº­p con medium, cÃ¡c sá»‘ lÃ  52,364, 6,545 vÃ  6,545 tÆ°Æ¡ng á»©ng.

3.8 Text-to-code generation
Äá»ƒ thá»±c hiá»‡n tÃ¡c vá»¥ nÃ y, chÃºng tÃ´i sá»­ dá»¥ng CONCODE [38], má»™t dataset sinh mÃ£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i, Ä‘Æ°á»£c thu tháº­p tá»« khoáº£ng 33,000 dá»± Ã¡n Java trÃªn GitHub. NÃ³ chá»©a 100,000 vÃ­ dá»¥ cho training vÃ  4,000 vÃ­ dá»¥ cho validation vÃ  testing. Má»—i vÃ­ dá»¥ lÃ  má»™t tuple bao gá»“m mÃ´ táº£ NL, mÃ´i trÆ°á»ng mÃ£ vÃ  Ä‘oáº¡n mÃ£. Dataset Ä‘Æ°á»£c giao nhiá»‡m vá»¥ sinh cÃ¡c hÃ m thÃ nh viÃªn lá»›p tá»« mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn (comment phÆ°Æ¡ng thá»©c kiá»ƒu Javadoc) vÃ  mÃ´i trÆ°á»ng lá»›p. MÃ´i trÆ°á»ng lá»›p lÃ  ngá»¯ cáº£nh láº­p trÃ¬nh Ä‘Æ°á»£c cung cáº¥p bá»Ÿi pháº§n cÃ²n láº¡i cá»§a lá»›p, bao gá»“m cÃ¡c biáº¿n thÃ nh viÃªn vÃ  hÃ m thÃ nh viÃªn khÃ¡c trong lá»›p.

3.9 Code summarization
Cho tÃ³m táº¯t mÃ£, chÃºng tÃ´i sá»­ dá»¥ng dataset CodeSearchNet [35], bao gá»“m sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh; tá»©c lÃ  Python, Java, JavaScript,

Â¹Â¹ https://www.gharchive.org/
Â¹Â² https://developer.github.com/v3/repos/commits/#compare-two-commits

PHP, Ruby vÃ  Go. Dá»¯ liá»‡u Ä‘áº¿n tá»« cÃ¡c repository GitHub mÃ£ nguá»“n má»Ÿ khÃ´ng fork cÃ³ sáºµn cÃ´ng khai vÃ  má»—i tÃ i liá»‡u lÃ  Ä‘oáº¡n Ä‘áº§u tiÃªn. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng má»™t sá»‘ tÃ i liá»‡u chá»©a ná»™i dung khÃ´ng liÃªn quan Ä‘áº¿n hÃ m, cháº³ng háº¡n nhÆ° liÃªn káº¿t "http://..." tham chiáº¿u Ä‘áº¿n tÃ i nguyÃªn bÃªn ngoÃ i vÃ  tháº» hÃ¬nh áº£nh HTML "<img ...>" chÃ¨n hÃ¬nh áº£nh. Do Ä‘Ã³, chÃºng tÃ´i lá»c dataset Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng cá»§a nÃ³ vá»›i cÃ¹ng bá»‘n quy táº¯c Ä‘Æ°á»£c Ä‘á» cáº­p trong Pháº§n 3.6.

Thá»‘ng kÃª vá» dataset CodeSearchNet Ä‘Æ°á»£c lá»c Ä‘Æ°á»£c sá»­ dá»¥ng trong CodeXGLUE Ä‘Æ°á»£c liá»‡t kÃª trong Báº£ng 3.

Báº£ng 3: Thá»‘ng kÃª dá»¯ liá»‡u vá» dataset CodeSearchNet Ä‘Æ°á»£c lá»c cho tÃ¡c vá»¥ tÃ³m táº¯t mÃ£.

NgÃ´n ngá»¯ | Training | Dev | Testing
Go | 167,288 | 7,325 | 8,122
Java | 164,923 | 5,183 | 10,955
JavaScript | 58,025 | 3,885 | 3,291
PHP | 241,241 | 12,982 | 14,014
Python | 251,820 | 13,914 | 14,918
Ruby | 24,927 | 1,400 | 1,261

3.10 Documentation translation
Documentation translation nháº±m dá»‹ch tÃ i liá»‡u mÃ£ tá»± Ä‘á»™ng tá»« má»™t ngÃ´n ngá»¯ tá»± nhiÃªn (vÃ­ dá»¥: tiáº¿ng Anh) sang ngÃ´n ngá»¯ tá»± nhiÃªn khÃ¡c (vÃ­ dá»¥: tiáº¿ng Trung), nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 7. Dataset chÃºng tÃ´i sá»­ dá»¥ng Ä‘Æ°á»£c thu tháº­p tá»« Microsoft DocumentationÂ¹Â³, bao gá»“m cÃ¡c tÃ i liá»‡u mÃ´ táº£ pháº§n má»m vÃ  mÃ£ trong cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau. ChÃºng tÃ´i táº­p trung vÃ o cÃ¡c cáº·p ngÃ´n ngá»¯ Ã­t tÃ i nguyÃªn, nÆ¡i dá»¯ liá»‡u song song khan hiáº¿m, vÃ  giá»›i thiá»‡u cÃ¡c tÃ¡c vá»¥ dá»‹ch mÃ¡y Ä‘a ngÃ´n ngá»¯, vÃ­ dá»¥: English â‡”Latvian, Danish, Norwegian vÃ  Chinese. Äá»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»¯ liá»‡u, chÃºng tÃ´i lá»c corpus báº±ng cÃ¡ch loáº¡i bá» cÃ¡c vÃ­ dá»¥ sau.

(1) CÃ¡c cáº·p cÃ³ cÃ¢u nguá»“n giá»‘ng vá»›i cÃ¢u Ä‘Ã­ch;
(2) CÃ¡c cáº·p cÃ³ Ä‘á»™ dÃ i ngÃ´n ngá»¯ nguá»“n hoáº·c ngÃ´n ngá»¯ Ä‘Ã­ch Ã­t hÆ¡n ba tá»«;
(3) CÃ¡c cáº·p cÃ³ tá»· lá»‡ Ä‘á»™ dÃ i giá»¯a ngÃ´n ngá»¯ nguá»“n vÃ  Ä‘Ã­ch lá»›n hÆ¡n ba;
(4) CÃ¡c cáº·p cÃ³ tá»· lá»‡ cÄƒn chá»‰nh tá»« Ä‘Æ°á»£c tÃ­nh bá»Ÿi fast_alignÂ¹â´ Ã­t hÆ¡n 0.6.

Dá»¯ liá»‡u training cuá»‘i cÃ¹ng bao gá»“m 43K, 19K, 44K vÃ  50K cáº·p cÃ¢u cho Englishâ‡”Latvian, Englishâ‡”Danish, Englishâ‡”Norwegian vÃ  English â‡”Chinese tÆ°Æ¡ng á»©ng. NgoÃ i ra, má»—i cáº·p ngÃ´n ngá»¯ cÃ³ 1K cáº·p cÃ¢u development vÃ  test tÆ°Æ¡ng á»©ng.

4 CÃC Há»† THá»NG BASELINE
ChÃºng tÃ´i cung cáº¥p ba loáº¡i mÃ´ hÃ¬nh baseline Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³, bao gá»“m má»™t mÃ´ hÃ¬nh pretrained kiá»ƒu BERT (trong trÆ°á»ng há»£p nÃ y lÃ  CodeBERT), há»— trá»£ cÃ¡c váº¥n Ä‘á» hiá»ƒu chÆ°Æ¡ng trÃ¬nh, má»™t mÃ´ hÃ¬nh pretrained kiá»ƒu GPT Ä‘Æ°á»£c gá»i lÃ  CodeGPT giÃºp chÃºng ta giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» hoÃ n thÃ nh vÃ  sinh, vÃ  má»™t

Â¹Â³ https://docs.microsoft.com/, cÃ³ tÃ i liá»‡u Ä‘Æ°á»£c Ä‘áº·t táº¡i https://github.com/MicrosoftDocs/.
Â¹â´ https://github.com/clab/fast_align.

--- TRANG 7 ---
CodeXGLUE: Má»™t Benchmark Dataset cho Machine Learning trong viá»‡c Hiá»ƒu vÃ  Sinh Code

Hiá»ƒu | Sinh
CÃ¡c tÃ¡c vá»¥ Ä‘Æ°á»£c há»— trá»£: | CÃ¡c tÃ¡c vá»¥ Ä‘Æ°á»£c há»— trá»£:
â€¢ tÃ¬m kiáº¿m mÃ£ | â€¢ hoÃ n thÃ nh mÃ£
â€¢ phÃ¡t hiá»‡n clone mÃ£ | â€¢ sinh mÃ£

CÃ¡c tÃ¡c vá»¥ Ä‘Æ°á»£c há»— trá»£:
â€¢ sá»­a mÃ£
â€¢ dá»‹ch mÃ£

Token mÃ£ trÆ°á»›c | Token mÃ£ tiáº¿p theo
CodeGPT Decoder

MÃ£ Ä‘áº§u vÃ o | MÃ£ Ä‘áº§u ra
CodeBERT
[SEP] [CLS] text/code code [SEP]
FFNN + Softmax
CodeBERT
0 1 PhÃ¢n phá»‘i danh má»¥c

Token Ä‘áº§u vÃ o

HÃ¬nh 6: Ba pipeline, bao gá»“m CodeBERT, CodeGPT vÃ  Encoder-Decoder, Ä‘Æ°á»£c cung cáº¥p.

Äáº§u vÃ o (tiáº¿ng Anh):
Multinomial Logistic Regression (Softmax regression) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n xÃ¡c suáº¥t cá»§a má»™t sá»‘ káº¿t quáº£ cÃ³ thá»ƒ trong cÃ¡c váº¥n Ä‘á» phÃ¢n loáº¡i.

Äáº§u ra (tiáº¿ng Trung):
å¤šé¡¹å¼é€»è¾‘å›å½’ ï¼ˆSoftmaxå›å½’ï¼‰ç”¨äºè®¡ç®—åˆ†ç±»é—®é¢˜ä¸­å‡ ç§å¯èƒ½ç»“æœçš„æ¦‚ç‡ã€‚

HÃ¬nh 7: Má»™t vÃ­ dá»¥ tiáº¿ng Anh sang tiáº¿ng Trung trong dataset dá»‹ch tÃ i liá»‡u.

framework Encoder-Decoder giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» sinh sequence-to-sequence. Má»™t minh há»a cá»§a ba pipeline nÃ y Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6.

4.1 CodeBERT
Äá»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ hiá»ƒu mÃ£ nhÆ° phÃ¡t hiá»‡n clone, phÃ¡t hiá»‡n lá»—i, cloze test vÃ  tÃ¬m kiáº¿m mÃ£, chÃºng tÃ´i sá»­ dá»¥ng CodeBERT [18] lÃ m encoder cá»§a chÃºng tÃ´i. ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh pretrained bimodal dá»±a trÃªn Transformer vá»›i 12 táº§ng, 768 dimensional hidden state vÃ  12 attention head cho ngÃ´n ngá»¯ láº­p trÃ¬nh (PL) vÃ  ngÃ´n ngá»¯ tá»± nhiÃªn (NL). Feng et al. [18] pretrain CodeBERT báº±ng cÃ¡c má»¥c tiÃªu masked language modeling vÃ  replaced token detection trÃªn dataset CodeSearchNet [35], bao gá»“m 2.4M hÃ m vá»›i cÃ¡c cáº·p tÃ i liá»‡u cho sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh. MÃ´ hÃ¬nh há»— trá»£ cÃ¡c loáº¡i Ä‘áº§u vÃ o sequence khÃ¡c nhau nhÆ° text/code vÃ  code/code vá»›i má»™t token Ä‘áº·c biá»‡t [ğ¶ğ¿ğ‘†] á»Ÿ phÃ­a trÆ°á»›c cá»§a sequence vÃ  má»™t kÃ½ hiá»‡u Ä‘áº·c biá»‡t [ğ‘†ğ¸ğ‘ƒ] Ä‘á»ƒ chia hai loáº¡i dá»¯ liá»‡u.

MÃ´ hÃ¬nh cÃ³ sáºµn cÃ´ng khai táº¡i https://huggingface.co/microsoft/codebert-base.

4.2 CodeGPT
ChÃºng tÃ´i cung cáº¥p CodeGPT, lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»±a trÃªn Transformer Ä‘Æ°á»£c pretrain trÃªn ngÃ´n ngá»¯ láº­p trÃ¬nh (PL), Ä‘á»ƒ há»— trá»£ cÃ¡c tÃ¡c vá»¥ hoÃ n thÃ nh mÃ£ vÃ  sinh mÃ£ tá»« text. CodeGPT cÃ³ cÃ¹ng kiáº¿n trÃºc mÃ´ hÃ¬nh vÃ  má»¥c tiÃªu training cá»§a GPT-2 [59], bao gá»“m 12 táº§ng Transformer decoder. CÃ¡c thiáº¿t láº­p mÃ´ hÃ¬nh khÃ¡c Ä‘Æ°á»£c liá»‡t kÃª trong Báº£ng 4. ChÃºng tÃ´i pretrain cÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n ngÃ´n ngá»¯ trÃªn cÃ¡c corpus Python vÃ  Java tá»« dataset CodeSearchNet [35], bao gá»“m 1.1M hÃ m Python vÃ  1.6M phÆ°Æ¡ng thá»©c Java. Má»—i hÃ m trong dataset training cÃ³ chá»¯ kÃ½ hÃ m vÃ  thÃ¢n hÃ m. Má»™t sá»‘ hÃ m cÅ©ng chá»©a tÃ i liá»‡u ngÃ´n ngá»¯ tá»± nhiÃªn.

ChÃºng tÃ´i Ä‘Ã o táº¡o hai mÃ´ hÃ¬nh CodeGPT cho má»—i ngÃ´n ngá»¯ láº­p trÃ¬nh. Má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c pretrain tá»« Ä‘áº§u, Ä‘á»ƒ tá»« vá»±ng BPE (byte pair encoder) [67] Ä‘Æ°á»£c láº¥y má»›i trÃªn corpus mÃ£ vÃ  cÃ¡c tham sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn. MÃ´ hÃ¬nh khÃ¡c lÃ  má»™t mÃ´ hÃ¬nh domain-adaptive, sá»­ dá»¥ng mÃ´ hÃ¬nh GPT-2 lÃ m Ä‘iá»ƒm khá»Ÿi Ä‘áº§u vÃ  Ä‘Æ°á»£c Ä‘Ã o táº¡o liÃªn tá»¥c trÃªn corpus mÃ£. Káº¿t quáº£ lÃ , mÃ´ hÃ¬nh thá»© hai cÃ³ cÃ¹ng tá»« vá»±ng GPT-2 vÃ  kháº£ nÄƒng hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn. ChÃºng tÃ´i gá»i mÃ´ hÃ¬nh nÃ y lÃ  CodeGPT-adapted, vÃ  coi nÃ³ lÃ  mÃ´ hÃ¬nh máº·c Ä‘á»‹nh cho cÃ¡c tÃ¡c vá»¥ hoÃ n thÃ nh mÃ£ vÃ  sinh mÃ£ tá»« text. Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u cÃ³ sáºµn cÃ´ng khai táº¡i https://huggingface.co/microsoft/CodeGPT-small-java vÃ  https://huggingface.co/microsoft/CodeGPT-small-java-adaptedGPT2.Â¹âµ

Â¹âµ Thay tháº¿ "java" báº±ng "py" cho cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c pre-train trÃªn dataset python.

--- TRANG 8 ---
Lu, Guo, Ren vÃ  Huang, et al.

Báº£ng 4: Tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh CodeBERT vÃ  CodeGPT.

| | CodeBERT | CodeGPT |
|---|---|---|
| Sá»‘ lÆ°á»£ng táº§ng | 12 | 12 |
| Äá»™ dÃ i tá»‘i Ä‘a cá»§a position | 512 | 1,024 |
| KÃ­ch thÆ°á»›c embedding | 768 | 768 |
| Attention head | 12 | 12 |
| KÃ­ch thÆ°á»›c attention head | 64 | 64 |
| KÃ­ch thÆ°á»›c tá»« vá»±ng | 50,265 | 50,000 |
| Tá»•ng sá»‘ tham sá»‘ | 125M | 124M |

4.3 Encoder-Decoder
Cho cÃ¡c váº¥n Ä‘á» sinh sequence-to-sequence nhÆ° sá»­a mÃ£, dá»‹ch mÃ£, tÃ³m táº¯t mÃ£ vÃ  dá»‹ch tÃ i liá»‡u, chÃºng tÃ´i cung cáº¥p má»™t framework Encoder-Decoder. ChÃºng tÃ´i khá»Ÿi táº¡o encoder báº±ng CodeBERT [18] vÃ  sá»­ dá»¥ng má»™t Transformer Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn vá»›i 6 táº§ng, 768 dimensional hidden state vÃ  12 attention head lÃ m decoder trong táº¥t cáº£ cÃ¡c thiáº¿t láº­p.

5 THá»°C NGHIá»†M
Trong pháº§n nÃ y, chÃºng tÃ´i bÃ¡o cÃ¡o sá»‘ liá»‡u Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c há»‡ thá»‘ng baseline trÃªn 10 tÃ¡c vá»¥. ChÃºng tÃ´i cÅ©ng sáº½ hiá»ƒn thá»‹ thá»i gian cáº§n thiáº¿t Ä‘á»ƒ Ä‘Ã o táº¡o mÃ´ hÃ¬nh vÃ  thá»±c hiá»‡n suy luáº­n trÃªn mÃ´ hÃ¬nh.

5.1 Clone Detection
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c dataset BigCloneBench vÃ  POJ-104 cho phÃ¡t hiá»‡n clone. TÃ¡c vá»¥ cá»§a dataset BigCloneBench Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° má»™t phÃ¢n loáº¡i nhá»‹ phÃ¢n Ä‘á»ƒ dá»± Ä‘oÃ¡n liá»‡u má»™t cáº·p mÃ£ cho trÆ°á»›c cÃ³ cÃ¹ng ngá»¯ nghÄ©a hay khÃ´ng, vá»›i Ä‘iá»ƒm F1 Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m metric Ä‘Ã¡nh giÃ¡. TÃ¡c vá»¥ cá»§a dataset POJ-104 nháº±m truy xuáº¥t 499 mÃ£ cho má»™t mÃ£ cho trÆ°á»›c tá»« táº­p development/test cho validation/testing, vá»›i Mean Average Precision (MAP) lÃ m metric Ä‘Ã¡nh giÃ¡. Äiá»ƒm tá»•ng thá»ƒ cá»§a tÃ¡c vá»¥ phÃ¡t hiá»‡n clone lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a Ä‘iá»ƒm F1 vÃ  MAP.

Báº£ng 5: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ phÃ¡t hiá»‡n clone.

| MÃ´ hÃ¬nh | BigCloneBench F1 | POJ-104 MAP | Tá»•ng thá»ƒ |
|---|---|---|---|
| RtvNN | 1.0 | - | - |
| Deckard | 3.0 | - | - |
| CDLH | 82.0 | - | - |
| ASTNN | 93.0 | - | - |
| FA-AST-GMN | 95.0 | - | - |
| TBCCD | 95.0 | - | - |
| code2vec* | - | 1.98 | - |
| NCC* | - | 54.19 | - |
| Aroma* | - | 55.12 | - |
| MISIM-GNN* | - | 82.45 | - |
| RoBERTa | 94.9 | 79.96 | 87.4 |
| CodeBERT | 96.5 | 84.29 | 90.4 |

Káº¿t quáº£. Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 5. RtvNN [89] Ä‘Ã o táº¡o má»™t recursive autoencoder Ä‘á»ƒ há»c biá»ƒu diá»…n cho AST. Deckard [39] tÃ­nh toÃ¡n vector cho thÃ´ng tin cáº¥u trÃºc trong AST vÃ  sá»­ dá»¥ng Locality Sensitive Hashing (LSH) [14] Ä‘á»ƒ phÃ¢n cá»¥m cÃ¡c vector tÆ°Æ¡ng tá»±. CDLH [88] há»c biá»ƒu diá»…n cá»§a cÃ¡c Ä‘oáº¡n mÃ£ thÃ´ng qua LSTM dá»±a trÃªn AST. ASTNN [97] sá»­ dá»¥ng RNN Ä‘á»ƒ mÃ£ hÃ³a cÃ¡c subtree AST cho cÃ¡c cÃ¢u lá»‡nh. NÃ³ Ä‘Æ°a cÃ¡c mÃ£ hÃ³a cá»§a táº¥t cáº£ cÃ¡c cÃ¢y cÃ¢u lá»‡nh vÃ o má»™t RNN Ä‘á»ƒ há»c biá»ƒu diá»…n cho má»™t chÆ°Æ¡ng trÃ¬nh. FA-AST-GMN [84] sá»­ dá»¥ng GNN trÃªn má»™t AST Ä‘Æ°á»£c tÄƒng cÆ°á»ng flow Ä‘á»ƒ táº­n dá»¥ng thÃ´ng tin control vÃ  data flow rÃµ rÃ ng. TBCCD [96] Ä‘á» xuáº¥t má»™t position-aware character embedding vÃ  sá»­ dá»¥ng tree-based convolution Ä‘á»ƒ náº¯m báº¯t cáº£ thÃ´ng tin cáº¥u trÃºc cá»§a má»™t Ä‘oáº¡n mÃ£ tá»« AST cá»§a nÃ³ vÃ  thÃ´ng tin tá»« vá»±ng tá»« cÃ¡c token mÃ£. Code2vec [6] há»c biá»ƒu diá»…n cá»§a cÃ¡c Ä‘oáº¡n mÃ£ báº±ng cÃ¡ch tá»•ng há»£p nhiá»u Ä‘Æ°á»ng dáº«n cÃº phÃ¡p thÃ nh má»™t vector duy nháº¥t. NCC [7] mÃ£ hÃ³a cÃ¡c chÆ°Æ¡ng trÃ¬nh báº±ng cÃ¡ch táº­n dá»¥ng cáº£ data flow vÃ  control flow cÆ¡ báº£n cá»§a cÃ¡c chÆ°Æ¡ng trÃ¬nh. Aroma [51] lÃ  má»™t cÃ´ng cá»¥ gá»£i Ã½ mÃ£ láº¥y má»™t Ä‘oáº¡n mÃ£ má»™t pháº§n vÃ  gá»£i Ã½ má»™t táº­p nhá» cÃ¡c Ä‘oáº¡n mÃ£ ngáº¯n gá»n chá»©a Ä‘oáº¡n truy váº¥n. MISIM-GNN [93] há»c má»™t biá»ƒu diá»…n cáº¥u trÃºc cá»§a mÃ£ tá»« cáº¥u trÃºc ngá»¯ nghÄ©a nháº­n biáº¿t ngá»¯ cáº£nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ nÃ¢ng Ã½ nghÄ©a ngá»¯ nghÄ©a tá»« cÃº phÃ¡p mÃ£.

Trong thÃ­ nghiá»‡m nÃ y, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh pretrained nhÆ° RoBERTa [50] vÃ  CodeBERT [18] Ä‘á»ƒ mÃ£ hÃ³a mÃ£ nguá»“n vÃ  láº¥y biá»ƒu diá»…n Ä‘á»ƒ tÃ­nh toÃ¡n má»©c Ä‘á»™ liÃªn quan ngá»¯ nghÄ©a cá»§a hai mÃ£ thÃ´ng qua má»™t máº¡ng feed forward hoáº·c tÃ­ch trong. Máº·c dÃ¹ CodeBERT khÃ´ng táº­n dá»¥ng cáº¥u trÃºc mÃ£ Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  hiá»‡u quáº£ trong viá»‡c Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± mÃ£ [7,84,88,93,97], mÃ´ hÃ¬nh váº«n hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n RoBERTa trÃªn tÃ¡c vá»¥ phÃ¡t hiá»‡n clone, Ä‘áº¡t Ä‘iá»ƒm tá»•ng thá»ƒ 90.4. Nhá»¯ng káº¿t quáº£ thÃ­ nghiá»‡m nÃ y chá»©ng minh ráº±ng pretraining há»¯u Ã­ch cho phÃ¡t hiá»‡n clone. Váº«n cÃ²n chá»— Ä‘á»ƒ cáº£i thiá»‡n thÃªm náº¿u cáº¥u trÃºc mÃ£ Ä‘Æ°á»£c táº­n dá»¥ng thÃªm.

5.2 Defect Detection
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng dataset Ä‘Æ°á»£c Ä‘á» cáº­p trong Pháº§n 3.2 cho phÃ¡t hiá»‡n lá»—i, nháº±m dá»± Ä‘oÃ¡n liá»‡u má»™t mÃ£ nguá»“n cÃ³ chá»©a lá»—i cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¥n cÃ´ng cÃ¡c há»‡ thá»‘ng pháº§n má»m hay khÃ´ng. Metric Ä‘Ã¡nh giÃ¡ lÃ  Ä‘iá»ƒm accuracy. ChÃºng tÃ´i sá»­ dá»¥ng baseline CodeBERT Ä‘á»ƒ mÃ£ hÃ³a mÃ£ nguá»“n vÃ  láº¥y biá»ƒu diá»…n cá»§a mÃ£ nguá»“n Ä‘á»ƒ tÃ­nh toÃ¡n xÃ¡c suáº¥t bá»‹ phÆ¡i bÃ y vá»›i cÃ¡c lá»— há»•ng.

Káº¿t quáº£. Báº£ng 7 hiá»ƒn thá»‹ káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh chÃºng tÃ´i triá»ƒn khai. ChÃºng tÃ´i sá»­ dá»¥ng Bidirectional LTSM (BiLTSM) [32], TextCNN [43], RoBERTa [50] vÃ  CodeBERT [18] Ä‘á»ƒ mÃ£ hÃ³a biá»ƒu diá»…n cá»§a má»™t mÃ£ nguá»“n tÆ°Æ¡ng á»©ng. Sau Ä‘Ã³, má»™t máº¡ng feed forward hai táº§ng Ä‘Æ°á»£c theo sau bá»Ÿi má»™t táº§ng softmax Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n xÃ¡c suáº¥t gáº·p cÃ¡c lá»— há»•ng. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong káº¿t quáº£, CodeBERT Ä‘áº¡t Ä‘iá»ƒm accuracy 62.1, dáº«n Ä‘áº¿n hiá»‡u suáº¥t state-of-the-art. Tuy nhiÃªn, cáº£i thiá»‡n Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi cÃ¡c mÃ´ hÃ¬nh pretrained bá»‹ háº¡n cháº¿ so vá»›i TextCNN. Má»™t hÆ°á»›ng tiá»m nÄƒng Ä‘á»ƒ cáº£i thiá»‡n cÃ¡c mÃ´ hÃ¬nh pretrained nÃ y lÃ  káº¿t há»£p thÃ´ng tin tá»« cÃ¡c cáº¥u trÃºc mÃ£ nhÆ° Abstract Syntax Tree, data flow, control flow, v.v.

5.3 Cloze test
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c dataset CT-all vÃ  CT-maxmin cho tÃ¡c vá»¥ cloze test. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c ká»³ vá»ng dá»± Ä‘oÃ¡n token mÃ£ bá»‹ che báº±ng cÃ¡ch táº­n dá»¥ng tÃ i liá»‡u vÃ  ngá»¯ cáº£nh cá»§a mÃ£. Accuracy

--- TRANG 9 ---
CodeXGLUE: Má»™t Benchmark Dataset cho Machine Learning trong viá»‡c Hiá»ƒu vÃ  Sinh Code

Báº£ng 6: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ cloze test.

| MÃ´ hÃ¬nh | CT-all | | | | | | CT-maxmin | | | | | | Tá»•ng thá»ƒ |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | Ruby | JS | Go | Python | Java | PHP | Ruby | JS | Go | Python | Java | PHP | |
| RoBERTa | 47.44 | 59.96 | 40.77 | 54.35 | 50.73 | 60.16 | 73.68 | 64.71 | 71.71 | 59.18 | 59.75 | 69.78 | 62.45 |
| CodeBERT(MLM) | 80.17 | 81.77 | 83.31 | 87.21 | 80.63 | 85.05 | 86.84 | 86.40 | 90.79 | 82.20 | 90.46 | 88.21 | 85.66 |

Báº£ng 7: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ phÃ¡t hiá»‡n lá»—i.

| MÃ´ hÃ¬nh | Accuracy |
|---|---|
| BiLSTM | 59.37 |
| TextCNN | 60.69 |
| RoBERTa | 61.05 |
| CodeBERT | 62.08 |

Ä‘Æ°á»£c bÃ¡o cÃ¡o cho má»—i ngÃ´n ngá»¯, vá»›i Ä‘iá»ƒm accuracy trung bÃ¬nh macro cho táº¥t cáº£ ngÃ´n ngá»¯ lÃ m metric Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ.

Káº¿t quáº£. Báº£ng 6 hiá»ƒn thá»‹ káº¿t quáº£ trÃªn cÃ¡c dataset CT-all vÃ  CT-maxmin. ChÃºng tÃ´i bÃ¡o cÃ¡o hiá»‡u suáº¥t cá»§a RoBERTa [50] vÃ  CodeBERT (Masked Language Modeling, MLM) [18], Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i RoBERTa vÃ  Ä‘Æ°á»£c Ä‘Ã o táº¡o thÃªm vá»›i má»¥c tiÃªu masked language modeling. Káº¿t quáº£ chá»©ng minh ráº±ng CodeBERT hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n RoBERTa chá»‰ há»c tá»« ngÃ´n ngá»¯ tá»± nhiÃªn.

5.4 Code completion
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c dataset PY150 vÃ  Github Java Corpus cho cÃ¡c tÃ¡c vá»¥ hoÃ n thÃ nh mÃ£ cáº¥p token vÃ  cáº¥p dÃ²ng. TÃ¡c vá»¥ cáº¥p token lÃ  dá»± Ä‘oÃ¡n token tiáº¿p theo cho ngá»¯ cáº£nh cá»§a cÃ¡c token trÆ°á»›c Ä‘Ã³, vÃ  cÃ¡c dá»± Ä‘oÃ¡n Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ theo accuracy cáº¥p token; trong khi tÃ¡c vá»¥ cáº¥p dÃ²ng bao gá»“m viá»‡c hoÃ n thÃ nh má»™t dÃ²ng mÃ£ hoÃ n chá»‰nh, vÃ  cháº¥t lÆ°á»£ng cá»§a mÃ£ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ thÃ´ng qua cÃ¡c metric Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  exact match accuracy vÃ  Levenshtein edit similarity [72]. Levenshtein edit similarity Ä‘o lÆ°á»ng cÃ³ bao nhiÃªu chá»‰nh sá»­a kÃ½ tá»± Ä‘Æ¡n cáº§n thiáº¿t Ä‘á»ƒ biáº¿n Ä‘á»•i má»™t chuá»—i thÃ nh chuá»—i khÃ¡c. ÄÃ¢y lÃ  má»™t metric Ä‘Ã¡nh giÃ¡ quan trá»ng cho tÃ¬nh huá»‘ng hoÃ n thÃ nh mÃ£ vÃ¬ nÃ³ Ä‘o lÆ°á»ng bao nhiÃªu ná»— lá»±c cáº§n thiáº¿t Ä‘á»ƒ cÃ¡c láº­p trÃ¬nh viÃªn sá»­a lá»—i trong mÃ£. Äiá»ƒm trÃªn má»—i dataset lÃ  giÃ¡ trá»‹ trung bÃ¬nh cá»§a accuracy trÃªn hoÃ n thÃ nh cáº¥p token vÃ  edit similarity trÃªn hoÃ n thÃ nh cáº¥p dÃ²ng. Äiá»ƒm tá»•ng thá»ƒ cá»§a tÃ¡c vá»¥ hoÃ n thÃ nh mÃ£ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch láº¥y trung bÃ¬nh Ä‘iá»ƒm trÃªn cáº£ hai dataset.

Káº¿t quáº£. Báº£ng 8 hiá»ƒn thá»‹ káº¿t quáº£ cá»§a táº¥t cáº£ mÃ´ hÃ¬nh trÃªn cáº£ hai dataset. ChÃºng tÃ´i fine-tune LSTM [32], Transformer [77], GPT-2 [59], CodeGPT vÃ  CodeGPT-adapted Ä‘á»ƒ sinh cÃ¡c token tiáº¿p theo. CÃ¡c mÃ´ hÃ¬nh CodeGPT vÃ  CodeGPT-adapted Ä‘Æ°á»£c mÃ´ táº£ trong Pháº§n 4.2. CodeGPT-adapted Ä‘áº¡t hiá»‡u suáº¥t state-of-the-art vá»›i Ä‘iá»ƒm tá»•ng thá»ƒ 71.28.

5.5 Code search
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c dataset CodeSearchNet AdvTest vÃ  WebQueryTest Ä‘Æ°á»£c Ä‘á» cáº­p trong Pháº§n 3.6 cho tÃ¬m kiáº¿m mÃ£. Äá»ƒ cáº£i thiá»‡n hiá»‡u quáº£, chÃºng tÃ´i mÃ£ hÃ³a riÃªng biá»‡t vÄƒn báº£n vÃ  mÃ£ Ä‘á»ƒ thá»±c hiá»‡n tÃ¬m kiáº¿m mÃ£. Cho dataset CodeSearchNet AdvTest, tÃ¡c vá»¥ lÃ  tÃ¬m mÃ£ liÃªn quan nháº¥t tá»« má»™t táº­p há»£p cÃ¡c á»©ng viÃªn cho má»™t truy váº¥n vÃ  Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ thÃ´ng qua metric Mean Reciprocal Rank (MRR). Cho dataset WebQueryTest, tÃ¡c vá»¥ Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° má»™t phÃ¢n loáº¡i nhá»‹ phÃ¢n Ä‘á»ƒ dá»± Ä‘oÃ¡n liá»‡u má»™t mÃ£ cÃ³ thá»ƒ tráº£ lá»i má»™t truy váº¥n cho trÆ°á»›c hay khÃ´ng vÃ  chÃºng tÃ´i sá»­ dá»¥ng Ä‘iá»ƒm F1 vÃ  accuracy lÃ m metric Ä‘Ã¡nh giÃ¡. Äiá»ƒm tá»•ng thá»ƒ cho tÃ¬m kiáº¿m mÃ£ lÃ  trung bÃ¬nh cá»§a cÃ¡c giÃ¡ trá»‹ Ä‘Æ°á»£c ghi nháº­n cho hai subtask.

Káº¿t quáº£. Báº£ng 9 trÃ¬nh bÃ y káº¿t quáº£ trÃªn cÃ¡c dataset CodeSearchNet AdvTest vÃ  WebQueryTest. ChÃºng tÃ´i bÃ¡o cÃ¡o hiá»‡u suáº¥t cá»§a RoBERTa [50] vÃ  CodeBERT [18]. Báº£ng cho tháº¥y CodeBERT hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n RoBERTa.

5.6 Text-to-code generation
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng dataset CONCODE cho sinh mÃ£ tá»« text. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c ká»³ vá»ng sinh mÃ£ nguá»“n cá»§a cÃ¡c hÃ m thÃ nh viÃªn lá»›p Java, cho cÃ¡c mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  mÃ´i trÆ°á»ng lá»›p. ChÃºng tÃ´i bÃ¡o cÃ¡o exact match accuracy, Ä‘iá»ƒm BLEU [56] vÃ  Ä‘iá»ƒm CodeBLEU [65]. ChÃºng tÃ´i sá»­ dá»¥ng Ä‘iá»ƒm CodeBLEU lÃ m metric Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ.

Káº¿t quáº£. Báº£ng 10 trÃ¬nh bÃ y káº¿t quáº£ trÃªn táº­p test CONCODE. Seq2Seq [70] lÃ  má»™t mÃ´ hÃ¬nh sequence to sequence dá»±a trÃªn RNN. Seq2Action + MAML [26] káº¿t há»£p má»™t mÃ´ hÃ¬nh truy xuáº¥t nháº­n biáº¿t ngá»¯ cáº£nh vá»›i model-agnostic meta-learning (MAML). Iyer-Simp + 200 idioms [36] trÃ­ch xuáº¥t cÃ¡c idiom mÃ£ vÃ  Ã¡p dá»¥ng giáº£i mÃ£ dá»±a trÃªn idiom. ChÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh pretrained, bao gá»“m GPT-2 [59], CodeGPT vÃ  CodeGPT-adapted. CodeGPT-adapted Ä‘áº¡t Ä‘iá»ƒm CodeBLEU 35.98, dáº«n Ä‘áº¿n hiá»‡u suáº¥t state-of-the-art.

5.7 Code translation
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng dataset chÃºng tÃ´i xÃ¢y dá»±ng nhÆ° mÃ´ táº£ trong Pháº§n 3.5. Dataset chá»©a cÃ¡c máº«u khá»›p cá»§a cÃ¡c hÃ m Java vÃ  C#. ChÃºng tÃ´i bÃ¡o cÃ¡o exact match accuracy, Ä‘iá»ƒm BLEU [56] vÃ  Ä‘iá»ƒm CodeBLEU [65] trÃªn tÃ¡c vá»¥ nÃ y. CodeBLEU Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m metric Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ.

Káº¿t quáº£. Báº£ng 12 hiá»ƒn thá»‹ káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh trÃªn cáº£ hai hÆ°á»›ng dá»‹ch. PhÆ°Æ¡ng phÃ¡p Naive trá»±c tiáº¿p sao chÃ©p mÃ£ nguá»“n lÃ m káº¿t quáº£ dá»‹ch. PBSMT lÃ  viáº¿t táº¯t cá»§a phrase-based statistical machine translation [44]. Transformer sá»­ dá»¥ng cÃ¹ng sá»‘ lÆ°á»£ng táº§ng vÃ  kÃ­ch thÆ°á»›c áº©n nhÆ° cÃ¡c mÃ´ hÃ¬nh pretrained. Báº£ng cho tháº¥y Transformer Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i CodeBERT vÃ  Ä‘Æ°á»£c fine-tune vá»›i cÃ¡c cáº·p máº«u khá»›p táº¡o ra káº¿t quáº£ tá»‘t nháº¥t.

5.8 Code repair
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng dataset ban Ä‘áº§u Ä‘Æ°á»£c phÃ¡t hÃ nh bá»Ÿi Tufano et al. [75], Ä‘Æ°á»£c mÃ´ táº£ trong Pháº§n 3.7. Dataset chá»©a hai

--- TRANG 10 ---
Lu, Guo, Ren vÃ  Huang, et al.

Báº£ng 8: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ hoÃ n thÃ nh mÃ£.

| MÃ´ hÃ¬nh | PY150 | | Github Java Corpus | | Tá»•ng thá»ƒ |
|---|---|---|---|---|---|
| | token-level Accuracy | line-level EM | Edit Sim | token-level Accuracy | line-level EM | Edit Sim | |
| LSTM | 58.00 | 17.93 | 50.05 | 56.02 | 10.30 | 41.55 | 51.41 |
| Transformer | 73.26 | 36.65 | 67.51 | 64.16 | 15.33 | 50.39 | 63.83 |
| GPT-2 | 74.22 | 38.55 | 68.94 | 74.89 | 24.30 | 60.70 | 69.69 |
| CodeGPT | 74.93 | 39.11 | 69.69 | 76.45 | 25.30 | 61.54 | 70.65 |
| CodeGPT-adapted | 75.11 | 39.65 | 69.84 | 77.13 | 26.43 | 63.03 | 71.28 |

Báº£ng 9: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ tÃ¬m kiáº¿m mÃ£.

| MÃ´ hÃ¬nh | AdvTest MRR | WebQueryTest F1 | Accuracy | Tá»•ng thá»ƒ |
|---|---|---|---|---|
| RoBERTa | 18.33 | 57.49 | 40.92 | 33.63 |
| CodeBERT | 27.19 | 58.95 | 47.80 | 40.28 |

Báº£ng 10: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ sinh mÃ£ tá»« text.

| MÃ´ hÃ¬nh | EM | BLEU | CodeBLEU |
|---|---|---|---|
| Seq2Seq | 3.05 | 21.31 | 26.39 |
| Seq2Action+MAML | 10.05 | 24.40 | 29.46 |
| Iyer-Simp+200 idioms | 12.20 | 26.60 | - |
| GPT-2 | 17.35 | 25.37 | 29.69 |
| CodeGPT | 18.25 | 28.69 | 32.71 |
| CodeGPT-adapted | 20.10 | 32.79 | 35.98 |

táº­p con Ä‘Æ°á»£c thiáº¿t láº­p theo Ä‘á»™ dÃ i cá»§a cÃ¡c hÃ m Java: smallâ‰¤50 vÃ  50<mediumâ‰¤100. ChÃºng tÃ´i bÃ¡o cÃ¡o exact match accuracy, Ä‘iá»ƒm BLEU [56] vÃ  Ä‘iá»ƒm CodeBLEU [65] trÃªn tÃ¡c vá»¥ nÃ y. Exact match accuracy Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m metric Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ.

Káº¿t quáº£. PhÆ°Æ¡ng phÃ¡p Naive trá»±c tiáº¿p sao chÃ©p mÃ£ bá»‹ lá»—i lÃ m káº¿t quáº£ sá»­a chá»¯a. Äá»‘i vá»›i Transformer, chÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng sá»‘ lÆ°á»£ng táº§ng vÃ  kÃ­ch thÆ°á»›c áº©n nhÆ° cÃ¡c mÃ´ hÃ¬nh pretrained. Vá» phÆ°Æ¡ng phÃ¡p CodeBERT, chÃºng tÃ´i khá»Ÿi táº¡o Transformer encoder vá»›i mÃ´ hÃ¬nh CodeBERT pretrained vÃ  khá»Ÿi táº¡o ngáº«u nhiÃªn cÃ¡c tham sá»‘ cá»§a decoder vÃ  source-to-target attention. Sau Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng dá»¯ liá»‡u training Ä‘á»ƒ fine-tune toÃ n bá»™ mÃ´ hÃ¬nh. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong báº£ng, Transformer vá»›i khá»Ÿi táº¡o CodeBERT Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t trong táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh.

5.9 Code Summarization
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng dataset Ä‘Æ°á»£c Ä‘á» cáº­p trong Pháº§n 3.9 cho tÃ³m táº¯t mÃ£. Äá»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh, chÃºng tÃ´i theo Feng et al. [18], sá»­ dá»¥ng smoothed BLEU score [49] lÃ m metric Ä‘Ã¡nh giÃ¡, vÃ¬ Ä‘iá»u nÃ y phÃ¹ há»£p Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c tÃ i liá»‡u ngáº¯n. ChÃºng tÃ´i sá»­ dá»¥ng pipeline encoder-decoder Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y. Äá»™ dÃ i tá»‘i Ä‘a cá»§a input vÃ  inference Ä‘Æ°á»£c Ä‘áº·t lÃ  256 vÃ  128 tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i sá»­ dá»¥ng optimizer Adam Ä‘á»ƒ cáº­p nháº­t cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh. Learning rate vÃ  batch size láº§n lÆ°á»£t lÃ  5e-5 vÃ  32. ChÃºng tÃ´i Ä‘iá»u chá»‰nh cÃ¡c siÃªu tham sá»‘ vÃ  thá»±c hiá»‡n early stopping trÃªn táº­p development.

Káº¿t quáº£. Báº£ng 13 hiá»ƒn thá»‹ káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau trong tÃ³m táº¯t mÃ£. Seq2Seq lÃ  má»™t mÃ´ hÃ¬nh sequence to sequence dá»±a trÃªn RNN. Transformer vÃ  RoBERTa sá»­ dá»¥ng cÃ¹ng thiáº¿t láº­p nhÆ° CodeBERT, nhÆ°ng encoder Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn vÃ  bá»Ÿi RoBERTa [50] tÆ°Æ¡ng á»©ng. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng tá»« vá»±ng Byte Pair Encoding (BPE) [66]. Trong thÃ­ nghiá»‡m nÃ y, CodeBERT cÃ³ Ä‘Æ°á»£c cáº£i thiá»‡n 1.3% trong Ä‘iá»ƒm BLEU so vá»›i RoBERTa vÃ  Ä‘áº¡t hiá»‡u suáº¥t state-of-the-art trÃªn sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh.

5.10 Documentation translation
Thiáº¿t láº­p. ChÃºng tÃ´i sá»­ dá»¥ng dataset Microsoft Docs cho cÃ¡c tÃ¡c vá»¥ dá»‹ch text-to-text, táº­p trung vÃ o dá»‹ch Ä‘a ngÃ´n ngá»¯ Ã­t tÃ i nguyÃªn giá»¯a tiáº¿ng Anh (EN) vÃ  cÃ¡c ngÃ´n ngá»¯ khÃ¡c, bao gá»“m Latvian (LA), Danish (DA), Norwegian (NO) vÃ  Chinese (ZH). Theo Johnson et al. [40], chÃºng tÃ´i Ä‘Ã o táº¡o má»™t mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ duy nháº¥t lÃ m baseline. Äá»ƒ phÃ¢n biá»‡t giá»¯a cÃ¡c cáº·p dá»‹ch khÃ¡c nhau, chÃºng tÃ´i thÃªm má»™t language token (vÃ­ dá»¥: âŸ¨2enâŸ©, âŸ¨2zhâŸ©) á»Ÿ Ä‘áº§u cÃ¢u nguá»“n Ä‘á»ƒ chá»‰ ra ngÃ´n ngá»¯ Ä‘Ã­ch mÃ  mÃ´ hÃ¬nh nÃªn dá»‹ch. ChÃºng tÃ´i khá»Ÿi táº¡o encoder cá»§a mÃ´ hÃ¬nh dá»‹ch Ä‘a ngÃ´n ngá»¯ vá»›i XLM-R [13]. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ thÃ´ng qua Ä‘iá»ƒm BLEU [56], vÃ  Ä‘iá»ƒm tá»•ng thá»ƒ cho dá»‹ch tÃ i liá»‡u lÃ  Ä‘iá»ƒm BLEU trung bÃ¬nh trÃªn tÃ¡m hÆ°á»›ng dá»‹ch.

Káº¿t quáº£. Báº£ng 14 hiá»ƒn thá»‹ káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi cÃ¡c mÃ´ hÃ¬nh trÃªn tÃ¡m hÆ°á»›ng dá»‹ch. Transformer Baseline lÃ  mÃ´ hÃ¬nh dá»‹ch Ä‘a ngÃ´n ngá»¯ [40]. pretrained Transformer khá»Ÿi táº¡o encoder cá»§a Transformer Baseline vá»›i XLM-R [13]. Vá» hiá»‡u suáº¥t tá»•ng thá»ƒ trÃªn tÃ¡m hÆ°á»›ng dá»‹ch, Transformer Baseline vÃ  pretrained Transformer cÃ³ Ä‘Æ°á»£c Ä‘iá»ƒm BLEU láº§n lÆ°á»£t lÃ  52.67 vÃ  66.16. Káº¿t quáº£ thÃ­ nghiá»‡m chá»©ng minh ráº±ng pretraining Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n 13.49 trong Ä‘iá»ƒm BLEU so vá»›i mÃ´ hÃ¬nh baseline máº¡nh. HÃ¬nh 8 hiá»ƒn thá»‹ thá»i gian cáº§n thiáº¿t Ä‘á»ƒ Ä‘Ã o táº¡o mÃ´ hÃ¬nh vÃ  thá»±c hiá»‡n suy luáº­n trÃªn mÃ´ hÃ¬nh, cÅ©ng nhÆ° trong cÃ¡c tÃ¡c vá»¥ khÃ¡c.

6 CÃ”NG TRÃŒNH LIÃŠN QUAN
CÃ¡c benchmark dataset Ä‘Ã£ Ä‘Ã³ng vai trÃ² trung tÃ¢m trong sá»± phÃ¡t triá»ƒn cá»§a nghiÃªn cá»©u AI á»©ng dá»¥ng. VÃ­ dá»¥, cÃ¡c dataset LibriSpeech [55] vÃ  SQuAD [60] thÃºc Ä‘áº©y sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn dá»¯ liá»‡u cho nháº­n dáº¡ng giá»ng nÃ³i tá»± Ä‘á»™ng vÃ  Ä‘á»c hiá»ƒu cá»§a

--- TRANG 11 ---
CodeXGLUE: Má»™t Benchmark Dataset cho Machine Learning trong viá»‡c Hiá»ƒu vÃ  Sinh Code

Báº£ng 11: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ sá»­a mÃ£.

| PhÆ°Æ¡ng phÃ¡p | small | | | medium | | | Tá»•ng thá»ƒ |
|---|---|---|---|---|---|---|---|
| | BLEU | Acc | CodeBLEU | BLEU | Acc | CodeBLEU | |
| Naive | 78.06 | 0.000 | - | 90.91 | 0.000 | - | 0.000 |
| LSTM | 76.76 | 0.100 | - | 72.08 | 0.025 | - | 0.063 |
| Transformer | 77.21 | 0.147 | 73.31 | 89.25 | 0.037 | 81.72 | 0.092 |
| CodeBERT | 77.42 | 0.164 | 75.58 | 91.07 | 0.052 | 87.52 | 0.108 |

Báº£ng 12: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ dá»‹ch mÃ£.

| PhÆ°Æ¡ng phÃ¡p | Javaâ†’C# | | | C# â†’Java | | | Tá»•ng thá»ƒ |
|---|---|---|---|---|---|---|---|
| | BLEU | Acc | CodeBLEU | BLEU | Acc | CodeBLEU | |
| Naive | 18.54 | 0.000 | - | 18.69 | 0.000 | - | - |
| PBSMT | 43.53 | 0.125 | 42.71 | 40.06 | 0.161 | 43.48 | 43.10 |
| Transformer | 55.84 | 0.330 | 63.74 | 50.47 | 0.379 | 61.59 | 62.67 |
| RoBERTa (code) | 77.46 | 0.561 | 83.07 | 71.99 | 0.579 | 80.18 | 81.63 |
| CodeBERT | 79.92 | 0.590 | 85.10 | 72.14 | 0.580 | 79.41 | 82.26 |

Báº£ng 13: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ tÃ³m táº¯t mÃ£.

| MÃ´ hÃ¬nh | Ruby | Javascript | Go | Python | Java | PHP | Tá»•ng thá»ƒ |
|---|---|---|---|---|---|---|---|
| Seq2Seq | 9.64 | 10.21 | 13.98 | 15.93 | 15.09 | 21.08 | 14.32 |
| Transformer | 11.18 | 11.59 | 16.38 | 15.81 | 16.26 | 22.12 | 15.56 |
| RoBERTa | 11.17 | 11.90 | 17.72 | 18.14 | 16.47 | 24.02 | 16.57 |
| CodeBERT | 12.16 | 14.90 | 18.07 | 19.06 | 17.65 | 25.16 | 17.83 |

Chi phÃ­ Ä‘Ã o táº¡o vÃ  suy luáº­n

| TÃ¡c vá»¥ | TÃªn Dataset | NgÃ´n ngá»¯ | Chi phÃ­ ÄÃ o táº¡o | Chi phÃ­ Suy luáº­n |
|---|---|---|---|---|
| Clone Detection | BigCloneBench | Java | 3 giá» Ä‘Ã o táº¡o trÃªn P100 x2 | 2 giá» trÃªn p100 x2 |
| | POJ-104 | C/C++ | 2 giá» Ä‘Ã o táº¡o trÃªn P100 x2 | 10 phÃºt trÃªn p100 x2 |
| Defect Detection | Devign | C | 1 giá» trÃªn P100 x2 | 2 phÃºt trÃªn p100 x2 |
| Cloze Test | CT-all | Python, Java, PHP, JavaScript, Ruby, Go | N/A | 30 phÃºt trÃªn P100-16G x2 |
| | CT-max/min | Python, Java, PHP, JavaScript, Ruby, Go | N/A | 1 phÃºt trÃªn P100-16G x2 |
| Code Completion | PY150 | Python | 25 giá» trÃªn P100 x2 | 30 phÃºt trÃªn P100 x2 |
| | GitHub Java Corpus | Java | 2 giá» trÃªn P100 x2 | 10 phÃºt trÃªn P100 x2 |
| Code Repair | Bugs2Fix | Java | 24 giá» trÃªn P100 x2 | 20 phÃºt trÃªn P100 x2 |
| Code Translation | CodeTrans | Java-C# | 20 giá» trÃªn P100 x2 | 5 phÃºt trÃªn P100 x2 |
| NL Code Search | CodeSearchnet, AdvTest | Python | 5 giá» trÃªn P100 x2 | 7 phÃºt trÃªn p100 x2 |
| | CodeSearchNet, WebQueryTest | Python | 5 giá» trÃªn P100 x2 | 1 phÃºt trÃªn P100 x2 |
| Text-to-Code Generation | CONCODE | Java | 30 giá» trÃªn P100 x2 | 20 phÃºt trÃªn P100 x2 |
| Code Summarization | CodeSearchNet | Python, Java, PHP, JavaScript, Ruby, Go | Trung bÃ¬nh 12 giá» cho má»—i PL trÃªn P100 x2 | Trung bÃ¬nh 1 giá» cho má»—i PL trÃªn p100 x2 |
| Documentation Translation | Microsoft Docs | English-Latvian/Danish/Norwegian/Chinese | 30 giá» trÃªn P100x2 | 55 phÃºt trÃªn P100x2 |

HÃ¬nh 8: Chi phÃ­ thá»i gian Ä‘Ã o táº¡o vÃ  suy luáº­n cho má»—i tÃ¡c vá»¥, Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn hai GPU P100.

--- TRANG 12 ---
Lu, Guo, Ren vÃ  Huang, et al.

Báº£ng 14: Káº¿t quáº£ trÃªn tÃ¡c vá»¥ dá»‹ch tÃ i liá»‡u.

| TÃ¡c vá»¥ | Transformer Baseline | pretrained Transformer |
|---|---|---|
| ENâ†’DA | 53.31 | 67.09 |
| ENâ†’LA | 37.85 | 51.92 |
| ENâ†’NO | 53.84 | 68.00 |
| ENâ†’ZH | 59.90 | 70.60 |
| DAâ†’EN | 58.73 | 67.02 |
| LAâ†’EN | 50.37 | 68.30 |
| NOâ†’EN | 57.73 | 71.84 |
| ZHâ†’EN | 50.00 | 64.47 |
| Tá»•ng thá»ƒ | 52.67 | 66.16 |

vÄƒn báº£n tÆ°Æ¡ng á»©ng. Vá»›i nhu cáº§u ngÃ y cÃ ng tÄƒng Ä‘á»ƒ kiá»ƒm tra kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a cá»§a cÃ¡c mÃ´ hÃ¬nh trÃªn má»™t loáº¡t á»©ng dá»¥ng rá»™ng, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ táº¡o ra hoáº·c táº­p há»£p cÃ¡c dataset bao quÃ¡t nhiá»u tÃ¡c vá»¥. CÃ¡c máº«u Ä‘áº¡i diá»‡n cá»§a nhá»¯ng dataset nÃ y bao gá»“m ImageNet [15] cho computer vision, GLUE [81] cho hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn, XTREME [33] vÃ  XGLUE [48] cho xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘a ngÃ´n ngá»¯. Theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, CodeXGLUE lÃ  benchmark dataset Ä‘a dáº¡ng Ä‘áº§u tiÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c váº¥n Ä‘á» code intelligence khÃ¡c nhau.

Nhiá»u tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n machine learning cho software engineering [1] cÃ³ Ä‘á»§ lÆ°á»£ng dá»¯ liá»‡u Ä‘á»ƒ há»— trá»£ phÃ¡t triá»ƒn cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn dá»¯ liá»‡u, nhÆ°ng khÃ´ng Ä‘Æ°á»£c bao quÃ¡t bá»Ÿi CodeXGLUE. ChÃºng tÃ´i dá»± Ä‘á»‹nh má»Ÿ rá»™ng Ä‘áº¿n nhá»¯ng tÃ¡c vá»¥ nÃ y trong tÆ°Æ¡ng lai. VÃ­ dá»¥, tÃ¡c vá»¥ khai thÃ¡c idiom [5,36] lÃ  trÃ­ch xuáº¥t cÃ¡c idiom mÃ£, lÃ  cÃ¡c Ä‘oáº¡n cÃº phÃ¡p tÃ¡i diá»…n qua cÃ¡c dá»± Ã¡n pháº§n má»m vÃ  phá»¥c vá»¥ má»™t má»¥c Ä‘Ã­ch ngá»¯ nghÄ©a duy nháº¥t [5]. Bug localization [27,61,76] lÃ  chá»‰ ra vá»‹ trÃ­ lá»—i khi má»™t chÆ°Æ¡ng trÃ¬nh tháº¥t báº¡i trong cÃ¡c test. TÃ¡c vá»¥ sinh test case [22,74] lÃ  sinh cÃ¡c test case Ä‘Æ¡n vá»‹ tá»± Ä‘á»™ng. Program synthesis [20,45,53,64,68,79,98] má»Ÿ rá»™ng tÃ¡c vá»¥ sinh mÃ£ tá»« text nháº±m sinh cÃ¡c chÆ°Æ¡ng trÃ¬nh tá»« má»™t Ä‘áº·c táº£ [24], cháº³ng háº¡n nhÆ° pseudocode, mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  cÃ¡c vÃ­ dá»¥ input/output.

7 Káº¾T LUáº¬N
Vá»›i CodeXGLUE, chÃºng tÃ´i tÃ¬m cÃ¡ch há»— trá»£ phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c váº¥n Ä‘á» hiá»ƒu vÃ  sinh chÆ°Æ¡ng trÃ¬nh khÃ¡c nhau, vá»›i má»¥c tiÃªu tÄƒng nÄƒng suáº¥t cá»§a cÃ¡c láº­p trÃ¬nh viÃªn pháº§n má»m. ChÃºng tÃ´i khuyáº¿n khÃ­ch cÃ¡c nhÃ  nghiÃªn cá»©u tham gia vÃ o thá»­ thÃ¡ch má»Ÿ Ä‘á»ƒ táº¡o ra tiáº¿n bá»™ trong code intelligence. Tiáº¿n lÃªn phÃ­a trÆ°á»›c, chÃºng tÃ´i Ä‘ang lÃªn káº¿ hoáº¡ch má»Ÿ rá»™ng CodeXGLUE Ä‘áº¿n nhiá»u ngÃ´n ngá»¯ láº­p trÃ¬nh vÃ  tÃ¡c vá»¥ downstream hÆ¡n trong khi tiáº¿p tá»¥c phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh pretrained tiÃªn tiáº¿n báº±ng cÃ¡ch khÃ¡m phÃ¡ cÃ¡c cáº¥u trÃºc mÃ´ hÃ¬nh má»›i, giá»›i thiá»‡u cÃ¡c tÃ¡c vá»¥ pretraining má»›i, sá»­ dá»¥ng cÃ¡c loáº¡i dá»¯ liá»‡u khÃ¡c nhau vÃ  nhiá»u hÆ¡n ná»¯a.

TÃ€I LIá»†U THAM KHáº¢O
[1] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, vÃ  Charles Sutton. 2018. A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv. 51, 4, Article 81 (July 2018), 37 pages. https://doi.org/10.1145/3212695

[2] Miltiadis Allamanis, Marc Brockschmidt, vÃ  Mahmoud Khademi. 2017. Learning to represent programs with graphs. arXiv preprint arXiv:1711.00740 (2017).

[3] Miltiadis Allamanis, Hao Peng, vÃ  Charles Sutton. 2016. A convolutional attention network for extreme summarization of source code. In International conference on machine learning. 2091â€“2100.

[4] Miltiadis Allamanis vÃ  Charles Sutton. 2013. Mining Source Code Repositories at Massive Scale using Language Modeling. In 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 207â€“216.

[5] Miltiadis Allamanis vÃ  Charles Sutton. 2014. Mining idioms from source code. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. 472â€“483.

[6] Uri Alon, Meital Zilberstein, Omer Levy, vÃ  Eran Yahav. 2019. code2vec: Learning distributed representations of code. Proceedings of the ACM on Programming Languages 3, POPL (2019), 1â€“29.

[7] Tal Ben-Nun, Alice Shoshana Jakobovits, vÃ  Torsten Hoefler. 2018. Neural code comprehension: A learnable representation of code semantics. In Advances in Neural Information Processing Systems. 3585â€“3597.

[8] Pavol Bielik, Veselin Raychev, vÃ  Martin Vechev. 2016. PHOG: Probabilistic Model for Code. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48 (New York, NY, USA) (ICML'16). JMLR.org, 2933â€“2942.

[9] Marcel Bruch, Martin Monperrus, vÃ  Mira Mezini. 2009. Learning from Examples to Improve Code Completion Systems. In Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering (Amsterdam, The Netherlands) (ESEC/FSE '09). Association for Computing Machinery, New York, NY, USA, 213â€“222. https://doi.org/10.1145/1595696.1595728

[10] L. BÃ¼ch vÃ  A. Andrzejak. 2019. Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection. In 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER). 95â€“104. https://doi.org/10.1109/SANER.2019.8668039

[11] Xinyun Chen, Chang Liu, vÃ  Dawn Song. 2018. Tree-to-tree neural networks for program translation. In Advances in neural information processing systems. 2547â€“2557.

[12] Colin B Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, vÃ  Neel Sundaresan. 2020. PyMT5: multi-mode translation of natural language and Python code with transformers. arXiv preprint arXiv:2010.03150 (2020).

[13] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, vÃ  Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116 (2019).

[14] Mayur Datar, Nicole Immorlica, Piotr Indyk, vÃ  Vahab S Mirrokni. 2004. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry. 253â€“262.

[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, vÃ  Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248â€“255.

[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, vÃ  Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[17] Jean-RÃ©my Falleri, FlorÃ©al Morandat, Xavier Blanc, Matias Martinez, vÃ  Martin Monperrus. 2014. Fine-grained and accurate source code differencing. In Proceedings of the 29th ACM/IEEE international conference on Automated software engineering. 313â€“324.

[18] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).

[19] Patrick Fernandes, Miltiadis Allamanis, vÃ  Marc Brockschmidt. 2018. Structured neural summarization. arXiv preprint arXiv:1811.01824 (2018).

[20] John K. Feser, Swarat Chaudhuri, vÃ  Isil Dillig. 2015. Synthesizing Data Structure Transformations from Input-Output Examples. In Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (Portland, OR, USA) (PLDI '15). Association for Computing Machinery, New York, NY, USA, 229â€“239. https://doi.org/10.1145/2737924.2737977

[21] Michael Fischer, Martin Pinzger, vÃ  Harald Gall. 2003. Populating a release history database from version control and bug tracking systems. In International Conference on Software Maintenance, 2003. ICSM 2003. Proceedings. IEEE, 23â€“32.

[22] Gordon Fraser vÃ  Andrea Arcuri. 2011. Evosuite: automatic test suite generation for object-oriented software. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering. 416â€“419.

[23] Xiaodong Gu, Hongyu Zhang, vÃ  Sunghun Kim. 2018. Deep Code Search. In Proceedings of the 40th International Conference on Software Engineering (Gothenburg, Sweden) (ICSE '18). Association for Computing Machinery, New York, NY, USA, 933â€“944. https://doi.org/10.1145/3180155.3180167

[24] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. 2017. Program synthesis. Foundations and Trends Â®in Programming Languages 4, 1-2 (2017), 1â€“119.

[25] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin, Daxin Jiang, et al. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. arXiv preprint arXiv:2009.08366 (2020).

--- TRANG 13 ---
CodeXGLUE: Má»™t Benchmark Dataset cho Machine Learning trong viá»‡c Hiá»ƒu vÃ  Sinh Code

[26] Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, vÃ  Jian Yin. 2019. Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing. arXiv preprint arXiv:1906.07108 (2019).

[27] Rahul Gupta, Aditya Kanade, vÃ  Shirish Shevade. 2019. Neural Attribution for Semantic Bug-Localization in Student Programs. In Advances in Neural Information Processing Systems. 11884â€“11894.

[28] Rahul Gupta, Soham Pal, Aditya Kanade, vÃ  Shirish Shevade. 2017. DeepFix: Fixing Common C Language Errors by Deep Learning. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (San Francisco, California, USA) (AAAI'17). AAAI Press, 1345â€“1351.

[29] Vincent J Hellendoorn vÃ  Premkumar Devanbu. 2017. Are deep neural networks the best choice for modeling source code?. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. 763â€“773.

[30] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, vÃ  David Bieber. 2019. Global relational models of source code. In International Conference on Learning Representations.

[31] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, vÃ  Premkumar Devanbu. 2012. On the naturalness of software. In 2012 34th International Conference on Software Engineering (ICSE). IEEE, 837â€“847.

[32] Sepp Hochreiter vÃ  JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735â€“1780.

[33] Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, vÃ  Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. arXiv preprint arXiv:2003.11080 (2020).

[34] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, vÃ  Zhi Jin. 2018. Summarizing Source Code with Transferred API Knowledge. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (Stockholm, Sweden) (IJCAI'18). AAAI Press, 2269â€“2275.

[35] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, vÃ  Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

[36] Srinivasan Iyer, Alvin Cheung, vÃ  Luke Zettlemoyer. 2019. Learning programmatic idioms for scalable semantic parsing. arXiv preprint arXiv:1904.09086 (2019).

[37] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, vÃ  Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2073â€“2083.

[38] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, vÃ  Luke Zettlemoyer. 2018. Mapping language to code in programmatic context. arXiv preprint arXiv:1808.09588 (2018).

[39] Lingxiao Jiang, Ghassan Misherghi, Zhendong Su, vÃ  Stephane Glondu. 2007. Deckard: Scalable and accurate tree-based detection of code clones. In 29th International Conference on Software Engineering (ICSE'07). IEEE, 96â€“105.

[40] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda ViÃ©gas, Martin Wattenberg, Greg Corrado, et al. 2017. Google's multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics 5 (2017), 339â€“351.

[41] Svetoslav Karaivanov, Veselin Raychev, vÃ  Martin Vechev. 2014. Phrase-Based Statistical Translation of Programming Languages. In Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming Software (Portland, Oregon, USA) (Onward! 2014). Association for Computing Machinery, New York, NY, USA, 173â€“184. https://doi.org/10.1145/2661136.2661148

[42] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, vÃ  Andrea Janes. 2020. Big Code!= Big Vocabulary: Open-Vocabulary Models for Source Code. arXiv preprint arXiv:2003.07914 (2020).

[43] Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882 (2014).

[44] Philipp Koehn, Franz J Och, vÃ  Daniel Marcu. 2003. Statistical phrase-based translation. Technical Report. UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST.

[45] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, vÃ  Percy S Liang. 2019. Spoc: Search-based pseudocode to code. In Advances in Neural Information Processing Systems. 11906â€“11917.

[46] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, vÃ  Guillaume Lample. 2020. Unsupervised Translation of Programming Languages. arXiv preprint arXiv:2006.03511 (2020).

[47] Yi Li, Shaohua Wang, Tien N Nguyen, vÃ  Son Van Nguyen. 2019. Improving bug detection via context-based code representation learning and attention-based neural networks. Proceedings of the ACM on Programming Languages 3, OOPSLA (2019), 1â€“30.

[48] Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, et al. 2020. Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation. arXiv preprint arXiv:2004.01401 (2020).

[49] Chin-Yew Lin vÃ  Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics. 501â€“507.

[50] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, vÃ  Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).

[51] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, vÃ  Satish Chandra. 2019. Aroma: Code recommendation via structural code search. Proceedings of the ACM on Programming Languages 3, OOPSLA (2019), 1â€“28.

[52] Lili Mou, Ge Li, Lu Zhang, Tao Wang, vÃ  Zhi Jin. 2016. Convolutional neural networks over tree structures for programming language processing. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. 1287â€“1293.

[53] Arvind Neelakantan, Quoc V Le, vÃ  Ilya Sutskever. 2015. Neural programmer: Inducing latent programs with gradient descent. arXiv preprint arXiv:1511.04834 (2015).

[54] Anh Tuan Nguyen, Tung Thanh Nguyen, vÃ  Tien N Nguyen. 2015. Divide-and-conquer approach for multi-phase statistical migration for source code (t). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 585â€“596.

[55] Vassil Panayotov, Guoguo Chen, Daniel Povey, vÃ  Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 5206â€“5210.

[56] Kishore Papineni, Salim Roukos, Todd Ward, vÃ  Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311â€“318.

[57] Michael Pradel vÃ  Koushik Sen. 2018. DeepBugs: A Learning Approach to Name-Based Bug Detection. Proc. ACM Program. Lang. 2, OOPSLA, Article 147 (Oct. 2018), 25 pages. https://doi.org/10.1145/3276517

[58] Varot Premtoon, James Koppel, vÃ  Armando Solar-Lezama. 2020. Semantic Code Search via Equational Reasoning. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation (London, UK) (PLDI 2020). Association for Computing Machinery, New York, NY, USA, 1066â€“1082. https://doi.org/10.1145/3385412.3386001

[59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, vÃ  Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.

[60] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, vÃ  Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016).

[61] Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, vÃ  Premkumar Devanbu. 2016. On the" naturalness" of buggy code. In 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE). IEEE, 428â€“439.

[62] Veselin Raychev, Pavol Bielik, vÃ  Martin Vechev. 2016. Probabilistic Model for Code with Decision Trees. ACM SIGPLAN Notices (2016), 731â€“747.

[63] Veselin Raychev, Martin Vechev, vÃ  Eran Yahav. 2014. Code Completion with Statistical Language Models. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (Edinburgh, United Kingdom) (PLDI '14). Association for Computing Machinery, New York, NY, USA, 419â€“428. https://doi.org/10.1145/2594291.2594321

[64] Scott Reed vÃ  Nando De Freitas. 2015. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279 (2015).

[65] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Ming Zhou, Ambrosio Blanco, vÃ  Shuai Ma. 2020. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. arXiv preprint arXiv:2009.10297 (2020).

[66] Rico Sennrich, Barry Haddow, vÃ  Alexandra Birch. 2015. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 (2015).

[67] Rico Sennrich, Barry Haddow, vÃ  Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1715â€“1725.

[68] Rishabh Singh vÃ  Sumit Gulwani. 2015. Predicting a correct program in programming by example. In International Conference on Computer Aided Verification. Springer, 398â€“414.

[69] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203 (2019).

[70] Ilya Sutskever, Oriol Vinyals, vÃ  Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. 3104â€“3112.

[71] Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K Roy, vÃ  Mohammad Mamun Mia. 2014. Towards a big data curated benchmark of inter-project code clones. In 2014 IEEE International Conference on Software Maintenance and Evolution. IEEE, 476â€“480.

--- TRANG 14 ---
Lu, Guo, Ren vÃ  Huang, et al.

[72] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, vÃ  Neel Sundaresan. 2020. IntelliCode Compose: Code Generation Using Transformer. arXiv preprint arXiv:2005.08025 (2020).

[73] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, vÃ  Neel Sundaresan. 2019. Pythia: ai-assisted code completion system. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2727â€“2735.

[74] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, vÃ  Neel Sundaresan. 2020. Unit Test Case Generation with Transformers. arXiv preprint arXiv:2009.05617 (2020).

[75] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, vÃ  Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing patches in the wild via neural machine translation. ACM Transactions on Software Engineering and Methodology (TOSEM) 28, 4 (2019), 1â€“29.

[76] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, vÃ  Rishabh Singh. 2019. Neural program repair by jointly learning to localize and repair. arXiv preprint arXiv:1904.01720 (2019).

[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, vÃ  Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998â€“6008.

[78] Panagiotis Vekris, Benjamin Cosman, vÃ  Ranjit Jhala. 2016. Refinement Types for TypeScript. In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation (Santa Barbara, CA, USA) (PLDI '16). Association for Computing Machinery, New York, NY, USA, 310â€“325. https://doi.org/10.1145/2908080.2908110

[79] Murali Vijayaraghavan, Chaudhuri Swarat, vÃ  Jermaine Chris. 2017. Bayesian Sketch Learning for Program Synthesis. CoRR.â€”-2017.â€”-Vol. abs/1703.05698.â€”-1703.05698 (2017).

[80] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, vÃ  Philip S Yu. 2018. Improving automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. 397â€“407.

[81] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, vÃ  Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 (2018).

[82] Song Wang, Devin Chollak, Dana Movshovitz-Attias, vÃ  Lin Tan. 2016. Bugram: bug detection with n-gram language models. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. 708â€“719.

[83] Song Wang, Taiyue Liu, vÃ  Lin Tan. 2016. Automatically Learning Semantic Features for Defect Prediction. In Proceedings of the 38th International Conference on Software Engineering (Austin, Texas) (ICSE '16). Association for Computing Machinery, New York, NY, USA, 297â€“308. https://doi.org/10.1145/2884781.2884804

[84] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, vÃ  Zhi Jin. 2020. Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 261â€“271.

[85] Wenhua Wang, Yuqun Zhang, Zhengran Zeng, vÃ  Guandong Xu. 2020. TranSÂ³: A Transformer-based Framework for Unifying Code Summarization and Code Search. arXiv preprint arXiv:2003.03238 (2020).

[86] Yanlin Wang, Lun Du, Ensheng Shi, Yuxuan Hu, Shi Han, vÃ  Dongmei Zhang. 2020. CoCoGUM: Contextual Code Summarization with Multi-Relational GNN on UMLs.

[87] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, vÃ  Zhi Jin. 2019. Code generation as a dual task of code summarization. In Advances in Neural Information Processing Systems. 6563â€“6573.

[88] Huihui Wei vÃ  Ming Li. 2017. Supervised Deep Features for Software Functional Clone Detection by Exploiting Lexical and Syntactical Information in Source Code.. In IJCAI. 3034â€“3040.

[89] Martin White, Michele Tufano, Christopher Vendome, vÃ  Denys Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In 2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 87â€“98.

[90] Frank F Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, vÃ  Graham Neubig. 2020. Incorporating external knowledge through pre-training for natural language to code generation. arXiv preprint arXiv:2004.09015 (2020).

[91] S. Yan, H. Yu, Y. Chen, B. Shen, vÃ  L. Jiang. 2020. Are the Code Snippets What We Are Searching for? A Benchmark and an Empirical Study on Code Search with Natural-Language Queries. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). 344â€“354. https://doi.org/10.1109/SANER48275.2020.9054840

[92] Ziyu Yao, Daniel S Weld, Wei-Peng Chen, vÃ  Huan Sun. 2018. StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow. In Proceedings of the 2018 World Wide Web Conference. 1693â€“1703.

[93] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marucs, Nesime Tatbul, Jesmin Jahan Tithi, Paul Petersen, Timothy Mattson, Tim Kraska, Pradeep Dubey, et al. 2020. MISIM: An End-to-End Neural Code Similarity System. arXiv preprint arXiv:2006.05265 (2020).

[94] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, vÃ  Graham Neubig. 2018. Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow. In International Conference on Mining Software Repositories (MSR). ACM, 476â€“486. https://doi.org/10.1145/3196398.3196408

[95] Pengcheng Yin vÃ  Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. arXiv preprint arXiv:1704.01696 (2017).

[96] Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, vÃ  Qianxiang Wang. 2019. Neural detection of semantic code clones via tree-based convolution. In 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC). IEEE, 70â€“80.

[97] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, vÃ  Xudong Liu. 2019. A novel neural source code representation based on abstract syntax tree. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 783â€“794.

[98] Ruiqi Zhong, Mitchell Stern, vÃ  Dan Klein. 2020. Semantic Scaffolds for Pseudocode-to-Code Generation. arXiv preprint arXiv:2005.05927 (2020).

[99] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, vÃ  Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. In Advances in Neural Information Processing Systems. 10197â€“10207.