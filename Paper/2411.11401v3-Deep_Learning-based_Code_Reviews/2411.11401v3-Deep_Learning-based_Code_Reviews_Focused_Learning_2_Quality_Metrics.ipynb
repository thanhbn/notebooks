{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Measuring Code Review Quality Metrics\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the **multi-dimensional nature** of code review quality\n",
    "2. Implement metrics for measuring review effectiveness (from Table II)\n",
    "3. Map paper metrics to **DeepEval** evaluation framework\n",
    "4. Build a comprehensive quality assessment system\n",
    "\n",
    "## Paper Context\n",
    "**Section Reference**: Section II-D (Data Analysis) and Section III-B (RQ1: Impact on Quality Issues Found)\n",
    "\n",
    "**Key Metrics from Paper**:\n",
    "- Number of reported quality issues\n",
    "- Length of code review (sentences)\n",
    "- Covered code locations\n",
    "- Issue severity classification\n",
    "- Inter-rater agreement (Cohen's Kappa = 0.315)\n",
    "\n",
    "**Table Reference**: Table II - Variables used in the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import re\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define issue taxonomy based on Mäntylä and Lassenius (referenced in paper)\n",
    "class IssueCategory(Enum):\n",
    "    \"\"\"Issue categories from Mäntylä and Lassenius taxonomy\"\"\"\n",
    "    # Evolvability issues (77% in paper)\n",
    "    DOCUMENTATION = \"documentation\"\n",
    "    STRUCTURE = \"structure\"\n",
    "    VISUAL_REPRESENTATION = \"visual_representation\"\n",
    "    SOLUTION_APPROACH = \"solution_approach\"\n",
    "    \n",
    "    # Functional issues (23% in paper)\n",
    "    LOGIC = \"logic\"\n",
    "    RESOURCE = \"resource\"\n",
    "    CHECK = \"check\"\n",
    "    INTERFACE = \"interface\"\n",
    "\n",
    "@dataclass\n",
    "class CodeReviewComment:\n",
    "    \"\"\"Detailed code review comment with metadata\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    file_path: str\n",
    "    line_start: int\n",
    "    line_end: int\n",
    "    category: IssueCategory\n",
    "    severity: str  # low, medium, high\n",
    "    author_type: str  # human, llm, comprehensive\n",
    "    confidence: float = 0.5  # Reviewer confidence in the comment\n",
    "    \n",
    "@dataclass\n",
    "class CodeReviewMetrics:\n",
    "    \"\"\"Comprehensive metrics for a code review\"\"\"\n",
    "    # Basic metrics from paper\n",
    "    num_issues_reported: int\n",
    "    review_length_sentences: int\n",
    "    covered_lines: int\n",
    "    covered_files: int\n",
    "    \n",
    "    # Quality metrics\n",
    "    issues_by_severity: Dict[str, int]\n",
    "    issues_by_category: Dict[str, int]\n",
    "    \n",
    "    # Effectiveness metrics\n",
    "    true_positives: int = 0\n",
    "    false_positives: int = 0\n",
    "    false_negatives: int = 0\n",
    "    \n",
    "    # Time metrics\n",
    "    time_total_seconds: int = 0\n",
    "    time_per_issue: float = 0.0\n",
    "    \n",
    "    # Advanced metrics\n",
    "    comment_clarity_score: float = 0.0\n",
    "    actionability_score: float = 0.0\n",
    "    coverage_uniformity: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quality Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewQualityAnalyzer:\n",
    "    \"\"\"Analyzes code review quality using multiple metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.severity_weights = {\n",
    "            'low': 1,\n",
    "            'medium': 2,\n",
    "            'high': 3\n",
    "        }\n",
    "    \n",
    "    def calculate_basic_metrics(self, comments: List[CodeReviewComment]) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate basic metrics as defined in the paper\"\"\"\n",
    "        \n",
    "        # Number of issues reported\n",
    "        num_issues = len(comments)\n",
    "        \n",
    "        # Review length in sentences\n",
    "        all_text = ' '.join([c.text for c in comments])\n",
    "        sentences = sent_tokenize(all_text) if all_text else []\n",
    "        review_length = len(sentences)\n",
    "        \n",
    "        # Covered code locations\n",
    "        covered_lines = set()\n",
    "        covered_files = set()\n",
    "        \n",
    "        for comment in comments:\n",
    "            covered_files.add(comment.file_path)\n",
    "            for line in range(comment.line_start, comment.line_end + 1):\n",
    "                covered_lines.add((comment.file_path, line))\n",
    "        \n",
    "        return {\n",
    "            'num_issues_reported': num_issues,\n",
    "            'review_length_sentences': review_length,\n",
    "            'covered_lines': len(covered_lines),\n",
    "            'covered_files': len(covered_files),\n",
    "            'avg_comment_length': np.mean([len(c.text.split()) for c in comments]) if comments else 0\n",
    "        }\n",
    "    \n",
    "    def calculate_severity_distribution(self, comments: List[CodeReviewComment]) -> Dict[str, int]:\n",
    "        \"\"\"Calculate distribution of issues by severity\"\"\"\n",
    "        severity_counts = {'low': 0, 'medium': 0, 'high': 0}\n",
    "        \n",
    "        for comment in comments:\n",
    "            severity_counts[comment.severity] += 1\n",
    "        \n",
    "        return severity_counts\n",
    "    \n",
    "    def calculate_category_distribution(self, comments: List[CodeReviewComment]) -> Dict[str, int]:\n",
    "        \"\"\"Calculate distribution by issue category\"\"\"\n",
    "        category_counts = Counter([c.category.value for c in comments])\n",
    "        return dict(category_counts)\n",
    "    \n",
    "    def calculate_weighted_score(self, comments: List[CodeReviewComment]) -> float:\n",
    "        \"\"\"Calculate weighted quality score based on severity\"\"\"\n",
    "        if not comments:\n",
    "            return 0.0\n",
    "        \n",
    "        total_weight = sum(self.severity_weights[c.severity] for c in comments)\n",
    "        max_possible = len(comments) * self.severity_weights['high']\n",
    "        \n",
    "        return total_weight / max_possible if max_possible > 0 else 0.0\n",
    "    \n",
    "    def calculate_clarity_score(self, comments: List[CodeReviewComment]) -> float:\n",
    "        \"\"\"Estimate clarity of review comments\"\"\"\n",
    "        if not comments:\n",
    "            return 0.0\n",
    "        \n",
    "        clarity_indicators = [\n",
    "            'should', 'must', 'need to', 'consider', 'recommend',\n",
    "            'instead of', 'rather than', 'because', 'since', 'due to'\n",
    "        ]\n",
    "        \n",
    "        scores = []\n",
    "        for comment in comments:\n",
    "            text_lower = comment.text.lower()\n",
    "            \n",
    "            # Check for clarity indicators\n",
    "            indicator_count = sum(1 for ind in clarity_indicators if ind in text_lower)\n",
    "            \n",
    "            # Check for specific line references\n",
    "            has_line_ref = bool(re.search(r'line \\d+|lines? \\d+-\\d+', text_lower))\n",
    "            \n",
    "            # Check for code snippets\n",
    "            has_code = bool(re.search(r'`[^`]+`|```[^`]+```', comment.text))\n",
    "            \n",
    "            # Calculate clarity score\n",
    "            score = min(1.0, (indicator_count * 0.2 + (0.3 if has_line_ref else 0) + \n",
    "                            (0.3 if has_code else 0) + 0.2))\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def calculate_actionability_score(self, comments: List[CodeReviewComment]) -> float:\n",
    "        \"\"\"Measure how actionable the comments are\"\"\"\n",
    "        if not comments:\n",
    "            return 0.0\n",
    "        \n",
    "        action_keywords = [\n",
    "            'change', 'modify', 'update', 'fix', 'remove', 'add',\n",
    "            'replace', 'refactor', 'rename', 'move', 'extract'\n",
    "        ]\n",
    "        \n",
    "        actionable_count = 0\n",
    "        for comment in comments:\n",
    "            text_lower = comment.text.lower()\n",
    "            if any(keyword in text_lower for keyword in action_keywords):\n",
    "                actionable_count += 1\n",
    "        \n",
    "        return actionable_count / len(comments)\n",
    "    \n",
    "    def calculate_coverage_uniformity(self, comments: List[CodeReviewComment], \n",
    "                                    total_files: int) -> float:\n",
    "        \"\"\"Measure how uniformly the review covers the codebase\"\"\"\n",
    "        if not comments or total_files == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Count comments per file\n",
    "        file_comment_counts = Counter([c.file_path for c in comments])\n",
    "        \n",
    "        # Calculate entropy as measure of uniformity\n",
    "        total_comments = len(comments)\n",
    "        entropy = 0\n",
    "        \n",
    "        for count in file_comment_counts.values():\n",
    "            if count > 0:\n",
    "                p = count / total_comments\n",
    "                entropy -= p * np.log2(p)\n",
    "        \n",
    "        # Normalize by maximum possible entropy\n",
    "        max_entropy = np.log2(min(total_files, total_comments))\n",
    "        \n",
    "        return entropy / max_entropy if max_entropy > 0 else 0.0\n",
    "\n",
    "# Create analyzer instance\n",
    "analyzer = CodeReviewQualityAnalyzer()\n",
    "print(\"Code Review Quality Analyzer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulating Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_review(treatment: str, num_issues: int = 10) -> List[CodeReviewComment]:\n",
    "    \"\"\"Generate synthetic review data based on paper findings\"\"\"\n",
    "    \n",
    "    comments = []\n",
    "    \n",
    "    # Treatment-specific distributions based on paper\n",
    "    if treatment == \"MCR\":\n",
    "        severity_probs = [0.4, 0.4, 0.2]  # More balanced\n",
    "        category_weights = {\n",
    "            IssueCategory.DOCUMENTATION: 0.15,\n",
    "            IssueCategory.STRUCTURE: 0.20,\n",
    "            IssueCategory.SOLUTION_APPROACH: 0.25,\n",
    "            IssueCategory.LOGIC: 0.15,\n",
    "            IssueCategory.CHECK: 0.15,\n",
    "            IssueCategory.INTERFACE: 0.10\n",
    "        }\n",
    "    elif treatment == \"ACR\":\n",
    "        severity_probs = [0.6, 0.3, 0.1]  # More low-severity issues\n",
    "        category_weights = {\n",
    "            IssueCategory.DOCUMENTATION: 0.25,\n",
    "            IssueCategory.STRUCTURE: 0.30,\n",
    "            IssueCategory.SOLUTION_APPROACH: 0.20,\n",
    "            IssueCategory.LOGIC: 0.10,\n",
    "            IssueCategory.CHECK: 0.10,\n",
    "            IssueCategory.INTERFACE: 0.05\n",
    "        }\n",
    "    else:  # CCR\n",
    "        severity_probs = [0.3, 0.4, 0.3]  # Comprehensive coverage\n",
    "        category_weights = {\n",
    "            IssueCategory.DOCUMENTATION: 0.20,\n",
    "            IssueCategory.STRUCTURE: 0.20,\n",
    "            IssueCategory.SOLUTION_APPROACH: 0.20,\n",
    "            IssueCategory.LOGIC: 0.15,\n",
    "            IssueCategory.CHECK: 0.15,\n",
    "            IssueCategory.INTERFACE: 0.10\n",
    "        }\n",
    "    \n",
    "    # Generate comments\n",
    "    for i in range(num_issues):\n",
    "        severity = np.random.choice(['low', 'medium', 'high'], p=severity_probs)\n",
    "        category = np.random.choice(list(category_weights.keys()), \n",
    "                                  p=list(category_weights.values()))\n",
    "        \n",
    "        # Generate realistic comment text\n",
    "        comment_templates = {\n",
    "            'low': [\n",
    "                \"Consider adding documentation for this method.\",\n",
    "                \"This variable name could be more descriptive.\",\n",
    "                \"Minor: inconsistent spacing in this section.\"\n",
    "            ],\n",
    "            'medium': [\n",
    "                \"This method is too long and should be refactored.\",\n",
    "                \"Missing error handling for edge cases.\",\n",
    "                \"Performance issue: string concatenation in loop.\"\n",
    "            ],\n",
    "            'high': [\n",
    "                \"Critical bug: incorrect logic in condition.\",\n",
    "                \"Security vulnerability: SQL injection risk.\",\n",
    "                \"Memory leak: resources not properly released.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        text = np.random.choice(comment_templates[severity])\n",
    "        \n",
    "        comment = CodeReviewComment(\n",
    "            id=f\"{treatment}-{i+1}\",\n",
    "            text=text,\n",
    "            file_path=f\"src/module_{np.random.randint(1, 5)}.py\",\n",
    "            line_start=np.random.randint(1, 100),\n",
    "            line_end=0,  # Will be set below\n",
    "            category=category,\n",
    "            severity=severity,\n",
    "            author_type='llm' if treatment == 'ACR' else 'human',\n",
    "            confidence=np.random.uniform(0.6, 0.95)\n",
    "        )\n",
    "        comment.line_end = comment.line_start + np.random.randint(0, 5)\n",
    "        \n",
    "        comments.append(comment)\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Generate sample reviews\n",
    "mcr_review = generate_synthetic_review(\"MCR\", num_issues=8)\n",
    "acr_review = generate_synthetic_review(\"ACR\", num_issues=12)\n",
    "ccr_review = generate_synthetic_review(\"CCR\", num_issues=10)\n",
    "\n",
    "print(f\"Generated synthetic reviews:\")\n",
    "print(f\"  MCR: {len(mcr_review)} comments\")\n",
    "print(f\"  ACR: {len(acr_review)} comments\")\n",
    "print(f\"  CCR: {len(ccr_review)} comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing Comprehensive Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_review_metrics(comments: List[CodeReviewComment], \n",
    "                         treatment: str,\n",
    "                         time_seconds: int = None) -> CodeReviewMetrics:\n",
    "    \"\"\"Compute comprehensive metrics for a code review\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    basic = analyzer.calculate_basic_metrics(comments)\n",
    "    \n",
    "    # Quality distributions\n",
    "    severity_dist = analyzer.calculate_severity_distribution(comments)\n",
    "    category_dist = analyzer.calculate_category_distribution(comments)\n",
    "    \n",
    "    # Advanced scores\n",
    "    clarity = analyzer.calculate_clarity_score(comments)\n",
    "    actionability = analyzer.calculate_actionability_score(comments)\n",
    "    uniformity = analyzer.calculate_coverage_uniformity(comments, total_files=5)\n",
    "    \n",
    "    # Time metrics (from paper averages)\n",
    "    if time_seconds is None:\n",
    "        time_seconds = {\n",
    "            'MCR': 42 * 60,  # 42 minutes\n",
    "            'ACR': 56 * 60,  # 56 minutes\n",
    "            'CCR': 57 * 60   # 57 minutes\n",
    "        }.get(treatment, 50 * 60)\n",
    "    \n",
    "    metrics = CodeReviewMetrics(\n",
    "        num_issues_reported=basic['num_issues_reported'],\n",
    "        review_length_sentences=basic['review_length_sentences'],\n",
    "        covered_lines=basic['covered_lines'],\n",
    "        covered_files=basic['covered_files'],\n",
    "        issues_by_severity=severity_dist,\n",
    "        issues_by_category=category_dist,\n",
    "        time_total_seconds=time_seconds,\n",
    "        time_per_issue=time_seconds / basic['num_issues_reported'] if basic['num_issues_reported'] > 0 else 0,\n",
    "        comment_clarity_score=clarity,\n",
    "        actionability_score=actionability,\n",
    "        coverage_uniformity=uniformity\n",
    "    )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute metrics for all treatments\n",
    "metrics_mcr = compute_review_metrics(mcr_review, 'MCR')\n",
    "metrics_acr = compute_review_metrics(acr_review, 'ACR')\n",
    "metrics_ccr = compute_review_metrics(ccr_review, 'CCR')\n",
    "\n",
    "# Display metrics comparison\n",
    "metrics_data = {\n",
    "    'Treatment': ['MCR', 'ACR', 'CCR'],\n",
    "    'Issues Reported': [metrics_mcr.num_issues_reported, metrics_acr.num_issues_reported, metrics_ccr.num_issues_reported],\n",
    "    'Review Length (sentences)': [metrics_mcr.review_length_sentences, metrics_acr.review_length_sentences, metrics_ccr.review_length_sentences],\n",
    "    'Clarity Score': [metrics_mcr.comment_clarity_score, metrics_acr.comment_clarity_score, metrics_ccr.comment_clarity_score],\n",
    "    'Actionability': [metrics_mcr.actionability_score, metrics_acr.actionability_score, metrics_ccr.actionability_score],\n",
    "    'Time per Issue (min)': [metrics_mcr.time_per_issue/60, metrics_acr.time_per_issue/60, metrics_ccr.time_per_issue/60]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\nCode Review Metrics Comparison:\")\n",
    "print(metrics_df.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DeepEval Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewRelevanceMetric(BaseMetric):\n",
    "    \"\"\"Custom DeepEval metric for code review relevance\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.7):\n",
    "        self.threshold = threshold\n",
    "        self.score = 0\n",
    "        self.reason = \"\"\n",
    "        self.success = False\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase):\n",
    "        \"\"\"Measure relevance of code review comments to actual code issues\"\"\"\n",
    "        \n",
    "        # Extract code issues from expected output\n",
    "        expected_issues = set(test_case.expected_output.lower().split('\\n'))\n",
    "        \n",
    "        # Extract issues from actual output\n",
    "        actual_issues = set(test_case.actual_output.lower().split('\\n'))\n",
    "        \n",
    "        # Calculate relevance score\n",
    "        if not expected_issues:\n",
    "            self.score = 0\n",
    "        else:\n",
    "            # Check keyword overlap\n",
    "            relevance_keywords = ['bug', 'error', 'issue', 'problem', 'incorrect', \n",
    "                                'missing', 'wrong', 'fix', 'should', 'must']\n",
    "            \n",
    "            relevant_count = 0\n",
    "            for actual in actual_issues:\n",
    "                if any(keyword in actual for keyword in relevance_keywords):\n",
    "                    relevant_count += 1\n",
    "            \n",
    "            self.score = relevant_count / len(actual_issues) if actual_issues else 0\n",
    "        \n",
    "        self.success = self.score >= self.threshold\n",
    "        self.reason = f\"Relevance score: {self.score:.2f} (threshold: {self.threshold})\"\n",
    "        \n",
    "        return self.score\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "class CodeReviewCompletenessMetric(BaseMetric):\n",
    "    \"\"\"Measure completeness of code review coverage\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.score = 0\n",
    "        self.reason = \"\"\n",
    "        self.success = False\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase):\n",
    "        \"\"\"Measure how completely the review covers known issues\"\"\"\n",
    "        \n",
    "        # Parse expected issues (ground truth)\n",
    "        expected_lines = []\n",
    "        for line in test_case.expected_output.split('\\n'):\n",
    "            # Extract line numbers from expected output\n",
    "            import re\n",
    "            line_nums = re.findall(r'line (\\d+)', line.lower())\n",
    "            expected_lines.extend([int(n) for n in line_nums])\n",
    "        \n",
    "        # Parse actual review\n",
    "        actual_lines = []\n",
    "        for line in test_case.actual_output.split('\\n'):\n",
    "            line_nums = re.findall(r'line (\\d+)', line.lower())\n",
    "            actual_lines.extend([int(n) for n in line_nums])\n",
    "        \n",
    "        # Calculate completeness\n",
    "        if not expected_lines:\n",
    "            self.score = 1.0 if not actual_lines else 0.0\n",
    "        else:\n",
    "            covered = len(set(actual_lines) & set(expected_lines))\n",
    "            self.score = covered / len(set(expected_lines))\n",
    "        \n",
    "        self.success = self.score >= self.threshold\n",
    "        self.reason = f\"Completeness: {self.score:.2f} (covered {len(set(actual_lines) & set(expected_lines))} of {len(set(expected_lines))} expected locations)\"\n",
    "        \n",
    "        return self.score\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "# Initialize custom metrics\n",
    "relevance_metric = CodeReviewRelevanceMetric(threshold=0.7)\n",
    "completeness_metric = CodeReviewCompletenessMetric(threshold=0.5)\n",
    "\n",
    "print(\"Custom DeepEval metrics for code review initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inter-rater Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_inter_rater_agreement(n_issues: int = 50, n_raters: int = 2):\n",
    "    \"\"\"Simulate inter-rater agreement for severity classification\"\"\"\n",
    "    \n",
    "    # Generate ground truth severities\n",
    "    true_severities = np.random.choice(['low', 'medium', 'high'], \n",
    "                                     size=n_issues, \n",
    "                                     p=[0.5, 0.3, 0.2])\n",
    "    \n",
    "    # Simulate rater classifications with some noise\n",
    "    # Paper reports Cohen's Kappa of 0.315 (fair agreement)\n",
    "    rater1_severities = []\n",
    "    rater2_severities = []\n",
    "    \n",
    "    for true_sev in true_severities:\n",
    "        # Rater 1\n",
    "        if np.random.random() < 0.6:  # 60% agreement rate\n",
    "            rater1_severities.append(true_sev)\n",
    "        else:\n",
    "            # Random disagreement\n",
    "            options = ['low', 'medium', 'high']\n",
    "            options.remove(true_sev)\n",
    "            rater1_severities.append(np.random.choice(options))\n",
    "        \n",
    "        # Rater 2\n",
    "        if np.random.random() < 0.6:\n",
    "            rater2_severities.append(true_sev)\n",
    "        else:\n",
    "            options = ['low', 'medium', 'high']\n",
    "            options.remove(true_sev)\n",
    "            rater2_severities.append(np.random.choice(options))\n",
    "    \n",
    "    # Calculate Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(rater1_severities, rater2_severities)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(rater1_severities, rater2_severities, \n",
    "                         labels=['low', 'medium', 'high'])\n",
    "    \n",
    "    return {\n",
    "        'rater1': rater1_severities,\n",
    "        'rater2': rater2_severities,\n",
    "        'kappa': kappa,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Simulate agreement\n",
    "agreement_data = simulate_inter_rater_agreement(n_issues=100)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(agreement_data['confusion_matrix'], \n",
    "            annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Low', 'Medium', 'High'],\n",
    "            yticklabels=['Low', 'Medium', 'High'])\n",
    "plt.title(f\"Inter-rater Agreement Matrix\\nCohen's Kappa: {agreement_data['kappa']:.3f}\")\n",
    "plt.xlabel('Rater 2 Classification')\n",
    "plt.ylabel('Rater 1 Classification')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInter-rater Agreement Analysis:\")\n",
    "print(f\"Cohen's Kappa: {agreement_data['kappa']:.3f}\")\n",
    "print(f\"Interpretation: {'Poor' if agreement_data['kappa'] < 0.2 else 'Fair' if agreement_data['kappa'] < 0.4 else 'Moderate' if agreement_data['kappa'] < 0.6 else 'Good'}\")\n",
    "print(f\"\\nNote: Paper reported κ = 0.315 (Fair agreement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quality Metric Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quality_metrics(metrics_dict: Dict[str, CodeReviewMetrics]):\n",
    "    \"\"\"Create comprehensive visualization of review quality metrics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    treatments = list(metrics_dict.keys())\n",
    "    \n",
    "    # 1. Issue severity distribution\n",
    "    ax = axes[0]\n",
    "    severity_data = pd.DataFrame([\n",
    "        metrics.issues_by_severity for metrics in metrics_dict.values()\n",
    "    ], index=treatments)\n",
    "    severity_data.plot(kind='bar', ax=ax, color=['lightgreen', 'orange', 'red'])\n",
    "    ax.set_title('Issue Severity Distribution by Treatment')\n",
    "    ax.set_xlabel('Treatment')\n",
    "    ax.set_ylabel('Number of Issues')\n",
    "    ax.legend(title='Severity')\n",
    "    \n",
    "    # 2. Time efficiency\n",
    "    ax = axes[1]\n",
    "    time_data = [m.time_per_issue/60 for m in metrics_dict.values()]\n",
    "    bars = ax.bar(treatments, time_data, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    ax.set_title('Time Efficiency: Minutes per Issue Found')\n",
    "    ax.set_ylabel('Minutes per Issue')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, time_data):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                f'{val:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Quality scores\n",
    "    ax = axes[2]\n",
    "    quality_metrics = ['Clarity', 'Actionability', 'Coverage Uniformity']\n",
    "    quality_data = pd.DataFrame({\n",
    "        treatment: [\n",
    "            metrics.comment_clarity_score,\n",
    "            metrics.actionability_score,\n",
    "            metrics.coverage_uniformity\n",
    "        ] for treatment, metrics in metrics_dict.items()\n",
    "    }, index=quality_metrics)\n",
    "    \n",
    "    quality_data.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Quality Scores Comparison')\n",
    "    ax.set_ylabel('Score (0-1)')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.legend(title='Treatment')\n",
    "    \n",
    "    # 4. Issue category distribution (stacked)\n",
    "    ax = axes[3]\n",
    "    category_data = pd.DataFrame([\n",
    "        metrics.issues_by_category for metrics in metrics_dict.values()\n",
    "    ], index=treatments).fillna(0)\n",
    "    \n",
    "    category_data.T.plot(kind='bar', stacked=True, ax=ax)\n",
    "    ax.set_title('Issue Categories by Treatment')\n",
    "    ax.set_xlabel('Issue Category')\n",
    "    ax.set_ylabel('Number of Issues')\n",
    "    ax.legend(title='Treatment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 5. Radar chart for multi-dimensional comparison\n",
    "    ax = axes[4]\n",
    "    ax.remove()\n",
    "    ax = fig.add_subplot(2, 3, 5, projection='polar')\n",
    "    \n",
    "    # Metrics for radar chart\n",
    "    radar_metrics = ['Coverage', 'Clarity', 'Actionability', 'Efficiency', 'Completeness']\n",
    "    \n",
    "    for treatment, metrics in metrics_dict.items():\n",
    "        values = [\n",
    "            min(1.0, metrics.covered_lines / 100),  # Normalized coverage\n",
    "            metrics.comment_clarity_score,\n",
    "            metrics.actionability_score,\n",
    "            1 - min(1.0, metrics.time_per_issue / 3600),  # Inverse time (efficiency)\n",
    "            min(1.0, metrics.num_issues_reported / 15)  # Normalized completeness\n",
    "        ]\n",
    "        \n",
    "        # Add first value to close the polygon\n",
    "        values += values[:1]\n",
    "        \n",
    "        # Angles for each metric\n",
    "        angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=treatment)\n",
    "        ax.fill(angles, values, alpha=0.15)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(radar_metrics)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Multi-dimensional Quality Comparison', y=1.08)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 6. Summary statistics table\n",
    "    ax = axes[5]\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_data = []\n",
    "    for treatment, metrics in metrics_dict.items():\n",
    "        summary_data.append([\n",
    "            treatment,\n",
    "            metrics.num_issues_reported,\n",
    "            f\"{metrics.time_total_seconds/60:.0f}\",\n",
    "            f\"{metrics.comment_clarity_score:.2f}\",\n",
    "            f\"{metrics.actionability_score:.2f}\",\n",
    "            f\"{sum(metrics.issues_by_severity.values())}\"\n",
    "        ])\n",
    "    \n",
    "    table = ax.table(cellText=summary_data,\n",
    "                     colLabels=['Treatment', 'Issues', 'Time (min)', 'Clarity', 'Actionability', 'Total'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    ax.set_title('Summary Statistics', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize all metrics\n",
    "all_metrics = {\n",
    "    'MCR': metrics_mcr,\n",
    "    'ACR': metrics_acr,\n",
    "    'CCR': metrics_ccr\n",
    "}\n",
    "\n",
    "visualize_quality_metrics(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating a Composite Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeQualityScorer:\n",
    "    \"\"\"Calculate composite quality score for code reviews\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Weights based on importance\n",
    "        self.weights = {\n",
    "            'severity_score': 0.25,      # Finding high-severity issues\n",
    "            'completeness': 0.20,        # Coverage of codebase\n",
    "            'clarity': 0.15,             # Clear communication\n",
    "            'actionability': 0.15,       # Actionable feedback\n",
    "            'efficiency': 0.15,          # Time efficiency\n",
    "            'uniformity': 0.10          # Balanced coverage\n",
    "        }\n",
    "    \n",
    "    def calculate_severity_score(self, metrics: CodeReviewMetrics) -> float:\n",
    "        \"\"\"Score based on severity-weighted issues found\"\"\"\n",
    "        severity_weights = {'low': 1, 'medium': 2, 'high': 3}\n",
    "        \n",
    "        weighted_sum = sum(\n",
    "            count * severity_weights[sev] \n",
    "            for sev, count in metrics.issues_by_severity.items()\n",
    "        )\n",
    "        \n",
    "        # Normalize by maximum possible (all high severity)\n",
    "        max_possible = metrics.num_issues_reported * 3\n",
    "        \n",
    "        return weighted_sum / max_possible if max_possible > 0 else 0\n",
    "    \n",
    "    def calculate_completeness_score(self, metrics: CodeReviewMetrics) -> float:\n",
    "        \"\"\"Score based on code coverage\"\"\"\n",
    "        # Normalize by expected coverage (e.g., 100 lines)\n",
    "        return min(1.0, metrics.covered_lines / 100)\n",
    "    \n",
    "    def calculate_efficiency_score(self, metrics: CodeReviewMetrics) -> float:\n",
    "        \"\"\"Score based on time efficiency\"\"\"\n",
    "        # Inverse of time per issue, normalized\n",
    "        # Assume 5 minutes per issue is excellent\n",
    "        target_time = 5 * 60  # 5 minutes in seconds\n",
    "        \n",
    "        if metrics.time_per_issue == 0:\n",
    "            return 0\n",
    "        \n",
    "        return min(1.0, target_time / metrics.time_per_issue)\n",
    "    \n",
    "    def calculate_composite_score(self, metrics: CodeReviewMetrics) -> Dict[str, float]:\n",
    "        \"\"\"Calculate overall composite quality score\"\"\"\n",
    "        \n",
    "        scores = {\n",
    "            'severity_score': self.calculate_severity_score(metrics),\n",
    "            'completeness': self.calculate_completeness_score(metrics),\n",
    "            'clarity': metrics.comment_clarity_score,\n",
    "            'actionability': metrics.actionability_score,\n",
    "            'efficiency': self.calculate_efficiency_score(metrics),\n",
    "            'uniformity': metrics.coverage_uniformity\n",
    "        }\n",
    "        \n",
    "        # Calculate weighted composite\n",
    "        composite = sum(score * self.weights[name] for name, score in scores.items())\n",
    "        \n",
    "        return {\n",
    "            'component_scores': scores,\n",
    "            'composite_score': composite\n",
    "        }\n",
    "\n",
    "# Calculate composite scores\n",
    "scorer = CompositeQualityScorer()\n",
    "\n",
    "composite_results = {}\n",
    "for treatment, metrics in all_metrics.items():\n",
    "    composite_results[treatment] = scorer.calculate_composite_score(metrics)\n",
    "\n",
    "# Visualize composite scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Overall composite scores\n",
    "treatments = list(composite_results.keys())\n",
    "composite_scores = [r['composite_score'] for r in composite_results.values()]\n",
    "\n",
    "bars = ax1.bar(treatments, composite_scores, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax1.set_title('Composite Quality Scores by Treatment', fontsize=14)\n",
    "ax1.set_ylabel('Composite Score (0-1)')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, composite_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Component breakdown\n",
    "component_data = pd.DataFrame([\n",
    "    result['component_scores'] for result in composite_results.values()\n",
    "], index=treatments)\n",
    "\n",
    "component_data.plot(kind='bar', ax=ax2, stacked=False)\n",
    "ax2.set_title('Quality Score Components by Treatment', fontsize=14)\n",
    "ax2.set_ylabel('Component Score (0-1)')\n",
    "ax2.legend(title='Component', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nDetailed Composite Quality Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "for treatment, result in composite_results.items():\n",
    "    print(f\"\\n{treatment}:\")\n",
    "    print(f\"  Composite Score: {result['composite_score']:.3f}\")\n",
    "    print(\"  Component Scores:\")\n",
    "    for component, score in result['component_scores'].items():\n",
    "        weight = scorer.weights[component]\n",
    "        contribution = score * weight\n",
    "        print(f\"    {component:15} {score:.3f} (weight: {weight:.2f}, contribution: {contribution:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building a Quality Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def build_quality_predictor():\n",
    "    \"\"\"Build a model to predict review quality based on features\"\"\"\n",
    "    \n",
    "    # Generate synthetic training data\n",
    "    n_samples = 1000\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Simulate review characteristics\n",
    "        treatment = np.random.choice(['MCR', 'ACR', 'CCR'])\n",
    "        \n",
    "        # Treatment-specific distributions\n",
    "        if treatment == 'MCR':\n",
    "            num_issues = np.random.poisson(8)\n",
    "            clarity = np.random.beta(8, 2)  # Higher clarity\n",
    "            time_minutes = np.random.normal(42, 10)\n",
    "        elif treatment == 'ACR':\n",
    "            num_issues = np.random.poisson(12)\n",
    "            clarity = np.random.beta(6, 4)  # Medium clarity\n",
    "            time_minutes = np.random.normal(56, 15)\n",
    "        else:  # CCR\n",
    "            num_issues = np.random.poisson(10)\n",
    "            clarity = np.random.beta(7, 3)  # Good clarity\n",
    "            time_minutes = np.random.normal(57, 12)\n",
    "        \n",
    "        # Other features\n",
    "        actionability = np.random.beta(5, 5)\n",
    "        coverage = np.random.beta(4, 2)\n",
    "        high_severity_ratio = np.random.beta(2, 8) if treatment == 'ACR' else np.random.beta(3, 7)\n",
    "        \n",
    "        # Calculate quality score (simplified)\n",
    "        quality = (\n",
    "            0.3 * high_severity_ratio +\n",
    "            0.2 * clarity +\n",
    "            0.2 * actionability +\n",
    "            0.2 * coverage +\n",
    "            0.1 * (1 - min(1, time_minutes / 100))\n",
    "        )\n",
    "        \n",
    "        data.append({\n",
    "            'treatment_MCR': 1 if treatment == 'MCR' else 0,\n",
    "            'treatment_ACR': 1 if treatment == 'ACR' else 0,\n",
    "            'treatment_CCR': 1 if treatment == 'CCR' else 0,\n",
    "            'num_issues': num_issues,\n",
    "            'clarity': clarity,\n",
    "            'actionability': actionability,\n",
    "            'coverage': coverage,\n",
    "            'time_minutes': time_minutes,\n",
    "            'high_severity_ratio': high_severity_ratio,\n",
    "            'quality': quality\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    feature_cols = [col for col in df.columns if col != 'quality']\n",
    "    X = df[feature_cols]\n",
    "    y = df['quality']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return model, feature_importance, {'mse': mse, 'r2': r2}\n",
    "\n",
    "# Build and evaluate model\n",
    "model, feature_importance, metrics = build_quality_predictor()\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance.plot(x='feature', y='importance', kind='bar', ax=ax1)\n",
    "ax1.set_title('Feature Importance for Quality Prediction')\n",
    "ax1.set_xlabel('Feature')\n",
    "ax1.set_ylabel('Importance')\n",
    "\n",
    "# Model performance\n",
    "ax2.text(0.1, 0.7, f\"Model Performance:\", fontsize=16, weight='bold')\n",
    "ax2.text(0.1, 0.5, f\"R² Score: {metrics['r2']:.3f}\", fontsize=14)\n",
    "ax2.text(0.1, 0.3, f\"MSE: {metrics['mse']:.4f}\", fontsize=14)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQuality Prediction Model Built!\")\n",
    "print(f\"Model R² Score: {metrics['r2']:.3f}\")\n",
    "print(\"\\nTop 3 Most Important Features:\")\n",
    "for _, row in feature_importance.head(3).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = {\n",
    "    \"Metric Design Principles\": [\n",
    "        \"Multi-dimensional assessment captures review quality better than single metrics\",\n",
    "        \"Severity weighting is crucial - not all issues are equal\",\n",
    "        \"Time efficiency should be balanced with thoroughness\",\n",
    "        \"Inter-rater agreement of 0.315 indicates subjectivity in quality assessment\"\n",
    "    ],\n",
    "    \n",
    "    \"Key Findings from Analysis\": [\n",
    "        \"ACR generates more comments but lower severity-weighted quality\",\n",
    "        \"MCR shows better balance between efficiency and effectiveness\",\n",
    "        \"CCR has high coverage but diminishing returns on time investment\",\n",
    "        \"Clarity and actionability vary significantly between treatments\"\n",
    "    ],\n",
    "    \n",
    "    \"DeepEval Integration Benefits\": [\n",
    "        \"Automated quality assessment enables continuous improvement\",\n",
    "        \"Custom metrics can capture domain-specific quality aspects\",\n",
    "        \"Relevance and completeness metrics align with paper findings\",\n",
    "        \"Can be integrated into CI/CD pipelines for quality gates\"\n",
    "    ],\n",
    "    \n",
    "    \"Practical Implementation Guidelines\": [\n",
    "        \"Track both quantitative metrics (coverage, count) and qualitative (clarity, actionability)\",\n",
    "        \"Use composite scores but understand component contributions\",\n",
    "        \"Consider reviewer experience and confidence in quality assessment\",\n",
    "        \"Regular calibration sessions can improve inter-rater agreement\",\n",
    "        \"Automate metric collection to reduce manual overhead\"\n",
    "    ],\n",
    "    \n",
    "    \"Future Research Directions\": [\n",
    "        \"Develop ML models to predict review quality from code characteristics\",\n",
    "        \"Study correlation between review metrics and post-release defects\",\n",
    "        \"Create adaptive metrics that learn from reviewer feedback\",\n",
    "        \"Investigate cultural and team factors affecting quality perception\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS: Measuring Code Review Quality\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category, items in insights.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  • {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"Quality measurement in code review is multi-faceted. While automated\")\n",
    "print(\"tools can increase quantity of feedback, true quality requires balancing\")\n",
    "print(\"severity detection, clarity, actionability, and efficiency. The framework\")\n",
    "print(\"presented here provides a foundation for comprehensive quality assessment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}