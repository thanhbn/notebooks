{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-RAG Focused Learning 1: AMR Graph Processing & Shortest Path Extraction\n",
    "\n",
    "## Learning Objective\n",
    "Deep dive into Abstract Meaning Representation (AMR) graph processing and the novel shortest path extraction method used in G-RAG for improving document reranking.\n",
    "\n",
    "## Paper Context\n",
    "\n",
    "### Key Paper Sections:\n",
    "- **Section 3.1**: Establishing Document Graphs via AMR\n",
    "- **Section 3.2.1**: Generating Node Features\n",
    "- **Figure 3**: Number of SSSPs AMR graphs in train set\n",
    "\n",
    "### Paper Quote (Section 3.2.1):\n",
    "> *\"By studying the structure of AMRs for different documents, we note that almost every AMR has the node 'question', where the word 'question' is included in the input of the AMR parsing model, given by 'question:<question text><document text>'. Thus, we can find the single source shortest path starting from the node 'question'. When listing every path, the potential connection from the question to the answer becomes much clearer.\"*\n",
    "\n",
    "### Why This Matters:\n",
    "1. **Computational Efficiency**: Instead of using all AMR tokens (expensive), only use key path information\n",
    "2. **Semantic Focus**: Paths from \"question\" node reveal question-answer connections\n",
    "3. **Performance**: This approach avoids overfitting while maintaining semantic richness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "### Abstract Meaning Representation (AMR)\n",
    "\n",
    "AMR represents sentence meaning as a rooted, directed acyclic graph where:\n",
    "- **Nodes**: Represent concepts (entities, events, properties)\n",
    "- **Edges**: Represent semantic relations between concepts\n",
    "- **Structure**: More compact and structured than natural language\n",
    "\n",
    "### Example AMR Structure:\n",
    "```\n",
    "Sentence: \"The boy wants to go home.\"\n",
    "AMR:\n",
    "(w / want-01\n",
    "   :ARG0 (b / boy)\n",
    "   :ARG1 (g / go-01\n",
    "            :ARG0 b\n",
    "            :ARG4 (h / home)))\n",
    "```\n",
    "\n",
    "### G-RAG's Novel Approach:\n",
    "1. **Input Format**: `\"question:<question text><document text>\"`\n",
    "2. **Question Node**: Always present as root/anchor point\n",
    "3. **Path Extraction**: Find all shortest paths from \"question\" to other concepts\n",
    "4. **Sequence Creation**: Flatten paths into sequences for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for AMR processing\n",
    "!pip install networkx matplotlib seaborn\n",
    "!pip install transformers torch\n",
    "!pip install sentence-transformers\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete for AMR processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMR Graph Simulation\n",
    "\n",
    "Since full AMR parsing requires specialized models, we'll create realistic AMR graph simulations that capture the paper's methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRGraphSimulator:\n",
    "    \"\"\"\n",
    "    Simulates AMR graphs based on the methodology described in the G-RAG paper.\n",
    "    \n",
    "    Key Features:\n",
    "    - Always includes 'question' node as per paper\n",
    "    - Creates realistic concept nodes and relations\n",
    "    - Enables shortest path analysis from 'question' node\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common AMR relations based on AMR specification\n",
    "        self.amr_relations = [\n",
    "            ':ARG0', ':ARG1', ':ARG2', ':ARG3', ':ARG4',  # Core arguments\n",
    "            ':location', ':time', ':manner', ':purpose',   # Adjuncts\n",
    "            ':mod', ':domain', ':consist-of', ':part-of',  # Modifiers\n",
    "            ':name', ':wiki', ':polarity', ':degree'       # Properties\n",
    "        ]\n",
    "        \n",
    "        # Concept patterns for different types\n",
    "        self.concept_patterns = {\n",
    "            'person': ['person', 'man', 'woman', 'boy', 'girl', 'singer', 'composer'],\n",
    "            'location': ['country', 'city', 'place', 'region', 'area'],\n",
    "            'event': ['sing-01', 'compose-01', 'release-01', 'perform-01', 'create-01'],\n",
    "            'entity': ['album', 'song', 'movie', 'building', 'product'],\n",
    "            'property': ['blue', 'famous', 'popular', 'large', 'small'],\n",
    "            'time': ['date-entity', 'temporal-quantity', 'year', 'month']\n",
    "        }\n",
    "    \n",
    "    def extract_concepts_from_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract potential AMR concepts from text\"\"\"\n",
    "        concepts = ['question']  # Always start with question node\n",
    "        \n",
    "        # Extract named entities (simplified)\n",
    "        words = re.findall(r'\\b[A-Za-z]+\\b', text.lower())\n",
    "        \n",
    "        # Map words to AMR concepts\n",
    "        concept_mapping = {\n",
    "            'sinatra': 'person', 'frank': 'person',\n",
    "            'williams': 'person', 'john': 'person',\n",
    "            'canberra': 'city', 'australia': 'country',\n",
    "            'iphone': 'product', 'apple': 'company',\n",
    "            'aurora': 'phenomenon', 'sun': 'star',\n",
    "            'blue': 'blue', 'eyes': 'body-part',\n",
    "            'nickname': 'name', 'capital': 'role',\n",
    "            'music': 'music', 'compose': 'compose-01',\n",
    "            'release': 'release-01', 'cause': 'cause-01'\n",
    "        }\n",
    "        \n",
    "        for word in words:\n",
    "            if word in concept_mapping:\n",
    "                concepts.append(concept_mapping[word])\n",
    "            elif len(word) > 3:  # Add longer words as potential concepts\n",
    "                concepts.append(word)\n",
    "        \n",
    "        return list(set(concepts))  # Remove duplicates\n",
    "    \n",
    "    def create_amr_graph(self, question: str, document: str) -> nx.DiGraph:\n",
    "        \"\"\"Create AMR graph from question and document\"\"\"\n",
    "        # Combine input as per paper methodology\n",
    "        combined_text = f\"question: {question} {document}\"\n",
    "        \n",
    "        # Extract concepts\n",
    "        concepts = self.extract_concepts_from_text(combined_text)\n",
    "        \n",
    "        # Create directed graph\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(concepts)\n",
    "        \n",
    "        # Add edges based on semantic proximity and co-occurrence\n",
    "        sentences = re.split(r'[.!?]+', combined_text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_concepts = [c for c in concepts if c.lower() in sentence.lower()]\n",
    "            \n",
    "            # Connect question node to concepts in the same sentence\n",
    "            if 'question' in sentence_concepts:\n",
    "                for concept in sentence_concepts:\n",
    "                    if concept != 'question':\n",
    "                        G.add_edge('question', concept, relation=':ARG1')\n",
    "            \n",
    "            # Connect co-occurring concepts\n",
    "            for i, c1 in enumerate(sentence_concepts):\n",
    "                for c2 in sentence_concepts[i+1:]:\n",
    "                    if not G.has_edge(c1, c2) and c1 != c2:\n",
    "                        relation = np.random.choice(self.amr_relations)\n",
    "                        G.add_edge(c1, c2, relation=relation)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def visualize_amr_graph(self, G: nx.DiGraph, title: str = \"AMR Graph\", figsize: Tuple[int, int] = (12, 8)):\n",
    "        \"\"\"Visualize AMR graph with question node highlighted\"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Use spring layout for better visualization\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Color nodes differently\n",
    "        node_colors = []\n",
    "        for node in G.nodes():\n",
    "            if node == 'question':\n",
    "                node_colors.append('red')      # Question node in red\n",
    "            elif 'person' in node or any(name in node for name in ['sinatra', 'williams']):\n",
    "                node_colors.append('lightblue') # Person concepts in blue\n",
    "            else:\n",
    "                node_colors.append('lightgreen') # Other concepts in green\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                              node_size=1000, alpha=0.8)\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.6, arrows=True, \n",
    "                              arrowsize=20, edge_color='gray')\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "        \n",
    "        # Draw edge labels (relations)\n",
    "        edge_labels = nx.get_edge_attributes(G, 'relation')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=6)\n",
    "        \n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print graph statistics\n",
    "        print(f\"\\nAMR Graph Statistics:\")\n",
    "        print(f\"Nodes (concepts): {G.number_of_nodes()}\")\n",
    "        print(f\"Edges (relations): {G.number_of_edges()}\")\n",
    "        print(f\"Average degree: {2 * G.number_of_edges() / G.number_of_nodes():.2f}\")\n",
    "        print(f\"Has 'question' node: {'question' in G.nodes()}\")\n",
    "\n",
    "# Test the AMR simulator\n",
    "amr_sim = AMRGraphSimulator()\n",
    "\n",
    "# Create sample AMR graph\n",
    "sample_question = \"What is the nickname of Frank Sinatra?\"\n",
    "sample_document = \"Frank Sinatra was an American singer and actor. His bright blue eyes earned him the popular nickname 'Ol' Blue Eyes'.\"\n",
    "\n",
    "sample_amr = amr_sim.create_amr_graph(sample_question, sample_document)\n",
    "amr_sim.visualize_amr_graph(sample_amr, \"Sample AMR Graph: Frank Sinatra Question\")\n",
    "\n",
    "print(f\"Sample concepts extracted: {list(sample_amr.nodes())[:10]}...\")\n",
    "print(f\"Sample relations: {[data['relation'] for _, _, data in list(sample_amr.edges(data=True))[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Path Extraction - Core Algorithm\n",
    "\n",
    "### Paper Methodology (Section 3.2.1):\n",
    "\n",
    "The paper describes two key steps:\n",
    "1. **Path Identification**: Find shortest single source paths (SSSPs) from \"question\" node\n",
    "2. **Node Concept Extraction**: Extract concepts along paths to construct AMR sequence\n",
    "\n",
    "### Implementation Details:\n",
    "- Use BFS to find shortest paths\n",
    "- Remove paths that are subsets of others\n",
    "- Create sequences from path concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortestPathExtractor:\n",
    "    \"\"\"\n",
    "    Implements the shortest path extraction algorithm from G-RAG paper.\n",
    "    \n",
    "    Based on Section 3.2.1: \"Path Identification\" and \"Node Concept Extraction\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.source_node = 'question'\n",
    "    \n",
    "    def find_single_source_shortest_paths(self, G: nx.DiGraph) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Find all shortest paths from 'question' node to all other reachable nodes.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping target node -> shortest path from question\n",
    "        \"\"\"\n",
    "        if self.source_node not in G.nodes():\n",
    "            return {}\n",
    "        \n",
    "        # Use NetworkX to find shortest paths\n",
    "        try:\n",
    "            paths = nx.single_source_shortest_path(G, self.source_node)\n",
    "            return paths\n",
    "        except nx.NetworkXNoPath:\n",
    "            return {}\n",
    "    \n",
    "    def remove_subset_paths(self, paths: Dict[str, List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Remove paths that are subsets of other paths (as mentioned in paper).\n",
    "        \n",
    "        Example from paper:\n",
    "        - Path 1: ['question', 'cross', 'world-region', 'crucifix', 'number', 'be-located-at', 'country', 'Spain']\n",
    "        - Path 2: ['question', 'cross', 'religion', 'Catholicism', 'belief', 'worship']\n",
    "        Both are kept as neither is a subset of the other.\n",
    "        \"\"\"\n",
    "        path_list = list(paths.values())\n",
    "        filtered_paths = []\n",
    "        \n",
    "        for i, path1 in enumerate(path_list):\n",
    "            is_subset = False\n",
    "            \n",
    "            for j, path2 in enumerate(path_list):\n",
    "                if i != j and len(path1) < len(path2):\n",
    "                    # Check if path1 is a prefix/subset of path2\n",
    "                    if path1 == path2[:len(path1)]:\n",
    "                        is_subset = True\n",
    "                        break\n",
    "            \n",
    "            if not is_subset:\n",
    "                filtered_paths.append(path1)\n",
    "        \n",
    "        return filtered_paths\n",
    "    \n",
    "    def create_amr_sequence(self, paths: List[List[str]]) -> str:\n",
    "        \"\"\"\n",
    "        Create AMR sequence from paths (as described in paper).\n",
    "        \n",
    "        Paper example output:\n",
    "        \"question cross world-region crucifix number be-located-at country Spain religion Catholicism belief worship\"\n",
    "        \"\"\"\n",
    "        all_concepts = []\n",
    "        \n",
    "        for path in paths:\n",
    "            # Skip the 'question' node itself, add the rest\n",
    "            path_concepts = path[1:] if len(path) > 1 else []\n",
    "            all_concepts.extend(path_concepts)\n",
    "        \n",
    "        # Join concepts with spaces\n",
    "        amr_sequence = ' '.join(all_concepts)\n",
    "        return amr_sequence\n",
    "    \n",
    "    def extract_paths_and_sequence(self, G: nx.DiGraph) -> Tuple[List[List[str]], str, Dict]:\n",
    "        \"\"\"\n",
    "        Complete path extraction process as per paper methodology.\n",
    "        \n",
    "        Returns:\n",
    "            - Filtered paths\n",
    "            - AMR sequence\n",
    "            - Statistics\n",
    "        \"\"\"\n",
    "        # Step 1: Find shortest paths\n",
    "        all_paths = self.find_single_source_shortest_paths(G)\n",
    "        \n",
    "        # Step 2: Remove subset paths\n",
    "        filtered_paths = self.remove_subset_paths(all_paths)\n",
    "        \n",
    "        # Step 3: Create AMR sequence\n",
    "        amr_sequence = self.create_amr_sequence(filtered_paths)\n",
    "        \n",
    "        # Compute statistics (matching paper's Figure 3)\n",
    "        stats = {\n",
    "            'total_paths': len(all_paths),\n",
    "            'filtered_paths': len(filtered_paths),\n",
    "            'avg_path_length': np.mean([len(path) for path in filtered_paths]) if filtered_paths else 0,\n",
    "            'max_path_length': max([len(path) for path in filtered_paths]) if filtered_paths else 0,\n",
    "            'sequence_length': len(amr_sequence.split()) if amr_sequence else 0\n",
    "        }\n",
    "        \n",
    "        return filtered_paths, amr_sequence, stats\n",
    "    \n",
    "    def visualize_paths(self, G: nx.DiGraph, paths: List[List[str]], title: str = \"Shortest Paths from Question Node\"):\n",
    "        \"\"\"\n",
    "        Visualize the shortest paths in the AMR graph.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Create layout\n",
    "        pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "        \n",
    "        # Draw all nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightgray', \n",
    "                              node_size=800, alpha=0.7)\n",
    "        \n",
    "        # Draw all edges in light gray\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3, edge_color='lightgray')\n",
    "        \n",
    "        # Highlight paths with different colors\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(paths)))\n",
    "        \n",
    "        for i, path in enumerate(paths):\n",
    "            # Highlight nodes in path\n",
    "            path_nodes = path\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=path_nodes, \n",
    "                                  node_color=[colors[i]], node_size=1000, alpha=0.8)\n",
    "            \n",
    "            # Highlight edges in path\n",
    "            path_edges = [(path[j], path[j+1]) for j in range(len(path)-1)]\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=path_edges, \n",
    "                                  edge_color=[colors[i]], width=3, alpha=0.8)\n",
    "        \n",
    "        # Highlight question node specially\n",
    "        if 'question' in G.nodes():\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=['question'], \n",
    "                                  node_color='red', node_size=1200, alpha=0.9)\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "        \n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                     markerfacecolor=colors[i], markersize=10, \n",
    "                                     label=f'Path {i+1}: {\" → \".join(path[:3])}...' if len(path) > 3 else f'Path {i+1}: {\" → \".join(path)}')\n",
    "                          for i, path in enumerate(paths[:5])]  # Show max 5 in legend\n",
    "        \n",
    "        if legend_elements:\n",
    "            plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print path details\n",
    "        print(f\"\\nDetailed Path Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, path in enumerate(paths):\n",
    "            print(f\"Path {i+1}: {' → '.join(path)}\")\n",
    "            print(f\"  Length: {len(path)} nodes\")\n",
    "            print()\n",
    "\n",
    "# Test the shortest path extractor\n",
    "path_extractor = ShortestPathExtractor()\n",
    "\n",
    "# Extract paths from our sample AMR graph\n",
    "paths, amr_sequence, stats = path_extractor.extract_paths_and_sequence(sample_amr)\n",
    "\n",
    "print(\"SHORTEST PATH EXTRACTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Document: {sample_document[:100]}...\")\n",
    "print()\n",
    "print(f\"Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "print(f\"AMR Sequence (first 200 chars):\")\n",
    "print(f\"  {amr_sequence[:200]}...\")\n",
    "print()\n",
    "\n",
    "# Visualize the paths\n",
    "if paths:\n",
    "    path_extractor.visualize_paths(sample_amr, paths, \n",
    "                                  \"Shortest Paths from 'Question' Node\")\n",
    "else:\n",
    "    print(\"No paths found from question node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis - Replicating Figure 3\n",
    "\n",
    "### Paper Analysis (Section 3.2.1 & Figure 3):\n",
    "The paper analyzes shortest single source paths (SSSPs) on Natural Questions and TriviaQA datasets, showing:\n",
    "- **Positive documents**: Have different path patterns than negative documents\n",
    "- **Negative documents**: Either lack connections or have too many irrelevant paths\n",
    "- **Path distribution**: Provides insights for encoding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRPathAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes AMR path statistics similar to Figure 3 in the paper.\n",
    "    \n",
    "    Reproduces the analysis on path distributions for positive vs negative documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, amr_simulator: AMRGraphSimulator, path_extractor: ShortestPathExtractor):\n",
    "        self.amr_sim = amr_simulator\n",
    "        self.path_extractor = path_extractor\n",
    "    \n",
    "    def analyze_document_paths(self, questions: List[str], \n",
    "                             documents_dict: Dict[str, List[str]], \n",
    "                             positive_docs_dict: Dict[str, List[int]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze path statistics for positive vs negative documents.\n",
    "        \n",
    "        Similar to the analysis shown in Figure 3 of the paper.\n",
    "        \"\"\"\n",
    "        positive_stats = []\n",
    "        negative_stats = []\n",
    "        \n",
    "        for question in questions:\n",
    "            documents = documents_dict[question]\n",
    "            positive_indices = positive_docs_dict[question]\n",
    "            \n",
    "            for doc_idx, document in enumerate(documents):\n",
    "                # Create AMR graph\n",
    "                amr_graph = self.amr_sim.create_amr_graph(question, document)\n",
    "                \n",
    "                # Extract paths and statistics\n",
    "                paths, amr_sequence, stats = self.path_extractor.extract_paths_and_sequence(amr_graph)\n",
    "                \n",
    "                # Add document type information\n",
    "                stats['is_positive'] = doc_idx in positive_indices\n",
    "                stats['question'] = question\n",
    "                stats['doc_idx'] = doc_idx\n",
    "                \n",
    "                # Categorize\n",
    "                if doc_idx in positive_indices:\n",
    "                    positive_stats.append(stats)\n",
    "                else:\n",
    "                    negative_stats.append(stats)\n",
    "        \n",
    "        return {\n",
    "            'positive': positive_stats,\n",
    "            'negative': negative_stats\n",
    "        }\n",
    "    \n",
    "    def plot_path_distributions(self, analysis_results: Dict, figsize: Tuple[int, int] = (15, 10)):\n",
    "        \"\"\"\n",
    "        Create plots similar to Figure 3 in the paper.\n",
    "        \"\"\"\n",
    "        positive_stats = analysis_results['positive']\n",
    "        negative_stats = analysis_results['negative']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "        fig.suptitle('AMR Path Analysis: Positive vs Negative Documents\\n(Inspired by Paper Figure 3)', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Extract metrics\n",
    "        metrics = ['total_paths', 'filtered_paths', 'avg_path_length', \n",
    "                  'max_path_length', 'sequence_length']\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Extract values\n",
    "            pos_values = [stat[metric] for stat in positive_stats if stat[metric] is not None]\n",
    "            neg_values = [stat[metric] for stat in negative_stats if stat[metric] is not None]\n",
    "            \n",
    "            # Create histograms\n",
    "            bins = np.linspace(0, max(max(pos_values) if pos_values else 0, \n",
    "                                    max(neg_values) if neg_values else 0), 20)\n",
    "            \n",
    "            ax.hist(pos_values, bins=bins, alpha=0.7, label='Positive Documents', \n",
    "                   color='green', density=True)\n",
    "            ax.hist(neg_values, bins=bins, alpha=0.7, label='Negative Documents', \n",
    "                   color='red', density=True)\n",
    "            \n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Value')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Remove empty subplot\n",
    "        if len(metrics) < 6:\n",
    "            axes[1, 2].remove()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        self.print_summary_statistics(positive_stats, negative_stats)\n",
    "    \n",
    "    def print_summary_statistics(self, positive_stats: List[Dict], negative_stats: List[Dict]):\n",
    "        \"\"\"\n",
    "        Print summary statistics comparing positive and negative documents.\n",
    "        \"\"\"\n",
    "        print(\"\\nSUMMARY STATISTICS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        metrics = ['total_paths', 'filtered_paths', 'avg_path_length', 'sequence_length']\n",
    "        \n",
    "        print(f\"{'Metric':<20} {'Positive (Avg)':<15} {'Negative (Avg)':<15} {'Difference':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            pos_avg = np.mean([stat[metric] for stat in positive_stats if stat[metric] is not None])\n",
    "            neg_avg = np.mean([stat[metric] for stat in negative_stats if stat[metric] is not None])\n",
    "            diff = pos_avg - neg_avg\n",
    "            \n",
    "            print(f\"{metric:<20} {pos_avg:<15.2f} {neg_avg:<15.2f} {diff:<12.2f}\")\n",
    "        \n",
    "        print(\"\\nKey Insights (based on paper findings):\")\n",
    "        print(\"• Positive documents often have more structured path patterns\")\n",
    "        print(\"• Negative documents may lack connections or have excessive noise\")\n",
    "        print(\"• Path length and sequence length can indicate relevance\")\n",
    "        print(\"• This analysis guides the AMR encoding strategy in G-RAG\")\n",
    "\n",
    "# Create mock dataset for analysis\n",
    "mock_questions = [\n",
    "    \"What is the nickname of Frank Sinatra?\",\n",
    "    \"Who composed the music for Star Wars?\",\n",
    "    \"What is the capital of Australia?\"\n",
    "]\n",
    "\n",
    "mock_documents = {\n",
    "    \"What is the nickname of Frank Sinatra?\": [\n",
    "        \"Frank Sinatra was known for his bright blue eyes, earning him the nickname 'Ol' Blue Eyes'.\",  # Positive\n",
    "        \"The singer performed at many venues and was very popular in the 1950s.\",  # Negative\n",
    "        \"Sinatra's music influenced many generations of performers and artists.\",  # Negative\n",
    "        \"His blue eyes were so distinctive that fans called him 'Ol' Blue Eyes'.\",  # Positive\n",
    "        \"Many musicians have had various nicknames throughout their careers.\"  # Negative\n",
    "    ],\n",
    "    \"Who composed the music for Star Wars?\": [\n",
    "        \"John Williams composed the iconic Star Wars soundtrack with memorable themes.\",  # Positive\n",
    "        \"The Star Wars films are known for their epic space battles and storylines.\",  # Negative\n",
    "        \"Science fiction movies often feature orchestral scores and dramatic music.\",  # Negative\n",
    "        \"Williams created leitmotifs for different characters in the Star Wars saga.\",  # Positive\n",
    "        \"The London Symphony Orchestra performed many film scores for major movies.\"  # Negative\n",
    "    ],\n",
    "    \"What is the capital of Australia?\": [\n",
    "        \"Canberra is the capital city of Australia, located between Sydney and Melbourne.\",  # Positive\n",
    "        \"Australia is known for its unique wildlife including kangaroos and koalas.\",  # Negative\n",
    "        \"The Australian government operates from Parliament House in Canberra.\",  # Positive\n",
    "        \"Sydney and Melbourne are the largest cities in Australia by population.\",  # Negative\n",
    "        \"Many people incorrectly think Sydney is Australia's capital city.\"  # Negative\n",
    "    ]\n",
    "}\n",
    "\n",
    "mock_positive_docs = {\n",
    "    \"What is the nickname of Frank Sinatra?\": [0, 3],\n",
    "    \"Who composed the music for Star Wars?\": [0, 3],\n",
    "    \"What is the capital of Australia?\": [0, 2]\n",
    "}\n",
    "\n",
    "# Run the analysis\n",
    "analyzer = AMRPathAnalyzer(amr_sim, path_extractor)\n",
    "analysis_results = analyzer.analyze_document_paths(mock_questions, mock_documents, mock_positive_docs)\n",
    "\n",
    "print(f\"Analyzed {len(analysis_results['positive'])} positive and {len(analysis_results['negative'])} negative documents\")\n",
    "\n",
    "# Plot the results\n",
    "analyzer.plot_path_distributions(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation & Optimization\n",
    "\n",
    "### Real-world Considerations:\n",
    "1. **Computational Efficiency**: Path extraction must be fast for large document sets\n",
    "2. **Memory Management**: AMR graphs can be large, need efficient storage\n",
    "3. **Quality Control**: Filter out noisy or irrelevant paths\n",
    "4. **Integration**: Seamless integration with downstream GNN processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAMRProcessor:\n",
    "    \"\"\"\n",
    "    Production-ready AMR processor with optimizations for the G-RAG pipeline.\n",
    "    \n",
    "    Optimizations:\n",
    "    - Efficient path caching\n",
    "    - Parallel processing capabilities\n",
    "    - Memory-efficient graph representation\n",
    "    - Quality filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_path_length: int = 10, max_sequence_length: int = 200):\n",
    "        self.amr_sim = AMRGraphSimulator()\n",
    "        self.path_extractor = ShortestPathExtractor()\n",
    "        self.max_path_length = max_path_length\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.cache = {}  # Simple caching for repeated question-document pairs\n",
    "    \n",
    "    def process_question_document_pair(self, question: str, document: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single question-document pair efficiently.\n",
    "        \n",
    "        Returns processed AMR information ready for GNN consumption.\n",
    "        \"\"\"\n",
    "        # Create cache key\n",
    "        cache_key = hash(question + document)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Create AMR graph\n",
    "        amr_graph = self.amr_sim.create_amr_graph(question, document)\n",
    "        \n",
    "        # Extract paths and create sequence\n",
    "        paths, amr_sequence, stats = self.path_extractor.extract_paths_and_sequence(amr_graph)\n",
    "        \n",
    "        # Apply quality filters\n",
    "        filtered_paths = self._filter_paths(paths)\n",
    "        filtered_sequence = self._filter_sequence(amr_sequence)\n",
    "        \n",
    "        result = {\n",
    "            'amr_graph': amr_graph,\n",
    "            'paths': filtered_paths,\n",
    "            'amr_sequence': filtered_sequence,\n",
    "            'stats': stats,\n",
    "            'graph_info': {\n",
    "                'nodes': amr_graph.number_of_nodes(),\n",
    "                'edges': amr_graph.number_of_edges(),\n",
    "                'has_question_node': 'question' in amr_graph.nodes()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Cache result\n",
    "        self.cache[cache_key] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _filter_paths(self, paths: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"Filter paths based on quality criteria.\"\"\"\n",
    "        filtered = []\n",
    "        \n",
    "        for path in paths:\n",
    "            # Skip very short or very long paths\n",
    "            if 2 <= len(path) <= self.max_path_length:\n",
    "                # Skip paths with too many generic concepts\n",
    "                generic_concepts = {'thing', 'entity', 'something', 'anything'}\n",
    "                if not any(concept in generic_concepts for concept in path):\n",
    "                    filtered.append(path)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def _filter_sequence(self, sequence: str) -> str:\n",
    "        \"\"\"Filter AMR sequence to manageable length.\"\"\"\n",
    "        words = sequence.split()\n",
    "        if len(words) > self.max_sequence_length:\n",
    "            # Keep most important words (those appearing in multiple paths)\n",
    "            word_counts = defaultdict(int)\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "            \n",
    "            # Sort by frequency and keep top words\n",
    "            important_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            selected_words = [word for word, count in important_words[:self.max_sequence_length]]\n",
    "            return ' '.join(selected_words)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def batch_process(self, question: str, documents: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process multiple documents for a single question efficiently.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for document in documents:\n",
    "            result = self.process_question_document_pair(question, document)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Get caching statistics for performance monitoring.\"\"\"\n",
    "        return {\n",
    "            'cache_size': len(self.cache),\n",
    "            'cache_hit_potential': 'Caching enabled for repeated queries'\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the cache to free memory.\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "# Demonstrate the optimized processor\n",
    "optimized_processor = OptimizedAMRProcessor(max_path_length=8, max_sequence_length=100)\n",
    "\n",
    "# Test with sample data\n",
    "test_question = \"What is the nickname of Frank Sinatra?\"\n",
    "test_documents = [\n",
    "    \"Frank Sinatra was known for his bright blue eyes, earning him the nickname 'Ol' Blue Eyes'.\",\n",
    "    \"The singer performed at many venues and was very popular in the 1950s.\",\n",
    "    \"His blue eyes were so distinctive that fans called him 'Ol' Blue Eyes'.\"\n",
    "]\n",
    "\n",
    "print(\"OPTIMIZED AMR PROCESSING DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Process batch\n",
    "results = optimized_processor.batch_process(test_question, test_documents)\n",
    "\n",
    "print(f\"Processed {len(results)} documents for question: {test_question}\")\n",
    "print()\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"  Text: {test_documents[i][:60]}...\")\n",
    "    print(f\"  AMR Stats: {result['stats']}\")\n",
    "    print(f\"  Filtered Sequence: {result['amr_sequence'][:100]}...\")\n",
    "    print(f\"  Graph Info: {result['graph_info']}\")\n",
    "    print()\n",
    "\n",
    "# Show cache statistics\n",
    "cache_stats = optimized_processor.get_cache_stats()\n",
    "print(f\"Cache Statistics: {cache_stats}\")\n",
    "\n",
    "print(\"\\nOptimized processor ready for integration with G-RAG pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with G-RAG Pipeline\n",
    "\n",
    "### Connecting AMR Processing to Document Graph Construction\n",
    "Demonstrate how AMR path extraction feeds into the G-RAG document graph construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRToDocumentGraphBridge:\n",
    "    \"\"\"\n",
    "    Bridges AMR processing with document graph construction for G-RAG.\n",
    "    \n",
    "    This class demonstrates how the AMR path extraction feeds into\n",
    "    the document-level graph construction described in the paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, amr_processor: OptimizedAMRProcessor):\n",
    "        self.amr_processor = amr_processor\n",
    "    \n",
    "    def create_document_graph_features(self, question: str, documents: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Create document graph features using AMR processing.\n",
    "        \n",
    "        Returns data ready for GNN processing as described in paper Section 3.2.\n",
    "        \"\"\"\n",
    "        # Process all documents\n",
    "        amr_results = self.amr_processor.batch_process(question, documents)\n",
    "        \n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        # Initialize adjacency matrix and edge features\n",
    "        adjacency = np.zeros((n_docs, n_docs))\n",
    "        edge_features = np.zeros((n_docs, n_docs, 2))  # [common_nodes, common_edges]\n",
    "        \n",
    "        # Compute document connections based on shared AMR concepts\n",
    "        for i in range(n_docs):\n",
    "            for j in range(i + 1, n_docs):\n",
    "                # Get AMR graphs for both documents\n",
    "                graph_i = amr_results[i]['amr_graph']\n",
    "                graph_j = amr_results[j]['amr_graph']\n",
    "                \n",
    "                # Compute shared concepts (nodes)\n",
    "                nodes_i = set(graph_i.nodes())\n",
    "                nodes_j = set(graph_j.nodes())\n",
    "                common_nodes = len(nodes_i & nodes_j)\n",
    "                \n",
    "                # Compute shared relations (edges)\n",
    "                edges_i = set(graph_i.edges())\n",
    "                edges_j = set(graph_j.edges())\n",
    "                common_edges = len(edges_i & edges_j)\n",
    "                \n",
    "                # Create connection if documents share concepts\n",
    "                if common_nodes > 1:  # More than just 'question' node\n",
    "                    adjacency[i, j] = adjacency[j, i] = 1\n",
    "                    edge_features[i, j] = edge_features[j, i] = [common_nodes, common_edges]\n",
    "        \n",
    "        # Prepare node features (document + AMR sequence)\n",
    "        node_features = []\n",
    "        for i, (doc, amr_result) in enumerate(zip(documents, amr_results)):\n",
    "            node_features.append({\n",
    "                'document': doc,\n",
    "                'amr_sequence': amr_result['amr_sequence'],\n",
    "                'combined_text': f\"{doc} {amr_result['amr_sequence']}\",  # For embedding\n",
    "                'amr_stats': amr_result['stats'],\n",
    "                'graph_info': amr_result['graph_info']\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'adjacency': adjacency,\n",
    "            'edge_features': edge_features,\n",
    "            'node_features': node_features,\n",
    "            'amr_results': amr_results,\n",
    "            'question': question,\n",
    "            'n_documents': n_docs\n",
    "        }\n",
    "    \n",
    "    def visualize_document_connections(self, graph_data: Dict, title: str = \"Document Graph from AMR\"):\n",
    "        \"\"\"\n",
    "        Visualize the document-level graph created from AMR processing.\n",
    "        \"\"\"\n",
    "        adjacency = graph_data['adjacency']\n",
    "        edge_features = graph_data['edge_features']\n",
    "        n_docs = graph_data['n_documents']\n",
    "        \n",
    "        # Create NetworkX graph\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(n_docs))\n",
    "        \n",
    "        # Add edges with weights\n",
    "        for i in range(n_docs):\n",
    "            for j in range(i + 1, n_docs):\n",
    "                if adjacency[i, j] > 0:\n",
    "                    common_nodes = edge_features[i, j, 0]\n",
    "                    G.add_edge(i, j, weight=common_nodes)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Layout\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
    "                              node_size=1500, alpha=0.8)\n",
    "        \n",
    "        # Draw edges with thickness based on shared concepts\n",
    "        for (u, v, d) in G.edges(data=True):\n",
    "            weight = d['weight']\n",
    "            nx.draw_networkx_edges(G, pos, [(u, v)], width=weight, alpha=0.7)\n",
    "        \n",
    "        # Draw labels\n",
    "        labels = {i: f\"Doc {i}\" for i in range(n_docs)}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=12, font_weight='bold')\n",
    "        \n",
    "        # Add edge labels (shared concept counts)\n",
    "        edge_labels = {}\n",
    "        for i in range(n_docs):\n",
    "            for j in range(i + 1, n_docs):\n",
    "                if adjacency[i, j] > 0:\n",
    "                    edge_labels[(i, j)] = f\"{int(edge_features[i, j, 0])}\"\n",
    "        \n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10)\n",
    "        \n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print connection analysis\n",
    "        print(f\"\\nDocument Graph Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total documents: {n_docs}\")\n",
    "        print(f\"Connected pairs: {G.number_of_edges()}\")\n",
    "        print(f\"Graph density: {nx.density(G):.3f}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Connection Details:\")\n",
    "        for i in range(n_docs):\n",
    "            for j in range(i + 1, n_docs):\n",
    "                if adjacency[i, j] > 0:\n",
    "                    common_nodes = int(edge_features[i, j, 0])\n",
    "                    common_edges = int(edge_features[i, j, 1])\n",
    "                    print(f\"  Doc {i} ↔ Doc {j}: {common_nodes} shared concepts, {common_edges} shared relations\")\n",
    "\n",
    "# Demonstrate the bridge\n",
    "bridge = AMRToDocumentGraphBridge(optimized_processor)\n",
    "\n",
    "# Create document graph from AMR processing\n",
    "test_question = \"What is the nickname of Frank Sinatra?\"\n",
    "test_documents = [\n",
    "    \"Frank Sinatra was known for his bright blue eyes, earning him the nickname 'Ol' Blue Eyes'.\",\n",
    "    \"The famous singer Sinatra performed with blue eyes that captivated audiences worldwide.\",\n",
    "    \"Many musicians have unique characteristics, and performers often develop stage personas.\",\n",
    "    \"His distinctive blue eyes made Sinatra instantly recognizable to fans everywhere.\",\n",
    "    \"The entertainment industry has seen many talented artists throughout its history.\"\n",
    "]\n",
    "\n",
    "print(\"AMR TO DOCUMENT GRAPH BRIDGE DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create document graph features\n",
    "graph_data = bridge.create_document_graph_features(test_question, test_documents)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Processing {len(test_documents)} documents...\")\n",
    "print()\n",
    "\n",
    "# Show sample node features\n",
    "print(\"Sample Node Features:\")\n",
    "for i, nf in enumerate(graph_data['node_features'][:3]):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"  Text: {nf['document'][:60]}...\")\n",
    "    print(f\"  AMR sequence: {nf['amr_sequence'][:80]}...\")\n",
    "    print(f\"  AMR stats: {nf['amr_stats']}\")\n",
    "\n",
    "# Visualize the document graph\n",
    "bridge.visualize_document_connections(graph_data, \n",
    "                                    \"Document Graph: Sinatra Nickname Question\")\n",
    "\n",
    "print(\"\\nAMR processing successfully integrated with document graph construction!\")\n",
    "print(\"This data is now ready for the GNN-based reranking module.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights and Learning Summary\n",
    "\n",
    "### 🎯 What We've Learned:\n",
    "\n",
    "#### 1. **AMR's Role in G-RAG**\n",
    "- **Strategic Usage**: G-RAG doesn't use all AMR information (expensive), but only key paths from \"question\" node\n",
    "- **Efficiency**: This approach avoids computational overhead while preserving semantic richness\n",
    "- **Performance**: Better than using full AMR tokens (prevents overfitting)\n",
    "\n",
    "#### 2. **Shortest Path Extraction**\n",
    "- **Methodology**: Find all shortest paths from \"question\" node to other concepts\n",
    "- **Filtering**: Remove subset paths to avoid redundancy\n",
    "- **Sequence Creation**: Flatten paths into sequences for embedding\n",
    "\n",
    "#### 3. **Document Discrimination**\n",
    "- **Positive Documents**: Show structured path patterns with clear question-answer connections\n",
    "- **Negative Documents**: Either lack connections or have excessive noise\n",
    "- **Statistical Insight**: Path statistics can predict document relevance\n",
    "\n",
    "#### 4. **Implementation Considerations**\n",
    "- **Caching**: Essential for repeated question-document pairs\n",
    "- **Quality Filtering**: Remove noisy or irrelevant paths\n",
    "- **Integration**: Seamless flow to document graph construction\n",
    "\n",
    "### 🔬 Research Implications:\n",
    "\n",
    "1. **Graph-based Semantics**: AMR graphs provide richer semantic representation than text alone\n",
    "2. **Selective Information**: Using only key paths is more effective than full semantic parsing\n",
    "3. **Connection Discovery**: Path analysis reveals hidden question-answer connections\n",
    "4. **Scalable Approach**: Method can scale to large document collections\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "\n",
    "- **Notebook 2**: Document Graph Construction - How documents connect via shared concepts\n",
    "- **Notebook 3**: GNN Architecture - Graph neural networks for reranking\n",
    "- **Notebook 4**: Evaluation Metrics - Handling tied rankings from LLMs\n",
    "\n",
    "### 💡 Practical Applications:\n",
    "\n",
    "- **Search Engines**: Better semantic understanding of queries\n",
    "- **Question Answering**: Improved context retrieval\n",
    "- **Content Recommendation**: Semantic similarity beyond keywords\n",
    "- **Information Extraction**: Structured knowledge from unstructured text\n",
    "\n",
    "This deep dive into AMR processing provides the foundation for understanding how G-RAG achieves superior reranking performance through strategic semantic analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}