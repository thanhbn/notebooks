# 2501.03265v1.pdf
# Converted from PDF to TXT
# Source path: D:\llm\notebooks\AI-Papers\2501.03265v1.pdf
# File size: 1910259 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Optimizing Edge AI: A Comprehensive Survey on
Data, Model, and System Strategies
Xubin Wang1,2,3, Weijia Jia2,3∗,Fellow, IEEE
Hong Kong Baptist University1,BNU-HKBU United International College2,Beijing Normal University3
wangxubin@ieee.org, jiawj@bnu.edu.cn
Abstract —The emergence of 5G and edge computing hardware
has brought about a significant shift in artificial intelligence, with
edge AI becoming a crucial technology for enabling intelligent
applications. With the growing amount of data generated and
stored on edge devices, deploying AI models for local processing
and inference has become increasingly necessary. However, de-
ploying state-of-the-art AI models on resource-constrained edge
devices faces significant challenges that must be addressed. This
paper presents an optimization triad for efficient and reliable
edge AI deployment, including data, model, and system opti-
mization. First, we discuss optimizing data through data cleaning,
compression, and augmentation to make it more suitable for edge
deployment. Second, we explore model design and compression
methods at the model level, such as pruning, quantization, and
knowledge distillation. Finally, we introduce system optimization
techniques like framework support and hardware acceleration
to accelerate edge AI workflows. Based on an in-depth analysis
of various application scenarios and deployment challenges of
edge AI, this paper proposes an optimization paradigm based on
the data-model-system triad to enable a whole set of solutions to
effectively transfer ML models, which are initially trained in the
cloud, to various edge devices for supporting multiple scenarios.
Index Terms —Edge AI, Data Optimization, Model Optimiza-
tion, System Optimization.
I. I NTRODUCTION
FROM AlphaGo to ChatGPT, the rapid progress of AI
technology in recent years makes us marvel at its huge
potential. Simultaneously, media coverage of the threat of
artificial intelligence challenging human intelligence is in-
creasing. In fact, while the potential of AI has been shown,
there are still some gaps between AI applications and the real
world. To achieve better performance, larger datasets and more
parameters are often used to train AI models, which usually
requires a large amount of training consumption and also
makes the model complex. A typical example is the large-scale
language model GPT-3, which has 175 billion parameters and
requires about 800GB of storage [1]. Unfortunately, the high
cost of training makes it out of reach for the average person,
and only exists in labs like OpenAI with a deep accumulation.
Meanwhile, these cumbersome models are difficult to deploy
to small, resource-constrained models such as edge devices,
which are widely distributed in life. Therefore, there is an
urgent need to design efficient AI models and frameworks for
use on small devices.
With the development of communication technologies such
as 5G and the Internet of Things, the edge of the network
* Corresponding authorcan reach a variety of devices in multiple settings, including
schools, hospitals, factories, shops and homes [2]. These
widely distributed edge devices generate huge amounts of
data. According to Gartner, by 2025, about 75% of enterprise-
generated data will not come from traditional data centers
or the cloud, but from edge devices [3]. Storing and pro-
cessing these large amounts of data in the traditional cloud
will bring great system overhead and transmission bandwidth
requirements. Meanwhile, processing data at the edge is an
important requirement for some applications (eg. Smart Cities
[4], Autonomous driving [5]), even as rapid advances in net-
work technologies such as 5G bring increased communication
capabilities. Edge computing is a kind of computing mode
which is close to data source and strives to reduce transmission
delay through local computation [6]. By putting computing
on the edge, it relieves the real-time requirements that cloud
computing cannot meet in some scenarios [7].
Edge AI refers to AI algorithms deployed on edge devices
for local processing, which can process data without a network
connection. As more and more devices cannot rely on the
cloud to process data, the emergence and development of edge
AI can help alleviate such problems [8]. Especially with the
advent of the era of big data, the use of artificial intelligence
technology to improve the level of automatic processing
equipment is particularly important. For example, a prominent
feature of Industry 4.0 is smart automation, where industrial
robots need to process data at high speed with minimal latency
[9]. With the help of AI, industrial robots can process and
infer a large amount of multi-modal data from mobile devices,
sensors and Internet of Things platforms with an efficiency
beyond the reach of human beings, so as to find potential
risks and deal with them in a timely and effective manner, thus
improving the intelligence of factories [10]. In recent years,
deep learning has brought about breakthroughs in artificial
intelligence technology [11]. However, these models usually
need to consider a large number of parameters during training
and rely on high-performance multi-core processing devices
such as GPUs. For AI models to cover real-life scenario
problems, it is essential to deploy them on devices with limited
resources. At the same time, the deployment of the model
on the local device can also avoid data leakage during the
transmission process to the server, so as to meet the increasing
importance of security and privacy needs of people. Therefore,
it is necessary and practical to study efficient models that can
be deployed to resource-constrained edge devices.arXiv:2501.03265v1  [cs.LG]  4 Jan 2025

--- PAGE 2 ---
2
TABLE I
RELATED SURVEYS AND THEIR CONTRIBUTIONS
Paper Contributions Data Model System Applications Repository
Liuet al. [12] Edge AI for communication between edge devices. ✓
Chen et al. [13]Deep learning applications deployed at the edge of
networks. ✓✓✓
Letaief et al. [14] Edge AI for 6G. ✓✓
Xuet al. [15]Introduce edge intelligence from: caching, training,
inference, and offloading. ✓✓✓
Deng et al. [2]Introduce edge intelligence from: edge for AI and edge on
AI. ✓✓
Yao et al. [16]Introduce cloud AI and edge AI from the aspect of
edge-cloud collaboration. ✓✓✓
Murshed et al. [17]Deploying machine learning systems at the edge of the
network. ✓✓✓
Park et al. [18]Explore the key building blocks of wireless network
intelligence. ✓✓
Wang et al. [19]Introduce the benefits of edge intelligence and intelligent
edge. ✓✓✓
Zhou et al. [8]Introduce edge intelligence from: architecture, framework,
and key technologies. ✓✓
OursExplain how to learn an efficient model from the data,
model, and system perspectives. ✓✓✓✓ ✓
A. Related Surveys
Our main work in this survey is to provide a compre-
hensive overview of the current state-of-the-art techniques
and approaches for developing efficient models for resource-
constrained devices. Compared with the previous edge AI
survey, Shi et al. [12] discussed from the perspective of
efficient communication, Dai et al. [20] introduced from the
perspective of computation offloading, Zhang et al. talked [21]
mobile edge AI for the Internet of Vehicles, Park et al. [18]
provided an overview for wireless network intelligence, and
Letaief et al. [14] discussed edge AI for 6G. While these
surveys are essential and cover various aspects of edge AI,
they do not provide a comprehensive discussion of deploying
AI models on edge devices.
Deng et al. [2] discussed edge AI from the perspectives of
AI on edge and AI for edge, while Zhou et al. [8] provided a
comprehensive overview of relevant frameworks, technologies,
and structures for deep learning models involved in training
and reasoning on the network’s edge. Similarly, Xu et al. [22]
conducted an extensive review of edge intelligence, including
edge inference, edge training, edge caching, and edge offload-
ing. On the other hand, Hua et al. [23] introduced their survey
from the viewpoint of AI-assisted edge computing, while
Murshed et al. [17] reviewed edge AI from the perspective
of practical applications. Additionally, surveys [13] and [19]
also touched on edge inference and model deployment. Finally,
survey [24] and [25] covered the topics of model compression
and acceleration. Although some of these surveys have briefly
discussed the deployment of AI models on edge devices, none
has provided a comprehensive discussion of this crucial aspect.
Therefore, this survey aims to fill this gap and provide a
detailed and in-depth analysis of AI model deployment on
edge devices. Table 1 presents a comparison of our survey
with the existing and significant surveys on edge AI from the
data, model, system, applications, and repository aspects.
B. Our Contributions
In this survey, our focus is to provide an academic response
to the following research questions (RQs):•RQ1: What are the data challenges for building and
deploying machine learning (ML) models on edge devices
and how can we address them?
•RQ2: How can we optimize ML models for efficient
edge deployment without significantly compromising ac-
curacy?
•RQ3: What system infrastructure and tools can best sup-
port edge AI workflows and seamless model deployment
on heterogeneous edge hardware?
•RQ4: What are the applications of edge AI in daily life?
•RQ5: What are the challenges of edge AI and how can
they be mitigated and addressed?
•RQ6: What are the future trends of edge AI?
TABLE II
LIST OF ABBREVIATIONS
Acronym Explanation
5G 5thGeneration Mobile Networks
6G 6thGeneration Mobile Networks
AI Artificial Intelligence
ASIC Application Specific Integrated Circuit
CNN Convolutional Neural Network
CPU Central Processing Unit
DNN Deep Neural Network
DSP Digital Signal Processor
EC Edge Computing
Edge AI Edge Artificial Intelligence
FLOP Floating-point Operations Per Second
FPGA Field Programmable Gate Array
GAN Generative Adversarial Network
GIGO Garbage in, Garbage out
GPU Graphics Processing Unit
IoT Internet of Things
KD Knowledge Distillation
ML Machine Learning
NAS Neural Architecture Search
NLP Natural Language Processing
NPU Neural Processing Unit
RNN Recurrent Neural Network
SLAM Simultaneous Localization And Mapping
VPU Vision Processing Unit
By answering the aforementioned research questions, our
contributions can be summarized into the following six points:

--- PAGE 3 ---
3
Fig. 1. The taxonomy of the discussed topics in this survey.
1)Novel Taxonomy : We propose a novel taxonomy for
optimizing the deployment of ML models to edge
devices based on three dimensions: data, model, and
system. This “optimization triad” provides a systematic
perspective on the requirements, challenges and solu-
tions for enabling AI at the edge. The proposed triad
framework provides a conceptual advance in guiding
integrated edge AI solutions. It aims to inspire further
development of unified standards, tools, benchmarks and
best practices to accelerate progress in this space.
2)Comprehensive Review: We present a comprehensive
review of techniques for optimizing the deployment of
ML models to resource-constrained edge environments.
Enabling sophisticated AI on endpoint devices demands
overcoming constraints around data, computing, and
infrastructure through customized solutions. Our anal-
ysis provides an integrated perspective on capabilities
and open challenges at each layer of the ML-to-edge
pipeline.
3)Potential Applications : We present an in-depth analysis
of potential edge AI applications that could enhance
daily life through enhanced connectivity and intelligentpersonalization. By categorizing use cases based on
technical and experiential attributes, this review aims
to systematically uncover value propositions to motivate
further development of customized edge AI solutions.
4)Challenges and Mitigation Strategies : We analyze vari-
ous technological and social challenges confronting edge
AI that must be navigated to fulfill its promise of
improving daily life. Our review explains the limitations
and risks of edge AI, from data to models and systems,
proposing customized solutions and controls where rel-
evant.
5)Future Trends : We analyze emerging trends anticipated
to shape the continued progress of edge AI, guiding
responsible development and maximizing benefits. In
the future, edge AI is expected to have broader ap-
plications and become more intelligent, flexible, se-
cure, collaborative, and efficient. Advancements in AI
chip technology, edge computing capabilities, and new
technologies like blockchain will enable this evolution.
With improved processing power, privacy protection,
and security measures, edge devices will be able to
handle increasingly complex tasks, ensure user data

--- PAGE 4 ---
4
privacy, provide effective industry-specific solutions, and
unlock the full potential of edge AI.
6)Abundant Resources : We compile a comprehensive set of
resources on edge AI spanning backgrounds, literature,
and open source codes into an open access Github
repository available at https://github.com/wangxb96/
Awesome-AI-on-the-edge to provide researchers and
developers a foundation to build upon. By organizing
and annotating key materials in this space, our curated
collection aims to chart a roadmap of edge AI.
C. Organization
In this survey, we aim to explain how to learn efficient
models for edge deployment and edge inference from three
perspectives: data,model , and system . At the data level, we
focus on data preprocessing and improving the quality of
the trained model by removing noisy data and extracting key
features. This enables the model to effectively learn from the
data and produce accurate predictions. At the model level, we
focus on the design of the model architecture and a series of
model compression methods to further compact the model. By
reducing the model’s size, we can achieve faster inference and
reduce the computational resources required to run the model.
This is particularly important for edge AI, where resource-
constrained devices require efficient models. At the system
level, we explore the software and hardware level to accelerate
model training and inference methods. By leveraging these
techniques, we can achieve faster and more efficient model
training and inference, allowing for faster deployment and
more responsive edge AI systems.
The remaining sections of this paper are as follows: Section
2 reviews the fundamental concepts of edge computing and
edge AI. In Sections 3, 4, and 5, we explore optimizing ML
for resource-constrained edge environments from data, model
and system levels respectively (The work pipeline is shown in
Figure 2.). Section 6 shows the application scenarios of edge
AI. In Section 7, we discuss the challenges of edge AI. Finally,
we conclude the article and show the potential trends of edge
AI in Section 8. Specifically, we summarize the taxonomy of
the discussed topics of this paper in Figure 1, and the discussed
edge AI optimization triad is shown in Figure 31.
II. F UNDAMENTAL CONCEPTS
This section provides the fundamental concepts of edge
computing and edge artificial intelligence, and Figure 4 shows
the intersection between edge computing and artificial intelli-
gence and the focus of this survey.
A. Edge Computing
Cloud computing offers many benefits, including flexibility,
scalability, enhanced collaboration, and reduced costs for
modern enterprises [26]. However, cloud computing systems
are completely dependent on the Internet, and without a valid
Internet connection, users will not be able to access services.
1We list the explanations for the main abbreviations used throughout the
paper in Table 2.Additionally, since the cloud infrastructure is provided by the
cloud provider, the cloud user has limited control over appli-
cations, resources, and services. The risk of user data being
leaked in the cloud and during transmission is also noteworthy
[27]. Despite the many advantages of cloud computing, when
edge devices have real-time requirements for data processing,
the response time of modes that transport output from edge
to cloud for processing and then return may be too long,
especially in the case of an unstable network. Edge computing,
a distributed computing architecture, has been proposed to
address this issue. It moves data processing to the edge node
where the data is generated, addressing the issue of slow
response and high delay that can occur in cloud processing
[7]. Figure 5 shows the difference between cloud computing
and edge computing.
Edge application services reduce the transfer of data and aim
to keep processing locally, which alleviates problems such as
network latency and transmission overhead. Since the data is
stored and processed locally, the user has absolute ownership
of the data, which also avoids the risk of data leakage during
transmission between edge nodes and servers [28]. Edge
computing brings computing closer to the end-user and speeds
up the response time of services, which is necessary and
essential in services such as autonomous driving [5]. When
local resources are limited, the local device transmits data
to the edge network server instead of to the cloud server,
which can avoid long-distance transmission and response and
thus improve efficiency [29]. Moreover, the deployment and
access of edge devices and their ability to continue service
even when communication is slow or temporarily interrupted
ensure the scalability and reliability of edge computing [7].
The application of edge computing has been greatly successful
in many aspects, for example, IoT [30], autonomous driving
[5], smart cities [4], robotics [31], and so on.
B. Edge Artificial Intelligence
Edge artificial intelligence, or edge AI, is a combination
of edge computing and artificial intelligence. With the pro-
liferation of IoT devices, a large amount of multi-modal data
(such as audio, video, pictures, etc.) is continuously generated.
Advances in edge computing allow data on these edge devices
to be processed locally in real-time without being sent back to
the cloud, reducing latency and providing more efficient and
timely responses [7]. Artificial intelligence is an automated
technology that quickly analyzes large amounts of data to
extract information for further prediction and decision-making,
which makes it suitable for application on edge devices in
many scenarios [15]. As the computing power of edge devices
improves without a significant increase in hardware costs and
advances in algorithm-optimization techniques enable compu-
tationally demanding AI models to run on edge devices, the
generation and development of edge AI technology that meets
the requirements of real-time response is made possible [19].
As shown in Figure 6, edge AI allows data to be processed
at the local level, which greatly reduces latency between cloud
and local data processing. With less data being transferred, the
system’s bandwidth requirements and cost are reduced. More

--- PAGE 5 ---
5
Fig. 2. An overview of edge deployment. The figure shows a general pipeline from the three aspects of data, model and system. Note that not all steps are
necessary in real applications.
Fig. 3. Edge AI Optimization Triad.
Fig. 4. The intersection of edge computing and artificial intelligence. This survey focuses on edge deployment and inference in edge AI.
importantly, because data is stored and processed locally, data
security is improved, and there is less risk of data leakage.
Along with the application of AI technology, this increases
the level of automation of tasks handled by edge devices.
Furthermore, edge AI enables model training and reasoning
on edge devices, which allows real-time decisions to be made.
It also enables local decision making, which is independent
of network quality and cloud systems, further improving
the reliability of edge task execution. Edge AI is used in
a wide range of applications, including autonomous cars,
virtual reality games, smart factories, security cameras, andwearable healthcare devices [32]. Enabled by AI technology,
the automation and intelligence level of edge equipment is
enhanced.
III. D ATA OPTIMIZATION FOR EDGE AI D EPLOYMENT
Garbage in, garbage out (GIGO) is a commonly used idiom
in the computer world, which implies that poor quality data
entering a computer system will produce poor results. In indus-
try, it is widely recognized that data and features determine the
upper limit of ML, and data preprocessing methods, such as
feature engineering, play a crucial role in industrial processes.

--- PAGE 6 ---
6
Fig. 5. Cloud computing concentrates resources in centralized places like data centers, while edge computing places compute and storage resources closer to
the data source.
Fig. 6. Edge AI is the application of AI algorithms and technologies to edge computing devices to achieve faster and real-time data processing and applications.
An example of this is the GPT series, which shares a similar
basic model architecture but has seen improvements in both the
scale and quality of training data. As a result, the performance
of these models has been significantly enhanced [33]. To
improve the performance of the model in resource-constrained
devices, data preprocessing is often an essential step. Ad-
ditionally, feature compression techniques can significantly
reduce the data size, thereby reducing the model’s size and
resource requirements. By effectively preprocessing data, we
can remove noisy or irrelevant data, extract relevant features,
and normalize the data to achieve better model performance.
Common data preprocessing techniques include data cleaning,
feature selection, and feature extraction. These techniques can
be applied to various types of data, including text, image, and
time-series data. In this section, we will introduce common
data preprocessing methods and their applications in resource-
constrained environments. Figure 7 shows the operations of
data optimization.
A. Data Cleaning
Data cleaning is a crucial step in data preprocessing that
involves removing or correcting noisy or incorrect data and
removing irrelevant or redundant observations. In ML, the
presence of label noise in data can significantly impact the
accuracy of the trained model. However, re-labeling large
datasets accurately can be a challenging task particularly in
situations where resources are limited. To address this issue,
recent research has proposed innovative approaches, such as
active label cleaning, which identifies and prioritizes visiblymislabeled samples to clean up the noisy data [34]. Mishra
et al. [35] have developed an ensemble method based on
three deep learning models to handle noise labels of different
concentrations of human movement activities collected by
smartphones, which can alleviate the problem of label noise
arising from crowdsourcing or rapid labeling on the Internet.
With the proliferation of smart sensors in the IoT, vast
amounts of data are being collected. However, the harsh sensor
environment tends to introduce noise into the collected data.
To mitigate the problem that traditional sensor nodes are
not enough to handle big data, researchers have proposed
various innovative approaches. For instance, Wang et al. [36]
proposed a method of data cleaning during data collection
and optimized the model through online learning. Ma et al.
[37] proposed a federated data cleaning approach for future
edge-based distributed IoT applications while protecting data
privacy. Sun et al. [38] developed a data stream cleaning
system with the help of both the cloud servers and edge
devices. Additionally, Sun et al. [39] also proposed an adaptive
data cleaning method based on intelligent data collaboration
for filtering noise data. The work of Gupta et al. [40] proposed
a ProtoNN compression approach to reduce the model size
further by learning a small number of prototypes to represent
the training set to enable deployment on resource-scarce de-
vices. These approaches offer promising solutions for cleaning
data and enabling efficient processing of big data in resource-
constrained environments.
Discussion: While innovative approaches like active label
cleaning and ensemble methods based on deep learning models

--- PAGE 7 ---
7
Fig. 7. An overview of data optimization operations. Data cleaning improves data quality by removing errors and inconsistencies in the raw data. Feature
compression is used to eliminate irrelevant and redundant features. For scarce data, data augmentation is employed to increase the data size.
can alleviate the problem of label noise, they may have some
disadvantages. For instance, active label cleaning depends
on the availability of a small set of labeled samples, and
the performance of the approach may suffer if the labeled
samples are not representative of the entire dataset. Similarly,
ensemble methods can be computationally expensive and may
increase the risk of overfitting. In addition, some of the
proposed approaches for data cleaning in IoT environments,
such as data cleaning during data collection and federated
data cleaning, may require significant computational resources
and may not be feasible in resource-constrained environments.
Furthermore, intelligent data collaboration for filtering noise
data may require significant communication overhead, which
can be a challenge in IoT environments. Therefore, while
these approaches offer promising solutions for cleaning data
and enabling efficient processing of big data in resource-
constrained environments, they also have limitations that need
to be carefully considered.
B. Feature Compression
Feature compression is a common technique used in ML to
reduce the dimensionality of high-dimensional feature space.
Two popular methods of feature compression are feature
selection and feature extraction, which aim to remove re-
dundant and irrelevant features while retaining the necessary
information [41]. Feature selection involves choosing a subset
of relevant features from the original set while maintaining
maximum usefulness, resulting in improved model accuracy,
reduced complexity, and enhanced interpretability [42]. In
contrast, feature extraction creates new features based on the
functions of the original ones, ensuring that the newly created
features contain useful information while being non-redundant
[43]. By leveraging these techniques, researchers can compress
the feature space and improve the efficiency and performance
of their models.
With the increasing popularity and growth of computation-
ally constrained devices such as smartphones, wearables, and
IoT devices, there is a growing need to develop efficient and
effective ML algorithms for on-device analysis on these plat-
forms. Feature selection has emerged as a popular technique
for reducing the dimensionality of high-dimensional feature
spaces and improving the efficiency and accuracy of MLmodels. In recent years, researchers have applied feature se-
lection methods to various resource-constrained applications.
For example, Do et al. [44] proposed an accessible melanoma
detection method using smartphones, where they designed
a feature selection module to select the most discriminative
features for classification. Similarly, Fasih et al. [45] adopted
feature selection methods to reduce memory and computa-
tional requirements in their Active Feature Selection approach
for emotion recognition. Summerville et al. [46] designed
an ultra-lightweight deep approach based on feature selection
for anomaly detection in IoT devices. Sudhakar et al. [47]
proposed ActID, a framework for user identification based on
activity sensors, where the feature selection method is used to
evaluate and select discriminative high-quality features, thus
reducing the complexity of the algorithm and making it better
adapt to the requirements of resource-limited devices. Laddha
et al. [48] proposed a method for selecting features with
high invariance and robustness based on descriptor score to
achieve the required pose precision for real-time simultaneous
localization and mapping (SLAM) on resource-constrained
platforms.
In addition, feature selection has also been applied in other
edge environments to enhance the performance and efficiency
of ML algorithms. Several studies have demonstrated the use-
fulness of this technique in various edge-based applications,
such as Parkinson’s disease classification [49], atrial fibrilla-
tion recognition [50], data dimensionality reduction [51], fault
diagnosis on the edge of IoT [52], COVID-19 detection [53],
and more. For instance, swarm intelligence-based methods
[51], pre-training models [53], and social learning particle
swarm optimization [53] have been employed to select the
most informative features. Feature selection appears to be a
promising approach to reducing the computational complexity
of ML algorithms, enhancing their accuracy, and enabling real-
time analysis on resource-constrained devices. Therefore, its
broader adoption is expected to facilitate on-device analysis
in constrained environments and accelerate the development
of efficient and effective edge-based ML solutions.
The increasing demand for intelligent sensing and analysis
on edge has led to a growing need for efficient and effective
methods to reduce the energy and memory cost of deep
learning algorithms in resource-constrained edge computing

--- PAGE 8 ---
8
systems. To address this challenge, researchers have proposed
various feature compression methods. For example, Matsubara
et al. [54] proposed a supervised feature compression method
based on KD, while Chen et al. [55] proposed a sparse
projection method for face recognition. To better perform in-
telligent sensing at the front end, Chen et al. [56] proposed an
intermediate-layer deep learning feature compression method.
Liuet al. [57] developed a method for compressing features
along the spatiotemporal dimensions for action recognition,
while Shao et al. [58] designed a lightweight encoder-decoder
structure to reduce the size of corresponding features. Further,
Abdellatif et al. [59] proposed a lightweight classification
mechanism for detecting seizures with high accuracy and low
computational requirements at the edge of the network through
feature extraction of vital signs. Zhou et al. [60] applied image
preprocessing techniques to vision sensing and designed an
industrial wireless camera system to reduce energy consump-
tion. Similarly, Abdellatif et al. [61] designed a multi-modal
data compression and edge-based feature extraction method
for event detection. Moreno-Rodenas et al. [62] proposed a
method for monitoring wastewater pumping stations using in-
camera image processing, while Guo et al. [63] designed an
adaptive region-of-interest-based image compression scheme
for target detection.
Discussion: These feature compression methods have
shown great potential in reducing the energy and memory
cost of deep learning algorithms in resource-constrained edge
computing systems. By compressing features, these methods
enable efficient and effective analysis on edge with limited
resources, making it possible to perform intelligent sensing and
analysis in a wide range of applications. While feature com-
pression methods are promising for edge-based applications,
there is a trade-off between the size of the compressed features
and the accuracy of the resulting model. Thus, it is essential
to carefully balance the compression ratio and the accuracy
of the model to ensure the best performance on resource-
constrained devices. Moreover, further research is needed to
develop more sophisticated feature compression methods that
can better adapt to different applications and scenarios.
C. Data Augmentation
Data augmentation is a commonly used technique in ML
to increase the amount of a dataset by generating new data
through slight modifications of existing data. This technique
can be particularly useful when dealing with smaller datasets
and can help alleviate overfitting problems. In the field of
image processing, data augmentation can be achieved through
various techniques such as rotation, edge enhancement, de-
noising, and scaling of images [64]. By applying these mod-
ifications to existing images, new and diverse images can
be generated, thereby increasing the size of the dataset and
improving the performance of the model. In natural language
processing (NLP) tasks, data augmentation can be realized
by various techniques such as randomly adding or deleting
words, adjusting the order of words, auxiliary task utilization
[65], translating samples into a second language and then
translating back to form new samples, among others [66].These techniques help in generating new and diverse data,
which can be used to train better models and improve the
performance of the model on the test data.
To address the challenge of limited data availability in edge
devices, researchers have proposed various data augmentation
methods that generate new and diverse data for training ML
models. For instance, Wang et al. [67] designed a traffic
prediction method based on a 5G cellular infrastructure that
incorporates data augmentation to alleviate data shortages and
privacy issues on edge devices. Similarly, Liao et al. [68]
proposed three data augmentation methods to accelerate the
creation of a multi-user enhanced PHY layer authentication
system model. Another example is the work of Liu et al.
[69], who improved the prediction accuracy of a KITTI road
detection model by introducing appropriate data augmentation
strategies, such as adding road edge labels to small training
samples. In addition, data augmentation has been employed by
researchers to improve the generalization of images in complex
scenes, as demonstrated by Jiao et al. [70] in their method
for litchi monitoring. Gu et al. [71] proposed a line segment
detection method named M-LSD that leverages data augmen-
tation to provide auxiliary line data for the training process.
Furthermore, Liu et al. [72] utilized the data augmentation
technique to improve the performance of intrusion detection
systems in the industrial IoT by addressing the problem of data
imbalance. Pan et al. [73] expanded the amount of training
data for 1D tracking through the use of data augmentation,
which reduced the pressure of collecting more data from users.
Discussion: Although data augmentation is a powerful
technique for generating new and diverse data and improving
the performance of ML models, it has some limitations and
disadvantages. One major disadvantage is that data augmen-
tation may introduce bias or unrealistic assumptions into the
training data, which can negatively affect the performance of
the model on the test data. Moreover, the effectiveness of data
augmentation depends on the choice of augmentation tech-
niques and the specific application domain. For instance, some
augmentation techniques may not be suitable for certain types
of data, such as medical images, where introducing artificial
modifications can be risky. Additionally, data augmentation
can be computationally expensive, especially for large datasets
and complex models. Therefore, while data augmentation is
a valuable technique for improving the performance of ML
models, it is important to carefully consider its limitations and
potential drawbacks in different application scenarios.
IV. M ODEL OPTIMIZATION FOR EDGE AI D EPLOYMENT
Model optimization is a critical step in deploying ML
models to edge devices where computational resources are
limited. There are two main approaches to model optimiza-
tion: model design and model compression (as shown in
Figure 8). The former involves developing compact model
architectures and using automated neural architecture search
techniques to achieve superior performance while minimizing
the computational burden and number of model parameters.
The latter involves using methods such as pruning, parameter
sharing, quantization, knowledge distillation, and low-rank

--- PAGE 9 ---
9
factorization to shrink the size of deep learning models without
significantly affecting their accuracy or performance. These
techniques are crucial for deploying complicated models on
devices with limited resources or in large-scale distributed
systems with constrained processing, memory, and storage.
A. Model Design
Developing optimal model architectures is critical for
achieving superior performance across a range of ML applica-
tions. In this section, we will explore two approaches for ad-
dressing this challenge: the design of compact model structures
and the use of automated neural architecture search (NAS)
techniques. These strategies aim to achieve superior model
performance while minimizing the computational burden and
number of model parameters, enabling practical deployment
on various computational devices.
1) Compact Architecture Design: Compact neural network
architectures are typically characterized by their lower require-
ment for computing resources and fewer parameters. Due to
the limited computing power of edge devices, it is increasingly
important to develop neural network models that are both
efficient and compact. Therefore, in this section, we will
introduce some of the noteworthy lightweight neural network
models that have been proposed in the literature.
The rise of areas such as the IoT and edge computing, which
require processing huge amounts of data and the ability to
perform real-time analysis on edge devices, has boosted the de-
velopment of lightweight neural networks. These lightweight
neural networks typically use techniques such as convolu-
tional grouping, depth-separable convolution, width-separable
convolution, channel pruning, network pruning, and others
to compact the network architecture [74], resulting in higher
computational efficiency and lower memory consumption. For
example, the MobileNets series [75] [76] [77] is a collection
of lightweight neural networks built for mobile vision applica-
tions. These networks were developed by Google researchers
and have gained a lot of traction in the computer vision
world due to their high accuracy and minimal computing
complexity, making them perfect for usage on mobile and
embedded devices with limited resources. Moreover, Zhou et
al.[78] proposed to invert the structure and introduce a novel
bottleneck design, referred to as the ”sandglass block,” which
conducts identity mapping and spatial transformation at higher
dimensions, thereby reducing information loss and gradient
confusion more effectively. In Tan et al. [79]’s research, they
introduced an automated approach for mobile NAS that incor-
porates model latency as a crucial optimization objective to
solve the difficulty of manual solution for so many architecture
possibilities in CNN.
ShuffleNets series is a lightweight CNN proposed by
MegVII, which aims to solve the balance problem between
the accuracy and efficiency of lightweight neural networks.
The core idea of ShuffleNets is to enhance the information
flow of the network and improve its accuracy by performing
channel shuffling within groups. In ShuffleNetV1 [80], channel
shuffling is introduced, which divides the input group into
multiple sub-groups along the channel dimension and performsconvolution operations on each sub-group. The results are
then concatenated along the channel dimension. Through this
operation, ShuffleNetV1 can effectively reduce computational
complexity while improving accuracy. Based on ShuffleNetV1,
ShuffleNetV2 [81] employs a novel ShuffleNetV2 unit struc-
ture, which incorporates designs such as channel shuffling
and pointwise convolutions. This unit structure significantly
improves information flow, thereby further enhancing the
accuracy of the network. In OneShot proposed by Guo et al.
[82], it alleviates the weight adaptive problem by building a
simplified super network in which all architectures are single
paths.
SqueezeNet [83] achieves efficient information transfer with
few parameters by introducing a component named ”Fire
module,” which consists of a 1 x 1 convolutional layer called
squeeze layer and a 1 x 1 and 3 x 3 convolutional layer
called expand layer. Squeeze layer compresses the number
of channels in the input feature graph, and expand layer
increases the number of channels in the compressed feature
graph. The subsequent version of SqueezeNet, SqueezeNext,
using hardware simulation results of power consumption and
inference speed on embedded systems, showed that compared
to SqueezeNet, the model is 2.59x faster, 2.25x more energy-
efficient, and without any accuracy degradation [84]. Han et
al.[85] proposed a plug-and-play Ghost module, which tends
to generate more feature graphs through low-cost operations
to enhance feature extraction, and at the same time, it uses a
Ghost bottleneck structure to enhance the representation ability
of models.
The EfficientNet series [86] [87] [88], proposed by Tan et
al.of the Google Brain Group, are also famous efficient CNNs.
EfficientNet employs a technique called ”compound scaling,”
which adjusts not only the depth, width, and resolution of
the network when scaling it up but also the interdependent
relationships between these parameters. This results in a
more efficient and accurate network [86]. EfficientNetV2 is
an upgraded version of EfficientNet, which further improves
the performance of the network by using more efficient
network structure design and optimized training strategies,
and proposes an improved progressive learning method to
adaptively adjust the learning strategy [87]. EfficientDet is
based on EfficientNet as the backbone network and achieves
higher detection accuracy and faster inference speed through
innovative designs such as the introduction of the BiFPN
structure, carefully designed feature network hierarchy and
feature fusion mechanism, as well as optimized loss function
[88].
Huang et al. [89] proposed CondenseNet, an efficient CNN
architecture that encourages feature reuse through dense con-
nectivity and prunes filters associated with redundant feature
reuse through learned group convolutions. The pruned network
can be efficiently converted into a network with regular
group convolutions for efficient inference, which can be easily
implemented with limited computational costs during training.
Yang et al. [90] proposed an alternative scheme named Con-
denseNetV2 to improve the reuse efficiency of features. In this
approach, each layer has the capability to selectively utilize a
specific set of highly significant features from the previous

--- PAGE 10 ---
10
Fig. 8. An overview of model optimization operations. Model design involves creating lightweight models through manual and automated techniques,
including architecture selection, parameter tuning, and regularization. Model compression involves using various techniques, such as pruning, quantization,
and knowledge distillation, to reduce the size of the model and obtain a compact model that requires fewer resources while maintaining high accuracy.
TABLE III
CLASSICAL LIGHTWEIGHT NEURAL NETWORK MODELS
Family Model Highlights
MobileNetsMobileNets [75] Depth-separable convolution reduces computation while maintaining model accuracy.
MobileNetV2 [76] Introduce a novel inverted residual module with a linear bottleneck.
MobileNetsV3 [77] Explore how automatic search algorithms and network design can work together.
MobileNeXt [78] Propose a new bottleneck design named sandglass block.
MnasNet MnasNet [79] Propose an automated mobile NAS method to solve the difficulty of manual solution.
ShuffleNetsShuffleNetV1 [80] Utilize pointwise group convolution and channel shuffle.
ShuffleNetV2 [81] It introduces a novel unit structure, the ShuffleNetV2 unit.
OneShot [82] It alleviates the weight co-adaption problem by constructing a simplified super network.
SqueezeNetsSqueezeNet [83] It proposes an extremely compressed network structure design.
SqueezeNext [84] It introduces low rank filters, bottleneck module and fully connected layer.
GhostNet GhostNet [85] The ghost module and ghost bottleneck are designed to reduce the computation.
EfficientNetsEfficientNet [86] Propose a new scaling method that uses a simple and efficient compound coefficient.
EfficientNetV2 [87] Combine training perceptual neural architecture searching and scaling.
EfficientDet [88]Introduce innovative designs such as the BiFPN structure, elaborately designed feature network hierarchy and feature fusion
mechanism, as well as optimized loss functions.
CondenseNets CondenseNet [89] Encourage feature reuse and realize an efficient convolutional network architecture.
CondensenetV2 [90] An alternative method called sparse feature reactivation is proposed to improve feature reuse.
ESPNetsESPNet [91] Propose an efficient spatial pyramid (ESP) module.
ESPNetV2 [92] It’s an extension of ESPNet and uses depth-wise separable convolutions.
FBNetsFBNet [93] Use gradient-based methods to optimize the architecture of a ConvNet.
FBNetV2 [94] Propose DMaskingNAS to alleviate the problem of small search space of DNAS.
FBNetV3 [95] It takes into account the neglect of better architecture-recipe combinations in previous approaches.
PeleeNet PeleeNet [96] This is a variation of DenseNet, designed for mobile devices.
InceptionInceptionV1 [97] It improves the utilization rate of computing resources in the network.
InceptionV2 [98] It proposes batch normalization method.
InceptionV3 [99] Use convolution decomposition to improve efficiency and an efficient feature map to reduce dimension.
InceptionV4 [100] It introduces stem modules and reduction blocks.
Xception [101] Propose the use of depthwise separable convolutions.
TransformerMobileViT [102] Combine the advantages of CNNs and ViTs.
Lite-Transformer [103] Introduce a lightweight NLP architecture with Long-Short Range Attention.
Attention BasedCANet [104] Embed positional information into channel attention.
ECANet [105] Propose an Efficient Channel Attention module.
SANet [106] Feature grouping and channel attention information replacement are introduced.
Triplet attention [107] Propose triplet attention for cross-dimensional interaction.
ResNeSt [108] A modular Split-Attention block is proposed to enable attention across feature groups.
layer while concurrently updating a set of earlier features to
enhance their relevance to subsequent layers. Mehta et al.
proposed ESPNet [91] and ESPNetV2 [92], where ESPNet
reduces computation and learns representations with large
receptive fields by using point-wise convolutions and spatial
pyramid of dilated convolutions. ESPNetV2 is an extension
of ESPNet that uses depth-separable convolution and outper-
forms ESPNet by 4-5%. FBNets (Facebook-Berkeley-Nets), a
series of lightweight networks created by Facebook and UC
Berkeley, FBNet [93] uses a differentiable NAS framework to
optimize neural architecture using a gradient-based method,
while the second version FBNetV2 [94] focuses on the small
DNAS search space. The third version FBNetV3 [95], takes
into account the fact that the other approaches overlooked a
better architecture-recipe combination. PeleeNet, unlike recent
lightweight networks that heavily rely on depthwise separable
convolutions, utilizes conventional convolutions and is primar-
ily designed for deployment on mobile devices [96].
The Inception series is also a classic network proposed
by Google. The idea is to use multiple convolution kernelsof different sizes to process input data in parallel and then
concatenate their outputs along the channel dimension to form
the network output [97]. InceptionV2 uses batch normalization
and replaces large convolution kernels with small convolution
kernels [98]. In InceptionV3, the factorization into smaller
convolutions is introduced, the larger two-dimensional con-
volution is split into smaller one-dimensional convolutions,
and the Inception module structure is optimized [99]. Incep-
tionV4 introduced stem modules and reduction Blocks [100].
Xception primarily achieved complete separation of learning
spatial correlation and learning inter-channel correlation tasks
through the introduction of depthwise separable convolutions
[101].
Mehta et al. [102] proposed MobileViT to learn the global
representation of networks, which combines the advantages
of CNNs and ViTs and is superior to both. Wu et al. [103]
designed a lightweight NLP architecture, Lite-Transformer,
where the key is a Long Short Range of Attention, with one
set of heads focused on local context modeling and another
on long-distance relationship modeling. Recently, lightweight

--- PAGE 11 ---
11
networks based on attention have been proposed. For instance,
Hou et al. [104] took into account that some channel attention
studies ignored location information and embedded it into
channel attention to enhance network performance. To avoid
the complexity of the model caused by the sophisticated atten-
tion module, Wang et al. [105] designed an Efficient Channel
Attention (ECA) module that can bring clear performance
improvement with only a handful parameters involved. In
attention mechanisms, there are two types: spatial attention
and channel attention. Combining the two can improve perfor-
mance, but it inevitably leads to increased model complexity.
To address this, Zhang et al. [106] designed the Shuffle At-
tention (SA) module, which combines the advantages of both
types of attention while avoiding excessive model complexity.
Misra et al. [107] proposed triplet attention, a novel method
for efficient attention weight calculation by using a three-
branch structure to capture cross-dimensional interactions. In
the study by Zhang et al. [108], they designed a Split-Attention
block for use in ResNet, which allows attention to span
feature groups while ensuring the simplicity and ease of use
of ResNet. In addition, to help readers better understand these
lightweight networks, we summarized their characteristics in
Table 3.
Discussion: The advantages of compact architecture design
are that it produces efficient and compact neural network
models with higher computational efficiency, lower memory
consumption, and improved accuracy. These models are suit-
able for deployment on edge devices with limited resources,
making them ideal for IoT and edge computing applications.
However, designing optimal model architectures can be a
time-consuming and resource-intensive process. Additionally,
some model design techniques, such as depthwise separable
convolutions and pointwise convolutions, may be less effective
in capturing complex features compared to traditional con-
volutional layers, which may negatively impact the model’s
accuracy.
2) Neural Architecture Search: NAS aims to automate the
process of designing neural network architectures, which can
be a time-consuming and resource-intensive process when
done manually. NAS typically employs different optimization
methods such as evolutionary algorithms, reinforcement learn-
ing, or gradient-based optimization to search the space of neu-
ral architectures and identify the one that performs best on a
specific task while satisfying certain computational constraints.
Recently, with the rise of IoT and AI of Things (AIoT),
there has been a growing demand for intelligent devices with
low energy consumption, high computing efficiency, and low
resource usage. NAS has emerged as a promising approach
to design efficient and lightweight neural networks that can
be deployed on edge devices. In this section, we will discuss
various recent studies that have used NAS to design efficient
neural networks for edge computing.
One notable subject area in current studies is the employ-
ment of advanced search algorithms and multi-objective opti-
mization approaches to identify neural network architectures
that optimize different performance metrics such as accuracy,
resource consumption, and power efficiency. This work has
become increasingly important in recent years as edge com-puting applications require efficient yet accurate models. To
this end, researchers have proposed various approaches for
multi-objective architecture search. Lu et al. [109] and Lyu et
al.[110] are two such studies that employed multi-objective
optimization to identify efficient and accurate neural network
architectures for edge computing applications. Similarly, Chen
et al. [111] used performance-based strategies to search ef-
ficiently for architectures that are optimal with regard to
multiple objectives. These studies underscore the significance
of considering different objectives during the architecture
search process to ensure that the neural architectures are both
accurate and efficient. By using advanced search algorithms
and multi-objective optimization techniques, researchers can
design models that are effective in resource-constrained en-
vironments while still maintaining high accuracy, which is
crucial for edge computing applications.
The innovative techniques and algorithms to improve the
efficiency and effectiveness of NAS are often employed in
the research. For example, Mendis et al. [112] incorporated
intermittent execution behavior into the search process to find
accurate network architectures that can safely and efficiently
execute under intermittent power, which is a common chal-
lenge in edge computing applications. Similarly, Ning et al.
[113] identified factors that affect the fault resilience capability
of neural network models and used this knowledge to design
fault-tolerant CNN architectures for edge devices. These stud-
ies demonstrate the importance of considering unique chal-
lenges and constraints of edge computing applications when
designing neural networks through NAS. By incorporating
innovative techniques and algorithms, researchers can develop
architectures that are not only efficient and accurate but also
robust and reliable.
Furthermore, several studies proposed hardware-efficient
primitives and frameworks to optimize the search process and
improve the performance of the resulting networks. For in-
stance, Liu et al. [114] proposed the Point-V oxel Convolution
(PVConv) primitive, which combines the best of point-based
and voxel-based models for efficient NAS. This hardware-
efficient primitive achieves state-of-the-art performance with
significant speedup and has been successfully deployed in
real-world edge computing scenarios, such as an autonomous
racing vehicle. Donegan et al. [115] proposed the use of a
differentiable NAS method to find efficient CNNs for Intel
Movidius Vision Processing Unit (VPU), achieving state-of-
the-art classification accuracy on ImageNet. These studies
highlight the importance of considering hardware efficiency
when designing neural networks for edge computing applica-
tions. By leveraging hardware-efficient primitives and frame-
works, researchers can not only optimize the search process
but also develop neural architectures that are efficient and
effective in resource-constrained environments.
Moreover, some studies propose novel NAS approaches
that strictly adhere to resource constraints, while others fo-
cus on addressing non-i.i.d. data distribution in federated
learning scenarios. For instance, Nayman et al. [116] intro-
duced HardCoRe-NAS, which is a constrained NAS approach
that strictly adheres to multiple resource constraints such
as latency, energy, and memory, without compromising the

--- PAGE 12 ---
12
accuracy of the resulting networks. Similarly, MemNAS [117]
is a framework that optimizes both the performance and
memory usage of NAS by considering memory usage as an
optimization objective during the search process. On the other
hand, Zhang et al. [118] focused on addressing non-i.i.d.
data distribution in federated learning scenarios by proposing
the Federated Direct NAS (FDNAS) and Cluster Federated
Direct NAS (CFDNAS) frameworks. These frameworks lever-
age advanced proxylessNAS and meta-learning techniques to
achieve device-aware NAS, tailored to the particular data
distribution and hardware constraints of each device. These
studies demonstrate the importance of considering various
constraints and challenges in the design of neural networks
for edge computing applications.
Discussion: While NAS has shown promise in designing ef-
ficient and lightweight neural networks for edge computing ap-
plications, there are still some disadvantages to this approach.
One major limitation is that NAS can be computationally
expensive, especially when searching through a large space of
possible architectures. This can make it challenging to deploy
NAS-based models in resource-constrained environments, par-
ticularly those with limited computational power. Additionally,
while NAS can optimize for multiple objectives, it can be
difficult to find a balance between accuracy and efficiency,
especially when dealing with complex tasks or non-i.i.d. data
distribution. Furthermore, the resulting neural architectures
may not be easily interpretable, making it challenging to
understand how they work or explain their decisions.
B. Model Compression
Model compression is a group of methods (such as pruning,
parameter sharing, quantization, knowledge distillation and
low-rank factorization) for shrinking the size of deep learn-
ing models without significantly affecting their accuracy or
performance by eliminating extraneous components, such as
duplicated parameters or layers. Due to deep learning models’
high computational and storage needs, model compression has
become more and more crucial. These methods are created to
make it possible to deploy complicated models on devices
with limited resources or in large-scale distributed systems
with constrained processing, memory, and storage. To enhance
the efficacy and efficiency of deep learning models in various
applications, it is possible to employ these techniques either in
isolation or in conjunction. For instance, a classic example of
combining multiple techniques is Deep Compression, which
combines techniques such as pruning, quantization, and Huff-
man coding to achieve significant compression of deep neural
networks (DNN) [119].
1) Model Pruning: DNN models typically consist of nu-
merous parameters and hierarchies, making them computation-
ally and storage-intensive. Due to their frequent application in
scenarios with limited resources, such as mobile devices, it is
imperative that these models are smaller in size and require
less computational power to perform optimally. Pruning is
a prevalent model compression technique that reduces the
model’s size by eliminating extraneous layers or parameters,
thereby enhancing its efficiency and reasoning speed. Addi-tionally, pruning helps to prevent overfitting and bolsters the
model’s ability to generalize.
In recent years, there has been a growing interest in de-
veloping pruning techniques for AI models to reduce their
size and improve their efficiency, particularly for deployment
in resource-constrained environments. Various research efforts
have been undertaken to address this challenge. For instance,
Xuet al. [120] developed a framework named DiReCtX,
which includes improved CNN model pruning and accuracy
tuning strategies to achieve fast model reconfiguration in real-
time, resulting in significant computation acceleration, mem-
ory reduction, and energy savings. Ahmad et al. [121] pro-
posed SuperSlash, which utilizes a pruning technique guided
by a ranking function to significantly reduce off-chip memory
access volume compared to existing methods. DropNet [122]
is an iterative pruning method that reduces the complexity
of DNNs by removing nodes/filters with the lowest average
post-activation value across all training samples. The findings
suggest that DropNet can achieve significant pruning (up to
90% of nodes/filters) without sacrificing accuracy, and the
pruned network remains effective even after weight and bias
reinitialization. Interestingly, Li et al. [123] demonstrated in
their study that heavily compressed large models achieve
higher accuracy than lightly compressed small models. Ma
et al. [124] proposed gradient-based pruning strategies to
improve performances across languages.
Structural pruning is a prominent technique in pruning that
involves removing entire structures or modules from a neural
network. One approach to structural pruning is the method
developed by Gao et al. [125], which uses an efficient discrete
optimization method to directly optimize channel-wise differ-
entiable discrete gates under resource constraints while freez-
ing all other model parameters. This results in a compact CNN
with strong discriminative power. Wu et al. [126] proposed
MOBC, a pruned reinforcement learning-based approach with
a structured pruning method that adaptively reduces block
migrations and foreground segment cleanings in log-structured
file systems, resulting in improved storage endurance and
reduced latency with lower overheads. Furthermore, structural
pruning has also been utilized in the development of federated
learning frameworks. NestFL [127] is a learning-efficient
federated learning framework that improves training efficiency
and achieves personalization by assigning sparse-structured
subnetworks to edge devices through progressively structured
pruning.
Dynamic pruning is a commonly used technique in neural
networks that involves pruning during the training process. The
technique evaluates the importance of neural network weights
or neurons and selectively removes unimportant weights or
neurons. Geng et al. [128] developed an out-of-order ar-
chitecture called O3BNN-R that uses dynamic pruning to
significantly reduce the size of Binarized Neural Networks,
enabling efficient computation on cost- and power-restricted
edge devices. This technique improves the efficiency of neural
network tasks on edge devices, making them more practi-
cal. Another novel dynamic pruning technique is FuPruner
[129]. The approach optimizes both parametric and nonpara-
metric operators to accelerate neural networks on resource-

--- PAGE 13 ---
13
constrained edge devices. The approach uses an aggressive
fusion method to transform the model and a dynamic filter
pruning method to reduce computational costs while retaining
accuracy. Gu et al. [130] demonstrated that their proposed
mixed-training strategy, which combines two-level sparsity
and power-aware dynamic pruning, achieves superior opti-
mization stability, higher efficiency, and significant power
savings compared to existing methods. Some researchers have
focused on pruning after training, such as Kwon et al. [131],
who proposed a framework for pruning Transformers after
training, achieving significant reduction in FLOPs and infer-
ence latency while maintaining accuracy.
Several researchers have proposed jointly performing prun-
ing and other model compression methods to improve the
efficiency of neural networks. Lin et al. [132] introduced
HRank, a filter pruning method that prunes filters with
low-rank feature maps, resulting in significant reductions in
FLOPs and parameters while maintaining similar accuracies.
Liet al. [133] proposed a novel pruning technique called
kernel granularity decomposition, which combines low-rank
approximation with redundancy exploitation to achieve model
compactness and hardware efficiency simultaneously. Tung
et al. [134] developed CLIP-Q, a novel approach that com-
bines network pruning and weight quantization in a single
learning framework. Fedorov et al. [135] designed a Sparse
Architecture Search method that combines NAS with pruning
to automatically design CNNs that can fit onto memory-
limited MCUs while maintaining high prediction accuracy.
Khaleghi et al. [136] proposed a quantization and pruning
technique for hyperdimensional computing to achieve privacy-
preserving training and inference while obfuscating informa-
tion and enabling efficient hardware implementation. These
techniques demonstrate the effectiveness of jointly performing
pruning and other model compression methods to improve the
efficiency of neural networks while maintaining their accuracy.
Other approaches have incorporated context-aware pruning,
such as Huang et al. [137] with DeepAdapter, which im-
proves inference accuracy with a smaller and faster model,
and Liu et al. [138], who devised a novel content-aware
channel pruning approach for unconditional GANs that sig-
nificantly reduces the FLOPs of StyleGAN2 by 11x with
visually negligible image quality loss compared to the full-
size model. In addition, researchers have explored the use
of pruning for specific applications, such as radio frequency
fingerprinting tasks [139], super-resolution networks [140],
and privacy preserved hyperdimensional computing [136].
Moreover, some researchers have proposed hardware-software
co-design approaches for pruning, such as Sun et al. [141],
who presented a hardware-aware pruning technique for a novel
3D network called R(2+1)D that achieves high accuracy and
significant computation acceleration on FPGA.
Discussion: While model pruning can improve the effi-
ciency and speed of neural networks, there are also some
disadvantages to this approach. One major limitation is that
pruning can result in a loss of model accuracy, especially when
a significant number of parameters or layers are removed.
Additionally, pruning can be computationally expensive, par-
ticularly when searching for the optimal set of parameters toprune. Pruning can also result in a less interpretable model, as
the removed parameters or layers may have played a critical
role in the network’s decision-making process. Finally, some
pruning methods may not be compatible with certain hardware
or software configurations, limiting their practicality.
2) Parameter Sharing: Parameter sharing is a model com-
pression technique that involves sharing the weights of a neural
network among multiple layers. This approach results in a
significant reduction in the number of parameters required
to represent the model, thus reducing its computational and
memory requirements. Consequently, the model can be better
suited for deployment on resource-constrained devices. This
technique has been successfully applied to various deep learn-
ing architectures, including CNNs and RNNs.
Several studies have explored different parameter sharing
techniques to achieve high compression rates with minimal or
no loss of accuracy. For instance, Wu et al. [142] proposed
a novel scheme for compressing CNNs by applying k-means
clustering on the weights to achieve parameter sharing. The
proposed method includes a spectrally relaxed k-means reg-
ularization to make hard assignments of convolutional layer
weights to learned cluster centers during re-training, and is
evaluated across several CNN models demonstrating promis-
ing results in terms of compression ratio and energy consump-
tion reduction without incurring accuracy loss. Obukhove et
al.[143] designed T-Basis, a compact representation of a set
of tensors modeled using Tensor Rings for efficient neural
network weight compression while allowing for weight shar-
ing. T-Basis achieves high compression rates with acceptable
performance drops and is well-suited for resource-constrained
devices. In Ullrich et al. [144]’s research, they employed a
version of soft weight-sharing to realize competitive com-
pression rates. You et al. [145] proposed ShiftAddNAS, a
NAS method for hybrid neural networks that integrate both
powerful multiplication-based and efficient multiplication-free
operators. It highlights a novel weight sharing strategy that
effectively shares parameters among different operators with
heterogeneous distributions, leading to a largely reduced su-
pernet size and better searched networks. These methods have
shown promising results in achieving high compression rates
with minimal or no loss of accuracy. In research [146], the
proposed methods incorporate parameter sharing among candi-
date architectures to enable efficient search over a large design
space and consistently outperform manual design and random
search approaches, achieving up to 1.0% absolute word error
rate reduction and 28% relative model size reduction on the
Switchboard corpus.
Moreover, many studies have applied parameter shar-
ing methods to practical applications. For example, Effi-
cientTDNN [147] is a novel speaker embedding architecture
search framework that employs weight-sharing subnets to
efficiently search for TDNN architectures, achieving a favor-
able trade-off between accuracy and efficiency with parameter
sharing, as demonstrated on the V oxCeleb dataset. CpRec
[148] is a compressed sequential recommendation framework
that employs block-wise adaptive decomposition and layer-
wise parameter sharing schemes to reduce the number of
parameters in sandwich-structured DNNs used in sequential

--- PAGE 14 ---
14
recommender systems. Sindhwani et al. [149] proposed a
unified framework for learning structured parameter matrices
with low displacement rank, enabling a rich range of pa-
rameter sharing configurations that offer superior accuracy-
compactness-speed tradeoffs for mobile deep learning.
Discussion : While parameter sharing can lead to significant
reductions in the number of parameters required to represent
a neural network, there are also several disadvantages to this
approach. One major limitation is that parameter sharing can
result in a loss of model accuracy, particularly when the
shared parameters are not well-suited for the task at hand.
Additionally, parameter sharing may not be compatible with
certain types of neural networks or architectures, limiting its
applicability. Another potential issue is that parameter sharing
can result in a less interpretable model, as it may be more
difficult to understand how the shared parameters contribute
to the network’s decision-making process. Finally, some pa-
rameter sharing methods may be computationally expensive or
require significant computational resources, particularly when
searching for the optimal set of shared parameters.
3) Model Quantization: The quantization technique has be-
come increasingly important for optimizing DNN models for
deployment on resource-constrained devices. It offers multiple
benefits, including improved computational efficiency, reduced
memory and storage usage, and lower power consumption. By
reducing the precision of model parameters and activations,
quantization enables a significant reduction in model size
while minimizing the impact on task accuracy. It is a widely
adopted technique in the field of deep learning and has been
shown to offer significant improvements in model efficiency
and performance on a range of hardware platforms.
Recently, there has been a growing interest in applying
quantization techniques to design lightweight models for edge
devices. This trend is driven by the need to improve the effi-
ciency and performance of ML on edge devices, which often
have limited resources. One such technique is progressive frac-
tional quantization and dynamic fractional quantization, which
were proposed in FracTrain by Fu et al. [150]. This method
reduces the computational cost and energy/latency of DNN
training while maintaining comparable accuracy. Similarly,
edgeBERT [151] is a hardware system that employs a combi-
nation of adaptive attention span, selective network pruning,
and floating-point quantization to alleviate computation and
memory footprint overheads on resource-constrained edge
platforms. PNMQ is a data-free method for network compres-
sion using Parametric Non-uniform Mixed precision Quan-
tization, which allows efficient implementations for network
compression on edge devices without requiring any model
retraining or expensive calculations [152]. Another technique
for quantized deep neural networks (QDNNs) is SPEQ [153],
a novel stochastic precision ensemble training method that
employs KD without the need for a separate teacher network.
Moreover, Cui et al. [154] proposed a quantization-based
approach for reducing the storage and memory consumption
of deep ensemble models, using a differentiable and paralleliz-
able bit sharing scheme that allows the members to share less
significant bits of parameters, while maintaining accuracy, and
an efficient encoding-decoding scheme for real deployment onedge devices.
Some studies have also explored the quantization of other
neural networks. For example, Capsule Networks (CapsNets)
have been shown to outperform traditional CNNs in image
classification, but they are computationally intensive and chal-
lenging to deploy on resource-constrained edge devices. A
specialized quantization framework for CapsNets has been
developed to enable efficient edge implementations, which can
reduce the memory footprint by 6.2x with minimal accuracy
loss [155]. Moreover, FSpiNN is a memory and energy-
efficient framework for spiking neural networks (SNNs) that
incorporates fixed-point quantization while maintaining accu-
racy, making it suitable for deployment on edge devices with
unsupervised learning capability [156].
Hardware-based quantization design are also a recent re-
search hotspot. Zhou et al. [157] proposed an INT8 training
method implemented in Octo, a lightweight cross-platform
system that achieves higher training efficiency and memory
reduction over full-precision training on commercial AI chips.
Similarly, Wang et al. [158] introduced the Hardware-Aware
Automated Quantization (HAQ) framework, which determines
the optimal quantization policy for each layer in a DNN based
on the hardware architecture, resulting in reduced latency
and energy consumption without significant accuracy loss. Li
et al. [159] devised a novel quantization framework, RaQu,
that combines information about neural network models and
hardware structures to improve resource utilization and com-
putation efficiency on resistive-memory-based processing-in-
memory (RRAM-based PIM) for edge devices. Additionally,
a hardware/software co-design solution is proposed via an
inexact multiplier and a retraining strategy to quantize neural
network weights to Fibonacci encoded values for computa-
tionally demanding algorithms on resource-constrained edge
devices [160].
Discussion: Model quantization is a useful technique for
improving the efficiency and performance of deep neural
networks on resource-constrained devices. However, it also has
some drawbacks. One major limitation is that quantization can
result in a loss of model accuracy, especially if the precision
of model parameters and activations is reduced too much.
Additionally, finding the optimal set of precision levels can be
computationally expensive. Reduced precision can also make
the model less interpretable, making it difficult to comprehend
how the model is making its decisions. Moreover, certain
hardware or software configurations may not be compatible
with some quantization methods, limiting their practicality.
4) Knowledge Distillation: KD is a model compression
technique that aims to reduce the size and computational
cost of DNNs by transferring knowledge from a large and
complex teacher model to a smaller and simpler student model.
It works by softening the teacher’s output into a probability
distribution and using it to train the student model to mimic the
teacher’s behavior. This technique allows for the compression
of complex models while still maintaining their performance,
making them more efficient and practical for deployment in
real-world applications.
KD, introduced by Geoffrey Hinton et al. [161], has been
effectively employed in various domains. One approach is the

--- PAGE 15 ---
15
self-distillation framework proposed by Zhang et al. [162] that
enhances the performance of CNNs by compressing the knowl-
edge within the network. This framework improves accuracy
while providing depth-wise scalable inference on resource-
limited edge devices. Another approach is the DynaBERT
model introduced by Hou et al. [163], which is a dynamically
adjustable BERT model that can adapt to the requirements
of different edge devices by selecting adaptive width and
depth. The training process involves KD from the full-sized
model to small sub-networks, resulting in superior perfor-
mance compared to existing BERT compression methods.
Zhang et al. [164] proposed the SCAN framework, which
divides DNNs into multiple sections and constructs shallow
classifiers using attention modules and KD. This framework’s
threshold-controlled scalable inference mechanism allows for
sample-specific inference, resulting in significant performance
gains on CIFAR100 and ImageNet. In addition, Zhang et al.
[165] proposed the dynamic knowledge distillation (DKD)
framework for deep CNNs, which leverages a dynamic global
distillation module for multiscale features imitation and a dy-
namic instance selection distillation module for self-judgment.
The framework also tailors a training-status-aware loss to
handle hard samples in regression, enabling the deployment
of models on low computation edge devices such as satellites
and unmanned aerial vehicles. Hao et al. [166] designed the
CDFKD-MFS framework, which compresses multiple pre-
trained models into a tiny model for resource-limited edge
devices without requiring the original dataset. This framework
utilizes a multi-header student module, an asymmetric adver-
sarial data-free KD module, and an attention-based aggrega-
tion module. Additionally, Hao et al. [167] proposed a fine-
grained manifold distillation method for transformer-based
networks to compress the architecture of vision transformers
into compact students, achieving high accuracy with lower
computational costs. Zhang et al. [168] used a comparable
model to teach lexical knowledge to its counterpart model,
achieving significant performance gain. Furthermore, Shen et
al.[169] used two models to teach each other and reach
the trade-off between the two models. Their results show the
teacher models in KD don’t necessarily have to be larger or
much stronger models.
In addition, various other KD methods have been pro-
posed for different applications. For example, Liu et al.
[138] proposed novel content-aware KD and effective chan-
nel pruning schemes specialized for unconditional GANs,
achieving a substantial improvement over the state-of-the-
art compression method. Ni et al. [170] introduced an end-
to-end Vision-to-Sensor KD (VSKD) framework for human
activity recognition (HAR) based on a multi-modal approach,
which reduces computational demands on edge devices and
produces a learning model that closely matches the per-
formance of the computational expensive approach, using
only time-series data. Jin et al. [171] presented a Personal-
ized Federated Learning (PFL) framework for edge devices,
named pFedSD, which utilizes self-KD to train models that
perform well for individual clients. By distilling knowledge
from previous personalized models, pFedSD accelerates the
process of recalling personalized knowledge and provides animplicit ensemble of local models. Li et al. [172] developed
an instance-specific multi-teacher KD model (IsMt-KD) for
distracted driver posture classification on embedded systems
with limited memory space and computing resources. This
model utilizes an instance-specific teacher grading module
to dynamically assign weights to teacher models based on
individual instances, achieving high accuracy and real-time
inference on edge hardware platforms.
KD methods can be combined with other model compres-
sion approaches to further improve the performance of DNNs
on edge devices. For instance, Boo et al. [153] proposed a
stochastic precision ensemble training scheme for QDNNs
that utilizes KD with a continuously changing teacher model
formed by sharing the student network’s parameters. This
allows for improved performance in various edge device tasks
without the need for cumbersome teacher networks. Xia et
al.[173] presented an on-device recommender system for a
session-based recommendation that uses ultra-compact models
and a self-supervised KD framework to address the challenges
of limited memory and computing resources. The compressed
model achieves a 30x size reduction with almost no accuracy
loss and even outperforms its uncompressed counterpart. Xu
et al. [174] devised a lightweight Identity-aware Dynamic
Network (IDN) for subject-agnostic face swapping on edge
devices, which utilizes an efficient Identity Injection Module
(IIM) and a KD-based method for stable training. More-
over, the proposed lightweight SegFormer model in [175]
for efficient semantic segmentation on edge devices utilizes
a dynamic gated linear layer to prune uninformative neurons
based on input instance and a two-stage KD to transfer
knowledge from the original teacher to the pruned student
network, achieving more than 60% computation savings with
a minimal drop in mIoU.
Discussion: KD is a powerful method for compressing
complex models while maintaining their performance, en-
abling efficient deployment on resource-limited edge devices.
However, it can result in a loss of model accuracy if the
precision level is not appropriately chosen, and it can be
computationally expensive, particularly when dealing with
large datasets or complex models. Additionally, while there
are many successful applications of KD, its effectiveness can
vary depending on the specific domain and task, and it may
require careful tuning and experimentation to achieve optimal
results.
5) Low-rank Factorization: DNNs often require high mem-
ory consumption and large computational loads, which limits
their deployment on edge or mobile devices. Low-rank fac-
torization is a method that can help by approximating weight
matrices with low-rank matrices, finding a lower-dimensional
representation of the data that retains the most important
information. For example, SVD training [176] is a new method
that achieves low-rank DNNs during training without applying
SVD on every step, using sparsity-inducing regularizers on
singular values. This method achieves a higher reduction
in computation load under the same accuracy compared to
previous factorization and filter pruning methods. Efforts have
also been made to apply low-rank factorization on resource-
constrained edge devices. MicroNet [177] is an efficient CNN

--- PAGE 16 ---
16
designed for edge devices, achieving low computational cost
by using Micro-Factorized convolution that factorizes point-
wise and depthwise convolutions into low-rank matrices. The
network compensates for network depth reduction with the
introduction of the Dynamic Shift-Max activation function.
MicroNet-M1 achieves 61.1% top-1 accuracy on ImageNet
classification with 12 MFLOPs, outperforming MobileNetV3
by 11.3%.
Discussion: Low-rank factorization is a promising approach
for reducing the computational and memory requirements
of DNNs, making them more practical for deployment on
resource-limited edge devices. However, implementing low-
rank factorization can be challenging due to the high compu-
tational cost of the factorization operation, and the need for
extensive retraining to achieve convergence.
V. S YSTEM OPTIMIZATION FOR EDGE AI D EPLOYMENT
As the demand for real-time performance and resource-
efficient deep learning models increases, system optimization
has become a crucial area of research. To address the need
for deploying deep learning models on edge devices, it is
necessary to optimize their computational efficiency. In this
section, we present frameworks for lightweight model training
and inference from a software perspective, as well as methods
for accelerating models using hardware-based approaches. The
workflow of system optimization is shown in Figure 9.
A. Software Optimization
1) Edge AI Learning Frameworks: PyTorch and Tensor-
Flow are widely-used deep learning frameworks, but they
may not be suitable for mobile applications due to their
relative heaviness and third-party dependencies, which can be
problematic for mobile devices. However, with the evolution
and development of the AI ecosystem, these frameworks have
been specifically designed for mobile deep learning through
TensorFlow Lite and PyTorch Mobile, allowing for efficient
training and deployment on mobile devices.
Both TensorFlow Lite and PyTorch Mobile are lightweight
deep learning frameworks designed specifically for mobile
applications. They provide a more streamlined development
process and facilitate effective model training and deployment
on mobile devices. The restricted computing and memory
capabilities of mobile devices have been taken into account
in the optimization of these frameworks for them. This op-
timization makes the models suitable for edge computing
scenarios as they can be trained and deployed on mobile
devices with less resource consumption. TensorFlow Lite and
PyTorch Mobile have a range of features that are specifically
designed for mobile applications. Specifically, some features
of these two frameworks are highlighted in Table 4. Moreover,
TensorFlow Lite is found to perform better with lightweight
deep learning models and is suitable for edge computing
applications, especially for mobile devices [178].
2) Edge AI Inference Frameworks: Lightweight model in-
ference is becoming increasingly important for applications at
the edge, where computational resources are often limited. To
address this, a number of software frameworks have emergedthat allow for efficient inference of lightweight models, such as
NCNN, OpenVINO and ONNX Runtime. These frameworks
typically provide optimized implementations of common oper-
ations and architectures, and can run on a variety of hardware
platforms, including IoT devices, mobile devices, and edge
servers. In Table 5, we have presented a list of commonly
used AI inference frameworks, including their manufacturers,
supported hardware, advantages, and limitations.
Efficient AI inference frameworks for edge devices have
seen rapid progress in academic research recently, with a focus
on deploying CNN models in resource-constrained scenarios.
Xia et al. [179] introduced a lightweight neural network
architecture called SparkNet, which reduces weight parameters
and computation demands for CNN feedforward inference on
edge devices. Memsqueezer, an on-chip memory architecture
for deep CNN inference acceleration on mobile/embedded
devices, was designed by Wang et al. [180], achieving 2x
performance improvement and 80% energy reduction with
compression and redundancy detection. Wang et al. [181]
presented an end-to-end solution for CNN model inference
on integrated GPUs at the edge, using a unified IR and
ML-based scheduling search schemes. Pipe-it, a pipelined
framework limiting parallelization of convolution kernels to
assigned clusters, was proposed by Wang et al. [182] for
CNN inference on ARM big.LITTLE architecture in edge
devices. SCA, a secure CNN accelerator using stochastic
computing to protect models and weights during both training
and inference phases, was devised by Zhao et al. [183],
achieving significant speedup and energy reduction over non-
secure and inference-only secure baselines. Hou et al. [184]
developed NeuLens, a dynamic CNN acceleration framework
for mobile and edge platforms that achieves significant latency
reduction and accuracy improvement using a novel dynamic
inference mechanism and operation reduction compatible with
hardware-level accelerations.
Studies have also focused on deploying recurrent neu-
ral network (RNN) models. For example, Srivastava et al.
[185] proposed an efficient RNN compression method for
human action recognition, which uses a Variational Informa-
tion Bottleneck theory-based pruning approach and a group-
lasso regularization technique to significantly reduce model
parameters and memory footprint. EdgeDRNN was designed
by Gao et al. [186] for edge RNN inference with a batch
size of 1, adopting the delta network algorithm to exploit
temporal sparsity in RNNs to realize high performance and
power efficiency. Wen et al. [187] developed a structured
pruning method through neuron selection to reduce the overall
storage and computation costs of RNNs, achieving significant
practical speedup during inference without performance loss.
Zhang et al. [188] proposed DirNet, a model compression
approach for RNNs that dynamically adjusts the compression
rate and sparsity of sparse codes across hierarchical layers
while maintaining minimum accuracy loss.
Numerous works have explored the deployment of DNNs on
edge devices to optimize performance and resource utilization.
For example, Ding et al. [197] proposed a new task-mapping
programming paradigm to embed scheduling into tensor pro-
grams for efficient DNN inference and introduced the Hidet

--- PAGE 17 ---
17
Fig. 9. An overview of system optimization operations. Software optimization involves developing frameworks for lightweight model training and inference,
while hardware optimization focuses on accelerating models using hardware-based approaches to improve computational efficiency on edge devices.
TABLE IV
EDGE AI L EARNING FRAMEWORKS
Framework Producer Highlights
TensorFlow Lite Google•Optimization of on-device ML by addressing: latency, privacy, connectivity, size, and power consumption
•Support for multiple platforms including Android, iOS, embedded Linux, and microcontrollers
•Support for multiple programming languages, including Java, Swift, Objective-C, C++, and Python
•High-performance with hardware acceleration and model optimization support
•Examples for common ML tasks on various platforms, including image/object/text classification
Pytorch Mobile Facebook•Works on iOS, Android, and Linux platforms
•Provides APIs for common preprocessing and integration tasks
•Supports tracing and scripting via TorchScript IR
•Offers XNNPACK floating point and QNNPACK 8-bit quantized kernels for Arm CPUs
•Provides an efficient mobile interpreter, build level optimization, and streamline model optimization via optimize formobile
TABLE V
EDGE AI I NFERENCE FRAMEWORKS
Framework Producer Supported Hardware Advantages Limitations
ONNX Runtime [189] Microsoft CPU, GPU, etc•It has built-in optimizations that can boost inferencing
speed up to 17 times and training speed up to 1.4 times
•It supports multiple frameworks, operating systems, and
hardware platforms
•High performance, and low latency•Limited support for non-ONNX mod-
els
•No support for some hardware back-
ends
OpenVINO [190] Intel CPU, GPU, VPU, FPGA, etc•It optimizes deep learning pipelines for high perfor-
mance and throughput
•Support for advanced functions such as FP16, INT8
quantization
•It supports multiple deep learning frameworks and
multiple operating systems•Only Intel hardware products are sup-
ported
•Deploying and integrating models still
requires some technical knowledge
and experience
NCNN [191] Tencent CPU, GPU, etc•High performance and low memory usage
•Supports a variety of hardware devices and model
formats
•Supports 8-bit quantization and ARM NEON optimiza-
tion•Limited support for non-NCNN mod-
els
Arm NN [192] Arm CPU, GPU, etc•Cross platform
•Supports a variety of hardware devices and model
formats
•Existing software can automatically take advantage of
new hardware features
•Support for ARM Compute Library•Limited support for operators and net-
work structures
MNN [193] Alibaba CPU, GPU, NPU•MNN is a lightweight, device-optimized framework
with quantization support
•MNN is versatile, supporting various neural networks
and models, multiple inputs/outputs, and hybrid com-
puting on multiple devices
•MNN achieves high performance through optimized
assembly, GPU inference, and efficient convolution
algorithms.
•MNN is easy to use, with support for numerical calcu-
lation, image processing, and Python API•Limited community support
•Technical expertise required
TensorRT [194] NVIDIA CPU, GPU•Maximize throughput by quantifying the model to INT8
while maintaining high accuracy
•Optimize GPU video memory and bandwidth usage by
merging nodes in the kernel
•Select the best data layer and algorithm based on the
target GPU platform
•Minimize video memory footprint and efficiently reuses
memory for tensors
•An extensible design for processing multiple input
streams in parallel•It only runs on NVIDIA graphics cards
•It does not open source the kernel
TVM [195] Apache CPU, GPU, DSP, etc•Compilation and minimal runtimes optimize ML work-
loads on existing hardware for better performance
•Supports a variety of hardware devices and model
formats
•TVM’s design enables flexibility for block sparsity,
quantization, classical ML, memory planning, etc•Deploying and integrating models still
requires some technical knowledge
and experience
deep learning compiler. Liu et al. [202] devised edgeEye, a
high-level API for real-time intelligent video analytics ap-
plications, allowing developers to focus on application logic,while Wang et al. [158] developed the HAQ framework, which
utilizes reinforcement learning to determine the optimal quan-
tization policy for each layer of a DNN, taking into account

--- PAGE 18 ---
18
TABLE VI
SUMMARY OF MODEL INFERENCE METHODS FROM THE LITERATURE
Method Model Goal Performance
SparkNet [179] CNN •To reduce parameters and computation demands•Compress CNN by a factor of 150x
•Performance: 337.2 GOP/s
•Energy efficiency: 44.48 GOP/s/w
Memsqueezer [180] CNN •To enable CNN inference on edge devices•2x performance improvement
•80% energy consumption reduction
UGIO [181] CNN•To propose an end-to-end solution for CNN inference on edge
devices•Achieves similar, or even better (up to 1.62x), performance
ACG-Engine [196] CNN•To address the performance bottleneck of instance normalization
in generative networks on edge devices•Speed: 4.56x
•Power efficiency: 29x
Pipe-it [182] CNN•To perform CNN inference on ARM big.LITTLE architecture
in edge devices•Achieve a 39% higher throughput
SCA [183] CNN •Enable IP protection when deploying CNN on edge devices•4.8x speedup over a non-secure baseline
•34.2x speedup over an inference-only secure baseline
•Reduce 84.3% over a non-secure baseline
•Reduce 98.5% speedup over an inference-only secure baseline
NeuLens [184] CNN•To achieve latency reduction and accuracy improvement in CNN
acceleration on edge systems•Reduce up to 58% latency
•Improve up to 67.9% accuracy improvement
TS-VIB-LSTM [185] RNN•To compress RNN for human action recognition on edge devices•More than 70x compression rate
edgeDRNN [186] RNN•To enable a low-latency, low-power RNN accelerator for real-
time applications on edge devices•It updates a 5M 2-layer GRU-RNN in 0.5ms for low-latency
edge inference
•5x faster than commercial edge AI platforms
•It achieves 20.2GOp/s throughput and 4x higher power effi-
ciency than commercial platforms
SP-RNN [187] RNN•To enable the deployment of RNN models on edge devices
through network pruning techniques•Nearly 20x speedup
•Without performance loss
DirNet [188] RNN•To introduce a model compression approach for RNNs to make
it can be deployed on resource constrained devices•Reduce 8x model size
•With negligible performance loss
Hidet [197] DNN •To design efficient tensor programs for deep learning operators•Outperform by up to 1.48x (1.22x on average)
•Reduce tuning time by 11x
edgeEye [197] DNN •For real-time intelligent video analytics applications•Achieve efficient and high-performance deep learning inference
HAQ [158] DNN•Optimize specialized neural network for a particular hardware
architecture•Reduce latency by 1.4 - 1.95x
•Reduce energy consumption by 1.9x
•With the negligible loss of accuracy
GRACE [198] DNN•To enable efficient source compression at IoT devices without
disturbing the inference performance•Reduce a source size by 23%
•Achieves 7.5% higher inference accuracy
•Reduce 90% bandwidth consumption
LcDNN [199] DNN •To enable deep learning on the mobile web•Reduce the model size by 16x - 29x
•Reduce the end-to-end latency
•Reduce the mobile energy cost
DA3 [200] DNN •To enable on-device multi-domain learning•Reduce training memory by 5x - 37x
•Reduce training time by 2x
RAPID-RL [201] DNN •To enable deep RL systems to be deployed on edge devices•Incur 0.34x (0.25x)’s operations while maintaining 0.88x
(0.91x)’s performance
the specific hardware architecture and resource constraints.
Xieet al. [198] proposed GRACE, a DNN-aware compression
algorithm that optimizes the compression strategy for a target
DNN model, enabling efficient source compression at IoT
devices without disturbing the inference performance. Huang
et al. [199] designed LcDNN, a lightweight collaborative DNN
for the mobile web that executes a lightweight binary neural
network (BNN) branch on the mobile device to reduce the
model size, accelerates inference, and reduces energy cost.
Farhadi et al. [203] proposed a system-level design to improve
the energy consumption of object detection on resource-limited
user-end devices by deploying a shallow neural network
(SHNN) and implementing a knowledge transfer mechanism to
update the SHNN model using DNN knowledge from a pow-
erful edge device. Yang et al. [200] designed DA3, a memory-
efficient on-device multi-domain learning approach that re-
duces activation memory usage during training on resource-
limited edge devices while maintaining high accuracy per-
formance. Additionally, Kosta et al. [201] proposed RAPID-
RL, an architecture for efficient deep reinforcement learning
that allows conditional activation of DNN layers based on
input difficulty level, dynamically adjusting computational
effort during inference while maintaining performance. Table
6 summarizes the optimized models, goals, and performance
of these studies. Moreover, some studies have explored other
models, such as Weightless Neural Networks (WNN) [204],Binarized Neural Networks (BNN) [128] [205], Long Short-
Term Memory (LSTM) [206], transformer [207] and Graph
Neural Networks (GNN) [208] [209] [210].
B. Hardware Optimization
In addition to software optimizations, hardware acceleration
is crucial for achieving high performance for lightweight
models on edge devices. Various approaches to hardware
acceleration include using specialized processors such as Cen-
tral Processing Unit (CPU), Graphic Processing Unit (GPU),
Field Programmable Gate Array (FPGA), Application Specific
Integrated Circuit (ASIC), and Neural Processing Unit (NPU),
as well as implementing custom hardware designs for specific
models. Table 7 provides information on common processors
for edge devices, including basic information, examples, and
features.
In recent years, there has been a growing interest in using
CPU-based accelerators for model inference on edge devices
due to their versatility and stable computing performance.
REDUCT [211] is a solution that bypasses traditional CPU
resources to enable efficient data parallel DNN inference
workloads, achieving significant performance/Watt improve-
ments and raw performance scaling on multi-core CPUs. By
maximizing the utilization of existing bandwidth resources
and distributing lightweight tensor compute near all caches,

--- PAGE 19 ---
19
TABLE VII
EDGE AI M ODEL ACCELERATOR
Hardware Basic Information Examples Advantages Limitations
CPUA kind of universal computing
equipment•ARM Cortex-M55
•Intel Atom x7-E3950
•Qualcomm Snapdragon 865
•Apple A14 Bionic
•MediaTek Helio P90•It has a wide range of application
scenarios and versatility
•Stable computing performance
•Rich hardware and software ecosys-
tem and support•CPU performance and efficiency
may not be as good as dedicated
GPU and ASIC
GPUCan be used to accelerate deep
learning algorithms•Qualcomm Adreno 640
•Imagination Technologies Pow-
erVR Series9XE
•Intel Iris Plus Graphics G7
•NVIDIA Tegra X1
•ARM Mali-G76•High parallelism
•Flexibility
•Wide application support•High power consumption
FPGATo accelerate deep learning al-
gorithms through customized
hardware logic•Lattice sensAI
•QuickLogic EOS S3
•Xilinx Zynq UltraScale+
•Intel Movidius Neural Compute
Stick•Highly flexible
•Low power consumption
•Customizable•High development and deployment
costs
ASICTo achieve high performance
and power efficiency through
hardware optimization•Google edge TPU
•Horizon Robotics Sunrise
•MediaTek NeuroPilot
•Cambricon MLU100•Highly optimized hardware struc-
ture
•Low power consumption
•High performance•High development and production
costs
NPUDesigned to accelerate deep
learning algorithms•Qualcomm Hexagon 680
•Apple Neural Engine
•Huawei Kirin 990
•MediaTek APU
•NVIDIA Jetson Nano•High efficiency
•Low power consumption
•Highly optimized hardware struc-
ture•Limited support for some models
TABLE VIII
SUMMARY OF EDGE AI M ODEL ACCELERATOR FROM THE LITERATURE
Method Hardware Model Strategy Performance
REDUCT [211] CPU DNN•It bypasses CPU resources to optimize DNN
inference•2.3x increase in convolution performance/Watt
•2x to 3.94x scaling in raw performance
•1.8x increase in inner-product performance/Watt
•2.8x scaling in performance
NCPU [212] CPU BNN •Propose a unified architecture•Achieved 35% area reduction and 12% energy saving
compared to conventional heterogeneous architecture
•Implemented two-core NCPU SoC achieves an end-to-
end performance speed-up of 43% or an equivalent 74%
energy saving
Prototype [213] GPU DNN•The schedulability of repeated real-time GPU
tasks is significantly improved•Achieved 35% area reduction and 12% energy saving
compared to conventional heterogeneous architecture
•Implemented two-core NCPU SoC achieves an end-to-
end performance speed-up of 43% or an equivalent 74%
energy saving
SparkNoC [179] FPGA CNN •Simultaneous pipelined work•Performance: 337.2 GOP/s
•Energy efficiency: 44.48 GOP/s/w
FPGA Overlay [214] FPGA CNN•It exploits all forms of parallelism inside a
convolution operation•An improvement of 1.2x to 5x in maximum throughput
•An improvement of 1.3x to 4x in performance density
Light-OPU [215] FPGA CNN •With a corresponding compilation flow•Achievement of 5.5x better latency and 3.0x higher
power efficiency on average compared with NVIDIA
Jetson TX2
•Achievement of 1.3x to 8.4x better power efficiency
compared with previous customized FPGA accelerators
edgeBert [151] ASIC Transformer•It employs entropy-based early exit predication•The energy savings are up to 7x, 2.5x, and 53x com-
pared to conventional inference without early stopping,
latency-unbounded early exit approach
ApGAN [216] ASIC GAN•By binarizing weights and using a hardware-
configurable in-memory addition scheme•Achieve energy efficiency improvements of up to 28.6x
•Achieve a 35-fold speedup
Fluid Batching [217] NPU DNN•Fluid Batching and Stackable Processing Ele-
ments are introduced•1.97x improvement in average latency
•6.7x improvement in tail latency SLO satisfaction
BitSystolic [218] NPU DNN •Based on a systolic array structure•It achieves high power efficiency of up to 26.7 TOPS/W
with 17.8 mW peak power consumption
PL-NPU [219] NPU DNN•A posit-based logarithm-domain processing
element, a reconfigurable inter-intra-channel-
reuse dataflow, and a pointed-stake-shaped
codec unit are employed•3.75x higher energy efficiency
•1.68x speedup
FARNN [220] FPGA + GPU RNN•To separate RNN computations into different
tasks that are suitable for GPU or FPGA•Improve by up to 4.2x
DART [221] CPU + GPU DNN•It offers deterministic response time to real-
time tasks and increased throughput to best-
effort tasks•Response time was reduced by up to 98.5%
•Achieve up to 17.9% higher throughput
REDUCT achieves performance similar to or better than
state-of-the-art Domain Specific Accelerators (DSA) for DNN
inference. Similarly, Zhu et al. [212] designed the Neural CPU
(NCPU) architecture to optimize the end-to-end performance
of low-cost embedded systems. It combines a binary neural
network accelerator with an in-order RISC-V CPU pipeline,
achieving energy savings and area reduction compared to
conventional heterogeneous architectures, while supporting
flexible programmability and local data storage to avoid costlycore-to-core data transfer.
GPUs are recognized as powerful hardware accelerators for
deep learning, thanks to their highly parallel architecture and
programmability. Capodieci et al. [213] presented a prototype
real-time scheduler for GPU activities on an embedded SoC
featuring an NVIDIA GPU architecture, with preemptive EDF
scheduling and bandwidth isolations using a Constant Band-
width Server. FPGAs are also increasingly used as hardware
accelerators for deep learning due to their highly flexible and

--- PAGE 20 ---
20
customizable hardware logic. For example, Xia et al. [179]
proposed an FPGA-based accelerator architecture specifically
built for SparkNet, achieving high performance and energy
efficiency with a fully pipelined CNN hardware accelerator.
Choudhury et al. [214] proposed an FPGA overlay for efficient
CNN processing that exploits all forms of parallelism inside a
convolution operation and can be scaled based on available
compute and memory resources. Yu et al. [215] explored
FPGA-based overlay processors for an efficient acceleration of
lightweight operations in LW-CNNs, utilizing a corresponding
compilation flow. Overall, GPUs and FPGAs offer different
advantages for hardware acceleration, with GPUs providing
powerful parallel computing capabilities and FPGAs offering
highly customizable hardware logic for efficient and scalable
acceleration.
ASICs are increasingly popular as dedicated hardware ac-
celerators for deep learning, owing to their highly optimized
hardware structure and low power consumption. For example,
Tambe et al. [151] proposed edgeBERT for multi-task NLP
inference, which adopts algorithm-hardware co-design and
early exit prediction based on entropy. Roohi et al. [216]
designed ApGAN to alleviate the computationally intensive
GAN problem, especially for running on resource-constrained
edge devices. NPUs can be considered ASICs designed specif-
ically for AI, and they are also gaining popularity as dedicated
hardware accelerators for deep learning due to their high
efficiency and low power consumption. Some studies aim
to improve task performance by enhancing the NPU. For
example, Kouris et al. [217] introduced two new dimensions to
the NPU hardware architecture design space, Fluid Batching,
and Stackable Processing Elements, to improve NPU utiliza-
tion. Yang et al. [218] proposed BitSystolic, a 2b-8b NPU
that supports mixed-precision DNN models with configurable
numerical precision and configurable data flows. Multiple
measures are adopted to improve hardware utilization, such
as in PL-NPU [219], where a posit-based logarithm-domain
processing element, a reconfigurable inter-intra-channel-reuse
dataflow, and a pointed-stake-shaped codec unit are employed.
Some studies have also used acceleration methods that com-
bine multiple processors, such as FPGA + GPU [220] and
CPU + GPU [221]. Overall, ASICs and NPUs offer highly
optimized hardware structures and low power consumption,
making them ideal for efficient and high-performance deep
learning on edge devices. Specifically, the basic information
on these hardware acceleration methods is listed in Table 8.
VI. A PPLICATION SCENARIOS
Edge AI is a technology that deploys AI algorithms and
models on edge devices such as smartphones, cameras, sen-
sors, and robots, enabling data processing and decision-making
at the device end. This approach avoids data transmission
delays and privacy issues while improving response speed and
security. Edge AI has a wide range of application scenarios,
including smart homes, industrial automation, healthcare, and
many others. With edge AI, devices can perform advanced
tasks such as object detection, face recognition, and action
recognition without relying on the cloud or other external
computing resources.A. Smart Homes
In traditional smart home systems, sensors, cameras, and
other devices collect home environment data and transmit it to
the cloud for processing and decision-making. However, data
transmission delays and privacy issues can cause real-time and
security problems in the system, greatly reducing the quality of
the user experience. Edge AI technology enables the deploy-
ment of AI algorithms and models on smart home devices,
allowing for data processing and decision-making to occur at
the device level. This approach can enable many aspects of
home life, such as electricity demand forecasting [222], human
activity prediction [223], and energy management [224]. In
particular, edge AI techniques effectively circumvent issues
related to data transmission delay and privacy, resulting in
improved security and response times. For example, [225]
identifies new vulnerabilities and attacks in widely-used smart
home platforms by analyzing the complex interactions among
the participating entities. With edge AI, smart home devices
can become more intelligent and autonomous, providing bene-
fits such as energy efficiency, improved security, and enhanced
user experience.
B. Industrial Automation
Smart industry is a technological approach that aims to im-
prove the efficiency and quality of production by deploying AI
algorithms at the device level [226]. This is achieved through
the implementation of industrial automation, which involves
the use of advanced technologies and systems to control and
monitor industrial processes. By deploying AI algorithms on
industrial devices, smart industry enables machines to learn
from data, adapt to changes in the production environment,
and optimize their own operations in real-time. This results
in improved efficiency, reduced waste, and enhanced product
quality, which ultimately leads to increased profitability for
industrial businesses [227]. The application of smart industry
has been shown to have a significant impact on various
aspects of industrial operations, including maintenance, qual-
ity control, and supply chain management. By leveraging
AI-powered analytics and predictive maintenance, industrial
companies can reduce downtime and increase the lifespan of
their equipment while also improving product quality through
real-time monitoring and control. Overall, the smart industry
represents a major technological advancement in the field of
industrial automation, providing businesses with the tools and
capabilities needed to optimize their production processes and
remain competitive in today’s fast-paced business environ-
ment.
C. Healthcare
Edge AI technology can provide faster and more accu-
rate disease diagnosis and treatment for intelligent healthcare
systems, thereby improving medical efficacy and efficiency.
Recent research in healthcare has explored various aspects of
the field, such as heart patient analysis [228], voice disorder
detection [229], COVID-19 analysis [230] [231], and pathol-
ogy detection [232] [233]. In [234], a privacy-preserving Faster

--- PAGE 21 ---
21
R-CNN framework is proposed for object detection in medical
images using additive secret sharing techniques and edge
computing. This approach ensures the privacy and security
of medical data while also enabling accurate and efficient
diagnosis. The use of edge AI in healthcare also allows for
real-time monitoring of patient vitals and early detection of
potential health issues, leading to improved patient outcomes.
Overall, edge AI holds great promise for the healthcare indus-
try, providing opportunities for improved medical diagnosis,
treatment, and patient care.
D. Autonomous Vehicles
In the field of autonomous driving, real-time performance
is critical for ensuring safe and efficient operation. Previous
research has extensively studied the collaborative work be-
tween cloud/edge and edge computing, such as data scheduling
[235], resource allocation [236], perception information shar-
ing [237], object detection [238], and computation offloading
[239] [240]. By utilizing edge algorithms on vehicles, it is
possible to decrease the dependence on cloud computing
resources and enhance response speed. For example, in [241],
a compiler-aware pruning search framework was proposed,
which enables real-time 3D object detection on resource-
limited mobile devices for autonomous driving. This approach
allows for faster and more accurate detection of objects
in the environment, improving the safety and efficiency of
autonomous vehicles. In the future, we can expect to see
more edge AI technologies developed and applied to empower
autonomous driving on local devices.
E. Public Safety
The introduction of edge AI technology into public safety
has significant potential for providing faster, more accurate,
and reliable responses to public safety incidents. For example,
Wang et al. [242] proposed the Surveiledge system, which
enables real-time queries of large-scale surveillance video
streams through a collaborative cloud-edge approach, balanc-
ing load among different computing nodes and achieving a
latency-accuracy tradeoff. Experiments show that Surveiledge
significantly reduces bandwidth costs and query response
times compared to cloud-only solutions while improving
query accuracy and speed compared to edge-only approaches.
Trinh et al. [243] investigated techniques for detecting urban
anomalies by utilizing mobile traffic monitoring, where they
proposed the use of the mobile network as an additional
sensing platform to monitor crowded events for potential
risks to public safety. In addition, methods for improving
performance by combining various techniques have also been
proposed. For instance, in [244], the proposed Metropolitan
Intelligent Surveillance System (MISS) combines IoT, cloud
computing, edge computing, and big data to enable a unified
approach for implementing an ISS at an urban scale, increasing
performance and security levels. These applications of edge
AI in public safety demonstrate the potential for improving
the response time and accuracy of public safety incidents,
enhancing situational awareness, and ultimately improving
public safety.F . Agriculture
Edge AI technology has shown promising potential for
applications in the agriculture sector. For example, Men-
shchikov et al. [245] presented an approach for fast and
accurate detection of the harmful and fast-growing Hogweed
of Sosnowskyi using an unmanned aerial vehicle with an
embedded system running Fully CNNs. The proposed ap-
proach achieves high accuracy in segmentation and processing
speed for individual plants and leaves, which can provide
comprehensive and relevant data to control the expansion of
this plant. In addition, there are interesting applications in the
agricultural sector, such as real-time strawberry detection [246]
and pest management [247]. These applications leverage edge
AI technology to provide real-time data analysis, enabling
farmers to make informed decisions and optimize their crop
yields. For example, real-time strawberry detection can help
farmers quickly identify and harvest ripe fruit, while pest
management can help to reduce the use of harmful chemicals
by targeting pests more precisely.
G. Retail
Smart retail is a new retail mode that improves the efficiency
and experience of retail through intelligent technology and
data analysis. Retailers are increasingly leveraging edge AI
to analyze data from in-store cameras and sensors to create
intelligent stores [248]. One practical application of edge AI
in smart retail is to alert store employees when shelf inventory
levels are low, reducing the impact of stockouts. Another
use case is the automated checkout system in unmanned
supermarkets that utilizes edge AI algorithms and sensors to
identify user and product information for automatic checkout.
With the help of edge AI, intelligent customer service can also
be achieved to better serve customers. In addition, edge AI can
be utilized to analyze inventory loss caused by theft, errors,
fraud, waste, and damage. By analyzing data from cameras
and sensors, retailers can identify patterns and potential issues,
allowing them to take proactive measures to reduce losses and
improve profitability. Overall, the use of edge AI in smart retail
has the potential to revolutionize the way we approach retail,
providing retailers with the tools and capabilities needed to
optimize their operations, reduce losses, and enhance customer
experience.
H. Energy Management
Edge AI has been widely applied in the field of energy
management [249], including energy monitoring, intelligent
control, energy optimization, and smart device management.
By real-time monitoring energy usage and collecting data,
edge AI can assist energy managers in understanding energy
consumption status and providing energy efficiency improve-
ment suggestions through data analysis and prediction [250].
Furthermore, edge AI can achieve automated energy control,
optimize energy consumption, improve energy efficiency, and
reduce energy costs. For example, an edge AI-based intelligent
control system can adjust the HV AC system automatically
based on occupancy patterns and environmental conditions

--- PAGE 22 ---
22
to reduce energy waste. Similarly, edge AI can optimize
the operation of distributed energy resources, such as solar
panels and wind turbines, to maximize energy production and
minimize costs. In addition, edge AI can enable smart device
management, allowing energy managers to remotely monitor
and control energy consumption in real-time. By using edge
AI to analyze data from smart devices, energy managers can
identify potential issues and take proactive measures to prevent
energy waste and reduce costs.
I. Logistics
Logistics automation has the potential to significantly boost
production capacity. By incorporating intelligent systems, pro-
ductivity can be improved, human error reduced, and work
efficiency enhanced. In particular, edge AI has emerged as
a key technology for enabling intelligent logistics systems.
Through the deployment of edge AI at the unit-level of lo-
gistics terminals, such as intelligent sorting robots, unmanned
aerial vehicles for express delivery, and logistics distribution
robots, these terminals can be imbued with higher levels of
intelligence. By utilizing edge AI algorithms, these intelligent
terminals can learn from data, adapt to changes in their
environment, and optimize their operations in real-time. For
example, edge AI can enable intelligent sorting robots to
rapidly sort packages based on weight, size, and destination,
reducing the time required for manual sorting and increasing
efficiency. In addition, edge AI can enable unmanned aerial
vehicles for express delivery to autonomously navigate through
complex environments, such as urban areas, and identify the
optimal delivery routes, reducing delivery times and costs.
Furthermore, edge AI-powered logistics distribution robots can
autonomously navigate through warehouses and distribution
centers, reducing the need for human intervention and increas-
ing productivity.
VII. C HALLENGES
Compared to server devices, edge devices are resource-
constrained in multiple aspects, including their computational
power, storage capacity, energy resources, and communication
bandwidth. These limitations pose significant challenges for
deploying AI models on edge devices.
A. Limited Computing Power
Edge devices typically deploy low-power processors with
limited computational capabilities, often unable to handle the
large and complex computations required by AI models. To
enable effective AI on resource-constrained edge devices, sev-
eral optimization techniques can be employed. One approach
is to optimize algorithms to reduce unnecessary computation
and improve computational efficiency. This can be achieved by
designing lightweight models (such as MobileNets series [75]
[76] [77]) or through NAS [111] [113], model compression
(pruning [137] [125], quantization [150] [157], parameter
sharing [142] [143], knowledge distillation [162] [165], etc.),
which reduce the size and complexity of AI models without
compromising their accuracy and performance.Another approach is to enhance the hardware performance
of edge devices. This can be achieved through the use of
specialized hardware, such as GPUs [213], FPGAs [179]
[214] [215] or ASICs [151] [216] [219], which can accelerate
computational tasks and improve performance. In addition,
cloud-edge/edge-edge collaboration can be used to augment
the computational capabilities of edge devices [251] [252]
[253] [16].
Overall, a combination of optimization techniques, hardware
enhancements, and cloud-edge/edge-edge collaboration can
be used to overcome the limitations of edge devices and
enable more sophisticated AI applications on these devices.
By leveraging these techniques, edge devices can perform
complex computational tasks while conserving energy and
maintaining computational accuracy and performance.
B. Limited Memory
Edge AI aims to deploy AI models directly on resource-
constrained edge devices rather than in the cloud. However,
edge devices have highly limited storage capacity compared
to the cloud, posing challenges for edge AI implementation.
For instance, state-of-the-art deep learning models can require
hundreds of megabytes to over a gigabyte of storage, which
exceeds the storage limits of many edge devices. As such,
minimizing storage demands and avoiding large model and
data storage requirements is critical for making edge AI
feasible. To address this challenge, several methods have
been adopted. As previously mentioned, model compression
is an effective way to reduce the model size and storage
demands in edge AI [137] [153] [157]. This technique reduces
the storage requirements of AI models, enabling them to be
deployed on edge devices with limited storage capacity. In
addition, incremental learning can be used to dynamically
update models on edge devices by learning from new data,
avoiding the need to store large amounts of historical data
[254]. This approach reduces the storage requirements of edge
devices by only storing the most recent data and models.
Furthermore, storing data and models in a distributed manner
across multiple edge devices can alleviate the limited storage
capacity of individual devices [255]. This approach enables
edge devices to leverage the storage capacity of other devices,
reducing the storage requirements of individual devices. Edge
caching technology can also be utilized to cache data and
models between edge devices and the cloud, reducing storage
demands and communication costs [256] [257] [258]. This
approach enables frequently used data and models to be
stored locally on edge devices, reducing the need to access
the cloud for every request. Overall, these techniques enable
the deployment of AI on edge devices with limited storage
capacity.
C. Power Consumption
Edge devices are often battery-powered with limited energy
budgets, while AI models can have high computational de-
mands. This mismatch between the energy limitations of edge
devices and the intensive computations of AI models poses
a key challenge for practical edge AI deployment. Balancing

--- PAGE 23 ---
23
computational efficiency and energy consumption presents a
challenging task, necessitating the adoption of energy-efficient
algorithms and hardware that can effectively minimize en-
ergy consumption while preserving computational accuracy
and performance. One solution to address energy limitations
is software design, as demonstrated by the development of
energy-efficient algorithms such as PhiNets [259]. These algo-
rithms are designed to minimize computational requirements,
enabling them to run efficiently on edge devices with limited
energy resources. Hardware design is another approach to ad-
dress energy limitations. Researchers have developed energy-
efficient hardware, such as [260] [179] [261], that reduces
energy consumption and improves computational efficiency. In
addition, hardware and software co-design, such as [262], can
optimize both hardware and software for energy efficiency.
Energy management is another solution to address energy
limitations. Researchers have proposed energy management
techniques, such as the use of AI-based controllers [263] [264],
to optimize energy consumption in edge devices.
Overall, the adoption of energy-efficient algorithms and
hardware, as well as energy management techniques, can
effectively minimize energy consumption while preserving
computational accuracy and performance in edge devices. As
the technology continues to advance, we can expect to see even
more innovative solutions for addressing the energy limitations
of edge devices.
D. Limited Communication Bandwidth
Compared to servers, edge devices typically have limited
communication bandwidth, making it challenging to transfer
large amounts of data between the edge device and the cloud.
The limited connectivity poses challenges for transmitting
large volumes of data between devices and the cloud, which
many AI models require. To minimize communication costs,
it is necessary to reduce the amount of data transmitted.
One approach to reducing data transmission is through data
preprocessing algorithms, which reduce the amount of data
that needs to be transmitted during communication [36] [38].
These algorithms can be used to filter and compress data,
enabling only relevant information to be transmitted. Edge
caching technology is another approach that can be utilized to
store data and models on edge devices, reducing the frequency
of communication with the cloud and minimizing the amount
of data transmitted [265]. This approach enables frequently
used data and models to be stored locally on edge devices,
reducing the need to access the cloud for every request. On-
device computation is another technique that can be used to
enable real-time response at the edge [266] [267] [268]. By
performing computation on the edge device, only relevant
data needs to be transmitted to the cloud, reducing commu-
nication costs and enabling faster response times. Overall,
these techniques can be used to minimize the amount of
data transmitted between edge devices and the cloud, enabling
efficient communication and reducing communication costs.
E. Security and Privacy
Deploying AI models on edge devices poses significant
security and privacy challenges due to the distributed natureof edge computing and the processing of sensitive data on
devices outside the control of data owners. To address these
challenges, several techniques have been proposed, including
data anonymization [269], trusted execution environment tech-
nology [270] [271], homomorphic encryption [272] [273] and
secure multi-party computation [274].
Federated learning has become a research hotspot, enabling
AI models to be trained on a distributed network of edge
devices while preserving data privacy and security [275] [276]
[277] [278]. Recently, hybrid approaches have been proposed
that combine multiple techniques to address the challenges
of edge computing. For example, StarFL is a new hybrid
Federated Learning architecture that combines a trusted execu-
tion environment, secure multi-party computation, and beidou
satellites to address communication-heavy, high-frequent, and
asynchronous data in urban computing scenarios [279].
F . Model Management and Scheduling
Due to the limited resources in edge computing and the
mobility of users and devices, it is necessary to perform
efficient model management and scheduling when deploying
AI models to the edge. Model scheduling can primarily be
divided into model placement, model migration, and elastic
scaling of models.
During model placement, the first challenge to address is de-
signing effective feature extraction methods to extract features
from the edge environment and user tasks due to the hetero-
geneity of AI model requests and the edge environment [280].
Second, given the complex request and restriction relationships
between user tasks and models, considering various restriction
conditions such as task dependency, deadline restrictions,
bandwidth limitations, etc., to carry out model placement
is the second challenge [281] [282] [283]. Lastly, in light
of the latency requirements for edge AI model deployment,
another issue to solve is how to schedule by further mining the
dependency relationships between the model’s image layers to
reduce the cold start time of the AI model [284] [285] [286].
After model placement, due to the mobility of users or
devices, the AI model needs to be further migrated [287].
Firstly, how to migrate the AI model requested by the user
to the appropriate new edge node to achieve better Quality of
Service (QoS) considering user mobility is the first challenge
[288] [289] [290]. Secondly, during the migration process,
considering the storage structure characteristics of the AI
model and the limited computing resources in edge computing
to further optimize the migration cost is another pressing
challenge to solve [291] [292].
Finally, to cope with the common scenarios of a sudden
large number of AI model requests or peak period requests in
the edge environment, in the phase of elastic scaling of the AI
model, the first challenge is how to efficiently and accurately
predict the resource utilization rate of different edge nodes and
reasonably price the resources [293] [294] [295]. Secondly,
in view of the geographic distribution characteristics of edge
computing, designing innovative elastic scaling strategies to
meet the requests of users in different regions is another
challenge to address [296] [297].

--- PAGE 24 ---
24
VIII. C ONCLUSION AND FUTURE DIRECTIONS
In conclusion, enabling AI on the edge has numerous
benefits, including faster inference, improved privacy, and
reduced latency. However, this approach also presents nu-
merous challenges, such as limited computational resources,
memory, and power availability. To address these challenges,
a range of techniques can be used to optimize AI on the
edge, including data optimization, model optimization, and
system optimization. These techniques have been successfully
applied in a range of use cases, including smart homes,
industrial automation, autonomous vehicles, agriculture, retail
and healthcare and medical applications. As the field of AI
on the edge continues to evolve, new hardware and software
developments will also play an important role, as well as
integration with 5G networks and edge-to-cloud orchestration.
By leveraging these techniques and advancements, we can
continue to enable AI on the edge and unlock the potential
benefits of this approach to computing.
In the future, edge AI is expected to have even broader
applications, and several key development trends can be iden-
tified:
•More intelligent: With the advancement of AI chip
technology and edge computing capabilities, the devel-
opment of more intelligent edge devices is expected.
These devices will be capable of processing increasingly
complex data and tasks, resulting in the deployment
of more complex and high-performance AI models on
edge devices. This will enable them to perform more
sophisticated data processing and decision-making tasks,
providing users with more personalized and efficient
services. Furthermore, the level of automated processing
and adaptability of edge AI for different tasks will also
be improved, leading to even more efficient and effective
processing of data at the edge.
•More flexible: As the computing capabilities of edge
AI devices improve and edge AI algorithms continue
to develop, edge AI technology will evolve from serv-
ing specific scenarios to becoming more universal and
flexible. This will enable edge AI to better adapt to
different application scenarios, providing more versatile
and adaptable solutions to a wider range of use cases.
This increased flexibility will allow edge AI to be de-
ployed in various industries and domains, making it a
more ubiquitous and reliable technology for processing
and analyzing data at the edge.
•More secure: In addition to enhancing efficiency and
enabling autonomous decision-making, edge AI has an
important role in improving user privacy and data se-
curity. In the future, edge AI is expected to continue
to enhance security by adopting technologies such as
blockchain to enable secure and decentralized data shar-
ing and analysis. By using blockchain, edge AI can
ensure that data remains secure and private, avoiding
issues such as user data leakage. As edge AI becomes
more widespread, ensuring user privacy and data security
will become increasingly important, and the adoption
of technologies such as blockchain will be critical inachieving this goal.
•More collaborative: As individual edge devices have
limited resources, more complex tasks will be achieved
through collaboration between edge devices and through
cloud-edge collaboration. This collaboration will enable
edge devices to work together to process and analyze
data, providing more robust and efficient solutions. Mean-
while, scenarios such as intelligent transportation urgently
require improved collaboration and greater intelligence in
edge AI to ensure the safety and efficiency of transporta-
tion systems. By using collaboration and leveraging the
strengths of both edge and cloud computing, edge AI
can be used to address increasingly complex problems
and provide more effective solutions in a wide range of
applications.
•More efficient: Efficiency is a critical factor in edge AI,
and more efficient algorithms and hardware, along with
optimization for different scenarios, will help edge AI to
have more efficient processing capabilities. By continu-
ally improving algorithms and hardware, edge AI devices
can process data more quickly and accurately, while using
fewer resources. Additionally, optimizing edge AI for
different scenarios will enable it to be more effective
in specific use cases, such as industrial automation or
healthcare. These improvements in efficiency will enable
edge AI to become more widely adopted and provide
more efficient and effective solutions for processing and
analyzing data at the edge.
Overall, the future of edge AI is promising, and continued
advancements in hardware, software, and collaboration will
enable it to unlock its full potential in a wide range of
applications.
REFERENCES
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, Shyam et al. , “Language models are few-shot learn-
ers,” Advances in Neural Information Processing Systems , vol. 33, pp.
1877–1901, 2020.
[2] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y . Zomaya,
“Edge intelligence: The confluence of edge computing and artificial
intelligence,” IEEE Internet of Things Journal , vol. 7, no. 8, pp. 7457–
7469, 2020.
[3] B. Gill and S. Rao, “Technology insight: Edge computing in support
of the internet of things,” Gartner Research Report, Tech. Rep., 2017.
[4] T. Taleb, S. Dutta, A. Ksentini, M. Iqbal, and H. Flinck, “Mobile edge
computing potential in making cities smarter,” IEEE Communications
Magazine , vol. 55, no. 3, pp. 38–43, 2017.
[5] S. Liu, L. Liu, J. Tang, B. Yu, Y . Wang, and W. Shi, “Edge computing
for autonomous driving: Opportunities and challenges,” Proceedings of
the IEEE , vol. 107, no. 8, pp. 1697–1716, 2019.
[6] P. Garcia Lopez, A. Montresor, D. Epema, A. Datta, T. Higashino,
A. Iamnitchi, M. Barcellos, P. Felber, and E. Riviere, “Edge-centric
computing: Vision and challenges,” pp. 37–42, 2015.
[7] W. Shi, J. Cao, Q. Zhang, Y . Li, and L. Xu, “Edge computing: Vision
and challenges,” IEEE Internet of Things Journal , vol. 3, no. 5, pp.
637–646, 2016.
[8] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge
intelligence: Paving the last mile of artificial intelligence with edge
computing,” Proceedings of the IEEE , vol. 107, no. 8, pp. 1738–1762,
2019.
[9] C. Bai, P. Dallasega, G. Orzes, and J. Sarkis, “Industry 4.0 technologies
assessment: A sustainability perspective,” International Journal of
Production Economics , vol. 229, p. 107776, 2020.

--- PAGE 25 ---
25
[10] W. Wang, R. Li, Y . Chen, Z. M. Diekel, and Y . Jia, “Facilitating human–
robot collaborative tasks by teaching-learning-collaboration from hu-
man demonstrations,” IEEE Transactions on Automation Science and
Engineering , vol. 16, no. 2, pp. 640–653, 2018.
[11] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature , vol.
521, no. 7553, pp. 436–444, 2015.
[12] Y . Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communication-
efficient edge ai: Algorithms and systems,” IEEE Communications
Surveys & Tutorials , vol. 22, no. 4, pp. 2167–2191, 2020.
[13] J. Chen and X. Ran, “Deep learning with edge computing: A review,”
Proceedings of the IEEE , vol. 107, no. 8, pp. 1655–1674, 2019.
[14] K. B. Letaief, Y . Shi, J. Lu, and J. Lu, “Edge artificial intelligence for
6g: Vision, enabling technologies, and applications,” IEEE Journal on
Selected Areas in Communications , vol. 40, no. 1, pp. 5–36, 2021.
[15] D. Xu, T. Li, Y . Li, X. Su, S. Tarkoma, T. Jiang, J. Crowcroft, and
P. Hui, “Edge intelligence: Architectures, challenges, and applications,”
arXiv preprint arXiv:2003.12172 , 2020.
[16] J. Yao, S. Zhang, Y . Yao et al. , “Edge-cloud polarization and collabora-
tion: A comprehensive survey for ai,” IEEE Transactions on Knowledge
and Data Engineering , 2022.
[17] M. S. Murshed, C. Murphy, D. Hou, N. Khan, G. Ananthanarayanan,
and F. Hussain, “Machine learning at the network edge: A survey,”
ACM Computing Surveys (CSUR) , vol. 54, no. 8, pp. 1–37, 2021.
[18] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless network
intelligence at the edge,” Proceedings of the IEEE , vol. 107, no. 11,
pp. 2204–2239, 2019.
[19] X. Wang, Y . Han, V . C. Leung, D. Niyato, X. Yan, and X. Chen,
“Convergence of edge computing and deep learning: A comprehensive
survey,” IEEE Communications Surveys & Tutorials , vol. 22, no. 2, pp.
869–904, 2020.
[20] Y . Dai, K. Zhang, S. Maharjan, and Y . Zhang, “Edge intelligence for
energy-efficient computation offloading and resource allocation in 5g
beyond,” IEEE Transactions on Vehicular Technology , vol. 69, no. 10,
pp. 12 175–12 186, 2020.
[21] J. Zhang and K. B. Letaief, “Mobile edge intelligence and computing
for the internet of vehicles,” Proceedings of the IEEE , vol. 108, no. 2,
pp. 246–261, 2019.
[22] D. Xu, T. Li, Y . Li, X. Su, S. Tarkoma, T. Jiang, J. Crowcroft, and
P. Hui, “Edge intelligence: Empowering intelligence to the edge of
network,” Proceedings of the IEEE , vol. 109, no. 11, pp. 1778–1837,
2021.
[23] H. Hua, Y . Li, T. Wang, N. Dong, W. Li, and J. Cao, “Edge computing
with artificial intelligence: A machine learning perspective,” ACM
Computing Surveys , vol. 55, no. 9, pp. 1–35, 2023.
[24] Y . Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model
compression and acceleration for deep neural networks,” arXiv preprint
arXiv:1710.09282 , 2017.
[25] L. Deng, G. Li, S. Han, L. Shi, and Y . Xie, “Model compression and
hardware acceleration for neural networks: A comprehensive survey,”
Proceedings of the IEEE , vol. 108, no. 4, pp. 485–532, 2020.
[26] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. Katz, A. Konwinski,
G. Lee, D. Patterson, A. Rabkin, I. Stoica et al. , “A view of cloud
computing,” Communications of the ACM , vol. 53, no. 4, pp. 50–58,
2010.
[27] K. Jamsa, Cloud computing . Jones & Bartlett Learning, 2022.
[28] K. Gai, Y . Wu, L. Zhu, L. Xu, and Y . Zhang, “Permissioned blockchain
and edge computing empowered privacy-preserving smart grid net-
works,” IEEE Internet of Things Journal , vol. 6, no. 5, pp. 7992–8004,
2019.
[29] M. Satyanarayanan, “The emergence of edge computing,” Computer ,
vol. 50, no. 1, pp. 30–39, 2017.
[30] G. Premsankar, M. Di Francesco, and T. Taleb, “Edge computing for
the internet of things: A case study,” IEEE Internet of Things Journal ,
vol. 5, no. 2, pp. 1275–1284, 2018.
[31] P. Huang, L. Zeng, X. Chen, K. Luo, Z. Zhou, and S. Yu, “Edge
robotics: Edge-computing-accelerated multi-robot simultaneous local-
ization and mapping,” IEEE Internet of Things Journal , 2022.
[32] A. Y . Ding, E. Peltonen, T. Meuser, A. Aral, C. Becker, S. Dustdar,
T. Hiessl, D. Kranzlm ¨uller, M. Liyanage, S. Maghsudi et al. , “Roadmap
for edge ai: a dagstuhl perspective,” pp. 28–33, 2022.
[33] D. Zha, Z. P. Bhat, K.-H. Lai, F. Yang, Z. Jiang, S. Zhong, and
X. Hu, “Data-centric artificial intelligence: A survey,” arXiv preprint
arXiv:2303.10158 , 2023.
[34] M. Bernhardt, D. C. Castro, R. Tanno, A. Schwaighofer, K. C. Tezcan,
M. Monteiro, S. Bannur, Lungren et al. , “Active label cleaning for
improved dataset quality under resource constraints,” Nature Commu-
nications , vol. 13, no. 1, p. 1161, 2022.[35] R. Mishra, A. Gupta, and H. P. Gupta, “Locomotion mode recognition
using sensory data with noisy labels: A deep learning approach,” IEEE
Transactions on Mobile Computing , 2021.
[36] T. Wang, H. Ke, X. Zheng, K. Wang, A. K. Sangaiah, and A. Liu, “Big
data cleaning based on mobile edge computing in industrial sensor-
cloud,” IEEE Transactions on Industrial Informatics , vol. 16, no. 2,
pp. 1321–1329, 2019.
[37] L. Ma, Q. Pei, L. Zhou, H. Zhu, L. Wang, and Y . Ji, “Federated data
cleaning: Collaborative and privacy-preserving data cleaning for edge
intelligence,” IEEE Internet of Things Journal , vol. 8, no. 8, pp. 6757–
6770, 2020.
[38] D. Sun, S. Xue, H. Wu, and J. Wu, “A data stream cleaning system
using edge intelligence for smart city industrial environments,” IEEE
Transactions on Industrial Informatics , vol. 18, no. 2, pp. 1165–1174,
2021.
[39] D. Sun, J. Wu, J. Yang, and H. Wu, “Intelligent data collaboration
in heterogeneous-device iot platforms,” ACM Transactions on Sensor
Networks (TOSN) , vol. 17, no. 3, pp. 1–17, 2021.
[40] C. Gupta, A. S. Suggala, A. Goyal, H. V . Simhadri, B. Paranjape,
A. Kumar, S. Goyal, R. Udupa, M. Varma, and P. Jain, “Protonn: Com-
pressed and accurate knn for resource-scarce devices,” in International
Conference on Machine Learning . PMLR, 2017, pp. 1331–1340.
[41] L. Van Der Maaten, E. Postma, J. Van den Herik et al. , “Dimensionality
reduction: a comparative,” J Mach Learn Res , vol. 10, no. 66-71, p. 13,
2009.
[42] A. Kratsios and C. Hyndman, “Neu: A meta-algorithm for universal
uap-invariant feature representation,” The Journal of Machine Learning
Research , vol. 22, no. 1, pp. 4102–4152, 2021.
[43] J. P. Cunningham and Z. Ghahramani, “Linear dimensionality reduc-
tion: Survey, insights, and generalizations,” The Journal of Machine
Learning Research , vol. 16, no. 1, pp. 2859–2900, 2015.
[44] T.-T. Do, T. Hoang, V . Pomponiu, Y . Zhou, Z. Chen, N.-M. Cheung
et al. , “Accessible melanoma detection using smartphones and mobile
image analysis,” IEEE Transactions on Multimedia , vol. 20, no. 10,
pp. 2849–2864, 2018.
[45] F. Haider, S. Pollak, P. Albert, and S. Luz, “Emotion recognition in
low-resource settings: An evaluation of automatic feature selection
methods,” Computer Speech & Language , vol. 65, p. 101119, 2021.
[46] D. H. Summerville, K. M. Zach, and Y . Chen, “Ultra-lightweight
deep packet anomaly detection for internet of things devices,” in 2015
IEEE 34th international performance computing and communications
conference (IPCCC) . IEEE, 2015, pp. 1–8.
[47] S. R. V . Sudhakar, N. Kayastha, and K. Sha, “Actid: An efficient
framework for activity sensor based user identification,” Computers
& Security , vol. 108, p. 102319, 2021.
[48] P. Laddha, O. J. Omer, G. S. Kalsi, D. K. Mandal, and S. Subramoney,
“Descriptor scoring for feature selection in real-time visual slam,”
in2020 IEEE International Conference on Image Processing (ICIP) .
IEEE, 2020, pp. 2601–2605.
[49] M. Masud, P. Singh, G. S. Gaba, A. Kaur, R. Alroobaea, M. Alrashoud,
and S. A. Alqahtani, “Crowd: crow search and deep learning based
feature extractor for classification of parkinson’s disease,” ACM Trans-
actions on Internet Technology (TOIT) , vol. 21, no. 3, pp. 1–18, 2021.
[50] J. Chen, Y . Zheng, Y . Liang, Z. Zhan, M. Jiang, X. Zhang, D. S.
da Silva, W. Wu, and V . H. C. de Albuquerque, “Edge2analysis: a
novel aiot platform for atrial fibrillation recognition and detection,”
IEEE Journal of Biomedical and Health Informatics , vol. 26, no. 12,
pp. 5772–5782, 2022.
[51] T. Li, S. Fong, X. Li, Z. Lu, and A. H. Gandomi, “Swarm decision table
and ensemble search methods in fog computing environment: case of
day-ahead prediction of building energy demands using iot sensors,”
IEEE Internet of Things Journal , vol. 7, no. 3, pp. 2321–2342, 2019.
[52] R. Marino, C. Wisultschew, A. Otero, J. M. Lanza-Gutierrez, J. Portilla,
and E. de la Torre, “A machine-learning-based distributed system for
fault diagnosis with scalable detection quality in industrial iot,” IEEE
Internet of Things Journal , vol. 8, no. 6, pp. 4339–4352, 2020.
[53] C. Shen, K. Zhang, and J. Tang, “A covid-19 detection algorithm using
deep features and discrete social learning particle swarm optimization
for edge computing devices,” ACM Transactions on Internet Technol-
ogy (TOIT) , vol. 22, no. 3, pp. 1–17, 2021.
[54] Y . Matsubara, R. Yang, M. Levorato, and S. Mandt, “Supervised
compression for resource-constrained edge computing systems,” in
Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , 2022, pp. 2685–2695.
[55] D. Chen, X. Cao, F. Wen, and J. Sun, “Blessing of dimensionality:
High-dimensional feature and its efficient compression for face verifi-

--- PAGE 26 ---
26
cation,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2013, pp. 3025–3032.
[56] Z. Chen, K. Fan, S. Wang, L. Duan, W. Lin, and A. C. Kot, “Toward
intelligent sensing: Intermediate deep feature compression,” IEEE
Transactions on Image Processing , vol. 29, pp. 2230–2243, 2019.
[57] C. Liu, X. Li, H. Chen, D. Modolo, and J. Tighe, “Selective feature
compression for efficient activity recognition inference,” in Proceedings
of the IEEE/CVF International Conference on Computer Vision , 2021,
pp. 13 628–13 637.
[58] J. Shao and J. Zhang, “Communication-computation trade-off in
resource-constrained edge inference,” IEEE Communications Maga-
zine, vol. 58, no. 12, pp. 20–26, 2020.
[59] A. A. Abdellatif, A. Emam, C.-F. Chiasserini et al. , “Edge-based
compression and classification for smart healthcare systems: Concept,
implementation and evaluation,” Expert Systems with Applications , vol.
117, pp. 1–14, 2019.
[60] S. Zhou, D. Van Le, J. Q. Yang, R. Tan, and D. Ho, “Efcam:
Configuration-adaptive fog-assisted wireless cameras with reinforce-
ment learning,” in 2021 18th Annual IEEE International Conference
on Sensing, Communication, and Networking (SECON) . IEEE, 2021,
pp. 1–9.
[61] A. A. Abdellatif, A. Mohamed, C. F. Chiasserini, M. Tlili, and A. Er-
bad, “Edge computing for smart health: Context-aware approaches,
opportunities, and challenges,” IEEE Network , vol. 33, no. 3, pp. 196–
203, 2019.
[62] A. M. Moreno-Rodenas, A. Duinmeijer, and F. H. Clemens, “Deep-
learning based monitoring of fog layer dynamics in wastewater pump-
ing stations,” Water Research , vol. 202, p. 117482, 2021.
[63] Y . Guo, B. Zou, J. Ren, Q. Liu, D. Zhang, and Y . Zhang, “Distributed
and efficient object detection via interactions among devices, edge, and
cloud,” IEEE Transactions on Multimedia , vol. 21, no. 11, pp. 2903–
2915, 2019.
[64] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmen-
tation for deep learning,” Journal of Big Data , vol. 6, no. 1, pp. 1–48,
2019.
[65] W. Ma, R. Lou, K. Zhang, L. Wang, and S. V osoughi, “GradTS: A
gradient-based automatic auxiliary task selection method based on
transformer networks,” in Proceedings of EMNLP 2021 , 2021, pp.
5621–5632.
[66] S. Y . Feng, V . Gangal, J. Wei, S. Chandar, S. V osoughi, T. Mitamura,
and E. Hovy, “A survey of data augmentation approaches for nlp,”
arXiv preprint arXiv:2105.03075 , 2021.
[67] Z. Wang, J. Hu, G. Min, Z. Zhao, and J. Wang, “Data-augmentation-
based cellular traffic prediction in edge-computing-enabled smart city,”
IEEE Transactions on Industrial Informatics , vol. 17, no. 6, pp. 4179–
4187, 2020.
[68] R.-F. Liao, H. Wen, S. Chen, F. Xie, F. Pan, J. Tang, and H. Song,
“Multiuser physical layer authentication in internet of things with data
augmentation,” IEEE Internet of Things Journal , vol. 7, no. 3, pp.
2077–2088, 2019.
[69] X. Liu and Z. Deng, “Segmentation of drivable road using deep
fully convolutional residual network with pyramid pooling,” Cognitive
Computation , vol. 10, pp. 272–281, 2018.
[70] Z. Jiao, K. Huang, G. Jia, H. Lei, Y . Cai, and Z. Zhong, “An effective
litchi detection method based on edge devices in a complex scene,”
Biosystems Engineering , vol. 222, pp. 15–28, 2022.
[71] G. Gu, B. Ko, S. Go, S.-H. Lee, J. Lee, and M. Shin, “Towards light-
weight and real-time line segment detection,” in Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 36, no. 1, 2022, pp.
726–734.
[72] C. Liu, R. Antypenko, I. Sushko, and O. Zakharchenko, “Intrusion
detection system after data augmentation schemes based on the vae
and cvae,” IEEE Transactions on Reliability , vol. 71, no. 2, pp. 1000–
1010, 2022.
[73] H. Pan, Y .-C. Chen, Q. Ye, and G. Xue, “Magicinput: Training-
free multi-lingual finger input system using data augmentation based
on mnists,” in Proceedings of the 20th International Conference on
Information Processing in Sensor Networks , 2021, pp. 119–131.
[74] Y . Zhou, S. Chen, Y . Wang, and W. Huan, “Review of research
on lightweight convolutional neural networks,” in 2020 IEEE 5th
Information Technology and Mechatronics Engineering Conference
(ITOEC) . IEEE, 2020, pp. 1713–1720.
[75] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861 , 2017.[76] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-
bilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition ,
2018, pp. 4510–4520.
[77] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan,
W. Wang, Y . Zhu et al. , “Searching for mobilenetv3,” in Proceedings
of the IEEE/CVF International Conference on Computer Vision , 2019,
pp. 1314–1324.
[78] D. Zhou, Q. Hou, Y . Chen, J. Feng, and S. Yan, “Rethinking bottleneck
structure for efficient mobile network design,” in Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part III 16 . Springer, 2020, pp. 680–697.
[79] M. Tan, B. Chen, R. Pang, V . Vasudevan, M. Sandler, A. Howard,
and Q. V . Le, “Mnasnet: Platform-aware neural architecture search for
mobile,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2019, pp. 2820–2828.
[80] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely effi-
cient convolutional neural network for mobile devices,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2018, pp. 6848–6856.
[81] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical
guidelines for efficient cnn architecture design,” in Proceedings of the
European Conference on Computer Vision (ECCV) , 2018, pp. 116–131.
[82] Z. Guo, X. Zhang, H. Mu et al. , “Single path one-shot neural ar-
chitecture search with uniform sampling,” in Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XVI 16 . Springer, 2020, pp. 544–560.
[83] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360 ,
2016.
[84] A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. Jin, et al. ,
“Squeezenext: Hardware-aware neural network design,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops , 2018, pp. 1638–1647.
[85] K. Han, Y . Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, “Ghostnet:
More features from cheap operations,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2020, pp.
1580–1589.
[86] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for con-
volutional neural networks,” in International Conference on Machine
Learning . PMLR, 2019, pp. 6105–6114.
[87] M. Tan and Q. Le, “Efficientnetv2: Smaller models and faster training,”
inInternational Conference on Machine Learning . PMLR, 2021, pp.
10 096–10 106.
[88] M. Tan, R. Pang, and Q. V . Le, “Efficientdet: Scalable and efficient
object detection,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2020, pp. 10 781–10 790.
[89] G. Huang, S. Liu, L. Van der Maaten, and K. Q. Weinberger, “Con-
densenet: An efficient densenet using learned group convolutions,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2018, pp. 2752–2761.
[90] L. Yang, H. Jiang, R. Cai, Y . Wang, S. Song, G. Huang, and Q. Tian,
“Condensenet v2: Sparse feature reactivation for deep networks,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2021, pp. 3569–3578.
[91] S. Mehta, M. Rastegari, A. Caspi, L. Shapiro, and H. Hajishirzi,
“Espnet: Efficient spatial pyramid of dilated convolutions for semantic
segmentation,” in Proceedings of the European Conference on Com-
puter Vision (ECCV) , 2018, pp. 552–568.
[92] S. Mehta, M. Rastegari, L. Shapiro et al. , “Espnetv2: A light-weight,
power efficient, and general purpose convolutional neural network,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2019, pp. 9190–9200.
[93] B. Wu, X. Dai, P. Zhang et al. , “Fbnet: Hardware-aware efficient
convnet design via differentiable neural architecture search,” in Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2019, pp. 10 734–10 742.
[94] A. Wan, X. Dai, P. Zhang et al. , “Fbnetv2: Differentiable neural
architecture search for spatial and channel dimensions,” in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2020, pp. 12 965–12 974.
[95] X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei et al. , “Fbnetv3: Joint
architecture-recipe search using predictor pretraining,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition , 2021, pp. 16 276–16 285.

--- PAGE 27 ---
27
[96] R. J. Wang, X. Li, and C. X. Ling, “Pelee: A real-time object detection
system on mobile devices,” Advances in Neural Information Processing
Systems , vol. 31, 2018.
[97] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov
et al. , “Going deeper with convolutions,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2015, pp.
1–9.
[98] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in International
Conference on Machine Learning . pmlr, 2015, pp. 448–456.
[99] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-
ing the inception architecture for computer vision,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition ,
2016, pp. 2818–2826.
[100] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi, “Inception-v4,
inception-resnet and the impact of residual connections on learning,” in
Proceedings of the AAAI conference on Artificial Intelligence , vol. 31,
no. 1, 2017.
[101] F. Chollet, “Xception: Deep learning with depthwise separable convo-
lutions,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2017, pp. 1251–1258.
[102] S. Mehta and M. Rastegari, “Mobilevit: light-weight, general-purpose,
and mobile-friendly vision transformer,” 2021.
[103] Z. Wu, Z. Liu, J. Lin, Y . Lin, and S. Han, “Lite transformer with
long-short range attention,” 2020.
[104] Q. Hou, D. Zhou, and J. Feng, “Coordinate attention for efficient
mobile network design,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2021, pp. 13 713–13 722.
[105] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “Eca-net:
Efficient channel attention for deep convolutional neural networks,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2020, pp. 11 534–11 542.
[106] Q.-L. Zhang and Y .-B. Yang, “Sa-net: Shuffle attention for deep con-
volutional neural networks,” in ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) .
IEEE, 2021, pp. 2235–2239.
[107] D. Misra, T. Nalamada, A. U. Arasanipalai, and Q. Hou, “Rotate to
attend: Convolutional triplet attention module,” in Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision ,
2021, pp. 3139–3148.
[108] H. Zhang, C. Wu, Z. Zhang, Y . Zhu, H. Lin, Z. Zhang, Y . Sun, T. He,
J. Mueller, R. Manmatha et al. , “Resnest: Split-attention networks,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 2736–2746.
[109] H. Lu, M. Du, X. He, K. Qian, J. Chen, Y . Sun, and K. Wang, “An
adaptive neural architecture search design for collaborative edge-cloud
computing,” IEEE Network , vol. 35, no. 5, pp. 83–89, 2021.
[110] B. Lyu, S. Wen, K. Shi, and T. Huang, “Multiobjective reinforcement
learning-based neural architecture search for efficient portrait parsing,”
IEEE Transactions on Cybernetics , 2021.
[111] H. Chen, L. Zhuo, B. Zhang, X. Zheng, J. Liu, R. Ji, D. Doermann,
and G. Guo, “Binarized neural architecture search for efficient object
recognition,” International Journal of Computer Vision , vol. 129, pp.
501–516, 2021.
[112] H. R. Mendis, C.-K. Kang, and P.-c. Hsiu, “Intermittent-aware neu-
ral architecture search,” ACM Transactions on Embedded Computing
Systems (TECS) , vol. 20, no. 5s, pp. 1–27, 2021.
[113] X. Ning, G. Ge, W. Li, Z. Zhu, Y . Zheng, X. Chen et al. , “Ftt-
nas: Discovering fault-tolerant convolutional neural architecture,” ACM
Transactions on Design Automation of Electronic Systems (TODAES) ,
vol. 26, no. 6, pp. 1–24, 2021.
[114] Z. Liu, H. Tang, S. Zhao, K. Shao, and S. Han, “Pvnas: 3d neural
architecture search with point-voxel convolution,” IEEE Transactions
on Pattern Analysis and Machine Intelligence , vol. 44, no. 11, pp.
8552–8568, 2021.
[115] C. Donegan, H. Yous, S. Sinha, and J. Byrne, “Vpu specific cnns
through neural architecture search,” in 2020 25th International Con-
ference on Pattern Recognition (ICPR) . IEEE, 2021, pp. 9772–9779.
[116] N. Nayman, Y . Aflalo, A. Noy, and L. Zelnik, “Hardcore-nas: Hard
constrained differentiable neural architecture search,” in International
Conference on Machine Learning . PMLR, 2021, pp. 7979–7990.
[117] P. Liu, B. Wu, H. Ma, and M. Seok, “Memnas: Memory-efficient neural
architecture search with grow-trim learning,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 2108–2116.
[118] C. Zhang, X. Yuan, Q. Zhang, G. Zhu, L. Cheng, and N. Zhang,
“Toward tailored models on private aiot devices: Federated direct neuralarchitecture search,” IEEE Internet of Things Journal , vol. 9, no. 18,
pp. 17 309–17 322, 2022.
[119] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huff-
man coding,” International Conference on Learning Representations
(ICLR) , 2016.
[120] Z. Xu, F. Yu, Z. Qin et al. , “Directx: Dynamic resource-aware cnn
reconfiguration framework for real-time mobile applications,” IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems , vol. 40, no. 2, pp. 246–259, 2020.
[121] H. Ahmad, T. Arif, M. A. Hanif, R. Hafiz, and M. Shafique, “Su-
perslash: A unified design space exploration and model compression
methodology for design of deep learning accelerators with reduced off-
chip memory access volume,” IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems , vol. 39, no. 11, pp. 4191–
4204, 2020.
[122] C. M. J. Tan and M. Motani, “Dropnet: Reducing neural network
complexity via iterative pruning,” in International Conference on
Machine Learning . PMLR, 2020, pp. 9356–9366.
[123] Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. Gon-
zalez, “Train big, then compress: Rethinking model size for efficient
training and inference of transformers,” in International Conference on
Machine Learning . PMLR, 2020, pp. 5958–5968.
[124] W. Ma, K. Zhang, R. Lou, L. Wang, and S. V osoughi, “Contributions
of transformer attention heads in multi- and cross-lingual tasks,” in
Proceedings of ACL-IJCNLP 2021 , 2021.
[125] S. Gao, F. Huang, J. Pei, and H. Huang, “Discrete model compression
with resource constraint for deep neural networks,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2020, pp. 1899–1908.
[126] C. Wu, Y . Cui, C. Ji, T.-W. Kuo, and C. J. Xue, “Pruning deep
reinforcement learning for dual user experience and storage lifetime
improvement on mobile devices,” IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems , vol. 39, no. 11, pp.
3993–4005, 2020.
[127] X. Zhou, Q. Jia et al. , “Nestfl: efficient federated learning through
progressive model pruning in heterogeneous edge computing,” in
Proceedings of the 28th Annual International Conference on Mobile
Computing And Networking , 2022, pp. 817–819.
[128] T. Geng, A. Li, T. Wang, C. Wu et al. , “O3bnn-r: An out-of-order
architecture for high-performance and regularized bnn inference,” IEEE
Transactions on Parallel and Distributed Systems , vol. 32, no. 1, pp.
199–213, 2020.
[129] G. Li, X. Ma, X. Wang, L. Liu et al. , “Fusion-catalyzed pruning for
optimizing deep learning on intelligent edge devices,” IEEE Transac-
tions on Computer-Aided Design of Integrated Circuits and Systems ,
vol. 39, no. 11, pp. 3614–3626, 2020.
[130] J. Gu, C. Feng, Z. Zhao, Z. Ying et al. , “Efficient on-chip learning
for optical neural networks through power-aware sparse zeroth-order
optimization,” in Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 35, no. 9, 2021, pp. 7583–7591.
[131] W. Kwon, S. Kim, M. W. Mahoney, J. Hassoun, K. Keutzer, and
A. Gholami, “A fast post-training pruning framework for transformers,”
2022.
[132] M. Lin, R. Ji, Y . Wang, Y . Zhang, B. Zhang, Y . Tian, and L. Shao,
“Hrank: Filter pruning using high-rank feature map,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition , 2020, pp. 1529–1538.
[133] S. Li, E. Hanson, H. Li, and Y . Chen, “Penni: Pruned kernel sharing
for efficient cnn inference,” in International Conference on Machine
Learning . PMLR, 2020, pp. 5863–5873.
[134] F. Tung and G. Mori, “Clip-q: Deep network compression learning by
in-parallel pruning-quantization,” in Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition , 2018, pp. 7873–
7882.
[135] I. Fedorov, R. P. Adams, M. Mattina, and P. Whatmough, “Sparse:
Sparse architecture search for cnns on resource-constrained microcon-
trollers,” Advances in Neural Information Processing Systems , vol. 32,
2019.
[136] B. Khaleghi, M. Imani, and T. Rosing, “Prive-hd: Privacy-preserved
hyperdimensional computing,” in 2020 57th ACM/IEEE Design Au-
tomation Conference (DAC) . IEEE, 2020, pp. 1–6.
[137] Y . Huang, X. Qiao, J. Tang, P. Ren et al. , “Deepadapter: A collaborative
deep learning framework for the mobile web using context-aware
network pruning,” in IEEE INFOCOM 2020-IEEE Conference on
Computer Communications . IEEE, 2020, pp. 834–843.

--- PAGE 28 ---
28
[138] Y . Liu, Z. Shu, Y . Li, Z. Lin, F. Perazzi, and S.-Y . Kung, “Content-
aware gan compression,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2021, pp. 12 156–12 166.
[139] T. Jian, Y . Gong, Z. Zhan, R. Shi, N. Soltani, Z. Wang, J. Dy,
K. Chowdhury, Y . Wang, and S. Ioannidis, “Radio frequency fin-
gerprinting on the edge,” IEEE Transactions on Mobile Computing ,
vol. 21, no. 11, pp. 4078–4093, 2021.
[140] L. Wang, X. Dong, Y . Wang, X. Ying et al. , “Exploring sparsity in
image super-resolution for efficient inference,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2021, pp. 4917–4926.
[141] M. Sun, P. Zhao, M. Gungor, M. Pedram, M. Leeser, and X. Lin, “3d
cnn acceleration on fpga using hardware-aware pruning,” in 2020 57th
ACM/IEEE Design Automation Conference (DAC) . IEEE, 2020, pp.
1–6.
[142] J. Wu, Y . Wang, Z. Wu et al. , “Deep k-means: Re-training and
parameter sharing with harder cluster assignments for compressing
deep convolutions,” in International Conference on Machine Learning .
PMLR, 2018, pp. 5363–5372.
[143] A. Obukhov, M. Rakhuba, S. Georgoulis, M. Kanakis, D. Dai, and
L. Van Gool, “T-basis: a compact representation for neural networks,”
inInternational Conference on Machine Learning . PMLR, 2020, pp.
7392–7404.
[144] K. Ullrich, E. Meeds, and M. Welling, “Soft weight-sharing for neural
network compression,” 2017.
[145] H. You, B. Li, S. Huihong, Y . Fu, and Y . Lin, “Shiftaddnas: Hardware-
inspired search for more accurate and efficient neural networks,” in
International Conference on Machine Learning . PMLR, 2022, pp.
25 566–25 580.
[146] S. Hu, X. Xie, M. Cui, J. Deng, S. Liu, J. Yu et al. , “Neural architecture
search for lf-mmi trained time delay neural networks,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing , vol. 30, pp.
1093–1107, 2022.
[147] R. Wang, Z. Wei, H. Duan, S. Ji, Y . Long, and Z. Hong, “Efficienttdnn:
Efficient architecture search for speaker recognition,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing , vol. 30, pp.
2267–2279, 2022.
[148] Y . Sun, F. Yuan, M. Yang, G. Wei et al. , “A generic network compres-
sion framework for sequential recommender systems,” in Proceedings
of the 43rd International ACM SIGIR Conference on Research and
Development in Information Retrieval , 2020, pp. 1299–1308.
[149] V . Sindhwani, T. Sainath, and S. Kumar, “Structured transforms for
small-footprint deep learning,” Advances in Neural Information Pro-
cessing Systems , vol. 28, 2015.
[150] Y . Fu, H. You, Y . Zhao, Y . Wang, C. Li et al. , “Fractrain: Fractionally
squeezing bit savings both temporally and spatially for efficient dnn
training,” Advances in Neural Information Processing Systems , vol. 33,
pp. 12 127–12 139, 2020.
[151] T. Tambe, C. Hooper, L. Pentecost et al. , “Edgebert: Sentence-level
energy optimizations for latency-aware multi-task nlp inference,” in
MICRO-54: 54th Annual IEEE/ACM International Symposium on Mi-
croarchitecture , 2021, pp. 830–844.
[152] V . Chikin and M. Antiukh, “Data-free network compression via para-
metric non-uniform mixed precision quantization,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2022, pp. 450–459.
[153] Y . Boo, S. Shin, J. Choi, and W. Sung, “Stochastic precision ensemble:
self-knowledge distillation for quantized deep neural networks,” in
Proceedings of the AAAI Conference on Artificial Intelligence , vol. 35,
no. 8, 2021, pp. 6794–6802.
[154] Y . Cui, S. Wu, Q. Li, A. B. Chan, T.-W. Kuo, and C. J. Xue, “Bits-
ensemble: Toward light-weight robust deep ensemble by bits-sharing,”
IEEE Transactions on Computer-Aided Design of Integrated Circuits
and Systems , vol. 41, no. 11, pp. 4397–4408, 2022.
[155] A. Marchisio, B. Bussolino, A. Colucci, M. Martina, G. Masera, and
M. Shafique, “Q-capsnets: A specialized framework for quantizing
capsule networks,” in 2020 57th ACM/IEEE Design Automation Con-
ference (DAC) . IEEE, 2020, pp. 1–6.
[156] R. V . W. Putra et al. , “Fspinn: An optimization framework for memory-
efficient and energy-efficient spiking neural networks,” IEEE Transac-
tions on Computer-Aided Design of Integrated Circuits and Systems ,
vol. 39, no. 11, pp. 3601–3613, 2020.
[157] Q. Zhou, S. Guo, Z. Qu et al. , “Octo: Int8 training with loss-aware
compensation and backward quantization for tiny on-device learning.”
inUSENIX Annual Technical Conference , 2021, pp. 177–191.
[158] K. Wang, Z. Liu, Y . Lin, J. Lin, and S. Han, “Haq: Hardware-aware
automated quantization with mixed precision,” in Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2019, pp. 8612–8620.
[159] B. Li, S. Qu, and Y . Wang, “An automated quantization framework
for high-utilization rram-based pim,” IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems , vol. 41, no. 3, pp.
583–596, 2021.
[160] W. A. Simon, V . Ray, A. Levisse, G. Ansaloni, M. Zapater, and
D. Atienza, “Exact neural networks from inexact multipliers via fi-
bonacci weight encoding,” in 2021 58th ACM/IEEE Design Automation
Conference (DAC) . IEEE, 2021, pp. 805–810.
[161] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
neural network,” arXiv preprint arXiv:1503.02531 , 2015.
[162] L. Zhang, J. Song, A. Gao et al. , “Be your own teacher: Improve the
performance of convolutional neural networks via self distillation,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2019, pp. 3713–3722.
[163] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and Q. Liu, “Dynabert:
Dynamic bert with adaptive width and depth,” Advances in Neural
Information Processing Systems , vol. 33, pp. 9782–9793, 2020.
[164] L. Zhang, Z. Tan, J. Song, J. Chen, C. Bao, and K. Ma, “Scan: A
scalable neural networks framework towards compact and efficient
models,” Advances in Neural Information Processing Systems , vol. 32,
2019.
[165] Y . Zhang, Z. Yan, X. Sun, W. Diao, K. Fu, and L. Wang, “Learning
efficient and accurate detectors with dynamic knowledge distillation
in remote sensing imagery,” IEEE Transactions on Geoscience and
Remote Sensing , vol. 60, pp. 1–19, 2021.
[166] Z. Hao, Y . Luo, Z. Wang, H. Hu, and J. An, “Cdfkd-mfs: Collaborative
data-free knowledge distillation via multi-level feature sharing,” IEEE
Transactions on Multimedia , vol. 24, pp. 4262–4274, 2022.
[167] Z. Hao, J. Guo, D. Jia, K. Han, Y . Tang, C. Zhang, H. Hu, and Y . Wang,
“Learning efficient vision transformers via fine-grained manifold dis-
tillation,” Advances in Neural Information Processing Systems , vol. 35,
pp. 9164–9175, 2022.
[168] K. Zhang, C. Tao, T. Shen, C. Xu, X. Geng, B. Jiao, and
D. Jiang, “Led: Lexicon-enlightened dense retriever for large-scale
retrieval,” in Proceedings of WWW 2023 , 2023. [Online]. Available:
https://doi.org/10.1145/3543507.3583294
[169] T. Shen, X. Geng, C. Tao, C. Xu, G. Long, K. Zhang, and D. Jiang,
“Unifier: A unified retriever for large-scale retrieval,” 2023.
[170] J. Ni, R. Sarbajna, Y . Liu et al. , “Cross-modal knowledge distillation
for vision-to-sensor action recognition,” in ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2022, pp. 4448–4452.
[171] H. Jin, D. Bai, D. Yao, Y . Dai, L. Gu, C. Yu, and L. Sun, “Personal-
ized edge intelligence via federated self-knowledge distillation,” IEEE
Transactions on Parallel and Distributed Systems , vol. 34, no. 2, pp.
567–580, 2022.
[172] W. Li, J. Wang, T. Ren, F. Li, J. Zhang, and Z. Wu, “Learning
accurate, speedy, lightweight cnns via instance-specific multi-teacher
knowledge distillation for distracted driver posture identification,” IEEE
Transactions on Intelligent Transportation Systems , vol. 23, no. 10, pp.
17 922–17 935, 2022.
[173] X. Xia, H. Yin, J. Yu et al. , “On-device next-item recommendation
with self-supervised knowledge distillation,” in Proceedings of the 45th
International ACM SIGIR Conference on Research and Development
in Information Retrieval , 2022, pp. 546–555.
[174] Z. Xu, Z. Hong, C. Ding, Z. Zhu, J. Han, J. Liu, and E. Ding,
“Mobilefaceswap: A lightweight framework for video face swapping,”
inProceedings of the AAAI Conference on Artificial Intelligence ,
vol. 36, no. 3, 2022, pp. 2973–2981.
[175] H. Bai, H. Mao, and D. Nair, “Dynamically pruning segformer for
efficient semantic segmentation,” in ICASSP 2022-2022 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2022, pp. 3298–3302.
[176] H. Yang et al. , “Learning low-rank deep neural networks via singular
vector orthogonality regularization and singular value sparsification,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops , 2020, pp. 678–679.
[177] Y . Li, Y . Chen, X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, and
N. Vasconcelos, “Micronet: Towards image recognition with extremely
low flops,” arXiv preprint arXiv:2011.12289 , 2020.
[178] G. Verma, Y . Gupta, A. M. Malik, and B. Chapman, “Performance
evaluation of deep learning compilers for edge inference,” in 2021
IEEE International Parallel and Distributed Processing Symposium
Workshops (IPDPSW) . IEEE, 2021, pp. 858–865.

--- PAGE 29 ---
29
[179] M. Xia, Z. Huang, L. Tian, H. Wang, V . Chang, Y . Zhu, and
S. Feng, “Sparknoc: An energy-efficiency fpga-based accelerator using
optimized lightweight cnn for edge computing,” Journal of Systems
Architecture , vol. 115, p. 101991, 2021.
[180] Y . Wang, H. Li, and X. Li, “Re-architecting the on-chip memory
sub-system of machine-learning accelerator for embedded devices,” in
2016 IEEE/ACM International Conference on Computer-Aided Design
(ICCAD) . IEEE, 2016, pp. 1–6.
[181] L. Wang, Z. Chen, Y . Liu, Y . Wang, L. Zheng, M. Li, and Y . Wang, “A
unified optimization approach for cnn model inference on integrated
gpus,” in Proceedings of the 48th International Conference on Parallel
Processing , 2019, pp. 1–10.
[182] S. Wang, G. Ananthanarayanan, Y . Zeng et al. , “High-throughput cnn
inference on embedded arm big. little multicore processors,” IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems , vol. 39, no. 10, pp. 2254–2267, 2019.
[183] L. Zhao, Y . Zhang, and J. Yang, “Sca: a secure cnn accelerator for both
training and inference,” in 2020 57th ACM/IEEE Design Automation
Conference (DAC) . IEEE, 2020, pp. 1–6.
[184] X. Hou, Y . Guan, and T. Han, “Neulens: spatial-based dynamic
acceleration of convolutional neural networks on edge,” in Proceedings
of the 28th Annual International Conference on Mobile Computing And
Networking , 2022, pp. 186–199.
[185] A. Srivastava, O. Dutta, J. Gupta et al. , “A variational information
bottleneck based method to compress sequential networks for human
action recognition,” in Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , 2021, pp. 2745–2754.
[186] C. Gao, A. Rios-Navarro, X. Chen, S.-C. Liu, and T. Delbruck, “Edge-
drnn: Recurrent neural network accelerator for edge inference,” IEEE
Journal on Emerging and Selected Topics in Circuits and Systems ,
vol. 10, no. 4, pp. 419–432, 2020.
[187] L. Wen, X. Zhang, H. Bai, and Z. Xu, “Structured pruning of recurrent
neural networks through neuron selection,” Neural Networks , vol. 123,
pp. 134–141, 2020.
[188] J. Zhang, X. Wang, D. Li, and Y . Wang, “Dynamically hierarchy
revolution: dirnet for compressing recurrent neural network on mobile
devices,” in Proceedings of the 27th International Joint Conference on
Artificial Intelligence , 2018, pp. 3089–3096.
[189] Microsoft, “Onnx runtime: cross-platform, high performance ml
inferencing and training accelerator. https://github.com/microsoft/
onnxruntime,” GitHub repository , 2019.
[190] Intel, “ openvinoTMtoolkit repository. https://github.com/
openvinotoolkit/openvino,” GitHub repository , 2018.
[191] Tencent, “Ncnn is a high-performance neural network inference frame-
work optimized for the mobile platform. https://github.com/Tencent/
ncnn,” GitHub repository , 2017.
[192] Arm, “Arm nn ml software. https://github.com/ARM-software/armnn,”
GitHub repository , 2018.
[193] Alibaba, “Mnn is a blazing fast, lightweight deep learning framework
https://github.com/alibaba/MNN,” GitHub repository , 2018.
[194] NVIDIA, “ nvidia®tensorrtTM, an sdk for high-performance deep
learning inference. https://github.com/NVIDIA/TensorRT,” GitHub
repository , 2017.
[195] Apache, “Open deep learning compiler stack for cpu, gpu and special-
ized accelerators. https://github.com/apache/tvm,” GitHub repository ,
2018.
[196] H. Xu, Y . Wang, Y . Wang, J. Li, B. Liu, and Y . Han, “Acg-engine:
An inference accelerator for content generative neural networks,” in
2019 IEEE/ACM International Conference on Computer-Aided Design
(ICCAD) . IEEE, 2019, pp. 1–7.
[197] Y . Ding, C. H. Yu et al. , “Hidet: Task-mapping programming paradigm
for deep learning tensor programs,” in Proceedings of the 28th ACM
International Conference on Architectural Support for Programming
Languages and Operating Systems, Volume 2 , 2023, pp. 370–384.
[198] X. Xie and K.-H. Kim, “Source compression with bounded dnn
perception loss for iot edge computer vision,” in The 25th Annual
International Conference on Mobile Computing and Networking , 2019,
pp. 1–16.
[199] Y . Huang, X. Qiao, P. Ren, L. Liu, C. Pu, S. Dustdar, and J. Chen, “A
lightweight collaborative deep neural network for the mobile web in
edge cloud,” IEEE Transactions on Mobile Computing , vol. 21, no. 7,
pp. 2289–2305, 2020.
[200] L. Yang, A. S. Rakin, and D. Fan, “Da3: Dynamic additive attention
adaption for memory-efficient on-device multi-domain learning,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 2619–2627.[201] A. K. Kosta, M. A. Anwar et al. , “Rapid-rl: A reconfigurable architec-
ture with preemptive-exits for efficient deep-reinforcement learning,”
in2022 International Conference on Robotics and Automation (ICRA) .
IEEE, 2022, pp. 7492–7498.
[202] P. Liu, B. Qi, and S. Banerjee, “Edgeeye: An edge service framework
for real-time intelligent video analytics,” in Proceedings of the 1st
International Workshop on Edge Systems, Analytics and Networking ,
2018, pp. 1–6.
[203] M. Farhadi, M. Ghasemi, S. Vrudhula, and Y . Yang, “Enabling in-
cremental knowledge transfer for object detection at the edge,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops , 2020, pp. 396–397.
[204] Z. Susskind, A. Arora, I. D. Miranda, L. A. Villon, R. F. Katopodis,
L. S. de Ara ´ujo, D. L. Dutra, P. M. Lima, F. M. Franc ¸a, M. Breternitz Jr
et al. , “Weightless neural networks for efficient edge inference,” in
Proceedings of the International Conference on Parallel Architectures
and Compilation Techniques , 2022, pp. 279–290.
[205] S. He, H. Meng, Z. Zhou, Y . Liu, K. Huang, and G. Chen, “An efficient
gpu-accelerated inference engine for binary neural network on mobile
phones,” Journal of Systems Architecture , vol. 117, p. 102156, 2021.
[206] X. Zhang, W. Jiang, and J. Hu, “Achieving full parallelism in lstm
via a unified accelerator design,” in 2020 IEEE 38th International
Conference on Computer Design (ICCD) . IEEE, 2020, pp. 469–477.
[207] Z. Zhou, J. Liu, Z. Gu, and G. Sun, “Energon: Toward efficient
acceleration of transformers using dynamic sparse attention,” IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems , vol. 42, no. 1, pp. 136–149, 2022.
[208] Z. Zhou, B. Shi, Z. Zhang, Y . Guan, G. Sun, and G. Luo, “Blockgnn:
Towards efficient gnn acceleration using block-circulant weight matri-
ces,” in 2021 58th ACM/IEEE Design Automation Conference (DAC) .
IEEE, 2021, pp. 1009–1014.
[209] M. Kwon, D. Gouk et al. , “Hardware/software co-programmable
framework for computational ssds to accelerate deep learning service
on large-scale graphs,” in 20th USENIX Conference on File and Storage
Technologies (FAST 22) , 2022, pp. 147–164.
[210] T. Yang, D. Li, F. Ma, Z. Song, Y . Zhao, J. Zhang, F. Liu, and L. Jiang,
“Pasgcn: An reram-based pim design for gcn with adaptively sparsified
graphs,” IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems , vol. 42, no. 1, pp. 150–163, 2022.
[211] A. V . Nori, R. Bera et al. , “Reduct: Keep it close, keep it cool!: Efficient
scaling of dnn inference on multi-core cpus with near-cache compute,”
in2021 ACM/IEEE 48th Annual International Symposium on Computer
Architecture (ISCA) . IEEE, 2021, pp. 167–180.
[212] T. Jia, Y . Ju et al. , “Ncpu: An embedded neural cpu architecture on
resource-constrained low power devices for real-time end-to-end per-
formance,” in 2020 53rd Annual IEEE/ACM International Symposium
on Microarchitecture (MICRO) . IEEE, 2020, pp. 1097–1109.
[213] N. Capodieci, R. Cavicchioli, M. Bertogna, and A. Paramakuru,
“Deadline-based scheduling for gpu with preemption support,” in 2018
IEEE Real-Time Systems Symposium (RTSS) . IEEE, 2018, pp. 119–
130.
[214] Z. Choudhury, S. Shrivastava, L. Ramapantulu, and S. Purini, “An fpga
overlay for cnn inference with fine-grained flexible parallelism,” ACM
Transactions on Architecture and Code Optimization (TACO) , vol. 19,
no. 3, pp. 1–26, 2022.
[215] Y . Yu, T. Zhao, K. Wang, and L. He, “Light-opu: An fpga-based
overlay processor for lightweight convolutional neural networks,” in
Proceedings of the 2020 ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays , 2020, pp. 122–132.
[216] A. Roohi, S. Sheikhfaal, S. Angizi, D. Fan, and R. F. DeMara, “Apgan:
Approximate gan for robust low energy learning from imprecise
components,” IEEE Transactions on Computers , vol. 69, no. 3, pp.
349–360, 2019.
[217] A. Kouris, S. I. Venieris, S. Laskaridis, and N. D. Lane, “Fluid
batching: Exit-aware preemptive serving of early-exit neural networks
on edge npus,” arXiv preprint arXiv:2209.13443 , 2022.
[218] Q. Yang and H. Li, “Bitsystolic: A 26.7 tops/w 2b˜ 8b npu with
configurable data flows for edge devices,” IEEE Transactions on
Circuits and Systems I: Regular Papers , vol. 68, no. 3, pp. 1134–1145,
2020.
[219] Y . Wang, D. Deng, L. Liu et al. , “Pl-npu: An energy-efficient edge-
device dnn training processor with posit-based logarithm-domain com-
puting,” IEEE Transactions on Circuits and Systems I: Regular Papers ,
vol. 69, no. 10, pp. 4042–4055, 2022.
[220] H. Cho, J. Lee, and J. Lee, “Farnn: Fpga-gpu hybrid acceleration
platform for recurrent neural networks,” IEEE Transactions on Parallel
and Distributed Systems , vol. 33, no. 7, pp. 1725–1738, 2021.

--- PAGE 30 ---
30
[221] Y . Xiang and H. Kim, “Pipelined data-parallel cpu/gpu scheduling
for multi-dnn real-time inference,” in 2019 IEEE Real-Time Systems
Symposium (RTSS) . IEEE, 2019, pp. 392–405.
[222] S. Zhou and L. Zhang, “Smart home electricity demand forecasting
system based on edge computing,” in 2018 IEEE 9th International
Conference on Software Engineering and Service Science (ICSESS) .
IEEE, 2018, pp. 164–167.
[223] Y . Zhan and H. Haddadi, “Towards automating smart homes: Con-
textual and temporal dynamics of activity prediction,” in Adjunct
Proceedings of the 2019 ACM International Joint Conference on
Pervasive and Ubiquitous Computing and Proceedings of the 2019
ACM International Symposium on Wearable Computers , 2019, pp. 413–
417.
[224] T. Han, K. Muhammad, T. Hussain, J. Lloret, and S. W. Baik, “An
efficient deep learning framework for intelligent energy management
in iot networks,” IEEE Internet of Things Journal , vol. 8, no. 5, pp.
3170–3179, 2020.
[225] W. Zhou, Y . Jia, Y . Yao, L. Zhu, L. Guan, Y . Mao, P. Liu, and
Y . Zhang, “Discovering and understanding the security hazards in the
interactions between iot devices, mobile apps, and clouds on smart
home platforms,” pp. 1133–1150, 2019.
[226] L. Li, K. Ota, and M. Dong, “Deep learning for smart industry:
Efficient manufacture inspection system with fog computing,” IEEE
Transactions on Industrial Informatics , vol. 14, no. 10, pp. 4665–4673,
2018.
[227] Y . Chu, D. Feng, Z. Liu, L. Zhang, Z. Zhao, Z. Wang, Z. Feng, and X.-
G. Xia, “A fine-grained attention model for high accuracy operational
robot guidance,” IEEE Internet of Things Journal , 2022.
[228] S. Tuli, N. Basumatary, S. S. Gill et al. , “Healthfog: An ensemble
deep learning based smart healthcare system for automatic diagnosis
of heart diseases in integrated iot and fog computing environments,”
Future Generation Computer Systems , vol. 104, pp. 187–200, 2020.
[229] L. Verde, N. Brancati, G. De Pietro, M. Frucci, and G. Sannino, “A deep
learning approach for voice disorder detection for smart connected liv-
ing environments,” ACM Transactions on Internet Technology (TOIT) ,
vol. 22, no. 1, pp. 1–16, 2021.
[230] M. A. Rahman, M. S. Hossain, N. A. Alrajeh, and N. Guizani, “B5g
and explainable deep learning assisted healthcare vertical at the edge:
Covid-i9 perspective,” IEEE Network , vol. 34, no. 4, pp. 98–105, 2020.
[231] X. Kong, K. Wang, S. Wang, X. Wang, X. Jiang, Y . Guo, G. Shen,
X. Chen, and Q. Ni, “Real-time mask identification for covid-19:
An edge-computing-based deep learning framework,” IEEE Internet
of Things Journal , vol. 8, no. 21, pp. 15 929–15 938, 2021.
[232] M. S. Hossain and G. Muhammad, “Deep learning based pathology
detection for smart connected healthcare,” IEEE Network , vol. 34, no. 6,
pp. 120–125, 2020.
[233] G. Muhammad, M. F. Alhamid, and X. Long, “Computing and process-
ing on the edge: Smart pathology detection for connected healthcare,”
IEEE Network , vol. 33, no. 6, pp. 44–49, 2019.
[234] Y . Liu, Z. Ma, X. Liu, S. Ma, and K. Ren, “Privacy-preserving object
detection for medical images with faster r-cnn,” IEEE Transactions on
Information Forensics and Security , vol. 17, pp. 69–84, 2019.
[235] Q. Luo, C. Li, T. H. Luan, and W. Shi, “Collaborative data scheduling
for vehicular edge computing via deep reinforcement learning,” IEEE
Internet of Things Journal , vol. 7, no. 10, pp. 9637–9650, 2020.
[236] X. Jiang, F. R. Yu, T. Song, and V . C. Leung, “Intelligent resource allo-
cation for video analytics in blockchain-enabled internet of autonomous
vehicles with edge computing,” IEEE Internet of Things Journal , vol. 9,
no. 16, pp. 14 260–14 272, 2020.
[237] Q. Liu, T. Han, J. L. Xie, and B. Kim, “Livemap: Real-time dynamic
map in automotive edge computing,” in IEEE INFOCOM 2021-IEEE
Conference on Computer Communications . IEEE, 2021, pp. 1–10.
[238] S. Liang, H. Wu, L. Zhen et al. , “Edge yolo: Real-time intelligent object
detection system based on edge-cloud cooperation in autonomous
vehicles,” IEEE Transactions on Intelligent Transportation Systems ,
vol. 23, no. 12, pp. 25 345–25 360, 2022.
[239] A. Malawade, M. Odema, S. Lajeunesse-DeGroot, and M. A.
Al Faruque, “Sage: A split-architecture methodology for efficient end-
to-end autonomous vehicle control,” ACM Transactions on Embedded
Computing Systems (TECS) , vol. 20, no. 5s, pp. 1–22, 2021.
[240] J. Wang, H. Ke, X. Liu, and H. Wang, “Optimization for computational
offloading in multi-access edge computing: A deep reinforcement
learning scheme,” Computer Networks , vol. 204, p. 108690, 2022.
[241] P. Zhao, W. Niu, G. Yuan et al. , “Brief industry paper: Towards
real-time 3d object detection for autonomous vehicles with pruning
search,” in 2021 IEEE 27th Real-Time and Embedded Technology and
Applications Symposium (RTAS) . IEEE, 2021, pp. 425–428.[242] S. Wang, S. Yang, and C. Zhao, “Surveiledge: Real-time video query
based on collaborative cloud-edge deep learning,” in IEEE INFOCOM
2020-IEEE Conference on Computer Communications . IEEE, 2020,
pp. 2519–2528.
[243] H. D. Trinh, L. Giupponi, and P. Dini, “Urban anomaly detection by
processing mobile traffic traces with lstm neural networks,” in 2019
16th Annual IEEE International Conference on Sensing, Communica-
tion, and Networking (SECON) . IEEE, 2019, pp. 1–8.
[244] R. Dautov, S. Distefano, D. Bruneo, F. Longo, G. Merlino, A. Puliafito,
and R. Buyya, “Metropolitan intelligent surveillance systems for urban
areas by harnessing iot and edge computing paradigms,” Software:
Practice and experience , vol. 48, no. 8, pp. 1475–1492, 2018.
[245] A. Menshchikov, D. Shadrin, V . Prutyanov, D. Lopatkin, S. Sosnin
et al. , “Real-time detection of hogweed: Uav platform empowered by
deep learning,” IEEE Transactions on Computers , vol. 70, no. 8, pp.
1175–1188, 2021.
[246] Y . Zhang, J. Yu, Y . Chen, W. Yang, W. Zhang, and Y . He, “Real-
time strawberry detection using deep neural networks on embedded
system (rtsd-net): An edge ai application,” Computers and Electronics
in Agriculture , vol. 192, p. 106586, 2022.
[247] D. J. A. Rustia, L.-Y . Chiu, C.-Y . Lu, Y .-F. Wu, S.-K. Chen et al. ,
“Towards intelligent and integrated pest management through an aiot-
based monitoring system,” Pest Management Science , vol. 78, no. 10,
pp. 4288–4302, 2022.
[248] X. Liu, Y . Jiang, P. Jain, and K.-H. Kim, “Tar: Enabling fine-grained
targeted advertising in retail stores,” in Proceedings of the 16th An-
nual International Conference on Mobile Systems, Applications, and
Services , 2018, pp. 323–336.
[249] Y . Liu, C. Yang, L. Jiang, S. Xie, and Y . Zhang, “Intelligent edge
computing for iot-based energy management in smart cities,” IEEE
Network , vol. 33, no. 2, pp. 111–117, 2019.
[250] A. Alsalemi, Y . Himeur, F. Bensaali, and A. Amira, “An innovative
edge-based internet of energy solution for promoting energy saving in
buildings,” Sustainable Cities and Society , vol. 78, p. 103571, 2022.
[251] E. Li, L. Zeng, Z. Zhou, and X. Chen, “Edge ai: On-demand ac-
celerating deep neural network inference via edge computing,” IEEE
Transactions on Wireless Communications , vol. 19, no. 1, pp. 447–457,
2019.
[252] N. Shlezinger, E. Farhan, H. Morgenstern, and Y . C. Eldar, “Collabora-
tive inference via ensembles on the edge,” in ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2021, pp. 8478–8482.
[253] A. Banitalebi-Dehkordi, N. Vedula, J. Pei, F. Xia, L. Wang, and
Y . Zhang, “Auto-split: a general framework of collaborative edge-
cloud ai,” in Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining , 2021, pp. 2543–2553.
[254] Y . Luo and S. Yu, “Ailc: Accelerate on-chip incremental learning with
compute-in-memory technology,” IEEE Transactions on Computers ,
vol. 70, no. 8, pp. 1225–1238, 2021.
[255] Z. Lv, L. Qiao, and S. Verma, “Ai-enabled iot-edge data analytics for
connected living,” ACM Transactions on Internet Technology , vol. 21,
no. 4, pp. 1–20, 2021.
[256] T. X. Tran, D. V . Le, G. Yue, and D. Pompili, “Cooperative hierarchical
caching and request scheduling in a cloud radio access network,” IEEE
Transactions on Mobile Computing , vol. 17, no. 12, pp. 2729–2743,
2018.
[257] Z. Ning, K. Zhang, X. Wang, L. Guo et al. , “Intelligent edge computing
in internet of vehicles: a joint computation offloading and caching
solution,” IEEE Transactions on Intelligent Transportation Systems ,
vol. 22, no. 4, pp. 2212–2225, 2020.
[258] Q. Ren, O. Abbasi, G. K. Kurt et al. , “Caching and computation
offloading in high altitude platform station (haps) assisted intelligent
transportation systems,” IEEE Transactions on Wireless Communica-
tions , vol. 21, no. 11, pp. 9010–9024, 2022.
[259] F. Paissan, A. Ancilotto, and E. Farella, “Phinets: a scalable backbone
for low-power ai at the edge,” ACM Transactions on Embedded
Computing Systems , vol. 21, no. 5, pp. 1–18, 2022.
[260] A. Moran, C. F. Frasser, M. Roca, and J. L. Rossello, “Energy-efficient
pattern recognition hardware with elementary cellular automata,” IEEE
Transactions on Computers , vol. 69, no. 3, pp. 392–401, 2019.
[261] J. Nunez-Yanez and N. Howard, “Energy-efficient neural networks
with near-threshold processors and hardware accelerators,” Journal of
Systems Architecture , vol. 116, p. 102062, 2021.
[262] N. K. Jayakodi, J. R. Doppa, and P. P. Pande, “A general hardware
and software co-design framework for energy-efficient edge ai,” in
2021 IEEE/ACM International Conference On Computer Aided Design
(ICCAD) . IEEE, 2021, pp. 1–7.

--- PAGE 31 ---
31
[263] A. H. Sodhro, S. Pirbhulal, and V . H. C. De Albuquerque, “Artificial
intelligence-driven mechanism for edge computing-based industrial
applications,” IEEE Transactions on Industrial Informatics , vol. 15,
no. 7, pp. 4235–4243, 2019.
[264] T. Tambe, E.-Y . Yang, G. G. Ko et al. , “A 16-nm soc for noise-robust
speech and nlp edge ai inference with bayesian sound source separation
and attention-based dnns,” IEEE Journal of Solid-State Circuits , 2022.
[265] Y . Hao, Y . Miao, L. Hu, M. S. Hossain, G. Muhammad, and S. U.
Amin, “Smart-edge-cocaco: Ai-enabled smart edge with joint com-
putation, caching, and communication in heterogeneous iot,” IEEE
Network , vol. 33, no. 2, pp. 58–64, 2019.
[266] M. Ham, J. Moon, G. Lim et al. , “Nnstreamer: Efficient and agile
development of on-device ai systems,” in 2021 IEEE/ACM 43rd Inter-
national Conference on Software Engineering: Software Engineering
in Practice (ICSE-SEIP) . IEEE, 2021, pp. 198–207.
[267] M. Ham, S. Woo, J. Jung, W. Song et al. , “Toward among-device
ai from on-device ai with stream pipelines,” in Proceedings of the
44th International Conference on Software Engineering: Software
Engineering in Practice , 2022, pp. 285–294.
[268] W.-H. Chen, C. Dou, K.-X. Li, W.-Y . Lin, P.-Y . Li, J.-H. Huang, J.-H.
Wang et al. , “Cmos-integrated memristive non-volatile computing-in-
memory for ai edge processors,” Nature Electronics , vol. 2, no. 9, pp.
420–428, 2019.
[269] J. Xiong, M. Zhao, M. Z. A. Bhuiyan, L. Chen, and Y . Tian, “An
ai-enabled three-party game framework for guaranteed data privacy in
mobile edge crowdsensing of iot,” IEEE Transactions on Industrial
Informatics , vol. 17, no. 2, pp. 922–933, 2019.
[270] Q. Zhang, H. Zhong, W. Shi, and L. Liu, “A trusted and collaborative
framework for deep learning in iot,” Computer Networks , vol. 193, p.
108055, 2021.
[271] Q. Li, J. Ren, X. Pan et al. , “Enigma: Low-latency and privacy-
preserving edge inference on heterogeneous neural network acceler-
ators,” in 2022 IEEE 42nd International Conference on Distributed
Computing Systems (ICDCS) . IEEE, 2022, pp. 458–469.
[272] S. Sinha, S. Saha, M. Alam et al. , “Exploring bitslicing architectures
for enabling fhe-assisted machine learning,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems , vol. 41,
no. 11, pp. 4004–4015, 2022.
[273] M. S. Rahman, I. Khalil et al. , “Towards privacy preserving ai based
composition framework in edge networks using fully homomorphic
encryption,” Engineering Applications of Artificial Intelligence , vol. 94,
p. 103737, 2020.
[274] K. Wang, S. P. Xu, C.-M. Chen, S. H. Islam, M. M. Hassan et al. ,
“A trusted consensus scheme for collaborative learning in the edge ai
computing domain,” IEEE Network , vol. 35, no. 1, pp. 204–210, 2021.
[275] M. Song, Z. Wang, Z. Zhang, Y . Song, Q. Wang, J. Ren, and H. Qi,
“Analyzing user-level privacy attack against federated learning,” IEEE
Journal on Selected Areas in Communications , vol. 38, no. 10, pp.
2430–2444, 2020.
[276] W. Y . B. Lim, J. S. Ng, Z. Xiong, J. Jin et al. , “Decentralized edge
intelligence: A dynamic resource allocation framework for hierarchical
federated learning,” IEEE Transactions on Parallel and Distributed
Systems , vol. 33, no. 3, pp. 536–550, 2021.
[277] S. Yu, P. Nguyen, W. Abebe et al. , “Spatl: salient parameter aggregation
and transfer learning for heterogeneous federated learning,” in 2022
SC22: International Conference for High Performance Computing,
Networking, Storage and Analysis (SC) . IEEE Computer Society,
2022, pp. 495–508.
[278] J. Guo, J. Wu, A. Liu, and N. N. Xiong, “Lightfed: An efficient
and secure federated edge learning system on model splitting,” IEEE
Transactions on Parallel and Distributed Systems , vol. 33, no. 11, pp.
2701–2713, 2021.
[279] A. Huang, Y . Liu, T. Chen, Y . Zhou, Q. Sun, H. Chai, and Q. Yang,
“Starfl: Hybrid federated learning architecture for smart urban comput-
ing,” ACM Transactions on Intelligent Systems and Technology (TIST) ,
vol. 12, no. 4, pp. 1–23, 2021.
[280] Z. Tang, W. Jia, X. Zhou, W. Yang, and Y . You, “Representation and
reinforcement learning for task scheduling in edge computing,” IEEE
Transactions on Big Data , vol. 8, no. 3, pp. 795–808, 2020.
[281] J. Lou, Z. Tang, W. Jia, W. Zhao, and J. Li, “Startup-aware dependent
task scheduling with bandwidth constraints in edge computing,” IEEE
Transactions on Mobile Computing , 2023.
[282] J. Lou, Z. Tang, S. Zhang, W. Jia, W. Zhao, and J. Li, “Cost-effective
scheduling for dependent tasks with tight deadline constraints in mobile
edge computing,” IEEE Transactions on Mobile Computing , 2022.[283] S. Zhang, W. Jia, Z. Tang, J. Lou, and W. Zhao, “Efficient instance
reuse approach for service function chain placement in mobile edge
computing,” Computer Networks , vol. 211, p. 109010, 2022.
[284] J. Lou, H. Luo, Z. Tang, W. Jia, and W. Zhao, “Efficient container as-
signment and layer sequencing in edge computing,” IEEE Transactions
on Services Computing , 2022.
[285] Z. Tang, J. Lou, and W. Jia, “Layer dependency-aware learning
scheduling algorithms for containers in mobile edge computing,” IEEE
Transactions on Mobile Computing , 2022.
[286] L. Gu, D. Zeng, J. Hu, H. Jin, S. Guo, and A. Y . Zomaya, “Exploring
layered container structure for cost efficient microservice deployment,”
inIEEE INFOCOM 2021-IEEE Conference on Computer Communi-
cations . IEEE, 2021, pp. 1–9.
[287] J. Lou, Z. Tang, and W. Jia, “Energy-efficient joint task assignment and
migration in data centers: A deep reinforcement learning approach,”
IEEE Transactions on Network and Service Management , 2022.
[288] Z. Tang, X. Zhou, F. Zhang, W. Jia, and W. Zhao, “Migration
modeling and learning algorithms for containers in fog computing,”
IEEE Transactions on Services Computing , vol. 12, no. 5, pp. 712–
725, 2018.
[289] S. Wang, R. Urgaonkar, M. Zafer, T. He, K. Chan, and K. K. Leung,
“Dynamic service migration in mobile edge computing based on
markov decision process,” IEEE/ACM Transactions on Networking ,
vol. 27, no. 3, pp. 1272–1288, 2019.
[290] Y . Ma, W. Liang, J. Li, X. Jia, and S. Guo, “Mobility-aware and delay-
sensitive service provisioning in mobile edge-cloud networks,” IEEE
Transactions on Mobile Computing , vol. 21, no. 1, pp. 196–210, 2020.
[291] L. Ma, S. Yi, N. Carter, and Q. Li, “Efficient live migration of edge
services leveraging container layered storage,” IEEE Transactions on
Mobile Computing , vol. 18, no. 9, pp. 2020–2033, 2018.
[292] T. Benjaponpitak, M. Karakate, and K. Sripanidkulchai, “Enabling
live migration of containerized applications across clouds,” in In
Proceedings of 2020 IEEE Conference on Computer Communications
(INFOCOM) . IEEE, 2020, pp. 2529–2538.
[293] Z. Tang, F. Zhang, X. Zhou, W. Jia, and W. Zhao, “Pricing model for
dynamic resource overbooking in edge computing,” IEEE Transactions
on Cloud Computing , 2022.
[294] S. Hu, W. Shi, and G. Li, “Cec: A containerized edge computing
framework for dynamic resource provisioning,” IEEE Transactions on
Mobile Computing , 2022.
[295] Z. Luo, C. Wu, Z. Li, and W. Zhou, “Scaling geo-distributed network
function chains: A prediction and learning framework,” IEEE Journal
on Selected Areas in Communications , vol. 37, no. 8, pp. 1838–1850,
2019.
[296] S. Wang, Z. Ding, and C. Jiang, “Elastic scheduling for microservice
applications in clouds,” IEEE Transactions on Parallel and Distributed
Systems , vol. 32, no. 1, pp. 98–115, 2020.
[297] W. Lv, Q. Wang, P. Yang, Y . Ding, B. Yi, Z. Wang, and C. Lin, “Mi-
croservice deployment in edge computing based on deep q learning,”
IEEE Transactions on Parallel and Distributed Systems , vol. 33, no. 11,
pp. 2968–2978, 2022.
