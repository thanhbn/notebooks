# 2004.13820v2.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: D:\llm\notebooks\AI-Papers\2004.13820v2.pdf
# KÃ­ch thÆ°á»›c file: 2784225 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================


--- TRANG 1 ---
Sá»± PhÃ¡t triá»ƒn cá»§a Äá»™ TÆ°Æ¡ng Ä‘á»“ng Ngá»¯ nghÄ©a - Má»™t Kháº£o sÃ¡t
DHIVYA CHANDRASEKARAN vÃ  VIJAY MAGO, Äáº¡i há»c Lakehead

Æ¯á»›c lÆ°á»£ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a dá»¯ liá»‡u vÄƒn báº£n lÃ  má»™t trong nhá»¯ng bÃ i toÃ¡n nghiÃªn cá»©u thÃ¡ch thá»©c vÃ  cÃ²n má»Ÿ trong lÄ©nh vá»±c Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP). TÃ­nh Ä‘a dáº¡ng cá»§a ngÃ´n ngá»¯ tá»± nhiÃªn khiáº¿n viá»‡c Ä‘á»‹nh nghÄ©a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn quy táº¯c Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a trá»Ÿ nÃªn khÃ³ khÄƒn. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, nhiá»u phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t qua cÃ¡c nÄƒm. BÃ i kháº£o sÃ¡t nÃ y truy tÃ¬m sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã³ báº¯t Ä‘áº§u tá»« cÃ¡c ká»¹ thuáº­t NLP truyá»n thá»‘ng nhÆ° phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kernel Ä‘áº¿n cÃ´ng trÃ¬nh nghiÃªn cá»©u gáº§n Ä‘Ã¢y nháº¥t vá» cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn transformer, phÃ¢n loáº¡i chÃºng dá»±a trÃªn cÃ¡c nguyÃªn lÃ½ cÆ¡ báº£n nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c, dá»±a trÃªn corpus, dá»±a trÃªn máº¡ng neural sÃ¢u, vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p lai. Tháº£o luáº­n vá» Ä‘iá»ƒm máº¡nh vÃ  Ä‘iá»ƒm yáº¿u cá»§a tá»«ng phÆ°Æ¡ng phÃ¡p, kháº£o sÃ¡t nÃ y cung cáº¥p cÃ¡i nhÃ¬n toÃ n diá»‡n vá» cÃ¡c há»‡ thá»‘ng hiá»‡n cÃ³, cho cÃ¡c nhÃ  nghiÃªn cá»©u má»›i thá»­ nghiá»‡m vÃ  phÃ¡t triá»ƒn cÃ¡c Ã½ tÆ°á»Ÿng sÃ¡ng táº¡o Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a.

CCS Concepts: â€¢Tá»•ng quÃ¡t vÃ  tham kháº£o â†’Kháº£o sÃ¡t vÃ  tá»•ng quan ;â€¢Há»‡ thá»‘ng thÃ´ng tin â†’
Ontology ;â€¢LÃ½ thuyáº¿t tÃ­nh toÃ¡n â†’Há»c khÃ´ng giÃ¡m sÃ¡t vÃ  phÃ¢n cá»¥m ;â€¢PhÆ°Æ¡ng phÃ¡p luáº­n 
tÃ­nh toÃ¡n â†’Ngá»¯ nghÄ©a tá»« vá»±ng .

Tá»« khÃ³a vÃ  Cá»¥m tá»« bá»• sung: tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a, ngÃ´n ngá»¯ há»c, phÆ°Æ¡ng phÃ¡p cÃ³ giÃ¡m sÃ¡t vÃ  khÃ´ng giÃ¡m sÃ¡t, phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c, word embeddings, phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus

Äá»‹nh dáº¡ng Tham kháº£o ACM:
Dhivya Chandrasekaran vÃ  Vijay Mago. 2020. Evolution of Semantic Similarity - A Survey. 1, 1 (February 2020), 35 pages. https://doi.org/---------

1 GIá»šI THIá»†U
Vá»›i sá»± gia tÄƒng theo cáº¥p sá»‘ nhÃ¢n cá»§a dá»¯ liá»‡u vÄƒn báº£n Ä‘Æ°á»£c táº¡o ra theo thá»i gian, Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn (NLP) Ä‘Ã£ nháº­n Ä‘Æ°á»£c sá»± chÃº Ã½ Ä‘Ã¡ng ká»ƒ tá»« cÃ¡c chuyÃªn gia TrÃ­ tuá»‡ NhÃ¢n táº¡o (AI). Äo lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a cÃ¡c thÃ nh pháº§n vÄƒn báº£n khÃ¡c nhau nhÆ° tá»«, cÃ¢u, hoáº·c tÃ i liá»‡u Ä‘Ã³ng vai trÃ² quan trá»ng trong má»™t loáº¡t cÃ¡c nhiá»‡m vá»¥ NLP nhÆ° truy xuáº¥t thÃ´ng tin [48], tÃ³m táº¯t vÄƒn báº£n [80], phÃ¢n loáº¡i vÄƒn báº£n [49], Ä‘Ã¡nh giÃ¡ bÃ i luáº­n [42], dá»‹ch mÃ¡y [134], tráº£ lá»i cÃ¢u há»i [19,66], trong sá»‘ nhá»¯ng nhiá»‡m vá»¥ khÃ¡c. Trong nhá»¯ng ngÃ y Ä‘áº§u, hai Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c coi lÃ  tÆ°Æ¡ng tá»± náº¿u chÃºng chá»©a cÃ¹ng cÃ¡c tá»«/kÃ½ tá»±. CÃ¡c ká»¹ thuáº­t nhÆ° Bag of Words (BoW), Term Frequency - Inverse Document Frequency (TF-IDF) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biá»ƒu diá»…n vÄƒn báº£n, nhÆ° cÃ¡c vector giÃ¡ trá»‹ thá»±c Ä‘á»ƒ há»— trá»£ tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. Tuy nhiÃªn, nhá»¯ng ká»¹ thuáº­t nÃ y khÃ´ng tÃ­nh Ä‘áº¿n thá»±c táº¿ ráº±ng cÃ¡c tá»« cÃ³ nghÄ©a khÃ¡c nhau vÃ  cÃ¡c tá»« khÃ¡c nhau cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biá»ƒu diá»…n má»™t khÃ¡i niá»‡m tÆ°Æ¡ng tá»±. VÃ­ dá»¥, xem xÃ©t hai cÃ¢u "John vÃ  David Ä‘Ã£ há»c ToÃ¡n vÃ  Khoa há»c." vÃ  "John Ä‘Ã£ há»c ToÃ¡n vÃ  David Ä‘Ã£ há»c Khoa há»c." Máº·c dÃ¹ hai cÃ¢u nÃ y cÃ³ chÃ­nh xÃ¡c cÃ¹ng cÃ¡c tá»« nhÆ°ng chÃºng khÃ´ng truyá»n Ä‘áº¡t cÃ¹ng má»™t Ã½ nghÄ©a. TÆ°Æ¡ng tá»±, cÃ¡c cÃ¢u "Mary bá»‹ dá»‹ á»©ng vá»›i cÃ¡c sáº£n pháº©m tá»« sá»¯a." vÃ  "Mary khÃ´ng dung náº¡p lactose." truyá»n Ä‘áº¡t cÃ¹ng má»™t Ã½ nghÄ©a; tuy nhiÃªn, chÃºng khÃ´ng cÃ³ cÃ¹ng má»™t táº­p há»£p cÃ¡c tá»«. Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y náº¯m báº¯t Ä‘Æ°á»£c Ä‘áº·c trÆ°ng tá»« vá»±ng cá»§a vÄƒn báº£n vÃ  Ä‘Æ¡n giáº£n Ä‘á»ƒ triá»ƒn khai, tuy nhiÃªn, chÃºng bá» qua cÃ¡c thuá»™c tÃ­nh ngá»¯ nghÄ©a vÃ  cÃº phÃ¡p cá»§a vÄƒn báº£n. Äá»ƒ giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ nÃ y cá»§a cÃ¡c Ä‘á»™ Ä‘o tá»« vá»±ng, nhiá»u ká»¹ thuáº­t tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t trong ba tháº­p ká»· qua.

Äá»™ TÆ°Æ¡ng Ä‘á»“ng VÄƒn báº£n Ngá»¯ nghÄ©a (STS) Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘Æ°Æ¡ng ngá»¯ nghÄ©a giá»¯a hai khá»‘i vÄƒn báº£n. CÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a thÆ°á»ng Ä‘Æ°a ra má»™t xáº¿p háº¡ng hoáº·c tá»· lá»‡ pháº§n trÄƒm tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c vÄƒn báº£n, thay vÃ¬ má»™t quyáº¿t Ä‘á»‹nh nhá»‹ phÃ¢n lÃ  tÆ°Æ¡ng tá»± hay khÃ´ng tÆ°Æ¡ng tá»±. Äá»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»“ng nghÄ©a vá»›i má»‘i liÃªn quan ngá»¯ nghÄ©a. Tuy nhiÃªn, má»‘i liÃªn quan ngá»¯ nghÄ©a khÃ´ng chá»‰ tÃ­nh Ä‘áº¿n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a cÃ¡c vÄƒn báº£n mÃ  cÃ²n xem xÃ©t má»™t gÃ³c nhÃ¬n rá»™ng hÆ¡n phÃ¢n tÃ­ch cÃ¡c thuá»™c tÃ­nh ngá»¯ nghÄ©a chung cá»§a hai tá»«. VÃ­ dá»¥, cÃ¡c tá»« 'cÃ  phÃª' vÃ  'cá»‘c' cÃ³ thá»ƒ liÃªn quan cháº·t cháº½ vá»›i nhau, nhÆ°ng chÃºng khÃ´ng Ä‘Æ°á»£c coi lÃ  tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a trong khi cÃ¡c tá»« 'cÃ  phÃª' vÃ  'trÃ ' thÃ¬ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. Do Ä‘Ã³, Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t trong nhá»¯ng khÃ­a cáº¡nh cá»§a má»‘i liÃªn quan ngá»¯ nghÄ©a. Má»‘i quan há»‡ ngá»¯ nghÄ©a bao gá»“m tÆ°Æ¡ng Ä‘á»“ng Ä‘Æ°á»£c Ä‘o báº±ng khoáº£ng cÃ¡ch ngá»¯ nghÄ©a, tá»· lá»‡ nghá»‹ch vá»›i má»‘i quan há»‡ [37].

HÃ¬nh 1. Kiáº¿n trÃºc Kháº£o sÃ¡t

1.1 Äá»™ng lá»±c Ä‘áº±ng sau kháº£o sÃ¡t
Háº§u háº¿t cÃ¡c bÃ i kháº£o sÃ¡t Ä‘Æ°á»£c xuáº¥t báº£n gáº§n Ä‘Ã¢y liÃªn quan Ä‘áº¿n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a, cung cáº¥p kiáº¿n thá»©c sÃ¢u sáº¯c vá» má»™t ká»¹ thuáº­t tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a cá»¥ thá»ƒ hoáº·c má»™t á»©ng dá»¥ng duy nháº¥t cá»§a Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. Lastra-DÃ­az et al. kháº£o sÃ¡t cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c khÃ¡c nhau [55] vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn IC [53], Camacho-Collados et al. [20] tháº£o luáº­n cÃ¡c phÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n vector khÃ¡c nhau cá»§a tá»«, Taieb et al. [37], máº·t khÃ¡c, mÃ´ táº£ cÃ¡c phÆ°Æ¡ng phÃ¡p liÃªn quan ngá»¯ nghÄ©a khÃ¡c nhau vÃ  Berna AltÄ±nel et al. [8] tÃ³m táº¯t cÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng cho phÃ¢n loáº¡i vÄƒn báº£n. Äá»™ng lá»±c Ä‘áº±ng sau kháº£o sÃ¡t nÃ y lÃ  cung cáº¥p má»™t bÃ¡o cÃ¡o toÃ n diá»‡n vá» cÃ¡c ká»¹ thuáº­t tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a khÃ¡c nhau bao gá»“m nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y nháº¥t sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn máº¡ng neural sÃ¢u.

--- TRANG 2 ---
2 D Chandrasekaran vÃ  V Mago

--- TRANG 3 ---
Sá»± PhÃ¡t triá»ƒn cá»§a Äá»™ TÆ°Æ¡ng Ä‘á»“ng Ngá»¯ nghÄ©a - Má»™t Kháº£o sÃ¡t 3

Kháº£o sÃ¡t nÃ y truy tÃ¬m sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c Ká»¹ thuáº­t TÆ°Æ¡ng Ä‘á»“ng Ngá»¯ nghÄ©a qua cÃ¡c tháº­p ká»· qua, phÃ¢n biá»‡t chÃºng dá»±a trÃªn cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ báº£n Ä‘Æ°á»£c sá»­ dá»¥ng trong Ä‘Ã³. HÃ¬nh 1 hiá»ƒn thá»‹ cáº¥u trÃºc cá»§a kháº£o sÃ¡t. Má»™t bÃ¡o cÃ¡o chi tiáº¿t vá» cÃ¡c dataset Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cÃ³ sáºµn cho Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a Ä‘Æ°á»£c cung cáº¥p trong Pháº§n 2. CÃ¡c Pháº§n 3 Ä‘áº¿n 6 cung cáº¥p mÃ´ táº£ chi tiáº¿t vá» cÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a Ä‘Æ°á»£c phÃ¢n loáº¡i rá»™ng rÃ£i thÃ nh 1) PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c, 2) PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus, 3) PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn máº¡ng neural sÃ¢u, vÃ  4) PhÆ°Æ¡ng phÃ¡p lai. Pháº§n 7 phÃ¢n tÃ­ch cÃ¡c khÃ­a cáº¡nh vÃ  suy luáº­n khÃ¡c nhau cá»§a kháº£o sÃ¡t Ä‘Æ°á»£c tiáº¿n hÃ nh. Kháº£o sÃ¡t nÃ y cung cáº¥p kiáº¿n thá»©c sÃ¢u vÃ  rá»™ng vá» cÃ¡c ká»¹ thuáº­t hiá»‡n cÃ³ cho cÃ¡c nhÃ  nghiÃªn cá»©u má»›i máº¡o hiá»ƒm khÃ¡m phÃ¡ má»™t trong nhá»¯ng nhiá»‡m vá»¥ NLP thÃ¡ch thá»©c nháº¥t, Äá»™ TÆ°Æ¡ng Ä‘á»“ng VÄƒn báº£n Ngá»¯ nghÄ©a.

2 DATASETS
Trong pháº§n nÃ y, chÃºng tÃ´i tháº£o luáº­n vá» má»™t sá»‘ dataset phá»• biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c thuáº­t toÃ¡n tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. CÃ¡c dataset cÃ³ thá»ƒ bao gá»“m cÃ¡c cáº·p tá»« hoáº·c cÃ¡c cáº·p cÃ¢u vá»›i cÃ¡c giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng tiÃªu chuáº©n liÃªn quan. Hiá»‡u suáº¥t cá»§a cÃ¡c thuáº­t toÃ¡n tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a khÃ¡c nhau Ä‘Æ°á»£c Ä‘o báº±ng sá»± tÆ°Æ¡ng quan cá»§a cÃ¡c káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c vá»›i cÃ¡c Ä‘á»™ Ä‘o tiÃªu chuáº©n cÃ³ sáºµn trong cÃ¡c dataset nÃ y.

Báº£ng 1 liá»‡t kÃª má»™t sá»‘ dataset phá»• biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c thuáº­t toÃ¡n tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. Pháº§n tiáº¿p theo mÃ´ táº£ cÃ¡c thuá»™c tÃ­nh cá»§a dataset vÃ  phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng chÃºng.

TÃªn Dataset | Cáº·p Tá»«/CÃ¢u | Khoáº£ng Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng | NÄƒm | Tham kháº£o
R&G | 65 | 0-4 | 1965 | [107]
M&C | 30 | 0-4 | 1991 | [78]
WS353 | 353 | 0-10 | 2002 | [30]
LiSent | 65 | 0-4 | 2007 | [63]  
SRS | 30 | 0-4 | 2007 | [94]
WS353-Sim | 203 | 0-10 | 2009 | [1]
STS2012 | 5250 | 0-5 | 2012 | [5]
STS2013 | 2250 | 0-5 | 2013 | [6]
WP300 | 300 | 0-1 | 2013 | [61]
STS2014 | 3750 | 0-5 | 2014 | [3]
SL7576 | 7576 | 1-5 | 2014 | [116]
SimLex-999 | 999 | 0-10 | 2014 | [40]
SICK | 10000 | 1-5 | 2014 | [69]
STS2015 | 3000 | 0-5 | 2015 | [2]
SimVerb | 3500 | 0-10 | 2016 | [34]
STS2016 | 1186 | 0-5 | 2016 | [4]
WiC | 5428 | NA | 2019 | [97]

Báº£ng 1. CÃ¡c dataset chuáº©n phá»• biáº¿n cho TÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a

--- TRANG 4 ---
4 D Chandrasekaran vÃ  V Mago

2.1 Dataset tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a
Sau Ä‘Ã¢y lÃ  danh sÃ¡ch cÃ¡c dataset tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»© tá»± thá»i gian.

â€¢ Rubenstein vÃ  Goodenough (R&G) [107]: Dataset nÃ y Ä‘Æ°á»£c táº¡o ra nhÆ° káº¿t quáº£ cá»§a má»™t thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i 51 sinh viÃªn Ä‘áº¡i há»c (ngÆ°á»i báº£n ngá»¯ tiáº¿ng Anh) trong hai phiÃªn khÃ¡c nhau. CÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c cung cáº¥p 65 cáº·p danh tá»« tiáº¿ng Anh Ä‘Æ°á»£c chá»n lá»c vÃ  Ä‘Æ°á»£c yÃªu cáº§u gÃ¡n Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cho má»—i cáº·p trÃªn thang Ä‘iá»ƒm tá»« 0 Ä‘áº¿n 4, trong Ä‘Ã³ 0 thá»ƒ hiá»‡n ráº±ng cÃ¡c tá»« hoÃ n toÃ n khÃ´ng tÆ°Æ¡ng Ä‘á»“ng vÃ  4 thá»ƒ hiá»‡n ráº±ng chÃºng ráº¥t tÆ°Æ¡ng Ä‘á»“ng. Dataset nÃ y lÃ  dataset Ä‘áº§u tiÃªn vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t trong cÃ¡c nhiá»‡m vá»¥ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a [133].

â€¢ Miller vÃ  Charles (M&C) [78]: Miller vÃ  Charles Ä‘Ã£ láº·p láº¡i thÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Rubenstein vÃ  Goodenough vÃ o nÄƒm 1991 vá»›i má»™t táº­p con gá»“m 30 cáº·p tá»« tá»« 65 cáº·p tá»« gá»‘c. 38 Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i Ä‘Ã£ xáº¿p háº¡ng cÃ¡c cáº·p tá»« trÃªn thang Ä‘iá»ƒm tá»« 0 Ä‘áº¿n 4, 4 cÃ³ nghÄ©a lÃ  "tÆ°Æ¡ng Ä‘á»“ng nháº¥t."

â€¢ WS353 [30]: WS353 chá»©a 353 cáº·p tá»« vá»›i Ä‘iá»ƒm sá»‘ liÃªn quan tá»« 0 Ä‘áº¿n 10. 0 thá»ƒ hiá»‡n má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng tháº¥p nháº¥t vÃ  10 thá»ƒ hiá»‡n má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t. ThÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i má»™t nhÃ³m 16 Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i. Dataset nÃ y Ä‘o lÆ°á»ng má»‘i liÃªn quan ngá»¯ nghÄ©a hÆ¡n lÃ  tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. Sau Ä‘Ã³, dataset tiáº¿p theo Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t.

â€¢ WS353-Sim [1]: Dataset nÃ y lÃ  má»™t táº­p con cá»§a WS353 chá»©a 203 cáº·p tá»« tá»« 353 cáº·p tá»« ban Ä‘áº§u phÃ¹ há»£p hÆ¡n Ä‘áº·c biá»‡t cho cÃ¡c thuáº­t toÃ¡n tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a.

â€¢ LiSent [63]: 65 cáº·p cÃ¢u Ä‘Æ°á»£c xÃ¢y dá»±ng sá»­ dá»¥ng Ä‘á»‹nh nghÄ©a tá»« Ä‘iá»ƒn cá»§a 65 cáº·p tá»« Ä‘Æ°á»£c sá»­ dá»¥ng trong dataset R&G. 32 ngÆ°á»i báº£n ngá»¯ tiáº¿ng Anh tÃ¬nh nguyá»‡n cung cáº¥p khoáº£ng tÆ°Æ¡ng Ä‘á»“ng tá»« 0 Ä‘áº¿n 4, 4 lÃ  cao nháº¥t. GiÃ¡ trá»‹ trung bÃ¬nh cá»§a cÃ¡c Ä‘iá»ƒm sá»‘ Ä‘Æ°á»£c Ä‘Æ°a ra bá»Ÿi táº¥t cáº£ cÃ¡c tÃ¬nh nguyá»‡n viÃªn Ä‘Æ°á»£c láº¥y lÃ m Ä‘iá»ƒm sá»‘ cuá»‘i cÃ¹ng.

â€¢ SRS [94]: Pedersen et al. [94] Ä‘Ã£ cá»‘ gáº¯ng xÃ¢y dá»±ng má»™t dataset tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a chuyÃªn biá»‡t cho lÄ©nh vá»±c y sinh. Ban Ä‘áº§u 120 cáº·p Ä‘Æ°á»£c má»™t bÃ¡c sÄ© lá»±a chá»n phÃ¢n bá»‘ vá»›i 30 cáº·p trÃªn 4 giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng. CÃ¡c cáº·p thuáº­t ngá»¯ nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c xáº¿p háº¡ng bá»Ÿi 13 ngÆ°á»i mÃ£ hÃ³a y táº¿ trÃªn thang Ä‘iá»ƒm 1-10. 30 cáº·p tá»« tá»« 120 cáº·p Ä‘Æ°á»£c chá»n Ä‘á»ƒ tÄƒng Ä‘á»™ tin cáº­y vÃ  cÃ¡c cáº·p tá»« nÃ y Ä‘Æ°á»£c chÃº thÃ­ch bá»Ÿi 3 bÃ¡c sÄ© vÃ  9 (trong sá»‘ 13) ngÆ°á»i mÃ£ hÃ³a y táº¿ Ä‘á»ƒ táº¡o thÃ nh dataset cuá»‘i cÃ¹ng.

â€¢ SimLex-999 [40]: 999 cáº·p tá»« Ä‘Æ°á»£c chá»n tá»« Dataset UFS [89] trong Ä‘Ã³ 900 cáº·p tÆ°Æ¡ng Ä‘á»“ng vÃ  99 cáº·p liÃªn quan nhÆ°ng khÃ´ng tÆ°Æ¡ng Ä‘á»“ng. 500 ngÆ°á»i báº£n ngá»¯ tiáº¿ng Anh, Ä‘Æ°á»£c tuyá»ƒn dá»¥ng qua Amazon Mechanical Turk Ä‘Æ°á»£c yÃªu cáº§u xáº¿p háº¡ng sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c cáº·p tá»« trÃªn thang Ä‘iá»ƒm tá»« 0 Ä‘áº¿n 6, 6 lÃ  tÆ°Æ¡ng Ä‘á»“ng nháº¥t. Dataset chá»©a 666 cáº·p danh tá»«, 222 cáº·p Ä‘á»™ng tá»«, vÃ  111 cáº·p tÃ­nh tá»«.

â€¢ Dataset CÃ¢u LiÃªn quan Kiáº¿n thá»©c Tá»•ng há»£p (SICK) [69]: Dataset SICK bao gá»“m 10.000 cáº·p cÃ¢u, Ä‘Æ°á»£c dáº«n xuáº¥t tá»« hai dataset hiá»‡n cÃ³ lÃ  ImageFlickr 8 vÃ  dataset mÃ´ táº£ MSR-Video. Má»—i cáº·p cÃ¢u Ä‘Æ°á»£c liÃªn káº¿t vá»›i má»™t Ä‘iá»ƒm liÃªn quan vÃ  má»™t quan há»‡ entailment vÄƒn báº£n. Äiá»ƒm liÃªn quan dao Ä‘á»™ng tá»« 1 Ä‘áº¿n 5, vÃ  ba quan há»‡ entailment lÃ  "NEUTRAL, ENTAILMENT vÃ  CONTRADICTION." Viá»‡c chÃº thÃ­ch Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng ká»¹ thuáº­t crowd-sourcing.

â€¢ Dataset STS [2â€“6,24]: CÃ¡c dataset STS Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡ch káº¿t há»£p cÃ¡c cáº·p cÃ¢u tá»« cÃ¡c nguá»“n khÃ¡c nhau bá»Ÿi cÃ¡c tá»• chá»©c cá»§a nhiá»‡m vá»¥ chia sáº» SemEVAL. Dataset Ä‘Æ°á»£c chÃº thÃ­ch báº±ng Amazon Mechanical Turk vÃ  Ä‘Æ°á»£c xÃ¡c minh thÃªm bá»Ÿi chÃ­nh cÃ¡c tá»• chá»©c. Báº£ng 2 hiá»ƒn thá»‹ cÃ¡c nguá»“n khÃ¡c nhau tá»« Ä‘Ã³ dataset STS Ä‘Æ°á»£c xÃ¢y dá»±ng.

NÄƒm | Dataset | Cáº·p | Nguá»“n
2012 | MSRPar | 1500 | newswire
2012 | MSRvid | 1500 | videos
2012 | OnWN | 750 | glosses
2012 | SMTNews | 750 | WMT eval.
2012 | SMTeuroparl | 750 | WMT eval.
2013 | HDL | 750 | newswire
2013 | FNWN | 189 | glosses
2013 | OnWN | 561 | glosses
2013 | SMT | 750 | MT eval.

--- TRANG 5 ---
Sá»± PhÃ¡t triá»ƒn cá»§a Äá»™ TÆ°Æ¡ng Ä‘á»“ng Ngá»¯ nghÄ©a - Má»™t Kháº£o sÃ¡t 5

Báº£ng 2 tiáº¿p tá»¥c tá»« trang trÆ°á»›c
NÄƒm | Dataset | Cáº·p | Nguá»“n
2014 | HDL | 750 | newswire headlines
2014 | OnWN | 750 | glosses
2014 | Deft-forum | 450 | forum posts
2014 | Deft-news | 300 | news summary
2014 | Images | 750 | image descriptions
2014 | Tweet-news | 750 | tweet-news pairs
2015 | HDL | 750 | newswire headlines
2015 | Images | 750 | image descriptions
2015 | Ans.-student | 750 | student answers
2015 | Ans.-forum | 375 | Q & A forum answers
2015 | Belief | 375 | committed belief
2016 | HDL | 249 | newswire headlines
2016 | Plagiarism | 230 | short-answers plag.
2016 | post-editing | 244 | MT postedits
2016 | Ans.-Ans | 254 | Q & A forum answers
2016 | Quest.-Quest. | 209 | Q & A forum questions
2017 | Trail | 23 | Mixed STS 2016

Báº£ng 2. Dataset huáº¥n luyá»‡n tiáº¿ng Anh STS (2012-2017) [24]

3 PHÆ¯Æ NG PHÃP TÆ¯Æ NG Äá»’NG NGá»® NGHÄ¨A Dá»°A TRÃŠN KIáº¾N THá»¨C

CÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn kiáº¿n thá»©c tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a hai thuáº­t ngá»¯ dá»±a trÃªn thÃ´ng tin cÃ³ Ä‘Æ°á»£c tá»« má»™t hoáº·c nhiá»u nguá»“n kiáº¿n thá»©c cÆ¡ báº£n nhÆ° ontology/cÆ¡ sá»Ÿ dá»¯ liá»‡u tá»« vá»±ng, tá»« Ä‘iá»ƒn Ä‘á»“ng nghÄ©a, tá»« Ä‘iá»ƒn, v.v. CÆ¡ sá»Ÿ kiáº¿n thá»©c cÆ¡ báº£n cung cáº¥p cho cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y má»™t biá»ƒu diá»…n cÃ³ cáº¥u trÃºc cá»§a cÃ¡c thuáº­t ngá»¯ hoáº·c khÃ¡i niá»‡m Ä‘Æ°á»£c káº¿t ná»‘i bá»Ÿi cÃ¡c quan há»‡ ngá»¯ nghÄ©a, hÆ¡n ná»¯a cung cáº¥p má»™t Ä‘á»™ Ä‘o ngá»¯ nghÄ©a khÃ´ng cÃ³ sá»± mÆ¡ há»“, vÃ¬ Ã½ nghÄ©a thá»±c táº¿ cá»§a cÃ¡c thuáº­t ngá»¯ Ä‘Æ°á»£c xem xÃ©t [123]. Trong pháº§n nÃ y, chÃºng tÃ´i tháº£o luáº­n vá» bá»‘n cÆ¡ sá»Ÿ dá»¯ liá»‡u tá»« vá»±ng Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn kiáº¿n thá»©c vÃ  tháº£o luáº­n ngáº¯n gá»n cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘Æ°á»£c má»™t sá»‘ phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn kiáº¿n thá»©c Ã¡p dá»¥ng.

3.1 CÆ¡ sá»Ÿ dá»¯ liá»‡u tá»« vá»±ng

â€¢ WordNet [77] lÃ  má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u tá»« vá»±ng Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho cÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn kiáº¿n thá»©c bao gá»“m hÆ¡n 100.000 khÃ¡i niá»‡m tiáº¿ng Anh [123]. WordNet cÃ³ thá»ƒ Ä‘Æ°á»£c hÃ¬nh dung nhÆ° má»™t Ä‘á»“ thá»‹, trong Ä‘Ã³ cÃ¡c nÃºt biá»ƒu diá»…n Ã½ nghÄ©a cá»§a cÃ¡c tá»« (khÃ¡i niá»‡m), vÃ  cÃ¡c cáº¡nh Ä‘á»‹nh nghÄ©a má»‘i quan há»‡ giá»¯a cÃ¡c tá»« [133]. Cáº¥u trÃºc cá»§a WordNet chá»§ yáº¿u dá»±a trÃªn tá»« Ä‘á»“ng nghÄ©a, trong Ä‘Ã³ má»—i tá»« cÃ³ cÃ¡c synset khÃ¡c nhau Ä‘Æ°á»£c gÃ¡n cho cÃ¡c Ã½ nghÄ©a khÃ¡c nhau cá»§a chÃºng. Sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a hai tá»« phá»¥ thuá»™c vÃ o khoáº£ng cÃ¡ch Ä‘Æ°á»ng Ä‘i giá»¯a chÃºng [93].

â€¢ WiktionaryÂ¹ lÃ  má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u tá»« vá»±ng mÃ£ nguá»“n má»Ÿ bao gá»“m khoáº£ng 6,2 triá»‡u tá»« tá»« 4.000 ngÃ´n ngá»¯ khÃ¡c nhau. Má»—i má»¥c cÃ³ má»™t trang bÃ i viáº¿t liÃªn quan vÃ  nÃ³ tÃ­nh Ä‘áº¿n má»™t nghÄ©a khÃ¡c nhau cá»§a má»—i má»¥c. Wiktionary khÃ´ng cÃ³ má»™t má»‘i quan há»‡ tá»« vá»±ng phÃ¢n loáº¡i Ä‘Æ°á»£c thiáº¿t láº­p tá»‘t trong cÃ¡c má»¥c, khÃ´ng giá»‘ng nhÆ° WordNet, Ä‘iá»u nÃ y khiáº¿n nÃ³ khÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c thuáº­t toÃ¡n tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a [99].

Â¹https://en.wiktionary.org

--- TRANG 6 ---
6 D Chandrasekaran vÃ  V Mago

â€¢ Vá»›i sá»± ra Ä‘á»i cá»§a WikipediaÂ², háº§u háº¿t cÃ¡c ká»¹ thuáº­t tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a khai thÃ¡c dá»¯ liá»‡u vÄƒn báº£n phong phÃº cÃ³ sáºµn miá»…n phÃ­ Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh [74]. Wikipedia cÃ³ dá»¯ liá»‡u vÄƒn báº£n Ä‘Æ°á»£c tá»• chá»©c thÃ nh cÃ¡c BÃ i viáº¿t. Má»—i bÃ i viáº¿t cÃ³ má»™t tiÃªu Ä‘á» (khÃ¡i niá»‡m), hÃ ng xÃ³m, mÃ´ táº£, vÃ  danh má»¥c. NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° cáº£ dá»¯ liá»‡u phÃ¢n loáº¡i cÃ³ cáº¥u trÃºc vÃ /hoáº·c nhÆ° má»™t corpus Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus [100]. Cáº¥u trÃºc danh má»¥c phá»©c táº¡p cá»§a Wikipedia Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t Ä‘á»“ thá»‹ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ná»™i dung ThÃ´ng tin cá»§a cÃ¡c khÃ¡i niá»‡m, Ä‘iá»u nÃ y láº§n lÆ°á»£t há»— trá»£ tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a [44].

â€¢ BabelNet [88] lÃ  má»™t tÃ i nguyÃªn tá»« vá»±ng káº¿t há»£p WordNet vá»›i dá»¯ liá»‡u cÃ³ sáºµn trÃªn Wikipedia cho má»—i synset. NÃ³ lÃ  ontology ngá»¯ nghÄ©a Ä‘a ngÃ´n ngá»¯ lá»›n nháº¥t cÃ³ sáºµn vá»›i gáº§n hÆ¡n 13 triá»‡u synset vÃ  380 triá»‡u quan há»‡ ngá»¯ nghÄ©a trong 271 ngÃ´n ngá»¯. NÃ³ bao gá»“m hÆ¡n bá»‘n triá»‡u synset vá»›i Ã­t nháº¥t má»™t trang Wikipedia liÃªn quan cho tiáº¿ng Anh [22].

3.2 CÃ¡c loáº¡i phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn kiáº¿n thá»©c

Dá»±a trÃªn nguyÃªn lÃ½ cÆ¡ báº£n vá» cÃ¡ch Ä‘Ã¡nh giÃ¡ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a cÃ¡c tá»«, cÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn kiáº¿n thá»©c cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i thÃªm thÃ nh cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘áº¿m cáº¡nh, phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng, vÃ  phÆ°Æ¡ng phÃ¡p dá»±a trÃªn ná»™i dung thÃ´ng tin.

3.2.1 PhÆ°Æ¡ng phÃ¡p Ä‘áº¿m cáº¡nh: PhÆ°Æ¡ng phÃ¡p Ä‘áº¿m cáº¡nh Ä‘Æ¡n giáº£n nháº¥t lÃ  xem xÃ©t ontology cÆ¡ báº£n nhÆ° má»™t Ä‘á»“ thá»‹ káº¿t ná»‘i cÃ¡c tá»« theo phÃ¢n loáº¡i vÃ  Ä‘áº¿m cÃ¡c cáº¡nh giá»¯a hai thuáº­t ngá»¯ Ä‘á»ƒ Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a chÃºng. CÃ ng xa khoáº£ng cÃ¡ch giá»¯a cÃ¡c thuáº­t ngá»¯ thÃ¬ chÃºng cÃ ng Ã­t tÆ°Æ¡ng Ä‘á»“ng. Äá»™ Ä‘o nÃ y Ä‘Æ°á»£c gá»i lÃ  path Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Rada et al. [102] trong Ä‘Ã³ sá»± tÆ°Æ¡ng Ä‘á»“ng tá»· lá»‡ nghá»‹ch vá»›i Ä‘á»™ dÃ i Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t giá»¯a hai thuáº­t ngá»¯. Trong phÆ°Æ¡ng phÃ¡p Ä‘áº¿m cáº¡nh nÃ y, thá»±c táº¿ lÃ  cÃ¡c tá»« á»Ÿ sÃ¢u hÆ¡n trong há»‡ thá»‘ng phÃ¢n cáº¥p cÃ³ Ã½ nghÄ©a cá»¥ thá»ƒ hÆ¡n, vÃ  ráº±ng, chÃºng cÃ³ thá»ƒ tÆ°Æ¡ng Ä‘á»“ng vá»›i nhau hÆ¡n máº·c dÃ¹ chÃºng cÃ³ cÃ¹ng khoáº£ng cÃ¡ch nhÆ° hai tá»« Ä‘áº¡i diá»‡n cho má»™t khÃ¡i niá»‡m chung hÆ¡n khÃ´ng Ä‘Æ°á»£c xem xÃ©t. Wu vÃ  Palmer [131] Ä‘á» xuáº¥t Ä‘á»™ Ä‘o wup, trong Ä‘Ã³ Ä‘á»™ sÃ¢u cá»§a cÃ¡c tá»« trong ontology Ä‘Æ°á»£c coi lÃ  má»™t thuá»™c tÃ­nh quan trá»ng. Äá»™ Ä‘o wup Ä‘áº¿m sá»‘ cáº¡nh giá»¯a má»—i thuáº­t ngá»¯ vÃ  Least Common Subsumer (LCS) cá»§a chÃºng. LCS lÃ  tá»• tiÃªn chung Ä‘Æ°á»£c chia sáº» bá»Ÿi cáº£ hai thuáº­t ngá»¯ trong ontology Ä‘Ã£ cho. Xem xÃ©t, hai thuáº­t ngá»¯ Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  tâ‚,tâ‚‚, LCS cá»§a chÃºng Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  t_lcs, vÃ  Ä‘á»™ dÃ i Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t giá»¯a chÃºng Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  min_len(tâ‚,tâ‚‚),

path Ä‘Æ°á»£c Ä‘o nhÆ° sau:
sim_path(tâ‚,tâ‚‚) = 1/(1+min_len(tâ‚,tâ‚‚))    (1)

vÃ  wup Ä‘Æ°á»£c Ä‘o nhÆ° sau:
sim_wup(tâ‚,tâ‚‚) = 2depth(t_lcs)/(depth(tâ‚)+depth(tâ‚‚))    (2)

Li et al. [62] Ä‘á» xuáº¥t má»™t Ä‘á»™ Ä‘o tÃ­nh Ä‘áº¿n cáº£ khoáº£ng cÃ¡ch Ä‘Æ°á»ng Ä‘i tá»‘i thiá»ƒu vÃ  Ä‘á»™ sÃ¢u. li Ä‘Æ°á»£c Ä‘o nhÆ° sau:
sim_li = e^(-Î±min_len(tâ‚,tâ‚‚)) Â· (e^(Î²depth(t_lcs)) - e^(-Î²depth(t_lcs)))/(e^(Î²depth(t_lcs)) + e^(-Î²depth(t_lcs)))    (3)

Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘áº¿m cáº¡nh bá» qua thá»±c táº¿ ráº±ng cÃ¡c cáº¡nh trong ontology khÃ´ng nháº¥t thiáº¿t pháº£i cÃ³ Ä‘á»™ dÃ i báº±ng nhau. Äá»ƒ kháº¯c phá»¥c khuyáº¿t Ä‘iá»ƒm nÃ y cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘áº¿m cáº¡nh Ä‘Æ¡n giáº£n, cÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t.

Â²http://www.wikipedia.org

--- TRANG 7 ---
Sá»± PhÃ¡t triá»ƒn cá»§a Äá»™ TÆ°Æ¡ng Ä‘á»“ng Ngá»¯ nghÄ©a - Má»™t Kháº£o sÃ¡t 7

3.2.2 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng: CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng tÃ­nh toÃ¡n sá»± tÆ°Æ¡ng Ä‘á»“ng nhÆ° má»™t hÃ m cá»§a cÃ¡c thuá»™c tÃ­nh cá»§a cÃ¡c tá»«, nhÆ° gloss, cÃ¡c khÃ¡i niá»‡m lÃ¢n cáº­n, v.v. [123]. Gloss Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  Ã½ nghÄ©a cá»§a má»™t tá»« trong tá»« Ä‘iá»ƒn; má»™t táº­p há»£p cÃ¡c gloss Ä‘Æ°á»£c gá»i lÃ  tá»« Ä‘iá»ƒn thuáº­t ngá»¯. CÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a khÃ¡c nhau Ä‘Æ°á»£c Ä‘á» xuáº¥t dá»±a trÃªn gloss cá»§a cÃ¡c tá»«. CÃ¡c Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn gloss khai thÃ¡c kiáº¿n thá»©c ráº±ng cÃ¡c tá»« cÃ³ Ã½ nghÄ©a tÆ°Æ¡ng tá»± cÃ³ nhiá»u tá»« chung hÆ¡n trong gloss cá»§a chÃºng. Sá»± tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a Ä‘Æ°á»£c Ä‘o báº±ng má»©c Ä‘á»™ chá»“ng láº¥p giá»¯a gloss cá»§a cÃ¡c tá»« Ä‘ang xem xÃ©t. Äá»™ Ä‘o Lesk [11], gÃ¡n má»™t giÃ¡ trá»‹ liÃªn quan giá»¯a hai tá»« dá»±a trÃªn sá»± chá»“ng láº¥p cá»§a cÃ¡c tá»« trong gloss cá»§a chÃºng vÃ  cÃ¡c gloss cá»§a cÃ¡c khÃ¡i niá»‡m mÃ  chÃºng liÃªn quan Ä‘áº¿n trong má»™t ontology nhÆ° WordNet [55]. Jiang et al. [45] Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng trong Ä‘Ã³ sá»± tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a Ä‘Æ°á»£c Ä‘o báº±ng cÃ¡c gloss cá»§a cÃ¡c khÃ¡i niá»‡m cÃ³ máº·t trong Wikipedia. Háº§u háº¿t cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng tÃ­nh Ä‘áº¿n cÃ¡c Ä‘áº·c trÆ°ng chung vÃ  khÃ´ng chung giá»¯a hai tá»«/thuáº­t ngá»¯. CÃ¡c Ä‘áº·c trÆ°ng chung gÃ³p pháº§n tÄƒng giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng vÃ  cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng chung lÃ m giáº£m giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng. Háº¡n cháº¿ chÃ­nh cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘áº·c trÆ°ng lÃ  sá»± phá»¥ thuá»™c vÃ o cÃ¡c ontology cÃ³ cÃ¡c Ä‘áº·c trÆ°ng ngá»¯ nghÄ©a, vÃ  háº§u háº¿t cÃ¡c ontology Ã­t khi tÃ­ch há»£p báº¥t ká»³ Ä‘áº·c trÆ°ng ngá»¯ nghÄ©a nÃ o khÃ¡c ngoÃ i cÃ¡c má»‘i quan há»‡ phÃ¢n loáº¡i [123].

3.2.3 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn ná»™i dung thÃ´ng tin: Ná»™i dung thÃ´ng tin (IC) cá»§a má»™t khÃ¡i niá»‡m Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  thÃ´ng tin cÃ³ Ä‘Æ°á»£c tá»« khÃ¡i niá»‡m khi nÃ³ xuáº¥t hiá»‡n trong ngá»¯ cáº£nh [122]. Má»™t giÃ¡ trá»‹ IC cao cho tháº¥y ráº±ng tá»« Ä‘Ã³ cá»¥ thá»ƒ hÆ¡n vÃ  mÃ´ táº£ rÃµ rÃ ng má»™t khÃ¡i niá»‡m vá»›i Ã­t sá»± mÆ¡ há»“ hÆ¡n, trong khi cÃ¡c giÃ¡ trá»‹ IC tháº¥p hÆ¡n cho tháº¥y ráº±ng cÃ¡c tá»« cÃ³ Ã½ nghÄ©a trá»«u tÆ°á»£ng hÆ¡n [133]. TÃ­nh cá»¥ thá»ƒ cá»§a tá»« Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng Táº§n sá»‘ TÃ i liá»‡u Nghá»‹ch Ä‘áº£o (IDF), dá»±a trÃªn nguyÃªn táº¯c ráº±ng tá»« cÃ ng cá»¥ thá»ƒ thÃ¬ cÃ ng Ã­t xuáº¥t hiá»‡n trong tÃ i liá»‡u. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn ná»™i dung thÃ´ng tin Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c thuáº­t ngá»¯ báº±ng giÃ¡ trá»‹ IC liÃªn quan vá»›i chÃºng. Resnik vÃ  Philip [104] Ä‘á» xuáº¥t má»™t Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a gá»i lÃ  res Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng dá»±a trÃªn Ã½ tÆ°á»Ÿng ráº±ng náº¿u hai khÃ¡i niá»‡m chia sáº» má»™t subsumer chung, chÃºng chia sáº» nhiá»u thÃ´ng tin hÆ¡n vÃ¬ giÃ¡ trá»‹ IC cá»§a LCS cao hÆ¡n. Xem xÃ©t IC Ä‘áº¡i diá»‡n cho Ná»™i dung ThÃ´ng tin cá»§a thuáº­t ngá»¯ Ä‘Ã£ cho,

res Ä‘Æ°á»£c Ä‘o nhÆ° sau:
sim_res(tâ‚,tâ‚‚) = IC_t_lcs    (4)

D. Lin [64] Ä‘á» xuáº¥t má»™t má»Ÿ rá»™ng cá»§a Ä‘á»™ Ä‘o res xem xÃ©t giÃ¡ trá»‹ IC cá»§a cáº£ hai thuáº­t ngá»¯ gÃ¡n cho thÃ´ng tin cÃ¡ nhÃ¢n hoáº·c mÃ´ táº£ cá»§a cÃ¡c thuáº­t ngá»¯ vÃ  giÃ¡ trá»‹ IC cá»§a LCS cá»§a chÃºng cung cáº¥p tÃ­nh chung Ä‘Æ°á»£c chia sáº» giá»¯a cÃ¡c thuáº­t ngá»¯. lin Ä‘Æ°á»£c Ä‘o nhÆ° sau:

sim_lin(tâ‚,tâ‚‚) = 2IC_t_lcs/(IC_tâ‚ + IC_tâ‚‚)    (5)

Jiang vÃ  Conrath [43] tÃ­nh toÃ¡n má»™t Ä‘á»™ Ä‘o khoáº£ng cÃ¡ch dá»±a trÃªn sá»± khÃ¡c biá»‡t giá»¯a tá»•ng cÃ¡c giÃ¡ trá»‹ IC cÃ¡ nhÃ¢n cá»§a cÃ¡c thuáº­t ngá»¯ vÃ  giÃ¡ trá»‹ IC cá»§a LCS cá»§a chÃºng báº±ng phÆ°Æ¡ng trÃ¬nh dÆ°á»›i Ä‘Ã¢y:

dis_jcn(tâ‚,tâ‚‚) = IC_tâ‚ + IC_tâ‚‚ - 2IC_t_lcs    (6)

Äá»™ Ä‘o khoáº£ng cÃ¡ch thay tháº¿ Ä‘á»™ dÃ i Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t trong phÆ°Æ¡ng trÃ¬nh (1), vÃ  sá»± tÆ°Æ¡ng Ä‘á»“ng tá»· lá»‡ nghá»‹ch vá»›i khoáº£ng cÃ¡ch trÃªn. Do Ä‘Ã³ jcn Ä‘Æ°á»£c Ä‘o nhÆ° sau:

sim_jcn(tâ‚,tâ‚‚) = 1/(1 + dis_jcn(tâ‚,tâ‚‚))    (7)

IC cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘o báº±ng má»™t corpus cÆ¡ báº£n hoáº·c tá»« cáº¥u trÃºc ná»™i táº¡i cá»§a chÃ­nh ontology [108] dá»±a trÃªn giáº£ Ä‘á»‹nh ráº±ng cÃ¡c ontology Ä‘Æ°á»£c cáº¥u trÃºc theo cÃ¡ch cÃ³ Ã½ nghÄ©a. Má»™t sá»‘ thuáº­t ngá»¯ cÃ³ thá»ƒ khÃ´ng Ä‘Æ°á»£c bao gá»“m trong má»™t ontology, Ä‘iá»u nÃ y cung cáº¥p pháº¡m vi Ä‘á»ƒ sá»­ dá»¥ng nhiá»u ontology Ä‘á»ƒ tÃ­nh toÃ¡n má»‘i quan há»‡ cá»§a chÃºng [105]. Dá»±a trÃªn viá»‡c liá»‡u cÃ¡c thuáº­t ngá»¯ Ä‘Ã£ cho cÃ³ cáº£ hai Ä‘á»u cÃ³ máº·t trong má»™t ontology duy nháº¥t hay khÃ´ng, cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn IC cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh cÃ¡c phÆ°Æ¡ng phÃ¡p mono-ontological hoáº·c multi-ontological. Khi nhiá»u ontology tham gia, IC cá»§a Least Common Subsumer

--- TRANG 8 ---
8 D Chandrasekaran vÃ  V Mago

tá»« cáº£ hai ontology Ä‘Æ°á»£c truy cáº­p Ä‘á»ƒ Æ°á»›c lÆ°á»£ng cÃ¡c giÃ¡ trá»‹ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. Jiang et al. [44] Ä‘á» xuáº¥t cÃ¡c Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn IC dá»±a trÃªn cÃ¡c trang, khÃ¡i niá»‡m vÃ  hÃ ng xÃ³m cá»§a Wikipedia. Wikipedia Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng cáº£ nhÆ° má»™t phÃ¢n loáº¡i cÃ³ cáº¥u trÃºc cÅ©ng nhÆ° má»™t corpus Ä‘á»ƒ cung cáº¥p cÃ¡c giÃ¡ trá»‹ IC.

3.2.4 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c káº¿t há»£p: Nhiá»u Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c khÃ¡c nhau. Goa et al. [33] Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn ontology WordNet trong Ä‘Ã³ ba chiáº¿n lÆ°á»£c khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thÃªm trá»ng sá»‘ vÃ o cÃ¡c cáº¡nh vÃ  Ä‘Æ°á»ng Ä‘i cÃ³ trá»ng sá»‘ ngáº¯n nháº¥t Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. Theo chiáº¿n lÆ°á»£c Ä‘áº§u tiÃªn, Ä‘á»™ sÃ¢u cá»§a táº¥t cáº£ cÃ¡c thuáº­t ngá»¯ trong WordNet dá»c theo Ä‘Æ°á»ng Ä‘i giá»¯a hai thuáº­t ngá»¯ Ä‘ang xem xÃ©t Ä‘Æ°á»£c thÃªm nhÆ° má»™t trá»ng sá»‘ vÃ o Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t. Trong chiáº¿n lÆ°á»£c thá»© hai, chá»‰ Ä‘á»™ sÃ¢u cá»§a LCS cá»§a cÃ¡c thuáº­t ngá»¯ Ä‘Æ°á»£c thÃªm nhÆ° trá»ng sá»‘, vÃ  trong chiáº¿n lÆ°á»£c thá»© ba, giÃ¡ trá»‹ IC cá»§a cÃ¡c thuáº­t ngá»¯ Ä‘Æ°á»£c thÃªm nhÆ° trá»ng sá»‘. Äá»™ dÃ i Ä‘Æ°á»ng Ä‘i cÃ³ trá»ng sá»‘ ngáº¯n nháº¥t hiá»‡n Ä‘Æ°á»£c tÃ­nh toÃ¡n vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c biáº¿n Ä‘á»•i phi tuyáº¿n Ä‘á»ƒ táº¡o ra cÃ¡c Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a. So sÃ¡nh cho tháº¥y chiáº¿n lÆ°á»£c thá»© ba Ä‘áº¡t Ä‘Æ°á»£c sá»± tÆ°Æ¡ng quan tá»‘t hÆ¡n vá»›i cÃ¡c tiÃªu chuáº©n vÃ ng so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng vÃ  hai chiáº¿n lÆ°á»£c khÃ¡c Ä‘Æ°á»£c Ä‘á» xuáº¥t. Zhu vÃ  Iglesias [133] Ä‘á» xuáº¥t má»™t Ä‘á»™ Ä‘o Ä‘Æ°á»ng Ä‘i cÃ³ trá»ng sá»‘ khÃ¡c gá»i lÃ  wpath thÃªm giÃ¡ trá»‹ IC cá»§a Least Common Subsumer nhÆ° má»™t trá»ng sá»‘ vÃ o Ä‘á»™ dÃ i Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t.

wpath Ä‘Æ°á»£c tÃ­nh nhÆ° sau:
sim_wpath(tâ‚,tâ‚‚) = 1/(1 + min_len(tâ‚,tâ‚‚) * k^IC_t_lcs)    (8)

PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c Ä‘á»“ thá»‹ kiáº¿n thá»©c (KG) khÃ¡c nhau nhÆ° WordNet [77], DBPedia [17], YAGO [41], v.v. vÃ  tham sá»‘ k lÃ  má»™t siÃªu tham sá»‘ pháº£i Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho cÃ¡c KG khÃ¡c nhau vÃ  cÃ¡c lÄ©nh vá»±c khÃ¡c nhau vÃ¬ cÃ¡c KG khÃ¡c nhau cÃ³ phÃ¢n phá»‘i khÃ¡c nhau cá»§a cÃ¡c thuáº­t ngá»¯ trong má»—i lÄ©nh vá»±c. Cáº£ giÃ¡ trá»‹ IC dá»±a trÃªn corpus vÃ  IC ná»™i táº¡i Ä‘á»u Ä‘Æ°á»£c thá»­ nghiá»‡m vÃ  Ä‘á»™ Ä‘o wpath dá»±a trÃªn IC corpus Ä‘áº¡t Ä‘Æ°á»£c sá»± tÆ°Æ¡ng quan lá»›n hÆ¡n trong háº§u háº¿t cÃ¡c dataset tiÃªu chuáº©n vÃ ng.

CÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn kiáº¿n thá»©c Ä‘Æ¡n giáº£n vá» máº·t tÃ­nh toÃ¡n, vÃ  cÆ¡ sá»Ÿ kiáº¿n thá»©c cÆ¡ báº£n hoáº¡t Ä‘á»™ng nhÆ° má»™t xÆ°Æ¡ng sá»‘ng máº¡nh máº½ cho cÃ¡c mÃ´ hÃ¬nh, vÃ  váº¥n Ä‘á» phá»• biáº¿n nháº¥t vá» sá»± mÆ¡ há»“ nhÆ° tá»« Ä‘á»“ng nghÄ©a, thÃ nh ngá»¯, vÃ  cá»¥m tá»« Ä‘Æ°á»£c xá»­ lÃ½ hiá»‡u quáº£. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c cÃ³ thá»ƒ dá»… dÃ ng Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng tá»« cÃ¢u Ä‘áº¿n cÃ¢u báº±ng cÃ¡ch Ä‘á»‹nh nghÄ©a cÃ¡c quy táº¯c tá»•ng há»£p [58]. Lastra-DÃ­az et al. [54] Ä‘Ã£ phÃ¡t triá»ƒn má»™t thÆ° viá»‡n pháº§n má»m Half-Edge Semantic Measures Library (HESML) Ä‘á»ƒ triá»ƒn khai cÃ¡c Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn ontology khÃ¡c nhau Ä‘Æ°á»£c Ä‘á» xuáº¥t vÃ  Ä‘Ã£ chá»‰ ra sá»± gia tÄƒng trong thá»i gian hiá»‡u suáº¥t vÃ  kháº£ nÄƒng má»Ÿ rá»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh.

Tuy nhiÃªn, cÃ¡c há»‡ thá»‘ng dá»±a trÃªn kiáº¿n thá»©c phá»¥ thuá»™c cao vÃ o nguá»“n cÆ¡ báº£n dáº«n Ä‘áº¿n nhu cáº§u cáº­p nháº­t chÃºng thÆ°á»ng xuyÃªn Ä‘Ã²i há»i thá»i gian vÃ  tÃ i nguyÃªn tÃ­nh toÃ¡n cao. Máº·c dÃ¹ cÃ¡c ontology máº¡nh nhÆ° WordNet tá»“n táº¡i cho tiáº¿ng Anh, cÃ¡c tÃ i nguyÃªn tÆ°Æ¡ng tá»± khÃ´ng cÃ³ sáºµn cho cÃ¡c ngÃ´n ngá»¯ khÃ¡c dáº«n Ä‘áº¿n nhu cáº§u xÃ¢y dá»±ng cÃ¡c cÆ¡ sá»Ÿ kiáº¿n thá»©c máº¡nh vÃ  cÃ³ cáº¥u trÃºc Ä‘á»ƒ triá»ƒn khai cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c trong cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau vÃ  qua cÃ¡c lÄ©nh vá»±c khÃ¡c nhau. Nhiá»u cÃ´ng trÃ¬nh nghiÃªn cá»©u Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n vá» viá»‡c má»Ÿ rá»™ng cÃ¡c Ä‘á»™ Ä‘o tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a trong lÄ©nh vá»±c y sinh [94,118]. McInnes et al. [71] Ä‘Ã£ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh chuyÃªn biá»‡t gá»i lÃ  UMLS Ä‘á»ƒ Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c tá»« trong lÄ©nh vá»±c y sinh. Vá»›i gáº§n 6.500 ngÃ´n ngá»¯ tháº¿ giá»›i vÃ  nhiá»u lÄ©nh vá»±c, Ä‘iá»u nÃ y trá»Ÿ thÃ nh má»™t nhÆ°á»£c Ä‘iá»ƒm nghiÃªm trá»ng cho cÃ¡c há»‡ thá»‘ng dá»±a trÃªn kiáº¿n thá»©c.

4 PHÆ¯Æ NG PHÃP TÆ¯Æ NG Äá»’NG NGá»® NGHÄ¨A Dá»°A TRÃŠN CORPUS

CÃ¡c phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn corpus Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a giá»¯a cÃ¡c thuáº­t ngá»¯ báº±ng thÃ´ng tin thu Ä‘Æ°á»£c tá»« cÃ¡c corpus lá»›n. NguyÃªn lÃ½ cÆ¡ báº£n Ä‘Æ°á»£c gá»i lÃ  'giáº£ thuyáº¿t phÃ¢n phá»‘i' [36] khai thÃ¡c Ã½ tÆ°á»Ÿng ráº±ng "cÃ¡c tá»« tÆ°Æ¡ng tá»± xuáº¥t hiá»‡n cÃ¹ng nhau, thÆ°á»ng xuyÃªn"; tuy nhiÃªn, Ã½ nghÄ©a thá»±c táº¿ cá»§a cÃ¡c tá»« khÃ´ng Ä‘Æ°á»£c xem xÃ©t. Trong khi nhiá»u ká»¹ thuáº­t khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng biá»ƒu diá»…n vector cá»§a dá»¯ liá»‡u vÄƒn báº£n, má»™t sá»‘ Ä‘á»™ Ä‘o khoáº£ng cÃ¡ch ngá»¯ nghÄ©a dá»±a trÃªn giáº£ thuyáº¿t phÃ¢n phá»‘i

--- TRANG 9 ---
Sá»± PhÃ¡t triá»ƒn cá»§a Äá»™ TÆ°Æ¡ng Ä‘á»“ng Ngá»¯ nghÄ©a - Má»™t Kháº£o sÃ¡t 9

Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ Æ°á»›c lÆ°á»£ng sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c vector. Má»™t kháº£o sÃ¡t toÃ n diá»‡n vá» cÃ¡c Ä‘á»™ Ä‘o ngá»¯ nghÄ©a phÃ¢n phá»‘i khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Mohammad vÃ  Hurst [81], vÃ  cÃ¡c Ä‘á»™ Ä‘o khÃ¡c nhau vÃ  cÃ´ng thá»©c tÆ°Æ¡ng á»©ng cá»§a chÃºng Ä‘Æ°á»£c cung cáº¥p trong Báº£ng 4 trong Phá»¥ lá»¥c A. Tuy nhiÃªn, trong táº¥t cáº£ cÃ¡c Ä‘á»™ Ä‘o nÃ y, Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cÃ³ Ä‘Æ°á»£c Ã½ nghÄ©a vÃ  Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong cÃ¡c nhÃ  nghiÃªn cá»©u NLP Ä‘áº¿n nay [81]. Trong pháº§n nÃ y, chÃºng tÃ´i tháº£o luáº­n chi tiáº¿t vá» má»™t sá»‘ word embedding Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng giáº£ thuyáº¿t phÃ¢n phá»‘i vÃ  má»™t sá»‘ phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a dá»±a trÃªn corpus quan trá»ng.

4.1 Word Embeddings

Word embedding cung cáº¥p biá»ƒu diá»…n vector cá»§a cÃ¡c tá»« trong Ä‘Ã³ cÃ¡c vector nÃ y giá»¯ láº¡i má»‘i quan há»‡ ngÃ´n ngá»¯ cÆ¡ báº£n giá»¯a cÃ¡c tá»« [111]. CÃ¡c vector nÃ y Ä‘Æ°á»£c tÃ­nh toÃ¡n báº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n khÃ¡c nhau nhÆ° máº¡ng neural [75], ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n tá»« [95], hoáº·c biá»ƒu diá»…n theo ngá»¯ cáº£nh mÃ  tá»« xuáº¥t hiá»‡n [59]. Má»™t sá»‘ word embedding Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t bao gá»“m:

â€¢ word2vec [75]: ÄÆ°á»£c phÃ¡t triá»ƒn tá»« dataset Google News, chá»©a khoáº£ng 3 triá»‡u biá»ƒu diá»…n vector cá»§a cÃ¡c tá»« vÃ  cá»¥m tá»«, word2vec lÃ  má»™t mÃ´ hÃ¬nh máº¡ng neural Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra biá»ƒu diá»…n vector phÃ¢n phá»‘i cá»§a cÃ¡c tá»« dá»±a trÃªn má»™t corpus cÆ¡ báº£n. CÃ³ hai mÃ´ hÃ¬nh khÃ¡c nhau cá»§a word2vec Ä‘Æ°á»£c Ä‘á» xuáº¥t: Continuous Bag of Words (CBOW) vÃ  mÃ´ hÃ¬nh Skip-gram. Kiáº¿n trÃºc cá»§a máº¡ng khÃ¡ Ä‘Æ¡n giáº£n vÃ  chá»©a má»™t lá»›p Ä‘áº§u vÃ o, má»™t lá»›p áº©n, vÃ  má»™t lá»›p Ä‘áº§u ra. Máº¡ng Ä‘Æ°á»£c cung cáº¥p má»™t corpus vÄƒn báº£n lá»›n lÃ m Ä‘áº§u vÃ o, vÃ  Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh lÃ  cÃ¡c biá»ƒu diá»…n vector cá»§a cÃ¡c tá»«. MÃ´ hÃ¬nh CBOW dá»± Ä‘oÃ¡n tá»« hiá»‡n táº¡i báº±ng cÃ¡c tá»« ngá»¯ cáº£nh lÃ¢n cáº­n, trong khi mÃ´ hÃ¬nh Skip-gram dá»± Ä‘oÃ¡n cÃ¡c tá»« ngá»¯ cáº£nh lÃ¢n cáº­n Ä‘Æ°á»£c cho má»™t tá»« má»¥c tiÃªu. CÃ¡c mÃ´ hÃ¬nh word2vec hiá»‡u quáº£ trong viá»‡c biá»ƒu diá»…n cÃ¡c tá»« nhÆ° cÃ¡c vector giá»¯ láº¡i sá»± tÆ°Æ¡ng Ä‘á»“ng ngá»¯ cáº£nh giá»¯a cÃ¡c tá»«. CÃ¡c tÃ­nh toÃ¡n vector tá»« Ä‘Ã£ mang láº¡i káº¿t quáº£ tá»‘t trong viá»‡c dá»± Ä‘oÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a [76]. Nhiá»u nhÃ  nghiÃªn cá»©u Ä‘Ã£ má»Ÿ rá»™ng mÃ´ hÃ¬nh word2vec Ä‘á»ƒ Ä‘á» xuáº¥t cÃ¡c vector ngá»¯ cáº£nh [73], vector tá»« Ä‘iá»ƒn [127], vector cÃ¢u [91] vÃ  vector Ä‘oáº¡n vÄƒn [56].

â€¢ GloVe [95]: GloVe Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Äáº¡i há»c Stanford dá»±a trÃªn ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n tá»« toÃ n cá»¥c Ä‘Æ°á»£c hÃ¬nh thÃ nh dá»±a trÃªn corpus cÆ¡ báº£n. NÃ³ Æ°á»›c lÆ°á»£ng sá»± tÆ°Æ¡ng Ä‘á»“ng dá»±a trÃªn nguyÃªn táº¯c ráº±ng cÃ¡c tá»« tÆ°Æ¡ng tá»± nhau xuáº¥t hiá»‡n cÃ¹ng nhau. Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n Ä‘Æ°á»£c Ä‘iá»n Ä‘áº§y vá»›i cÃ¡c giÃ¡ trá»‹ xuáº¥t hiá»‡n báº±ng cÃ¡ch thá»±c hiá»‡n má»™t láº§n duyá»‡t qua cÃ¡c corpus lá»›n cÆ¡ báº£n. MÃ´ hÃ¬nh ğºğ‘™ğ‘œğ‘‰ğ‘’ Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng nÄƒm corpus khÃ¡c nhau, chá»§ yáº¿u lÃ  cÃ¡c báº£n dump Wikipedia. Trong khi táº¡o thÃ nh cÃ¡c vector, cÃ¡c tá»« Ä‘Æ°á»£c chá»n trong má»™t cá»­a sá»• ngá»¯ cáº£nh cá»¥ thá»ƒ do thá»±c táº¿ lÃ  cÃ¡c tá»« á»Ÿ xa cÃ³ Ã­t liÃªn quan Ä‘áº¿n tá»« ngá»¯ cáº£nh Ä‘ang xem xÃ©t. HÃ m loss cá»§a ğºğ‘™ğ‘œğ‘‰ğ‘’ tá»‘i thiá»ƒu hÃ³a khoáº£ng cÃ¡ch bÃ¬nh phÆ°Æ¡ng tá»‘i thiá»ƒu giá»¯a cÃ¡c giÃ¡ trá»‹ Ä‘á»“ng xuáº¥t hiá»‡n cá»­a sá»• ngá»¯ cáº£nh vÃ  cÃ¡c giÃ¡ trá»‹ Ä‘á»“ng xuáº¥t hiá»‡n toÃ n cá»¥c [55]. CÃ¡c vector ğºğ‘™ğ‘œğ‘‰ğ‘’ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ táº¡o thÃ nh cÃ¡c vector tá»« theo ngá»¯ cáº£nh nháº±m phÃ¢n biá»‡t cÃ¡c tá»« dá»±a trÃªn ngá»¯ cáº£nh [70].

â€¢fastText [18]: CÃ¡c nhÃ  nghiÃªn cá»©u AI cá»§a Facebook Ä‘Ã£ phÃ¡t triá»ƒn má»™t mÃ´ hÃ¬nh nhÃºng tá»« xÃ¢y dá»±ng cÃ¡c vector tá»« dá»±a trÃªn cÃ¡c mÃ´ hÃ¬nh Skip-gram trong Ä‘Ã³ má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t táº­p há»£p cÃ¡c n-gram kÃ½ tá»±. ğ‘“ğ‘ğ‘ ğ‘¡ğ‘‡ğ‘’ğ‘¥ğ‘¡ há»c cÃ¡c nhÃºng tá»« dÆ°á»›i dáº¡ng trung bÃ¬nh cá»§a cÃ¡c nhÃºng kÃ½ tá»± cá»§a nÃ³ do Ä‘Ã³ tÃ­nh Ä‘áº¿n cáº¥u trÃºc hÃ¬nh thÃ¡i cá»§a tá»« Ä‘iá»u nÃ y chá»©ng minh lÃ  hiá»‡u quáº£ trong cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau nhÆ° tiáº¿ng Pháº§n Lan vÃ  tiáº¿ng Thá»• NhÄ© Ká»³. Ngay cáº£ cÃ¡c tá»« ngoÃ i vá»‘n tá»« vá»±ng cÅ©ng Ä‘Æ°á»£c gÃ¡n cÃ¡c vector tá»« dá»±a trÃªn cÃ¡c kÃ½ tá»± hoáº·c Ä‘Æ¡n vá»‹ con cá»§a chÃºng.

â€¢Bidirectional Encoder Representations from Transformers(BERT) [29]: Devlin vÃ  cá»™ng sá»± [29] Ä‘Ã£ Ä‘á» xuáº¥t má»™t nhÃºng tá»« dá»±a trÃªn transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh báº±ng cÃ¡ch thÃªm má»™t lá»›p Ä‘áº§u ra cuá»‘i cÃ¹ng Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c nhÃºng cho cÃ¡c tÃ¡c vá»¥ NLP khÃ¡c nhau. BERT sá»­ dá»¥ng kiáº¿n trÃºc transformer Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Vaswani vÃ  cá»™ng sá»± [128], táº¡o ra cÃ¡c vector tá»« dá»±a trÃªn attention báº±ng cÃ¡ch sá»­ dá»¥ng bá»™ mÃ£ hÃ³a transformer hai chiá»u. Khung BERT bao gá»“m hai quy trÃ¬nh quan trá»ng Ä‘Æ°á»£c gá»i lÃ  'huáº¥n luyá»‡n trÆ°á»›c' vÃ  'tinh chá»‰nh'. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c báº±ng cÃ¡ch sá»­ dá»¥ng má»™t corpus gáº§n 3.300M tá»« tá»« cáº£ Book corpus vÃ  English Wikipedia. VÃ¬ mÃ´ hÃ¬nh lÃ  hai chiá»u Ä‘á»ƒ trÃ¡nh kháº£ nÄƒng mÃ´ hÃ¬nh biáº¿t chÃ­nh token Ä‘Ã³ khi huáº¥n luyá»‡n tá»« cáº£ hai hÆ°á»›ng, quy trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c Ä‘Æ°á»£c thá»±c hiá»‡n theo hai cÃ¡ch khÃ¡c nhau. Trong tÃ¡c vá»¥ Ä‘áº§u tiÃªn, cÃ¡c tá»« ngáº«u nhiÃªn trong corpus Ä‘Æ°á»£c che vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n nhá»¯ng tá»« nÃ y. Trong tÃ¡c vá»¥ thá»© hai, mÃ´ hÃ¬nh Ä‘Æ°á»£c trÃ¬nh bÃ y vá»›i cÃ¡c cáº·p cÃ¢u tá»« corpus, trong Ä‘Ã³ 50% cÃ¡c cÃ¢u thá»±c sá»± liÃªn tiáº¿p trong khi pháº§n cÃ²n láº¡i lÃ  cÃ¡c cáº·p ngáº«u nhiÃªn. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n xem cáº·p cÃ¢u Ä‘Ã£ cho cÃ³ liÃªn tiáº¿p hay khÃ´ng. Trong quy trÃ¬nh 'tinh chá»‰nh', mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n cho tÃ¡c vá»¥ NLP downstream cá»¥ thá»ƒ trong tay. MÃ´ hÃ¬nh Ä‘Æ°á»£c cáº¥u trÃºc Ä‘á»ƒ nháº­n Ä‘áº§u vÃ o cáº£ cÃ¢u Ä‘Æ¡n láº«n nhiá»u cÃ¢u Ä‘á»ƒ Ä‘Ã¡p á»©ng nhiá»u tÃ¡c vá»¥ NLP. Äá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»±c hiá»‡n tÃ¡c vá»¥ há»i Ä‘Ã¡p, mÃ´ hÃ¬nh Ä‘Æ°á»£c cung cáº¥p vá»›i cÃ¡c cáº·p cÃ¢u há»i-tráº£ lá»i khÃ¡c nhau vÃ  táº¥t cáº£ cÃ¡c tham sá»‘ Ä‘Æ°á»£c tinh chá»‰nh theo tÃ¡c vá»¥. CÃ¡c nhÃºng BERT cung cáº¥p káº¿t quáº£ tiÃªn tiáº¿n nháº¥t trong táº­p dá»¯ liá»‡u STS-B vá»›i há»‡ sá»‘ tÆ°Æ¡ng quan Spearman lÃ  86.5% vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh BiLSTM khÃ¡c bao gá»“m ELMo [96].

Word embeddings Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a cÃ¡c vÄƒn báº£n cá»§a cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau báº±ng cÃ¡ch Ã¡nh xáº¡ nhÃºng tá»« cá»§a má»™t ngÃ´n ngá»¯ lÃªn khÃ´ng gian vector cá»§a ngÃ´n ngá»¯ khÃ¡c. Khi huáº¥n luyá»‡n vá»›i má»™t sá»‘ lÆ°á»£ng háº¡n cháº¿ nhÆ°ng Ä‘á»§ cÃ¡c cáº·p dá»‹ch, ma tráº­n dá»‹ch cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘á»ƒ cho phÃ©p sá»± chá»“ng chÃ©o cá»§a cÃ¡c nhÃºng qua cÃ¡c ngÃ´n ngá»¯ [35]. Má»™t trong nhá»¯ng thÃ¡ch thá»©c chÃ­nh khi triá»ƒn khai word-embeddings Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± lÃ  Meaning Conflation Deficiency. NÃ³ chá»‰ ra ráº±ng word embeddings khÃ´ng gÃ¡n cho cÃ¡c nghÄ©a khÃ¡c nhau cá»§a má»™t tá»« Ä‘iá»u nÃ y lÃ m Ã´ nhiá»…m khÃ´ng gian ngá»¯ nghÄ©a vá»›i nhiá»…u báº±ng cÃ¡ch Ä‘Æ°a cÃ¡c tá»« khÃ´ng liÃªn quan gáº§n nhau hÆ¡n. VÃ­ dá»¥, cÃ¡c tá»« 'finance' vÃ  'river' cÃ³ thá»ƒ xuáº¥t hiá»‡n trong cÃ¹ng má»™t khÃ´ng gian ngá»¯ nghÄ©a vÃ¬ tá»« 'bank' cÃ³ hai nghÄ©a khÃ¡c nhau [20]. Äiá»u quan trá»ng cáº§n hiá»ƒu lÃ  word-embeddings khai thÃ¡c giáº£ thuyáº¿t phÃ¢n phá»‘i Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c vector vÃ  dá»±a vÃ o cÃ¡c corpus lá»›n, do Ä‘Ã³, chÃºng Ä‘Æ°á»£c phÃ¢n loáº¡i dÆ°á»›i cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a dá»±a trÃªn corpus. Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn máº¡ng neural sÃ¢u vÃ  háº§u háº¿t cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a lai sá»­ dá»¥ng word-embeddings Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u vÄƒn báº£n thÃ nh cÃ¡c vector chiá»u cao, vÃ  hiá»‡u quáº£ cá»§a cÃ¡c nhÃºng nÃ y Ä‘Ã³ng vai trÃ² quan trá»ng trong hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a [60, 79].

4.2 CÃ¡c loáº¡i phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a dá»±a trÃªn corpus

Dá»±a trÃªn cÃ¡c phÆ°Æ¡ng phÃ¡p cÆ¡ báº£n sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng word-vectors cÃ³ nhiá»u loáº¡i phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus khÃ¡c nhau, má»™t sá»‘ trong Ä‘Ã³ Ä‘Æ°á»£c tháº£o luáº­n trong pháº§n nÃ y.

4.2.1 Latent Semantic Analysis (LSA) [51]: LSA lÃ  má»™t trong nhá»¯ng ká»¹ thuáº­t dá»±a trÃªn corpus phá»• biáº¿n vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. Má»™t ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n tá»« Ä‘Æ°á»£c táº¡o thÃ nh trong Ä‘Ã³ cÃ¡c hÃ ng biá»ƒu diá»…n cÃ¡c tá»« vÃ  cÃ¡c cá»™t biá»ƒu diá»…n cÃ¡c Ä‘oáº¡n vÄƒn, vÃ  cÃ¡c Ã´ Ä‘Æ°á»£c Ä‘iá»n vá»›i sá»‘ lÆ°á»£ng tá»«. Ma tráº­n nÃ y Ä‘Æ°á»£c táº¡o thÃ nh vá»›i má»™t corpus lá»›n cÆ¡ báº£n, vÃ  viá»‡c giáº£m chiá»u Ä‘Æ°á»£c Ä‘áº¡t báº±ng má»™t ká»¹ thuáº­t toÃ¡n há»c gá»i lÃ  Singular Value Decomposition (SVD). SVD biá»ƒu diá»…n má»™t ma tráº­n Ä‘Ã£ cho nhÆ° tÃ­ch cá»§a ba ma tráº­n, trong Ä‘Ã³ hai ma tráº­n biá»ƒu diá»…n cÃ¡c hÃ ng vÃ  cá»™t nhÆ° cÃ¡c vector Ä‘Æ°á»£c dáº«n xuáº¥t tá»« cÃ¡c eigenvalue cá»§a chÃºng vÃ  ma tráº­n thá»© ba lÃ  má»™t ma tráº­n Ä‘Æ°á»ng chÃ©o cÃ³ cÃ¡c giÃ¡ trá»‹ sáº½ tÃ¡i táº¡o ma tráº­n gá»‘c khi nhÃ¢n vá»›i hai ma tráº­n khÃ¡c [52]. SVD giáº£m sá»‘ lÆ°á»£ng cá»™t trong khi giá»¯ nguyÃªn sá»‘ lÆ°á»£ng hÃ ng do Ä‘Ã³ báº£o tá»“n cáº¥u trÃºc tÆ°Æ¡ng tá»± giá»¯a cÃ¡c tá»«. Sau Ä‘Ã³ má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t vector báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c giÃ¡ trá»‹ trong cÃ¡c hÃ ng tÆ°Æ¡ng á»©ng cá»§a nÃ³ vÃ  Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a Ä‘Æ°á»£c tÃ­nh toÃ¡n nhÆ° giÃ¡ trá»‹ cosine giá»¯a cÃ¡c vector nÃ y. CÃ¡c mÃ´ hÃ¬nh LSA Ä‘Æ°á»£c tá»•ng quÃ¡t hÃ³a báº±ng cÃ¡ch thay tháº¿ cÃ¡c tá»« báº±ng vÄƒn báº£n vÃ  cÃ¡c cá»™t báº±ng cÃ¡c máº«u khÃ¡c nhau vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¡c cÃ¢u, Ä‘oáº¡n vÄƒn vÃ  tÃ i liá»‡u.

4.2.2 Hyperspace Analogue to Language(HAL) [68]: HAL xÃ¢y dá»±ng má»™t ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n tá»« cÃ³ cáº£ hÃ ng vÃ  cá»™t biá»ƒu diá»…n cÃ¡c tá»« trong vá»‘n tá»« vá»±ng vÃ  cÃ¡c pháº§n tá»­ ma tráº­n Ä‘Æ°á»£c Ä‘iá»n vá»›i cÃ¡c giÃ¡ trá»‹ cÆ°á»ng Ä‘á»™ káº¿t há»£p. CÃ¡c giÃ¡ trá»‹ cÆ°á»ng Ä‘á»™ káº¿t há»£p Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch trÆ°á»£t má»™t "cá»­a sá»•" cÃ³ kÃ­ch thÆ°á»›c cÃ³ thá»ƒ thay Ä‘á»•i qua corpus cÆ¡ báº£n. CÆ°á»ng Ä‘á»™ káº¿t há»£p giá»¯a cÃ¡c tá»« trong cá»­a sá»• giáº£m theo sá»± tÄƒng khoáº£ng cÃ¡ch cá»§a chÃºng tá»« tá»« Ä‘Æ°á»£c táº­p trung xem xÃ©t. VÃ­ dá»¥, trong cÃ¢u "This is a survey of various semantic similarity measures", cÃ¡c tá»« 'survey' vÃ  'variety' cÃ³ giÃ¡ trá»‹ káº¿t há»£p lá»›n hÆ¡n cÃ¡c tá»« 'survey' vÃ  'measures'. CÃ¡c vector tá»« Ä‘Æ°á»£c táº¡o thÃ nh báº±ng cÃ¡ch xem xÃ©t cáº£ hÃ ng vÃ  cá»™t cá»§a tá»« Ä‘Ã£ cho. Viá»‡c giáº£m chiá»u Ä‘Æ°á»£c Ä‘áº¡t báº±ng cÃ¡ch loáº¡i bá» báº¥t ká»³ cá»™t nÃ o cÃ³ giÃ¡ trá»‹ entropy tháº¥p. Äá»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a sau Ä‘Ã³ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch Ä‘o khoáº£ng cÃ¡ch Euclidean hoáº·c Manhattan giá»¯a cÃ¡c vector tá»«.

4.2.3 Explicit Semantic Analysis (ESA) [31]: ESA Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a dá»±a trÃªn cÃ¡c khÃ¡i niá»‡m Wikipedia. Viá»‡c sá»­ dá»¥ng Wikipedia Ä‘áº£m báº£o ráº±ng phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c lÄ©nh vá»±c vÃ  ngÃ´n ngá»¯ khÃ¡c nhau. VÃ¬ Wikipedia Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c, phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i cÃ¡c thay Ä‘á»•i theo thá»i gian. Äáº§u tiÃªn, má»—i khÃ¡i niá»‡m trong Wikipedia Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t vector thuá»™c tÃ­nh cá»§a cÃ¡c tá»« xuáº¥t hiá»‡n trong nÃ³, sau Ä‘Ã³ má»™t chá»‰ má»¥c ngÆ°á»£c Ä‘Æ°á»£c táº¡o thÃ nh, trong Ä‘Ã³ má»—i tá»« Ä‘Æ°á»£c liÃªn káº¿t vá»›i táº¥t cáº£ cÃ¡c khÃ¡i niá»‡m mÃ  nÃ³ Ä‘Æ°á»£c káº¿t há»£p vá»›i. CÆ°á»ng Ä‘á»™ káº¿t há»£p Ä‘Æ°á»£c tÃ­nh trá»ng sá»‘ báº±ng ká»¹ thuáº­t TF-IDF, vÃ  cÃ¡c khÃ¡i niá»‡m káº¿t há»£p yáº¿u vá»›i cÃ¡c tá»« Ä‘Æ°á»£c loáº¡i bá». Do Ä‘Ã³ vÄƒn báº£n Ä‘áº§u vÃ o Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng cÃ¡c vector cÃ³ trá»ng sá»‘ cá»§a cÃ¡c khÃ¡i niá»‡m gá»i lÃ  "interpretation vectors". Äá»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a Ä‘Æ°á»£c Ä‘o báº±ng cÃ¡ch tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c vector tá»« nÃ y.

4.2.4 Word-Alignment models [120]: CÃ¡c mÃ´ hÃ¬nh Word-Alignment tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a cá»§a cÃ¡c cÃ¢u dá»±a trÃªn sá»± cÄƒn chá»‰nh cá»§a chÃºng trÃªn má»™t corpus lá»›n [24,47,119]. Vá»‹ trÃ­ thá»© hai, thá»© ba vÃ  thá»© nÄƒm trong cÃ¡c tÃ¡c vá»¥ SemEval 2015 Ä‘Æ°á»£c Ä‘áº£m báº£o bá»Ÿi cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn word alignment. PhÆ°Æ¡ng phÃ¡p khÃ´ng giÃ¡m sÃ¡t á»Ÿ vá»‹ trÃ­ thá»© nÄƒm Ä‘Ã£ triá»ƒn khai ká»¹ thuáº­t word alignment dá»±a trÃªn Paraphrase Database (PPDB) [32]. Há»‡ thá»‘ng tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a hai cÃ¢u nhÆ° má»™t tá»· lá»‡ cá»§a cÃ¡c tá»« ngá»¯ cáº£nh Ä‘Æ°á»£c cÄƒn chá»‰nh trong cÃ¡c cÃ¢u so vá»›i tá»•ng sá»‘ tá»« trong cáº£ hai cÃ¢u. CÃ¡c phÆ°Æ¡ng phÃ¡p cÃ³ giÃ¡m sÃ¡t á»Ÿ vá»‹ trÃ­ thá»© hai vÃ  thá»© ba Ä‘Ã£ sá»­ dá»¥ng ğ‘¤ğ‘œğ‘Ÿğ‘‘ 2ğ‘£ğ‘’ğ‘ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c sá»± cÄƒn chá»‰nh cá»§a cÃ¡c tá»«. Trong phÆ°Æ¡ng phÃ¡p Ä‘áº§u tiÃªn, má»™t vector cÃ¢u Ä‘Æ°á»£c táº¡o thÃ nh báº±ng cÃ¡ch tÃ­nh "component-wise average" cá»§a cÃ¡c tá»« trong cÃ¢u, vÃ  Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c vector cÃ¢u nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t thÆ°á»›c Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. PhÆ°Æ¡ng phÃ¡p cÃ³ giÃ¡m sÃ¡t thá»© hai chá»‰ xem xÃ©t nhá»¯ng tá»« cÃ³ Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a theo ngá»¯ cáº£nh [120].

4.2.5 Latent Dirichlet Allocation (LDA) [117]: LDA Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biá»ƒu diá»…n má»™t chá»§ Ä‘á» hoáº·c Ã½ tÆ°á»Ÿng chung Ä‘áº±ng sau má»™t tÃ i liá»‡u nhÆ° má»™t vector thay vÃ¬ má»—i tá»« trong tÃ i liá»‡u. Ká»¹ thuáº­t nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho cÃ¡c tÃ¡c vá»¥ mÃ´ hÃ¬nh hÃ³a chá»§ Ä‘á» vÃ  nÃ³ cÃ³ lá»£i tháº¿ cá»§a viá»‡c giáº£m chiá»u xem xÃ©t ráº±ng cÃ¡c chá»§ Ä‘á» Ã­t hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c tá»« thá»±c táº¿ trong má»™t tÃ i liá»‡u [117]. Má»™t trong nhá»¯ng cÃ¡ch tiáº¿p cáº­n má»›i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™ tÆ°Æ¡ng tá»± tÃ i liá»‡u-Ä‘áº¿n-tÃ i liá»‡u lÃ  viá»‡c sá»­ dá»¥ng biá»ƒu diá»…n vector cá»§a cÃ¡c tÃ i liá»‡u vÃ  tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c vector Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a cÃ¡c tÃ i liá»‡u [16].

4.2.6 Normalised Google Distance [25]: NGD Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a hai thuáº­t ngá»¯ dá»±a trÃªn káº¿t quáº£ thu Ä‘Æ°á»£c khi cÃ¡c thuáº­t ngá»¯ Ä‘Æ°á»£c truy váº¥n báº±ng cÃ´ng cá»¥ tÃ¬m kiáº¿m Google. NÃ³ dá»±a trÃªn giáº£ Ä‘á»‹nh ráº±ng hai tá»« xuáº¥t hiá»‡n cÃ¹ng nhau thÆ°á»ng xuyÃªn hÆ¡n trong cÃ¡c trang web náº¿u chÃºng cÃ³ liÃªn quan hÆ¡n. Cho hai thuáº­t ngá»¯ ğ‘¡1 vÃ  ğ‘¡2, cÃ´ng thá»©c sau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh NGD giá»¯a hai thuáº­t ngá»¯.

ğ‘ğºğ·(ğ‘¥,ğ‘¦)=ğ‘šğ‘ğ‘¥{ğ‘™ğ‘œğ‘” ğ‘“(ğ‘¡1),ğ‘™ğ‘œğ‘” ğ‘“(ğ‘¡2)}âˆ’ğ‘™ğ‘œğ‘” ğ‘“(ğ‘¡1,ğ‘¡2) / ğ‘™ğ‘œğ‘”ğºâˆ’ğ‘šğ‘–ğ‘›{ğ‘™ğ‘œğ‘” ğ‘“(ğ‘¡1),ğ‘™ğ‘œğ‘” ğ‘“(ğ‘¡2)}(9)

trong Ä‘Ã³ cÃ¡c hÃ m ğ‘“(ğ‘¥) vÃ  ğ‘“(ğ‘¦) tráº£ vá» sá»‘ lÆ°á»£ng káº¿t quáº£ trong tÃ¬m kiáº¿m Google cá»§a cÃ¡c thuáº­t ngá»¯ Ä‘Ã£ cho, ğ‘“(ğ‘¥,ğ‘¦) tráº£ vá» sá»‘ lÆ°á»£ng káº¿t quáº£ trong tÃ¬m kiáº¿m Google khi cÃ¡c thuáº­t ngá»¯ Ä‘Æ°á»£c tÃ¬m kiáº¿m cÃ¹ng nhau vÃ  ğº Ä‘áº¡i diá»‡n cho tá»•ng sá»‘ trang trong tÃ¬m kiáº¿m google tá»•ng thá»ƒ. NGD Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ Ä‘o lÆ°á»ng liÃªn quan ngá»¯ nghÄ©a hÆ¡n lÃ  Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a vÃ¬ cÃ¡c thuáº­t ngá»¯ liÃªn quan xuáº¥t hiá»‡n cÃ¹ng nhau thÆ°á»ng xuyÃªn hÆ¡n trong cÃ¡c trang web máº·c dÃ¹ chÃºng cÃ³ thá»ƒ cÃ³ nghÄ©a Ä‘á»‘i láº­p.

4.2.7 Dependency-based models [1]: CÃ¡c cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn dependency xÃ¡c Ä‘á»‹nh nghÄ©a cá»§a má»™t tá»« hoáº·c cá»¥m tá»« Ä‘Ã£ cho báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c lÃ¡ng giá»ng cá»§a tá»« trong má»™t cá»­a sá»• Ä‘Ã£ cho. CÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn dependency ban Ä‘áº§u phÃ¢n tÃ­ch corpus dá»±a trÃªn phÃ¢n phá»‘i cá»§a nÃ³ báº±ng Inductive Dependency Parsing [90]. Äá»‘i vá»›i má»—i tá»« Ä‘Ã£ cho, má»™t "syntactic context template" Ä‘Æ°á»£c xÃ¢y dá»±ng xem xÃ©t cáº£ cÃ¡c nÃºt Ä‘i trÆ°á»›c vÃ  Ä‘i sau tá»« trong cÃ¢y phÃ¢n tÃ­ch Ä‘Æ°á»£c xÃ¢y dá»±ng. VÃ­ dá»¥, cá»¥m tá»« "thinks <term> delicious" cÃ³ thá»ƒ cÃ³ má»™t template ngá»¯ cáº£nh nhÆ° "pizza, burger, food". Biá»ƒu diá»…n vector cá»§a má»™t tá»« Ä‘Æ°á»£c táº¡o thÃ nh báº±ng cÃ¡ch thÃªm má»—i cá»­a sá»• qua vá»‹ trÃ­ cÃ³ tá»« Ä‘ang xem xÃ©t lÃ m tá»« gá»‘c cá»§a nÃ³, cÃ¹ng vá»›i táº§n suáº¥t cá»§a cá»­a sá»• tá»« xuáº¥t hiá»‡n trong toÃ n bá»™ corpus. Khi vector nÃ y Ä‘Æ°á»£c táº¡o thÃ nh, Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a Ä‘Æ°á»£c tÃ­nh báº±ng Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c vector nÃ y. Levy vÃ  cá»™ng sá»± [59] Ä‘Ã£ Ä‘á» xuáº¥t DEPS embedding nhÆ° má»™t mÃ´ hÃ¬nh word-embedding dá»±a trÃªn dependency-based bag of words. MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c thá»­ nghiá»‡m vá»›i táº­p dá»¯ liá»‡u WS353 trong Ä‘Ã³ tÃ¡c vá»¥ lÃ  xáº¿p háº¡ng cÃ¡c tá»« tÆ°Æ¡ng tá»± cao hÆ¡n cÃ¡c tá»« liÃªn quan. Khi váº½ Ä‘Æ°á»ng cong recall precision, Ä‘Æ°á»ng cong DEPS cho tháº¥y Æ°u tiÃªn lá»›n hÆ¡n Ä‘á»‘i vá»›i xáº¿p háº¡ng Ä‘á»™ tÆ°Æ¡ng tá»± so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p BoW Ä‘Æ°á»£c so sÃ¡nh.

4.2.8 Kernel-based models [115]: CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kernel Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¬m cÃ¡c máº«u trong dá»¯ liá»‡u vÄƒn báº£n do Ä‘Ã³ cho phÃ©p phÃ¡t hiá»‡n Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn báº£n. Hai loáº¡i kernel chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng trong dá»¯ liá»‡u vÄƒn báº£n cá»¥ thá»ƒ lÃ  string hoáº·c sequence kernel [23] vÃ  tree kernel [84]. Moschitti vÃ  cá»™ng sá»± [84] Ä‘á» xuáº¥t tree kernels vÃ o nÄƒm 2007, chá»©a ba cáº¥u trÃºc con khÃ¡c nhau trong khÃ´ng gian tree kernel cá»¥ thá»ƒ lÃ  subtree - má»™t cÃ¢y cÃ³ gá»‘c khÃ´ng pháº£i lÃ  nÃºt lÃ¡ cÃ¹ng vá»›i cÃ¡c nÃºt con cá»§a nÃ³, subset tree - má»™t cÃ¢y cÃ³ gá»‘c khÃ´ng pháº£i lÃ  nÃºt lÃ¡ nhÆ°ng khÃ´ng káº¿t há»£p táº¥t cáº£ cÃ¡c nÃºt con cá»§a nÃ³ vÃ  khÃ´ng phÃ¡ vá»¡ cÃ¡c quy táº¯c ngá»¯ phÃ¡p, partial tree - má»™t cáº¥u trÃºc cÃ¢y tÆ°Æ¡ng tá»± cháº·t cháº½ vá»›i subset tree nhÆ°ng nÃ³ khÃ´ng pháº£i lÃºc nÃ o cÅ©ng tuÃ¢n theo cÃ¡c quy táº¯c ngá»¯ phÃ¡p. Tree kernels Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong viá»‡c xÃ¡c Ä‘á»‹nh cáº¥u trÃºc trong cÃ¡c cÃ¢u Ä‘áº§u vÃ o dá»±a trÃªn constituency hoáº·c dependency, xem xÃ©t cÃ¡c quy táº¯c ngá»¯ phÃ¡p cá»§a ngÃ´n ngá»¯. Kernels Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c thuáº­t toÃ¡n machine learning nhÆ° Support Vector Machines(SVMs) Ä‘á»ƒ thÃ­ch á»©ng vá»›i dá»¯ liá»‡u vÄƒn báº£n trong cÃ¡c tÃ¡c vá»¥ khÃ¡c nhau nhÆ° Semantic Role Labelling, Paraphrase Identification [28], Answer Extraction [85], Question-Answer classification [86], Relational text categorization [83], Answer Re-ranking in QA tasks [112] vÃ  Relational text entailment [87]. Severyn vÃ  cá»™ng sá»± [113] Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a dá»±a trÃªn kernel biá»ƒu diá»…n vÄƒn báº£n trá»±c tiáº¿p nhÆ° "structural objects" báº±ng cÃ¡ch sá»­ dá»¥ng Syntactic tree kernel [27] vÃ  Partial tree kernels [82]. HÃ m kernel sau Ä‘Ã³ káº¿t há»£p cÃ¡c cáº¥u trÃºc cÃ¢y vá»›i cÃ¡c vector Ä‘áº·c trÆ°ng ngá»¯ nghÄ©a tá»« hai mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t trong STS 2012 cá»¥ thá»ƒ lÃ  UKP [12] vÃ  Takelab [110] vÃ  má»™t sá»‘ Ä‘áº·c trÆ°ng bá»• sung bao gá»“m Ä‘iá»ƒm Ä‘á»™ tÆ°Æ¡ng tá»± cosine dá»±a trÃªn named entities, part of speech tags, v.v. CÃ¡c tÃ¡c giáº£ so sÃ¡nh hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng bá»‘n cáº¥u trÃºc cÃ¢y khÃ¡c nhau cá»¥ thá»ƒ lÃ  shallow tree, constituency tree, dependency tree, phrase-dependency tree, vÃ  cÃ¡c vector Ä‘áº·c trÆ°ng Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ trÃªn. Há» thiáº¿t láº­p ráº±ng cÃ¡c mÃ´ hÃ¬nh tree kernel hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n táº¥t cáº£ cÃ¡c vector Ä‘áº·c trÆ°ng káº¿t há»£p. MÃ´ hÃ¬nh sá»­ dá»¥ng Support Vector Regression Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ä‘iá»ƒm Ä‘á»™ tÆ°Æ¡ng tá»± cuá»‘i cÃ¹ng vÃ  nÃ³ cÃ³ thá»ƒ há»¯u Ã­ch trong cÃ¡c á»©ng dá»¥ng NLP downstream khÃ¡c nhau nhÆ° question-answering, text-entailment extraction, v.v. Amir vÃ  cá»™ng sá»± [9] Ä‘á» xuáº¥t má»™t thuáº­t toÃ¡n Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a khÃ¡c sá»­ dá»¥ng cÃ¡c hÃ m kernel. Há» sá»­ dá»¥ng tree kernels dá»±a trÃªn constituency trong Ä‘Ã³ cÃ¢u Ä‘Æ°á»£c phÃ¢n tÃ­ch thÃ nh subject, verb, vÃ  object dá»±a trÃªn giáº£ Ä‘á»‹nh ráº±ng háº§u háº¿t cÃ¡c thuá»™c tÃ­nh ngá»¯ nghÄ©a cá»§a cÃ¢u Ä‘Æ°á»£c gÃ¡n cho cÃ¡c thÃ nh pháº§n nÃ y. CÃ¡c cÃ¢u Ä‘áº§u vÃ o Ä‘Æ°á»£c phÃ¢n tÃ­ch báº±ng Stanford Parser Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c káº¿t há»£p khÃ¡c nhau cá»§a subject, verb, vÃ  object. Äá»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¡c thÃ nh pháº§n khÃ¡c nhau cá»§a cÃ¡c cÃ¢u Ä‘Ã£ cho Ä‘Æ°á»£c tÃ­nh báº±ng má»™t knowledge base, vÃ  cÃ¡c ká»¹ thuáº­t tÃ­nh trung bÃ¬nh khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh trung bÃ¬nh cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± Ä‘á»ƒ Æ°á»›c tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± tá»•ng thá»ƒ, vÃ  ká»¹ thuáº­t tá»‘t nháº¥t trong sá»‘ chÃºng Ä‘Æ°á»£c chá»n dá»±a trÃªn giÃ¡ trá»‹ root mean squared error cho má»™t táº­p dá»¯ liá»‡u cá»¥ thá»ƒ. Trong nghiÃªn cá»©u gáº§n Ä‘Ã¢y, cÃ¡c phÆ°Æ¡ng phÃ¡p deep learning Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thay tháº¿ cÃ¡c mÃ´ hÃ¬nh machine learning truyá»n thá»‘ng vÃ  sá»­ dá»¥ng hiá»‡u quáº£ tÃ­nh toÃ n váº¹n cáº¥u trÃºc cá»§a kernels trong giai Ä‘oáº¡n trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng nhÃºng [26,28]. MÃ´ hÃ¬nh Ä‘áº¡t káº¿t quáº£ tá»‘t nháº¥t trong SemEval-2017 Task 1, Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Tian vÃ  cá»™ng sá»± [125] sá»­ dá»¥ng kernels Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u vÄƒn báº£n Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»±. MÃ´ hÃ¬nh Ä‘á» xuáº¥t má»™t mÃ´ hÃ¬nh ensemble sá»­ dá»¥ng cáº£ phÆ°Æ¡ng phÃ¡p NLP truyá»n thá»‘ng vÃ  phÆ°Æ¡ng phÃ¡p deep learning. Hai Ä‘áº·c trÆ°ng khÃ¡c nhau cá»¥ thá»ƒ lÃ  sentence pair matching features vÃ  single sentence features Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± báº±ng cÃ¡c regressor thÃªm tÃ­nh phi tuyáº¿n vÃ o dá»± Ä‘oÃ¡n. Trong trÃ­ch xuáº¥t single sentence feature, dependency-based tree kernels Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng dependency trong má»™t cÃ¢u Ä‘Ã£ cho, vÃ  trong sentence pair matching features, constituency-based parse tree kernels Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ¬m cÃ¡c cáº¥u trÃºc con chung giá»¯a ba Ä‘áº·c tÃ­nh khÃ¡c nhau cá»§a khÃ´ng gian tree kernel. Äiá»ƒm Ä‘á»™ tÆ°Æ¡ng tá»± cuá»‘i cÃ¹ng Ä‘Æ°á»£c truy cáº­p báº±ng cÃ¡ch tÃ­nh trung bÃ¬nh giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± NLP truyá»n thá»‘ng vÃ  giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± dá»±a trÃªn deep learning. MÃ´ hÃ¬nh Ä‘áº¡t há»‡ sá»‘ tÆ°Æ¡ng quan Pearson 73.16% trong táº­p dá»¯ liá»‡u STS.

4.2.9 Word-attention models [57]: Trong háº§u háº¿t cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus, táº¥t cáº£ cÃ¡c thÃ nh pháº§n vÄƒn báº£n Ä‘Æ°á»£c xem xÃ©t cÃ³ táº§m quan trá»ng báº±ng nhau; tuy nhiÃªn, viá»‡c diá»…n giáº£i cá»§a con ngÆ°á»i vá» Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± thÆ°á»ng phá»¥ thuá»™c vÃ o cÃ¡c tá»« khÃ³a trong má»™t ngá»¯ cáº£nh Ä‘Ã£ cho. CÃ¡c mÃ´ hÃ¬nh word attention náº¯m báº¯t táº§m quan trá»ng cá»§a cÃ¡c tá»« tá»« cÃ¡c corpus cÆ¡ báº£n [67] trÆ°á»›c khi tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. CÃ¡c ká»¹ thuáº­t khÃ¡c nhau nhÆ° word frequency, alignment, word association Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ náº¯m báº¯t attention-weights cá»§a vÄƒn báº£n Ä‘ang xem xÃ©t. Attention Constituency Vector Tree (ACV-Tree) Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Le vÃ  cá»™ng sá»± [57] tÆ°Æ¡ng tá»± nhÆ° má»™t parse tree trong Ä‘Ã³ má»™t tá»« cá»§a cÃ¢u Ä‘Æ°á»£c lÃ m gá»‘c vÃ  pháº§n cÃ²n láº¡i cá»§a cÃ¢u Ä‘Æ°á»£c phÃ¢n tÃ­ch nhÆ° má»™t Noun Phrase (NP) vÃ  má»™t Verb Phrase (VP). CÃ¡c nÃºt trong cÃ¢y lÆ°u trá»¯ ba thuá»™c tÃ­nh khÃ¡c nhau cá»§a tá»« Ä‘ang xem xÃ©t: word vector Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi má»™t corpus cÆ¡ báº£n, attention-weight, vÃ  "modification-relations" cá»§a tá»«. Modification relations cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  cÃ¡c tÃ­nh tá»« hoáº·c tráº¡ng tá»« sá»­a Ä‘á»•i nghÄ©a cá»§a tá»« khÃ¡c. Táº¥t cáº£ ba thÃ nh pháº§n Ä‘Æ°á»£c liÃªn káº¿t Ä‘á»ƒ táº¡o thÃ nh biá»ƒu diá»…n cá»§a tá»«. Má»™t hÃ m tree kernel Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a hai tá»« dá»±a trÃªn phÆ°Æ¡ng trÃ¬nh dÆ°á»›i Ä‘Ã¢y:

ğ‘‡ğ‘Ÿğ‘’ğ‘’ğ¾ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™(ğ‘‡1,ğ‘‡2)=âˆ‘ï¸ğ‘›1âˆˆğ‘ğ‘‡1âˆ‘ï¸ğ‘›2âˆˆğ‘ğ‘‡2Î”(ğ‘›1,ğ‘›2) (10)

Î”(ğ‘›1,ğ‘›2)=(0,if(ğ‘›1and / orğ‘›2are non-leaf-nodes) and ğ‘›1â‰ ğ‘›2
ğ´ğ‘¤Ã—ğ‘†ğ¼ğ‘€(ğ‘£ğ‘’ğ‘ 1,ğ‘£ğ‘’ğ‘ 2),ifğ‘›1,ğ‘›2are leaf nodes
ğœ‡(ğœ†2+Ãğ‘™ğ‘šğ‘=1ğ›¿ğ‘(ğ‘ğ‘›1,ğ‘ğ‘›2)),otherwise(11)

trong Ä‘Ã³ ğ‘›1,ğ‘›2 biá»ƒu diá»…n cÃ¡c nÃºt, ğ‘†ğ¼ğ‘€(ğ‘£ğ‘’ğ‘ 1,ğ‘£ğ‘’ğ‘ 2) Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c vector, ğ›¿ğ‘(.) tÃ­nh sá»‘ lÆ°á»£ng cÃ¡c dÃ£y con chung cÃ³ Ä‘á»™ dÃ i ğ‘, ğœ†,ğœ‡ biá»ƒu thá»‹ cÃ¡c yáº¿u tá»‘ decay cho Ä‘á»™ dÃ i cá»§a cÃ¡c dÃ£y con vÃ  chiá»u cao cá»§a cÃ¢y tÆ°Æ¡ng á»©ng, ğ‘ğ‘›1, ğ‘ğ‘›2 Ä‘á» cáº­p Ä‘áº¿n cÃ¡c nÃºt con vÃ  ğ‘™ğ‘š=ğ‘šğ‘–ğ‘›(ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘ğ‘›1),ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘ğ‘›2)). Thuáº­t toÃ¡n Ä‘Æ°á»£c thá»­ nghiá»‡m báº±ng cÃ¡c táº­p dá»¯ liá»‡u benchmark STS vÃ  Ä‘Ã£ cho tháº¥y hiá»‡u suáº¥t tá»‘t hÆ¡n trong 12 trÃªn 19 táº­p dá»¯ liá»‡u STS Ä‘Æ°á»£c chá»n [57, 101].

KhÃ´ng giá»‘ng nhÆ° cÃ¡c há»‡ thá»‘ng dá»±a trÃªn kiáº¿n thá»©c, cÃ¡c há»‡ thá»‘ng dá»±a trÃªn corpus Ä‘á»™c láº­p vá»›i ngÃ´n ngá»¯ vÃ  lÄ©nh vá»±c [8]. VÃ¬ chÃºng phá»¥ thuá»™c vÃ o cÃ¡c thÆ°á»›c Ä‘o thá»‘ng kÃª, cÃ¡c phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ dá»… dÃ ng thÃ­ch á»©ng qua cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau báº±ng cÃ¡ch sá»­ dá»¥ng má»™t corpus hiá»‡u quáº£. Vá»›i sá»± phÃ¡t triá»ƒn cá»§a internet, viá»‡c xÃ¢y dá»±ng corpus cá»§a háº§u háº¿t cÃ¡c ngÃ´n ngá»¯ hoáº·c lÄ©nh vá»±c Ä‘Ã£ trá»Ÿ nÃªn khÃ¡ dá»… dÃ ng. CÃ¡c ká»¹ thuáº­t web crawling Ä‘Æ¡n giáº£n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c corpus lá»›n [13]. Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus khÃ´ng xem xÃ©t nghÄ©a thá»±c táº¿ cá»§a cÃ¡c tá»«. ThÃ¡ch thá»©c khÃ¡c mÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus gáº·p pháº£i lÃ  nhu cáº§u xá»­ lÃ½ cÃ¡c corpus lá»›n Ä‘Æ°á»£c xÃ¢y dá»±ng, Ä‘Ã¢y lÃ  má»™t tÃ¡c vá»¥ khÃ¡ tá»‘n thá»i gian vÃ  phá»¥ thuá»™c vÃ o tÃ i nguyÃªn. VÃ¬ hiá»‡u suáº¥t cá»§a cÃ¡c thuáº­t toÃ¡n phá»¥ thuá»™c lá»›n vÃ o corpus cÆ¡ báº£n, viá»‡c xÃ¢y dá»±ng má»™t corpus hiá»‡u quáº£ lÃ  Ä‘iá»u tá»‘i quan trá»ng. Máº·c dÃ¹ cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ ná»— lá»±c Ä‘á»ƒ xÃ¢y dá»±ng má»™t corpus sáº¡ch vÃ  hiá»‡u quáº£ nhÆ° corpus C4 Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng web crawling vÃ  nÄƒm bÆ°á»›c Ä‘á»ƒ lÃ m sáº¡ch corpus [103], má»™t "corpus lÃ½ tÆ°á»Ÿng" váº«n chÆ°a Ä‘Æ°á»£c cÃ¡c nhÃ  nghiÃªn cá»©u xÃ¡c Ä‘á»‹nh.

5 CÃC PHÆ¯Æ NG PHÃP Dá»°A TRÃŠN Máº NG NEURAL SÃ‚U

CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a Ä‘Ã£ khai thÃ¡c nhá»¯ng phÃ¡t triá»ƒn gáº§n Ä‘Ã¢y trong máº¡ng neural Ä‘á»ƒ nÃ¢ng cao hiá»‡u suáº¥t. CÃ¡c ká»¹ thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t bao gá»“m Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM), Bidirectional Long Short Term Memory (Bi-LSTM), vÃ  Recursive Tree LSTM. CÃ¡c mÃ´ hÃ¬nh máº¡ng neural sÃ¢u Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn hai thao tÃ¡c cÆ¡ báº£n: convolution vÃ  pooling. Thao tÃ¡c convolution trong dá»¯ liá»‡u vÄƒn báº£n cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  tá»•ng cá»§a tÃ­ch element-wise cá»§a má»™t vector cÃ¢u vÃ  má»™t ma tráº­n trá»ng sá»‘. CÃ¡c thao tÃ¡c convolution Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng. CÃ¡c thao tÃ¡c pooling Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ loáº¡i bá» cÃ¡c Ä‘áº·c trÆ°ng cÃ³ tÃ¡c Ä‘á»™ng tiÃªu cá»±c, vÃ  chá»‰ xem xÃ©t nhá»¯ng giÃ¡ trá»‹ Ä‘áº·c trÆ°ng cÃ³ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n tÃ¡c vá»¥ trong tay. CÃ³ cÃ¡c loáº¡i thao tÃ¡c pooling khÃ¡c nhau vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t lÃ  Max pooling, trong Ä‘Ã³ chá»‰ giÃ¡ trá»‹ tá»‘i Ä‘a trong khÃ´ng gian filter Ä‘Ã£ cho Ä‘Æ°á»£c chá»n. Pháº§n nÃ y mÃ´ táº£ má»™t sá»‘ phÆ°Æ¡ng phÃ¡p triá»ƒn khai máº¡ng neural sÃ¢u Ä‘á»ƒ Æ°á»›c tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a cÃ¡c Ä‘oáº¡n vÄƒn báº£n. Máº·c dÃ¹ cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c mÃ´ táº£ dÆ°á»›i Ä‘Ã¢y khai thÃ¡c word embeddings Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡c corpus lá»›n, máº¡ng neural sÃ¢u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Æ°á»›c tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¡c word-embeddings, do Ä‘Ã³ chÃºng Ä‘Æ°á»£c phÃ¢n loáº¡i riÃªng khá»i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus.

5.1 CÃ¡c loáº¡i phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a dá»±a trÃªn máº¡ng neural sÃ¢u:

â€¢Wang vÃ  cá»™ng sá»± [130] Ä‘á» xuáº¥t má»™t mÃ´ hÃ¬nh Ä‘á»ƒ Æ°á»›c tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a hai cÃ¢u dá»±a trÃªn lexical decomposition vÃ  composition. MÃ´ hÃ¬nh sá»­ dá»¥ng cÃ¡c nhÃºng Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c ğ‘¤ğ‘œğ‘Ÿğ‘‘ 2ğ‘£ğ‘’ğ‘ Ä‘á»ƒ táº¡o thÃ nh biá»ƒu diá»…n vector cá»§a cÃ¡c cÃ¢u ğ‘ 1 vÃ  ğ‘ 2. Má»™t ma tráº­n tÆ°Æ¡ng tá»± ğ‘€ vá»›i chiá»u ğ‘–xğ‘— Ä‘Æ°á»£c xÃ¢y dá»±ng trong Ä‘Ã³ i vÃ  j lÃ  sá»‘ lÆ°á»£ng tá»« trong cÃ¢u 1 (ğ‘†1) vÃ  cÃ¢u 2 (ğ‘†2) tÆ°Æ¡ng á»©ng. CÃ¡c Ã´ cá»§a ma tráº­n Ä‘Æ°á»£c Ä‘iá»n vá»›i Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c tá»« trong cÃ¡c chá»‰ sá»‘ cá»§a ma tráº­n. Ba hÃ m khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c vector semantic matching Â®ğ‘ 1 vÃ  Â®ğ‘ 2, global function, local function, vÃ  max function. Global function xÃ¢y dá»±ng semantic matching vector cá»§a ğ‘†1 báº±ng cÃ¡ch láº¥y tá»•ng cÃ³ trá»ng sá»‘ cá»§a cÃ¡c vector, cá»§a táº¥t cáº£ cÃ¡c tá»« trong ğ‘†2, local function, chá»‰ xem xÃ©t cÃ¡c word vectors trong má»™t kÃ­ch thÆ°á»›c cá»­a sá»• Ä‘Ã£ cho, vÃ  max function chá»‰ láº¥y cÃ¡c vector cá»§a cÃ¡c tá»« cÃ³ Ä‘á»™ tÆ°Æ¡ng tá»± tá»‘i Ä‘a. Giai Ä‘oáº¡n thá»© hai cá»§a thuáº­t toÃ¡n sá»­ dá»¥ng ba decomposition functions khÃ¡c nhau - rigid, linear, vÃ  orthogonal - Ä‘á»ƒ Æ°á»›c tÃ­nh similarity component vÃ  dissimilarity component giá»¯a cÃ¡c sentence vectors vÃ  semantic matching vectors. Cáº£ similarity component vÃ  dissimilarity component vectors Ä‘Æ°á»£c truyá»n qua má»™t lá»›p convolution hai kÃªnh theo sau bá»Ÿi má»™t lá»›p max-pooling Ä‘Æ¡n. Äá»™ tÆ°Æ¡ng tá»± sau Ä‘Ã³ Ä‘Æ°á»£c tÃ­nh báº±ng má»™t lá»›p sigmoid Æ°á»›c tÃ­nh giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± trong pháº¡m vi tá»« 0 Ä‘áº¿n 1. MÃ´ hÃ¬nh Ä‘Æ°á»£c thá»­ nghiá»‡m báº±ng táº­p dá»¯ liá»‡u QASent [129] vÃ  táº­p dá»¯ liá»‡u WikiQA [72]. Hai thÆ°á»›c Ä‘o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Æ°á»›c tÃ­nh hiá»‡u suáº¥t lÃ  mean average precision (MAP) vÃ  mean reciprocal rank (MRR). MÃ´ hÃ¬nh Ä‘áº¡t MAP tá»‘t nháº¥t trong táº­p dá»¯ liá»‡u QASent vÃ  MAP vÃ  MRR tá»‘t nháº¥t trong táº­p dá»¯ liá»‡u WikiQA. Yang Shao [114] Ä‘á» xuáº¥t má»™t thuáº­t toÃ¡n Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a khai thÃ¡c phÃ¡t triá»ƒn gáº§n Ä‘Ã¢y trong máº¡ng neural báº±ng cÃ¡ch sá»­ dá»¥ng word embeddings ğºğ‘™ğ‘œğ‘‰ğ‘’. Cho hai cÃ¢u, mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn táº­p cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. CÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ bao gá»“m loáº¡i bá» dáº¥u cÃ¢u, tokenization, vÃ  sá»­ dá»¥ng cÃ¡c vector ğºğ‘™ğ‘œğ‘‰ğ‘’ Ä‘á»ƒ thay tháº¿ cÃ¡c tá»« báº±ng word embeddings. Äá»™ dÃ i cá»§a Ä‘áº§u vÃ o Ä‘Æ°á»£c Ä‘áº·t thÃ nh 30 tá»«, Ä‘iá»u nÃ y Ä‘Æ°á»£c Ä‘áº¡t báº±ng cÃ¡ch loáº¡i bá» hoáº·c padding khi cáº§n thiáº¿t. Má»™t sá»‘ Ä‘áº·c trÆ°ng thá»§ cÃ´ng Ä‘áº·c biá»‡t nhÆ° flag values chá»‰ ra liá»‡u cÃ¡c tá»« hoáº·c sá»‘ cÃ³ xuáº¥t hiá»‡n trong cáº£ hai cÃ¢u vÃ  POS tagging one hot encoded values, Ä‘Æ°á»£c thÃªm vÃ o cÃ¡c vector ğºğ‘™ğ‘œğ‘‰ğ‘’. CÃ¡c vector sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t CNN vá»›i 300 filters vÃ  má»™t lá»›p max-pooling Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o thÃ nh cÃ¡c sentence vectors. HÃ m kÃ­ch hoáº¡t ReLU Ä‘Æ°á»£c sá»­ dá»¥ng trong lá»›p convolution. Sá»± khÃ¡c biá»‡t ngá»¯ nghÄ©a giá»¯a cÃ¡c vector Ä‘Æ°á»£c tÃ­nh báº±ng element-wise absolute difference vÃ  element-wise multiplication cá»§a hai sentence-vectors Ä‘Æ°á»£c táº¡o. CÃ¡c vector Ä‘Æ°á»£c truyá»n tiáº¿p qua hai lá»›p fully-connected, dá»± Ä‘oÃ¡n phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. Hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng cÃ¡c táº­p dá»¯ liá»‡u SemEval trong Ä‘Ã³ mÃ´ hÃ¬nh Ä‘Æ°á»£c xáº¿p háº¡ng thá»© 3 trong track táº­p dá»¯ liá»‡u SemEval 2017.

â€¢Máº¡ng LSTM lÃ  má»™t loáº¡i Ä‘áº·c biá»‡t cá»§a Recurrent Neural Networks (RNN). Trong khi xá»­ lÃ½ dá»¯ liá»‡u vÄƒn báº£n, Ä‘iá»u cáº§n thiáº¿t cho cÃ¡c máº¡ng lÃ  nhá»› cÃ¡c tá»« trÆ°á»›c Ä‘Ã³, Ä‘á»ƒ náº¯m báº¯t ngá»¯ cáº£nh, vÃ  RNNs cÃ³ kháº£ nÄƒng lÃ m nhÆ° váº­y. Tuy nhiÃªn, khÃ´ng pháº£i táº¥t cáº£ ná»™i dung trÆ°á»›c Ä‘Ã³ Ä‘á»u cÃ³ Ã½ nghÄ©a Ä‘á»‘i vá»›i tá»«/cá»¥m tá»« tiáº¿p theo, do Ä‘Ã³ RNNs gáº·p nhÆ°á»£c Ä‘iá»ƒm cá»§a long term dependency. LSTMs Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ kháº¯c phá»¥c váº¥n Ä‘á» nÃ y. LSTMs cÃ³ cÃ¡c gates cho phÃ©p máº¡ng chá»n ná»™i dung nÃ³ pháº£i nhá»›. VÃ­ dá»¥, xem xÃ©t Ä‘oáº¡n vÄƒn báº£n, "Mary is from Finland. She is fluent in Finnish. She loves to travel." Trong khi chÃºng ta Ä‘áº¿n cÃ¢u thá»© hai cá»§a Ä‘oáº¡n vÄƒn báº£n, Ä‘iá»u cáº§n thiáº¿t lÃ  nhá»› cÃ¡c tá»« "Mary" vÃ  "Finland." Tuy nhiÃªn, khi Ä‘áº¿n cÃ¢u thá»© ba, máº¡ng cÃ³ thá»ƒ quÃªn tá»« "Finland." Kiáº¿n trÃºc cá»§a LSTMs cho phÃ©p Ä‘iá»u nÃ y. Nhiá»u nhÃ  nghiÃªn cá»©u sá»­ dá»¥ng kiáº¿n trÃºc LSTM Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a cÃ¡c khá»‘i vÄƒn báº£n. Tien vÃ  cá»™ng sá»± [126] sá»­ dá»¥ng má»™t máº¡ng káº¿t há»£p vá»›i LSTM vÃ  CNN Ä‘á»ƒ táº¡o thÃ nh sentence embedding tá»« cÃ¡c word embeddings Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c theo sau bá»Ÿi má»™t kiáº¿n trÃºc LSTM Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± cá»§a chÃºng. Tai vÃ  cá»™ng sá»± [124] Ä‘á» xuáº¥t má»™t kiáº¿n trÃºc LSTM Ä‘á»ƒ Æ°á»›c tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a hai cÃ¢u Ä‘Ã£ cho. Ban Ä‘áº§u, cÃ¡c cÃ¢u Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh sentence representations báº±ng cÃ¡ch sá»­ dá»¥ng Tree-LSTM trÃªn parse tree cá»§a cÃ¡c cÃ¢u. Nhá»¯ng sentence representations nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t máº¡ng neural tÃ­nh toÃ¡n khoáº£ng cÃ¡ch tuyá»‡t Ä‘á»‘i giá»¯a cÃ¡c vector vÃ  gÃ³c giá»¯a cÃ¡c vector. ThÃ­ nghiá»‡m Ä‘Æ°á»£c tiáº¿n hÃ nh báº±ng táº­p dá»¯ liá»‡u SICK, vÃ  thÆ°á»›c Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± thay Ä‘á»•i vá»›i pháº¡m vi tá»« 1 Ä‘áº¿n 5. Lá»›p áº©n bao gá»“m 50 neurons vÃ  lá»›p softmax cuá»‘i cÃ¹ng phÃ¢n loáº¡i cÃ¡c cÃ¢u trong pháº¡m vi Ä‘Ã£ cho. MÃ´ hÃ¬nh Tree-LSTM Ä‘áº¡t Ä‘Æ°á»£c há»‡ sá»‘ tÆ°Æ¡ng quan Pearson vÃ  Spearman tá»‘t hÆ¡n trong cÃ¡c táº­p dá»¯ liá»‡u gold standard, so vá»›i cÃ¡c mÃ´ hÃ¬nh máº¡ng neural khÃ¡c Ä‘Æ°á»£c so sÃ¡nh.

â€¢He vÃ  Lin [39] Ä‘á» xuáº¥t má»™t kiáº¿n trÃºc lai sá»­ dá»¥ng Bi-LSTM vÃ  CNN Ä‘á»ƒ Æ°á»›c tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a cá»§a mÃ´ hÃ¬nh. Bi-LSTMs cÃ³ hai LSTMs cháº¡y song song, má»™t tá»« Ä‘áº§u cÃ¢u vÃ  má»™t tá»« cuá»‘i, do Ä‘Ã³ náº¯m báº¯t toÃ n bá»™ ngá»¯ cáº£nh. Trong mÃ´ hÃ¬nh cá»§a há», He vÃ  Lin sá»­ dá»¥ng Bi-LSTM cho context modelling. Má»™t pairwise word interaction model Ä‘Æ°á»£c xÃ¢y dá»±ng tÃ­nh toÃ¡n má»™t comparison unit giá»¯a cÃ¡c vector Ä‘Æ°á»£c dáº«n xuáº¥t tá»« cÃ¡c hidden states cá»§a hai LSTMs báº±ng cÃ¡ch sá»­ dá»¥ng cÃ´ng thá»©c dÆ°á»›i Ä‘Ã¢y:

ğ¶ğ‘œğ‘ˆ(Â®â„1,Â®â„2)={ğ‘ğ‘œğ‘ (Â®â„1,Â®â„2),ğ‘’ğ‘¢ğ‘(Â®â„1,Â®â„2),ğ‘šğ‘ğ‘›â„((Â®â„1,Â®â„2)} (12)

trong Ä‘Ã³ Â®â„1 vÃ  Â®â„2 biá»ƒu diá»…n cÃ¡c vector tá»« hidden state cá»§a LSTMs vÃ  cÃ¡c hÃ m ğ‘ğ‘œğ‘ (),ğ‘’ğ‘¢ğ‘(),ğ‘šğ‘ğ‘›â„() tÃ­nh toÃ¡n Cosine distance, Euclidean distance, vÃ  Manhattan distance, tÆ°Æ¡ng á»©ng. MÃ´ hÃ¬nh nÃ y tÆ°Æ¡ng tá»± nhÆ° cÃ¡c mÃ´ hÃ¬nh word attention dá»±a trÃªn máº¡ng neural gáº§n Ä‘Ã¢y khÃ¡c [7,10]. Tuy nhiÃªn, attention weights khÃ´ng Ä‘Æ°á»£c thÃªm vÃ o, thay vÃ o Ä‘Ã³ cÃ¡c khoáº£ng cÃ¡ch Ä‘Æ°á»£c thÃªm nhÆ° weights. Word interaction model Ä‘Æ°á»£c theo sau bá»Ÿi má»™t similarity focus layer trong Ä‘Ã³ weights Ä‘Æ°á»£c thÃªm vÃ o cÃ¡c word interactions (Ä‘Æ°á»£c tÃ­nh trong cÃ¡c lá»›p trÆ°á»›c Ä‘Ã³) dá»±a trÃªn táº§m quan trá»ng cá»§a chÃºng trong viá»‡c xÃ¡c Ä‘á»‹nh Ä‘á»™ tÆ°Æ¡ng tá»±. Nhá»¯ng re-weighted vectors nÃ y Ä‘Æ°á»£c Ä‘Æ°a vÃ o máº¡ng convolution cuá»‘i cÃ¹ng. Máº¡ng bao gá»“m cÃ¡c lá»›p spatial convolution vÃ  spatial max pooling xen káº½, hÃ m kÃ­ch hoáº¡t ReLU Ä‘Æ°á»£c sá»­ dá»¥ng vÃ  á»Ÿ cuá»‘i máº¡ng káº¿t thÃºc báº±ng hai lá»›p fully connected theo sau bá»Ÿi má»™t lá»›p LogSoftmax Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t giáº£i phÃ¡p phi tuyáº¿n. MÃ´ hÃ¬nh nÃ y vÆ°á»£t trá»™i hÆ¡n mÃ´ hÃ¬nh Tree-LSTM Ä‘Æ°á»£c Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³ trÃªn táº­p dá»¯ liá»‡u SICK.

â€¢Lopez-Gazpio vÃ  cá»™ng sá»± [67] Ä‘á» xuáº¥t má»™t pháº§n má»Ÿ rá»™ng cho Decomposable Attention Model (DAM) hiá»‡n cÃ³ Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Parikh vÃ  cá»™ng sá»± [92] ban Ä‘áº§u Ä‘Æ°á»£c sá»­ dá»¥ng cho Natural Language Inference(NLI). NLI Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n loáº¡i má»™t khá»‘i vÄƒn báº£n Ä‘Ã£ cho thÃ nh má»™t quan há»‡ cá»¥ thá»ƒ nhÆ° entailment, neutral, hoáº·c contradiction. MÃ´ hÃ¬nh DAM sá»­ dá»¥ng feed-forward neural networks trong ba lá»›p liÃªn tiáº¿p lÃ  attention layer, comparison layer, vÃ  aggregation layer. Cho hai cÃ¢u, attention layer táº¡o ra hai attention vectors cho má»—i cÃ¢u báº±ng cÃ¡ch tÃ¬m sá»± chá»“ng chÃ©o giá»¯a chÃºng. Comparison layer ná»‘i cÃ¡c attention vectors vá»›i sentence vectors Ä‘á»ƒ táº¡o thÃ nh má»™t representative vector duy nháº¥t cho má»—i cÃ¢u. Aggregation layer cuá»‘i cÃ¹ng lÃ m pháº³ng cÃ¡c vector vÃ  tÃ­nh toÃ¡n phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn cÃ¡c giÃ¡ trá»‹ Ä‘Ã£ cho. Lopez-Gazpio vÃ  cá»™ng sá»± [67] sá»­ dá»¥ng word n-grams Ä‘á»ƒ náº¯m báº¯t attention trong lá»›p Ä‘áº§u tiÃªn thay vÃ¬ cÃ¡c tá»« riÃªng láº». ğ‘›âˆ’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘  cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t chuá»—i n tá»« liá»n ká» vá»›i tá»« Ä‘Ã£ cho, n-grams Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ náº¯m báº¯t ngá»¯ cáº£nh trong cÃ¡c tÃ¡c vá»¥ NLP khÃ¡c nhau. Äá»ƒ chá»©a n-grams, má»™t Recurrent Neural Network (RNN) Ä‘Æ°á»£c thÃªm vÃ o attention layer. CÃ¡c biáº¿n thá»ƒ Ä‘Æ°á»£c Ä‘á» xuáº¥t báº±ng cÃ¡ch thay tháº¿ RNN báº±ng Long-Term Short memory (LSTM) vÃ  Convolutional Neural Network (CNN). MÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a báº±ng cÃ¡ch thay tháº¿ cÃ¡c lá»›p cuá»‘i cÃ¹ng cá»§a entailment relationships báº±ng cÃ¡c pháº¡m vi Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a tá»« 0 Ä‘áº¿n 5. CÃ¡c mÃ´ hÃ¬nh Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n trong viá»‡c náº¯m báº¯t Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a trong táº­p dá»¯ liá»‡u SICK vÃ  táº­p dá»¯ liá»‡u benchmark STS khi so sÃ¡nh vá»›i DAM vÃ  cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhÆ° Sent2vec [91] vÃ  BiLSTM trong sá»‘ nhá»¯ng mÃ´ hÃ¬nh khÃ¡c.

â€¢Transformer-based models: Vaswani vÃ  cá»™ng sá»± [128] Ä‘á» xuáº¥t má»™t mÃ´ hÃ¬nh transformer dá»±a trÃªn cÃ¡c cÆ¡ cháº¿ attention Ä‘á»ƒ náº¯m báº¯t cÃ¡c thuá»™c tÃ­nh ngá»¯ nghÄ©a cá»§a cÃ¡c tá»« trong cÃ¡c embeddings. Transformer cÃ³ hai pháº§n 'encoder' vÃ  'decoder'. Encoder bao gá»“m cÃ¡c lá»›p multi-head attention mechanisms theo sau bá»Ÿi má»™t máº¡ng neural feed-forward fully connected. Decoder tÆ°Æ¡ng tá»± nhÆ° encoder vá»›i má»™t lá»›p bá»• sung cá»§a multi-head attention náº¯m báº¯t attention weights trong Ä‘áº§u ra cá»§a encoder. Máº·c dÃ¹ mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c Ä‘á» xuáº¥t cho tÃ¡c vá»¥ machine translation, Devlin vÃ  cá»™ng sá»± [29] Ä‘Ã£ sá»­ dá»¥ng mÃ´ hÃ¬nh transformer Ä‘á»ƒ táº¡o ra BERT word embeddings. Sun vÃ  cá»™ng sá»± [121] Ä‘á» xuáº¥t má»™t framework multi-tasking sá»­ dá»¥ng transformers gá»i lÃ  ERNIE 2.0. Trong framework nÃ y, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c liÃªn tá»¥c tá»©c lÃ  khi má»™t tÃ¡c vá»¥ má»›i Ä‘Æ°á»£c trÃ¬nh bÃ y, mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh Ä‘á»ƒ chá»©a tÃ¡c vá»¥ má»›i trong khi giá»¯ láº¡i kiáº¿n thá»©c Ä‘Ã£ cÃ³ trÆ°á»›c Ä‘Ã³. MÃ´ hÃ¬nh vÆ°á»£t trá»™i hÆ¡n BERT. XLNet Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Yang vÃ  cá»™ng sá»± [132] sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh autoregression trÃ¡i ngÆ°á»£c vá»›i mÃ´ hÃ¬nh autoencoder vÃ  vÆ°á»£t trá»™i hÆ¡n BERT vÃ  ERNIE 2.0. Má»™t sá»‘ biáº¿n thá»ƒ cá»§a cÃ¡c mÃ´ hÃ¬nh BERT Ä‘Æ°á»£c Ä‘á» xuáº¥t dá»±a trÃªn corpus Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  báº±ng cÃ¡ch tá»‘i Æ°u hÃ³a cÃ¡c tÃ i nguyÃªn tÃ­nh toÃ¡n. Lan vÃ  cá»™ng sá»± [50] Ä‘á» xuáº¥t ALBERT, vá»›i hai ká»¹ thuáº­t Ä‘á»ƒ giáº£m Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a BERT cá»¥ thá»ƒ lÃ  'factorized embedding parameterization' vÃ  'cross-layer parameter sharing'. ALBERT vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ ba mÃ´ hÃ¬nh trÃªn. CÃ¡c biáº¿n thá»ƒ khÃ¡c cá»§a cÃ¡c mÃ´ hÃ¬nh BERT sá»­ dá»¥ng transformers bao gá»“m TinyBERT [46], RoBERTa [65,109], vÃ  má»™t biáº¿n thá»ƒ specific-domain Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t corpus khoa há»c vá»›i táº­p trung vÃ o lÄ©nh vá»±c BioMedical lÃ  SciBERT [15]. Raffel vÃ  cá»™ng sá»± [103] Ä‘á» xuáº¥t má»™t mÃ´ hÃ¬nh transformer vá»›i má»™t corpus Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a rÃµ rÃ ng gá»i lÃ  'Colossal Clean Crawled Corpus' hoáº·c C4 Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cÃ³ tÃªn T5-11B. KhÃ´ng giá»‘ng nhÆ° BERT, há» Ã¡p dá»¥ng má»™t 'text-to-text framework' trong Ä‘Ã³ input sequence Ä‘Æ°á»£c Ä‘Ã­nh kÃ¨m vá»›i má»™t token Ä‘á»ƒ xÃ¡c Ä‘á»‹nh tÃ¡c vá»¥ NLP cáº§n thá»±c hiá»‡n do Ä‘Ã³ loáº¡i bá» hai giai Ä‘oáº¡n pre-training vÃ  fine-tuning. Há» Ä‘á» xuáº¥t nÄƒm phiÃªn báº£n khÃ¡c nhau cá»§a mÃ´ hÃ¬nh cá»§a há» dá»±a trÃªn sá»‘ lÆ°á»£ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n mÃ  má»—i mÃ´ hÃ¬nh cÃ³ cá»¥ thá»ƒ lÃ  1) T5-Small 2) T5-Base 3) T5-Large 4) T5-3B vÃ  5)T511B vÃ  chÃºng cÃ³ 60 triá»‡u, 220 triá»‡u, 770 triá»‡u, 3 tá»·, vÃ  11 tá»· tham sá»‘ tÆ°Æ¡ng á»©ng. MÃ´ hÃ¬nh nÃ y vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn transformer khÃ¡c vÃ  Ä‘áº¡t káº¿t quáº£ tiÃªn tiáº¿n nháº¥t. Káº¿t quáº£ tá»« nghiÃªn cá»©u cá»§a há», há» xÃ¡c nháº­n ráº±ng hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh tÄƒng lÃªn vá»›i dá»¯ liá»‡u vÃ  sá»©c máº¡nh tÃ­nh toÃ¡n tÄƒng lÃªn vÃ  hiá»‡u suáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n thÃªm náº¿u cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n Ä‘Æ°á»£c xÃ¢y dá»±ng vÃ  Ä‘iá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ  Ä‘á»ƒ tÃ¡i táº¡o mÃ´ hÃ¬nh tá»‘t nháº¥t cá»§a há» cáº§n nÄƒm GPUs trong sá»‘ cÃ¡c tÃ i nguyÃªn khÃ¡c. Má»™t biÃªn soáº¡n cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn transformer khÃ¡c nhau vÃ  há»‡ sá»‘ tÆ°Æ¡ng quan Pearson cá»§a chÃºng trÃªn táº­p dá»¯ liá»‡u STS-B Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y trong Báº£ng 3.

TÃªn MÃ´ HÃ¬nh | TiÃªu Äá» | NÄƒm | Há»‡ Sá»‘ TÆ°Æ¡ng Quan Pearson
T5-11B | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | 2019 | 0.925
XLNet | XLNet: Generalized Autoregressive Pretraining for Language Understanding | 2019 | 0.925
ALBERT | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | 2019 | 0.925
RoBERTa | RoBERTa: A Robustly Optimized BERT Pretraining Approach | 2019 | 0.922
ERNIE 2.0 | ERNIE 2.0: A Continual Pre-training Framework for Language Understanding | 2019 | 0.912
DistilBERT | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter | 2019 | 0.907
TinyBERT | TinyBERT: Distilling BERT for Natural Language Understanding | 2019 | 0.799

Báº£ng 3. Há»‡ sá»‘ tÆ°Æ¡ng quan Pearson cá»§a cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn transformer khÃ¡c nhau trÃªn táº­p dá»¯ liá»‡u benchmark STS.

CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn máº¡ng neural sÃ¢u vÆ°á»£t trá»™i hÆ¡n háº§u háº¿t cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng vÃ  sá»± thÃ nh cÃ´ng gáº§n Ä‘Ã¢y cá»§a cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn transformer Ä‘Ã£ Ä‘Ã³ng vai trÃ² nhÆ° má»™t bÆ°á»›c Ä‘á»™t phÃ¡ trong nghiÃªn cá»©u Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. Tuy nhiÃªn, viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh deep learning Ä‘Ã²i há»i tÃ i nguyÃªn tÃ­nh toÃ¡n lá»›n, máº·c dÃ¹ cÃ¡c biáº¿n thá»ƒ cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ giáº£m thiá»ƒu tÃ i nguyÃªn tÃ­nh toÃ¡n Ä‘ang Ä‘Æ°á»£c Ä‘á» xuáº¥t, chÃºng ta tháº¥y ráº±ng hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh cÅ©ng bá»‹ áº£nh hÆ°á»Ÿng, vÃ­ dá»¥ nhÆ° TinyBERT [46]. VÃ  hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ báº±ng viá»‡c sá»­ dá»¥ng corpus lá»›n hÆ¡n, Ä‘iá»u nÃ y láº¡i Ä‘áº·t ra thÃ¡ch thá»©c xÃ¢y dá»±ng má»™t corpus lÃ½ tÆ°á»Ÿng. Háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh deep learning lÃ  cÃ¡c mÃ´ hÃ¬nh "há»™p Ä‘en" vÃ  khÃ³ cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘áº·c trÆ°ng mÃ  hiá»‡u suáº¥t Ä‘Æ°á»£c Ä‘áº¡t Ä‘Æ°á»£c dá»±a trÃªn Ä‘Ã³, do Ä‘Ã³ trá»Ÿ nÃªn khÃ³ giáº£i thÃ­ch khÃ´ng giá»‘ng nhÆ° trÆ°á»ng há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus cÃ³ ná»n táº£ng toÃ¡n há»c máº¡nh máº½. CÃ¡c lÄ©nh vá»±c khÃ¡c nhau nhÆ° tÃ i chÃ­nh, báº£o hiá»ƒm, v.v., xá»­ lÃ½ dá»¯ liá»‡u nháº¡y cáº£m cÃ³ thá»ƒ miá»…n cÆ°á»¡ng triá»ƒn khai cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn máº¡ng neural sÃ¢u do thiáº¿u kháº£ nÄƒng giáº£i thÃ­ch.

6 CÃC PHÆ¯Æ NG PHÃP LAI

Dá»±a trÃªn táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã£ tháº£o luáº­n trÆ°á»›c Ä‘Ã³, chÃºng ta tháº¥y ráº±ng má»—i phÆ°Æ¡ng phÃ¡p Ä‘á»u cÃ³ Æ°u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c khai thÃ¡c cÃ¡c ontology cÆ¡ báº£n Ä‘á»ƒ phÃ¢n biá»‡t tá»« Ä‘á»“ng nghÄ©a, trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus thÃ¬ linh hoáº¡t vÃ¬ chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng qua cÃ¡c ngÃ´n ngá»¯. CÃ¡c há»‡ thá»‘ng dá»±a trÃªn máº¡ng neural sÃ¢u, máº·c dÃ¹ tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n, cung cáº¥p káº¿t quáº£ tá»‘t hÆ¡n. Tuy nhiÃªn, nhiá»u nhÃ  nghiÃªn cá»©u Ä‘Ã£ tÃ¬m ra cÃ¡ch khai thÃ¡c nhá»¯ng Ä‘iá»ƒm tá»‘t nháº¥t cá»§a má»—i phÆ°Æ¡ng phÃ¡p vÃ  xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh lai Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. Trong pháº§n nÃ y, chÃºng tÃ´i mÃ´ táº£ cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng trong má»™t sá»‘ mÃ´ hÃ¬nh lai Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i.

6.1 CÃ¡c loáº¡i phÆ°Æ¡ng phÃ¡p Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a lai:

â€¢Novel Approach to a Semantically-Aware Representation of Items (NASARI) [21]: Camacho Collados vÃ  cá»™ng sá»± [21] Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p NASARI trong Ä‘Ã³ nguá»“n kiáº¿n thá»©c BabelNet Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng má»™t corpus mÃ  dá»±a trÃªn Ä‘Ã³ biá»ƒu diá»…n vector cho cÃ¡c khÃ¡i niá»‡m (tá»« hoáº·c nhÃ³m tá»«) Ä‘Æ°á»£c hÃ¬nh thÃ nh. Ban Ä‘áº§u, cÃ¡c trang Wikipedia liÃªn káº¿t vá»›i má»™t khÃ¡i niá»‡m Ä‘Ã£ cho, trong trÆ°á»ng há»£p nÃ y lÃ  synset cá»§a BabelNet, vÃ  táº¥t cáº£ cÃ¡c liÃªn káº¿t Ä‘i ra tá»« trang Ä‘Ã£ cho Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o thÃ nh má»™t sub-corpus cho khÃ¡i niá»‡m cá»¥ thá»ƒ. Sub-corpus Ä‘Æ°á»£c má»Ÿ rá»™ng thÃªm vá»›i cÃ¡c trang Wikipedia cá»§a hypernyms vÃ  hyponyms cá»§a khÃ¡i niá»‡m trong máº¡ng BabelNet. ToÃ n bá»™ Wikipedia Ä‘Æ°á»£c xem nhÆ° corpus tham chiáº¿u. Hai loáº¡i biá»ƒu diá»…n vector khÃ¡c nhau Ä‘Æ°á»£c Ä‘á» xuáº¥t. Trong phÆ°Æ¡ng phÃ¡p Ä‘áº§u tiÃªn, cÃ¡c vector cÃ³ trá»ng sá»‘ Ä‘Æ°á»£c hÃ¬nh thÃ nh báº±ng cÃ¡ch sá»­ dá»¥ng tÃ­nh Ä‘áº·c thÃ¹ tá»« vá»±ng. TÃ­nh Ä‘áº·c thÃ¹ tá»« vá»±ng lÃ  má»™t phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª xÃ¡c Ä‘á»‹nh cÃ¡c tá»« Ä‘áº¡i diá»‡n nháº¥t cho má»™t vÄƒn báº£n Ä‘Ã£ cho, dá»±a trÃªn phÃ¢n phá»‘i siÃªu hÃ¬nh há»c (láº¥y máº«u khÃ´ng hoÃ n láº¡i). Cho "T vÃ  t" biá»ƒu thá»‹ tá»•ng sá»‘ tá»« ná»™i dung trong corpus tham chiáº¿u RC vÃ  sub-corpus SC tÆ°Æ¡ng á»©ng vÃ  "F vÃ  f" biá»ƒu thá»‹ táº§n suáº¥t cá»§a tá»« Ä‘Ã£ cho trong corpus tham chiáº¿u RC vÃ  sub-corpus SC tÆ°Æ¡ng á»©ng, thÃ¬ tÃ­nh Ä‘áº·c thÃ¹ tá»« vá»±ng cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng phÆ°Æ¡ng trÃ¬nh dÆ°á»›i Ä‘Ã¢y:

spec(T,t,F,f)=âˆ’logâ‚â‚€P(Xâ‰¥f) (13)

X biá»ƒu diá»…n má»™t biáº¿n ngáº«u nhiÃªn tuÃ¢n theo quan há»‡ siÃªu hÃ¬nh há»c vá»›i cÃ¡c tham sá»‘ T, t vÃ  F vÃ  P(Xâ‰¥f) Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ :

P(Xâ‰¥f)=âˆ‘á¶ áµ¢â‚Œf P(X=i) (14)

P(X=i) lÃ  xÃ¡c suáº¥t cá»§a má»™t thuáº­t ngá»¯ Ä‘Ã£ cho xuáº¥t hiá»‡n chÃ­nh xÃ¡c i láº§n trong sub-corpus Ä‘Ã£ cho trong phÃ¢n phá»‘i siÃªu hÃ¬nh há»c vá»›i T, t vÃ  F. PhÆ°Æ¡ng phÃ¡p thá»© hai táº¡o thÃ nh má»™t cá»¥m tá»« trong sub-corpus chia sáº» má»™t hypernym chung trong taxonomy WordNet Ä‘Æ°á»£c nhÃºng trong BabelNet. TÃ­nh Ä‘áº·c thÃ¹ sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘o dá»±a trÃªn táº§n suáº¥t cá»§a hypernym vÃ  táº¥t cáº£ hyponyms cá»§a nÃ³ trong taxonomy, ngay cáº£ nhá»¯ng tá»« khÃ´ng xuáº¥t hiá»‡n trong sub-corpus Ä‘Ã£ cho. Ká»¹ thuáº­t phÃ¢n cá»¥m nÃ y táº¡o thÃ nh má»™t biá»ƒu diá»…n thá»‘ng nháº¥t cá»§a cÃ¡c tá»« báº£o tá»“n cÃ¡c thuá»™c tÃ­nh ngá»¯ nghÄ©a. CÃ¡c giÃ¡ trá»‹ tÃ­nh Ä‘áº·c thÃ¹ Ä‘Æ°á»£c thÃªm lÃ m trá»ng sá»‘ trong cáº£ hai phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ xáº¿p háº¡ng cÃ¡c thuáº­t ngá»¯ trong má»™t vÄƒn báº£n Ä‘Ã£ cho. PhÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n vector Ä‘áº§u tiÃªn Ä‘Æ°á»£c gá»i lÃ  NASARIlexical vÃ  phÆ°Æ¡ng phÃ¡p thá»© hai Ä‘Æ°á»£c gá»i lÃ  NASARIunified. Äá»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¡c vector nÃ y Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch sá»­ dá»¥ng thÆ°á»›c Ä‘o gá»i lÃ  Weighted Overlap [98] nhÆ° sau:

WO(vâ‚,vâ‚‚)=âˆš(âˆ‘dâˆˆO(rank(d,vâ‚)+rank(d,vâ‚‚))â»Â¹/âˆ‘áµ¢â‚Œâ‚|O|(2i)â»Â¹) (15)

trong Ä‘Ã³ O biá»ƒu thá»‹ cÃ¡c thuáº­t ngá»¯ chá»“ng chÃ©o trong má»—i vector vÃ  rank(d,váµ¢) biá»ƒu diá»…n thá»© háº¡ng cá»§a thuáº­t ngá»¯ d trong vector váµ¢.

Camacho Collados vÃ  cá»™ng sá»± [22] Ä‘á» xuáº¥t má»™t pháº§n má»Ÿ rá»™ng cho cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã³ cá»§a há» vÃ  Ä‘á» xuáº¥t biá»ƒu diá»…n vector thá»© ba báº±ng cÃ¡ch Ã¡nh xáº¡ vector tá»« vá»±ng vÃ o khÃ´ng gian ngá»¯ nghÄ©a cá»§a word embeddings Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c ká»¹ thuáº­t word embedding phá»©c táº¡p nhÆ° word2vec. Biá»ƒu diá»…n nÃ y Ä‘Æ°á»£c gá»i lÃ  NASARIembedded. Äá»™ tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ä‘o nhÆ° Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a cÃ¡c vector nÃ y. Cáº£ ba phÆ°Æ¡ng phÃ¡p Ä‘á»u Ä‘Æ°á»£c thá»­ nghiá»‡m trÃªn cÃ¡c táº­p dá»¯ liá»‡u chuáº©n vÃ ng M&C, WS-Sim vÃ  SimLex-999. NASARIlexical Ä‘áº¡t há»‡ sá»‘ tÆ°Æ¡ng quan Pearson vÃ  Spearman cao hÆ¡n trung bÃ¬nh trÃªn ba táº­p dá»¯ liá»‡u so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhÆ° ESA, word2vec, vÃ  lin.

â€¢Most Suitable Sense Annotation (MSSA) [106]: Ruas vÃ  cá»™ng sá»± Ä‘á» xuáº¥t ba phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ táº¡o thÃ nh word-sense embeddings. Cho má»™t corpus, bÆ°á»›c phÃ¢n biá»‡t nghÄ©a tá»« Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng má»™t trong ba phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t: Most Suitable Sense Annotation (MSSA), Most Suitable Sense Annotation N Refined (MSSA-NR), vÃ  Most Suitable Sense Annotation Dijkstra (MSSA-D). Cho má»™t corpus, má»—i tá»« trong corpus Ä‘Æ°á»£c liÃªn káº¿t vá»›i má»™t synset trong ontology WordNet vÃ  "gloss-average-vector" Ä‘Æ°á»£c tÃ­nh cho má»—i synset. Gloss-average-vector Ä‘Æ°á»£c táº¡o thÃ nh báº±ng cÃ¡ch sá»­ dá»¥ng biá»ƒu diá»…n vector cá»§a cÃ¡c tá»« trong gloss cá»§a má»—i synset. MSSA tÃ­nh gloss-average-vector báº±ng cÃ¡ch sá»­ dá»¥ng má»™t cá»­a sá»• nhá» cÃ¡c tá»« vÃ  tráº£ vá» synset cá»§a tá»« cÃ³ giÃ¡ trá»‹ gloss-average-vector cao nháº¥t. MSSA-D, tuy nhiÃªn, xem xÃ©t toÃ n bá»™ tÃ i liá»‡u tá»« tá»« Ä‘áº§u tiÃªn Ä‘áº¿n tá»« cuá»‘i cÃ¹ng vÃ  sau Ä‘Ã³ xÃ¡c Ä‘á»‹nh synset liÃªn káº¿t. Hai há»‡ thá»‘ng nÃ y sá»­ dá»¥ng Google News vectors Ä‘á»ƒ táº¡o thÃ nh synset-embeddings. MSSA-NR lÃ  má»™t mÃ´ hÃ¬nh láº·p, trong Ä‘Ã³ láº§n cháº¡y Ä‘áº§u tiÃªn táº¡o ra synset-embeddings, Ä‘Æ°á»£c Ä‘Æ°a trá»Ÿ láº¡i trong láº§n cháº¡y thá»© hai nhÆ° má»™t sá»± thay tháº¿ cho gloss-average-vectors Ä‘á»ƒ táº¡o ra synset-embeddings tinh chá»‰nh hÆ¡n. Nhá»¯ng synset-embeddings nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o mÃ´ hÃ¬nh word2vec CBOW Ä‘á»ƒ táº¡o ra multi-sense word embeddings Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. Sá»± káº¿t há»£p cá»§a cÃ¡c biáº¿n thá»ƒ MSSA vÃ  word2vec táº¡o ra káº¿t quáº£ vá»¯ng cháº¯c trong cÃ¡c táº­p dá»¯ liá»‡u chuáº©n vÃ ng nhÆ° R&G, M&C, WS353-Sim, vÃ  SimLex-999 [106].

â€¢Unsupervised Ensemble Semantic Textual Similarity Methods (UESTS) [38]: Hassan vÃ  cá»™ng sá»± Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a ensemble dá»±a trÃªn má»™t word-aligner khÃ´ng giÃ¡m sÃ¡t cÆ¡ báº£n. MÃ´ hÃ¬nh tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a nhÆ° tá»•ng cÃ³ trá»ng sá»‘ cá»§a bá»‘n thÆ°á»›c Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a khÃ¡c nhau giá»¯a cÃ¡c cÃ¢u Sâ‚ vÃ  Sâ‚‚ báº±ng phÆ°Æ¡ng trÃ¬nh dÆ°á»›i Ä‘Ã¢y:

simUSETS(Sâ‚,Sâ‚‚)=Î±âˆ—simWAL(Sâ‚,Sâ‚‚)+Î²âˆ—simSC(Sâ‚,Sâ‚‚)+Î³âˆ—simembed(Sâ‚,Sâ‚‚)+Î¸âˆ—simED(Sâ‚,Sâ‚‚) (16)

simWAL(Sâ‚,Sâ‚‚) tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»± báº±ng word aligner dá»±a trÃªn synset. Äá»™ tÆ°Æ¡ng tá»± giá»¯a vÄƒn báº£n Ä‘Æ°á»£c Ä‘o dá»±a trÃªn sá»‘ lÆ°á»£ng lÃ¡ng giá»ng chung mÃ  má»—i thuáº­t ngá»¯ cÃ³ trong taxonomy BabelNet. simSC(Sâ‚,Sâ‚‚) Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± báº±ng thÆ°á»›c Ä‘o soft cardinality giá»¯a cÃ¡c thuáº­t ngá»¯ Ä‘Æ°á»£c so sÃ¡nh. HÃ m soft cardinality xá»­ lÃ½ má»—i tá»« nhÆ° má»™t táº­p há»£p vÃ  Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a chÃºng nhÆ° giao Ä‘iá»ƒm giá»¯a cÃ¡c táº­p há»£p. simembed(Sâ‚,Sâ‚‚) táº¡o thÃ nh biá»ƒu diá»…n word vector báº±ng word embeddings Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Baroni vÃ  cá»™ng sá»± [14]. Sau Ä‘Ã³ Ä‘á»™ tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ä‘o nhÆ° giÃ¡ trá»‹ cosine giá»¯a hai vector. simED(Sâ‚,Sâ‚‚) lÃ  thÆ°á»›c Ä‘o khÃ¡c biá»‡t giá»¯a hai cÃ¢u Ä‘Ã£ cho. Khoáº£ng cÃ¡ch chá»‰nh sá»­a Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  sá»‘ lÆ°á»£ng tá»‘i thiá»ƒu cÃ¡c chá»‰nh sá»­a cáº§n thiáº¿t Ä‘á»ƒ chuyá»ƒn Ä‘á»•i má»™t cÃ¢u thÃ nh cÃ¢u khÃ¡c. CÃ¡c chá»‰nh sá»­a cÃ³ thá»ƒ bao gá»“m chÃ¨n, xÃ³a, hoáº·c thay tháº¿. simED(Sâ‚,Sâ‚‚) sá»­ dá»¥ng khoáº£ng cÃ¡ch chá»‰nh sá»­a word-sense trong Ä‘Ã³ word-senses Ä‘Æ°á»£c xem xÃ©t thay vÃ¬ cÃ¡c tá»« thá»±c táº¿. CÃ¡c siÃªu tham sá»‘ Î±, Î², Î³, vÃ  Î¸ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘áº¿n cÃ¡c giÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 0.5 cho cÃ¡c táº­p dá»¯ liá»‡u benchmark STS khÃ¡c nhau. MÃ´ hÃ¬nh ensemble vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh khÃ´ng giÃ¡m sÃ¡t benchmark STS trong loáº¡t SemEval 2017 trÃªn nhiá»u táº­p dá»¯ liá»‡u benchmark STS.

CÃ¡c phÆ°Æ¡ng phÃ¡p lai khai thÃ¡c cáº£ hiá»‡u quáº£ cáº¥u trÃºc Ä‘Æ°á»£c cung cáº¥p bá»Ÿi cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c vÃ  tÃ­nh linh hoáº¡t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus. Nhiá»u nghiÃªn cá»©u Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh Ä‘á»ƒ xÃ¢y dá»±ng multi-sense embeddings nháº±m káº¿t há»£p nghÄ©a thá»±c táº¿ cá»§a cÃ¡c tá»« vÃ o word vectors. Iacobacci vÃ  cá»™ng sá»± táº¡o thÃ nh word embeddings gá»i lÃ  "Sensembed" báº±ng cÃ¡ch sá»­ dá»¥ng BabelNet Ä‘á»ƒ táº¡o thÃ nh má»™t corpus cÃ³ chÃº thÃ­ch nghÄ©a vÃ  sau Ä‘Ã³ sá»­ dá»¥ng word2vec Ä‘á»ƒ xÃ¢y dá»±ng word vectors do Ä‘Ã³ cÃ³ cÃ¡c vector khÃ¡c nhau cho cÃ¡c nghÄ©a khÃ¡c nhau cá»§a tá»«. NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, cÃ¡c mÃ´ hÃ¬nh lai bÃ¹ Ä‘áº¯p cho nhá»¯ng thiáº¿u sÃ³t cá»§a má»™t phÆ°Æ¡ng phÃ¡p báº±ng cÃ¡ch káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c. Do Ä‘Ã³ hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p lai tÆ°Æ¡ng Ä‘á»‘i cao. 5 vá»‹ trÃ­ Ä‘áº§u tiÃªn cá»§a cÃ¡c tÃ¡c vá»¥ Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a SemEval 2017 Ä‘Æ°á»£c trao cho cÃ¡c mÃ´ hÃ¬nh ensemble, Ä‘iá»u nÃ y cho tháº¥y rÃµ rÃ ng sá»± chuyá»ƒn Ä‘á»•i trong nghiÃªn cá»©u hÆ°á»›ng tá»›i cÃ¡c mÃ´ hÃ¬nh lai [24].

7 PHÃ‚N TÃCH KHáº¢O SÃT

Pháº§n nÃ y tháº£o luáº­n vá» phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng bÃ i kháº£o sÃ¡t nÃ y vÃ  cung cáº¥p tá»•ng quan vá» cÃ¡c bÃ i bÃ¡o nghiÃªn cá»©u khÃ¡c nhau Ä‘Æ°á»£c xem xÃ©t.

7.1 Chiáº¿n lÆ°á»£c tÃ¬m kiáº¿m:

CÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c xem xÃ©t cho kháº£o sÃ¡t nÃ y Ä‘Æ°á»£c thu tháº­p báº±ng cÃ´ng cá»¥ tÃ¬m kiáº¿m Google Scholar vÃ  cÃ¡c tá»« khÃ³a Ä‘Æ°á»£c sá»­ dá»¥ng bao gá»“m "semantic similarity, word embedding, knowledge-based methods, corpus-based methods, deep neural network-based semantic similarity, LSTM, text processing, vÃ  semantic similarity datasets." Káº¿t quáº£ tÃ¬m kiáº¿m Ä‘Æ°á»£c tinh chá»‰nh báº±ng cÃ¡c tham sá»‘ khÃ¡c nhau nhÆ° Xáº¿p háº¡ng Táº¡p chÃ­, Chá»‰ sá»‘ Google Scholar, sá»‘ lÆ°á»£ng trÃ­ch dáº«n, nÄƒm xuáº¥t báº£n, v.v. Chá»‰ cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c xuáº¥t báº£n trong cÃ¡c táº¡p chÃ­ cÃ³ xáº¿p háº¡ng Scimago Journal á»Ÿ Quartile 1 vÃ  cÃ¡c há»™i nghá»‹ cÃ³ chá»‰ sá»‘ H-index cá»§a Google metrics trÃªn 50 má»›i Ä‘Æ°á»£c xem xÃ©t. Ngoáº¡i lá»‡ Ä‘Æ°á»£c thá»±c hiá»‡n cho má»™t sá»‘ bÃ i bÃ¡o cÃ³ tÃ¡c Ä‘á»™ng vÃ  liÃªn quan cao hÆ¡n. Báº£ng tham kháº£o Ä‘Æ°á»£c sáº¯p xáº¿p theo nÄƒm xuáº¥t báº£n Ä‘Æ°á»£c bao gá»“m trong Phá»¥ lá»¥c B dÆ°á»›i dáº¡ng Báº£ng 5. Báº£ng ghi láº¡i 1) TiÃªu Ä‘á», 2) NÄƒm xuáº¥t báº£n, 3) TÃªn tÃ¡c giáº£, 4) Äá»‹a Ä‘iá»ƒm, 5) Quartile SJR (cho táº¡p chÃ­), 6) H-Index, vÃ  7) Sá»‘ lÆ°á»£ng trÃ­ch dáº«n (tÃ­nh Ä‘áº¿n 02.04.2020). Má»™t sá»‘ káº¿t quáº£ thá»‘ng kÃª cá»§a cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c chá»n Ä‘Æ°á»£c hiá»ƒn thá»‹ trong cÃ¡c hÃ¬nh dÆ°á»›i Ä‘Ã¢y. Nhá»¯ng hÃ¬nh nÃ y lÃ m ná»•i báº­t cháº¥t lÆ°á»£ng cá»§a cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c chá»n, tá»« Ä‘Ã³ lÃ m ná»•i báº­t cháº¥t lÆ°á»£ng cá»§a kháº£o sÃ¡t. HÃ¬nh 2 hiá»ƒn thá»‹ phÃ¢n phá»‘i cá»§a cÃ¡c bÃ i bÃ¡o tham kháº£o trÃªn cÃ¡c há»™i nghá»‹, táº¡p chÃ­, vÃ  khÃ¡c. 55% bÃ i bÃ¡o tá»« há»™i nghá»‹ vÃ  38% bÃ i bÃ¡o tá»« táº¡p chÃ­. 7% cÃ²n láº¡i tá»« arXiv vÃ  sÃ¡ch. Tuy nhiÃªn, chÃºng cÃ³ tÃ¡c Ä‘á»™ng khÃ¡ cao liÃªn quan Ä‘áº¿n chá»§ Ä‘á» cá»§a kháº£o sÃ¡t. HÃ¬nh 3 lÃ m ná»•i báº­t phÃ¢n phá»‘i cá»§a cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c chá»n theo nÄƒm. Gáº§n 72% bÃ i bÃ¡o Ä‘Æ°á»£c chá»n lÃ  cÃ¡c cÃ´ng trÃ¬nh Ä‘Æ°á»£c thá»±c hiá»‡n sau nÄƒm 2010, 28% cÃ²n láº¡i Ä‘áº¡i diá»‡n cho cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng Ä‘Æ°á»£c Ã¡p dá»¥ng trong giai Ä‘oáº¡n Ä‘áº§u cá»§a sá»± phÃ¡t triá»ƒn Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. HÃ¬nh 4 biá»ƒu diá»…n pháº¡m vi trÃ­ch dáº«n cá»§a cÃ¡c bÃ i bÃ¡o. 54% bÃ i bÃ¡o cÃ³ 50 Ä‘áº¿n 500 trÃ­ch dáº«n, 26% cÃ³ 1.000-5.000 trÃ­ch dáº«n, vÃ  12% bÃ i bÃ¡o cÃ³ hÆ¡n 5000 trÃ­ch dáº«n. ChÃºng ta tháº¥y ráº±ng 33% bÃ i bÃ¡o cÃ³ trÃ­ch dáº«n dÆ°á»›i 50 tuy nhiÃªn, táº¥t cáº£ nhá»¯ng bÃ i bÃ¡o nÃ y Ä‘Æ°á»£c xuáº¥t báº£n sau nÄƒm 2017 Ä‘iá»u nÃ y giáº£i thÃ­ch cho sá»‘ trÃ­ch dáº«n Ã­t hÆ¡n.

HÃ¬nh 2. PhÃ¢n phá»‘i bÃ i bÃ¡o theo Ä‘á»‹a Ä‘iá»ƒm.
HÃ¬nh 3. PhÃ¢n phá»‘i bÃ i bÃ¡o theo nÄƒm.

7.2 Táº¡o word-cloud:

ChÃºng tÃ´i triá»ƒn khai má»™t Ä‘oáº¡n mÃ£ python Ä‘Æ¡n giáº£n Ä‘á»ƒ táº¡o word cloud báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c tÃ³m táº¯t tá»« táº¥t cáº£ cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c sá»­ dá»¥ng trong kháº£o sÃ¡t nÃ y. CÃ¡c tÃ³m táº¯t tá»« táº¥t cáº£ 118 bÃ i bÃ¡o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng má»™t táº­p dá»¯ liá»‡u sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng trong mÃ£ python. CÃ¡c tÃ³m táº¯t Ä‘Æ°á»£c trÃ­ch xuáº¥t ban Ä‘áº§u Ä‘Æ°á»£c tiá»n xá»­ lÃ½ báº±ng cÃ¡ch chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh chá»¯ thÆ°á»ng, loáº¡i bá» dáº¥u cÃ¢u, vÃ  loáº¡i bá» cÃ¡c tá»« dá»«ng tiáº¿ng Anh thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng nháº¥t cÃ³ sáºµn trong thÆ° viá»‡n nltk. Sau Ä‘Ã³ word-cloud Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng thÆ° viá»‡n python wordcloud. Word cloud Ä‘Æ°á»£c xÃ¢y dá»±ng nhÆ° váº­y Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 5. Tá»« word cloud, chÃºng tÃ´i suy ra ráº±ng máº·c dÃ¹ cÃ¡c tá»« khÃ³a khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng trong tÃ¬m kiáº¿m bÃ i bÃ¡o cá»§a chÃºng tÃ´i, trá»ng tÃ¢m chung cá»§a cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c chá»n lÃ  Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a. Trong word cloud, kÃ­ch thÆ°á»›c cá»§a cÃ¡c tá»« tá»· lá»‡ thuáº­n vá»›i táº§n suáº¥t sá»­ dá»¥ng cá»§a nhá»¯ng tá»« nÃ y. Tá»« "word" lá»›n hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i tá»« "sentence" cho tháº¥y ráº±ng háº§u háº¿t cÃ¡c cÃ´ng trÃ¬nh nghiÃªn cá»©u táº­p trung vÃ o Ä‘á»™ tÆ°Æ¡ng tá»± tá»«-Ä‘áº¿n-tá»« hÆ¡n lÃ  Ä‘á»™ tÆ°Æ¡ng tá»± cÃ¢u-Ä‘áº¿n-cÃ¢u. ChÃºng ta cÅ©ng cÃ³ thá»ƒ suy ra ráº±ng cÃ¡c tá»« "vector" vÃ  "representation" Ä‘Æ°á»£c sá»­ dá»¥ng thÆ°á»ng xuyÃªn hÆ¡n cÃ¡c tá»« "information", "context", vÃ  "concept" cho tháº¥y áº£nh hÆ°á»Ÿng cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c. Vá»›i word cloud Ä‘Ã£ cho, chÃºng tÃ´i thá»ƒ hiá»‡n trá»ng tÃ¢m cá»§a kháº£o sÃ¡t má»™t cÃ¡ch Ä‘á»“ há»a.

HÃ¬nh 4. PhÃ¢n phá»‘i pháº¡m vi trÃ­ch dáº«n trÃªn cÃ¡c bÃ i bÃ¡o.
HÃ¬nh 5. Word cloud biá»ƒu diá»…n táº­p há»£p cÃ¡c tá»« tá»« tÃ³m táº¯t cá»§a cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c sá»­ dá»¥ng trong kháº£o sÃ¡t.

8 Káº¾T LUáº¬N

Äo lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a hai Ä‘oáº¡n vÄƒn báº£n Ä‘Ã£ lÃ  má»™t trong nhá»¯ng tÃ¡c vá»¥ thÃ¡ch thá»©c nháº¥t trong lÄ©nh vá»±c Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn. Nhiá»u phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t qua cÃ¡c nÄƒm Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a vÃ  kháº£o sÃ¡t nÃ y tháº£o luáº­n vá» sá»± phÃ¡t triá»ƒn, Æ°u Ä‘iá»ƒm, vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kiáº¿n thá»©c xem xÃ©t nghÄ©a thá»±c táº¿ cá»§a vÄƒn báº£n tuy nhiÃªn, chÃºng khÃ´ng thÃ­ch á»©ng Ä‘Æ°á»£c qua cÃ¡c lÄ©nh vá»±c vÃ  ngÃ´n ngá»¯ khÃ¡c nhau. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn corpus cÃ³ ná»n táº£ng thá»‘ng kÃª vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai qua cÃ¡c ngÃ´n ngá»¯ nhÆ°ng chÃºng khÃ´ng xem xÃ©t nghÄ©a thá»±c táº¿ cá»§a vÄƒn báº£n. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn máº¡ng neural sÃ¢u cho tháº¥y hiá»‡u suáº¥t tá»‘t hÆ¡n, nhÆ°ng chÃºng Ä‘Ã²i há»i tÃ i nguyÃªn tÃ­nh toÃ¡n cao vÃ  thiáº¿u kháº£ nÄƒng giáº£i thÃ­ch. CÃ¡c phÆ°Æ¡ng phÃ¡p lai Ä‘Æ°á»£c hÃ¬nh thÃ nh Ä‘á»ƒ táº­n dá»¥ng lá»£i Ã­ch tá»« cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau bÃ¹ Ä‘áº¯p cho nhá»¯ng thiáº¿u sÃ³t cá»§a nhau. RÃµ rÃ ng tá»« kháº£o sÃ¡t ráº±ng má»—i phÆ°Æ¡ng phÃ¡p Ä‘á»u cÃ³ Æ°u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a nÃ³ vÃ  khÃ³ cÃ³ thá»ƒ chá»n má»™t mÃ´ hÃ¬nh tá»‘t nháº¥t, tuy nhiÃªn, háº§u háº¿t cÃ¡c phÆ°Æ¡ng phÃ¡p lai gáº§n Ä‘Ã¢y Ä‘Ã£ cho tháº¥y káº¿t quáº£ há»©a háº¹n so vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘á»™c láº­p khÃ¡c. Trong khi trá»ng tÃ¢m cá»§a nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Æ°á»£c chuyá»ƒn hÆ°á»›ng tá»›i viá»‡c xÃ¢y dá»±ng word embeddings cÃ³ nháº­n thá»©c ngá»¯ nghÄ©a hÆ¡n, vÃ  cÃ¡c mÃ´ hÃ¬nh transformer Ä‘Ã£ cho tháº¥y káº¿t quáº£ há»©a háº¹n, nhu cáº§u xÃ¡c Ä‘á»‹nh sá»± cÃ¢n báº±ng giá»¯a hiá»‡u quáº£ tÃ­nh toÃ¡n vÃ  hiá»‡u suáº¥t váº«n Ä‘ang Ä‘Æ°á»£c nghiÃªn cá»©u. Khoáº£ng trá»‘ng nghiÃªn cá»©u cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c tháº¥y trong cÃ¡c lÄ©nh vá»±c nhÆ° xÃ¢y dá»±ng word embeddings cá»¥ thá»ƒ cho lÄ©nh vá»±c, giáº£i quyáº¿t nhu cáº§u vá» má»™t corpus lÃ½ tÆ°á»Ÿng. Kháº£o sÃ¡t nÃ y sáº½ phá»¥c vá»¥ nhÆ° má»™t ná»n táº£ng tá»‘t cho cÃ¡c nhÃ  nghiÃªn cá»©u cÃ³ Ã½ Ä‘á»‹nh tÃ¬m cÃ¡c phÆ°Æ¡ng phÃ¡p má»›i Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘á»™ tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a.

Lá»œI Cáº¢M Æ N

CÃ¡c tÃ¡c giáº£ muá»‘n gá»­i lá»i cáº£m Æ¡n Ä‘áº¿n nhÃ³m nghiÃªn cá»©u táº¡i DaTALab thuá»™c Äáº¡i há»c Lakehead vÃ¬ sá»± há»— trá»£ cá»§a há», Ä‘áº·c biá»‡t lÃ  Abhijit Rao, Mohiuddin Qudar, Punardeep Sikka, vÃ  Andrew Heppner vÃ¬ pháº£n há»“i vÃ  chá»‰nh sá»­a cá»§a há» cho áº¥n pháº©m nÃ y. ChÃºng tÃ´i cÅ©ng muá»‘n cáº£m Æ¡n Äáº¡i há»c Lakehead, CASES, vÃ  Há»™i Ä‘á»“ng Ontario vá» PhÃ¡t Ã¢m vÃ  Chuyá»ƒn giao (ONCAT), náº¿u khÃ´ng cÃ³ sá»± há»— trá»£ cá»§a há», nghiÃªn cá»©u nÃ y sáº½ khÃ´ng thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c.

TÃ€I LIá»†U THAM KHáº¢O

PHá»¤ Lá»¤C A CÃC THÆ¯á»šC ÄO KHOáº¢NG CÃCH NGá»® NGHÄ¨A VÃ€ CÃ”NG THá»¨C Cá»¦A CHÃšNG

STT | ThÆ°á»›c Ä‘o khoáº£ng cÃ¡ch ngá»¯ nghÄ©a | CÃ´ng thá»©c
1 | Î±-skew divergence (ASD) | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) P(w|wâ‚)log P(w|wâ‚)/(Î±P(w|wâ‚‚)+(1-Î±)P(w|wâ‚))
2 | Äá»™ tÆ°Æ¡ng tá»± Cosine | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) P(w|wâ‚)Ã—P(w|wâ‚‚)/âˆš(âˆ‘wâˆˆC(wâ‚) P(w|wâ‚)Â²)Ã—âˆš(âˆ‘wâˆˆC(wâ‚‚) P(w|wâ‚‚)Â²)
3 | Co-occurrence Retrieval Models (CRM) | Î³Â²Ã—PÃ—R/(P+R) + (1-Î³)/(Î²[P]+(1-Î²)[R])
4 | Há»‡ sá»‘ Dice | 2Ã—âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) min(P(w|wâ‚),P(w|wâ‚‚))/(âˆ‘wâˆˆC(wâ‚) P(w|wâ‚)+âˆ‘wâˆˆC(wâ‚‚) P(w|wâ‚‚))
5 | Khoáº£ng cÃ¡ch Manhattan hoáº·c chuáº©n L1 | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) |P(w|wâ‚)-P(w|wâ‚‚)|
6 | ThÆ°á»›c Ä‘o chia | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) log P(w|wâ‚)/P(w|wâ‚‚)

Báº£ng 4 tiáº¿p tá»¥c tá»« trang trÆ°á»›c
STT | ThÆ°á»›c Ä‘o khoáº£ng cÃ¡ch ngá»¯ nghÄ©a | CÃ´ng thá»©c
7 | Hindle | âˆ‘wâˆˆC(w) min(I(w,wâ‚),I(w,wâ‚‚)), náº¿u cáº£ I(w,wâ‚) vÃ  I(w,wâ‚‚)>0; |max(I(w,wâ‚),I(w,wâ‚‚))|, náº¿u cáº£ I(w,wâ‚) vÃ  I(w,wâ‚‚)<0; 0, náº¿u ngÆ°á»£c láº¡i
8 | Jaccard | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) min(P(w|wâ‚),P(w|wâ‚‚))/âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) max(P(w|wâ‚),P(w|wâ‚‚))
9 | Jensen-Shannon divergence (JSD) | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) P(w|wâ‚)log P(w|wâ‚)/(Â½(P(w|wâ‚)+P(w|wâ‚‚))) + P(w|wâ‚‚)log P(w|wâ‚‚)/(Â½(P(w|wâ‚)+P(w|wâ‚‚)))
10 | Kullback-Leibler divergence - common occurrence | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) P(w|wâ‚)log P(w|wâ‚)/P(w|wâ‚‚)
11 | Kullback-Leibler divergence - absolute | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) P(w|wâ‚)log P(w|wâ‚)/P(w|wâ‚‚)
12 | Kullback-Leibler divergence - average | Â½âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) (P(w|wâ‚)-P(w|wâ‚‚))log P(w|wâ‚)/P(w|wâ‚‚)
13 | Kullback-Leibler divergence - maximum | max(KLD(wâ‚,wâ‚‚),KLD(wâ‚‚,wâ‚))
14 | Khoáº£ng cÃ¡ch Euclidean hoáº·c chuáº©n L2 | âˆš(âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) (P(w|wâ‚)-P(w|wâ‚‚))Â²)
15 | Lin | âˆ‘(r,w)âˆˆT(wâ‚)âˆ©T(wâ‚‚) (I(wâ‚,r,w)+I(wâ‚‚,r,w))/(âˆ‘(r,w')âˆˆT(wâ‚) I(wâ‚,r,w')+âˆ‘(r,w'')âˆˆT(wâ‚‚) I(wâ‚‚,r,w''))
16 | ThÆ°á»›c Ä‘o tÃ­ch | âˆ‘wâˆˆC(wâ‚)âˆªC(wâ‚‚) P(w|wâ‚)Ã—P(w|wâ‚‚)/(Â½(P(w|wâ‚)+P(w|wâ‚‚)))Â²

Báº£ng 4. Báº£ng cÃ¡c thÆ°á»›c Ä‘o ngá»¯ nghÄ©a vÃ  cÃ´ng thá»©c cá»§a chÃºng - Ä‘Æ°á»£c Ä‘iá»u chá»‰nh tá»« Mohammad vÃ  Hurst [81]

PHá»¤ Lá»¤C B Báº¢NG TÃ€I LIá»†U THAM KHáº¢O

TrÃ­ch dáº«n | TiÃªu Ä‘á» | NÄƒm | TÃ¡c giáº£ | Äá»‹a Ä‘iá»ƒm | SJR Quartile | H-Index | Sá»‘ trÃ­ch dáº«n tÃ­nh Ä‘áº¿n 02.04.2020

Báº£ng 5. Báº£ng tÃ i liá»‡u tham kháº£o Ä‘Æ°á»£c sá»­ dá»¥ng trong phÃ¢n tÃ­ch kháº£o sÃ¡t.