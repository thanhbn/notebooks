{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧵 DeepSeek-Coder-V2: YARN Context Extension Deep Dive\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "Master **YARN (Yet Another RoPE extensioN)** technique được sử dụng trong DeepSeek-Coder-V2 để mở rộng context length từ 16K lên 128K tokens:\n",
    "\n",
    "1. **RoPE Fundamentals**: Hiểu Rotary Position Embedding\n",
    "2. **YARN Theory**: Cơ chế mở rộng context length\n",
    "3. **Implementation Details**: Code từ cơ bản đến nâng cao\n",
    "4. **Performance Analysis**: \"Needle in a Haystack\" evaluation\n",
    "5. **Long Context Applications**: Ứng dụng trong code understanding\n",
    "\n",
    "## 📚 Paper References\n",
    "\n",
    "**Section 3.4: Long Context Extension**\n",
    "> \"Following DeepSeek-V2, we extend the context length of DeepSeek-Coder-V2 to 128K using Yarn (Peng et al., 2023). The hyper-parameters of YARN are the same as DeepSeek-V2: the scale s to 40, α to 1, β to 32.\"\n",
    "\n",
    "**Key Statistics:**\n",
    "- **Original context**: 16K tokens (DeepSeek-Coder)\n",
    "- **Extended context**: 128K tokens (8x extension)\n",
    "- **YARN parameters**: s=40, α=1, β=32\n",
    "- **Training stages**: 32K (1000 steps) → 128K (1000 steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🧵 YARN Context Extension Learning Environment Ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌀 RoPE (Rotary Position Embedding) Foundation\n",
    "\n",
    "### 💡 What is RoPE?\n",
    "\n",
    "**Rotary Position Embedding** encode position information bằng cách rotate query và key vectors trong complex space.\n",
    "\n",
    "### 🔑 Mathematical Foundation:\n",
    "\n",
    "Cho position $m$ và dimension $d$, RoPE applies rotation:\n",
    "\n",
    "$$f_q(x_m, m) = (W_q x_m) \\otimes e^{im\\theta}$$\n",
    "$$f_k(x_n, n) = (W_k x_n) \\otimes e^{in\\theta}$$\n",
    "\n",
    "Trong đó:\n",
    "- $\\theta_j = 10000^{-2j/d}$ for dimension $j$\n",
    "- $\\otimes$ là element-wise complex multiplication\n",
    "- Relative position được encode trong dot product: $q^T k = \\text{Re}(q^* k e^{i(m-n)\\theta})$\n",
    "\n",
    "### 🎯 RoPE Benefits:\n",
    "1. **Relative positioning**: Attention depends on relative distance\n",
    "2. **Translation invariance**: Shifting sequence doesn't change attention pattern\n",
    "3. **Efficient implementation**: No additional parameters needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Standard RoPE implementation\n",
    "    \n",
    "    Based on \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Precompute frequencies\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Cache for efficiency\n",
    "        self._seq_len_cached = 0\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "    \n",
    "    def _update_cos_sin_cache(self, seq_len: int, device: torch.device, dtype: torch.dtype):\n",
    "        \"\"\"Update cosine and sine cache for given sequence length\"\"\"\n",
    "        if seq_len > self._seq_len_cached or self._cos_cached is None:\n",
    "            self._seq_len_cached = seq_len\n",
    "            \n",
    "            # Position indices\n",
    "            t = torch.arange(seq_len, device=device, dtype=dtype)\n",
    "            \n",
    "            # Frequency matrix [seq_len, dim//2]\n",
    "            freqs = torch.outer(t, self.inv_freq.to(device))\n",
    "            \n",
    "            # Combine sin and cos for all dimensions [seq_len, dim]\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            \n",
    "            self._cos_cached = emb.cos()\n",
    "            self._sin_cached = emb.sin()\n",
    "    \n",
    "    def rotate_half(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Rotate half the dimensions\n",
    "        \n",
    "        [x0, x1, x2, x3, ...] -> [-x1, x0, -x3, x2, ...]\n",
    "        \"\"\"\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    def apply_rotary_emb(self, q: torch.Tensor, k: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Apply rotary embedding to query and key tensors\n",
    "        \n",
    "        Args:\n",
    "            q: Query tensor [batch_size, num_heads, seq_len, head_dim]\n",
    "            k: Key tensor [batch_size, num_heads, seq_len, head_dim]\n",
    "            seq_len: Sequence length\n",
    "            \n",
    "        Returns:\n",
    "            Rotated query and key tensors\n",
    "        \"\"\"\n",
    "        self._update_cos_sin_cache(seq_len, q.device, q.dtype)\n",
    "        \n",
    "        # Get cos/sin for current sequence length\n",
    "        cos = self._cos_cached[:seq_len]\n",
    "        sin = self._sin_cached[:seq_len]\n",
    "        \n",
    "        # Apply rotation\n",
    "        q_rotated = q * cos + self.rotate_half(q) * sin\n",
    "        k_rotated = k * cos + self.rotate_half(k) * sin\n",
    "        \n",
    "        return q_rotated, k_rotated\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        seq_len = q.size(-2)\n",
    "        return self.apply_rotary_emb(q, k, seq_len)\n",
    "\n",
    "# Demo standard RoPE\n",
    "print(\"🌀 Testing Standard RoPE:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Parameters\n",
    "batch_size, num_heads, seq_len, head_dim = 2, 8, 16, 64\n",
    "rope = RotaryPositionalEmbedding(head_dim, max_seq_len=2048)\n",
    "\n",
    "# Create query and key tensors\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(f\"Input shapes - Q: {q.shape}, K: {k.shape}\")\n",
    "\n",
    "# Apply RoPE\n",
    "q_rot, k_rot = rope(q, k)\n",
    "print(f\"Output shapes - Q: {q_rot.shape}, K: {k_rot.shape}\")\n",
    "\n",
    "# Verify relative position property\n",
    "# Attention should be translation invariant\n",
    "attn_original = torch.matmul(q_rot, k_rot.transpose(-2, -1))\n",
    "\n",
    "# Shift by 1 position\n",
    "q_shifted = q[:, :, 1:]\n",
    "k_shifted = k[:, :, 1:]\n",
    "q_shifted_rot, k_shifted_rot = rope.apply_rotary_emb(q_shifted, k_shifted, seq_len-1)\n",
    "attn_shifted = torch.matmul(q_shifted_rot, k_shifted_rot.transpose(-2, -1))\n",
    "\n",
    "# Compare diagonals (should be similar due to translation invariance)\n",
    "diag_original = torch.diagonal(attn_original[0, 0], offset=0)\n",
    "diag_shifted = torch.diagonal(attn_shifted[0, 0], offset=0)\n",
    "\n",
    "print(f\"✅ RoPE applied successfully\")\n",
    "print(f\"📊 Translation invariance check:\")\n",
    "print(f\"   Original diagonal mean: {diag_original.mean().item():.4f}\")\n",
    "print(f\"   Shifted diagonal mean: {diag_shifted.mean().item():.4f}\")\n",
    "print(f\"   Relative difference: {abs(diag_original.mean() - diag_shifted.mean()).item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧵 YARN Theory & Implementation\n",
    "\n",
    "### 💡 What is YARN?\n",
    "\n",
    "**YARN (Yet Another RoPE extensioN)** là technique để mở rộng context length của pre-trained models sử dụng RoPE position embeddings.\n",
    "\n",
    "### 🔑 Core Concepts:\n",
    "\n",
    "1. **Frequency Scaling**: Scale down frequencies để accommodate longer sequences\n",
    "2. **Attention Scaling**: Scale attention weights để maintain distribution\n",
    "3. **Interpolation**: Interpolate between scaled and original frequencies\n",
    "\n",
    "### 📊 YARN Formula:\n",
    "\n",
    "For frequency dimension $i$ và scale factor $s$:\n",
    "\n",
    "$$\\theta_i' = \\begin{cases}\n",
    "\\theta_i / s & \\text{if } i < d \\cdot \\alpha \\\\\n",
    "\\theta_i & \\text{if } i \\geq d \\cdot \\beta \\\\\n",
    "\\text{interpolate}(\\theta_i/s, \\theta_i) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Trong đó:\n",
    "- $s$: Scale factor (40 in DeepSeek-V2)\n",
    "- $\\alpha$: Low frequency threshold (1)\n",
    "- $\\beta$: High frequency threshold (32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YARNRotaryEmbedding(nn.Module):\n",
    "    \"\"\"YARN: Yet Another RoPE extensioN\n",
    "    \n",
    "    Implementation based on \"YaRN: Efficient Context Window Extension of Large Language Models\"\n",
    "    with DeepSeek-V2 hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        max_seq_len: int = 2048,\n",
    "        base: float = 10000.0,\n",
    "        scale: float = 1.0,  # Context extension scale factor\n",
    "        original_max_seq_len: int = 2048,\n",
    "        # YARN hyperparameters (DeepSeek-V2 values)\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 32.0,\n",
    "        extrapolation_factor: float = 1.0,\n",
    "        finetuned: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        self.scale = scale\n",
    "        self.original_max_seq_len = original_max_seq_len\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.extrapolation_factor = extrapolation_factor\n",
    "        self.finetuned = finetuned\n",
    "        \n",
    "        # Compute YARN frequencies\n",
    "        self.inv_freq = self._compute_yarn_frequencies()\n",
    "        self.register_buffer('_inv_freq', self.inv_freq)\n",
    "        \n",
    "        # Cache\n",
    "        self._seq_len_cached = 0\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "    \n",
    "    def _compute_yarn_frequencies(self) -> torch.Tensor:\n",
    "        \"\"\"Compute YARN-modified frequencies\"\"\"\n",
    "        # Original RoPE frequencies\n",
    "        freq_base = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        \n",
    "        if self.scale <= 1.0:\n",
    "            # No scaling needed\n",
    "            return freq_base\n",
    "        \n",
    "        # YARN frequency modification\n",
    "        yarn_freqs = torch.zeros_like(freq_base)\n",
    "        \n",
    "        for i, freq in enumerate(freq_base):\n",
    "            # Determine frequency band\n",
    "            dim_ratio = 2 * i / self.dim  # Position in frequency spectrum [0, 1]\n",
    "            \n",
    "            if dim_ratio < self.alpha:\n",
    "                # Low frequency: apply full scaling\n",
    "                yarn_freqs[i] = freq / self.scale\n",
    "            elif dim_ratio >= self.beta:\n",
    "                # High frequency: no scaling\n",
    "                yarn_freqs[i] = freq\n",
    "            else:\n",
    "                # Mid frequency: interpolate\n",
    "                # Linear interpolation between scaled and original\n",
    "                interp_factor = (dim_ratio - self.alpha) / (self.beta - self.alpha)\n",
    "                scaled_freq = freq / self.scale\n",
    "                yarn_freqs[i] = scaled_freq * (1 - interp_factor) + freq * interp_factor\n",
    "        \n",
    "        return yarn_freqs\n",
    "    \n",
    "    def _get_attention_scale(self, seq_len: int) -> float:\n",
    "        \"\"\"Get attention scaling factor for YARN\"\"\"\n",
    "        if seq_len <= self.original_max_seq_len:\n",
    "            return 1.0\n",
    "        \n",
    "        # Scale attention based on sequence length extension\n",
    "        extension_ratio = seq_len / self.original_max_seq_len\n",
    "        \n",
    "        if self.finetuned:\n",
    "            # For finetuned models, use gentler scaling\n",
    "            return 1.0 / math.sqrt(extension_ratio)\n",
    "        else:\n",
    "            # For base models, use logarithmic scaling\n",
    "            return 1.0 / math.log(extension_ratio + 1)\n",
    "    \n",
    "    def _update_cos_sin_cache(self, seq_len: int, device: torch.device, dtype: torch.dtype):\n",
    "        \"\"\"Update cosine/sine cache with YARN modifications\"\"\"\n",
    "        if seq_len > self._seq_len_cached or self._cos_cached is None:\n",
    "            self._seq_len_cached = seq_len\n",
    "            \n",
    "            # Position indices\n",
    "            t = torch.arange(seq_len, device=device, dtype=dtype)\n",
    "            \n",
    "            # Apply extrapolation factor for very long sequences\n",
    "            if seq_len > self.original_max_seq_len:\n",
    "                # Use extrapolation factor to reduce high-frequency noise\n",
    "                t = t * self.extrapolation_factor\n",
    "            \n",
    "            # Frequency matrix with YARN frequencies\n",
    "            freqs = torch.outer(t, self.inv_freq.to(device))\n",
    "            \n",
    "            # Combine for all dimensions\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            \n",
    "            self._cos_cached = emb.cos()\n",
    "            self._sin_cached = emb.sin()\n",
    "    \n",
    "    def rotate_half(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Rotate half dimensions\"\"\"\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    def apply_rotary_emb(\n",
    "        self, \n",
    "        q: torch.Tensor, \n",
    "        k: torch.Tensor, \n",
    "        seq_len: int,\n",
    "        apply_attention_scaling: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, float]:\n",
    "        \"\"\"Apply YARN rotary embedding\n",
    "        \n",
    "        Returns:\n",
    "            q_rotated, k_rotated, attention_scale\n",
    "        \"\"\"\n",
    "        self._update_cos_sin_cache(seq_len, q.device, q.dtype)\n",
    "        \n",
    "        # Get cos/sin\n",
    "        cos = self._cos_cached[:seq_len]\n",
    "        sin = self._sin_cached[:seq_len]\n",
    "        \n",
    "        # Apply rotation\n",
    "        q_rotated = q * cos + self.rotate_half(q) * sin\n",
    "        k_rotated = k * cos + self.rotate_half(k) * sin\n",
    "        \n",
    "        # Get attention scaling\n",
    "        attention_scale = 1.0\n",
    "        if apply_attention_scaling:\n",
    "            attention_scale = self._get_attention_scale(seq_len)\n",
    "        \n",
    "        return q_rotated, k_rotated, attention_scale\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, float]:\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        seq_len = q.size(-2)\n",
    "        return self.apply_rotary_emb(q, k, seq_len)\n",
    "\n",
    "# Demo YARN with DeepSeek-V2 parameters\n",
    "print(\"🧵 Testing YARN with DeepSeek-V2 Parameters:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DeepSeek-V2 YARN configuration\n",
    "deepseek_yarn_config = {\n",
    "    'scale': 40.0,  # s=40 in paper\n",
    "    'alpha': 1.0,   # α=1 in paper  \n",
    "    'beta': 32.0,   # β=32 in paper\n",
    "    'original_max_seq_len': 16384,  # 16K original\n",
    "    'max_seq_len': 131072,  # 128K extended\n",
    "}\n",
    "\n",
    "# Create YARN embedding\n",
    "yarn = YARNRotaryEmbedding(\n",
    "    dim=64,\n",
    "    **deepseek_yarn_config\n",
    ")\n",
    "\n",
    "print(f\"📊 YARN Configuration:\")\n",
    "for key, value in deepseek_yarn_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Test at different sequence lengths\n",
    "test_lengths = [1024, 16384, 32768, 65536, 131072]  # 1K to 128K\n",
    "\n",
    "print(f\"\\n🧪 Testing YARN at Different Sequence Lengths:\")\n",
    "print(f\"{'Length':<8} {'Scale':<8} {'Status':<15}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for seq_len in test_lengths:\n",
    "    # Test with smaller tensors to avoid memory issues\n",
    "    q_test = torch.randn(1, 1, min(seq_len, 1024), 64)  # Limit to 1K for demo\n",
    "    k_test = torch.randn(1, 1, min(seq_len, 1024), 64)\n",
    "    \n",
    "    try:\n",
    "        q_rot, k_rot, attn_scale = yarn.apply_rotary_emb(q_test, k_test, seq_len)\n",
    "        status = \"✅ Success\"\n",
    "    except Exception as e:\n",
    "        attn_scale = float('nan')\n",
    "        status = f\"❌ Error: {str(e)[:10]}...\"\n",
    "    \n",
    "    print(f\"{seq_len:<8} {attn_scale:<8.4f} {status:<15}\")\n",
    "\n",
    "# Visualize frequency modifications\n",
    "def visualize_yarn_frequencies():\n",
    "    \"\"\"Visualize how YARN modifies RoPE frequencies\"\"\"\n",
    "    \n",
    "    dim = 128\n",
    "    \n",
    "    # Original RoPE frequencies\n",
    "    original_rope = RotaryPositionalEmbedding(dim)\n",
    "    original_freqs = original_rope.inv_freq.numpy()\n",
    "    \n",
    "    # YARN frequencies with different scales\n",
    "    scales = [1.0, 4.0, 16.0, 40.0]  # Include DeepSeek-V2's scale=40\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot frequency modifications\n",
    "    for i, scale in enumerate(scales):\n",
    "        yarn_test = YARNRotaryEmbedding(\n",
    "            dim=dim, \n",
    "            scale=scale, \n",
    "            alpha=1.0, \n",
    "            beta=32.0\n",
    "        )\n",
    "        yarn_freqs = yarn_test.inv_freq.numpy()\n",
    "        \n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        freq_indices = np.arange(len(original_freqs))\n",
    "        \n",
    "        plt.plot(freq_indices, original_freqs, 'b-', label='Original RoPE', linewidth=2)\n",
    "        plt.plot(freq_indices, yarn_freqs, 'r-', label=f'YARN (scale={scale})', linewidth=2)\n",
    "        \n",
    "        # Mark alpha and beta boundaries\n",
    "        alpha_idx = int(1.0 * len(freq_indices))\n",
    "        beta_idx = int(32.0 * len(freq_indices))\n",
    "        \n",
    "        plt.axvline(alpha_idx, color='green', linestyle='--', alpha=0.7, label=f'α={1.0}')\n",
    "        plt.axvline(beta_idx, color='orange', linestyle='--', alpha=0.7, label=f'β={32.0}')\n",
    "        \n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Frequency Index')\n",
    "        plt.ylabel('Inverse Frequency')\n",
    "        plt.title(f'YARN Frequency Modification (Scale={scale})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_yarn_frequencies()\n",
    "\n",
    "print(\"\\n🔍 Key YARN Insights:\")\n",
    "print(\"• Low frequencies (α<1): Full scaling applied\")\n",
    "print(\"• High frequencies (β≥32): No scaling (preserve fine details)\")\n",
    "print(\"• Mid frequencies: Interpolated scaling\")\n",
    "print(\"• DeepSeek-V2 uses aggressive scale=40 for 8x context extension\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 \"Needle in a Haystack\" Evaluation\n",
    "\n",
    "### 🎯 What is \"Needle in a Haystack\"?\n",
    "\n",
    "Test khả năng của model tìm specific information trong long context bằng cách:\n",
    "1. **Insert \"needle\"**: Thêm specific fact vào random position\n",
    "2. **Create \"haystack\"**: Surround với irrelevant text\n",
    "3. **Query**: Ask model to retrieve the needle\n",
    "4. **Evaluate**: Measure accuracy across different positions và context lengths\n",
    "\n",
    "### 📊 DeepSeek-V2 Results (Figure 2):\n",
    "Model đạt >95% accuracy across all context lengths up to 128K tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeedleInHaystackEvaluator:\n",
    "    \"\"\"Needle in a Haystack evaluation for long context models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"DeepSeek-Coder-V2\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Needle templates\n",
    "        self.needles = [\n",
    "            \"The secret code is: ALPHA-7429\",\n",
    "            \"Remember this number: 185304\",\n",
    "            \"The password is: quantum_bridge_2024\",\n",
    "            \"Important: The key is stored in vault-789\",\n",
    "            \"Critical information: Project Nebula status is ACTIVE\"\n",
    "        ]\n",
    "        \n",
    "        # Haystack text (code-like content for DeepSeek-Coder)\n",
    "        self.haystack_templates = [\n",
    "            \"def process_data(input_file):\\n    with open(input_file, 'r') as f:\\n        data = f.read()\\n    return data.strip()\",\n",
    "            \"class DataProcessor:\\n    def __init__(self, config):\\n        self.config = config\\n    def run(self):\\n        pass\",\n",
    "            \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\",\n",
    "            \"# Configuration settings\\nDATABASE_URL = 'postgresql://localhost/mydb'\\nAPI_KEY = 'test_key_123'\",\n",
    "            \"async function fetchData(url) {\\n    const response = await fetch(url);\\n    return response.json();\\n}\"\n",
    "        ]\n",
    "    \n",
    "    def generate_haystack(self, target_length: int) -> str:\n",
    "        \"\"\"Generate haystack text of target length\"\"\"\n",
    "        haystack = \"\"\n",
    "        \n",
    "        while len(haystack) < target_length:\n",
    "            # Add random code snippet\n",
    "            template = np.random.choice(self.haystack_templates)\n",
    "            haystack += template + \"\\n\\n\"\n",
    "            \n",
    "            # Add some random variables/comments\n",
    "            if np.random.random() > 0.7:\n",
    "                haystack += f\"# Random comment {np.random.randint(1000, 9999)}\\n\"\n",
    "            \n",
    "            if np.random.random() > 0.8:\n",
    "                haystack += f\"variable_{np.random.randint(100, 999)} = {np.random.randint(1, 100)}\\n\"\n",
    "        \n",
    "        return haystack[:target_length]\n",
    "    \n",
    "    def create_test_case(\n",
    "        self, \n",
    "        context_length: int, \n",
    "        needle_position: float\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Create a single needle-in-haystack test case\n",
    "        \n",
    "        Args:\n",
    "            context_length: Total context length in characters\n",
    "            needle_position: Position of needle (0.0 = start, 1.0 = end)\n",
    "        \"\"\"\n",
    "        # Select random needle\n",
    "        needle = np.random.choice(self.needles)\n",
    "        \n",
    "        # Generate haystack\n",
    "        haystack = self.generate_haystack(context_length - len(needle) - 100)  # Leave room\n",
    "        \n",
    "        # Calculate insertion position\n",
    "        insert_pos = int(len(haystack) * needle_position)\n",
    "        \n",
    "        # Insert needle\n",
    "        context = (\n",
    "            haystack[:insert_pos] + \n",
    "            \"\\n\" + needle + \"\\n\" + \n",
    "            haystack[insert_pos:]\n",
    "        )\n",
    "        \n",
    "        # Create query based on needle type\n",
    "        if \"code\" in needle.lower():\n",
    "            query = \"What is the secret code mentioned in the text?\"\n",
    "        elif \"number\" in needle.lower():\n",
    "            query = \"What number should I remember?\"\n",
    "        elif \"password\" in needle.lower():\n",
    "            query = \"What is the password?\"\n",
    "        elif \"vault\" in needle.lower():\n",
    "            query = \"Where is the key stored?\"\n",
    "        else:\n",
    "            query = \"What is the status of Project Nebula?\"\n",
    "        \n",
    "        return {\n",
    "            'context': context,\n",
    "            'query': query,\n",
    "            'needle': needle,\n",
    "            'expected_answer': needle.split(': ')[1] if ': ' in needle else needle,\n",
    "            'position': needle_position,\n",
    "            'length': len(context)\n",
    "        }\n",
    "    \n",
    "    def simulate_model_performance(\n",
    "        self, \n",
    "        context_length: int, \n",
    "        needle_position: float,\n",
    "        use_yarn: bool = True\n",
    "    ) -> float:\n",
    "        \"\"\"Simulate model performance on needle-in-haystack task\n",
    "        \n",
    "        Returns accuracy score (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        # Simulate DeepSeek-V2 performance based on paper results\n",
    "        base_accuracy = 0.95  # Paper reports >95% accuracy\n",
    "        \n",
    "        if not use_yarn:\n",
    "            # Without YARN, performance degrades significantly for long contexts\n",
    "            if context_length > 16384:  # Beyond original context length\n",
    "                degradation = min(0.8, (context_length - 16384) / 50000)\n",
    "                base_accuracy *= (1 - degradation)\n",
    "        \n",
    "        # Position effects (harder to find needle at the middle)\n",
    "        position_penalty = 0.05 * abs(needle_position - 0.5) * 2  # Max penalty at middle\n",
    "        \n",
    "        # Length effects (slight degradation for very long contexts)\n",
    "        length_penalty = min(0.1, context_length / 1000000)  # Very gentle degradation\n",
    "        \n",
    "        # Add some randomness\n",
    "        noise = np.random.normal(0, 0.02)  # Small noise\n",
    "        \n",
    "        accuracy = base_accuracy - position_penalty - length_penalty + noise\n",
    "        return np.clip(accuracy, 0.0, 1.0)\n",
    "    \n",
    "    def run_evaluation(\n",
    "        self,\n",
    "        context_lengths: List[int],\n",
    "        position_percentiles: List[float],\n",
    "        num_trials: int = 5\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Run comprehensive needle-in-haystack evaluation\"\"\"\n",
    "        \n",
    "        results_yarn = np.zeros((len(context_lengths), len(position_percentiles)))\n",
    "        results_no_yarn = np.zeros((len(context_lengths), len(position_percentiles)))\n",
    "        \n",
    "        print(f\"🔍 Running Needle-in-Haystack Evaluation...\")\n",
    "        print(f\"   Context lengths: {context_lengths}\")\n",
    "        print(f\"   Position percentiles: {position_percentiles}\")\n",
    "        print(f\"   Trials per condition: {num_trials}\")\n",
    "        \n",
    "        for i, length in enumerate(context_lengths):\n",
    "            for j, position in enumerate(position_percentiles):\n",
    "                # Multiple trials for averaging\n",
    "                yarn_scores = []\n",
    "                no_yarn_scores = []\n",
    "                \n",
    "                for trial in range(num_trials):\n",
    "                    # With YARN\n",
    "                    yarn_score = self.simulate_model_performance(\n",
    "                        length, position, use_yarn=True\n",
    "                    )\n",
    "                    yarn_scores.append(yarn_score)\n",
    "                    \n",
    "                    # Without YARN\n",
    "                    no_yarn_score = self.simulate_model_performance(\n",
    "                        length, position, use_yarn=False\n",
    "                    )\n",
    "                    no_yarn_scores.append(no_yarn_score)\n",
    "                \n",
    "                results_yarn[i, j] = np.mean(yarn_scores)\n",
    "                results_no_yarn[i, j] = np.mean(no_yarn_scores)\n",
    "        \n",
    "        return {\n",
    "            'yarn': results_yarn,\n",
    "            'no_yarn': results_no_yarn,\n",
    "            'context_lengths': context_lengths,\n",
    "            'positions': position_percentiles\n",
    "        }\n",
    "\n",
    "# Run needle-in-haystack evaluation\n",
    "print(\"🎯 Needle-in-Haystack Evaluation:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "evaluator = NeedleInHaystackEvaluator()\n",
    "\n",
    "# Test parameters\n",
    "context_lengths = [1000, 4000, 8000, 16000, 32000, 64000, 128000]  # Up to 128K\n",
    "position_percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Generate a sample test case\n",
    "sample_test = evaluator.create_test_case(context_length=2000, needle_position=0.5)\n",
    "print(f\"📋 Sample Test Case:\")\n",
    "print(f\"   Query: {sample_test['query']}\")\n",
    "print(f\"   Needle: {sample_test['needle']}\")\n",
    "print(f\"   Expected: {sample_test['expected_answer']}\")\n",
    "print(f\"   Context length: {sample_test['length']} chars\")\n",
    "print(f\"   Position: {sample_test['position']:.1%}\")\n",
    "\n",
    "# Run evaluation (reduced for demo)\n",
    "eval_results = evaluator.run_evaluation(\n",
    "    context_lengths=[1000, 8000, 16000, 64000, 128000],\n",
    "    position_percentiles=[0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    num_trials=3\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Visualizing YARN Performance\n",
    "\n",
    "### 🎨 Performance Heatmaps & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_needle_in_haystack_results(results: Dict[str, np.ndarray]):\n",
    "    \"\"\"Visualize needle-in-haystack evaluation results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    context_lengths = results['context_lengths']\n",
    "    positions = results['positions']\n",
    "    \n",
    "    # 1. YARN Performance Heatmap\n",
    "    ax1 = axes[0, 0]\n",
    "    im1 = ax1.imshow(results['yarn'], cmap='RdYlGn', vmin=0.5, vmax=1.0, aspect='auto')\n",
    "    ax1.set_title('DeepSeek-V2 with YARN\\nNeedle-in-Haystack Performance')\n",
    "    ax1.set_xlabel('Document Position (%)')\n",
    "    ax1.set_ylabel('Context Length')\n",
    "    \n",
    "    # Set ticks\n",
    "    ax1.set_xticks(range(len(positions)))\n",
    "    ax1.set_xticklabels([f'{p:.0%}' for p in positions])\n",
    "    ax1.set_yticks(range(len(context_lengths)))\n",
    "    ax1.set_yticklabels([f'{l//1000}K' for l in context_lengths])\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04, label='Accuracy')\n",
    "    \n",
    "    # 2. No-YARN Performance Heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    im2 = ax2.imshow(results['no_yarn'], cmap='RdYlGn', vmin=0.5, vmax=1.0, aspect='auto')\n",
    "    ax2.set_title('Without YARN\\nNeedle-in-Haystack Performance')\n",
    "    ax2.set_xlabel('Document Position (%)')\n",
    "    ax2.set_ylabel('Context Length')\n",
    "    \n",
    "    ax2.set_xticks(range(len(positions)))\n",
    "    ax2.set_xticklabels([f'{p:.0%}' for p in positions])\n",
    "    ax2.set_yticks(range(len(context_lengths)))\n",
    "    ax2.set_yticklabels([f'{l//1000}K' for l in context_lengths])\n",
    "    \n",
    "    plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04, label='Accuracy')\n",
    "    \n",
    "    # 3. Performance vs Context Length\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # Average across all positions\n",
    "    yarn_avg = results['yarn'].mean(axis=1)\n",
    "    no_yarn_avg = results['no_yarn'].mean(axis=1)\n",
    "    \n",
    "    ax3.plot([l//1000 for l in context_lengths], yarn_avg, 'g-o', linewidth=2, \n",
    "             markersize=8, label='With YARN')\n",
    "    ax3.plot([l//1000 for l in context_lengths], no_yarn_avg, 'r-s', linewidth=2, \n",
    "             markersize=8, label='Without YARN')\n",
    "    \n",
    "    ax3.set_xlabel('Context Length (K tokens)')\n",
    "    ax3.set_ylabel('Average Accuracy')\n",
    "    ax3.set_title('Performance vs Context Length')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0.5, 1.0)\n",
    "    \n",
    "    # Add DeepSeek-V2 reference line\n",
    "    ax3.axhline(y=0.95, color='blue', linestyle='--', alpha=0.7, \n",
    "                label='DeepSeek-V2 Target (95%)')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Performance vs Position\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Performance at longest context (128K)\n",
    "    longest_ctx_idx = -1  # Last index (longest context)\n",
    "    yarn_pos = results['yarn'][longest_ctx_idx, :]\n",
    "    no_yarn_pos = results['no_yarn'][longest_ctx_idx, :]\n",
    "    \n",
    "    position_labels = [f'{p:.0%}' for p in positions]\n",
    "    \n",
    "    ax4.plot(position_labels, yarn_pos, 'g-o', linewidth=2, markersize=8, label='With YARN')\n",
    "    ax4.plot(position_labels, no_yarn_pos, 'r-s', linewidth=2, markersize=8, label='Without YARN')\n",
    "    \n",
    "    ax4.set_xlabel('Needle Position in Document')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.set_title(f'Performance vs Position\\n(Context Length: {context_lengths[-1]//1000}K)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_ylim(0.5, 1.0)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\n📊 Performance Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"YARN Results:\")\n",
    "    print(f\"   Average accuracy: {results['yarn'].mean():.2%}\")\n",
    "    print(f\"   Min accuracy: {results['yarn'].min():.2%}\")\n",
    "    print(f\"   Max accuracy: {results['yarn'].max():.2%}\")\n",
    "    print(f\"   At 128K context: {results['yarn'][-1].mean():.2%}\")\n",
    "    \n",
    "    print(f\"\\nWithout YARN:\")\n",
    "    print(f\"   Average accuracy: {results['no_yarn'].mean():.2%}\")\n",
    "    print(f\"   Min accuracy: {results['no_yarn'].min():.2%}\")\n",
    "    print(f\"   Max accuracy: {results['no_yarn'].max():.2%}\")\n",
    "    print(f\"   At 128K context: {results['no_yarn'][-1].mean():.2%}\")\n",
    "    \n",
    "    improvement = (results['yarn'].mean() - results['no_yarn'].mean()) / results['no_yarn'].mean() * 100\n",
    "    print(f\"\\n🚀 YARN Improvement: {improvement:.1f}%\")\n",
    "\n",
    "# Visualize results\n",
    "visualize_needle_in_haystack_results(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 YARN Training Simulation\n",
    "\n",
    "### 🏋️ Simulating DeepSeek-V2's Two-Stage Training\n",
    "\n",
    "Theo paper:\n",
    "1. **Stage 1**: 32K context, 1152 batch size, 1000 steps\n",
    "2. **Stage 2**: 128K context, 288 batch size, 1000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YARNTrainingSimulator:\n",
    "    \"\"\"Simulate YARN training process for context extension\"\"\"\n",
    "    \n",
    "    def __init__(self, original_context_len: int = 16384):\n",
    "        self.original_context_len = original_context_len\n",
    "        self.training_stages = [\n",
    "            {\n",
    "                'name': 'Stage 1: Gradual Extension',\n",
    "                'context_length': 32768,  # 32K\n",
    "                'batch_size': 1152,\n",
    "                'steps': 1000,\n",
    "                'learning_rate': 1e-5\n",
    "            },\n",
    "            {\n",
    "                'name': 'Stage 2: Full Extension', \n",
    "                'context_length': 131072,  # 128K\n",
    "                'batch_size': 288,\n",
    "                'steps': 1000,\n",
    "                'learning_rate': 5e-6\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def simulate_training_stage(\n",
    "        self, \n",
    "        stage_config: Dict, \n",
    "        initial_performance: float = 0.85\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Simulate training performance over steps\"\"\"\n",
    "        \n",
    "        steps = stage_config['steps']\n",
    "        context_len = stage_config['context_length']\n",
    "        \n",
    "        # Performance starts lower for longer contexts\n",
    "        context_penalty = (context_len / self.original_context_len - 1) * 0.1\n",
    "        start_perf = max(0.7, initial_performance - context_penalty)\n",
    "        \n",
    "        # Target performance (based on paper results)\n",
    "        target_perf = 0.95\n",
    "        \n",
    "        # Simulate learning curve\n",
    "        performance_curve = []\n",
    "        \n",
    "        for step in range(steps + 1):\n",
    "            # Exponential improvement with some noise\n",
    "            progress = 1 - np.exp(-3 * step / steps)  # Exponential approach\n",
    "            current_perf = start_perf + (target_perf - start_perf) * progress\n",
    "            \n",
    "            # Add training noise\n",
    "            noise = np.random.normal(0, 0.01)\n",
    "            current_perf = np.clip(current_perf + noise, 0.6, 1.0)\n",
    "            \n",
    "            performance_curve.append(current_perf)\n",
    "        \n",
    "        return performance_curve\n",
    "    \n",
    "    def run_full_training(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"Run complete YARN training simulation\"\"\"\n",
    "        \n",
    "        print(\"🏋️ Simulating YARN Training Process:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        all_results = {}\n",
    "        initial_perf = 0.90  # Start with good base model performance\n",
    "        \n",
    "        for i, stage in enumerate(self.training_stages):\n",
    "            print(f\"\\n📈 {stage['name']}:\")\n",
    "            print(f\"   Context Length: {stage['context_length']:,} tokens\")\n",
    "            print(f\"   Batch Size: {stage['batch_size']}\")\n",
    "            print(f\"   Steps: {stage['steps']}\")\n",
    "            print(f\"   Learning Rate: {stage['learning_rate']}\")\n",
    "            \n",
    "            # Simulate training\n",
    "            performance = self.simulate_training_stage(stage, initial_perf)\n",
    "            all_results[f'stage_{i+1}'] = performance\n",
    "            \n",
    "            # Update initial performance for next stage\n",
    "            initial_perf = performance[-1]\n",
    "            \n",
    "            print(f\"   Final Performance: {performance[-1]:.2%}\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def visualize_training(self, results: Dict[str, List[float]]):\n",
    "        \"\"\"Visualize training progress\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        \n",
    "        # 1. Training curves\n",
    "        ax1 = axes[0, 0]\n",
    "        \n",
    "        colors = ['blue', 'green']\n",
    "        for i, (stage_name, performance) in enumerate(results.items()):\n",
    "            steps = range(len(performance))\n",
    "            ax1.plot(steps, performance, color=colors[i], linewidth=2, \n",
    "                    label=f'Stage {i+1}', alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Training Steps')\n",
    "        ax1.set_ylabel('Performance (Accuracy)')\n",
    "        ax1.set_title('YARN Training Progress')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0.7, 1.0)\n",
    "        \n",
    "        # Add target line\n",
    "        ax1.axhline(y=0.95, color='red', linestyle='--', alpha=0.7, \n",
    "                   label='Target (95%)')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # 2. Context length progression\n",
    "        ax2 = axes[0, 1]\n",
    "        \n",
    "        context_lengths = [16, 32, 128]  # K tokens\n",
    "        final_performances = [0.95, results['stage_1'][-1], results['stage_2'][-1]]\n",
    "        \n",
    "        bars = ax2.bar(range(len(context_lengths)), final_performances, \n",
    "                      color=['gray', 'blue', 'green'], alpha=0.7)\n",
    "        \n",
    "        ax2.set_xlabel('Context Length (K tokens)')\n",
    "        ax2.set_ylabel('Final Performance')\n",
    "        ax2.set_title('Performance vs Context Length')\n",
    "        ax2.set_xticks(range(len(context_lengths)))\n",
    "        ax2.set_xticklabels([f'{ctx}K' for ctx in context_lengths])\n",
    "        ax2.set_ylim(0.8, 1.0)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, final_performances):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                    f'{value:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. Memory usage simulation\n",
    "        ax3 = axes[1, 0]\n",
    "        \n",
    "        context_lens = [16, 32, 64, 128]  # K tokens\n",
    "        # Memory scales quadratically with sequence length (attention computation)\n",
    "        memory_usage = [ctx**2 / 16**2 for ctx in context_lens]  # Relative to 16K\n",
    "        \n",
    "        ax3.plot(context_lens, memory_usage, 'ro-', linewidth=2, markersize=8)\n",
    "        ax3.set_xlabel('Context Length (K tokens)')\n",
    "        ax3.set_ylabel('Relative Memory Usage')\n",
    "        ax3.set_title('Memory Scaling with Context Length')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_yscale('log')\n",
    "        \n",
    "        # Add annotations\n",
    "        for ctx, mem in zip(context_lens, memory_usage):\n",
    "            ax3.annotate(f'{mem:.1f}x', (ctx, mem), \n",
    "                        textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "        \n",
    "        # 4. Batch size adjustments\n",
    "        ax4 = axes[1, 1]\n",
    "        \n",
    "        stages = ['Original\\n(16K)', 'Stage 1\\n(32K)', 'Stage 2\\n(128K)']\n",
    "        batch_sizes = [2048, 1152, 288]  # Estimated original, then from paper\n",
    "        context_sizes = [16, 32, 128]\n",
    "        \n",
    "        # Effective tokens per batch\n",
    "        effective_tokens = [b * c for b, c in zip(batch_sizes, context_sizes)]\n",
    "        \n",
    "        ax4_twin = ax4.twinx()\n",
    "        \n",
    "        bars1 = ax4.bar(range(len(stages)), batch_sizes, alpha=0.7, \n",
    "                       color='skyblue', label='Batch Size')\n",
    "        line1 = ax4_twin.plot(range(len(stages)), effective_tokens, 'ro-', \n",
    "                             linewidth=2, markersize=8, label='Effective Tokens')\n",
    "        \n",
    "        ax4.set_xlabel('Training Stage')\n",
    "        ax4.set_ylabel('Batch Size', color='blue')\n",
    "        ax4_twin.set_ylabel('Effective Tokens (K)', color='red')\n",
    "        ax4.set_title('Batch Size Adjustment Strategy')\n",
    "        ax4.set_xticks(range(len(stages)))\n",
    "        ax4.set_xticklabels(stages)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (batch, tokens) in enumerate(zip(batch_sizes, effective_tokens)):\n",
    "            ax4.text(i, batch + 50, str(batch), ha='center', va='bottom')\n",
    "            ax4_twin.text(i, tokens + 1000, f'{tokens//1000}K', ha='center', va='bottom', color='red')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Run YARN training simulation\n",
    "trainer = YARNTrainingSimulator()\n",
    "training_results = trainer.run_full_training()\n",
    "trainer.visualize_training(training_results)\n",
    "\n",
    "print(\"\\n🎯 YARN Training Insights:\")\n",
    "print(\"• Two-stage training allows gradual adaptation\")\n",
    "print(\"• Batch size reduction compensates for memory growth\")\n",
    "print(\"• Performance maintained across 8x context extension\")\n",
    "print(\"• Memory scales quadratically, requiring careful optimization\")\n",
    "print(\"• DeepSeek-V2 achieves 95%+ accuracy up to 128K tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💻 Code Understanding with Long Context\n",
    "\n",
    "### 🔍 Real-world Applications của 128K Context\n",
    "\n",
    "Demonstrate practical applications của YARN context extension trong code understanding tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongContextCodeAnalyzer:\n",
    "    \"\"\"Analyze code understanding tasks that benefit from long context\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.context_limits = {\n",
    "            'original': 16384,  # 16K tokens\n",
    "            'yarn_extended': 131072  # 128K tokens\n",
    "        }\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Rough token estimation (1 token ≈ 4 characters for code)\"\"\"\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def generate_large_codebase_example(self, num_files: int = 10) -> Dict[str, str]:\n",
    "        \"\"\"Generate example of large codebase\"\"\"\n",
    "        \n",
    "        codebase = {}\n",
    "        \n",
    "        # Main application file\n",
    "        codebase['main.py'] = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Large-scale data processing application\n",
    "Processes millions of records with ML pipeline\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from data_processor import DataProcessor\n",
    "from ml_pipeline import MLPipeline\n",
    "from config_manager import ConfigManager\n",
    "from database import DatabaseManager\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 8\n",
    "    timeout: int = 300\n",
    "    retry_count: int = 3\n",
    "\n",
    "class ApplicationManager:\n",
    "    def __init__(self, config_path: str):\n",
    "        self.config = ConfigManager.load(config_path)\n",
    "        self.db = DatabaseManager(self.config.database_url)\n",
    "        self.processor = DataProcessor(self.config.processing)\n",
    "        self.ml_pipeline = MLPipeline(self.config.ml_config)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    async def run_processing_pipeline(self):\n",
    "        \"\"\"Main processing pipeline\"\"\"\n",
    "        try:\n",
    "            # Initialize components\n",
    "            await self.db.connect()\n",
    "            await self.processor.initialize()\n",
    "            \n",
    "            # Process data in batches\n",
    "            batch_count = 0\n",
    "            async for batch in self.db.get_data_batches():\n",
    "                processed_batch = await self.processor.process_batch(batch)\n",
    "                ml_results = await self.ml_pipeline.predict(processed_batch)\n",
    "                await self.db.save_results(ml_results)\n",
    "                \n",
    "                batch_count += 1\n",
    "                self.logger.info(f\"Processed batch {batch_count}\")\n",
    "                \n",
    "            self.logger.info(f\"Pipeline completed. Processed {batch_count} batches.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            await self.db.disconnect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = ApplicationManager(\"config.yaml\")\n",
    "    asyncio.run(app.run_processing_pipeline())\n",
    "'''\n",
    "        \n",
    "        # Data processor module\n",
    "        codebase['data_processor.py'] = '''import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.executor = ThreadPoolExecutor(max_workers=config.max_workers)\n",
    "        \n",
    "    async def initialize(self):\n",
    "        \"\"\"Initialize processor\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess data\"\"\"\n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "        \n",
    "        # Normalize text columns\n",
    "        text_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in text_cols:\n",
    "            df[col] = df[col].str.strip().str.lower()\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def _extract_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract features for ML\"\"\"\n",
    "        # Feature engineering logic here\n",
    "        features = df.copy()\n",
    "        \n",
    "        # Add derived features\n",
    "        if 'timestamp' in features.columns:\n",
    "            features['hour'] = pd.to_datetime(features['timestamp']).dt.hour\n",
    "            features['day_of_week'] = pd.to_datetime(features['timestamp']).dt.dayofweek\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    async def process_batch(self, batch_data: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Process a batch of data\"\"\"\n",
    "        df = pd.DataFrame(batch_data)\n",
    "        \n",
    "        # Run CPU-intensive operations in thread pool\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Clean data\n",
    "        df = await loop.run_in_executor(self.executor, self._clean_data, df)\n",
    "        \n",
    "        # Extract features\n",
    "        df = await loop.run_in_executor(self.executor, self._extract_features, df)\n",
    "        \n",
    "        return df\n",
    "'''\n",
    "\n",
    "        # Add more files to reach target size\n",
    "        for i in range(num_files - 2):\n",
    "            codebase[f'module_{i}.py'] = f'''# Module {i}: Utility functions and classes\n",
    "\n",
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class Utility{i}:\n",
    "    \"\"\"Utility class for module {i}\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(f\"module_{i}\")\n",
    "    \n",
    "    def process_data_{i}(self, data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process data specific to module {i}\"\"\"\n",
    "        processed = []\n",
    "        for item in data:\n",
    "            # Complex processing logic here\n",
    "            result = {{\n",
    "                'id': item.get('id'),\n",
    "                'processed_value': item.get('value', 0) * {i + 1},\n",
    "                'metadata': {{\n",
    "                    'processor': f'module_{i}',\n",
    "                    'version': '1.0.{i}'\n",
    "                }}\n",
    "            }}\n",
    "            processed.append(result)\n",
    "        return processed\n",
    "    \n",
    "    def validate_data_{i}(self, data: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Validate data for module {i}\"\"\"\n",
    "        required_fields = ['id', 'value', 'timestamp']\n",
    "        return all(field in data for field in required_fields)\n",
    "\n",
    "def helper_function_{i}(x: float, y: float) -> float:\n",
    "    \"\"\"Helper function for module {i}\"\"\"\n",
    "    return (x + y) * {i + 1} / (x - y + 0.001)\n",
    "\n",
    "# Constants for module {i}\n",
    "MODULE_{i}_VERSION = \"1.0.{i}\"\n",
    "MODULE_{i}_CONFIG = {{\n",
    "    'max_batch_size': {100 * (i + 1)},\n",
    "    'timeout': {30 + i * 5},\n",
    "    'retry_count': {3 + i}\n",
    "}}\n",
    "''' * 10  # Make each module longer\n",
    "        \n",
    "        return codebase\n",
    "    \n",
    "    def analyze_codebase_scenarios(self, codebase: Dict[str, str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze different scenarios for code understanding\"\"\"\n",
    "        \n",
    "        # Calculate total size\n",
    "        total_text = \"\\n\\n\".join(f\"# File: {filename}\\n{content}\" \n",
    "                                for filename, content in codebase.items())\n",
    "        total_tokens = self.estimate_tokens(total_text)\n",
    "        \n",
    "        scenarios = {\n",
    "            'cross_file_analysis': {\n",
    "                'description': 'Analyze dependencies and interactions across multiple files',\n",
    "                'requires_full_context': True,\n",
    "                'example_query': 'How does main.py interact with data_processor.py and what are the data flow patterns?',\n",
    "                'context_needed': total_tokens\n",
    "            },\n",
    "            'refactoring_analysis': {\n",
    "                'description': 'Identify refactoring opportunities across the entire codebase',\n",
    "                'requires_full_context': True,\n",
    "                'example_query': 'What duplicate code patterns exist across modules and how can they be consolidated?',\n",
    "                'context_needed': total_tokens\n",
    "            },\n",
    "            'bug_analysis': {\n",
    "                'description': 'Trace bugs that span multiple files and modules',\n",
    "                'requires_full_context': True,\n",
    "                'example_query': 'If there\\'s a data corruption issue, trace the data flow from input to output',\n",
    "                'context_needed': total_tokens\n",
    "            },\n",
    "            'architecture_review': {\n",
    "                'description': 'Understand overall system architecture and design patterns',\n",
    "                'requires_full_context': True,\n",
    "                'example_query': 'Describe the overall architecture and suggest improvements',\n",
    "                'context_needed': total_tokens\n",
    "            },\n",
    "            'single_file_analysis': {\n",
    "                'description': 'Analyze individual files in isolation',\n",
    "                'requires_full_context': False,\n",
    "                'example_query': 'Explain the DataProcessor class implementation',\n",
    "                'context_needed': self.estimate_tokens(codebase['data_processor.py'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_files': len(codebase),\n",
    "            'scenarios': scenarios\n",
    "        }\n",
    "    \n",
    "    def evaluate_context_requirements(self, analysis: Dict[str, Any]) -> Dict[str, bool]:\n",
    "        \"\"\"Evaluate which scenarios can be handled with different context limits\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for scenario_name, scenario in analysis['scenarios'].items():\n",
    "            context_needed = scenario['context_needed']\n",
    "            \n",
    "            results[scenario_name] = {\n",
    "                'can_handle_original': context_needed <= self.context_limits['original'],\n",
    "                'can_handle_yarn': context_needed <= self.context_limits['yarn_extended'],\n",
    "                'context_needed': context_needed,\n",
    "                'requires_full_context': scenario['requires_full_context']\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Demonstrate long context code analysis\n",
    "print(\"💻 Long Context Code Understanding Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "analyzer = LongContextCodeAnalyzer()\n",
    "\n",
    "# Generate large codebase\n",
    "print(\"🏗️ Generating large codebase example...\")\n",
    "large_codebase = analyzer.generate_large_codebase_example(num_files=15)\n",
    "\n",
    "# Analyze scenarios\n",
    "analysis = analyzer.analyze_codebase_scenarios(large_codebase)\n",
    "context_eval = analyzer.evaluate_context_requirements(analysis)\n",
    "\n",
    "print(f\"\\n📊 Codebase Statistics:\")\n",
    "print(f\"   Total files: {analysis['total_files']}\")\n",
    "print(f\"   Total tokens: {analysis['total_tokens']:,}\")\n",
    "print(f\"   Size vs 16K limit: {analysis['total_tokens'] / analyzer.context_limits['original']:.1f}x\")\n",
    "print(f\"   Size vs 128K limit: {analysis['total_tokens'] / analyzer.context_limits['yarn_extended']:.1f}x\")\n",
    "\n",
    "print(f\"\\n🔍 Scenario Analysis:\")\n",
    "print(f\"{'Scenario':<25} {'Tokens':<8} {'16K OK':<8} {'128K OK':<8} {'Full Context':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for scenario_name, eval_result in context_eval.items():\n",
    "    tokens = eval_result['context_needed']\n",
    "    can_16k = \"✅\" if eval_result['can_handle_original'] else \"❌\"\n",
    "    can_128k = \"✅\" if eval_result['can_handle_yarn'] else \"❌\"\n",
    "    full_context = \"Yes\" if eval_result['requires_full_context'] else \"No\"\n",
    "    \n",
    "    print(f\"{scenario_name:<25} {tokens:<8,} {can_16k:<8} {can_128k:<8} {full_context:<12}\")\n",
    "\n",
    "# Visualize context requirements\n",
    "def visualize_context_benefits():\n",
    "    \"\"\"Visualize benefits of YARN context extension\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Context limits comparison\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    scenarios = list(context_eval.keys())\n",
    "    tokens_needed = [context_eval[s]['context_needed'] for s in scenarios]\n",
    "    \n",
    "    bars = ax1.barh(scenarios, tokens_needed, alpha=0.7)\n",
    "    \n",
    "    # Add context limit lines\n",
    "    ax1.axvline(x=analyzer.context_limits['original'], color='red', \n",
    "               linestyle='--', linewidth=2, label='16K Limit (Original)')\n",
    "    ax1.axvline(x=analyzer.context_limits['yarn_extended'], color='green', \n",
    "               linestyle='--', linewidth=2, label='128K Limit (YARN)')\n",
    "    \n",
    "    # Color bars based on feasibility\n",
    "    for i, (bar, scenario) in enumerate(zip(bars, scenarios)):\n",
    "        if context_eval[scenario]['can_handle_original']:\n",
    "            bar.set_color('lightgreen')\n",
    "        elif context_eval[scenario]['can_handle_yarn']:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('lightcoral')\n",
    "    \n",
    "    ax1.set_xlabel('Tokens Required')\n",
    "    ax1.set_title('Context Requirements by Scenario')\n",
    "    ax1.legend()\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Capability comparison\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    capabilities = ['16K Context', '128K Context']\n",
    "    scenario_counts = {\n",
    "        '16K Context': sum(1 for s in context_eval.values() if s['can_handle_original']),\n",
    "        '128K Context': sum(1 for s in context_eval.values() if s['can_handle_yarn'])\n",
    "    }\n",
    "    \n",
    "    bars = ax2.bar(capabilities, [scenario_counts[cap] for cap in capabilities], \n",
    "                   color=['red', 'green'], alpha=0.7)\n",
    "    \n",
    "    ax2.set_ylabel('Scenarios Supported')\n",
    "    ax2.set_title('Scenarios Supported by Context Length')\n",
    "    ax2.set_ylim(0, len(scenarios))\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, cap in zip(bars, capabilities):\n",
    "        height = bar.get_height()\n",
    "        total = len(scenarios)\n",
    "        percentage = height / total * 100\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}/{total}\\n({percentage:.0f}%)', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_context_benefits()\n",
    "\n",
    "print(f\"\\n🚀 YARN Context Extension Benefits:\")\n",
    "original_capable = sum(1 for s in context_eval.values() if s['can_handle_original'])\n",
    "yarn_capable = sum(1 for s in context_eval.values() if s['can_handle_yarn'])\n",
    "total_scenarios = len(context_eval)\n",
    "\n",
    "print(f\"• 16K context handles: {original_capable}/{total_scenarios} scenarios ({original_capable/total_scenarios:.0%})\")\n",
    "print(f\"• 128K context handles: {yarn_capable}/{total_scenarios} scenarios ({yarn_capable/total_scenarios:.0%})\")\n",
    "print(f\"• YARN enables {yarn_capable - original_capable} additional complex scenarios\")\n",
    "print(f\"• Critical for: cross-file analysis, refactoring, architecture review\")\n",
    "print(f\"• 8x context extension unlocks repository-level understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Summary & Key Takeaways\n",
    "\n",
    "### 📋 YARN Deep Dive Summary\n",
    "\n",
    "1. **RoPE Foundation**: Rotary embeddings enable relative position encoding\n",
    "2. **YARN Innovation**: Frequency-selective scaling for context extension\n",
    "3. **DeepSeek-V2 Config**: s=40, α=1, β=32 for 8x extension (16K→128K)\n",
    "4. **Training Strategy**: Two-stage gradual extension with batch size adjustment\n",
    "5. **Performance**: >95% accuracy on \"Needle in Haystack\" across all context lengths\n",
    "6. **Applications**: Repository-level code understanding, cross-file analysis\n",
    "\n",
    "### 🔬 Research Impact\n",
    "\n",
    "YARN enables practical long-context applications in code intelligence:\n",
    "- **Cross-file dependency analysis**\n",
    "- **Large-scale refactoring**\n",
    "- **Architecture-level understanding**\n",
    "- **End-to-end bug tracing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final YARN implementation summary\n",
    "def create_yarn_summary_dashboard():\n",
    "    \"\"\"Create comprehensive YARN summary dashboard\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Context extension progression\n",
    "    ax1 = axes[0, 0]\n",
    "    models = ['GPT-3', 'Original\\nRoPE', 'DeepSeek-V2\\n(YARN)']\n",
    "    context_lengths = [2, 16, 128]  # K tokens\n",
    "    \n",
    "    bars = ax1.bar(models, context_lengths, color=['gray', 'blue', 'green'], alpha=0.7)\n",
    "    ax1.set_ylabel('Context Length (K tokens)')\n",
    "    ax1.set_title('Context Length Evolution')\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    for bar, length in zip(bars, context_lengths):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "                f'{length}K', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. YARN frequency modification visualization\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    dim_ratios = np.linspace(0, 1, 100)\n",
    "    alpha, beta = 1.0, 32.0\n",
    "    scale = 40.0\n",
    "    \n",
    "    # YARN scaling function\n",
    "    scaling_factors = np.ones_like(dim_ratios)\n",
    "    \n",
    "    # Low frequency: full scaling\n",
    "    low_freq_mask = dim_ratios < alpha\n",
    "    scaling_factors[low_freq_mask] = 1.0 / scale\n",
    "    \n",
    "    # Mid frequency: interpolation\n",
    "    mid_freq_mask = (dim_ratios >= alpha) & (dim_ratios < beta)\n",
    "    for i, ratio in enumerate(dim_ratios):\n",
    "        if mid_freq_mask[i]:\n",
    "            interp_factor = (ratio - alpha) / (beta - alpha)\n",
    "            scaling_factors[i] = (1.0 / scale) * (1 - interp_factor) + 1.0 * interp_factor\n",
    "    \n",
    "    ax2.plot(dim_ratios * 100, scaling_factors, linewidth=3, color='red')\n",
    "    ax2.axvline(alpha * 100, color='green', linestyle='--', alpha=0.7, label=f'α={alpha}')\n",
    "    ax2.axvline(beta * 100, color='orange', linestyle='--', alpha=0.7, label=f'β={beta}')\n",
    "    ax2.set_xlabel('Frequency Dimension (%)')\n",
    "    ax2.set_ylabel('Scaling Factor')\n",
    "    ax2.set_title('YARN Frequency Scaling (s=40)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Memory scaling comparison\n",
    "    ax3 = axes[0, 2]\n",
    "    \n",
    "    seq_lengths = [1, 2, 4, 8, 16, 32, 64, 128]  # K tokens\n",
    "    memory_quadratic = [s**2 for s in seq_lengths]  # O(n²) attention\n",
    "    memory_linear = [s for s in seq_lengths]        # Hypothetical O(n)\n",
    "    \n",
    "    ax3.plot(seq_lengths, memory_quadratic, 'r-o', linewidth=2, label='Standard Attention O(n²)')\n",
    "    ax3.plot(seq_lengths, memory_linear, 'g--s', linewidth=2, label='Linear Attention O(n)')\n",
    "    \n",
    "    ax3.set_xlabel('Sequence Length (K tokens)')\n",
    "    ax3.set_ylabel('Relative Memory Usage')\n",
    "    ax3.set_title('Memory Scaling Challenges')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Training stages\n",
    "    ax4 = axes[1, 0]\n",
    "    \n",
    "    stages = ['Pre-training\\n(16K)', 'Stage 1\\n(32K)', 'Stage 2\\n(128K)']\n",
    "    context_lens = [16, 32, 128]\n",
    "    batch_sizes = [2048, 1152, 288]  # Adjusted for memory\n",
    "    \n",
    "    ax4_twin = ax4.twinx()\n",
    "    \n",
    "    bars = ax4.bar(stages, context_lens, alpha=0.7, color='skyblue', label='Context Length')\n",
    "    line = ax4_twin.plot(stages, batch_sizes, 'ro-', linewidth=2, markersize=8, label='Batch Size')\n",
    "    \n",
    "    ax4.set_ylabel('Context Length (K)', color='blue')\n",
    "    ax4_twin.set_ylabel('Batch Size', color='red')\n",
    "    ax4.set_title('YARN Training Strategy')\n",
    "    \n",
    "    # 5. Performance on different tasks\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    tasks = ['Code\\nGeneration', 'Long Context\\nQA', 'Repository\\nAnalysis', 'Bug\\nTracing']\n",
    "    original_perf = [90, 60, 30, 25]  # Hypothetical performance without YARN\n",
    "    yarn_perf = [90, 95, 90, 85]     # With YARN\n",
    "    \n",
    "    x = np.arange(len(tasks))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax5.bar(x - width/2, original_perf, width, label='16K Context', alpha=0.7, color='red')\n",
    "    bars2 = ax5.bar(x + width/2, yarn_perf, width, label='128K Context (YARN)', alpha=0.7, color='green')\n",
    "    \n",
    "    ax5.set_ylabel('Performance (%)')\n",
    "    ax5.set_title('Task Performance: 16K vs 128K Context')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(tasks)\n",
    "    ax5.legend()\n",
    "    ax5.set_ylim(0, 100)\n",
    "    \n",
    "    # 6. Key achievements\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    achievements = [\n",
    "        '🎯 8x Context Extension (16K → 128K)',\n",
    "        '📊 >95% Needle-in-Haystack Accuracy',\n",
    "        '🧵 YARN Frequency-Selective Scaling',\n",
    "        '🏋️ Two-Stage Training Strategy',\n",
    "        '💻 Repository-Level Understanding',\n",
    "        '🔍 Cross-File Dependency Analysis',\n",
    "        '⚡ Efficient Implementation',\n",
    "        '🚀 State-of-the-Art Results'\n",
    "    ]\n",
    "    \n",
    "    ax6.text(0.05, 0.95, 'YARN Key Achievements:', fontsize=14, fontweight='bold', \n",
    "             transform=ax6.transAxes)\n",
    "    \n",
    "    for i, achievement in enumerate(achievements):\n",
    "        ax6.text(0.05, 0.85 - i*0.1, achievement, fontsize=11, \n",
    "                transform=ax6.transAxes)\n",
    "    \n",
    "    plt.suptitle('YARN Context Extension: Complete Technical Deep Dive', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Technical specifications\n",
    "    print(\"🧵 YARN Technical Specifications:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📊 Scale Factor (s): 40.0\")\n",
    "    print(f\"📊 Alpha (α): 1.0 (low frequency threshold)\")\n",
    "    print(f\"📊 Beta (β): 32.0 (high frequency threshold)\")\n",
    "    print(f\"📊 Context Extension: 16K → 128K (8x)\")\n",
    "    print(f\"📊 Training: 2-stage (32K → 128K)\")\n",
    "    print(f\"📊 Performance: >95% accuracy maintained\")\n",
    "    \n",
    "    print(\"\\n💡 Implementation Insights:\")\n",
    "    print(\"• Low frequencies get full scaling (preserve global structure)\")\n",
    "    print(\"• High frequencies unchanged (preserve fine details)\")\n",
    "    print(\"• Interpolation in middle frequencies (smooth transition)\")\n",
    "    print(\"• Attention scaling prevents distribution collapse\")\n",
    "    print(\"• Two-stage training enables gradual adaptation\")\n",
    "\n",
    "create_yarn_summary_dashboard()\n",
    "\n",
    "print(\"\\n🎉 YARN Context Extension Deep Dive Complete!\")\n",
    "print(\"\\n📚 Further Reading:\")\n",
    "print(\"• YaRN: Efficient Context Window Extension (Peng et al., 2023)\")\n",
    "print(\"• RoFormer: Enhanced Transformer with Rotary Position Embedding\")\n",
    "print(\"• DeepSeek-V2: A Strong, Economical, and Efficient MoE LLM\")\n",
    "print(\"• Long Range Arena: A Benchmark for Efficient Transformers\")\n",
    "print(\"\\n✨ Next: Explore Fill-In-the-Middle Training! ✨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}