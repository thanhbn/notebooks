{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Error Traceback and Analysis in CoderGen\n",
    "\n",
    "## Mục tiêu học tập\n",
    "\n",
    "Notebook này tập trung vào một trong những khái niệm quan trọng nhất của CoderGen framework: **Error Traceback and Analysis** (Section 3.1). Chúng ta sẽ:\n",
    "\n",
    "1. Hiểu cách CoderGen phân tích lỗi từ code execution\n",
    "2. Implement error analysis mechanism với pattern matching\n",
    "3. Xây dựng suggestion system để fix errors\n",
    "4. Tạo feedback loop giữa error analysis và code generation\n",
    "\n",
    "## Trích xuất từ paper\n",
    "\n",
    "**Section 3.1: Error Traceback and Analysis** (trang 5-6)\n",
    "- \"The CoderGen framework includes a robust error traceback and analysis mechanism\"\n",
    "- \"The system captures the error traceback, which provides a detailed record of the path through the code that led to the failure\"\n",
    "- Figure 3 shows error traceback analyze example\n",
    "- \"The error analysis component leverages the fine-tuned language model to interpret the error messages and suggest potential fixes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lý thuyết: Error Analysis trong Code Generation\n",
    "\n",
    "### 1.1. Types of Errors in AI Code Generation\n",
    "\n",
    "Khi generate code cho AI tasks, có nhiều loại lỗi có thể xảy ra:\n",
    "\n",
    "1. **Syntax Errors**: Lỗi cú pháp Python\n",
    "2. **Import Errors**: Missing hoặc incorrect library imports\n",
    "3. **API Errors**: Wrong API usage, deprecated methods\n",
    "4. **Runtime Errors**: Type mismatches, null pointers, resource errors\n",
    "5. **Logic Errors**: Code runs nhưng output sai\n",
    "\n",
    "### 1.2. Error Traceback Components\n",
    "\n",
    "Python traceback chứa các thông tin quan trọng:\n",
    "- **Error Type**: Class của exception (e.g., ValueError, ImportError)\n",
    "- **Error Message**: Mô tả cụ thể về lỗi\n",
    "- **Stack Trace**: Call stack showing execution path\n",
    "- **Line Numbers**: Exact location của error\n",
    "\n",
    "### 1.3. Analysis Strategy\n",
    "\n",
    "$$\\text{Error Fix} = f(\\text{Error Type}, \\text{Context}, \\text{Domain Knowledge})$$\n",
    "\n",
    "Trong đó:\n",
    "- Error Type giúp classify approach\n",
    "- Context từ code xung quanh\n",
    "- Domain Knowledge về AI libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import traceback\n",
    "import sys\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import json\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Classification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorCategory(Enum):\n",
    "    \"\"\"Categories of errors in AI code generation\"\"\"\n",
    "    SYNTAX = \"syntax_error\"\n",
    "    IMPORT = \"import_error\"\n",
    "    API_USAGE = \"api_usage_error\"\n",
    "    RUNTIME = \"runtime_error\"\n",
    "    LOGIC = \"logic_error\"\n",
    "    RESOURCE = \"resource_error\"\n",
    "    TYPE = \"type_error\"\n",
    "\n",
    "@dataclass\n",
    "class ErrorInfo:\n",
    "    \"\"\"Structured error information\"\"\"\n",
    "    error_type: str\n",
    "    error_message: str\n",
    "    file_path: str\n",
    "    line_number: int\n",
    "    code_context: List[str]\n",
    "    stack_trace: List[Dict[str, Any]]\n",
    "    category: ErrorCategory\n",
    "    \n",
    "@dataclass\n",
    "class ErrorFix:\n",
    "    \"\"\"Suggested fix for an error\"\"\"\n",
    "    description: str\n",
    "    code_changes: List[Dict[str, str]]  # [{\"old\": \"...\", \"new\": \"...\"}]\n",
    "    confidence: float\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Parser Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorParser:\n",
    "    \"\"\"Parse Python error tracebacks into structured format\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Regex patterns for parsing tracebacks\n",
    "        self.traceback_pattern = re.compile(r'File \"(.+?)\", line (\\d+), in (.+)')\n",
    "        self.error_pattern = re.compile(r'^(\\w+Error|\\w+Exception): (.+)$', re.MULTILINE)\n",
    "        \n",
    "    def parse_traceback(self, traceback_str: str) -> ErrorInfo:\n",
    "        \"\"\"Parse a traceback string into ErrorInfo\"\"\"\n",
    "        lines = traceback_str.strip().split('\\n')\n",
    "        \n",
    "        # Extract error type and message\n",
    "        error_match = self.error_pattern.search(traceback_str)\n",
    "        if error_match:\n",
    "            error_type = error_match.group(1)\n",
    "            error_message = error_match.group(2)\n",
    "        else:\n",
    "            error_type = \"UnknownError\"\n",
    "            error_message = lines[-1] if lines else \"Unknown error\"\n",
    "        \n",
    "        # Parse stack trace\n",
    "        stack_trace = []\n",
    "        for line in lines:\n",
    "            match = self.traceback_pattern.search(line)\n",
    "            if match:\n",
    "                stack_trace.append({\n",
    "                    \"file\": match.group(1),\n",
    "                    \"line\": int(match.group(2)),\n",
    "                    \"function\": match.group(3)\n",
    "                })\n",
    "        \n",
    "        # Get the most recent error location\n",
    "        if stack_trace:\n",
    "            latest_error = stack_trace[-1]\n",
    "            file_path = latest_error[\"file\"]\n",
    "            line_number = latest_error[\"line\"]\n",
    "        else:\n",
    "            file_path = \"unknown\"\n",
    "            line_number = 0\n",
    "        \n",
    "        # Extract code context (simplified)\n",
    "        code_context = self._extract_code_context(traceback_str)\n",
    "        \n",
    "        # Categorize error\n",
    "        category = self._categorize_error(error_type, error_message)\n",
    "        \n",
    "        return ErrorInfo(\n",
    "            error_type=error_type,\n",
    "            error_message=error_message,\n",
    "            file_path=file_path,\n",
    "            line_number=line_number,\n",
    "            code_context=code_context,\n",
    "            stack_trace=stack_trace,\n",
    "            category=category\n",
    "        )\n",
    "    \n",
    "    def _extract_code_context(self, traceback_str: str) -> List[str]:\n",
    "        \"\"\"Extract code lines from traceback\"\"\"\n",
    "        context = []\n",
    "        lines = traceback_str.split('\\n')\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip().startswith(\"    \") and not line.strip().startswith(\"    ^\"):\n",
    "                context.append(line.strip())\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _categorize_error(self, error_type: str, error_message: str) -> ErrorCategory:\n",
    "        \"\"\"Categorize error based on type and message\"\"\"\n",
    "        error_type_lower = error_type.lower()\n",
    "        error_message_lower = error_message.lower()\n",
    "        \n",
    "        if \"syntax\" in error_type_lower:\n",
    "            return ErrorCategory.SYNTAX\n",
    "        elif \"import\" in error_type_lower or \"module\" in error_message_lower:\n",
    "            return ErrorCategory.IMPORT\n",
    "        elif \"type\" in error_type_lower:\n",
    "            return ErrorCategory.TYPE\n",
    "        elif any(api in error_message_lower for api in [\"pipeline\", \"model\", \"tokenizer\"]):\n",
    "            return ErrorCategory.API_USAGE\n",
    "        elif any(resource in error_message_lower for resource in [\"file\", \"path\", \"directory\"]):\n",
    "            return ErrorCategory.RESOURCE\n",
    "        else:\n",
    "            return ErrorCategory.RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AI-Powered Error Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIErrorAnalyzer:\n",
    "    \"\"\"Analyze errors and suggest fixes using AI knowledge\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Knowledge base of common errors and fixes\n",
    "        self.error_patterns = self._build_error_patterns()\n",
    "        self.api_knowledge = self._build_api_knowledge()\n",
    "    \n",
    "    def analyze_error(self, error_info: ErrorInfo, original_code: str) -> List[ErrorFix]:\n",
    "        \"\"\"Analyze error and suggest fixes\"\"\"\n",
    "        fixes = []\n",
    "        \n",
    "        # Strategy based on error category\n",
    "        if error_info.category == ErrorCategory.IMPORT:\n",
    "            fixes.extend(self._analyze_import_error(error_info, original_code))\n",
    "        elif error_info.category == ErrorCategory.API_USAGE:\n",
    "            fixes.extend(self._analyze_api_error(error_info, original_code))\n",
    "        elif error_info.category == ErrorCategory.TYPE:\n",
    "            fixes.extend(self._analyze_type_error(error_info, original_code))\n",
    "        elif error_info.category == ErrorCategory.RESOURCE:\n",
    "            fixes.extend(self._analyze_resource_error(error_info, original_code))\n",
    "        else:\n",
    "            fixes.extend(self._analyze_generic_error(error_info, original_code))\n",
    "        \n",
    "        # Sort by confidence\n",
    "        fixes.sort(key=lambda x: x.confidence, reverse=True)\n",
    "        \n",
    "        return fixes\n",
    "    \n",
    "    def _build_error_patterns(self) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Build knowledge base of error patterns\"\"\"\n",
    "        return {\n",
    "            \"ImportError\": [\n",
    "                {\n",
    "                    \"pattern\": \"No module named 'transformers'\",\n",
    "                    \"fix\": \"pip install transformers\",\n",
    "                    \"code_change\": None\n",
    "                },\n",
    "                {\n",
    "                    \"pattern\": \"cannot import name 'pipeline'\",\n",
    "                    \"fix\": \"from transformers import pipeline\",\n",
    "                    \"code_change\": {\"old\": \"import pipeline\", \"new\": \"from transformers import pipeline\"}\n",
    "                }\n",
    "            ],\n",
    "            \"NameError\": [\n",
    "                {\n",
    "                    \"pattern\": \"name 'pipeline' is not defined\",\n",
    "                    \"fix\": \"Import pipeline from transformers\",\n",
    "                    \"code_change\": {\"add_import\": \"from transformers import pipeline\"}\n",
    "                }\n",
    "            ],\n",
    "            \"TypeError\": [\n",
    "                {\n",
    "                    \"pattern\": \"expected str, bytes or os.PathLike object\",\n",
    "                    \"fix\": \"Convert input to string\",\n",
    "                    \"code_change\": {\"wrap\": \"str()\"}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def _build_api_knowledge(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Build API usage knowledge base\"\"\"\n",
    "        return {\n",
    "            \"pipeline\": {\n",
    "                \"correct_usage\": \"pipeline('task-name', model='model-name')\",\n",
    "                \"common_tasks\": [\"text-classification\", \"sentiment-analysis\", \"ner\", \"question-answering\"],\n",
    "                \"required_imports\": [\"from transformers import pipeline\"]\n",
    "            },\n",
    "            \"AutoModel\": {\n",
    "                \"correct_usage\": \"AutoModel.from_pretrained('model-name')\",\n",
    "                \"variants\": [\"AutoModelForSequenceClassification\", \"AutoModelForTokenClassification\"],\n",
    "                \"required_imports\": [\"from transformers import AutoModel\"]\n",
    "            },\n",
    "            \"torch.load\": {\n",
    "                \"correct_usage\": \"torch.load('path/to/model.pt')\",\n",
    "                \"common_errors\": [\"map_location\", \"pickle errors\"],\n",
    "                \"required_imports\": [\"import torch\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _analyze_import_error(self, error_info: ErrorInfo, code: str) -> List[ErrorFix]:\n",
    "        \"\"\"Analyze import errors and suggest fixes\"\"\"\n",
    "        fixes = []\n",
    "        \n",
    "        # Check for missing module\n",
    "        if \"No module named\" in error_info.error_message:\n",
    "            module_match = re.search(r\"No module named '([^']+)'\", error_info.error_message)\n",
    "            if module_match:\n",
    "                module_name = module_match.group(1)\n",
    "                \n",
    "                # Map common module names to packages\n",
    "                package_mapping = {\n",
    "                    \"transformers\": \"transformers\",\n",
    "                    \"torch\": \"torch\",\n",
    "                    \"torchvision\": \"torchvision\",\n",
    "                    \"PIL\": \"pillow\",\n",
    "                    \"cv2\": \"opencv-python\"\n",
    "                }\n",
    "                \n",
    "                package = package_mapping.get(module_name, module_name)\n",
    "                \n",
    "                fixes.append(ErrorFix(\n",
    "                    description=f\"Install missing package '{package}'\",\n",
    "                    code_changes=[{\n",
    "                        \"old\": \"\",\n",
    "                        \"new\": f\"# Add to requirements: pip install {package}\"\n",
    "                    }],\n",
    "                    confidence=0.95,\n",
    "                    explanation=f\"The module '{module_name}' is not installed. Install it using pip.\"\n",
    "                ))\n",
    "        \n",
    "        # Check for incorrect import syntax\n",
    "        if \"cannot import name\" in error_info.error_message:\n",
    "            name_match = re.search(r\"cannot import name '([^']+)'\", error_info.error_message)\n",
    "            if name_match:\n",
    "                import_name = name_match.group(1)\n",
    "                \n",
    "                # Suggest correct import based on API knowledge\n",
    "                for api_name, api_info in self.api_knowledge.items():\n",
    "                    if import_name == api_name:\n",
    "                        fixes.append(ErrorFix(\n",
    "                            description=f\"Use correct import for {import_name}\",\n",
    "                            code_changes=[{\n",
    "                                \"old\": f\"import {import_name}\",\n",
    "                                \"new\": api_info[\"required_imports\"][0]\n",
    "                            }],\n",
    "                            confidence=0.9,\n",
    "                            explanation=f\"The correct way to import {import_name} is: {api_info['required_imports'][0]}\"\n",
    "                        ))\n",
    "        \n",
    "        return fixes\n",
    "    \n",
    "    def _analyze_api_error(self, error_info: ErrorInfo, code: str) -> List[ErrorFix]:\n",
    "        \"\"\"Analyze API usage errors\"\"\"\n",
    "        fixes = []\n",
    "        \n",
    "        # Check for pipeline errors\n",
    "        if \"pipeline\" in error_info.error_message:\n",
    "            # Common pipeline error: wrong task name\n",
    "            if \"Unknown task\" in error_info.error_message or \"not a valid\" in error_info.error_message:\n",
    "                fixes.append(ErrorFix(\n",
    "                    description=\"Use valid pipeline task name\",\n",
    "                    code_changes=[{\n",
    "                        \"old\": \"pipeline('text-classification')\",\n",
    "                        \"new\": \"pipeline('sentiment-analysis')  # or 'text-classification' with model\"\n",
    "                    }],\n",
    "                    confidence=0.85,\n",
    "                    explanation=\"Pipeline requires valid task names. Common tasks: sentiment-analysis, ner, question-answering\"\n",
    "                ))\n",
    "            \n",
    "            # Missing model specification\n",
    "            if \"model\" in error_info.error_message:\n",
    "                fixes.append(ErrorFix(\n",
    "                    description=\"Specify model for pipeline\",\n",
    "                    code_changes=[{\n",
    "                        \"old\": \"pipeline('task-name')\",\n",
    "                        \"new\": \"pipeline('task-name', model='model-name')\"\n",
    "                    }],\n",
    "                    confidence=0.8,\n",
    "                    explanation=\"Some tasks require explicit model specification\"\n",
    "                ))\n",
    "        \n",
    "        # Check for model loading errors\n",
    "        if \"from_pretrained\" in error_info.error_message:\n",
    "            fixes.append(ErrorFix(\n",
    "                description=\"Check model name and availability\",\n",
    "                code_changes=[{\n",
    "                    \"old\": \"AutoModel.from_pretrained('model-name')\",\n",
    "                    \"new\": \"AutoModel.from_pretrained('bert-base-uncased')  # Use valid model name\"\n",
    "                }],\n",
    "                confidence=0.75,\n",
    "                explanation=\"Ensure the model name exists on HuggingFace Hub\"\n",
    "            ))\n",
    "        \n",
    "        return fixes\n",
    "    \n",
    "    def _analyze_type_error(self, error_info: ErrorInfo, code: str) -> List[ErrorFix]:\n",
    "        \"\"\"Analyze type errors\"\"\"\n",
    "        fixes = []\n",
    "        \n",
    "        # String conversion errors\n",
    "        if \"expected str\" in error_info.error_message:\n",
    "            fixes.append(ErrorFix(\n",
    "                description=\"Convert input to string\",\n",
    "                code_changes=[{\n",
    "                    \"old\": \"function(variable)\",\n",
    "                    \"new\": \"function(str(variable))\"\n",
    "                }],\n",
    "                confidence=0.8,\n",
    "                explanation=\"The function expects a string input. Use str() to convert.\"\n",
    "            ))\n",
    "        \n",
    "        # List/tensor conversion\n",
    "        if \"expected Tensor\" in error_info.error_message:\n",
    "            fixes.append(ErrorFix(\n",
    "                description=\"Convert to PyTorch tensor\",\n",
    "                code_changes=[{\n",
    "                    \"old\": \"model(data)\",\n",
    "                    \"new\": \"model(torch.tensor(data))\"\n",
    "                }],\n",
    "                confidence=0.85,\n",
    "                explanation=\"PyTorch models expect tensor inputs\"\n",
    "            ))\n",
    "        \n",
    "        return fixes\n",
    "    \n",
    "    def _analyze_resource_error(self, error_info: ErrorInfo, code: str) -> List[ErrorFix]:\n",
    "        \"\"\"Analyze resource/file errors\"\"\"\n",
    "        fixes = []\n",
    "        \n",
    "        if \"FileNotFoundError\" in error_info.error_type or \"No such file\" in error_info.error_message:\n",
    "            fixes.append(ErrorFix(\n",
    "                description=\"Check file path and create if needed\",\n",
    "                code_changes=[{\n",
    "                    \"old\": \"open('file.txt')\",\n",
    "                    \"new\": \"\"\"import os\n",
    "if not os.path.exists('file.txt'):\n",
    "    # Create file or download from URL\n",
    "    raise FileNotFoundError('Please provide the file')\n",
    "open('file.txt')\"\"\"\n",
    "                }],\n",
    "                confidence=0.9,\n",
    "                explanation=\"The file doesn't exist. Check the path or create it.\"\n",
    "            ))\n",
    "        \n",
    "        return fixes\n",
    "    \n",
    "    def _analyze_generic_error(self, error_info: ErrorInfo, code: str) -> List[ErrorFix]:\n",
    "        \"\"\"Generic error analysis\"\"\"\n",
    "        fixes = []\n",
    "        \n",
    "        # Try to find pattern matches\n",
    "        for error_type, patterns in self.error_patterns.items():\n",
    "            if error_type in error_info.error_type:\n",
    "                for pattern in patterns:\n",
    "                    if pattern[\"pattern\"] in error_info.error_message:\n",
    "                        fix = ErrorFix(\n",
    "                            description=pattern[\"fix\"],\n",
    "                            code_changes=[pattern.get(\"code_change\", {\"old\": \"\", \"new\": \"\"})],\n",
    "                            confidence=0.7,\n",
    "                            explanation=\"Based on error pattern matching\"\n",
    "                        )\n",
    "                        fixes.append(fix)\n",
    "        \n",
    "        # If no specific fixes found, suggest generic debugging\n",
    "        if not fixes:\n",
    "            fixes.append(ErrorFix(\n",
    "                description=\"Add error handling and debugging\",\n",
    "                code_changes=[{\n",
    "                    \"old\": \"# Your code\",\n",
    "                    \"new\": \"\"\"try:\n",
    "    # Your code\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\"\"\"\n",
    "                }],\n",
    "                confidence=0.5,\n",
    "                explanation=\"Add try-except block for better error handling\"\n",
    "            ))\n",
    "        \n",
    "        return fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error-Driven Code Regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeRegenerator:\n",
    "    \"\"\"Regenerate code based on error analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer: AIErrorAnalyzer):\n",
    "        self.analyzer = analyzer\n",
    "        self.regeneration_history = []\n",
    "    \n",
    "    def regenerate_with_fixes(self, original_code: str, error_info: ErrorInfo, \n",
    "                             task_description: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"Regenerate code incorporating error fixes\"\"\"\n",
    "        \n",
    "        # Get suggested fixes\n",
    "        fixes = self.analyzer.analyze_error(error_info, original_code)\n",
    "        \n",
    "        if not fixes:\n",
    "            return original_code, [\"No fixes suggested\"]\n",
    "        \n",
    "        # Apply top fixes\n",
    "        modified_code = original_code\n",
    "        applied_fixes = []\n",
    "        \n",
    "        for fix in fixes[:3]:  # Apply top 3 fixes\n",
    "            if fix.confidence > 0.7:\n",
    "                modified_code = self._apply_fix(modified_code, fix)\n",
    "                applied_fixes.append(fix.description)\n",
    "        \n",
    "        # Generate improved prompt for LLM\n",
    "        improved_prompt = self._generate_improved_prompt(\n",
    "            task_description, error_info, fixes[0]\n",
    "        )\n",
    "        \n",
    "        # Track regeneration\n",
    "        self.regeneration_history.append({\n",
    "            \"error\": error_info,\n",
    "            \"fixes_applied\": applied_fixes,\n",
    "            \"original_code\": original_code,\n",
    "            \"modified_code\": modified_code\n",
    "        })\n",
    "        \n",
    "        return modified_code, applied_fixes\n",
    "    \n",
    "    def _apply_fix(self, code: str, fix: ErrorFix) -> str:\n",
    "        \"\"\"Apply a specific fix to the code\"\"\"\n",
    "        modified_code = code\n",
    "        \n",
    "        for change in fix.code_changes:\n",
    "            if \"old\" in change and \"new\" in change:\n",
    "                modified_code = modified_code.replace(change[\"old\"], change[\"new\"])\n",
    "            elif \"add_import\" in change:\n",
    "                # Add import at the beginning\n",
    "                import_line = change[\"add_import\"]\n",
    "                if import_line not in modified_code:\n",
    "                    lines = modified_code.split('\\n')\n",
    "                    # Find where to insert import\n",
    "                    import_idx = 0\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if line.startswith('import') or line.startswith('from'):\n",
    "                            import_idx = i + 1\n",
    "                    lines.insert(import_idx, import_line)\n",
    "                    modified_code = '\\n'.join(lines)\n",
    "        \n",
    "        return modified_code\n",
    "    \n",
    "    def _generate_improved_prompt(self, task: str, error: ErrorInfo, \n",
    "                                 fix: ErrorFix) -> str:\n",
    "        \"\"\"Generate improved prompt based on error analysis\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Task: {task}\n",
    "\n",
    "Previous attempt resulted in {error.error_type}: {error.error_message}\n",
    "\n",
    "Please generate code that:\n",
    "1. {fix.description}\n",
    "2. {fix.explanation}\n",
    "3. Includes proper error handling\n",
    "4. Uses correct API calls for the libraries\n",
    "\n",
    "Make sure to:\n",
    "- Import all necessary modules\n",
    "- Handle edge cases\n",
    "- Follow the library's best practices\n",
    "\"\"\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Error Analysis Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "parser = ErrorParser()\n",
    "analyzer = AIErrorAnalyzer()\n",
    "regenerator = CodeRegenerator(analyzer)\n",
    "\n",
    "# Example 1: Import Error\n",
    "print(\"=== Example 1: Import Error ===\")\n",
    "traceback_1 = \"\"\"Traceback (most recent call last):\n",
    "  File \"test.py\", line 3, in <module>\n",
    "    classifier = pipeline('text-classification')\n",
    "NameError: name 'pipeline' is not defined\"\"\"\n",
    "\n",
    "code_1 = \"\"\"# Text classification example\n",
    "def classify_text(text):\n",
    "    classifier = pipeline('text-classification')\n",
    "    return classifier(text)\n",
    "\"\"\"\n",
    "\n",
    "error_info_1 = parser.parse_traceback(traceback_1)\n",
    "print(f\"Error Type: {error_info_1.error_type}\")\n",
    "print(f\"Category: {error_info_1.category.value}\")\n",
    "print(f\"Line Number: {error_info_1.line_number}\")\n",
    "\n",
    "fixes_1 = analyzer.analyze_error(error_info_1, code_1)\n",
    "print(f\"\\nSuggested Fixes ({len(fixes_1)}):\")\n",
    "for i, fix in enumerate(fixes_1[:3]):\n",
    "    print(f\"{i+1}. {fix.description} (confidence: {fix.confidence:.2f})\")\n",
    "    print(f\"   Explanation: {fix.explanation}\")\n",
    "\n",
    "# Apply fixes\n",
    "fixed_code_1, applied_1 = regenerator.regenerate_with_fixes(\n",
    "    code_1, error_info_1, \"Create text classification function\"\n",
    ")\n",
    "print(\"\\nFixed Code:\")\n",
    "print(fixed_code_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: API Usage Error\n",
    "print(\"\\n=== Example 2: API Usage Error ===\")\n",
    "traceback_2 = \"\"\"Traceback (most recent call last):\n",
    "  File \"test.py\", line 5, in <module>\n",
    "    model = AutoModel.from_pretrained('invalid-model-xyz')\n",
    "  File \"/transformers/modeling_utils.py\", line 123, in from_pretrained\n",
    "    raise EnvironmentError(f\"Model {model_name} not found\")\n",
    "EnvironmentError: Model invalid-model-xyz not found\"\"\"\n",
    "\n",
    "code_2 = \"\"\"from transformers import AutoModel\n",
    "\n",
    "def load_model():\n",
    "    # Load pretrained model\n",
    "    model = AutoModel.from_pretrained('invalid-model-xyz')\n",
    "    return model\n",
    "\"\"\"\n",
    "\n",
    "error_info_2 = parser.parse_traceback(traceback_2)\n",
    "fixes_2 = analyzer.analyze_error(error_info_2, code_2)\n",
    "\n",
    "print(f\"Error: {error_info_2.error_type} - {error_info_2.error_message}\")\n",
    "print(f\"\\nTop Fix: {fixes_2[0].description}\")\n",
    "print(f\"Code Change: {fixes_2[0].code_changes[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Type Error\n",
    "print(\"\\n=== Example 3: Type Error ===\")\n",
    "traceback_3 = \"\"\"Traceback (most recent call last):\n",
    "  File \"test.py\", line 8, in process_image\n",
    "    output = model(image_array)\n",
    "TypeError: forward() expected Tensor but got numpy.ndarray\"\"\"\n",
    "\n",
    "code_3 = \"\"\"import numpy as np\n",
    "import torch\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Load and process image\n",
    "    image_array = np.random.rand(224, 224, 3)\n",
    "    model = load_model()\n",
    "    output = model(image_array)\n",
    "    return output\n",
    "\"\"\"\n",
    "\n",
    "error_info_3 = parser.parse_traceback(traceback_3)\n",
    "fixes_3 = analyzer.analyze_error(error_info_3, code_3)\n",
    "\n",
    "fixed_code_3, applied_3 = regenerator.regenerate_with_fixes(\n",
    "    code_3, error_info_3, \"Process image with model\"\n",
    ")\n",
    "\n",
    "print(f\"Applied fixes: {applied_3}\")\n",
    "print(\"\\nModified section:\")\n",
    "print(\"output = model(torch.tensor(image_array))  # Convert to tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis Statistics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate error analysis on multiple code samples\n",
    "error_samples = [\n",
    "    {\"type\": \"NameError\", \"category\": ErrorCategory.IMPORT, \"fixed\": True, \"iterations\": 1},\n",
    "    {\"type\": \"ImportError\", \"category\": ErrorCategory.IMPORT, \"fixed\": True, \"iterations\": 1},\n",
    "    {\"type\": \"TypeError\", \"category\": ErrorCategory.TYPE, \"fixed\": True, \"iterations\": 2},\n",
    "    {\"type\": \"EnvironmentError\", \"category\": ErrorCategory.API_USAGE, \"fixed\": True, \"iterations\": 2},\n",
    "    {\"type\": \"FileNotFoundError\", \"category\": ErrorCategory.RESOURCE, \"fixed\": True, \"iterations\": 1},\n",
    "    {\"type\": \"RuntimeError\", \"category\": ErrorCategory.RUNTIME, \"fixed\": False, \"iterations\": 3},\n",
    "    {\"type\": \"ValueError\", \"category\": ErrorCategory.RUNTIME, \"fixed\": True, \"iterations\": 2},\n",
    "    {\"type\": \"AttributeError\", \"category\": ErrorCategory.API_USAGE, \"fixed\": True, \"iterations\": 2},\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "error_df = pd.DataFrame(error_samples)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Error categories distribution\n",
    "ax1 = axes[0, 0]\n",
    "category_counts = error_df['category'].apply(lambda x: x.value).value_counts()\n",
    "ax1.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
    "ax1.set_title('Distribution of Error Categories')\n",
    "\n",
    "# 2. Fix success rate by category\n",
    "ax2 = axes[0, 1]\n",
    "fix_rate = error_df.groupby('category')['fixed'].mean()\n",
    "ax2.bar([c.value for c in fix_rate.index], fix_rate.values)\n",
    "ax2.set_xlabel('Error Category')\n",
    "ax2.set_ylabel('Fix Success Rate')\n",
    "ax2.set_title('Fix Success Rate by Category')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Iterations needed for fix\n",
    "ax3 = axes[1, 0]\n",
    "fixed_errors = error_df[error_df['fixed']]\n",
    "ax3.hist(fixed_errors['iterations'], bins=range(1, 5), edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Iterations Needed')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Number of Iterations to Fix Errors')\n",
    "\n",
    "# 4. Error type frequency\n",
    "ax4 = axes[1, 1]\n",
    "error_type_counts = error_df['type'].value_counts()\n",
    "ax4.barh(error_type_counts.index, error_type_counts.values)\n",
    "ax4.set_xlabel('Frequency')\n",
    "ax4.set_ylabel('Error Type')\n",
    "ax4.set_title('Frequency of Error Types')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building Error Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorKnowledgeBase:\n",
    "    \"\"\"Maintain knowledge base of errors and successful fixes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.knowledge = defaultdict(list)\n",
    "        self.success_patterns = []\n",
    "    \n",
    "    def add_error_resolution(self, error_info: ErrorInfo, fix: ErrorFix, success: bool):\n",
    "        \"\"\"Add error-fix pair to knowledge base\"\"\"\n",
    "        entry = {\n",
    "            \"error_pattern\": f\"{error_info.error_type}: {error_info.error_message}\",\n",
    "            \"category\": error_info.category.value,\n",
    "            \"fix_description\": fix.description,\n",
    "            \"fix_confidence\": fix.confidence,\n",
    "            \"success\": success,\n",
    "            \"timestamp\": pd.Timestamp.now()\n",
    "        }\n",
    "        \n",
    "        self.knowledge[error_info.category].append(entry)\n",
    "        \n",
    "        if success:\n",
    "            self.success_patterns.append({\n",
    "                \"pattern\": error_info.error_message,\n",
    "                \"fix\": fix\n",
    "            })\n",
    "    \n",
    "    def get_similar_fixes(self, error_info: ErrorInfo, threshold: float = 0.7) -> List[ErrorFix]:\n",
    "        \"\"\"Find similar successful fixes from knowledge base\"\"\"\n",
    "        similar_fixes = []\n",
    "        \n",
    "        # Search in same category\n",
    "        category_knowledge = self.knowledge.get(error_info.category, [])\n",
    "        \n",
    "        for entry in category_knowledge:\n",
    "            if entry[\"success\"] and self._similarity(error_info.error_message, \n",
    "                                                    entry[\"error_pattern\"]) > threshold:\n",
    "                similar_fixes.append({\n",
    "                    \"fix\": entry[\"fix_description\"],\n",
    "                    \"confidence\": entry[\"fix_confidence\"] * 0.9,  # Slightly lower confidence\n",
    "                    \"source\": \"knowledge_base\"\n",
    "                })\n",
    "        \n",
    "        return similar_fixes\n",
    "    \n",
    "    def _similarity(self, str1: str, str2: str) -> float:\n",
    "        \"\"\"Simple string similarity (Jaccard similarity)\"\"\"\n",
    "        set1 = set(str1.lower().split())\n",
    "        set2 = set(str2.lower().split())\n",
    "        \n",
    "        if not set1 or not set2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        \n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate knowledge base report\"\"\"\n",
    "        report = [\"Error Knowledge Base Report\"]\n",
    "        report.append(\"=\" * 40)\n",
    "        \n",
    "        for category, entries in self.knowledge.items():\n",
    "            success_count = sum(1 for e in entries if e[\"success\"])\n",
    "            total_count = len(entries)\n",
    "            \n",
    "            report.append(f\"\\n{category.value}:\")\n",
    "            report.append(f\"  Total errors: {total_count}\")\n",
    "            report.append(f\"  Successfully fixed: {success_count}\")\n",
    "            report.append(f\"  Success rate: {success_count/total_count*100:.1f}%\")\n",
    "        \n",
    "        report.append(f\"\\nTotal unique fix patterns: {len(self.success_patterns)}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Demo knowledge base\n",
    "kb = ErrorKnowledgeBase()\n",
    "\n",
    "# Add some sample resolutions\n",
    "sample_errors = [\n",
    "    (error_info_1, fixes_1[0], True),\n",
    "    (error_info_2, fixes_2[0], True),\n",
    "    (error_info_3, fixes_3[0], True),\n",
    "]\n",
    "\n",
    "for error, fix, success in sample_errors:\n",
    "    kb.add_error_resolution(error, fix, success)\n",
    "\n",
    "print(kb.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Error Analysis là Core Component**:\n",
    "   - Không chỉ detect error mà còn understand root cause\n",
    "   - Categorization giúp apply targeted fixes\n",
    "   - Pattern matching với knowledge base tăng efficiency\n",
    "\n",
    "2. **Multi-level Analysis**:\n",
    "   - Syntax level: Parse traceback structure\n",
    "   - Semantic level: Understand error meaning\n",
    "   - Domain level: Apply AI library knowledge\n",
    "\n",
    "3. **Iterative Improvement**:\n",
    "   - Each error provides learning opportunity\n",
    "   - Knowledge base grows over time\n",
    "   - Fix confidence improves with experience\n",
    "\n",
    "4. **Integration with Code Generation**:\n",
    "   - Error analysis feeds back to prompt improvement\n",
    "   - Specific fixes guide code regeneration\n",
    "   - Reduces iterations needed for working code\n",
    "\n",
    "5. **Practical Applications**:\n",
    "   - Can be extended to any programming domain\n",
    "   - Knowledge base can be shared across projects\n",
    "   - Enables automated debugging assistance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}