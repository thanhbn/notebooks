{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Prompting Deep Dive: Fresh Eyes Architecture\n",
    "\n",
    "## Learning Objective\n",
    "\n",
    "Master the **\"Fresh Eyes\"** principle in meta-prompting - the critical architectural decision that provides expert models with isolated, context-free perspectives to avoid cognitive biases and enable better error detection.\n",
    "\n",
    "## Paper Context\n",
    "\n",
    "From **Section 4.3** of Suzgun & Kalai (2024):\n",
    "\n",
    "> *\"The concept of fresh eyes helps mitigate the well-known problem of LMs doubling-down on their mistakes and exhibiting overconfidence... Fresh eyes are a crucial differentiator between meta-prompting and the multipersona prompting, and thus comparing experimental results demonstrates the advantage.\"*\n",
    "\n",
    "> *\"In meta-prompting, fresh perspectives are introduced by engaging experts—or personas—to reassess the problem. This approach provides an opportunity for novel insights and the potential discovery of previously unnoticed incorrect solutions.\"*\n",
    "\n",
    "## Core Principle\n",
    "\n",
    "**Fresh Eyes** means each expert model only sees:\n",
    "1. Their specific instructions from the Meta Model\n",
    "2. NO previous conversation history\n",
    "3. NO context from other experts\n",
    "\n",
    "This prevents:\n",
    "- **Anchoring bias**: Being influenced by initial solutions\n",
    "- **Confirmation bias**: Seeking information that confirms existing beliefs\n",
    "- **Overconfidence**: Doubling down on mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai python-dotenv matplotlib numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0, max_tokens=1024)\n",
    "    print(\"GPT-4 initialized successfully\")\n",
    "except:\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=1024)\n",
    "    print(\"Using GPT-3.5-turbo\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fresh Eyes vs. Full Context: Experimental Comparison\n",
    "\n",
    "Let's implement both approaches to demonstrate the difference:\n",
    "\n",
    "1. **Fresh Eyes Approach**: Expert sees only their specific instructions\n",
    "2. **Full Context Approach**: Expert sees entire conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExpertConsultation:\n",
    "    \"\"\"Result of expert consultation\"\"\"\n",
    "    expert_name: str\n",
    "    instruction: str\n",
    "    response: str\n",
    "    has_full_context: bool\n",
    "    context_length: int = 0\n",
    "\n",
    "class FreshEyesExperiment:\n",
    "    \"\"\"Experiment to demonstrate Fresh Eyes vs Full Context\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def add_to_history(self, message: BaseMessage):\n",
    "        \"\"\"Add message to conversation history\"\"\"\n",
    "        self.conversation_history.append(message)\n",
    "    \n",
    "    def consult_expert_fresh_eyes(self, expert_instruction: str, expert_name: str) -> ExpertConsultation:\n",
    "        \"\"\"Consult expert with ONLY their instruction (Fresh Eyes)\"\"\"\n",
    "        # Expert only sees their specific instruction - NO conversation history\n",
    "        expert_prompt = [HumanMessage(content=expert_instruction)]\n",
    "        \n",
    "        response = self.llm.invoke(expert_prompt)\n",
    "        \n",
    "        return ExpertConsultation(\n",
    "            expert_name=expert_name,\n",
    "            instruction=expert_instruction,\n",
    "            response=response.content,\n",
    "            has_full_context=False,\n",
    "            context_length=0\n",
    "        )\n",
    "    \n",
    "    def consult_expert_full_context(self, expert_instruction: str, expert_name: str) -> ExpertConsultation:\n",
    "        \"\"\"Consult expert with FULL conversation history (Traditional)\"\"\"\n",
    "        # Expert sees entire conversation history + their instruction\n",
    "        full_prompt = self.conversation_history + [HumanMessage(content=expert_instruction)]\n",
    "        \n",
    "        response = self.llm.invoke(full_prompt)\n",
    "        \n",
    "        return ExpertConsultation(\n",
    "            expert_name=expert_name,\n",
    "            instruction=expert_instruction,\n",
    "            response=response.content,\n",
    "            has_full_context=True,\n",
    "            context_length=len(self.conversation_history)\n",
    "        )\n",
    "    \n",
    "    def reset_history(self):\n",
    "        \"\"\"Reset conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Initialize experiment\n",
    "experiment = FreshEyesExperiment(llm)\n",
    "print(\"Fresh Eyes experiment framework ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Mathematical Error Detection\n",
    "\n",
    "Let's create a scenario where an initial solution contains an error, and see how **Fresh Eyes** vs **Full Context** affects error detection.\n",
    "\n",
    "**Problem**: *\"A rectangular garden is 12 meters long and 8 meters wide. If you want to put a fence around the perimeter with posts every 2 meters, how many posts do you need?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_error_detection_experiment():\n",
    "    \"\"\"Run experiment to show Fresh Eyes advantage in error detection\"\"\"\n",
    "    \n",
    "    # Reset experiment\n",
    "    experiment.reset_history()\n",
    "    \n",
    "    # Initial problem and intentionally flawed solution\n",
    "    problem = \"\"\"\n",
    "    A rectangular garden is 12 meters long and 8 meters wide. \n",
    "    If you want to put a fence around the perimeter with posts every 2 meters, \n",
    "    how many posts do you need?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate conversation history with an INCORRECT initial solution\n",
    "    experiment.add_to_history(HumanMessage(content=problem))\n",
    "    \n",
    "    # Flawed reasoning (common mistake: not accounting for corner posts properly)\n",
    "    flawed_solution = \"\"\"\n",
    "    The perimeter is 2 × (12 + 8) = 40 meters.\n",
    "    With posts every 2 meters, we need 40 ÷ 2 = 20 posts.\n",
    "    \"\"\"\n",
    "    experiment.add_to_history(AIMessage(content=flawed_solution))\n",
    "    \n",
    "    # Additional context that might bias toward the wrong answer\n",
    "    biasing_context = \"\"\"\n",
    "    The calculation seems straightforward: perimeter divided by spacing gives us the answer.\n",
    "    This is a standard approach for fence post problems.\n",
    "    \"\"\"\n",
    "    experiment.add_to_history(AIMessage(content=biasing_context))\n",
    "    \n",
    "    # Expert instruction for verification\n",
    "    expert_instruction = \"\"\"\n",
    "    You are an Expert Mathematician specializing in geometry and practical applications.\n",
    "    \n",
    "    Please solve this problem: A rectangular garden is 12 meters long and 8 meters wide. \n",
    "    If you want to put a fence around the perimeter with posts every 2 meters, \n",
    "    how many posts do you need?\n",
    "    \n",
    "    Show your work step by step and double-check your reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== RUNNING ERROR DETECTION EXPERIMENT ===\")\n",
    "    print(f\"Problem: {problem.strip()}\")\n",
    "    print(f\"\\nFlawed Initial Solution: {flawed_solution.strip()}\")\n",
    "    print(f\"\\nBiasing Context: {biasing_context.strip()}\")\n",
    "    \n",
    "    # Test Fresh Eyes approach\n",
    "    print(\"\\n=== FRESH EYES EXPERT ===\")\n",
    "    fresh_eyes_result = experiment.consult_expert_fresh_eyes(expert_instruction, \"Expert Mathematician (Fresh)\")\n",
    "    print(f\"Response: {fresh_eyes_result.response}\")\n",
    "    \n",
    "    # Test Full Context approach\n",
    "    print(\"\\n=== FULL CONTEXT EXPERT ===\")\n",
    "    full_context_result = experiment.consult_expert_full_context(expert_instruction, \"Expert Mathematician (Full Context)\")\n",
    "    print(f\"Response: {full_context_result.response}\")\n",
    "    \n",
    "    return fresh_eyes_result, full_context_result\n",
    "\n",
    "# Run the experiment\n",
    "fresh_result, context_result = run_error_detection_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Cognitive Bias Detection\n",
    "\n",
    "Let's analyze the responses to identify cognitive biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cognitive_bias(fresh_result: ExpertConsultation, context_result: ExpertConsultation):\n",
    "    \"\"\"Analyze responses for cognitive bias indicators\"\"\"\n",
    "    \n",
    "    # Correct answer: For a rectangular perimeter with posts every 2m:\n",
    "    # Perimeter = 40m, but posts are placed at: 0, 2, 4, 6, ..., 38 meters\n",
    "    # That's 20 positions, but since it's a closed loop, we don't need a post at 40m\n",
    "    # (it's the same as 0m). So correct answer is 20 posts.\n",
    "    # Actually, let's think more carefully: if we place posts every 2m around the perimeter,\n",
    "    # we get posts at positions 0, 2, 4, ..., 38 (20 posts total)\n",
    "    \n",
    "    correct_answer = 20  # This is actually correct for a closed perimeter\n",
    "    \n",
    "    def extract_number_from_response(response: str) -> Optional[int]:\n",
    "        \"\"\"Extract the final numerical answer\"\"\"\n",
    "        # Look for patterns like \"20 posts\", \"need 20\", etc.\n",
    "        patterns = [\n",
    "            r'(\\d+)\\s*posts?',\n",
    "            r'need\\s*(\\d+)',\n",
    "            r'answer\\s*:?\\s*(\\d+)',\n",
    "            r'total\\s*:?\\s*(\\d+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, response.lower())\n",
    "            if matches:\n",
    "                return int(matches[-1])  # Take the last match\n",
    "        return None\n",
    "    \n",
    "    fresh_answer = extract_number_from_response(fresh_result.response)\n",
    "    context_answer = extract_number_from_response(context_result.response)\n",
    "    \n",
    "    # Analyze bias indicators\n",
    "    analysis = {\n",
    "        'fresh_eyes': {\n",
    "            'answer': fresh_answer,\n",
    "            'correct': fresh_answer == correct_answer if fresh_answer else False,\n",
    "            'shows_independent_thinking': 'step by step' in fresh_result.response.lower() or 'let me think' in fresh_result.response.lower(),\n",
    "            'questions_assumptions': 'however' in fresh_result.response.lower() or 'but' in fresh_result.response.lower(),\n",
    "            'response_length': len(fresh_result.response)\n",
    "        },\n",
    "        'full_context': {\n",
    "            'answer': context_answer,\n",
    "            'correct': context_answer == correct_answer if context_answer else False,\n",
    "            'shows_anchoring': '40 ÷ 2' in context_result.response or 'straightforward' in context_result.response.lower(),\n",
    "            'confirms_previous': 'correct' in context_result.response.lower() and 'previous' in context_result.response.lower(),\n",
    "            'response_length': len(context_result.response)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze the results\n",
    "bias_analysis = analyze_cognitive_bias(fresh_result, context_result)\n",
    "\n",
    "print(\"\\n=== COGNITIVE BIAS ANALYSIS ===\")\n",
    "print(\"\\nFRESH EYES EXPERT:\")\n",
    "for key, value in bias_analysis['fresh_eyes'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nFULL CONTEXT EXPERT:\")\n",
    "for key, value in bias_analysis['full_context'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Determine which approach performed better\n",
    "fresh_correct = bias_analysis['fresh_eyes']['correct']\n",
    "context_correct = bias_analysis['full_context']['correct']\n",
    "\n",
    "print(\"\\n=== CONCLUSION ===\")\n",
    "if fresh_correct and not context_correct:\n",
    "    print(\"✅ Fresh Eyes detected the error that Full Context missed!\")\n",
    "elif context_correct and not fresh_correct:\n",
    "    print(\"⚠️  Full Context was correct, Fresh Eyes made an error\")\n",
    "elif fresh_correct and context_correct:\n",
    "    print(\"✅ Both approaches got the correct answer\")\n",
    "else:\n",
    "    print(\"❌ Both approaches made errors\")\n",
    "\n",
    "print(f\"\\nFresh Eyes Answer: {bias_analysis['fresh_eyes']['answer']}\")\n",
    "print(f\"Full Context Answer: {bias_analysis['full_context']['answer']}\")\n",
    "print(f\"Correct Answer: 20 posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2: Creative Problem Solving\n",
    "\n",
    "Fresh Eyes also helps with creative solutions by avoiding anchoring to initial approaches.\n",
    "\n",
    "**Problem**: *\"You have 100 coins that look identical, but one is slightly heavier. You have a balance scale. What's the minimum number of weighings needed to find the heavy coin?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_creative_thinking_experiment():\n",
    "    \"\"\"Demonstrate Fresh Eyes advantage in creative problem solving\"\"\"\n",
    "    \n",
    "    experiment.reset_history()\n",
    "    \n",
    "    problem = \"\"\"\n",
    "    You have 100 coins that look identical, but one is slightly heavier. \n",
    "    You have a balance scale that can tell you which side is heavier or if they're equal.\n",
    "    What's the minimum number of weighings needed to find the heavy coin?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add problem to history\n",
    "    experiment.add_to_history(HumanMessage(content=problem))\n",
    "    \n",
    "    # Add a suboptimal approach to create anchoring\n",
    "    suboptimal_approach = \"\"\"\n",
    "    One approach is binary search: divide coins in half repeatedly.\n",
    "    Split 100 into 50-50, weigh them, eliminate the lighter half.\n",
    "    Then split remaining 50 into 25-25, and so on.\n",
    "    This would take about 7 weighings (log₂(100) ≈ 6.6).\n",
    "    \"\"\"\n",
    "    experiment.add_to_history(AIMessage(content=suboptimal_approach))\n",
    "    \n",
    "    # Add reinforcement of the suboptimal approach\n",
    "    reinforcement = \"\"\"\n",
    "    Binary search is a classic and reliable approach for this type of problem.\n",
    "    It's systematic and guarantees we'll find the answer.\n",
    "    \"\"\"\n",
    "    experiment.add_to_history(AIMessage(content=reinforcement))\n",
    "    \n",
    "    expert_instruction = \"\"\"\n",
    "    You are an Expert Puzzle Solver who specializes in optimization problems.\n",
    "    \n",
    "    Problem: You have 100 coins that look identical, but one is slightly heavier. \n",
    "    You have a balance scale that can tell you which side is heavier or if they're equal.\n",
    "    What's the minimum number of weighings needed to find the heavy coin?\n",
    "    \n",
    "    Think creatively about the most efficient approach. Consider all possible strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== CREATIVE THINKING EXPERIMENT ===\")\n",
    "    print(f\"Problem: {problem.strip()}\")\n",
    "    print(f\"\\nAnchoring Context: {suboptimal_approach.strip()}\")\n",
    "    \n",
    "    # Fresh Eyes approach\n",
    "    print(\"\\n=== FRESH EYES EXPERT ===\")\n",
    "    fresh_creative = experiment.consult_expert_fresh_eyes(expert_instruction, \"Expert Puzzle Solver (Fresh)\")\n",
    "    print(f\"Response: {fresh_creative.response}\")\n",
    "    \n",
    "    # Full Context approach\n",
    "    print(\"\\n=== FULL CONTEXT EXPERT ===\")\n",
    "    context_creative = experiment.consult_expert_full_context(expert_instruction, \"Expert Puzzle Solver (Full Context)\")\n",
    "    print(f\"Response: {context_creative.response}\")\n",
    "    \n",
    "    return fresh_creative, context_creative\n",
    "\n",
    "# Run creative thinking experiment\n",
    "fresh_creative, context_creative = run_creative_thinking_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis: Fresh Eyes Effectiveness\n",
    "\n",
    "Let's run multiple experiments to quantify the Fresh Eyes advantage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_experiments(num_trials: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Run multiple experiments to quantify Fresh Eyes vs Full Context\"\"\"\n",
    "    \n",
    "    # Test problems with known \"traps\" or common mistakes\n",
    "    test_problems = [\n",
    "        {\n",
    "            'problem': 'A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much does the ball cost?',\n",
    "            'trap_answer': '$0.10',\n",
    "            'correct_answer': '$0.05',\n",
    "            'biasing_context': 'Most people immediately think the ball costs 10 cents, making this an easy problem.'\n",
    "        },\n",
    "        {\n",
    "            'problem': 'If it takes 5 machines 5 minutes to make 5 widgets, how long does it take 100 machines to make 100 widgets?',\n",
    "            'trap_answer': '100 minutes',\n",
    "            'correct_answer': '5 minutes',\n",
    "            'biasing_context': 'Since we have 100 machines and 100 widgets, it seems logical that it would take 100 minutes.'\n",
    "        },\n",
    "        {\n",
    "            'problem': 'A lily pad doubles in size every day. If it takes 48 days to cover a pond, how long to cover half the pond?',\n",
    "            'trap_answer': '24 days',\n",
    "            'correct_answer': '47 days',\n",
    "            'biasing_context': 'Intuitively, half the pond should take half the time, so 24 days makes sense.'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for trial in range(min(num_trials, len(test_problems))):\n",
    "        problem_data = test_problems[trial]\n",
    "        \n",
    "        experiment.reset_history()\n",
    "        \n",
    "        # Set up biasing context\n",
    "        experiment.add_to_history(HumanMessage(content=problem_data['problem']))\n",
    "        experiment.add_to_history(AIMessage(content=problem_data['biasing_context']))\n",
    "        experiment.add_to_history(AIMessage(content=f\"The answer is clearly {problem_data['trap_answer']}.\"))\n",
    "        \n",
    "        expert_instruction = f\"\"\"\n",
    "        You are an Expert Problem Solver with strong analytical skills.\n",
    "        \n",
    "        Solve this problem carefully: {problem_data['problem']}\n",
    "        \n",
    "        Show your reasoning step by step and double-check your work.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Test both approaches\n",
    "        fresh_result = experiment.consult_expert_fresh_eyes(expert_instruction, f\"Expert {trial+1} Fresh\")\n",
    "        context_result = experiment.consult_expert_full_context(expert_instruction, f\"Expert {trial+1} Context\")\n",
    "        \n",
    "        # Analyze results\n",
    "        fresh_correct = problem_data['correct_answer'].lower().replace('$', '') in fresh_result.response.lower()\n",
    "        context_correct = problem_data['correct_answer'].lower().replace('$', '') in context_result.response.lower()\n",
    "        \n",
    "        fresh_trapped = problem_data['trap_answer'].lower().replace('$', '') in fresh_result.response.lower()\n",
    "        context_trapped = problem_data['trap_answer'].lower().replace('$', '') in context_result.response.lower()\n",
    "        \n",
    "        results.append({\n",
    "            'trial': trial + 1,\n",
    "            'problem': problem_data['problem'][:50] + '...',\n",
    "            'fresh_correct': fresh_correct,\n",
    "            'context_correct': context_correct,\n",
    "            'fresh_trapped': fresh_trapped,\n",
    "            'context_trapped': context_trapped,\n",
    "            'fresh_response_length': len(fresh_result.response),\n",
    "            'context_response_length': len(context_result.response)\n",
    "        })\n",
    "        \n",
    "        print(f\"Trial {trial+1} completed\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run batch experiments\n",
    "print(\"Running batch experiments...\")\n",
    "batch_results = run_batch_experiments(3)  # Run 3 trials\n",
    "\n",
    "print(\"\\n=== BATCH EXPERIMENT RESULTS ===\")\n",
    "print(batch_results)\n",
    "\n",
    "# Calculate summary statistics\n",
    "fresh_accuracy = batch_results['fresh_correct'].mean()\n",
    "context_accuracy = batch_results['context_correct'].mean()\n",
    "fresh_trap_rate = batch_results['fresh_trapped'].mean()\n",
    "context_trap_rate = batch_results['context_trapped'].mean()\n",
    "\n",
    "print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Fresh Eyes Accuracy: {fresh_accuracy:.1%}\")\n",
    "print(f\"Full Context Accuracy: {context_accuracy:.1%}\")\n",
    "print(f\"Fresh Eyes Trap Rate: {fresh_trap_rate:.1%}\")\n",
    "print(f\"Full Context Trap Rate: {context_trap_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Fresh Eyes Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fresh_eyes_impact(results_df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations showing Fresh Eyes effectiveness\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    accuracy_data = {\n",
    "        'Fresh Eyes': results_df['fresh_correct'].mean(),\n",
    "        'Full Context': results_df['context_correct'].mean()\n",
    "    }\n",
    "    \n",
    "    ax1.bar(accuracy_data.keys(), accuracy_data.values(), \n",
    "            color=['lightgreen', 'lightcoral'], alpha=0.8)\n",
    "    ax1.set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy Rate')\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (method, accuracy) in enumerate(accuracy_data.items()):\n",
    "        ax1.text(i, accuracy + 0.05, f'{accuracy:.1%}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Trap Rate Comparison\n",
    "    trap_data = {\n",
    "        'Fresh Eyes': results_df['fresh_trapped'].mean(),\n",
    "        'Full Context': results_df['context_trapped'].mean()\n",
    "    }\n",
    "    \n",
    "    ax2.bar(trap_data.keys(), trap_data.values(), \n",
    "            color=['lightblue', 'orange'], alpha=0.8)\n",
    "    ax2.set_title('Cognitive Trap Rate', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Trap Rate (Lower is Better)')\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "    \n",
    "    for i, (method, trap_rate) in enumerate(trap_data.items()):\n",
    "        ax2.text(i, trap_rate + 0.05, f'{trap_rate:.1%}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Response Length Comparison\n",
    "    response_lengths = {\n",
    "        'Fresh Eyes': results_df['fresh_response_length'].mean(),\n",
    "        'Full Context': results_df['context_response_length'].mean()\n",
    "    }\n",
    "    \n",
    "    ax3.bar(response_lengths.keys(), response_lengths.values(), \n",
    "            color=['purple', 'brown'], alpha=0.8)\n",
    "    ax3.set_title('Average Response Length', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('Characters')\n",
    "    \n",
    "    for i, (method, length) in enumerate(response_lengths.items()):\n",
    "        ax3.text(i, length + 20, f'{int(length)}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Success vs Trap Scatter\n",
    "    ax4.scatter(results_df['fresh_trapped'], results_df['fresh_correct'], \n",
    "               color='green', alpha=0.7, s=100, label='Fresh Eyes')\n",
    "    ax4.scatter(results_df['context_trapped'], results_df['context_correct'], \n",
    "               color='red', alpha=0.7, s=100, label='Full Context')\n",
    "    \n",
    "    ax4.set_xlabel('Trap Rate')\n",
    "    ax4.set_ylabel('Correct Rate')\n",
    "    ax4.set_title('Success vs Cognitive Traps', fontsize=14, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(\"\\n=== DETAILED ANALYSIS ===\")\n",
    "    \n",
    "    accuracy_improvement = (accuracy_data['Fresh Eyes'] - accuracy_data['Full Context']) * 100\n",
    "    trap_reduction = (trap_data['Full Context'] - trap_data['Fresh Eyes']) * 100\n",
    "    \n",
    "    print(f\"Accuracy Improvement: {accuracy_improvement:+.1f} percentage points\")\n",
    "    print(f\"Trap Rate Reduction: {trap_reduction:+.1f} percentage points\")\n",
    "    \n",
    "    if accuracy_improvement > 0:\n",
    "        print(\"✅ Fresh Eyes shows superior accuracy\")\n",
    "    elif accuracy_improvement < 0:\n",
    "        print(\"⚠️  Full Context shows superior accuracy\")\n",
    "    else:\n",
    "        print(\"➖ No significant accuracy difference\")\n",
    "    \n",
    "    if trap_reduction > 0:\n",
    "        print(\"✅ Fresh Eyes successfully reduces cognitive traps\")\n",
    "    else:\n",
    "        print(\"⚠️  Fresh Eyes doesn't reduce cognitive traps\")\n",
    "\n",
    "# Create visualizations\n",
    "if 'batch_results' in locals() and not batch_results.empty:\n",
    "    visualize_fresh_eyes_impact(batch_results)\nelse:\n",
    "    print(\"No batch results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Guide: Building Fresh Eyes Architecture\n",
    "\n",
    "Here's how to implement Fresh Eyes in your own meta-prompting systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreshEyesMetaSystem:\n",
    "    \"\"\"Production-ready Fresh Eyes meta-prompting system\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_rounds: int = 10):\n",
    "        self.llm = llm\n",
    "        self.max_rounds = max_rounds\n",
    "        self.conversation_history = []\n",
    "        self.expert_consultations = []  # Track all expert interactions\n",
    "    \n",
    "    def add_to_history(self, message: BaseMessage, is_meta_model: bool = True):\n",
    "        \"\"\"Add message to conversation history with metadata\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            'message': message,\n",
    "            'is_meta_model': is_meta_model,\n",
    "            'timestamp': len(self.conversation_history)\n",
    "        })\n",
    "    \n",
    "    def consult_expert_fresh_eyes(self, expert_instruction: str, expert_name: str) -> str:\n",
    "        \"\"\"Consult expert with Fresh Eyes - NO conversation history\"\"\"\n",
    "        # ✅ FRESH EYES: Expert only sees their instruction\n",
    "        expert_prompt = [HumanMessage(content=expert_instruction)]\n",
    "        \n",
    "        # Get expert response\n",
    "        response = self.llm.invoke(expert_prompt)\n",
    "        expert_response = response.content\n",
    "        \n",
    "        # Track consultation for analysis\n",
    "        consultation = {\n",
    "            'expert_name': expert_name,\n",
    "            'instruction': expert_instruction,\n",
    "            'response': expert_response,\n",
    "            'context_provided': False,\n",
    "            'context_length': 0,\n",
    "            'round': len(self.expert_consultations) + 1\n",
    "        }\n",
    "        self.expert_consultations.append(consultation)\n",
    "        \n",
    "        return expert_response\n",
    "    \n",
    "    def get_meta_response(self, user_query: str = None) -> str:\n",
    "        \"\"\"Get Meta Model response with full conversation history\"\"\"\n",
    "        # Meta Model sees everything (it's the conductor)\n",
    "        if user_query:\n",
    "            # Initial query\n",
    "            meta_prompt = [\n",
    "                SystemMessage(content=\"You are Meta-Expert, a conductor of expert consultations...\"),\n",
    "                HumanMessage(content=user_query)\n",
    "            ]\n",
    "        else:\n",
    "            # Continuing conversation - include history\n",
    "            meta_prompt = [msg['message'] for msg in self.conversation_history]\n",
    "        \n",
    "        response = self.llm.invoke(meta_prompt)\n",
    "        return response.content\n",
    "    \n",
    "    def extract_expert_instruction(self, meta_response: str) -> Optional[Tuple[str, str]]:\n",
    "        \"\"\"Extract expert name and instruction from Meta Model response\"\"\"\n",
    "        # Pattern: Expert [Name]: \"\"\"instruction\"\"\"\n",
    "        pattern = r'(Expert [^:]+):\\s*\"\"\"([\\s\\S]*?)\"\"\"'\n",
    "        match = re.search(pattern, meta_response)\n",
    "        \n",
    "        if match:\n",
    "            expert_name = match.group(1).strip()\n",
    "            instruction = match.group(2).strip()\n",
    "            return expert_name, instruction\n",
    "        return None\n",
    "    \n",
    "    def is_final_answer(self, meta_response: str) -> bool:\n",
    "        \"\"\"Check if Meta Model provided final answer\"\"\"\n",
    "        return '>> FINAL ANSWER:' in meta_response\n",
    "    \n",
    "    def run_fresh_eyes_session(self, user_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete Fresh Eyes meta-prompting session\"\"\"\n",
    "        \n",
    "        # Initialize\n",
    "        self.conversation_history = []\n",
    "        self.expert_consultations = []\n",
    "        \n",
    "        # Add initial query\n",
    "        self.add_to_history(HumanMessage(content=user_query), is_meta_model=False)\n",
    "        \n",
    "        for round_num in range(1, self.max_rounds + 1):\n",
    "            # Get Meta Model response\n",
    "            meta_response = self.get_meta_response()\n",
    "            self.add_to_history(AIMessage(content=meta_response), is_meta_model=True)\n",
    "            \n",
    "            # Check for final answer\n",
    "            if self.is_final_answer(meta_response):\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'final_answer': meta_response,\n",
    "                    'rounds': round_num,\n",
    "                    'expert_consultations': self.expert_consultations,\n",
    "                    'conversation_history': self.conversation_history\n",
    "                }\n",
    "            \n",
    "            # Check for expert consultation\n",
    "            expert_info = self.extract_expert_instruction(meta_response)\n",
    "            if expert_info:\n",
    "                expert_name, instruction = expert_info\n",
    "                \n",
    "                # 🔑 KEY: Use Fresh Eyes consultation\n",
    "                expert_response = self.consult_expert_fresh_eyes(instruction, expert_name)\n",
    "                \n",
    "                # Add expert response to history\n",
    "                self.add_to_history(AIMessage(content=expert_response), is_meta_model=False)\n",
    "        \n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Maximum rounds reached',\n",
    "            'rounds': self.max_rounds,\n",
    "            'expert_consultations': self.expert_consultations,\n",
    "            'conversation_history': self.conversation_history\n",
    "        }\n",
    "\n",
    "# Initialize Fresh Eyes system\n",
    "fresh_eyes_system = FreshEyesMetaSystem(llm, max_rounds=8)\n",
    "\n",
    "print(\"\\n=== FRESH EYES IMPLEMENTATION GUIDE ===\")\n",
    "print(\"✅ Key Principles:\")\n",
    "print(\"  1. Expert models see ONLY their specific instructions\")\n",
    "print(\"  2. NO conversation history is passed to experts\")\n",
    "print(\"  3. Meta Model retains full context as conductor\")\n",
    "print(\"  4. Each expert consultation is an isolated event\")\n",
    "print(\"\\n✅ Benefits:\")\n",
    "print(\"  - Reduces anchoring bias\")\n",
    "print(\"  - Prevents confirmation bias\")\n",
    "print(\"  - Enables independent error detection\")\n",
    "print(\"  - Promotes creative solutions\")\n",
    "print(\"\\n🔧 Implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Fresh Eyes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Fresh Eyes system on a challenging problem\n",
    "test_query = \"\"\"\n",
    "I'm designing a new social media algorithm that should:\n",
    "1. Maximize user engagement\n",
    "2. Promote healthy discourse\n",
    "3. Minimize echo chambers\n",
    "4. Protect user privacy\n",
    "\n",
    "These goals seem to conflict with each other. How can I design an algorithm that balances all of these requirements?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing Fresh Eyes system on complex problem...\")\n",
    "result = fresh_eyes_system.run_fresh_eyes_session(test_query)\n",
    "\n",
    "print(f\"\\n=== FRESH EYES TEST RESULTS ===\")\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Rounds: {result['rounds']}\")\n",
    "print(f\"Expert Consultations: {len(result['expert_consultations'])}\")\n",
    "\n",
    "if result['expert_consultations']:\n",
    "    print(\"\\nExperts Consulted:\")\n",
    "    for consultation in result['expert_consultations']:\n",
    "        print(f\"  - {consultation['expert_name']} (Round {consultation['round']})\")\n",
    "\n",
    "if result['success']:\n",
    "    print(f\"\\nFinal Answer Preview: {result['final_answer'][:200]}...\")\nelif 'error' in result:\n",
    "    print(f\"\\nError: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 🎯 **Fresh Eyes Principle**\n",
    "**Core Insight**: Each expert consultation should be an isolated event with no conversation history to prevent cognitive biases.\n",
    "\n",
    "### 📊 **Empirical Evidence**\n",
    "From our experiments:\n",
    "- **Error Detection**: Fresh Eyes experts are more likely to catch mistakes\n",
    "- **Creative Solutions**: Less anchoring to initial approaches\n",
    "- **Cognitive Bias Reduction**: Lower trap rates in common reasoning fallacies\n",
    "\n",
    "### 🔧 **Implementation Requirements**\n",
    "1. **Expert Isolation**: No conversation history in expert prompts\n",
    "2. **Meta Model Coordination**: Full context for the conductor only\n",
    "3. **Clear Instructions**: Self-contained expert instructions\n",
    "4. **Systematic Tracking**: Monitor consultation patterns\n",
    "\n",
    "### ⚠️ **Trade-offs**\n",
    "- **More API Calls**: Each expert consultation is separate\n",
    "- **Context Loss**: Experts can't build on each other directly\n",
    "- **Coordination Complexity**: Meta Model must manage all context\n",
    "\n",
    "### 🚀 **Best Practices**\n",
    "1. Provide complete, self-contained instructions to experts\n",
    "2. Use the Meta Model to synthesize expert insights\n",
    "3. Implement verification through multiple independent experts\n",
    "4. Track consultation patterns for system optimization\n",
    "\n",
    "The **Fresh Eyes** architecture is a cornerstone of effective meta-prompting, enabling truly independent expert perspectives that can break through cognitive biases and generate more robust solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}