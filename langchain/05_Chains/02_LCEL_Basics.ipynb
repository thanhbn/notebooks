{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL) Basics\n",
    "\n",
    "## LCEL l√† g√¨?\n",
    "\n",
    "**LangChain Expression Language (LCEL)** l√† ng√¥n ng·ªØ bi·ªÉu th·ª©c declarative ƒë·ªÉ compose chains m·ªôt c√°ch d·ªÖ d√†ng v√† hi·ªáu qu·∫£. LCEL ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:\n",
    "\n",
    "### üéØ **L·ª£i √≠ch ch√≠nh c·ªßa LCEL**\n",
    "\n",
    "1. **üîÑ Streaming Support**: Automatic streaming c·ªßa intermediate steps\n",
    "2. **‚ö° Async Support**: Native async/await support\n",
    "3. **üîç Optimized Parallel Execution**: T·ª± ƒë·ªông t·ªëi ∆∞u parallel operations\n",
    "4. **üõ†Ô∏è Retries v√† Fallbacks**: Built-in retry logic v√† fallback mechanisms\n",
    "5. **üìä Traceability**: Detailed tracing v√† debugging information\n",
    "6. **üîß Flexibility**: Easy composition v√† modification\n",
    "\n",
    "### üîó **Pipe Operator (`|`)**\n",
    "LCEL s·ª≠ d·ª•ng pipe operator ƒë·ªÉ chain components:\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "```\n",
    "\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a:\n",
    "- Output c·ªßa `component1` becomes input c·ªßa `component2`\n",
    "- Output c·ªßa `component2` becomes input c·ªßa `component3`\n",
    "- K·∫øt qu·∫£ cu·ªëi c√πng l√† output c·ªßa `component3`\n",
    "\n",
    "### üß© **Core Runnable Components**\n",
    "- **RunnablePassthrough**: Pass input through unchanged\n",
    "- **RunnableParallel**: Execute multiple runnables in parallel\n",
    "- **RunnableLambda**: Wrap functions as runnables\n",
    "- **RunnableBranch**: Conditional execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup v√† Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    RunnableLambda,\n",
    "    RunnableBranch,\n",
    "    Runnable\n",
    ")\n",
    "\n",
    "# LangChain Anthropic\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Pydantic cho structured output\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import t·∫•t c·∫£ dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o ChatAnthropic\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0.7,\n",
    "    anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "# Test LLM\n",
    "test_response = llm.invoke(\"Ch√†o b·∫°n! LCEL l√† g√¨?\")\n",
    "print(\"‚úÖ LLM ready\")\n",
    "print(f\"Test response: {test_response.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LCEL Syntax Basics - Pipe Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL basic chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Vi·∫øt m·ªôt b√†i th∆° ng·∫Øn v·ªÅ {topic}. Style: {style}\"\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LCEL chain v·ªõi pipe operator\n",
    "lcel_chain = prompt | llm | parser\n",
    "\n",
    "print(\"‚úÖ LCEL chain created: prompt | llm | parser\")\n",
    "print(f\"Chain type: {type(lcel_chain)}\")\n",
    "\n",
    "# Test basic chain\n",
    "result = lcel_chain.invoke({\n",
    "    \"topic\": \"m∆∞a thu\", \n",
    "    \"style\": \"tr·ªØ t√¨nh, nh·∫π nh√†ng\"\n",
    "})\n",
    "\n",
    "print(f\"\\nüìù Poem result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh LCEL v·ªõi traditional approach\n",
    "def traditional_chain(input_data):\n",
    "    \"\"\"Traditional way without LCEL\"\"\"\n",
    "    # Step 1: Format prompt\n",
    "    formatted_prompt = prompt.format_messages(**input_data)\n",
    "    \n",
    "    # Step 2: Call LLM\n",
    "    llm_response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Step 3: Parse output\n",
    "    parsed_result = parser.parse(llm_response.content)\n",
    "    \n",
    "    return parsed_result\n",
    "\n",
    "# Test both approaches\n",
    "test_input = {\"topic\": \"n√∫i r·ª´ng\", \"style\": \"h√πng tr√°ng, m·∫°nh m·∫Ω\"}\n",
    "\n",
    "print(\"=== Comparison: LCEL vs Traditional ===\")\n",
    "\n",
    "# LCEL approach\n",
    "start_time = time.time()\n",
    "lcel_result = lcel_chain.invoke(test_input)\n",
    "lcel_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüîó LCEL Result ({lcel_time:.2f}s):\")\n",
    "print(lcel_result[:150] + \"...\")\n",
    "\n",
    "# Traditional approach\n",
    "start_time = time.time()\n",
    "traditional_result = traditional_chain(test_input)\n",
    "traditional_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüîß Traditional Result ({traditional_time:.2f}s):\")\n",
    "print(traditional_result[:150] + \"...\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance: LCEL c√≥ th·ªÉ optimize parallel operations internally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RunnablePassthrough - Pass Through Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough ƒë·ªÉ preserve original input\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Simple passthrough example\n",
    "print(\"=== RunnablePassthrough Basics ===\")\n",
    "\n",
    "# Test passthrough\n",
    "passthrough = RunnablePassthrough()\n",
    "test_data = {\"name\": \"Alice\", \"age\": 25, \"city\": \"Hanoi\"}\n",
    "\n",
    "passthrough_result = passthrough.invoke(test_data)\n",
    "print(f\"Input: {test_data}\")\n",
    "print(f\"Passthrough output: {passthrough_result}\")\n",
    "print(f\"Same object? {test_data is passthrough_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnablePassthrough.assign ƒë·ªÉ add computed values\n",
    "def compute_description(data):\n",
    "    \"\"\"Compute description t·ª´ input data\"\"\"\n",
    "    return f\"{data['name']} l√† ng∆∞·ªùi {data['age']} tu·ªïi s·ªëng t·∫°i {data['city']}\"\n",
    "\n",
    "def compute_category(data):\n",
    "    \"\"\"Compute age category\"\"\"\n",
    "    age = data['age']\n",
    "    if age < 18:\n",
    "        return \"Tr·∫ª em\"\n",
    "    elif age < 65:\n",
    "        return \"Ng∆∞·ªùi l·ªõn\"\n",
    "    else:\n",
    "        return \"Ng∆∞·ªùi cao tu·ªïi\"\n",
    "\n",
    "# Chain v·ªõi RunnablePassthrough.assign\n",
    "enrich_data_chain = RunnablePassthrough.assign(\n",
    "    description=RunnableLambda(compute_description),\n",
    "    category=RunnableLambda(compute_category)\n",
    ")\n",
    "\n",
    "print(\"\\n=== RunnablePassthrough.assign ===\")\n",
    "\n",
    "# Test assign\n",
    "enriched_result = enrich_data_chain.invoke(test_data)\n",
    "print(f\"Original data: {test_data}\")\n",
    "print(f\"Enriched data: {enriched_result}\")\n",
    "\n",
    "# Show what was added\n",
    "print(f\"\\nüìù Added fields:\")\n",
    "print(f\"- description: {enriched_result['description']}\")\n",
    "print(f\"- category: {enriched_result['category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use case: Preserve context trong LLM chain\n",
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"D·ª±a tr√™n th√¥ng tin sau, vi·∫øt m·ªôt introduction ng·∫Øn:\n",
    "    \n",
    "    T√™n: {name}\n",
    "    Tu·ªïi: {age}\n",
    "    Th√†nh ph·ªë: {city}\n",
    "    M√¥ t·∫£: {description}\n",
    "    Nh√≥m tu·ªïi: {category}\n",
    "    \n",
    "    Introduction:\"\"\"\n",
    ")\n",
    "\n",
    "# Chain preserves input + adds LLM output\n",
    "context_preserving_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        description=RunnableLambda(compute_description),\n",
    "        category=RunnableLambda(compute_category)\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        introduction=context_prompt | llm | StrOutputParser()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n=== Context Preserving Chain ===\")\n",
    "\n",
    "# Test v·ªõi multiple people\n",
    "people = [\n",
    "    {\"name\": \"Minh\", \"age\": 28, \"city\": \"Ho Chi Minh\"},\n",
    "    {\"name\": \"Lan\", \"age\": 35, \"city\": \"Da Nang\"},\n",
    "    {\"name\": \"Duc\", \"age\": 45, \"city\": \"Hanoi\"}\n",
    "]\n",
    "\n",
    "for person in people:\n",
    "    result = context_preserving_chain.invoke(person)\n",
    "    print(f\"\\nüë§ {person['name']}:\")\n",
    "    print(f\"üìã Category: {result['category']}\")\n",
    "    print(f\"üìù Introduction: {result['introduction']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RunnableParallel - Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableParallel ƒë·ªÉ execute multiple runnables c√πng l√∫c\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Different analysis prompts\n",
    "sentiment_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Ph√¢n t√≠ch c·∫£m x√∫c c·ªßa ƒëo·∫°n text sau (Positive/Negative/Neutral): {text}\"\n",
    ")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"T√≥m t·∫Øt ƒëo·∫°n text sau trong 1 c√¢u: {text}\"\n",
    ")\n",
    "\n",
    "keywords_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tr√≠ch xu·∫•t 3 t·ª´ kh√≥a ch√≠nh t·ª´ ƒëo·∫°n text sau: {text}\"\n",
    ")\n",
    "\n",
    "# Parallel analysis chain\n",
    "parallel_analysis = RunnableParallel(\n",
    "    sentiment=sentiment_prompt | llm | StrOutputParser(),\n",
    "    summary=summary_prompt | llm | StrOutputParser(),\n",
    "    keywords=keywords_prompt | llm | StrOutputParser(),\n",
    "    original=RunnablePassthrough()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Parallel analysis chain created\")\n",
    "print(\"Components: sentiment, summary, keywords, original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parallel execution\n",
    "sample_texts = [\n",
    "    \"H√¥m nay tr·ªùi ƒë·∫πp qu√°! T√¥i r·∫•t vui v√¨ ƒë∆∞·ª£c ƒëi d·∫°o c√¥ng vi√™n v·ªõi gia ƒë√¨nh. Kh√¥ng kh√≠ trong l√†nh v√† m·ªçi ng∆∞·ªùi ƒë·ªÅu t∆∞∆°i c∆∞·ªùi.\",\n",
    "    \"D·ª± √°n n√†y g·∫∑p nhi·ªÅu kh√≥ khƒÉn. Timeline b·ªã delay v√† budget v∆∞·ª£t qu√° d·ª± ki·∫øn. Team ƒëang stress v√† c·∫ßn support.\",\n",
    "    \"B√°o c√°o h√¥m nay cho th·∫•y doanh s·ªë tƒÉng 15% so v·ªõi qu√Ω tr∆∞·ªõc. S·∫£n ph·∫©m m·ªõi ƒë∆∞·ª£c kh√°ch h√†ng ƒë√≥n nh·∫≠n t√≠ch c·ª±c.\"\n",
    "]\n",
    "\n",
    "print(\"=== Parallel Analysis Results ===\")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"\\nüìÑ Text {i}: {text[:80]}...\")\n",
    "    \n",
    "    # Measure time for parallel execution\n",
    "    start_time = time.time()\n",
    "    result = parallel_analysis.invoke({\"text\": text})\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Execution time: {execution_time:.2f}s\")\n",
    "    print(f\"üòä Sentiment: {result['sentiment']}\")\n",
    "    print(f\"üìù Summary: {result['summary']}\")\n",
    "    print(f\"üîë Keywords: {result['keywords']}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh parallel vs sequential execution\n",
    "def sequential_analysis(text_input):\n",
    "    \"\"\"Sequential execution cho comparison\"\"\"\n",
    "    text = text_input[\"text\"]\n",
    "    \n",
    "    # Execute each analysis sequentially\n",
    "    sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
    "    summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "    keywords_chain = keywords_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    return {\n",
    "        \"sentiment\": sentiment_chain.invoke({\"text\": text}),\n",
    "        \"summary\": summary_chain.invoke({\"text\": text}),\n",
    "        \"keywords\": keywords_chain.invoke({\"text\": text}),\n",
    "        \"original\": text_input\n",
    "    }\n",
    "\n",
    "# Benchmark comparison\n",
    "test_text = {\"text\": sample_texts[0]}\n",
    "\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "\n",
    "# Parallel execution\n",
    "start_time = time.time()\n",
    "parallel_result = parallel_analysis.invoke(test_text)\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"üîÑ Parallel execution: {parallel_time:.2f}s\")\n",
    "\n",
    "# Sequential execution\n",
    "start_time = time.time()\n",
    "sequential_result = sequential_analysis(test_text)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è© Sequential execution: {sequential_time:.2f}s\")\n",
    "print(f\"‚ö° Speedup: {sequential_time/parallel_time:.1f}x faster v·ªõi parallel\")\n",
    "\n",
    "# Verify results are similar\n",
    "print(f\"\\n‚úÖ Results comparison:\")\n",
    "print(f\"Parallel sentiment: {parallel_result['sentiment']}\")\n",
    "print(f\"Sequential sentiment: {sequential_result['sentiment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complex LCEL Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex chain: preprocessing + parallel analysis + postprocessing\n",
    "def preprocess_text(input_data):\n",
    "    \"\"\"Preprocess input text\"\"\"\n",
    "    text = input_data[\"text\"]\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    cleaned_text = text.strip()\n",
    "    word_count = len(cleaned_text.split())\n",
    "    char_count = len(cleaned_text)\n",
    "    \n",
    "    return {\n",
    "        \"text\": cleaned_text,\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"is_long\": word_count > 50\n",
    "    }\n",
    "\n",
    "def postprocess_results(parallel_results):\n",
    "    \"\"\"Combine v√† process parallel results\"\"\"\n",
    "    # Extract components\n",
    "    preprocessed = parallel_results[\"preprocessed\"]\n",
    "    analysis = parallel_results[\"analysis\"]\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    report = {\n",
    "        \"text_stats\": {\n",
    "            \"word_count\": preprocessed[\"word_count\"],\n",
    "            \"char_count\": preprocessed[\"char_count\"],\n",
    "            \"is_long\": preprocessed[\"is_long\"]\n",
    "        },\n",
    "        \"analysis\": analysis,\n",
    "        \"confidence_score\": 0.95 if not preprocessed[\"is_long\"] else 0.85\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Complex composed chain\n",
    "complex_chain = (\n",
    "    # Step 1: Parallel preprocessing v√† analysis\n",
    "    RunnableParallel(\n",
    "        preprocessed=RunnableLambda(preprocess_text),\n",
    "        analysis=parallel_analysis\n",
    "    )\n",
    "    # Step 2: Postprocess results\n",
    "    | RunnableLambda(postprocess_results)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Complex composed chain created\")\n",
    "print(\"Structure: parallel(preprocess, analysis) | postprocess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complex chain\n",
    "complex_test_cases = [\n",
    "    {\n",
    "        \"text\": \"AI tuy·ªát v·ªùi!\",  # Short text\n",
    "        \"name\": \"Short positive\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"Tr√≠ tu·ªá nh√¢n t·∫°o ƒëang thay ƒë·ªïi c√°ch ch√∫ng ta l√†m vi·ªác v√† s·ªëng. \n",
    "        T·ª´ vi·ªác t·ª± ƒë·ªông h√≥a c√°c t√°c v·ª• ƒë∆°n gi·∫£n ƒë·∫øn h·ªó tr·ª£ ra quy·∫øt ƒë·ªãnh ph·ª©c t·∫°p, \n",
    "        AI mang l·∫°i nhi·ªÅu c∆° h·ªôi nh∆∞ng c≈©ng ƒë·∫∑t ra nh·ªØng th√°ch th·ª©c v·ªÅ ƒë·∫°o ƒë·ª©c \n",
    "        v√† t√°c ƒë·ªông x√£ h·ªôi m√† ch√∫ng ta c·∫ßn xem x√©t c·∫©n th·∫≠n.\"\"\",  # Long text\n",
    "        \"name\": \"Long complex\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Complex Chain Results ===\")\n",
    "\n",
    "for test_case in complex_test_cases:\n",
    "    print(f\"\\nüìã Test: {test_case['name']}\")\n",
    "    print(f\"Text: {test_case['text'][:100]}...\")\n",
    "    \n",
    "    result = complex_chain.invoke(test_case)\n",
    "    \n",
    "    # Display results\n",
    "    stats = result[\"text_stats\"]\n",
    "    analysis = result[\"analysis\"]\n",
    "    \n",
    "    print(f\"\\nüìä Text Statistics:\")\n",
    "    print(f\"   Words: {stats['word_count']}, Chars: {stats['char_count']}\")\n",
    "    print(f\"   Is long: {stats['is_long']}\")\n",
    "    print(f\"   Confidence: {result['confidence_score']}\")\n",
    "    \n",
    "    print(f\"\\nüîç Analysis:\")\n",
    "    print(f\"   Sentiment: {analysis['sentiment']}\")\n",
    "    print(f\"   Summary: {analysis['summary']}\")\n",
    "    print(f\"   Keywords: {analysis['keywords']}\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RunnableBranch - Conditional Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableBranch ƒë·ªÉ conditional execution\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Different prompts cho different content types\n",
    "technical_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Ph√¢n t√≠ch k·ªπ thu·∫≠t cho ƒëo·∫°n text sau:\n",
    "    {text}\n",
    "    \n",
    "    T·∫≠p trung v√†o: terminologies, complexity, technical accuracy.\"\"\"\n",
    ")\n",
    "\n",
    "creative_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Ph√¢n t√≠ch s√°ng t·∫°o cho ƒëo·∫°n text sau:\n",
    "    {text}\n",
    "    \n",
    "    T·∫≠p trung v√†o: imagery, emotions, artistic expression.\"\"\"\n",
    ")\n",
    "\n",
    "general_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Ph√¢n t√≠ch t·ªïng qu√°t cho ƒëo·∫°n text sau:\n",
    "    {text}\n",
    "    \n",
    "    Cung c·∫•p analysis c√¢n b·∫±ng v·ªÅ content v√† style.\"\"\"\n",
    ")\n",
    "\n",
    "# Content type detection function\n",
    "def detect_content_type(input_data):\n",
    "    \"\"\"Detect content type t·ª´ text\"\"\"\n",
    "    text = input_data[\"text\"].lower()\n",
    "    \n",
    "    # Simple keyword-based detection\n",
    "    technical_keywords = [\"algorithm\", \"api\", \"database\", \"framework\", \"technology\", \"system\", \"software\", \"ai\", \"machine learning\"]\n",
    "    creative_keywords = [\"th∆°\", \"poem\", \"story\", \"c·∫£m x√∫c\", \"t√¨nh y√™u\", \"m∆° ∆∞·ªõc\", \"ho√†i ni·ªám\"]\n",
    "    \n",
    "    technical_score = sum(1 for keyword in technical_keywords if keyword in text)\n",
    "    creative_score = sum(1 for keyword in creative_keywords if keyword in text)\n",
    "    \n",
    "    if technical_score > creative_score and technical_score > 0:\n",
    "        return \"technical\"\n",
    "    elif creative_score > 0:\n",
    "        return \"creative\"\n",
    "    else:\n",
    "        return \"general\"\n",
    "\n",
    "# Conditional chain v·ªõi RunnableBranch\n",
    "def create_conditional_chain():\n",
    "    return RunnableBranch(\n",
    "        # (condition, runnable) pairs\n",
    "        (lambda x: detect_content_type(x) == \"technical\", technical_prompt | llm | StrOutputParser()),\n",
    "        (lambda x: detect_content_type(x) == \"creative\", creative_prompt | llm | StrOutputParser()),\n",
    "        # Default case\n",
    "        general_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "conditional_chain = create_conditional_chain()\n",
    "\n",
    "print(\"‚úÖ Conditional chain v·ªõi RunnableBranch created\")\n",
    "print(\"Branches: technical, creative, general\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conditional execution\n",
    "test_contents = [\n",
    "    {\n",
    "        \"text\": \"Machine learning algorithms require careful tuning of hyperparameters to optimize model performance. The API design should follow RESTful principles.\",\n",
    "        \"expected_type\": \"technical\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"M∆∞a thu r∆°i l·∫∑ng l·∫Ω tr√™n ph·ªë ph∆∞·ªùng, mang theo nh·ªØng ho√†i ni·ªám xa x√¥i. T√¨nh y√™u nh∆∞ c√°nh b∆∞·ªõm mong manh.\",\n",
    "        \"expected_type\": \"creative\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"H√¥m nay t√¥i ƒëi mua s·∫Øm ·ªü si√™u th·ªã. C√≥ nhi·ªÅu s·∫£n ph·∫©m m·ªõi v√† gi√° c·∫£ h·ª£p l√Ω.\",\n",
    "        \"expected_type\": \"general\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Conditional Chain Testing ===\")\n",
    "\n",
    "for i, content in enumerate(test_contents, 1):\n",
    "    detected_type = detect_content_type(content)\n",
    "    \n",
    "    print(f\"\\nüìÑ Content {i}:\")\n",
    "    print(f\"Text: {content['text'][:80]}...\")\n",
    "    print(f\"Expected type: {content['expected_type']}\")\n",
    "    print(f\"Detected type: {detected_type}\")\n",
    "    print(f\"Match: {'‚úÖ' if detected_type == content['expected_type'] else '‚ùå'}\")\n",
    "    \n",
    "    # Execute conditional chain\n",
    "    result = conditional_chain.invoke(content)\n",
    "    print(f\"\\nüîç Analysis result:\")\n",
    "    print(f\"{result[:200]}...\")\n",
    "    \n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Async Support trong LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL automatic async support\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "# Create async-compatible chain\n",
    "async_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Vi·∫øt m·ªôt fact th√∫ v·ªã v·ªÅ {topic}. Gi·ªØ ng·∫Øn g·ªçn v√† h·∫•p d·∫´n.\"\n",
    ")\n",
    "\n",
    "async_chain = async_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úÖ Async chain created\")\n",
    "\n",
    "# Async batch processing\n",
    "async def process_topics_async(topics: List[str]):\n",
    "    \"\"\"Process multiple topics asynchronously\"\"\"\n",
    "    print(f\"üöÄ Processing {len(topics)} topics asynchronously...\")\n",
    "    \n",
    "    # Create inputs\n",
    "    inputs = [{\"topic\": topic} for topic in topics]\n",
    "    \n",
    "    # Use abatch cho async batch processing\n",
    "    start_time = time.time()\n",
    "    results = await async_chain.abatch(inputs)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Async batch completed in {end_time - start_time:.2f}s\")\n",
    "    return results\n",
    "\n",
    "# Sequential processing ƒë·ªÉ compare\n",
    "def process_topics_sync(topics: List[str]):\n",
    "    \"\"\"Process topics sequentially\"\"\"\n",
    "    print(f\"üêå Processing {len(topics)} topics sequentially...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    for topic in topics:\n",
    "        result = async_chain.invoke({\"topic\": topic})\n",
    "        results.append(result)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Sequential processing completed in {end_time - start_time:.2f}s\")\n",
    "    return results\n",
    "\n",
    "# Test topics\n",
    "test_topics = [\"v≈© tr·ª•\", \"ƒë·∫°i d∆∞∆°ng\", \"AI\", \"l·ªãch s·ª≠\", \"√¢m nh·∫°c\"]\n",
    "\n",
    "print(f\"\\n=== Async vs Sequential Comparison ===\")\n",
    "print(f\"Topics: {test_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run async comparison\n",
    "# Note: In Jupyter, we need to handle the event loop properly\n",
    "\n",
    "try:\n",
    "    # Try to get existing event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # In Jupyter, create a new task\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        \n",
    "        # Run async processing\n",
    "        async_results = await process_topics_async(test_topics)\n",
    "    else:\n",
    "        # Run in new event loop\n",
    "        async_results = asyncio.run(process_topics_async(test_topics))\nexcept:\n",
    "    # Fallback: simulate async v·ªõi batch\n",
    "    print(\"üîÑ Using batch processing as async simulation...\")\n",
    "    inputs = [{\"topic\": topic} for topic in test_topics]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    async_results = async_chain.batch(inputs)\n",
    "    async_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è Batch processing completed in {async_time:.2f}s\")\n",
    "\n",
    "# Sequential processing\n",
    "sync_results = process_topics_sync(test_topics)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìã Results comparison:\")\n",
    "for i, (topic, async_result, sync_result) in enumerate(zip(test_topics, async_results, sync_results)):\n",
    "    print(f\"\\n{i+1}. {topic}:\")\n",
    "    print(f\"   Async: {async_result[:100]}...\")\n",
    "    print(f\"   Sync:  {sync_result[:100]}...\")\n",
    "    print(f\"   Same: {'‚úÖ' if async_result == sync_result else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL streaming support\n",
    "streaming_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Vi·∫øt m·ªôt c√¢u chuy·ªán ng·∫Øn v·ªÅ {theme}. K·ªÉ chi ti·∫øt v√† sinh ƒë·ªông.\"\n",
    ")\n",
    "\n",
    "streaming_chain = streaming_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"=== Streaming Example ===\")\n",
    "print(\"üìñ Story theme: 'cu·ªôc phi√™u l∆∞u trong r·ª´ng'\")\n",
    "print(\"\\nüîÑ Streaming output:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Stream the response\n",
    "full_response = \"\"\n",
    "for chunk in streaming_chain.stream({\"theme\": \"cu·ªôc phi√™u l∆∞u trong r·ª´ng\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    full_response += chunk\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"‚úÖ Streaming completed. Total length: {len(full_response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming v·ªõi intermediate steps\n",
    "intermediate_streaming_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        processed_theme=RunnableLambda(lambda x: f\"Enhanced theme: {x['theme']} v·ªõi magic elements\")\n",
    "    )\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Vi·∫øt story v·ªÅ: {processed_theme}. Make it exciting!\"\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"\\n=== Intermediate Streaming ===\")\n",
    "print(\"üé≠ Theme: 'robot th√¥ng minh'\")\n",
    "print(\"\\nüìù Story v·ªõi intermediate processing:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Stream v·ªõi intermediate steps\n",
    "for chunk in intermediate_streaming_chain.stream({\"theme\": \"robot th√¥ng minh\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"‚úÖ Intermediate streaming completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling v√† Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL error handling v·ªõi fallbacks\n",
    "from langchain_core.runnables import RunnableWithFallbacks\n",
    "\n",
    "# Primary chain c√≥ th·ªÉ fail\n",
    "def risky_processing(input_data):\n",
    "    \"\"\"Function c√≥ th·ªÉ fail based on input\"\"\"\n",
    "    text = input_data.get(\"text\", \"\")\n",
    "    \n",
    "    # Simulate failure cho certain inputs\n",
    "    if \"error\" in text.lower():\n",
    "        raise ValueError(\"Processing failed: error keyword detected\")\n",
    "    \n",
    "    if len(text) > 200:\n",
    "        raise ValueError(\"Text too long for processing\")\n",
    "    \n",
    "    return {\"processed_text\": f\"Successfully processed: {text}\"}\n",
    "\n",
    "def safe_fallback(input_data):\n",
    "    \"\"\"Fallback processing\"\"\"\n",
    "    return {\"processed_text\": f\"Fallback processing: {input_data.get('text', 'No text')[:50]}...\"}\n",
    "\n",
    "# Primary v√† fallback chains\n",
    "primary_chain = RunnableLambda(risky_processing)\n",
    "fallback_chain = RunnableLambda(safe_fallback)\n",
    "\n",
    "# Chain v·ªõi fallback\n",
    "robust_chain = primary_chain.with_fallbacks([fallback_chain])\n",
    "\n",
    "print(\"‚úÖ Robust chain v·ªõi fallback created\")\n",
    "print(\"Primary: risky_processing, Fallback: safe_fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling\n",
    "test_cases_error = [\n",
    "    {\"text\": \"Normal text for processing\", \"should_fail\": False},\n",
    "    {\"text\": \"This will cause an error\", \"should_fail\": True},\n",
    "    {\"text\": \"A\" * 250, \"should_fail\": True},  # Too long\n",
    "    {\"text\": \"Short safe text\", \"should_fail\": False}\n",
    "]\n",
    "\n",
    "print(\"=== Error Handling Testing ===\")\n",
    "\n",
    "for i, test_case in enumerate(test_cases_error, 1):\n",
    "    text_preview = test_case[\"text\"][:50] + (\"...\" if len(test_case[\"text\"]) > 50 else \"\")\n",
    "    \n",
    "    print(f\"\\nüß™ Test {i}: {text_preview}\")\n",
    "    print(f\"Expected to fail: {test_case['should_fail']}\")\n",
    "    \n",
    "    try:\n",
    "        # Test primary chain alone\n",
    "        primary_result = primary_chain.invoke(test_case)\n",
    "        print(f\"‚úÖ Primary succeeded: {primary_result['processed_text'][:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Primary failed: {str(e)}\")\n",
    "    \n",
    "    # Test robust chain v·ªõi fallback\n",
    "    try:\n",
    "        robust_result = robust_chain.invoke(test_case)\n",
    "        print(f\"üõ°Ô∏è Robust result: {robust_result['processed_text'][:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"üí• Even fallback failed: {str(e)}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced LCEL Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pattern: Dynamic chain construction\n",
    "def create_dynamic_chain(analysis_type: str, complexity: str):\n",
    "    \"\"\"Dynamically create chain based on parameters\"\"\"\n",
    "    \n",
    "    # Base components\n",
    "    base_prompt = \"Ph√¢n t√≠ch {type} cho text: {text}\"\n",
    "    \n",
    "    if complexity == \"simple\":\n",
    "        instruction = \"Gi·ªØ analysis ng·∫Øn g·ªçn v√† d·ªÖ hi·ªÉu.\"\n",
    "    elif complexity == \"detailed\":\n",
    "        instruction = \"Cung c·∫•p analysis chi ti·∫øt v√† s√¢u s·∫Øc.\"\n",
    "    else:\n",
    "        instruction = \"Cung c·∫•p analysis c√¢n b·∫±ng.\"\n",
    "    \n",
    "    # Dynamic prompt construction\n",
    "    full_prompt = f\"{base_prompt} {instruction}\"\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_template(full_prompt)\n",
    "    \n",
    "    # Dynamic chain based on analysis type\n",
    "    if analysis_type == \"sentiment\":\n",
    "        chain = (\n",
    "            RunnablePassthrough.assign(type=lambda _: \"c·∫£m x√∫c\")\n",
    "            | prompt_template\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    elif analysis_type == \"technical\":\n",
    "        chain = (\n",
    "            RunnablePassthrough.assign(type=lambda _: \"k·ªπ thu·∫≠t\")\n",
    "            | prompt_template\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    else:\n",
    "        chain = (\n",
    "            RunnablePassthrough.assign(type=lambda _: \"t·ªïng qu√°t\")\n",
    "            | prompt_template\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "# Test dynamic chains\n",
    "print(\"=== Dynamic Chain Construction ===\")\n",
    "\n",
    "configurations = [\n",
    "    (\"sentiment\", \"simple\"),\n",
    "    (\"technical\", \"detailed\"),\n",
    "    (\"general\", \"balanced\")\n",
    "]\n",
    "\n",
    "test_text = \"Tr√≠ tu·ªá nh√¢n t·∫°o ƒëang ph√°t tri·ªÉn nhanh ch√≥ng v√† t·∫°o ra nhi·ªÅu c∆° h·ªôi m·ªõi.\"\n",
    "\n",
    "for analysis_type, complexity in configurations:\n",
    "    print(f\"\\nüîß Configuration: {analysis_type} - {complexity}\")\n",
    "    \n",
    "    # Create dynamic chain\n",
    "    dynamic_chain = create_dynamic_chain(analysis_type, complexity)\n",
    "    \n",
    "    # Execute\n",
    "    result = dynamic_chain.invoke({\"text\": test_text})\n",
    "    \n",
    "    print(f\"üìù Result: {result[:150]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pattern: Chain with memory/state\n",
    "class StatefulProcessor:\n",
    "    \"\"\"Processor c√≥ state ƒë·ªÉ track processed items\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.processed_count = 0\n",
    "        self.processed_items = []\n",
    "    \n",
    "    def process(self, input_data):\n",
    "        \"\"\"Process input v√† update state\"\"\"\n",
    "        text = input_data[\"text\"]\n",
    "        \n",
    "        # Update state\n",
    "        self.processed_count += 1\n",
    "        self.processed_items.append(text[:50] + \"...\" if len(text) > 50 else text)\n",
    "        \n",
    "        # Return processed data v·ªõi state info\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"processing_id\": self.processed_count,\n",
    "            \"context\": f\"This is item #{self.processed_count} processed today\"\n",
    "        }\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            \"total_processed\": self.processed_count,\n",
    "            \"items\": self.processed_items\n",
    "        }\n",
    "\n",
    "# Create stateful processor\n",
    "processor = StatefulProcessor()\n",
    "\n",
    "# Stateful chain\n",
    "stateful_chain = (\n",
    "    RunnableLambda(processor.process)\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Process the following text (ID: {processing_id}):\\n{text}\\n\\nContext: {context}\\n\\nSummary:\"\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"=== Stateful Chain Testing ===\")\n",
    "\n",
    "# Process multiple items\n",
    "test_items = [\n",
    "    \"H√¥m nay tr·ªùi ƒë·∫πp\",\n",
    "    \"AI ƒëang thay ƒë·ªïi th·∫ø gi·ªõi\",\n",
    "    \"T√¥i th√≠ch h·ªçc l·∫≠p tr√¨nh\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for item in test_items:\n",
    "    result = stateful_chain.invoke({\"text\": item})\n",
    "    results.append(result)\n",
    "    print(f\"\\nüìù Processed: {item}\")\n",
    "    print(f\"üîç Summary: {result[:100]}...\")\n",
    "\n",
    "# Show final state\n",
    "summary = processor.get_summary()\n",
    "print(f\"\\nüìä Final State:\")\n",
    "print(f\"Total processed: {summary['total_processed']}\")\n",
    "print(f\"Items: {summary['items']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LCEL Best Practices v√† Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL Best Practices demonstration\n",
    "def demonstrate_lcel_best_practices():\n",
    "    print(\"=== LCEL BEST PRACTICES ===\")\n",
    "    \n",
    "    practices = [\n",
    "        {\n",
    "            \"title\": \"1. üîó USE PIPE OPERATOR\",\n",
    "            \"good\": \"prompt | llm | parser\",\n",
    "            \"bad\": \"parser.parse(llm.invoke(prompt.format(...)))\",\n",
    "            \"benefit\": \"Automatic optimizations, streaming, async support\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"2. ‚ö° LEVERAGE PARALLEL EXECUTION\",\n",
    "            \"good\": \"RunnableParallel({a: chain_a, b: chain_b})\",\n",
    "            \"bad\": \"result_a = chain_a.invoke(); result_b = chain_b.invoke()\",\n",
    "            \"benefit\": \"Significant speedup for independent operations\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"3. üõ°Ô∏è IMPLEMENT FALLBACKS\",\n",
    "            \"good\": \"primary_chain.with_fallbacks([fallback_chain])\",\n",
    "            \"bad\": \"try: primary() except: fallback()\",\n",
    "            \"benefit\": \"Built-in retry logic v√† error handling\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"4. üìä USE PASSTHROUGH WISELY\",\n",
    "            \"good\": \"RunnablePassthrough.assign(new_field=computation)\",\n",
    "            \"bad\": \"Manually merging dictionaries\",\n",
    "            \"benefit\": \"Clean data flow, preserved context\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"5. üîÑ UTILIZE STREAMING\",\n",
    "            \"good\": \"for chunk in chain.stream(input): ...\",\n",
    "            \"bad\": \"result = chain.invoke(input)  # Wait for complete\",\n",
    "            \"benefit\": \"Better user experience, progressive results\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for practice in practices:\n",
    "        print(f\"\\n{practice['title']}\")\n",
    "        print(f\"   ‚úÖ Good: {practice['good']}\")\n",
    "        print(f\"   ‚ùå Bad:  {practice['bad']}\")\n",
    "        print(f\"   üí° Benefit: {practice['benefit']}\")\n",
    "\n",
    "demonstrate_lcel_best_practices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization examples\n",
    "def demonstrate_performance_optimizations():\n",
    "    print(\"\\n=== PERFORMANCE OPTIMIZATION TIPS ===\")\n",
    "    \n",
    "    tips = [\n",
    "        {\n",
    "            \"tip\": \"üöÄ Batch Processing\",\n",
    "            \"description\": \"Use .batch() thay v√¨ multiple .invoke() calls\",\n",
    "            \"example\": \"chain.batch([input1, input2, input3])\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"‚ö° Async When Possible\",\n",
    "            \"description\": \"Use .abatch() cho async batch processing\",\n",
    "            \"example\": \"await chain.abatch(inputs)\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"üîÑ Smart Parallelization\",\n",
    "            \"description\": \"Group independent operations trong RunnableParallel\",\n",
    "            \"example\": \"RunnableParallel({task1: chain1, task2: chain2})\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"üíæ Avoid Unnecessary Copies\",\n",
    "            \"description\": \"Use RunnablePassthrough thay v√¨ copying data\",\n",
    "            \"example\": \"RunnablePassthrough.assign(new_field=compute)\"\n",
    "        },\n",
    "        {\n",
    "            \"tip\": \"üéØ Minimize LLM Calls\",\n",
    "            \"description\": \"Combine multiple tasks trong single prompt when possible\",\n",
    "            \"example\": \"'Analyze sentiment AND extract keywords: {text}'\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for tip in tips:\n",
    "        print(f\"\\n{tip['tip']}\")\n",
    "        print(f\"   üìã {tip['description']}\")\n",
    "        print(f\"   üíª Example: {tip['example']}\")\n",
    "\n",
    "demonstrate_performance_optimizations()\n",
    "\n",
    "print(\"\\n‚úÖ LCEL Basics tutorial completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T·ªïng k·∫øt\n",
    "\n",
    "### **LCEL (LangChain Expression Language): Key Benefits**\n",
    "\n",
    "#### **üîó Core Features**\n",
    "- **Pipe Operator (`|`)**: Intuitive chaining syntax\n",
    "- **Automatic Optimizations**: Built-in performance enhancements\n",
    "- **Streaming Support**: Progressive output delivery\n",
    "- **Async Support**: Native async/await capabilities\n",
    "- **Error Handling**: Built-in retry v√† fallback mechanisms\n",
    "\n",
    "#### **üß© Essential Components**\n",
    "1. **RunnablePassthrough**: Preserve v√† enrich data\n",
    "2. **RunnableParallel**: Execute multiple operations concurrently\n",
    "3. **RunnableLambda**: Wrap custom functions\n",
    "4. **RunnableBranch**: Conditional execution logic\n",
    "5. **RunnableWithFallbacks**: Error recovery patterns\n",
    "\n",
    "#### **‚ö° Performance Advantages**\n",
    "- **Parallel Execution**: Automatic optimization c·ªßa independent operations\n",
    "- **Batch Processing**: Efficient handling c·ªßa multiple inputs\n",
    "- **Streaming**: Real-time output delivery\n",
    "- **Async Support**: Non-blocking execution\n",
    "\n",
    "#### **üõ†Ô∏è Advanced Patterns h·ªçc ƒë∆∞·ª£c**\n",
    "- **Complex Compositions**: Multi-step workflows\n",
    "- **Conditional Logic**: Dynamic chain selection\n",
    "- **State Management**: Stateful processing\n",
    "- **Error Recovery**: Robust fallback strategies\n",
    "\n",
    "### **Best Practices**\n",
    "1. **Always Use Pipe Operator**: Leverage LCEL optimizations\n",
    "2. **Parallel When Possible**: Group independent operations\n",
    "3. **Implement Fallbacks**: Handle errors gracefully\n",
    "4. **Utilize Streaming**: Better user experience\n",
    "5. **Batch Operations**: Avoid multiple individual calls\n",
    "6. **Preserve Context**: Use RunnablePassthrough for data flow\n",
    "\n",
    "### **Next Steps**\n",
    "- **Sequential Chains**: Multi-step dependent workflows\n",
    "- **Retrieval Chains**: Integration v·ªõi vector stores\n",
    "- **Agent Patterns**: Decision-making workflows\n",
    "- **Production Deployment**: Scaling v√† monitoring\n",
    "\n",
    "LCEL l√† foundation m·∫°nh m·∫Ω cho building efficient v√† maintainable LangChain applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}