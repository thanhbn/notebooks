{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Efficient SFT with Model-Generated Data\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand why model-generated data outperforms human-written solutions for training\n",
    "2. Learn the multi-stage generation process for creating high-quality training data\n",
    "3. Master efficient supervised fine-tuning (SFT) techniques for code generation\n",
    "4. Implement data efficiency strategies that achieve 40x performance improvement\n",
    "\n",
    "## Concept Source\n",
    "- **Paper Section**: Section 4 (Efficient Training) and Section 2.1 (Data Collection)\n",
    "- **Key Table**: Table 4 (Model SFT-training results)\n",
    "- **Critical Finding**: \"2.6K model-generated LeetCode samples achieved superior performance on HumanEval (79.9%) and MBPP (77.5%), surpassing models trained on much larger datasets (9.5Kâ€“111.1K rows)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data Quality vs. Quantity Paradigm\n",
    "\n",
    "### Why Model-Generated Data Works Better\n",
    "\n",
    "The paper reveals a counterintuitive finding: **Model-generated solutions are better for training than human-written ones**.\n",
    "\n",
    "Key insights:\n",
    "- **Human solutions**: Often terse, minimal comments, optimized for contests\n",
    "- **Model-generated solutions**: Explicit reasoning, clear structure, educational value\n",
    "- **Training efficiency**: Quality trumps quantity in code generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Stage Data Generation Process\n",
    "\n",
    "Following the paper's approach using Qwen2.5-Coder-32B-Instruct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    \"\"\"Configuration for multi-stage generation\"\"\"\n",
    "    model_name: str = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "    high_temp: float = 1.0  # For diversity\n",
    "    low_temp: float = 0.2   # For quality\n",
    "    max_attempts: int = 5\n",
    "    use_ground_truth_hints: bool = True\n",
    "\n",
    "class ModelBasedDataGenerator:\n",
    "    \"\"\"Generate high-quality training data using LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GenerationConfig):\n",
    "        self.config = config\n",
    "        # In practice, you'd load the actual model\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        # self.model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "        \n",
    "    def generate_solution_variants(self, problem: Dict, \n",
    "                                 num_variants: int = 5) -> List[Dict]:\n",
    "        \"\"\"Stage 1: Generate diverse solution candidates with high temperature\"\"\"\n",
    "        \n",
    "        prompt_template = \"\"\"\n",
    "You are an expert competitive programmer. Solve this problem with clear reasoning:\n",
    "\n",
    "Problem: {problem_description}\n",
    "\n",
    "Starter Code:\n",
    "{starter_code}\n",
    "\n",
    "Provide a complete solution with step-by-step reasoning. Focus on:\n",
    "1. Understanding the problem\n",
    "2. Identifying the algorithm/approach\n",
    "3. Implementation with clear variable names\n",
    "4. Time/space complexity analysis\n",
    "\"\"\"\n",
    "        \n",
    "        variants = []\n",
    "        \n",
    "        for i in range(num_variants):\n",
    "            # Simulate diverse generation with different approaches\n",
    "            approaches = [\n",
    "                self._generate_iterative_solution(problem),\n",
    "                self._generate_mathematical_solution(problem),\n",
    "                self._generate_two_pointer_solution(problem),\n",
    "                self._generate_binary_search_solution(problem),\n",
    "                self._generate_optimized_solution(problem)\n",
    "            ]\n",
    "            \n",
    "            variant = approaches[i % len(approaches)]\n",
    "            variant['generation_id'] = i\n",
    "            variant['temperature'] = self.config.high_temp\n",
    "            \n",
    "            variants.append(variant)\n",
    "        \n",
    "        return variants\n",
    "    \n",
    "    def _generate_iterative_solution(self, problem: Dict) -> Dict:\n",
    "        \"\"\"Generate iterative approach solution\"\"\"\n",
    "        return {\n",
    "            'approach': 'iterative',\n",
    "            'reasoning': \"\"\"\n",
    "Step 1: Understanding the Problem\n",
    "We need to find the missing number in an arithmetic progression.\n",
    "In an AP, consecutive differences are constant.\n",
    "\n",
    "Step 2: Algorithm\n",
    "- Calculate the expected common difference\n",
    "- Iterate through array to find the gap\n",
    "- Return the missing number\n",
    "\n",
    "Step 3: Implementation\n",
    "\"\"\",\n",
    "            'code': \"\"\"\n",
    "class Solution:\n",
    "    def missingNumber(self, arr: List[int]) -> int:\n",
    "        n = len(arr)\n",
    "        # Calculate expected common difference\n",
    "        total_diff = arr[-1] - arr[0]\n",
    "        common_diff = total_diff // n\n",
    "        \n",
    "        # Find the missing number by checking consecutive differences\n",
    "        for i in range(n - 1):\n",
    "            if arr[i + 1] - arr[i] != common_diff:\n",
    "                return arr[i] + common_diff\n",
    "        \n",
    "        # Edge case: missing at end\n",
    "        return arr[-1] + common_diff\n",
    "\"\"\",\n",
    "            'complexity': 'Time: O(n), Space: O(1)'\n",
    "        }\n",
    "    \n",
    "    def _generate_mathematical_solution(self, problem: Dict) -> Dict:\n",
    "        \"\"\"Generate mathematical approach\"\"\"\n",
    "        return {\n",
    "            'approach': 'mathematical',\n",
    "            'reasoning': \"\"\"\n",
    "Step 1: Mathematical Insight\n",
    "In arithmetic progression: sum = n * (first + last) / 2\n",
    "But we have (n-1) elements, so one is missing.\n",
    "\n",
    "Step 2: Formula Application\n",
    "- Calculate expected sum of complete AP\n",
    "- Subtract actual sum to find missing number\n",
    "\n",
    "Step 3: Edge Case Handling\n",
    "\"\"\",\n",
    "            'code': \"\"\"\n",
    "class Solution:\n",
    "    def missingNumber(self, arr: List[int]) -> int:\n",
    "        n = len(arr)\n",
    "        # Expected sum of complete arithmetic progression\n",
    "        # Formula: (n+1) * (first + last) / 2\n",
    "        expected_sum = (n + 1) * (arr[0] + arr[-1]) // 2\n",
    "        \n",
    "        # Actual sum of given array\n",
    "        actual_sum = sum(arr)\n",
    "        \n",
    "        # Missing number is the difference\n",
    "        return expected_sum - actual_sum\n",
    "\"\"\",\n",
    "            'complexity': 'Time: O(n), Space: O(1)'\n",
    "        }\n",
    "    \n",
    "    def _generate_two_pointer_solution(self, problem: Dict) -> Dict:\n",
    "        \"\"\"Generate two-pointer approach\"\"\"\n",
    "        return {\n",
    "            'approach': 'two_pointer',\n",
    "            'reasoning': \"\"\"\n",
    "Step 1: Two Pointer Strategy\n",
    "Use binary search concept to narrow down the missing position.\n",
    "\n",
    "Step 2: Logic\n",
    "- Calculate expected value at each position\n",
    "- Use binary search to find the break point\n",
    "\n",
    "Step 3: Optimization\n",
    "\"\"\",\n",
    "            'code': \"\"\"\n",
    "class Solution:\n",
    "    def missingNumber(self, arr: List[int]) -> int:\n",
    "        n = len(arr)\n",
    "        common_diff = (arr[-1] - arr[0]) // n\n",
    "        \n",
    "        left, right = 0, n - 1\n",
    "        \n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            expected_val = arr[0] + mid * common_diff\n",
    "            \n",
    "            if arr[mid] == expected_val:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid - 1\n",
    "        \n",
    "        return arr[0] + left * common_diff\n",
    "\"\"\",\n",
    "            'complexity': 'Time: O(log n), Space: O(1)'\n",
    "        }\n",
    "    \n",
    "    def _generate_binary_search_solution(self, problem: Dict) -> Dict:\n",
    "        \"\"\"Generate binary search solution\"\"\"\n",
    "        return self._generate_two_pointer_solution(problem)  # Similar approach\n",
    "    \n",
    "    def _generate_optimized_solution(self, problem: Dict) -> Dict:\n",
    "        \"\"\"Generate most optimized solution\"\"\"\n",
    "        return self._generate_mathematical_solution(problem)  # Math is most optimal\n",
    "\n",
    "# Test the generator\n",
    "config = GenerationConfig()\n",
    "generator = ModelBasedDataGenerator(config)\n",
    "\n",
    "test_problem = {\n",
    "    'problem_description': \"Find missing number in arithmetic progression\",\n",
    "    'starter_code': \"class Solution:\\n    def missingNumber(self, arr: List[int]) -> int:\"\n",
    "}\n",
    "\n",
    "variants = generator.generate_solution_variants(test_problem, num_variants=3)\n",
    "\n",
    "print(\"Generated Solution Variants:\")\n",
    "for i, variant in enumerate(variants):\n",
    "    print(f\"\\nVariant {i+1} ({variant['approach']}):\")\n",
    "    print(f\"Complexity: {variant['complexity']}\")\n",
    "    print(\"Reasoning excerpt:\", variant['reasoning'][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automated Test Case Verification\n",
    "\n",
    "### Stage 2: Filter Functionally Correct Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolutionVerifier:\n",
    "    \"\"\"Verify solution correctness against test cases\"\"\"\n",
    "    \n",
    "    def __init__(self, timeout: int = 5):\n",
    "        self.timeout = timeout\n",
    "        \n",
    "    def verify_solutions(self, variants: List[Dict], \n",
    "                        test_cases: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Filter variants by correctness\"\"\"\n",
    "        verified_solutions = []\n",
    "        \n",
    "        for variant in variants:\n",
    "            verification_result = self._verify_single_solution(\n",
    "                variant, test_cases\n",
    "            )\n",
    "            \n",
    "            if verification_result['passed']:\n",
    "                variant['verification'] = verification_result\n",
    "                verified_solutions.append(variant)\n",
    "        \n",
    "        return verified_solutions\n",
    "    \n",
    "    def _verify_single_solution(self, variant: Dict, \n",
    "                               test_cases: List[Dict]) -> Dict:\n",
    "        \"\"\"Verify a single solution against test cases\"\"\"\n",
    "        results = {\n",
    "            'total_tests': len(test_cases),\n",
    "            'passed_tests': 0,\n",
    "            'failed_tests': [],\n",
    "            'execution_time': 0,\n",
    "            'passed': False\n",
    "        }\n",
    "        \n",
    "        # Simulate test execution for each test case\n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            try:\n",
    "                # In real implementation, execute the code safely\n",
    "                # result = self._execute_code(variant['code'], test_case)\n",
    "                \n",
    "                # Simulate execution result\n",
    "                if self._simulate_test_execution(variant, test_case):\n",
    "                    results['passed_tests'] += 1\n",
    "                else:\n",
    "                    results['failed_tests'].append({\n",
    "                        'test_id': i,\n",
    "                        'input': test_case['input'],\n",
    "                        'expected': test_case['output'],\n",
    "                        'actual': 'simulated_wrong_output'\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results['failed_tests'].append({\n",
    "                    'test_id': i,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        # Solution passes if it gets all test cases correct\n",
    "        results['passed'] = results['passed_tests'] == results['total_tests']\n",
    "        results['pass_rate'] = results['passed_tests'] / results['total_tests']\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _simulate_test_execution(self, variant: Dict, test_case: Dict) -> bool:\n",
    "        \"\"\"Simulate test execution (replace with actual execution)\"\"\"\n",
    "        # Simulate different success rates for different approaches\n",
    "        success_rates = {\n",
    "            'mathematical': 0.95,\n",
    "            'iterative': 0.90,\n",
    "            'two_pointer': 0.85\n",
    "        }\n",
    "        \n",
    "        approach = variant.get('approach', 'unknown')\n",
    "        success_rate = success_rates.get(approach, 0.80)\n",
    "        \n",
    "        return np.random.random() < success_rate\n",
    "    \n",
    "    def generate_verification_report(self, variants: List[Dict], \n",
    "                                   verified: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate comprehensive verification report\"\"\"\n",
    "        report = {\n",
    "            'initial_variants': len(variants),\n",
    "            'verified_solutions': len(verified),\n",
    "            'success_rate': len(verified) / len(variants) if variants else 0,\n",
    "            'approach_performance': {},\n",
    "            'quality_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Analyze by approach\n",
    "        for variant in variants:\n",
    "            approach = variant['approach']\n",
    "            if approach not in report['approach_performance']:\n",
    "                report['approach_performance'][approach] = {\n",
    "                    'total': 0, 'verified': 0\n",
    "                }\n",
    "            report['approach_performance'][approach]['total'] += 1\n",
    "        \n",
    "        for variant in verified:\n",
    "            approach = variant['approach']\n",
    "            report['approach_performance'][approach]['verified'] += 1\n",
    "        \n",
    "        # Calculate success rates by approach\n",
    "        for approach, stats in report['approach_performance'].items():\n",
    "            stats['success_rate'] = stats['verified'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        \n",
    "        # Quality metrics\n",
    "        if verified:\n",
    "            pass_rates = [v.get('verification', {}).get('pass_rate', 0) for v in verified]\n",
    "            report['quality_metrics'] = {\n",
    "                'avg_pass_rate': np.mean(pass_rates),\n",
    "                'min_pass_rate': min(pass_rates),\n",
    "                'max_pass_rate': max(pass_rates)\n",
    "            }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Test the verifier\n",
    "test_cases = [\n",
    "    {'input': {'arr': [5, 7, 11, 13]}, 'output': 9},\n",
    "    {'input': {'arr': [15, 13, 12]}, 'output': 14},\n",
    "    {'input': {'arr': [1, 3, 5, 9]}, 'output': 7}\n",
    "]\n",
    "\n",
    "verifier = SolutionVerifier()\n",
    "verified_solutions = verifier.verify_solutions(variants, test_cases)\n",
    "verification_report = verifier.generate_verification_report(variants, verified_solutions)\n",
    "\n",
    "print(\"Verification Report:\")\n",
    "print(json.dumps(verification_report, indent=2))\n",
    "\n",
    "print(f\"\\nVerified {len(verified_solutions)}/{len(variants)} solutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ground Truth Integration for Difficult Problems\n",
    "\n",
    "### Stage 3: Handle Persistently Failing Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruthIntegrator:\n",
    "    \"\"\"Integrate ground truth hints for difficult problems\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: float = 0.3):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        \n",
    "    def identify_difficult_problems(self, problem_results: Dict[str, List[Dict]]) -> List[str]:\n",
    "        \"\"\"Identify problems with low success rates\"\"\"\n",
    "        difficult_problems = []\n",
    "        \n",
    "        for problem_id, solutions in problem_results.items():\n",
    "            if not solutions:  # No solutions passed\n",
    "                difficult_problems.append(problem_id)\n",
    "            else:\n",
    "                # Check if verification pass rate is low\n",
    "                avg_pass_rate = np.mean([\n",
    "                    sol.get('verification', {}).get('pass_rate', 0) \n",
    "                    for sol in solutions\n",
    "                ])\n",
    "                \n",
    "                if avg_pass_rate < self.failure_threshold:\n",
    "                    difficult_problems.append(problem_id)\n",
    "        \n",
    "        return difficult_problems\n",
    "    \n",
    "    def create_hint_enhanced_prompts(self, problem: Dict, \n",
    "                                   ground_truth_solution: str) -> List[str]:\n",
    "        \"\"\"Create prompts with progressive hints\"\"\"\n",
    "        \n",
    "        # Extract key insights from ground truth\n",
    "        hints = self._extract_solution_hints(ground_truth_solution)\n",
    "        \n",
    "        prompts = []\n",
    "        \n",
    "        # Level 1: Algorithmic hint\n",
    "        prompts.append(f\"\"\"\n",
    "Problem: {problem['description']}\n",
    "\n",
    "Hint: This problem can be solved using {hints['algorithm_type']}.\n",
    "Think about {hints['key_insight']}.\n",
    "\n",
    "Provide a complete solution with explanation.\n",
    "\"\"\")\n",
    "        \n",
    "        # Level 2: Implementation hint\n",
    "        prompts.append(f\"\"\"\n",
    "Problem: {problem['description']}\n",
    "\n",
    "Algorithm: {hints['algorithm_type']}\n",
    "Key insight: {hints['key_insight']}\n",
    "Implementation approach: {hints['implementation_hint']}\n",
    "\n",
    "Complete the solution:\n",
    "\"\"\")\n",
    "        \n",
    "        # Level 3: Code structure hint\n",
    "        prompts.append(f\"\"\"\n",
    "Problem: {problem['description']}\n",
    "\n",
    "Solution structure:\n",
    "{hints['code_structure']}\n",
    "\n",
    "Fill in the implementation details:\n",
    "\"\"\")\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def _extract_solution_hints(self, ground_truth: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract hints from ground truth solution\"\"\"\n",
    "        # Analyze the ground truth to extract patterns\n",
    "        hints = {\n",
    "            'algorithm_type': 'mathematical approach',\n",
    "            'key_insight': 'arithmetic progression sum formula',\n",
    "            'implementation_hint': 'calculate expected sum vs actual sum',\n",
    "            'code_structure': \"\"\"\n",
    "# Step 1: Calculate expected sum of complete AP\n",
    "# Step 2: Calculate actual sum of given array  \n",
    "# Step 3: Return the difference\n",
    "\"\"\"\n",
    "        }\n",
    "        \n",
    "        # In practice, use AST analysis or LLM to extract hints\n",
    "        if 'binary' in ground_truth.lower():\n",
    "            hints['algorithm_type'] = 'binary search'\n",
    "        elif 'two' in ground_truth.lower() and 'pointer' in ground_truth.lower():\n",
    "            hints['algorithm_type'] = 'two pointer technique'\n",
    "        elif 'dp' in ground_truth.lower() or 'dynamic' in ground_truth.lower():\n",
    "            hints['algorithm_type'] = 'dynamic programming'\n",
    "        \n",
    "        return hints\n",
    "    \n",
    "    def generate_hint_based_solutions(self, difficult_problems: List[str],\n",
    "                                    problem_data: Dict[str, Dict],\n",
    "                                    ground_truth_solutions: Dict[str, str]) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Generate solutions with hints for difficult problems\"\"\"\n",
    "        \n",
    "        hint_solutions = {}\n",
    "        \n",
    "        for problem_id in difficult_problems:\n",
    "            if problem_id not in ground_truth_solutions:\n",
    "                continue\n",
    "                \n",
    "            problem = problem_data[problem_id]\n",
    "            ground_truth = ground_truth_solutions[problem_id]\n",
    "            \n",
    "            # Create hint-enhanced prompts\n",
    "            hint_prompts = self.create_hint_enhanced_prompts(problem, ground_truth)\n",
    "            \n",
    "            # Generate solutions with different hint levels\n",
    "            solutions = []\n",
    "            for i, prompt in enumerate(hint_prompts):\n",
    "                # In practice, call LLM with this prompt\n",
    "                solution = self._simulate_hint_based_generation(prompt, ground_truth, i)\n",
    "                solution['hint_level'] = i + 1\n",
    "                solution['prompt_used'] = prompt\n",
    "                solutions.append(solution)\n",
    "            \n",
    "            hint_solutions[problem_id] = solutions\n",
    "        \n",
    "        return hint_solutions\n",
    "    \n",
    "    def _simulate_hint_based_generation(self, prompt: str, \n",
    "                                      ground_truth: str, \n",
    "                                      hint_level: int) -> Dict:\n",
    "        \"\"\"Simulate hint-based solution generation\"\"\"\n",
    "        # Simulate increasing success rate with more hints\n",
    "        success_rates = [0.6, 0.8, 0.95]  # Level 1, 2, 3 hints\n",
    "        success_rate = success_rates[min(hint_level, len(success_rates) - 1)]\n",
    "        \n",
    "        if np.random.random() < success_rate:\n",
    "            # Return a solution based on ground truth\n",
    "            return {\n",
    "                'approach': 'hint_guided',\n",
    "                'code': ground_truth,  # Simplified for demo\n",
    "                'reasoning': f\"Used hint level {hint_level + 1} to solve\",\n",
    "                'success': True\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'approach': 'hint_guided',\n",
    "                'code': \"# Failed to generate correct solution\",\n",
    "                'reasoning': \"Could not solve even with hints\",\n",
    "                'success': False\n",
    "            }\n",
    "\n",
    "# Test ground truth integration\n",
    "integrator = GroundTruthIntegrator(failure_threshold=0.5)\n",
    "\n",
    "# Simulate problem results\n",
    "problem_results = {\n",
    "    'easy_problem': verified_solutions,  # Has solutions\n",
    "    'hard_problem': [],  # No solutions\n",
    "    'medium_problem': [{'verification': {'pass_rate': 0.2}}]  # Low pass rate\n",
    "}\n",
    "\n",
    "difficult_problems = integrator.identify_difficult_problems(problem_results)\n",
    "print(f\"Identified difficult problems: {difficult_problems}\")\n",
    "\n",
    "# Mock data for hint generation\n",
    "problem_data = {\n",
    "    'hard_problem': {'description': 'Complex algorithm problem'},\n",
    "    'medium_problem': {'description': 'Medium difficulty problem'}\n",
    "}\n",
    "\n",
    "ground_truth_solutions = {\n",
    "    'hard_problem': 'def solve(): return binary_search_solution()',\n",
    "    'medium_problem': 'def solve(): return mathematical_solution()'\n",
    "}\n",
    "\n",
    "hint_solutions = integrator.generate_hint_based_solutions(\n",
    "    difficult_problems, problem_data, ground_truth_solutions\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated hint-based solutions for {len(hint_solutions)} problems\")\n",
    "for problem_id, solutions in hint_solutions.items():\n",
    "    print(f\"  {problem_id}: {len(solutions)} solutions with different hint levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Dataset Construction\n",
    "\n",
    "### Creating High-Quality Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingExample:\n",
    "    \"\"\"Single training example for SFT\"\"\"\n",
    "    problem_id: str\n",
    "    difficulty: str\n",
    "    query: str  # Problem description + starter code\n",
    "    response: str  # Model-generated solution with reasoning\n",
    "    metadata: Dict\n",
    "\n",
    "class TrainingDatasetBuilder:\n",
    "    \"\"\"Build high-quality training dataset from verified solutions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.examples = []\n",
    "        \n",
    "    def build_training_dataset(self, \n",
    "                             verified_solutions: Dict[str, List[Dict]],\n",
    "                             problem_data: Dict[str, Dict],\n",
    "                             max_examples_per_problem: int = 2) -> List[TrainingExample]:\n",
    "        \"\"\"Build training dataset from verified solutions\"\"\"\n",
    "        \n",
    "        training_examples = []\n",
    "        \n",
    "        for problem_id, solutions in verified_solutions.items():\n",
    "            if problem_id not in problem_data:\n",
    "                continue\n",
    "                \n",
    "            problem = problem_data[problem_id]\n",
    "            \n",
    "            # Select best solutions for training\n",
    "            selected_solutions = self._select_best_solutions(\n",
    "                solutions, max_examples_per_problem\n",
    "            )\n",
    "            \n",
    "            for solution in selected_solutions:\n",
    "                # Create query following LiveCodeBench format\n",
    "                query = self._format_query(problem)\n",
    "                \n",
    "                # Create response with reasoning + code\n",
    "                response = self._format_response(solution)\n",
    "                \n",
    "                example = TrainingExample(\n",
    "                    problem_id=problem_id,\n",
    "                    difficulty=problem.get('difficulty', 'Unknown'),\n",
    "                    query=query,\n",
    "                    response=response,\n",
    "                    metadata={\n",
    "                        'approach': solution['approach'],\n",
    "                        'verification_score': solution.get('verification', {}).get('pass_rate', 0),\n",
    "                        'generation_temperature': solution.get('temperature', 0.2)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                training_examples.append(example)\n",
    "        \n",
    "        return training_examples\n",
    "    \n",
    "    def _select_best_solutions(self, solutions: List[Dict], \n",
    "                             max_count: int) -> List[Dict]:\n",
    "        \"\"\"Select the best solutions for training\"\"\"\n",
    "        # Sort by verification score and approach diversity\n",
    "        scored_solutions = []\n",
    "        \n",
    "        for solution in solutions:\n",
    "            score = solution.get('verification', {}).get('pass_rate', 0)\n",
    "            \n",
    "            # Bonus for diverse approaches\n",
    "            approach_bonus = {\n",
    "                'mathematical': 0.1,\n",
    "                'iterative': 0.05,\n",
    "                'two_pointer': 0.08\n",
    "            }.get(solution['approach'], 0)\n",
    "            \n",
    "            final_score = score + approach_bonus\n",
    "            scored_solutions.append((final_score, solution))\n",
    "        \n",
    "        # Sort by score and take top solutions\n",
    "        scored_solutions.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Ensure approach diversity\n",
    "        selected = []\n",
    "        used_approaches = set()\n",
    "        \n",
    "        for score, solution in scored_solutions:\n",
    "            if len(selected) >= max_count:\n",
    "                break\n",
    "                \n",
    "            approach = solution['approach']\n",
    "            if approach not in used_approaches or len(selected) == 0:\n",
    "                selected.append(solution)\n",
    "                used_approaches.add(approach)\n",
    "        \n",
    "        # Fill remaining slots if needed\n",
    "        for score, solution in scored_solutions:\n",
    "            if len(selected) >= max_count:\n",
    "                break\n",
    "            if solution not in selected:\n",
    "                selected.append(solution)\n",
    "        \n",
    "        return selected[:max_count]\n",
    "    \n",
    "    def _format_query(self, problem: Dict) -> str:\n",
    "        \"\"\"Format query following LiveCodeBench construction\"\"\"\n",
    "        return f\"\"\"\n",
    "Solve the following coding problem:\n",
    "\n",
    "{problem['description']}\n",
    "\n",
    "```python\n",
    "{problem['starter_code']}\n",
    "```\n",
    "\n",
    "Please provide a complete solution with explanation.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    def _format_response(self, solution: Dict) -> str:\n",
    "        \"\"\"Format response with reasoning + code\"\"\"\n",
    "        return f\"\"\"\n",
    "{solution['reasoning']}\n",
    "\n",
    "```python\n",
    "{solution['code']}\n",
    "```\n",
    "\n",
    "**Complexity Analysis:**\n",
    "{solution.get('complexity', 'Time: O(n), Space: O(1)')}\n",
    "\"\"\".strip()\n",
    "    \n",
    "    def analyze_dataset_quality(self, examples: List[TrainingExample]) -> Dict:\n",
    "        \"\"\"Analyze the quality of the training dataset\"\"\"\n",
    "        analysis = {\n",
    "            'total_examples': len(examples),\n",
    "            'difficulty_distribution': {},\n",
    "            'approach_distribution': {},\n",
    "            'avg_verification_score': 0,\n",
    "            'unique_problems': len(set(ex.problem_id for ex in examples)),\n",
    "            'avg_response_length': 0\n",
    "        }\n",
    "        \n",
    "        if not examples:\n",
    "            return analysis\n",
    "        \n",
    "        # Difficulty distribution\n",
    "        difficulties = [ex.difficulty for ex in examples]\n",
    "        analysis['difficulty_distribution'] = dict(pd.Series(difficulties).value_counts())\n",
    "        \n",
    "        # Approach distribution\n",
    "        approaches = [ex.metadata['approach'] for ex in examples]\n",
    "        analysis['approach_distribution'] = dict(pd.Series(approaches).value_counts())\n",
    "        \n",
    "        # Average verification score\n",
    "        scores = [ex.metadata['verification_score'] for ex in examples]\n",
    "        analysis['avg_verification_score'] = np.mean(scores)\n",
    "        \n",
    "        # Average response length\n",
    "        lengths = [len(ex.response) for ex in examples]\n",
    "        analysis['avg_response_length'] = np.mean(lengths)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def save_dataset(self, examples: List[TrainingExample], \n",
    "                    filepath: str, format: str = 'jsonl'):\n",
    "        \"\"\"Save training dataset in specified format\"\"\"\n",
    "        if format == 'jsonl':\n",
    "            with open(filepath, 'w') as f:\n",
    "                for example in examples:\n",
    "                    data = {\n",
    "                        'problem_id': example.problem_id,\n",
    "                        'difficulty': example.difficulty,\n",
    "                        'query': example.query,\n",
    "                        'response': example.response,\n",
    "                        'metadata': example.metadata\n",
    "                    }\n",
    "                    f.write(json.dumps(data) + '\\n')\n",
    "        elif format == 'hf':  # Hugging Face format\n",
    "            dataset_dict = {\n",
    "                'problem_id': [ex.problem_id for ex in examples],\n",
    "                'difficulty': [ex.difficulty for ex in examples],\n",
    "                'text': [f\"### Query:\\n{ex.query}\\n\\n### Response:\\n{ex.response}\" \n",
    "                        for ex in examples]\n",
    "            }\n",
    "            df = pd.DataFrame(dataset_dict)\n",
    "            df.to_parquet(filepath.replace('.jsonl', '.parquet'))\n",
    "\n",
    "# Test the dataset builder\n",
    "builder = TrainingDatasetBuilder()\n",
    "\n",
    "# Mock verified solutions and problem data\n",
    "mock_verified_solutions = {\n",
    "    'problem_1': verified_solutions[:2],  # Use our earlier solutions\n",
    "    'problem_2': verified_solutions[1:3] if len(verified_solutions) > 2 else verified_solutions\n",
    "}\n",
    "\n",
    "mock_problem_data = {\n",
    "    'problem_1': {\n",
    "        'description': 'Find missing number in arithmetic progression',\n",
    "        'starter_code': 'class Solution:\\n    def missingNumber(self, arr: List[int]) -> int:',\n",
    "        'difficulty': 'Easy'\n",
    "    },\n",
    "    'problem_2': {\n",
    "        'description': 'Another problem description',\n",
    "        'starter_code': 'class Solution:\\n    def solve(self, nums: List[int]) -> int:',\n",
    "        'difficulty': 'Medium'\n",
    "    }\n",
    "}\n",
    "\n",
    "training_examples = builder.build_training_dataset(\n",
    "    mock_verified_solutions, mock_problem_data, max_examples_per_problem=2\n",
    ")\n",
    "\n",
    "print(f\"Built training dataset with {len(training_examples)} examples\")\n",
    "\n",
    "# Analyze dataset quality\n",
    "quality_analysis = builder.analyze_dataset_quality(training_examples)\n",
    "print(\"\\nDataset Quality Analysis:\")\n",
    "print(json.dumps(quality_analysis, indent=2))\n",
    "\n",
    "# Show sample training example\n",
    "if training_examples:\n",
    "    print(\"\\nSample Training Example:\")\n",
    "    example = training_examples[0]\n",
    "    print(f\"Problem ID: {example.problem_id}\")\n",
    "    print(f\"Difficulty: {example.difficulty}\")\n",
    "    print(\"Query:\", example.query[:200] + \"...\")\n",
    "    print(\"Response:\", example.response[:200] + \"...\")\n",
    "    print(\"Metadata:\", example.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Efficient Supervised Fine-Tuning Implementation\n",
    "\n",
    "### Following the Paper's Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientSFTTrainer:\n",
    "    \"\"\"Efficient SFT trainer following paper's configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen2.5-Coder-7B\"):\n",
    "        self.model_name = model_name\n",
    "        # In practice, load the actual model\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "    def prepare_training_config(self, num_examples: int) -> TrainingArguments:\n",
    "        \"\"\"Prepare training configuration following paper's setup\"\"\"\n",
    "        \n",
    "        # Paper's configuration:\n",
    "        # - 3 epochs\n",
    "        # - Learning rate 1e-5\n",
    "        # - Warmup ratio 0.1\n",
    "        # - Cosine learning rate scheduling\n",
    "        # - Batch size 32\n",
    "        \n",
    "        return TrainingArguments(\n",
    "            output_dir=\"./leetcode_sft_model\",\n",
    "            num_train_epochs=3,\n",
    "            learning_rate=1e-5,\n",
    "            per_device_train_batch_size=8,  # Adjusted for demo\n",
    "            gradient_accumulation_steps=4,  # Effective batch size = 32\n",
    "            warmup_ratio=0.1,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            logging_steps=10,\n",
    "            save_steps=500,\n",
    "            eval_steps=100,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=\"wandb\",  # For tracking\n",
    "            run_name=f\"leetcode_sft_{num_examples}_examples\",\n",
    "            dataloader_num_workers=4,\n",
    "            fp16=True,  # Memory optimization\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "    \n",
    "    def create_dataset(self, examples: List[TrainingExample], \n",
    "                      split_ratio: float = 0.9) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Create train/validation datasets\"\"\"\n",
    "        \n",
    "        # Convert to training format\n",
    "        formatted_examples = []\n",
    "        for example in examples:\n",
    "            # Format as instruction-following\n",
    "            text = f\"<|im_start|>user\\n{example.query}<|im_end|>\\n<|im_start|>assistant\\n{example.response}<|im_end|>\"\n",
    "            \n",
    "            formatted_examples.append({\n",
    "                'text': text,\n",
    "                'problem_id': example.problem_id,\n",
    "                'difficulty': example.difficulty\n",
    "            })\n",
    "        \n",
    "        # Split into train/validation\n",
    "        split_idx = int(len(formatted_examples) * split_ratio)\n",
    "        train_examples = formatted_examples[:split_idx]\n",
    "        val_examples = formatted_examples[split_idx:]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = CustomDataset(train_examples)\n",
    "        val_dataset = CustomDataset(val_examples)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def calculate_efficiency_metrics(self, \n",
    "                                   dataset_size: int, \n",
    "                                   baseline_datasets: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate efficiency metrics vs. baseline datasets\"\"\"\n",
    "        \n",
    "        # Paper's baseline comparisons\n",
    "        baselines = {\n",
    "            'Magicoder Evol-Instruct-110K': {'size': 111100, 'humaneval': 77.4, 'mbpp': 74.1},\n",
    "            'Magicoder OSS-Instruct-75K': {'size': 75100, 'humaneval': 73.8, 'mbpp': 76.5},\n",
    "            'Open-R1 CodeForces-CoT': {'size': 9500, 'humaneval': 79.9, 'mbpp': 74.1},\n",
    "            'OpenThoughts 114k': {'size': 19900, 'humaneval': 77.4, 'mbpp': 75.7},\n",
    "            'LeetCodeDataset (model)': {'size': 2600, 'humaneval': 79.9, 'mbpp': 77.5}\n",
    "        }\n",
    "        \n",
    "        # Calculate efficiency\n",
    "        efficiency_metrics = {\n",
    "            'dataset_size': dataset_size,\n",
    "            'size_reduction_vs_largest': dataset_size / max(b['size'] for b in baselines.values()),\n",
    "            'efficiency_comparisons': []\n",
    "        }\n",
    "        \n",
    "        leetcode_performance = baselines['LeetCodeDataset (model)']\n",
    "        \n",
    "        for name, baseline in baselines.items():\n",
    "            if name == 'LeetCodeDataset (model)':\n",
    "                continue\n",
    "                \n",
    "            size_ratio = dataset_size / baseline['size']\n",
    "            performance_ratio_he = leetcode_performance['humaneval'] / baseline['humaneval']\n",
    "            performance_ratio_mbpp = leetcode_performance['mbpp'] / baseline['mbpp']\n",
    "            \n",
    "            efficiency_comparisons = {\n",
    "                'baseline': name,\n",
    "                'size_ratio': size_ratio,\n",
    "                'performance_ratio_humaneval': performance_ratio_he,\n",
    "                'performance_ratio_mbpp': performance_ratio_mbpp,\n",
    "                'efficiency_score': (performance_ratio_he + performance_ratio_mbpp) / (2 * size_ratio)\n",
    "            }\n",
    "            \n",
    "            efficiency_metrics['efficiency_comparisons'].append(efficiency_comparisons)\n",
    "        \n",
    "        return efficiency_metrics\n",
    "    \n",
    "    def simulate_training_results(self, dataset_size: int) -> Dict:\n",
    "        \"\"\"Simulate training results based on paper's findings\"\"\"\n",
    "        \n",
    "        # Simulate performance based on dataset size\n",
    "        # Paper shows diminishing returns after optimal size\n",
    "        optimal_size = 2600\n",
    "        \n",
    "        if dataset_size <= optimal_size:\n",
    "            # Linear improvement up to optimal size\n",
    "            size_factor = dataset_size / optimal_size\n",
    "            humaneval_score = 45 + (79.9 - 45) * size_factor\n",
    "            mbpp_score = 40 + (77.5 - 40) * size_factor\n",
    "        else:\n",
    "            # Diminishing returns beyond optimal size\n",
    "            excess_factor = (dataset_size - optimal_size) / optimal_size\n",
    "            decay = np.exp(-excess_factor * 0.1)  # Exponential decay\n",
    "            humaneval_score = 79.9 * decay\n",
    "            mbpp_score = 77.5 * decay\n",
    "        \n",
    "        return {\n",
    "            'dataset_size': dataset_size,\n",
    "            'humaneval_pass1': round(humaneval_score, 1),\n",
    "            'mbpp_pass1': round(mbpp_score, 1),\n",
    "            'training_time_hours': dataset_size * 0.01,  # Estimate\n",
    "            'is_optimal_size': abs(dataset_size - optimal_size) < 500\n",
    "        }\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Custom dataset for training\"\"\"\n",
    "    \n",
    "    def __init__(self, examples: List[Dict]):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Test the SFT trainer\n",
    "trainer = EfficientSFTTrainer()\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "efficiency_metrics = trainer.calculate_efficiency_metrics(\n",
    "    dataset_size=len(training_examples), \n",
    "    baseline_datasets=[]\n",
    ")\n",
    "\n",
    "print(\"Efficiency Metrics:\")\n",
    "print(json.dumps(efficiency_metrics, indent=2))\n",
    "\n",
    "# Simulate training results for different dataset sizes\n",
    "sizes = [500, 1000, 2600, 5000, 10000, 50000, 111000]\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    result = trainer.simulate_training_results(size)\n",
    "    results.append(result)\n",
    "\n",
    "# Visualize efficiency curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sizes_plot = [r['dataset_size'] for r in results]\n",
    "humaneval_scores = [r['humaneval_pass1'] for r in results]\n",
    "mbpp_scores = [r['mbpp_pass1'] for r in results]\n",
    "\n",
    "# Performance vs dataset size\n",
    "ax1.plot(sizes_plot, humaneval_scores, 'o-', label='HumanEval', linewidth=2, markersize=8)\n",
    "ax1.plot(sizes_plot, mbpp_scores, 's-', label='MBPP', linewidth=2, markersize=8)\n",
    "ax1.axvline(x=2600, color='red', linestyle='--', alpha=0.7, label='Optimal Size (2.6K)')\n",
    "ax1.set_xlabel('Dataset Size')\n",
    "ax1.set_ylabel('Pass@1 Score (%)')\n",
    "ax1.set_title('Performance vs Dataset Size')\n",
    "ax1.set_xscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency ratio\n",
    "efficiency_ratios = []\n",
    "for r in results:\n",
    "    # Efficiency = Performance / Size\n",
    "    avg_performance = (r['humaneval_pass1'] + r['mbpp_pass1']) / 2\n",
    "    efficiency = avg_performance / (r['dataset_size'] / 1000)  # Per 1K examples\n",
    "    efficiency_ratios.append(efficiency)\n",
    "\n",
    "ax2.plot(sizes_plot, efficiency_ratios, 'D-', color='green', linewidth=2, markersize=8)\n",
    "ax2.axvline(x=2600, color='red', linestyle='--', alpha=0.7, label='Optimal Size')\n",
    "ax2.set_xlabel('Dataset Size')\n",
    "ax2.set_ylabel('Efficiency (Performance / 1K Examples)')\n",
    "ax2.set_title('Data Efficiency Analysis')\n",
    "ax2.set_xscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "optimal_result = next(r for r in results if r['dataset_size'] == 2600)\n",
    "largest_result = results[-1]\n",
    "\n",
    "print(f\"Optimal size (2.6K): HumanEval {optimal_result['humaneval_pass1']}%, MBPP {optimal_result['mbpp_pass1']}%\")\n",
    "print(f\"Largest baseline (111K): HumanEval {largest_result['humaneval_pass1']}%, MBPP {largest_result['mbpp_pass1']}%\")\n",
    "print(f\"Size reduction: {2600/111000:.1%} of largest dataset\")\n",
    "print(f\"Performance retention: {optimal_result['humaneval_pass1']/largest_result['humaneval_pass1']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparative Analysis: Human vs Model-Generated Data\n",
    "\n",
    "### Understanding the Quality Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityAnalyzer:\n",
    "    \"\"\"Analyze quality differences between human and model-generated data\"\"\"\n",
    "    \n",
    "    def compare_solution_quality(self, human_solutions: List[str], \n",
    "                               model_solutions: List[str]) -> Dict:\n",
    "        \"\"\"Compare quality metrics between human and model solutions\"\"\"\n",
    "        \n",
    "        comparison = {\n",
    "            'human_solutions': self._analyze_solution_set(human_solutions),\n",
    "            'model_solutions': self._analyze_solution_set(model_solutions),\n",
    "            'quality_differences': {}\n",
    "        }\n",
    "        \n",
    "        # Calculate differences\n",
    "        human_metrics = comparison['human_solutions']\n",
    "        model_metrics = comparison['model_solutions']\n",
    "        \n",
    "        comparison['quality_differences'] = {\n",
    "            'avg_length_ratio': model_metrics['avg_length'] / human_metrics['avg_length'],\n",
    "            'comment_density_ratio': model_metrics['comment_density'] / human_metrics['comment_density'],\n",
    "            'reasoning_score_ratio': model_metrics['reasoning_score'] / human_metrics['reasoning_score'],\n",
    "            'readability_ratio': model_metrics['readability_score'] / human_metrics['readability_score']\n",
    "        }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _analyze_solution_set(self, solutions: List[str]) -> Dict:\n",
    "        \"\"\"Analyze a set of solutions\"\"\"\n",
    "        if not solutions:\n",
    "            return {}\n",
    "            \n",
    "        metrics = {\n",
    "            'count': len(solutions),\n",
    "            'avg_length': np.mean([len(sol) for sol in solutions]),\n",
    "            'comment_density': np.mean([self._calculate_comment_density(sol) for sol in solutions]),\n",
    "            'reasoning_score': np.mean([self._calculate_reasoning_score(sol) for sol in solutions]),\n",
    "            'readability_score': np.mean([self._calculate_readability_score(sol) for sol in solutions]),\n",
    "            'variable_naming_score': np.mean([self._calculate_variable_naming_score(sol) for sol in solutions])\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_comment_density(self, solution: str) -> float:\n",
    "        \"\"\"Calculate density of comments in solution\"\"\"\n",
    "        lines = solution.split('\\n')\n",
    "        comment_lines = [line for line in lines if line.strip().startswith('#')]\n",
    "        code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]\n",
    "        \n",
    "        if not code_lines:\n",
    "            return 0\n",
    "        \n",
    "        return len(comment_lines) / len(code_lines)\n",
    "    \n",
    "    def _calculate_reasoning_score(self, solution: str) -> float:\n",
    "        \"\"\"Calculate reasoning quality score\"\"\"\n",
    "        reasoning_indicators = [\n",
    "            'step', 'because', 'since', 'therefore', 'thus',\n",
    "            'algorithm', 'approach', 'strategy', 'intuition',\n",
    "            'complexity', 'time', 'space', 'optimization'\n",
    "        ]\n",
    "        \n",
    "        solution_lower = solution.lower()\n",
    "        score = sum(1 for indicator in reasoning_indicators \n",
    "                   if indicator in solution_lower)\n",
    "        \n",
    "        # Normalize by solution length\n",
    "        return score / max(len(solution.split()), 1) * 100\n",
    "    \n",
    "    def _calculate_readability_score(self, solution: str) -> float:\n",
    "        \"\"\"Calculate readability score based on variable names and structure\"\"\"\n",
    "        lines = solution.split('\\n')\n",
    "        code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]\n",
    "        \n",
    "        if not code_lines:\n",
    "            return 0\n",
    "        \n",
    "        # Count descriptive variable names\n",
    "        descriptive_vars = 0\n",
    "        total_vars = 0\n",
    "        \n",
    "        for line in code_lines:\n",
    "            # Simple heuristic: look for assignment operators\n",
    "            if '=' in line and not line.strip().startswith('def'):\n",
    "                var_part = line.split('=')[0].strip()\n",
    "                if var_part and not var_part.startswith(('if', 'for', 'while')):\n",
    "                    total_vars += 1\n",
    "                    # Check if variable name is descriptive (length > 2)\n",
    "                    if len(var_part) > 2 and var_part.isidentifier():\n",
    "                        descriptive_vars += 1\n",
    "        \n",
    "        if total_vars == 0:\n",
    "            return 50  # Neutral score\n",
    "        \n",
    "        return (descriptive_vars / total_vars) * 100\n",
    "    \n",
    "    def _calculate_variable_naming_score(self, solution: str) -> float:\n",
    "        \"\"\"Score variable naming quality\"\"\"\n",
    "        # Extract variable names using regex\n",
    "        import re\n",
    "        var_pattern = r'\\b([a-zA-Z_][a-zA-Z0-9_]*)\\s*='\n",
    "        variables = re.findall(var_pattern, solution)\n",
    "        \n",
    "        if not variables:\n",
    "            return 50\n",
    "        \n",
    "        good_names = 0\n",
    "        for var in variables:\n",
    "            # Good naming: descriptive, not single letter (except i, j for loops)\n",
    "            if len(var) > 2 or var in ['i', 'j', 'k']:\n",
    "                good_names += 1\n",
    "        \n",
    "        return (good_names / len(variables)) * 100\n",
    "    \n",
    "    def create_quality_visualization(self, comparison: Dict):\n",
    "        \"\"\"Create visualization of quality comparison\"\"\"\n",
    "        human = comparison['human_solutions']\n",
    "        model = comparison['model_solutions']\n",
    "        \n",
    "        metrics = ['comment_density', 'reasoning_score', 'readability_score', 'variable_naming_score']\n",
    "        human_values = [human[metric] for metric in metrics]\n",
    "        model_values = [model[metric] for metric in metrics]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, human_values, width, label='Human Solutions', alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, model_values, width, label='Model Solutions', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Quality Metrics')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title('Human vs Model-Generated Solution Quality')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        def autolabel(bars):\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.annotate(f'{height:.1f}',\n",
    "                           xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                           xytext=(0, 3),\n",
    "                           textcoords=\"offset points\",\n",
    "                           ha='center', va='bottom')\n",
    "        \n",
    "        autolabel(bars1)\n",
    "        autolabel(bars2)\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create sample solutions for comparison\n",
    "human_solutions = [\n",
    "    \"\"\"\n",
    "class Solution:\n",
    "    def missingNumber(self, arr):\n",
    "        n = len(arr)\n",
    "        d = (arr[-1] - arr[0]) // n\n",
    "        for i in range(n-1):\n",
    "            if arr[i+1] - arr[i] != d:\n",
    "                return arr[i] + d\n",
    "        return arr[-1] + d\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "class Solution:\n",
    "    def missingNumber(self, arr):\n",
    "        total = (len(arr) + 1) * (arr[0] + arr[-1]) // 2\n",
    "        return total - sum(arr)\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "model_solutions = [\n",
    "    \"\"\"\n",
    "class Solution:\n",
    "    def missingNumber(self, arr: List[int]) -> int:\n",
    "        # Step 1: Understanding the problem\n",
    "        # We have an arithmetic progression with one missing element\n",
    "        # Need to find that missing number\n",
    "        \n",
    "        n = len(arr)\n",
    "        \n",
    "        # Step 2: Calculate the common difference\n",
    "        # In a complete AP, total difference = (last - first)\n",
    "        # With n+1 elements, common difference = total_diff / n\n",
    "        total_difference = arr[-1] - arr[0]\n",
    "        common_diff = total_difference // n\n",
    "        \n",
    "        # Step 3: Find the missing number\n",
    "        # Check each consecutive pair for the gap\n",
    "        for i in range(n - 1):\n",
    "            expected_next = arr[i] + common_diff\n",
    "            if arr[i + 1] != expected_next:\n",
    "                return expected_next\n",
    "        \n",
    "        # Edge case: missing number is at the end\n",
    "        return arr[-1] + common_diff\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "class Solution:\n",
    "    def missingNumber(self, arr: List[int]) -> int:\n",
    "        # Mathematical approach using sum formula\n",
    "        # For arithmetic progression: sum = n * (first + last) / 2\n",
    "        \n",
    "        array_length = len(arr)\n",
    "        first_element = arr[0]\n",
    "        last_element = arr[-1]\n",
    "        \n",
    "        # Calculate expected sum of complete progression\n",
    "        # We have (n+1) elements in complete sequence\n",
    "        complete_sequence_length = array_length + 1\n",
    "        expected_sum = complete_sequence_length * (first_element + last_element) // 2\n",
    "        \n",
    "        # Calculate actual sum of given array\n",
    "        actual_sum = sum(arr)\n",
    "        \n",
    "        # The difference gives us the missing number\n",
    "        missing_number = expected_sum - actual_sum\n",
    "        \n",
    "        return missing_number\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# Analyze quality differences\n",
    "analyzer = DataQualityAnalyzer()\n",
    "quality_comparison = analyzer.compare_solution_quality(human_solutions, model_solutions)\n",
    "\n",
    "print(\"Quality Comparison Results:\")\n",
    "print(json.dumps(quality_comparison, indent=2))\n",
    "\n",
    "# Create visualization\n",
    "analyzer.create_quality_visualization(quality_comparison)\n",
    "\n",
    "# Key insights\n",
    "differences = quality_comparison['quality_differences']\n",
    "print(\"\\nKey Quality Differences:\")\n",
    "print(f\"Model solutions are {differences['avg_length_ratio']:.1f}x longer (more detailed)\")\n",
    "print(f\"Model solutions have {differences['comment_density_ratio']:.1f}x more comments\")\n",
    "print(f\"Model solutions have {differences['reasoning_score_ratio']:.1f}x more reasoning indicators\")\n",
    "print(f\"Model solutions have {differences['readability_ratio']:.1f}x better readability scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Efficiency Analysis\n",
    "\n",
    "### Quantifying the 40x Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficiencyAnalyzer:\n",
    "    \"\"\"Analyze training efficiency improvements\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Paper's benchmark results\n",
    "        self.benchmark_results = {\n",
    "            'Magicoder Evol-Instruct-110K': {\n",
    "                'size': 111100, 'humaneval': 77.4, 'mbpp': 74.1,\n",
    "                'type': 'large_synthetic'\n",
    "            },\n",
    "            'Magicoder OSS-Instruct-75K': {\n",
    "                'size': 75100, 'humaneval': 73.8, 'mbpp': 76.5,\n",
    "                'type': 'large_oss'\n",
    "            },\n",
    "            'Open-R1 CodeForces-CoT': {\n",
    "                'size': 9500, 'humaneval': 79.9, 'mbpp': 74.1,\n",
    "                'type': 'medium_reasoning'\n",
    "            },\n",
    "            'OpenThoughts 114k': {\n",
    "                'size': 19900, 'humaneval': 77.4, 'mbpp': 75.7,\n",
    "                'type': 'medium_synthetic'\n",
    "            },\n",
    "            'LeetCodeDataset (human)': {\n",
    "                'size': 2600, 'humaneval': 55.5, 'mbpp': 53.4,\n",
    "                'type': 'small_human'\n",
    "            },\n",
    "            'LeetCodeDataset (model)': {\n",
    "                'size': 2600, 'humaneval': 79.9, 'mbpp': 77.5,\n",
    "                'type': 'small_model'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_efficiency_scores(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate comprehensive efficiency scores\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for name, results in self.benchmark_results.items():\n",
    "            # Calculate efficiency metrics\n",
    "            avg_performance = (results['humaneval'] + results['mbpp']) / 2\n",
    "            size_k = results['size'] / 1000  # Size in thousands\n",
    "            \n",
    "            efficiency_score = avg_performance / size_k\n",
    "            \n",
    "            # Performance per parameter (assuming standard scaling)\n",
    "            perf_per_1k = avg_performance / size_k\n",
    "            \n",
    "            data.append({\n",
    "                'dataset': name,\n",
    "                'size': results['size'],\n",
    "                'size_k': size_k,\n",
    "                'humaneval': results['humaneval'],\n",
    "                'mbpp': results['mbpp'],\n",
    "                'avg_performance': avg_performance,\n",
    "                'efficiency_score': efficiency_score,\n",
    "                'type': results['type']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def analyze_improvement_factors(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze improvement factors\"\"\"\n",
    "        leetcode_model = df[df['dataset'] == 'LeetCodeDataset (model)'].iloc[0]\n",
    "        leetcode_human = df[df['dataset'] == 'LeetCodeDataset (human)'].iloc[0]\n",
    "        largest_dataset = df.loc[df['size'].idxmax()]\n",
    "        \n",
    "        analysis = {\n",
    "            'model_vs_human_same_size': {\n",
    "                'humaneval_improvement': leetcode_model['humaneval'] / leetcode_human['humaneval'],\n",
    "                'mbpp_improvement': leetcode_model['mbpp'] / leetcode_human['mbpp'],\n",
    "                'avg_improvement': leetcode_model['avg_performance'] / leetcode_human['avg_performance']\n",
    "            },\n",
    "            'model_vs_largest': {\n",
    "                'size_reduction': leetcode_model['size'] / largest_dataset['size'],\n",
    "                'performance_retention': leetcode_model['avg_performance'] / largest_dataset['avg_performance'],\n",
    "                'efficiency_gain': leetcode_model['efficiency_score'] / largest_dataset['efficiency_score']\n",
    "            },\n",
    "            'overall_efficiency': {\n",
    "                'best_efficiency': df.loc[df['efficiency_score'].idxmax(), 'dataset'],\n",
    "                'worst_efficiency': df.loc[df['efficiency_score'].idxmin(), 'dataset'],\n",
    "                'efficiency_range': df['efficiency_score'].max() / df['efficiency_score'].min()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def create_efficiency_dashboard(self, df: pd.DataFrame):\n",
    "        \"\"\"Create comprehensive efficiency visualization\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Size vs Performance scatter\n",
    "        colors = {'large_synthetic': 'red', 'large_oss': 'orange', \n",
    "                 'medium_reasoning': 'blue', 'medium_synthetic': 'purple',\n",
    "                 'small_human': 'gray', 'small_model': 'green'}\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            ax1.scatter(row['size_k'], row['avg_performance'], \n",
    "                       c=colors[row['type']], s=200, alpha=0.7,\n",
    "                       label=row['type'] if row['type'] not in ax1.get_legend_handles_labels()[1] else \"\")\n",
    "            ax1.annotate(row['dataset'].split()[0], \n",
    "                        (row['size_k'], row['avg_performance']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        ax1.set_xlabel('Dataset Size (thousands)')\n",
    "        ax1.set_ylabel('Average Performance (%)')\n",
    "        ax1.set_title('Dataset Size vs Performance')\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Efficiency scores\n",
    "        df_sorted = df.sort_values('efficiency_score', ascending=True)\n",
    "        bars = ax2.barh(range(len(df_sorted)), df_sorted['efficiency_score'])\n",
    "        ax2.set_yticks(range(len(df_sorted)))\n",
    "        ax2.set_yticklabels([d.split()[0] for d in df_sorted['dataset']])\n",
    "        ax2.set_xlabel('Efficiency Score (Performance / 1K examples)')\n",
    "        ax2.set_title('Dataset Efficiency Ranking')\n",
    "        \n",
    "        # Highlight LeetCodeDataset\n",
    "        for i, (_, row) in enumerate(df_sorted.iterrows()):\n",
    "            if 'LeetCodeDataset (model)' in row['dataset']:\n",
    "                bars[i].set_color('green')\n",
    "                bars[i].set_alpha(0.8)\n",
    "        \n",
    "        # 3. HumanEval vs MBPP\n",
    "        ax3.scatter(df['humaneval'], df['mbpp'], s=200, alpha=0.7)\n",
    "        for _, row in df.iterrows():\n",
    "            ax3.annotate(row['dataset'].split()[0], \n",
    "                        (row['humaneval'], row['mbpp']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        ax3.plot([0, 100], [0, 100], 'k--', alpha=0.5, label='Equal Performance')\n",
    "        ax3.set_xlabel('HumanEval Pass@1 (%)')\n",
    "        ax3.set_ylabel('MBPP Pass@1 (%)')\n",
    "        ax3.set_title('HumanEval vs MBPP Performance')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Efficiency improvement over size\n",
    "        df_by_size = df.sort_values('size')\n",
    "        ax4.plot(df_by_size['size_k'], df_by_size['efficiency_score'], 'o-', linewidth=2, markersize=8)\n",
    "        \n",
    "        # Highlight optimal point\n",
    "        optimal_idx = df['efficiency_score'].idxmax()\n",
    "        optimal_row = df.loc[optimal_idx]\n",
    "        ax4.scatter(optimal_row['size_k'], optimal_row['efficiency_score'], \n",
    "                   color='red', s=300, marker='*', label='Optimal Efficiency')\n",
    "        \n",
    "        ax4.set_xlabel('Dataset Size (thousands)')\n",
    "        ax4.set_ylabel('Efficiency Score')\n",
    "        ax4.set_title('Efficiency vs Dataset Size')\n",
    "        ax4.set_xscale('log')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_efficiency_report(self, df: pd.DataFrame, analysis: Dict) -> str:\n",
    "        \"\"\"Generate comprehensive efficiency report\"\"\"\n",
    "        report = f\"\"\"\n",
    "# LeetCodeDataset Efficiency Analysis Report\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Model-Generated vs Human-Written Data (Same Size)\n",
    "- HumanEval improvement: {analysis['model_vs_human_same_size']['humaneval_improvement']:.1f}x\n",
    "- MBPP improvement: {analysis['model_vs_human_same_size']['mbpp_improvement']:.1f}x\n",
    "- Average improvement: {analysis['model_vs_human_same_size']['avg_improvement']:.1f}x\n",
    "\n",
    "### 2. Model-Generated vs Largest Baseline\n",
    "- Dataset size reduction: {1/analysis['model_vs_largest']['size_reduction']:.0f}x smaller\n",
    "- Performance retention: {analysis['model_vs_largest']['performance_retention']:.1%}\n",
    "- Efficiency gain: {analysis['model_vs_largest']['efficiency_gain']:.1f}x\n",
    "\n",
    "### 3. Overall Efficiency Ranking\n",
    "- Most efficient: {analysis['overall_efficiency']['best_efficiency']}\n",
    "- Least efficient: {analysis['overall_efficiency']['worst_efficiency']}\n",
    "- Efficiency range: {analysis['overall_efficiency']['efficiency_range']:.1f}x difference\n",
    "\n",
    "## Data Details\n",
    "\"\"\"\n",
    "        \n",
    "        # Add detailed table\n",
    "        report += \"\\n### Dataset Comparison Table\\n\\n\"\n",
    "        report += df.to_string(index=False, float_format='%.1f')\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Run efficiency analysis\n",
    "analyzer = EfficiencyAnalyzer()\n",
    "efficiency_df = analyzer.calculate_efficiency_scores()\n",
    "improvement_analysis = analyzer.analyze_improvement_factors(efficiency_df)\n",
    "\n",
    "print(\"Efficiency Analysis:\")\n",
    "print(json.dumps(improvement_analysis, indent=2))\n",
    "\n",
    "# Create visualization dashboard\n",
    "analyzer.create_efficiency_dashboard(efficiency_df)\n",
    "\n",
    "# Generate report\n",
    "efficiency_report = analyzer.generate_efficiency_report(efficiency_df, improvement_analysis)\n",
    "print(efficiency_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways and Best Practices\n",
    "\n",
    "### Critical Insights from the Paper:\n",
    "\n",
    "1. **Quality > Quantity**: 2.6K high-quality examples outperform 111K lower-quality ones\n",
    "2. **Model-Generated > Human**: For training purposes, model solutions are superior due to explicit reasoning\n",
    "3. **Multi-Stage Generation**: High-temp diversity â†’ verification â†’ ground-truth hints for difficult cases\n",
    "4. **Efficiency Sweet Spot**: There's an optimal dataset size beyond which returns diminish\n",
    "\n",
    "### Implementation Best Practices:\n",
    "\n",
    "1. **Generation Strategy**:\n",
    "   - Use high temperature (T=1.0) for diversity\n",
    "   - Generate multiple solution approaches per problem\n",
    "   - Implement robust verification with comprehensive test cases\n",
    "\n",
    "2. **Quality Control**:\n",
    "   - Verify ALL solutions against test cases\n",
    "   - Use ground-truth hints for persistently failing problems\n",
    "   - Prioritize solutions with explicit reasoning and clear explanations\n",
    "\n",
    "3. **Training Configuration**:\n",
    "   - 3 epochs, learning rate 1e-5\n",
    "   - Warmup ratio 0.1, cosine scheduling\n",
    "   - Batch size 32 (adjust for hardware)\n",
    "\n",
    "4. **Data Efficiency**:\n",
    "   - Aim for 2-3K high-quality examples rather than 10K+ lower quality\n",
    "   - Focus on approach diversity within the dataset\n",
    "   - Monitor efficiency metrics during collection\n",
    "\n",
    "### Research Implications:\n",
    "\n",
    "1. **Rethink Data Collection**: Manual curation may be less effective than automated high-quality generation\n",
    "2. **Model as Teacher**: Use strong models to create training data for weaker models\n",
    "3. **Domain-Specific Optimization**: Competitive programming benefits from reasoning-rich solutions\n",
    "4. **Efficiency Metrics**: Track performance-per-example rather than just absolute performance\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "1. **Cross-Domain Application**: Apply these principles to other coding domains\n",
    "2. **Dynamic Generation**: Adapt generation strategy based on model weaknesses\n",
    "3. **Multi-Modal Integration**: Include visual/algorithmic reasoning in solutions\n",
    "4. **Continuous Learning**: Update training data as new problems emerge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}