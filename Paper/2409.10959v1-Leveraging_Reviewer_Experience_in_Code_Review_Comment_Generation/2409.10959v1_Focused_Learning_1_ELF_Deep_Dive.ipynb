{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Experience-Aware Loss Functions (ELF) Deep Dive\n",
    "\n",
    "## Learning Objective\n",
    "Master the mathematical foundations and implementation details of Experience-Aware Loss Functions (ELF), understanding how reviewer experience can be embedded into neural model training through weighted loss functions.\n",
    "\n",
    "## Paper Reference\n",
    "- **Section 3.3**: Experience-Aware Loss Functions (Pages 8-9)\n",
    "- **Equation (3)**: L_RCG = ω * Σ(-log P(w_t|c,w_<t))\n",
    "- **Figure 2**: Overview of Experimental Design\n",
    "\n",
    "## Why ELF is Complex and Important\n",
    "1. **Novel Integration**: First method to directly incorporate software engineering metrics into neural loss functions\n",
    "2. **Dynamic Weighting**: Continuous ownership values replace discrete thresholds\n",
    "3. **Multi-strategy Design**: Four different weighting strategies for different experience types\n",
    "4. **Gradient Impact**: Weighted loss affects gradient direction and model convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation of ELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Standard Cross-Entropy Loss vs ELF\n",
    "\n",
    "Let's first understand how ELF modifies the standard loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardLoss(nn.Module):\n",
    "    \"\"\"Standard Cross-Entropy Loss for sequence generation\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Standard loss: L = Σ(-log P(w_t|c,w_<t))\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(-1, vocab_size)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        losses = self.ce_loss(logits_flat, targets_flat)\n",
    "        losses = losses.view(batch_size, seq_len)\n",
    "        \n",
    "        # Average over sequence and batch\n",
    "        return losses.mean()\n",
    "\n",
    "class ExperienceAwareLoss(nn.Module):\n",
    "    \"\"\"ELF: Experience-weighted Cross-Entropy Loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, logits, targets, weights):\n",
    "        \"\"\"\n",
    "        ELF loss: L = ω * Σ(-log P(w_t|c,w_<t))\n",
    "        where ω is experience-based weight\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(-1, vocab_size)\n",
    "        targets_flat = targets.view(-1)\n",
    "        \n",
    "        losses = self.ce_loss(logits_flat, targets_flat)\n",
    "        losses = losses.view(batch_size, seq_len)\n",
    "        \n",
    "        # Apply experience weights to each sample\n",
    "        weighted_losses = []\n",
    "        for i in range(batch_size):\n",
    "            weighted_loss = weights[i] * losses[i].mean()\n",
    "            weighted_losses.append(weighted_loss)\n",
    "        \n",
    "        return torch.stack(weighted_losses).mean()\n",
    "\n",
    "# Demonstrate the difference\n",
    "batch_size, seq_len, vocab_size = 4, 10, 100\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Experience weights (high vs low experience)\n",
    "weights = torch.tensor([7.39, 7.39, 2.72, 2.72])  # e^(1+0.3) vs e^(1+0.0)\n",
    "\n",
    "standard_loss = StandardLoss()\n",
    "elf_loss = ExperienceAwareLoss()\n",
    "\n",
    "loss_standard = standard_loss(logits, targets)\n",
    "loss_elf = elf_loss(logits, targets, weights)\n",
    "\n",
    "print(f\"Standard Loss: {loss_standard:.4f}\")\n",
    "print(f\"ELF Loss: {loss_elf:.4f}\")\n",
    "print(f\"Ratio: {loss_elf/loss_standard:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Four ELF Weighting Strategies\n",
    "\n",
    "Deep dive into each weighting strategy and their mathematical formulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELFWeightCalculator:\n",
    "    \"\"\"Calculate weights for all four ELF strategies\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_aco(aco: float) -> float:\n",
    "        \"\"\"ω_aco = e^(1+aco) - Authoring experience only\"\"\"\n",
    "        return np.exp(1 + aco)\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_rso(rso: float) -> float:\n",
    "        \"\"\"ω_rso = e^(1+rso) - Reviewing experience only\"\"\"\n",
    "        return np.exp(1 + rso)\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_avg(aco: float, rso: float) -> float:\n",
    "        \"\"\"ω_avg = e^(1+(rso+aco)/2) - Average of both experiences\"\"\"\n",
    "        return np.exp(1 + (rso + aco) / 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight_max(aco: float, rso: float) -> float:\n",
    "        \"\"\"ω_max = e^(1+max(rso,aco)) - Maximum experience\"\"\"\n",
    "        return np.exp(1 + max(rso, aco))\n",
    "\n",
    "# Visualize weight functions\n",
    "ownership_range = np.linspace(0, 0.5, 100)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: ACO weight function\n",
    "ax1 = axes[0, 0]\n",
    "weights_aco = [ELFWeightCalculator.weight_aco(x) for x in ownership_range]\n",
    "ax1.plot(ownership_range, weights_aco, 'b-', linewidth=2)\n",
    "ax1.set_title('ω_aco = e^(1+aco)', fontsize=14)\n",
    "ax1.set_xlabel('ACO (Authoring Code Ownership)')\n",
    "ax1.set_ylabel('Weight')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: RSO weight function\n",
    "ax2 = axes[0, 1]\n",
    "weights_rso = [ELFWeightCalculator.weight_rso(x) for x in ownership_range]\n",
    "ax2.plot(ownership_range, weights_rso, 'g-', linewidth=2)\n",
    "ax2.set_title('ω_rso = e^(1+rso)', fontsize=14)\n",
    "ax2.set_xlabel('RSO (Review-Specific Ownership)')\n",
    "ax2.set_ylabel('Weight')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: 3D surface for AVG strategy\n",
    "ax3 = fig.add_subplot(223, projection='3d')\n",
    "aco_grid, rso_grid = np.meshgrid(ownership_range, ownership_range)\n",
    "weights_avg = np.array([[ELFWeightCalculator.weight_avg(a, r) \n",
    "                        for a in ownership_range] for r in ownership_range])\n",
    "surf = ax3.plot_surface(aco_grid, rso_grid, weights_avg, cmap='viridis', alpha=0.8)\n",
    "ax3.set_title('ω_avg = e^(1+(rso+aco)/2)', fontsize=14)\n",
    "ax3.set_xlabel('ACO')\n",
    "ax3.set_ylabel('RSO')\n",
    "ax3.set_zlabel('Weight')\n",
    "\n",
    "# Plot 4: Comparison of all strategies\n",
    "ax4 = axes[1, 1]\n",
    "aco_fixed = 0.2\n",
    "rso_values = ownership_range\n",
    "ax4.plot(rso_values, [ELFWeightCalculator.weight_aco(aco_fixed) for _ in rso_values], \n",
    "         'b-', label=f'ACO only (ACO={aco_fixed})', linewidth=2)\n",
    "ax4.plot(rso_values, [ELFWeightCalculator.weight_rso(r) for r in rso_values], \n",
    "         'g-', label='RSO only', linewidth=2)\n",
    "ax4.plot(rso_values, [ELFWeightCalculator.weight_avg(aco_fixed, r) for r in rso_values], \n",
    "         'r-', label='AVG', linewidth=2)\n",
    "ax4.plot(rso_values, [ELFWeightCalculator.weight_max(aco_fixed, r) for r in rso_values], \n",
    "         'm-', label='MAX', linewidth=2)\n",
    "ax4.set_title('Strategy Comparison (ACO=0.2)', fontsize=14)\n",
    "ax4.set_xlabel('RSO')\n",
    "ax4.set_ylabel('Weight')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show example weights for different reviewer profiles\n",
    "print(\"\\nExample Weights for Different Reviewer Profiles:\")\n",
    "profiles = [\n",
    "    {\"name\": \"Expert Reviewer\", \"aco\": 0.35, \"rso\": 0.45},\n",
    "    {\"name\": \"Mid-level Reviewer\", \"aco\": 0.10, \"rso\": 0.20},\n",
    "    {\"name\": \"New Reviewer\", \"aco\": 0.02, \"rso\": 0.05}\n",
    "]\n",
    "\n",
    "for profile in profiles:\n",
    "    print(f\"\\n{profile['name']} (ACO={profile['aco']:.2f}, RSO={profile['rso']:.2f}):\")\n",
    "    print(f\"  ω_aco = {ELFWeightCalculator.weight_aco(profile['aco']):.3f}\")\n",
    "    print(f\"  ω_rso = {ELFWeightCalculator.weight_rso(profile['rso']):.3f}\")\n",
    "    print(f\"  ω_avg = {ELFWeightCalculator.weight_avg(profile['aco'], profile['rso']):.3f}\")\n",
    "    print(f\"  ω_max = {ELFWeightCalculator.weight_max(profile['aco'], profile['rso']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impact on Gradient Updates\n",
    "\n",
    "Understanding how ELF affects the learning process through gradient modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAnalyzer:\n",
    "    \"\"\"Analyze how ELF affects gradient updates\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 10, hidden_dim: int = 20, output_dim: int = 10):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def compute_gradients(self, x, y, weight=1.0):\n",
    "        \"\"\"Compute gradients with given weight\"\"\"\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        weighted_loss = weight * loss\n",
    "        weighted_loss.backward()\n",
    "        \n",
    "        # Extract gradients\n",
    "        gradients = []\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                gradients.append(param.grad.clone().flatten())\n",
    "        \n",
    "        return torch.cat(gradients)\n",
    "    \n",
    "    def visualize_gradient_impact(self):\n",
    "        \"\"\"Visualize how different weights affect gradients\"\"\"\n",
    "        # Create sample data\n",
    "        x = torch.randn(1, 10)\n",
    "        y = torch.tensor([5])\n",
    "        \n",
    "        # Different reviewer profiles\n",
    "        profiles = [\n",
    "            {\"name\": \"Low Exp (ω=2.72)\", \"weight\": 2.72, \"color\": \"blue\"},\n",
    "            {\"name\": \"Mid Exp (ω=4.48)\", \"weight\": 4.48, \"color\": \"green\"},\n",
    "            {\"name\": \"High Exp (ω=7.39)\", \"weight\": 7.39, \"color\": \"red\"}\n",
    "        ]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Gradient magnitudes\n",
    "        plt.subplot(1, 3, 1)\n",
    "        for profile in profiles:\n",
    "            grads = self.compute_gradients(x, y, profile[\"weight\"])\n",
    "            grad_magnitude = torch.norm(grads).item()\n",
    "            plt.bar(profile[\"name\"], grad_magnitude, color=profile[\"color\"], alpha=0.7)\n",
    "        plt.title(\"Gradient Magnitude by Experience Level\", fontsize=14)\n",
    "        plt.ylabel(\"||∇L||\")\n",
    "        \n",
    "        # Plot 2: Gradient direction comparison\n",
    "        plt.subplot(1, 3, 2)\n",
    "        base_grads = self.compute_gradients(x, y, 1.0)\n",
    "        for i, profile in enumerate(profiles):\n",
    "            grads = self.compute_gradients(x, y, profile[\"weight\"])\n",
    "            # Normalize for direction comparison\n",
    "            grads_norm = grads / torch.norm(grads)\n",
    "            base_norm = base_grads / torch.norm(base_grads)\n",
    "            cosine_sim = torch.dot(grads_norm, base_norm).item()\n",
    "            plt.bar(profile[\"name\"], cosine_sim, color=profile[\"color\"], alpha=0.7)\n",
    "        plt.title(\"Gradient Direction Similarity to Baseline\", fontsize=14)\n",
    "        plt.ylabel(\"Cosine Similarity\")\n",
    "        plt.ylim(0.98, 1.01)\n",
    "        \n",
    "        # Plot 3: Learning rate effect\n",
    "        plt.subplot(1, 3, 3)\n",
    "        learning_rates = np.logspace(-4, -1, 50)\n",
    "        for profile in profiles:\n",
    "            effective_lr = learning_rates * profile[\"weight\"]\n",
    "            plt.plot(learning_rates, effective_lr, color=profile[\"color\"], \n",
    "                    label=profile[\"name\"], linewidth=2)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"Base Learning Rate\")\n",
    "        plt.ylabel(\"Effective Learning Rate\")\n",
    "        plt.title(\"Effective Learning Rate by Experience\", fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Analyze gradient impacts\n",
    "analyzer = GradientAnalyzer()\n",
    "analyzer.visualize_gradient_impact()\n",
    "\n",
    "# Mathematical explanation\n",
    "print(\"\\nMathematical Impact of ELF on Gradients:\")\n",
    "print(\"\\nFor standard loss: ∇θ L = ∇θ Σ(-log P(w_t|c,w_<t))\")\n",
    "print(\"For ELF loss: ∇θ L_ELF = ω * ∇θ Σ(-log P(w_t|c,w_<t))\")\n",
    "print(\"\\nThe weight ω directly scales the gradient magnitude:\")\n",
    "print(\"- High experience (ω=7.39): 7.39x larger gradient updates\")\n",
    "print(\"- Low experience (ω=2.72): 2.72x larger gradient updates\")\n",
    "print(\"- Ratio: High/Low = 2.72x more influence for experienced reviewers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing ELF in Practice\n",
    "\n",
    "Complete implementation with batch processing and optimization considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELFTrainer:\n",
    "    \"\"\"Complete ELF training implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, strategy=\"aco\", granularity=\"package\"):\n",
    "        self.model = model\n",
    "        self.strategy = strategy\n",
    "        self.granularity = granularity\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def calculate_batch_weights(self, batch_metrics):\n",
    "        \"\"\"Calculate ELF weights for a batch of samples\"\"\"\n",
    "        weights = []\n",
    "        \n",
    "        for metrics in batch_metrics:\n",
    "            # Extract ownership values based on granularity\n",
    "            if self.granularity == \"repository\":\n",
    "                aco, rso = metrics['aco_repo'], metrics['rso_repo']\n",
    "            elif self.granularity == \"subsystem\":\n",
    "                aco, rso = metrics['aco_sys'], metrics['rso_sys']\n",
    "            else:  # package\n",
    "                aco, rso = metrics['aco_pkg'], metrics['rso_pkg']\n",
    "            \n",
    "            # Apply strategy\n",
    "            if self.strategy == \"aco\":\n",
    "                weight = np.exp(1 + aco)\n",
    "            elif self.strategy == \"rso\":\n",
    "                weight = np.exp(1 + rso)\n",
    "            elif self.strategy == \"avg\":\n",
    "                weight = np.exp(1 + (aco + rso) / 2)\n",
    "            else:  # max\n",
    "                weight = np.exp(1 + max(aco, rso))\n",
    "                \n",
    "            weights.append(weight)\n",
    "            \n",
    "        return torch.tensor(weights, dtype=torch.float32)\n",
    "    \n",
    "    def train_step(self, batch_data, batch_targets, batch_metrics):\n",
    "        \"\"\"Single training step with ELF\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(batch_data)\n",
    "        \n",
    "        # Calculate weights\n",
    "        weights = self.calculate_batch_weights(batch_metrics)\n",
    "        \n",
    "        # Calculate ELF loss\n",
    "        ce_loss = F.cross_entropy(outputs, batch_targets, reduction='none')\n",
    "        weighted_losses = weights * ce_loss\n",
    "        loss = weighted_losses.mean()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Log statistics\n",
    "        stats = {\n",
    "            'loss': loss.item(),\n",
    "            'avg_weight': weights.mean().item(),\n",
    "            'weight_std': weights.std().item(),\n",
    "            'max_weight': weights.max().item(),\n",
    "            'min_weight': weights.min().item()\n",
    "        }\n",
    "        \n",
    "        self.loss_history.append(stats)\n",
    "        return stats\n",
    "\n",
    "# Demonstrate training simulation\n",
    "def simulate_elf_training():\n",
    "    \"\"\"Simulate ELF training process\"\"\"\n",
    "    \n",
    "    # Simple model for demonstration\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(50, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 10)\n",
    "    )\n",
    "    \n",
    "    # Create trainers for different strategies\n",
    "    strategies = [\"aco\", \"rso\", \"avg\", \"max\"]\n",
    "    trainers = {s: ELFTrainer(model, strategy=s) for s in strategies}\n",
    "    \n",
    "    # Simulate training\n",
    "    n_steps = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Generate mock batch\n",
    "        batch_data = torch.randn(batch_size, 50)\n",
    "        batch_targets = torch.randint(0, 10, (batch_size,))\n",
    "        \n",
    "        # Generate mock metrics (mixed experience levels)\n",
    "        batch_metrics = []\n",
    "        for i in range(batch_size):\n",
    "            if i < batch_size // 3:  # High experience\n",
    "                metrics = {\n",
    "                    'aco_repo': np.random.uniform(0.2, 0.3),\n",
    "                    'aco_sys': np.random.uniform(0.25, 0.35),\n",
    "                    'aco_pkg': np.random.uniform(0.3, 0.4),\n",
    "                    'rso_repo': np.random.uniform(0.3, 0.4),\n",
    "                    'rso_sys': np.random.uniform(0.35, 0.45),\n",
    "                    'rso_pkg': np.random.uniform(0.4, 0.5)\n",
    "                }\n",
    "            elif i < 2 * batch_size // 3:  # Medium experience\n",
    "                metrics = {\n",
    "                    'aco_repo': np.random.uniform(0.05, 0.15),\n",
    "                    'aco_sys': np.random.uniform(0.08, 0.18),\n",
    "                    'aco_pkg': np.random.uniform(0.1, 0.2),\n",
    "                    'rso_repo': np.random.uniform(0.1, 0.2),\n",
    "                    'rso_sys': np.random.uniform(0.15, 0.25),\n",
    "                    'rso_pkg': np.random.uniform(0.2, 0.3)\n",
    "                }\n",
    "            else:  # Low experience\n",
    "                metrics = {\n",
    "                    'aco_repo': np.random.uniform(0.01, 0.05),\n",
    "                    'aco_sys': np.random.uniform(0.02, 0.06),\n",
    "                    'aco_pkg': np.random.uniform(0.03, 0.08),\n",
    "                    'rso_repo': np.random.uniform(0.02, 0.08),\n",
    "                    'rso_sys': np.random.uniform(0.03, 0.1),\n",
    "                    'rso_pkg': np.random.uniform(0.05, 0.15)\n",
    "                }\n",
    "            batch_metrics.append(metrics)\n",
    "        \n",
    "        # Train each strategy\n",
    "        for strategy, trainer in trainers.items():\n",
    "            trainer.train_step(batch_data, batch_targets, batch_metrics)\n",
    "    \n",
    "    # Visualize training dynamics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    for idx, (strategy, trainer) in enumerate(trainers.items()):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Extract history\n",
    "        losses = [s['loss'] for s in trainer.loss_history]\n",
    "        avg_weights = [s['avg_weight'] for s in trainer.loss_history]\n",
    "        weight_stds = [s['weight_std'] for s in trainer.loss_history]\n",
    "        \n",
    "        # Plot loss and weight evolution\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        line1 = ax.plot(losses, 'b-', label='Loss', alpha=0.7)\n",
    "        line2 = ax2.plot(avg_weights, 'r-', label='Avg Weight', alpha=0.7)\n",
    "        \n",
    "        # Add weight std as shaded area\n",
    "        ax2.fill_between(range(len(avg_weights)), \n",
    "                        np.array(avg_weights) - np.array(weight_stds),\n",
    "                        np.array(avg_weights) + np.array(weight_stds),\n",
    "                        alpha=0.2, color='red')\n",
    "        \n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Loss', color='b')\n",
    "        ax2.set_ylabel('Weight', color='r')\n",
    "        ax.set_title(f'Strategy: {strategy.upper()}', fontsize=14)\n",
    "        \n",
    "        # Combine legends\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax.legend(lines, labels, loc='upper right')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nTraining Summary (Final 10 steps):\")\n",
    "    for strategy, trainer in trainers.items():\n",
    "        recent_stats = trainer.loss_history[-10:]\n",
    "        avg_loss = np.mean([s['loss'] for s in recent_stats])\n",
    "        avg_weight = np.mean([s['avg_weight'] for s in recent_stats])\n",
    "        weight_range = np.mean([s['max_weight'] - s['min_weight'] for s in recent_stats])\n",
    "        \n",
    "        print(f\"\\n{strategy.upper()} Strategy:\")\n",
    "        print(f\"  Avg Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Avg Weight: {avg_weight:.3f}\")\n",
    "        print(f\"  Weight Range: {weight_range:.3f}\")\n",
    "\n",
    "# Run simulation\n",
    "simulate_elf_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Considerations and Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedELF:\n",
    "    \"\"\"Advanced ELF implementation with optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weight_cache = {}  # Cache computed weights\n",
    "        self.gradient_clipper = nn.utils.clip_grad_norm_\n",
    "        \n",
    "    def adaptive_weight_scaling(self, weights: torch.Tensor, epoch: int, max_epochs: int):\n",
    "        \"\"\"Gradually increase weight influence during training\"\"\"\n",
    "        # Start with less aggressive weighting, gradually increase\n",
    "        scale_factor = min(1.0, epoch / (max_epochs * 0.3))\n",
    "        \n",
    "        # Scale weights towards 1.0 (no weighting) initially\n",
    "        scaled_weights = 1.0 + (weights - 1.0) * scale_factor\n",
    "        return scaled_weights\n",
    "    \n",
    "    def weight_regularization(self, weights: torch.Tensor, lambda_reg: float = 0.01):\n",
    "        \"\"\"Regularize extreme weights to prevent instability\"\"\"\n",
    "        # Penalize very high weights\n",
    "        reg_term = lambda_reg * torch.mean(torch.square(weights - weights.mean()))\n",
    "        return reg_term\n",
    "    \n",
    "    def batch_weight_normalization(self, weights: torch.Tensor):\n",
    "        \"\"\"Normalize weights within batch to maintain stable gradients\"\"\"\n",
    "        # Z-score normalization then rescale\n",
    "        normalized = (weights - weights.mean()) / (weights.std() + 1e-8)\n",
    "        # Rescale to positive range [0.5, 2.0]\n",
    "        rescaled = 1.25 + 0.75 * torch.tanh(normalized)\n",
    "        return rescaled\n",
    "    \n",
    "    def compute_weight_statistics(self, weights: List[float], strategy: str):\n",
    "        \"\"\"Compute and visualize weight statistics\"\"\"\n",
    "        weights_array = np.array(weights)\n",
    "        \n",
    "        stats = {\n",
    "            'mean': np.mean(weights_array),\n",
    "            'std': np.std(weights_array),\n",
    "            'min': np.min(weights_array),\n",
    "            'max': np.max(weights_array),\n",
    "            'p25': np.percentile(weights_array, 25),\n",
    "            'p50': np.percentile(weights_array, 50),\n",
    "            'p75': np.percentile(weights_array, 75),\n",
    "            'skewness': self._skewness(weights_array),\n",
    "            'kurtosis': self._kurtosis(weights_array)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _skewness(self, x):\n",
    "        \"\"\"Calculate skewness of distribution\"\"\"\n",
    "        mean = np.mean(x)\n",
    "        std = np.std(x)\n",
    "        return np.mean(((x - mean) / std) ** 3)\n",
    "    \n",
    "    def _kurtosis(self, x):\n",
    "        \"\"\"Calculate kurtosis of distribution\"\"\"\n",
    "        mean = np.mean(x)\n",
    "        std = np.std(x)\n",
    "        return np.mean(((x - mean) / std) ** 4) - 3\n",
    "\n",
    "# Demonstrate advanced techniques\n",
    "advanced_elf = AdvancedELF()\n",
    "\n",
    "# Generate sample weights for different experience distributions\n",
    "n_samples = 1000\n",
    "distributions = {\n",
    "    \"Uniform\": np.random.uniform(0.0, 0.5, n_samples),\n",
    "    \"Bimodal\": np.concatenate([np.random.normal(0.05, 0.02, n_samples//2),\n",
    "                               np.random.normal(0.35, 0.05, n_samples//2)]),\n",
    "    \"Skewed\": np.random.gamma(2, 0.05, n_samples),\n",
    "    \"Real-world\": np.concatenate([np.random.gamma(2, 0.02, int(n_samples*0.7)),\n",
    "                                 np.random.normal(0.25, 0.05, int(n_samples*0.2)),\n",
    "                                 np.random.uniform(0.35, 0.45, int(n_samples*0.1))])\n",
    "}\n",
    "\n",
    "# Visualize weight distributions and their effects\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for idx, (dist_name, ownership_values) in enumerate(distributions.items()):\n",
    "    # Calculate weights for ACO strategy\n",
    "    weights = [np.exp(1 + x) for x in ownership_values]\n",
    "    \n",
    "    # Plot ownership distribution\n",
    "    ax1 = axes[0, idx]\n",
    "    ax1.hist(ownership_values, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax1.set_title(f'{dist_name} Ownership Distribution', fontsize=12)\n",
    "    ax1.set_xlabel('Ownership Value')\n",
    "    ax1.set_ylabel('Count')\n",
    "    \n",
    "    # Plot weight distribution\n",
    "    ax2 = axes[1, idx]\n",
    "    ax2.hist(weights, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "    ax2.set_title(f'Resulting Weight Distribution', fontsize=12)\n",
    "    ax2.set_xlabel('Weight (ω)')\n",
    "    ax2.set_ylabel('Count')\n",
    "    \n",
    "    # Add statistics\n",
    "    stats = advanced_elf.compute_weight_statistics(weights, \"aco\")\n",
    "    stats_text = f\"μ={stats['mean']:.2f}\\nσ={stats['std']:.2f}\\nskew={stats['skewness']:.2f}\"\n",
    "    ax2.text(0.7, 0.9, stats_text, transform=ax2.transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "             verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate weight normalization techniques\n",
    "print(\"\\nWeight Normalization Examples:\")\n",
    "test_weights = torch.tensor([2.72, 2.72, 4.48, 7.39, 7.39, 12.18])  # Mix of experience levels\n",
    "print(f\"Original weights: {test_weights.numpy()}\")\n",
    "print(f\"Batch normalized: {advanced_elf.batch_weight_normalization(test_weights).numpy()}\")\n",
    "\n",
    "# Show adaptive scaling over epochs\n",
    "print(\"\\nAdaptive Weight Scaling:\")\n",
    "epochs = [0, 10, 20, 30, 50, 100]\n",
    "for epoch in epochs:\n",
    "    scaled = advanced_elf.adaptive_weight_scaling(test_weights, epoch, 100)\n",
    "    print(f\"Epoch {epoch:3d}: {scaled.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Implementation Challenges and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELFChallenges:\n",
    "    \"\"\"Common challenges and solutions when implementing ELF\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_missing_metrics():\n",
    "        \"\"\"Handle cases where reviewer metrics are missing\"\"\"\n",
    "        print(\"Challenge 1: Missing Reviewer Metrics\")\n",
    "        print(\"Solutions:\")\n",
    "        print(\"1. Use default weight of 1.0 (no weighting)\")\n",
    "        print(\"2. Impute based on repository average\")\n",
    "        print(\"3. Use minimum observed weight\")\n",
    "        print(\"4. Skip sample in training\\n\")\n",
    "        \n",
    "        # Example implementation\n",
    "        def get_weight_safe(metrics, default_aco=0.05, default_rso=0.10):\n",
    "            if metrics is None:\n",
    "                # Use conservative defaults\n",
    "                return np.exp(1 + (default_aco + default_rso) / 2)\n",
    "            return calculate_weight(metrics)\n",
    "        \n",
    "        return get_weight_safe\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_extreme_weights():\n",
    "        \"\"\"Handle extremely high or low weights\"\"\"\n",
    "        print(\"Challenge 2: Extreme Weight Values\")\n",
    "        print(\"Solutions:\")\n",
    "        print(\"1. Clip weights to reasonable range [0.5, 10.0]\")\n",
    "        print(\"2. Use log-scale transformation\")\n",
    "        print(\"3. Apply weight decay regularization\\n\")\n",
    "        \n",
    "        def clip_weights(weights, min_w=0.5, max_w=10.0):\n",
    "            return torch.clamp(weights, min=min_w, max=max_w)\n",
    "        \n",
    "        return clip_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_imbalanced_experience():\n",
    "        \"\"\"Handle datasets with imbalanced experience distributions\"\"\"\n",
    "        print(\"Challenge 3: Imbalanced Experience Distribution\")\n",
    "        print(\"Solutions:\")\n",
    "        print(\"1. Stratified sampling by experience level\")\n",
    "        print(\"2. Experience-aware batch construction\")\n",
    "        print(\"3. Focal loss adaptation\\n\")\n",
    "        \n",
    "        class StratifiedBatchSampler:\n",
    "            def __init__(self, dataset, batch_size):\n",
    "                self.dataset = dataset\n",
    "                self.batch_size = batch_size\n",
    "                self._stratify_by_experience()\n",
    "                \n",
    "            def _stratify_by_experience(self):\n",
    "                # Group samples by experience level\n",
    "                self.high_exp = []\n",
    "                self.mid_exp = []\n",
    "                self.low_exp = []\n",
    "                \n",
    "                # Stratified sampling logic here\n",
    "                pass\n",
    "        \n",
    "        return StratifiedBatchSampler\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_challenges():\n",
    "        \"\"\"Visualize common challenges\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Challenge 1: Missing metrics distribution\n",
    "        ax1 = axes[0]\n",
    "        missing_rates = [0.05, 0.15, 0.10, 0.08, 0.12]  # Per repository\n",
    "        ax1.bar(range(len(missing_rates)), missing_rates, color='coral')\n",
    "        ax1.set_title(\"Missing Metrics by Repository\")\n",
    "        ax1.set_xlabel(\"Repository ID\")\n",
    "        ax1.set_ylabel(\"Missing Rate\")\n",
    "        ax1.axhline(y=0.1, color='red', linestyle='--', label='10% threshold')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Challenge 2: Weight distribution with outliers\n",
    "        ax2 = axes[1]\n",
    "        normal_weights = np.random.lognormal(1.5, 0.5, 950)\n",
    "        outlier_weights = np.random.uniform(15, 25, 50)\n",
    "        all_weights = np.concatenate([normal_weights, outlier_weights])\n",
    "        ax2.hist(all_weights, bins=50, color='skyblue', edgecolor='black')\n",
    "        ax2.axvline(x=10, color='red', linestyle='--', label='Clipping threshold')\n",
    "        ax2.set_title(\"Weight Distribution with Outliers\")\n",
    "        ax2.set_xlabel(\"Weight Value\")\n",
    "        ax2.set_ylabel(\"Count\")\n",
    "        ax2.set_xlim(0, 30)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Challenge 3: Experience imbalance\n",
    "        ax3 = axes[2]\n",
    "        exp_distribution = [700, 200, 100]  # Low, Mid, High\n",
    "        ax3.pie(exp_distribution, labels=['Low Exp', 'Mid Exp', 'High Exp'],\n",
    "                autopct='%1.1f%%', colors=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "        ax3.set_title(\"Experience Distribution Imbalance\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate challenges and solutions\n",
    "challenges = ELFChallenges()\n",
    "\n",
    "# Show each challenge\n",
    "print(\"=== ELF Implementation Challenges ===\")\n",
    "print()\n",
    "challenges.handle_missing_metrics()\n",
    "challenges.handle_extreme_weights()\n",
    "challenges.handle_imbalanced_experience()\n",
    "\n",
    "# Visualize challenges\n",
    "challenges.visualize_challenges()\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\n=== ELF Implementation Best Practices ===\")\n",
    "print(\"1. Always validate ownership metrics before training\")\n",
    "print(\"2. Monitor weight distribution during training\")\n",
    "print(\"3. Use gradient clipping with high weights\")\n",
    "print(\"4. Consider curriculum learning: start with uniform weights\")\n",
    "print(\"5. Log weight statistics for debugging\")\n",
    "print(\"6. Implement checkpointing for model recovery\")\n",
    "print(\"7. Use mixed precision training for efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Takeaways\n",
    "\n",
    "### Core Concepts Mastered\n",
    "1. **ELF Formula**: L_RCG = ω * Σ(-log P(w_t|c,w_<t))\n",
    "2. **Four Strategies**: ACO, RSO, AVG, MAX - each capturing different aspects of experience\n",
    "3. **Exponential Weighting**: e^(1+ownership) creates strong differentiation\n",
    "4. **Gradient Scaling**: Weights directly scale gradient magnitudes\n",
    "\n",
    "### Implementation Insights\n",
    "1. **Cache weights** for efficiency in large-scale training\n",
    "2. **Normalize weights** within batches to prevent instability\n",
    "3. **Clip extreme values** to maintain training stability\n",
    "4. **Monitor weight distribution** throughout training\n",
    "\n",
    "### Research Extensions\n",
    "1. **Adaptive strategies**: Dynamically select strategy based on data\n",
    "2. **Multi-granularity fusion**: Combine multiple granularity levels\n",
    "3. **Temporal dynamics**: Consider how ownership changes over time\n",
    "4. **Cross-project transfer**: How do weights transfer between projects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final implementation template for researchers\n",
    "class ELFResearchTemplate:\n",
    "    \"\"\"Template for implementing ELF in your own research\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.strategy = config.get('strategy', 'aco')\n",
    "        self.granularity = config.get('granularity', 'package')\n",
    "        self.clip_range = config.get('clip_range', (0.5, 10.0))\n",
    "        self.normalize_batch = config.get('normalize_batch', False)\n",
    "        self.adaptive_scaling = config.get('adaptive_scaling', True)\n",
    "        \n",
    "    def compute_elf_loss(self, model_output, targets, reviewer_metrics, epoch=0, max_epochs=100):\n",
    "        \"\"\"\n",
    "        Complete ELF loss computation with all optimizations\n",
    "        \n",
    "        Args:\n",
    "            model_output: Model predictions [batch_size, seq_len, vocab_size]\n",
    "            targets: Ground truth [batch_size, seq_len]\n",
    "            reviewer_metrics: List of ReviewerMetrics objects\n",
    "            epoch: Current training epoch\n",
    "            max_epochs: Total training epochs\n",
    "            \n",
    "        Returns:\n",
    "            loss: Weighted loss scalar\n",
    "            metrics: Dictionary of training metrics\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def analyze_results(self, generated_comments, ground_truth, reviewer_metrics):\n",
    "        \"\"\"Analyze ELF model outputs\"\"\"\n",
    "        # Your analysis here\n",
    "        pass\n",
    "\n",
    "print(\"ELF Deep Dive Complete!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Implement ELF with your own code review dataset\")\n",
    "print(\"2. Experiment with different strategies and granularities\")\n",
    "print(\"3. Analyze how weights affect generated comment quality\")\n",
    "print(\"4. Consider hybrid approaches combining multiple strategies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}