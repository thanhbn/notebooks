{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs\n",
    "\n",
    "## Paper Information\n",
    "- **Title**: LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs\n",
    "- **Authors**: Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, Xiaolong Xu\n",
    "- **Link**: https://arxiv.org/abs/2504.14655\n",
    "- **Date**: April 20, 2025\n",
    "\n",
    "## Paper Summary\n",
    "This paper introduces LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models. The dataset addresses two key challenges:\n",
    "1. **Lack of reasoning-focused coding benchmarks**: Provides comprehensive evaluation with rich metadata\n",
    "2. **Self-contained training testbeds**: Enables contamination-free evaluation and efficient supervised fine-tuning (SFT)\n",
    "\n",
    "Key contributions:\n",
    "- Curated 2,869 LeetCode Python problems with 100+ test cases per problem\n",
    "- Temporal splits (pre/post July 2024) for contamination-free evaluation\n",
    "- Demonstrated that reasoning models significantly outperform non-reasoning counterparts\n",
    "- Achieved comparable performance with only 2.6K model-generated solutions vs. 110K-sample datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langchain-anthropic langchain-community\n",
    "!pip install -q langgraph langsmith deepeval\n",
    "!pip install -q datasets transformers torch\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "!pip install -q python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Set up environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "Since the actual LeetCodeDataset is available on Hugging Face, we'll demonstrate how to load and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Note: The actual dataset is at: https://huggingface.co/datasets/newfacade/LeetCodeDataset\n",
    "# For demonstration, we'll create a mock dataset structure\n",
    "\n",
    "class LeetCodeProblem(BaseModel):\n",
    "    \"\"\"Schema for a LeetCode problem\"\"\"\n",
    "    slug: str = Field(description=\"URL identifier and primary key\")\n",
    "    question_id: int = Field(description=\"Unique sequential number\")\n",
    "    difficulty: str = Field(description=\"Easy/Medium/Hard\")\n",
    "    problem_description: str = Field(description=\"Full text with examples and constraints\")\n",
    "    starter_code: str = Field(description=\"Language template code\")\n",
    "    topic_tags: List[str] = Field(description=\"Problem tags like Array, Dynamic Programming\")\n",
    "    release_date: str = Field(description=\"Problem release date\")\n",
    "    test_cases: List[Dict[str, any]] = Field(description=\"Input-output test cases\")\n",
    "\n",
    "# Create mock dataset for demonstration\n",
    "def create_mock_leetcode_dataset():\n",
    "    \"\"\"Create a mock dataset for demonstration purposes\"\"\"\n",
    "    problems = []\n",
    "    \n",
    "    # Example problem from the paper\n",
    "    problem1 = LeetCodeProblem(\n",
    "        slug=\"missing-number-in-arithmetic-progression\",\n",
    "        question_id=1228,\n",
    "        difficulty=\"Easy\",\n",
    "        problem_description=\"\"\"In some array arr, the values were in arithmetic progression: \n",
    "the values arr[i + 1] - arr[i] are all equal for every 0 <= i < arr.length - 1.\n",
    "A value from arr was removed that was not the first or last value in the array.\n",
    "Given arr, return the removed value.\n",
    "\n",
    "Example 1:\n",
    "Input: arr = [5,7,11,13]\n",
    "Output: 9\n",
    "Explanation: The previous array was [5,7,9,11,13].\n",
    "\n",
    "Example 2:\n",
    "Input: arr = [15,13,12]\n",
    "Output: 14\n",
    "Explanation: The previous array was [15,14,13,12].\n",
    "\n",
    "Constraints:\n",
    "3 <= arr.length <= 1000\n",
    "0 <= arr[i] <= 10^5\n",
    "The given array is guaranteed to be a valid array.\"\"\",\n",
    "        starter_code=\"\"\"class Solution:\n",
    "    def missingNumber(self, arr: List[int]) -> int:\n",
    "        \"\"\",\n",
    "        topic_tags=[\"Array\", \"Math\"],\n",
    "        release_date=\"2019-10-13\",\n",
    "        test_cases=[\n",
    "            {\"input\": {\"arr\": [5, 7, 11, 13]}, \"output\": 9},\n",
    "            {\"input\": {\"arr\": [15, 13, 12]}, \"output\": 14},\n",
    "            {\"input\": {\"arr\": [1, 2, 3, 5]}, \"output\": 4},\n",
    "        ]\n",
    "    )\n",
    "    problems.append(problem1)\n",
    "    \n",
    "    # Add more mock problems\n",
    "    difficulties = [\"Easy\", \"Medium\", \"Hard\"]\n",
    "    tags = [\"Array\", \"String\", \"Dynamic Programming\", \"Binary Search\", \"Tree\", \"Graph\"]\n",
    "    \n",
    "    for i in range(2, 11):  # Create 10 mock problems\n",
    "        problems.append(LeetCodeProblem(\n",
    "            slug=f\"problem-{i}\",\n",
    "            question_id=1000 + i,\n",
    "            difficulty=np.random.choice(difficulties),\n",
    "            problem_description=f\"Mock problem {i} description\",\n",
    "            starter_code=f\"class Solution:\\n    def solve{i}(self, nums: List[int]) -> int:\\n        \",\n",
    "            topic_tags=list(np.random.choice(tags, size=np.random.randint(1, 4), replace=False)),\n",
    "            release_date=f\"2024-{np.random.randint(1, 12):02d}-{np.random.randint(1, 28):02d}\",\n",
    "            test_cases=[{\"input\": {\"nums\": [1, 2, 3]}, \"output\": i}]\n",
    "        ))\n",
    "    \n",
    "    return problems\n",
    "\n",
    "# Create mock dataset\n",
    "mock_problems = create_mock_leetcode_dataset()\n",
    "print(f\"Created {len(mock_problems)} mock problems for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm Implementation: LeetCodeDataset Construction Pipeline\n",
    "\n",
    "We'll implement the key components of the dataset construction pipeline using LangChain and LangGraph for better structure and reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why use LangChain/LangGraph for this implementation?\n",
    "# 1. LangChain provides structured prompting and output parsing - essential for generating test cases\n",
    "# 2. LangGraph enables workflow orchestration for the multi-stage pipeline\n",
    "# 3. Built-in support for multiple LLMs (GPT-4, Claude, etc.) for response generation\n",
    "# 4. Robust error handling and retry mechanisms\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "class DatasetConstructionState(TypedDict):\n",
    "    \"\"\"State for the dataset construction pipeline\"\"\"\n",
    "    problem: Dict\n",
    "    metadata: Dict\n",
    "    canonical_solution: Optional[str]\n",
    "    entry_point: Optional[str]\n",
    "    test_inputs: List[Dict]\n",
    "    test_cases: List[Dict]\n",
    "    model_generated_solution: Optional[str]\n",
    "    \n",
    "# Initialize LLM for test case generation\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# Step 1: Metadata Acquisition (simulated)\n",
    "def acquire_metadata(state: DatasetConstructionState) -> DatasetConstructionState:\n",
    "    \"\"\"Acquire problem metadata from LeetCode API\"\"\"\n",
    "    # In real implementation, this would call LeetCode GraphQL API\n",
    "    # Reference: https://github.com/fspv/python-leetcode\n",
    "    state[\"metadata\"] = {\n",
    "        \"slug\": state[\"problem\"][\"slug\"],\n",
    "        \"question_id\": state[\"problem\"][\"question_id\"],\n",
    "        \"difficulty\": state[\"problem\"][\"difficulty\"],\n",
    "        \"topic_tags\": state[\"problem\"][\"topic_tags\"],\n",
    "        \"release_date\": state[\"problem\"][\"release_date\"]\n",
    "    }\n",
    "    return state\n",
    "\n",
    "# Step 2: Entry Point Identification\n",
    "def identify_entry_point(state: DatasetConstructionState) -> DatasetConstructionState:\n",
    "    \"\"\"Identify the function entry point from starter code\"\"\"\n",
    "    import re\n",
    "    starter_code = state[\"problem\"][\"starter_code\"]\n",
    "    \n",
    "    # Extract function name using regex\n",
    "    match = re.search(r'def\\s+(\\w+)\\s*\\(', starter_code)\n",
    "    if match:\n",
    "        state[\"entry_point\"] = match.group(1)\n",
    "    else:\n",
    "        state[\"entry_point\"] = None\n",
    "    return state\n",
    "\n",
    "# Step 3: Input Generation using LangChain\n",
    "class TestInput(BaseModel):\n",
    "    \"\"\"Schema for test input\"\"\"\n",
    "    inputs: List[Dict] = Field(description=\"List of input dictionaries\")\n",
    "\n",
    "def generate_test_inputs(state: DatasetConstructionState) -> DatasetConstructionState:\n",
    "    \"\"\"Generate test inputs using LLM with one-shot prompting\"\"\"\n",
    "    \n",
    "    # Create parser\n",
    "    parser = PydanticOutputParser(pydantic_object=TestInput)\n",
    "    \n",
    "    # Create prompt template (from Figure 4 in paper)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are an expert Python programmer. You will be given a question (including a problem specification and starter code). Your task is to generate inputs that are consistent with the problem specification and starter code.\"),\n",
    "        HumanMessage(content=\"\"\"**** Example ****\n",
    "#### Question:\n",
    "Given an array of integers, return the sum.\n",
    "class Solution:\n",
    "    def arraySum(self, nums: List[int]) -> int:\n",
    "\n",
    "#### Some valid inputs of the starter code (json format):\n",
    "```json\n",
    "[{\"nums\": [1, 2, 3]}, {\"nums\": [4, 5, 6]}, {\"nums\": [-1, 0, 1]}]\n",
    "```\n",
    "\n",
    "**** Now Your Task ****\n",
    "#### Question:\n",
    "{problem_description}\n",
    "{starter_code}\n",
    "\n",
    "#### Some valid inputs of the starter code (json format):\n",
    "{format_instructions}\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Generate inputs\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.invoke({\n",
    "        \"problem_description\": state[\"problem\"][\"problem_description\"],\n",
    "        \"starter_code\": state[\"problem\"][\"starter_code\"],\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    })\n",
    "    \n",
    "    # For demonstration, use mock inputs\n",
    "    state[\"test_inputs\"] = [\n",
    "        {\"arr\": [5, 7, 11, 13]},\n",
    "        {\"arr\": [15, 13, 12]},\n",
    "        {\"arr\": [1, 2, 3, 5]},\n",
    "        {\"arr\": [100, 200, 400]},  # Complex case\n",
    "        {\"arr\": [10, 20, 30, 40, 60]},  # Complex case\n",
    "    ]\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 4: Test Case Generation with Sandboxed Execution\n",
    "def generate_test_cases(state: DatasetConstructionState) -> DatasetConstructionState:\n",
    "    \"\"\"Generate test cases by executing canonical solution in sandbox\"\"\"\n",
    "    \n",
    "    # In real implementation, this would:\n",
    "    # 1. Set up sandboxed execution environment\n",
    "    # 2. Execute canonical solution with generated inputs\n",
    "    # 3. Capture outputs\n",
    "    \n",
    "    # For demonstration, create mock test cases\n",
    "    test_cases = []\n",
    "    for input_data in state[\"test_inputs\"]:\n",
    "        # Simulate execution of canonical solution\n",
    "        if \"arr\" in input_data:\n",
    "            arr = input_data[\"arr\"]\n",
    "            # Mock solution for arithmetic progression\n",
    "            if len(arr) >= 2:\n",
    "                diff = (arr[-1] - arr[0]) // len(arr)\n",
    "                expected_sum = len(arr) * (arr[0] + arr[-1]) // 2 + diff\n",
    "                actual_sum = sum(arr)\n",
    "                output = expected_sum - actual_sum\n",
    "            else:\n",
    "                output = 0\n",
    "        else:\n",
    "            output = 0\n",
    "            \n",
    "        test_cases.append({\n",
    "            \"input\": input_data,\n",
    "            \"output\": output\n",
    "        })\n",
    "    \n",
    "    state[\"test_cases\"] = test_cases\n",
    "    return state\n",
    "\n",
    "# Step 5: Model-Generated Solution (for SFT)\n",
    "def generate_model_solution(state: DatasetConstructionState) -> DatasetConstructionState:\n",
    "    \"\"\"Generate high-quality solution using Qwen2.5-Coder-32B-Instruct\"\"\"\n",
    "    \n",
    "    # In real implementation, this would use Qwen2.5-Coder-32B-Instruct\n",
    "    # with temperature=1.0 for diverse solutions\n",
    "    \n",
    "    solution_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are an expert competitive programmer. Provide a clean, efficient solution with clear reasoning.\"),\n",
    "        HumanMessage(content=\"\"\"Solve this problem:\n",
    "{problem_description}\n",
    "\n",
    "Starter code:\n",
    "{starter_code}\n",
    "\n",
    "Provide a complete solution with step-by-step reasoning.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # For demonstration, provide a mock solution\n",
    "    state[\"model_generated_solution\"] = \"\"\"class Solution:\n",
    "    def missingNumber(self, arr: List[int]) -> int:\n",
    "        # Step 1: Calculate the common difference\n",
    "        # In an arithmetic progression, the difference should be constant\n",
    "        n = len(arr)\n",
    "        total_diff = arr[-1] - arr[0]\n",
    "        common_diff = total_diff // n\n",
    "        \n",
    "        # Step 2: Find the missing number\n",
    "        # Check each consecutive pair for the anomaly\n",
    "        for i in range(n - 1):\n",
    "            expected_next = arr[i] + common_diff\n",
    "            if arr[i + 1] != expected_next:\n",
    "                return expected_next\n",
    "        \n",
    "        # Edge case: missing number is at the beginning or end\n",
    "        return arr[0] + common_diff\n",
    "\"\"\"\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LangGraph workflow\n",
    "workflow = StateGraph(DatasetConstructionState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"acquire_metadata\", acquire_metadata)\n",
    "workflow.add_node(\"identify_entry_point\", identify_entry_point)\n",
    "workflow.add_node(\"generate_inputs\", generate_test_inputs)\n",
    "workflow.add_node(\"generate_test_cases\", generate_test_cases)\n",
    "workflow.add_node(\"generate_solution\", generate_model_solution)\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"acquire_metadata\", \"identify_entry_point\")\n",
    "workflow.add_edge(\"identify_entry_point\", \"generate_inputs\")\n",
    "workflow.add_edge(\"generate_inputs\", \"generate_test_cases\")\n",
    "workflow.add_edge(\"generate_test_cases\", \"generate_solution\")\n",
    "workflow.add_edge(\"generate_solution\", END)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"acquire_metadata\")\n",
    "\n",
    "# Compile the graph\n",
    "dataset_pipeline = workflow.compile()\n",
    "\n",
    "# Test the pipeline with a mock problem\n",
    "test_problem = mock_problems[0]\n",
    "initial_state = DatasetConstructionState(\n",
    "    problem=test_problem.dict(),\n",
    "    metadata={},\n",
    "    canonical_solution=None,\n",
    "    entry_point=None,\n",
    "    test_inputs=[],\n",
    "    test_cases=[],\n",
    "    model_generated_solution=None\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "result = dataset_pipeline.invoke(initial_state)\n",
    "print(f\"Pipeline completed. Generated {len(result['test_cases'])} test cases.\")\n",
    "print(f\"Entry point identified: {result['entry_point']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "We'll implement the evaluation framework and demonstrate how to use deepeval for metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why use deepeval for this evaluation?\n",
    "# 1. Provides comprehensive code evaluation metrics\n",
    "# 2. Supports custom metrics for pass@k evaluation\n",
    "# 3. Built-in test case execution and validation\n",
    "# 4. Integration with LangChain for end-to-end testing\n",
    "\n",
    "from deepeval import assert_test\n",
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import ast\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "class CodeExecutionMetric(BaseMetric):\n",
    "    \"\"\"Custom metric for code execution evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, test_cases: List[Dict], timeout: int = 5):\n",
    "        self.test_cases = test_cases\n",
    "        self.timeout = timeout\n",
    "        self.threshold = 1.0  # All test cases must pass\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"Code Execution Pass Rate\"\n",
    "    \n",
    "    def execute_code(self, code: str, test_case: Dict) -> Tuple[bool, str]:\n",
    "        \"\"\"Execute code with test case in sandboxed environment\"\"\"\n",
    "        try:\n",
    "            # Create temporary file\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "                # Write the solution and test code\n",
    "                test_code = f\"\"\"\n",
    "from typing import List\n",
    "\n",
    "{code}\n",
    "\n",
    "# Test execution\n",
    "solution = Solution()\n",
    "result = solution.{test_case['function_name']}(**{test_case['input']})\n",
    "expected = {test_case['output']}\n",
    "assert result == expected, f\"Expected {{expected}}, got {{result}}\"\n",
    "print(\"Test passed!\")\n",
    "\"\"\"\n",
    "                f.write(test_code)\n",
    "                f.flush()\n",
    "                \n",
    "            # Execute the code\n",
    "            result = subprocess.run(\n",
    "                ['python', f.name],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            \n",
    "            # Clean up\n",
    "            os.unlink(f.name)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                return True, \"Test passed\"\n",
    "            else:\n",
    "                return False, result.stderr\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return False, \"Execution timeout\"\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        \"\"\"Measure the pass rate of generated code\"\"\"\n",
    "        code = test_case.actual_output\n",
    "        \n",
    "        passed = 0\n",
    "        total = len(self.test_cases)\n",
    "        \n",
    "        for tc in self.test_cases:\n",
    "            success, _ = self.execute_code(code, tc)\n",
    "            if success:\n",
    "                passed += 1\n",
    "        \n",
    "        self.score = passed / total\n",
    "        self.success = self.score >= self.threshold\n",
    "        return self.score\n",
    "    \n",
    "    async def a_measure(self, test_case: LLMTestCase) -> float:\n",
    "        return self.measure(test_case)\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement evaluation pipeline\n",
    "class LeetCodeEvaluator:\n",
    "    \"\"\"Evaluator for LeetCode problems\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-4o\"):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0.2)\n",
    "        self.results = []\n",
    "        \n",
    "    def generate_solution(self, problem: Dict) -> str:\n",
    "        \"\"\"Generate solution for a problem\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"You are an expert competitive programmer. Provide only the code solution without explanation.\"),\n",
    "            HumanMessage(content=\"\"\"Problem:\n",
    "{problem_description}\n",
    "\n",
    "Starter code:\n",
    "{starter_code}\n",
    "\n",
    "Complete the solution:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        response = chain.invoke({\n",
    "            \"problem_description\": problem[\"problem_description\"],\n",
    "            \"starter_code\": problem[\"starter_code\"]\n",
    "        })\n",
    "        \n",
    "        return response[\"text\"]\n",
    "    \n",
    "    def evaluate_problem(self, problem: Dict) -> Dict:\n",
    "        \"\"\"Evaluate a single problem\"\"\"\n",
    "        # Generate solution\n",
    "        solution = self.generate_solution(problem)\n",
    "        \n",
    "        # Create test case for deepeval\n",
    "        test_case = LLMTestCase(\n",
    "            input=problem[\"problem_description\"],\n",
    "            actual_output=solution,\n",
    "            expected_output=None  # We use test cases for validation\n",
    "        )\n",
    "        \n",
    "        # Prepare test cases with function name\n",
    "        enhanced_test_cases = []\n",
    "        for tc in problem[\"test_cases\"]:\n",
    "            enhanced_tc = tc.copy()\n",
    "            enhanced_tc[\"function_name\"] = \"missingNumber\"  # Extract from problem\n",
    "            enhanced_test_cases.append(enhanced_tc)\n",
    "        \n",
    "        # Create metric\n",
    "        metric = CodeExecutionMetric(test_cases=enhanced_test_cases)\n",
    "        \n",
    "        # Measure performance\n",
    "        score = metric.measure(test_case)\n",
    "        \n",
    "        return {\n",
    "            \"problem_id\": problem[\"question_id\"],\n",
    "            \"difficulty\": problem[\"difficulty\"],\n",
    "            \"topic_tags\": problem[\"topic_tags\"],\n",
    "            \"pass_rate\": score,\n",
    "            \"passed\": metric.is_successful()\n",
    "        }\n",
    "    \n",
    "    def evaluate_dataset(self, problems: List[Dict], limit: int = None) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate multiple problems\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, problem in enumerate(tqdm(problems[:limit], desc=\"Evaluating problems\")):\n",
    "            result = self.evaluate_problem(problem)\n",
    "            results.append(result)\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Demonstrate evaluation\n",
    "evaluator = LeetCodeEvaluator(model_name=\"gpt-4o\")\n",
    "\n",
    "# Convert mock problems to dict format\n",
    "problems_dict = [p.dict() for p in mock_problems[:3]]\n",
    "\n",
    "# Note: In real implementation, this would evaluate actual problems\n",
    "# results_df = evaluator.evaluate_dataset(problems_dict)\n",
    "print(\"Evaluation framework implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization\n",
    "\n",
    "We'll create visualizations similar to those in the paper to analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock results data based on paper's findings\n",
    "models_data = {\n",
    "    \"Model\": [\"GPT-4o-0806\", \"Claude-3.7-Sonnet\", \"DeepSeek-V3\", \"DeepSeek-R1\", \"Qwen2.5-Max\", \"QwQ-Plus\"],\n",
    "    \"Easy\": [81.48, 87.04, 77.78, 94.44, 74.07, 92.59],\n",
    "    \"Medium\": [32.76, 54.31, 31.90, 68.97, 25.00, 62.93],\n",
    "    \"Hard\": [10.47, 23.26, 13.95, 41.86, 10.47, 24.42],\n",
    "    \"Overall\": [35.55, 50.78, 35.55, 65.23, 30.47, 56.25],\n",
    "    \"Type\": [\"Non-Reasoning\", \"Non-Reasoning\", \"Non-Reasoning\", \"Reasoning\", \"Non-Reasoning\", \"Reasoning\"]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(models_data)\n",
    "\n",
    "# Visualization 1: Pass rates by difficulty level\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot for overall performance\n",
    "colors = ['#FF6B6B' if t == \"Non-Reasoning\" else '#4ECDC4' for t in results_df['Type']]\n",
    "bars = ax1.bar(results_df['Model'], results_df['Overall'], color=colors)\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Pass@1 Rate (%)')\n",
    "ax1.set_title('Overall Model Performance on LeetCodeDataset')\n",
    "ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, results_df['Overall']):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{value:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#4ECDC4', label='Reasoning Model'),\n",
    "                   Patch(facecolor='#FF6B6B', label='Non-Reasoning Model')]\n",
    "ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Grouped bar plot for difficulty levels\n",
    "difficulty_data = results_df[['Model', 'Easy', 'Medium', 'Hard']].set_index('Model')\n",
    "difficulty_data.plot(kind='bar', ax=ax2, width=0.8)\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_ylabel('Pass@1 Rate (%)')\n",
    "ax2.set_title('Model Performance by Difficulty Level')\n",
    "ax2.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax2.legend(title='Difficulty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Topic-wise performance heatmap\n",
    "# Create mock topic performance data\n",
    "topics = ['Array', 'String', 'Dynamic Programming', 'Binary Search', 'Tree', 'Graph']\n",
    "models = ['GPT-4o', 'DeepSeek-R1', 'Claude-3.7']\n",
    "\n",
    "# Mock data based on paper's findings\n",
    "topic_performance = np.array([\n",
    "    [32.1, 67.9, 51.2],  # Array\n",
    "    [37.3, 68.7, 49.3],  # String\n",
    "    [10.5, 70.2, 31.6],  # Dynamic Programming\n",
    "    [7.7, 73.1, 30.8],   # Binary Search\n",
    "    [27.3, 72.7, 9.1],   # Tree\n",
    "    [40.0, 66.7, 53.3]   # Graph\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(topic_performance, \n",
    "            xticklabels=models, \n",
    "            yticklabels=topics,\n",
    "            annot=True, \n",
    "            fmt='.1f',\n",
    "            cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Pass Rate (%)'})\n",
    "plt.title('Model Performance Across Different Topic Tags')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Topic Tag')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Temporal analysis\n",
    "# Create mock monthly performance data\n",
    "months = ['2024-07', '2024-08', '2024-09', '2024-10', '2024-11', '2024-12', '2025-01', '2025-02']\n",
    "model_monthly_performance = {\n",
    "    'GPT-4o-0806': [38.5, 42.1, 35.2, 33.8, 31.5, 36.9, 35.1, 34.3],\n",
    "    'DeepSeek-R1': [68.2, 70.5, 63.8, 65.1, 62.9, 67.4, 64.2, 63.7],\n",
    "    'Claude-3.7-Sonnet': [52.3, 55.1, 48.9, 50.2, 49.5, 52.8, 50.1, 49.6]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model, performance in model_monthly_performance.items():\n",
    "    plt.plot(months, performance, marker='o', label=model, linewidth=2)\n",
    "\n",
    "plt.xlabel('LeetCode Problem Release Month')\n",
    "plt.ylabel('Pass@1 Rate (%)')\n",
    "plt.title('Monthly Pass Rates of Various Models on LeetCodeDataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Efficient SFT Training Analysis\n",
    "\n",
    "Analyze the efficiency of training with model-generated data vs. human-written solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training efficiency comparison\n",
    "training_data = {\n",
    "    'Dataset': ['Magicoder Evol-Instruct-110K', 'Magicoder OSS-Instruct-75K', \n",
    "                'Open-R1 CodeForces-CoT', 'OpenThoughts 114k',\n",
    "                'LeetCodeDataset (human)', 'LeetCodeDataset (model)'],\n",
    "    'Rows': [111100, 75100, 9500, 19900, 2600, 2600],\n",
    "    'HumanEval': [77.4, 73.8, 79.9, 77.4, 55.5, 79.9],\n",
    "    'MBPP': [74.1, 76.5, 74.1, 75.7, 53.4, 77.5],\n",
    "    'Type': ['Large', 'Large', 'Medium', 'Medium', 'Small', 'Small']\n",
    "}\n",
    "\n",
    "sft_df = pd.DataFrame(training_data)\n",
    "\n",
    "# Create efficiency plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create bubble chart\n",
    "colors = {'Large': '#FF6B6B', 'Medium': '#4ECDC4', 'Small': '#95E1D3'}\n",
    "for _, row in sft_df.iterrows():\n",
    "    ax.scatter(row['Rows'], row['HumanEval'], \n",
    "               s=row['MBPP']*10,  # Size based on MBPP score\n",
    "               alpha=0.6,\n",
    "               color=colors[row['Type']],\n",
    "               edgecolors='black',\n",
    "               linewidth=2)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.annotate(row['Dataset'].split()[0], \n",
    "                (row['Rows'], row['HumanEval']),\n",
    "                xytext=(5, 5), \n",
    "                textcoords='offset points',\n",
    "                fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Training Dataset Size (rows)')\n",
    "ax.set_ylabel('HumanEval Pass@1 (%)')\n",
    "ax.set_title('Training Efficiency: Dataset Size vs. Performance')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add reference lines\n",
    "ax.axvline(x=2600, color='green', linestyle='--', alpha=0.5, label='LeetCodeDataset Size')\n",
    "ax.axhline(y=79.9, color='green', linestyle='--', alpha=0.5, label='LeetCodeDataset Performance')\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors['Large'], label='Large Dataset (>75K)'),\n",
    "                   Patch(facecolor=colors['Medium'], label='Medium Dataset (10K-75K)'),\n",
    "                   Patch(facecolor=colors['Small'], label='Small Dataset (<10K)')]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insight\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"LeetCodeDataset with only 2.6K model-generated samples achieves 79.9% on HumanEval,\")\n",
    "print(\"outperforming datasets that are 40x larger. This demonstrates exceptional data efficiency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Research Template: Extending LeetCodeDataset\n",
    "\n",
    "Template for researchers to build upon this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Extension Template\n",
    "class ResearchExtension:\n",
    "    \"\"\"Template for extending LeetCodeDataset research\"\"\"\n",
    "    \n",
    "    def __init__(self, research_focus: str):\n",
    "        self.research_focus = research_focus\n",
    "        self.experiments = []\n",
    "        \n",
    "    def define_hypothesis(self, hypothesis: str):\n",
    "        \"\"\"Define your research hypothesis\"\"\"\n",
    "        self.hypothesis = hypothesis\n",
    "        print(f\"Research Hypothesis: {hypothesis}\")\n",
    "        \n",
    "    def design_experiment(self, name: str, description: str, metrics: List[str]):\n",
    "        \"\"\"Design an experiment\"\"\"\n",
    "        experiment = {\n",
    "            \"name\": name,\n",
    "            \"description\": description,\n",
    "            \"metrics\": metrics,\n",
    "            \"status\": \"designed\"\n",
    "        }\n",
    "        self.experiments.append(experiment)\n",
    "        return experiment\n",
    "    \n",
    "    def implement_metric(self, metric_name: str):\n",
    "        \"\"\"Template for implementing custom metrics\"\"\"\n",
    "        # Example: Complexity analysis metric\n",
    "        if metric_name == \"time_complexity\":\n",
    "            return \"\"\"\n",
    "class TimeComplexityMetric(BaseMetric):\n",
    "    def measure(self, code: str, test_cases: List[Dict]) -> Dict:\n",
    "        # 1. Parse code to AST\n",
    "        # 2. Analyze loops and recursive calls\n",
    "        # 3. Run with increasing input sizes\n",
    "        # 4. Measure execution time\n",
    "        # 5. Fit complexity curve\n",
    "        pass\n",
    "\"\"\"\n",
    "        \n",
    "    def suggest_improvements(self):\n",
    "        \"\"\"Suggest dataset improvements based on research focus\"\"\"\n",
    "        suggestions = {\n",
    "            \"complexity_analysis\": [\n",
    "                \"Add time/space complexity ground truth\",\n",
    "                \"Include performance test cases\",\n",
    "                \"Create complexity-aware evaluation metrics\"\n",
    "            ],\n",
    "            \"multi_language\": [\n",
    "                \"Extend to Java, C++, JavaScript\",\n",
    "                \"Create language-specific test harnesses\",\n",
    "                \"Study cross-language performance\"\n",
    "            ],\n",
    "            \"reasoning_analysis\": [\n",
    "                \"Capture intermediate reasoning steps\",\n",
    "                \"Create reasoning complexity taxonomy\",\n",
    "                \"Develop reasoning-aware metrics\"\n",
    "            ],\n",
    "            \"robustness\": [\n",
    "                \"Add adversarial test cases\",\n",
    "                \"Include edge case detection\",\n",
    "                \"Develop mutation testing framework\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return suggestions.get(self.research_focus, [])\n",
    "\n",
    "# Example research directions\n",
    "print(\"Potential Research Directions:\\n\")\n",
    "\n",
    "# Direction 1: Complexity Analysis\n",
    "research1 = ResearchExtension(\"complexity_analysis\")\n",
    "research1.define_hypothesis(\"Models that generate efficient solutions (O(n)) perform better on larger test cases\")\n",
    "research1.design_experiment(\n",
    "    \"complexity_correlation\",\n",
    "    \"Analyze correlation between solution complexity and model architecture\",\n",
    "    [\"time_complexity\", \"space_complexity\", \"pass_rate_large_inputs\"]\n",
    ")\n",
    "\n",
    "# Direction 2: Reasoning Enhancement\n",
    "research2 = ResearchExtension(\"reasoning_analysis\")\n",
    "research2.define_hypothesis(\"Explicit reasoning steps in training data improve performance on hard problems\")\n",
    "research2.design_experiment(\n",
    "    \"reasoning_ablation\",\n",
    "    \"Compare models trained with/without reasoning traces\",\n",
    "    [\"hard_problem_pass_rate\", \"reasoning_step_quality\", \"solution_correctness\"]\n",
    ")\n",
    "\n",
    "# Show improvement suggestions\n",
    "print(\"\\nSuggested improvements for complexity analysis research:\")\n",
    "for suggestion in research1.suggest_improvements():\n",
    "    print(f\"- {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Key Takeaways\n",
    "\n",
    "### Summary of LeetCodeDataset Contributions:\n",
    "\n",
    "1. **Comprehensive Coverage**: 2,869 Python problems (>90% of LeetCode)\n",
    "2. **Temporal Split**: Clean separation for contamination-free evaluation\n",
    "3. **Rich Metadata**: Difficulty levels, topic tags, release dates\n",
    "4. **Robust Testing**: 100+ test cases per problem\n",
    "5. **Exceptional Efficiency**: 2.6K samples match 110K dataset performance\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Reasoning Models Dominate**: DeepSeek-R1 (65.23%) significantly outperforms non-reasoning models\n",
    "2. **Model-Generated > Human Data**: For code generation tasks\n",
    "3. **Topic-Specific Strengths**: Large performance gaps in DP, Binary Search, Tree problems\n",
    "4. **Data Quality > Quantity**: Careful curation beats large-scale collection\n",
    "\n",
    "### Implementation Notes:\n",
    "\n",
    "- **Why LangChain/LangGraph**: Structured prompting, workflow orchestration, multi-LLM support\n",
    "- **Why deepeval**: Comprehensive code evaluation metrics, custom metric support\n",
    "- **Future Work**: Complexity analysis, multi-language support, reasoning enhancement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}