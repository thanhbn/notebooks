{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeRAG Focused Learning 3: Bigraph Mapping and Code Anchor Selection\n",
    "\n",
    "**Má»¥c tiÃªu**: Hiá»ƒu sÃ¢u vá» Bigraph Mapping mechanism vÃ  Code Anchor selection strategy\n",
    "\n",
    "**Paper Reference**: Section 3.3 - Bigraph Mapping\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ KhÃ¡i niá»‡m cá»‘t lÃµi\n",
    "\n",
    "### Tá»« Paper (Section 3.3):\n",
    "> *\"After acquiring the requirement graph and DS-code graph, we map the selected sub-requirement nodes and semantically similar requirement nodes of the target requirement into code nodes in DS-code graph.\"*\n",
    "\n",
    "> *\"The code nodes of sub-requirement are typically invoked by the target code. The code nodes of semantically similar requirements usually have similar functionalities to the target code.\"*\n",
    "\n",
    "### Äáº·c Ä‘iá»ƒm phá»©c táº¡p:\n",
    "1. **Bi-directional Mapping**: Map tá»« requirements sang code nodes\n",
    "2. **Multi-type Code Anchors**: Sub-requirement codes, similar codes, local codes\n",
    "3. **Intelligent Selection**: KhÃ´ng pháº£i táº¥t cáº£ mapped codes Ä‘á»u há»¯u Ã­ch\n",
    "4. **Context-aware Filtering**: Filter based on target requirement context\n",
    "5. **Confidence Scoring**: ÄÃ¡nh giÃ¡ relevance cá»§a each anchor\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Set, Any\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "# For embeddings and similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For LLM-based analysis\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Set environment\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š LÃ½ thuyáº¿t sÃ¢u: Bigraph Mapping Strategy\n",
    "\n",
    "### From Paper Section 3.3:\n",
    "> *\"CodeRAG introduces code nodes of the file where the target code locals in because the local file contents are usually related to the target code.\"*\n",
    "\n",
    "### Advanced Mapping Features:\n",
    "1. **Multi-level Mapping**: Direct mapping + contextual expansion\n",
    "2. **Relevance Scoring**: ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ liÃªn quan cá»§a mapped codes\n",
    "3. **Anchor Classification**: PhÃ¢n loáº¡i anchors theo importance\n",
    "4. **Dynamic Filtering**: Lá»c anchors based on target context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CodeAnchor:\n",
    "    \"\"\"Enhanced Code Anchor vá»›i comprehensive attributes\"\"\"\n",
    "    code_node_id: str\n",
    "    anchor_type: str  # 'sub_requirement', 'similar_requirement', 'local_file', 'context_expanded'\n",
    "    source_requirement_id: Optional[str] = None\n",
    "    relevance_score: float = 0.0\n",
    "    confidence_score: float = 0.0\n",
    "    \n",
    "    # Relationship metadata\n",
    "    mapping_method: str = \"direct\"  # direct, semantic, contextual\n",
    "    mapping_confidence: float = 0.0\n",
    "    \n",
    "    # Usage context\n",
    "    usage_frequency: int = 0\n",
    "    co_occurrence_score: float = 0.0\n",
    "    \n",
    "    # Quality indicators\n",
    "    is_primary_anchor: bool = False\n",
    "    dependency_depth: int = 0\n",
    "    \n",
    "@dataclass\n",
    "class MappingResult:\n",
    "    \"\"\"Result of bigraph mapping process\"\"\"\n",
    "    target_requirement_id: str\n",
    "    anchors: List[CodeAnchor] = field(default_factory=list)\n",
    "    mapping_statistics: Dict[str, Any] = field(default_factory=dict)\n",
    "    quality_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "class AdvancedBigraphMapper:\n",
    "    \"\"\"Advanced Bigraph Mapper vá»›i intelligent anchor selection\"\"\"\n",
    "    \n",
    "    def __init__(self, requirement_graph, ds_code_graph, llm_model=\"gpt-3.5-turbo\"):\n",
    "        self.req_graph = requirement_graph\n",
    "        self.code_graph = ds_code_graph\n",
    "        self.llm = ChatOpenAI(model=llm_model, temperature=0)\n",
    "        \n",
    "        # Mapping strategies\n",
    "        self.mapping_strategies = {\n",
    "            'direct_name_match': self._direct_name_mapping,\n",
    "            'semantic_similarity': self._semantic_similarity_mapping,\n",
    "            'contextual_expansion': self._contextual_expansion_mapping,\n",
    "            'file_locality': self._file_locality_mapping\n",
    "        }\n",
    "        \n",
    "        # Scoring weights\n",
    "        self.anchor_weights = {\n",
    "            'sub_requirement': 0.9,\n",
    "            'similar_requirement': 0.7,\n",
    "            'local_file': 0.6,\n",
    "            'context_expanded': 0.4\n",
    "        }\n",
    "        \n",
    "        # Caching for efficiency\n",
    "        self.mapping_cache = {}\n",
    "        \n",
    "    def map_requirement_to_anchors(self, target_requirement_id: str, \n",
    "                                  max_anchors: int = 10,\n",
    "                                  min_relevance: float = 0.3) -> MappingResult:\n",
    "        \"\"\"Complete mapping from requirement to code anchors\"\"\"\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"{target_requirement_id}_{max_anchors}_{min_relevance}\"\n",
    "        if cache_key in self.mapping_cache:\n",
    "            return self.mapping_cache[cache_key]\n",
    "        \n",
    "        print(f\"Mapping requirement: {target_requirement_id}\")\n",
    "        \n",
    "        result = MappingResult(target_requirement_id=target_requirement_id)\n",
    "        \n",
    "        # Step 1: Find related requirements\n",
    "        related_reqs = self._find_related_requirements(target_requirement_id)\n",
    "        print(f\"Found {len(related_reqs['sub_requirements'])} sub-requirements, \"\n",
    "              f\"{len(related_reqs['similar_requirements'])} similar requirements\")\n",
    "        \n",
    "        # Step 2: Map requirements to code nodes using multiple strategies\n",
    "        all_anchors = []\n",
    "        \n",
    "        # Map sub-requirements\n",
    "        for req_id in related_reqs['sub_requirements']:\n",
    "            anchors = self._map_single_requirement(req_id, 'sub_requirement')\n",
    "            all_anchors.extend(anchors)\n",
    "        \n",
    "        # Map similar requirements\n",
    "        for req_id in related_reqs['similar_requirements']:\n",
    "            anchors = self._map_single_requirement(req_id, 'similar_requirement')\n",
    "            all_anchors.extend(anchors)\n",
    "        \n",
    "        # Add local file anchors\n",
    "        local_anchors = self._get_local_file_anchors(target_requirement_id)\n",
    "        all_anchors.extend(local_anchors)\n",
    "        \n",
    "        # Step 3: Score and rank anchors\n",
    "        scored_anchors = self._score_anchors(all_anchors, target_requirement_id)\n",
    "        \n",
    "        # Step 4: Filter and select top anchors\n",
    "        selected_anchors = self._select_top_anchors(\n",
    "            scored_anchors, max_anchors, min_relevance\n",
    "        )\n",
    "        \n",
    "        result.anchors = selected_anchors\n",
    "        result.mapping_statistics = self._compute_mapping_statistics(selected_anchors)\n",
    "        result.quality_metrics = self._compute_quality_metrics(result)\n",
    "        \n",
    "        # Cache result\n",
    "        self.mapping_cache[cache_key] = result\n",
    "        \n",
    "        print(f\"Selected {len(selected_anchors)} code anchors\")\n",
    "        return result\n",
    "    \n",
    "    def _find_related_requirements(self, target_requirement_id: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Find sub and similar requirements\"\"\"\n",
    "        related = {\n",
    "            'sub_requirements': [],\n",
    "            'similar_requirements': []\n",
    "        }\n",
    "        \n",
    "        if target_requirement_id not in self.req_graph.graph:\n",
    "            return related\n",
    "        \n",
    "        # Find sub-requirements (children)\n",
    "        for successor in self.req_graph.graph.successors(target_requirement_id):\n",
    "            edge_data = self.req_graph.graph.get_edge_data(target_requirement_id, successor)\n",
    "            if edge_data and edge_data.get('relation_type') == 'parent-child':\n",
    "                related['sub_requirements'].append(successor)\n",
    "        \n",
    "        # Find similar requirements\n",
    "        for successor in self.req_graph.graph.successors(target_requirement_id):\n",
    "            edge_data = self.req_graph.graph.get_edge_data(target_requirement_id, successor)\n",
    "            if edge_data and edge_data.get('relation_type') == 'similarity':\n",
    "                related['similar_requirements'].append(successor)\n",
    "        \n",
    "        # Also check incoming edges for bidirectional relationships\n",
    "        for predecessor in self.req_graph.graph.predecessors(target_requirement_id):\n",
    "            edge_data = self.req_graph.graph.get_edge_data(predecessor, target_requirement_id)\n",
    "            if edge_data and edge_data.get('relation_type') == 'similarity':\n",
    "                if predecessor not in related['similar_requirements']:\n",
    "                    related['similar_requirements'].append(predecessor)\n",
    "        \n",
    "        return related\n",
    "    \n",
    "    def _map_single_requirement(self, req_id: str, anchor_type: str) -> List[CodeAnchor]:\n",
    "        \"\"\"Map single requirement to code nodes using multiple strategies\"\"\"\n",
    "        anchors = []\n",
    "        \n",
    "        if req_id not in self.req_graph.nodes:\n",
    "            return anchors\n",
    "        \n",
    "        req_node = self.req_graph.nodes[req_id]\n",
    "        \n",
    "        # Try each mapping strategy\n",
    "        for strategy_name, strategy_func in self.mapping_strategies.items():\n",
    "            try:\n",
    "                strategy_anchors = strategy_func(req_node, anchor_type)\n",
    "                anchors.extend(strategy_anchors)\n",
    "            except Exception as e:\n",
    "                print(f\"Strategy {strategy_name} failed for {req_id}: {e}\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_anchors = self._deduplicate_anchors(anchors)\n",
    "        \n",
    "        return unique_anchors\n",
    "    \n",
    "    def _direct_name_mapping(self, req_node, anchor_type: str) -> List[CodeAnchor]:\n",
    "        \"\"\"Direct mapping based on function/class names\"\"\"\n",
    "        anchors = []\n",
    "        \n",
    "        # Try exact name match\n",
    "        target_code_id = f\"function:{req_node.file_path}:{req_node.function_name}\"\n",
    "        \n",
    "        if target_code_id in self.code_graph.nodes:\n",
    "            anchor = CodeAnchor(\n",
    "                code_node_id=target_code_id,\n",
    "                anchor_type=anchor_type,\n",
    "                source_requirement_id=req_node.id,\n",
    "                mapping_method=\"direct\",\n",
    "                mapping_confidence=0.95,\n",
    "                is_primary_anchor=True\n",
    "            )\n",
    "            anchors.append(anchor)\n",
    "        \n",
    "        # Try pattern matching for similar names\n",
    "        for code_id, code_node in self.code_graph.nodes.items():\n",
    "            if (code_node.node_type in ['Function', 'Method'] and\n",
    "                code_node.file_path == req_node.file_path and\n",
    "                self._is_name_similar(req_node.function_name, code_node.name)):\n",
    "                \n",
    "                confidence = self._compute_name_similarity(req_node.function_name, code_node.name)\n",
    "                \n",
    "                anchor = CodeAnchor(\n",
    "                    code_node_id=code_id,\n",
    "                    anchor_type=anchor_type,\n",
    "                    source_requirement_id=req_node.id,\n",
    "                    mapping_method=\"direct\",\n",
    "                    mapping_confidence=confidence\n",
    "                )\n",
    "                anchors.append(anchor)\n",
    "        \n",
    "        return anchors\n",
    "    \n",
    "    def _semantic_similarity_mapping(self, req_node, anchor_type: str) -> List[CodeAnchor]:\n",
    "        \"\"\"Mapping based on semantic similarity\"\"\"\n",
    "        anchors = []\n",
    "        \n",
    "        # Find semantically similar code nodes\n",
    "        for code_id, code_node in self.code_graph.nodes.items():\n",
    "            if code_node.node_type in ['Function', 'Method']:\n",
    "                similarity = self._compute_semantic_similarity(req_node, code_node)\n",
    "                \n",
    "                if similarity >= 0.6:  # Threshold for semantic similarity\n",
    "                    anchor = CodeAnchor(\n",
    "                        code_node_id=code_id,\n",
    "                        anchor_type=anchor_type,\n",
    "                        source_requirement_id=req_node.id,\n",
    "                        mapping_method=\"semantic\",\n",
    "                        mapping_confidence=similarity\n",
    "                    )\n",
    "                    anchors.append(anchor)\n",
    "        \n",
    "        return anchors\n",
    "    \n",
    "    def _contextual_expansion_mapping(self, req_node, anchor_type: str) -> List[CodeAnchor]:\n",
    "        \"\"\"Expand mapping based on code context and dependencies\"\"\"\n",
    "        anchors = []\n",
    "        \n",
    "        # Find directly mapped nodes first\n",
    "        direct_anchors = self._direct_name_mapping(req_node, anchor_type)\n",
    "        \n",
    "        # Expand to related nodes\n",
    "        for direct_anchor in direct_anchors:\n",
    "            related_nodes = self.code_graph.get_one_hop_neighbors(\n",
    "                direct_anchor.code_node_id, \n",
    "                edge_types=['call', 'similarity']\n",
    "            )\n",
    "            \n",
    "            for related_node_id, edge_data in related_nodes:\n",
    "                confidence = edge_data.get('confidence', 0.5) * 0.7  # Reduced confidence for expansion\n",
    "                \n",
    "                anchor = CodeAnchor(\n",
    "                    code_node_id=related_node_id,\n",
    "                    anchor_type='context_expanded',\n",
    "                    source_requirement_id=req_node.id,\n",
    "                    mapping_method=\"contextual\",\n",
    "                    mapping_confidence=confidence,\n",
    "                    dependency_depth=1\n",
    "                )\n",
    "                anchors.append(anchor)\n",
    "        \n",
    "        return anchors\n",
    "    \n",
    "    def _file_locality_mapping(self, req_node, anchor_type: str) -> List[CodeAnchor]:\n",
    "        \"\"\"Map based on file locality (as mentioned in paper)\"\"\"\n",
    "        anchors = []\n",
    "        \n",
    "        # Find all code nodes in the same file\n",
    "        for code_id, code_node in self.code_graph.nodes.items():\n",
    "            if (code_node.file_path == req_node.file_path and \n",
    "                code_node.node_type in ['Function', 'Method', 'Class']):\n",
    "                \n",
    "                # Calculate file locality score\n",
    "                locality_score = self._compute_file_locality_score(req_node, code_node)\n",
    "                \n",
    "                if locality_score >= 0.3:\n",
    "                    anchor = CodeAnchor(\n",
    "                        code_node_id=code_id,\n",
    "                        anchor_type='local_file',\n",
    "                        source_requirement_id=req_node.id,\n",
    "                        mapping_method=\"file_locality\",\n",
    "                        mapping_confidence=locality_score\n",
    "                    )\n",
    "                    anchors.append(anchor)\n",
    "        \n",
    "        return anchors\n",
    "    \n",
    "    def _get_local_file_anchors(self, target_requirement_id: str) -> List[CodeAnchor]:\n",
    "        \"\"\"Get local file anchors as mentioned in paper\"\"\"\n",
    "        anchors = []\n",
    "        \n",
    "        if target_requirement_id not in self.req_graph.nodes:\n",
    "            return anchors\n",
    "        \n",
    "        target_req = self.req_graph.nodes[target_requirement_id]\n",
    "        target_file = target_req.file_path\n",
    "        \n",
    "        # Add all relevant nodes from the target file\n",
    "        for code_id, code_node in self.code_graph.nodes.items():\n",
    "            if (code_node.file_path == target_file and \n",
    "                code_node.node_type in ['Function', 'Method'] and\n",
    "                code_node.name != target_req.function_name):  # Exclude target itself\n",
    "                \n",
    "                anchor = CodeAnchor(\n",
    "                    code_node_id=code_id,\n",
    "                    anchor_type='local_file',\n",
    "                    mapping_method=\"file_locality\",\n",
    "                    mapping_confidence=0.6  # Default confidence for local files\n",
    "                )\n",
    "                anchors.append(anchor)\n",
    "        \n",
    "        return anchors\n",
    "    \n",
    "    def _score_anchors(self, anchors: List[CodeAnchor], target_requirement_id: str) -> List[CodeAnchor]:\n",
    "        \"\"\"Score anchors based on multiple factors\"\"\"\n",
    "        \n",
    "        for anchor in anchors:\n",
    "            # Base score from mapping confidence\n",
    "            base_score = anchor.mapping_confidence * self.anchor_weights.get(anchor.anchor_type, 0.5)\n",
    "            \n",
    "            # Adjust based on anchor type priority\n",
    "            type_bonus = {\n",
    "                'sub_requirement': 0.2,\n",
    "                'similar_requirement': 0.1,\n",
    "                'local_file': 0.05,\n",
    "                'context_expanded': 0.0\n",
    "            }.get(anchor.anchor_type, 0.0)\n",
    "            \n",
    "            # Primary anchor bonus\n",
    "            primary_bonus = 0.1 if anchor.is_primary_anchor else 0.0\n",
    "            \n",
    "            # Mapping method quality\n",
    "            method_bonus = {\n",
    "                'direct': 0.15,\n",
    "                'semantic': 0.1,\n",
    "                'contextual': 0.05,\n",
    "                'file_locality': 0.02\n",
    "            }.get(anchor.mapping_method, 0.0)\n",
    "            \n",
    "            # Compute final scores\n",
    "            anchor.relevance_score = min(1.0, base_score + type_bonus + primary_bonus + method_bonus)\n",
    "            anchor.confidence_score = anchor.mapping_confidence\n",
    "        \n",
    "        return anchors\n",
    "    \n",
    "    def _select_top_anchors(self, anchors: List[CodeAnchor], \n",
    "                          max_anchors: int, min_relevance: float) -> List[CodeAnchor]:\n",
    "        \"\"\"Select top anchors based on relevance and diversity\"\"\"\n",
    "        \n",
    "        # Filter by minimum relevance\n",
    "        filtered = [a for a in anchors if a.relevance_score >= min_relevance]\n",
    "        \n",
    "        # Sort by relevance score (descending)\n",
    "        sorted_anchors = sorted(filtered, key=lambda x: x.relevance_score, reverse=True)\n",
    "        \n",
    "        # Ensure diversity in anchor types\n",
    "        selected = []\n",
    "        type_counts = defaultdict(int)\n",
    "        max_per_type = max(2, max_anchors // 3)  # At least 2 per type, or 1/3 of total\n",
    "        \n",
    "        for anchor in sorted_anchors:\n",
    "            if (len(selected) < max_anchors and \n",
    "                type_counts[anchor.anchor_type] < max_per_type):\n",
    "                selected.append(anchor)\n",
    "                type_counts[anchor.anchor_type] += 1\n",
    "        \n",
    "        # Fill remaining slots with best available\n",
    "        remaining_slots = max_anchors - len(selected)\n",
    "        remaining_anchors = [a for a in sorted_anchors if a not in selected]\n",
    "        \n",
    "        selected.extend(remaining_anchors[:remaining_slots])\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    # Helper methods\n",
    "    def _is_name_similar(self, name1: str, name2: str) -> bool:\n",
    "        \"\"\"Check if two names are similar\"\"\"\n",
    "        # Simple similarity check\n",
    "        return (name1.lower() in name2.lower() or \n",
    "                name2.lower() in name1.lower() or\n",
    "                abs(len(name1) - len(name2)) <= 2)\n",
    "    \n",
    "    def _compute_name_similarity(self, name1: str, name2: str) -> float:\n",
    "        \"\"\"Compute similarity between two names\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "        return SequenceMatcher(None, name1.lower(), name2.lower()).ratio()\n",
    "    \n",
    "    def _compute_semantic_similarity(self, req_node, code_node) -> float:\n",
    "        \"\"\"Compute semantic similarity between requirement and code\"\"\"\n",
    "        # Use embeddings if available, otherwise fall back to simple text similarity\n",
    "        try:\n",
    "            if hasattr(req_node, 'embedding') and hasattr(code_node, 'embedding'):\n",
    "                if req_node.embedding is not None and code_node.embedding is not None:\n",
    "                    return cosine_similarity(\n",
    "                        req_node.embedding.reshape(1, -1),\n",
    "                        code_node.embedding.reshape(1, -1)\n",
    "                    )[0][0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Fallback to simple text similarity\n",
    "        from difflib import SequenceMatcher\n",
    "        return SequenceMatcher(None, req_node.description.lower(), \n",
    "                              code_node.source_code.lower()).ratio()\n",
    "    \n",
    "    def _compute_file_locality_score(self, req_node, code_node) -> float:\n",
    "        \"\"\"Compute file locality score\"\"\"\n",
    "        # Same file gets base score\n",
    "        base_score = 0.5\n",
    "        \n",
    "        # Bonus for same naming patterns\n",
    "        if req_node.function_name and code_node.name:\n",
    "            name_similarity = self._compute_name_similarity(req_node.function_name, code_node.name)\n",
    "            base_score += name_similarity * 0.3\n",
    "        \n",
    "        # Bonus for proximity in file (line numbers)\n",
    "        if (hasattr(req_node, 'line_start') and hasattr(code_node, 'line_start') and\n",
    "            req_node.line_start and code_node.line_start):\n",
    "            line_distance = abs(req_node.line_start - code_node.line_start)\n",
    "            proximity_bonus = max(0, 0.2 - line_distance / 1000)  # Closer = higher score\n",
    "            base_score += proximity_bonus\n",
    "        \n",
    "        return min(1.0, base_score)\n",
    "    \n",
    "    def _deduplicate_anchors(self, anchors: List[CodeAnchor]) -> List[CodeAnchor]:\n",
    "        \"\"\"Remove duplicate anchors, keeping the best one\"\"\"\n",
    "        anchor_map = {}\n",
    "        \n",
    "        for anchor in anchors:\n",
    "            key = anchor.code_node_id\n",
    "            \n",
    "            if key not in anchor_map or anchor.mapping_confidence > anchor_map[key].mapping_confidence:\n",
    "                anchor_map[key] = anchor\n",
    "        \n",
    "        return list(anchor_map.values())\n",
    "    \n",
    "    def _compute_mapping_statistics(self, anchors: List[CodeAnchor]) -> Dict[str, Any]:\n",
    "        \"\"\"Compute mapping statistics\"\"\"\n",
    "        stats = {\n",
    "            'total_anchors': len(anchors),\n",
    "            'by_type': defaultdict(int),\n",
    "            'by_method': defaultdict(int),\n",
    "            'avg_relevance': 0.0,\n",
    "            'avg_confidence': 0.0,\n",
    "            'primary_anchors': 0\n",
    "        }\n",
    "        \n",
    "        if not anchors:\n",
    "            return stats\n",
    "        \n",
    "        relevance_scores = []\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for anchor in anchors:\n",
    "            stats['by_type'][anchor.anchor_type] += 1\n",
    "            stats['by_method'][anchor.mapping_method] += 1\n",
    "            relevance_scores.append(anchor.relevance_score)\n",
    "            confidence_scores.append(anchor.confidence_score)\n",
    "            \n",
    "            if anchor.is_primary_anchor:\n",
    "                stats['primary_anchors'] += 1\n",
    "        \n",
    "        stats['avg_relevance'] = np.mean(relevance_scores)\n",
    "        stats['avg_confidence'] = np.mean(confidence_scores)\n",
    "        \n",
    "        return dict(stats)\n",
    "    \n",
    "    def _compute_quality_metrics(self, result: MappingResult) -> Dict[str, float]:\n",
    "        \"\"\"Compute quality metrics for mapping result\"\"\"\n",
    "        metrics = {\n",
    "            'coverage': 0.0,  # How well different types are covered\n",
    "            'diversity': 0.0,  # Diversity of anchor types\n",
    "            'confidence': 0.0,  # Overall confidence\n",
    "            'relevance': 0.0  # Overall relevance\n",
    "        }\n",
    "        \n",
    "        if not result.anchors:\n",
    "            return metrics\n",
    "        \n",
    "        # Confidence and relevance\n",
    "        metrics['confidence'] = np.mean([a.confidence_score for a in result.anchors])\n",
    "        metrics['relevance'] = np.mean([a.relevance_score for a in result.anchors])\n",
    "        \n",
    "        # Diversity (Shannon entropy of anchor types)\n",
    "        type_counts = Counter(a.anchor_type for a in result.anchors)\n",
    "        total = len(result.anchors)\n",
    "        entropy = -sum((count/total) * np.log2(count/total) for count in type_counts.values())\n",
    "        max_entropy = np.log2(len(type_counts)) if type_counts else 1\n",
    "        metrics['diversity'] = entropy / max_entropy if max_entropy > 0 else 0\n",
    "        \n",
    "        # Coverage (how many different types are represented)\n",
    "        expected_types = {'sub_requirement', 'similar_requirement', 'local_file'}\n",
    "        actual_types = set(a.anchor_type for a in result.anchors)\n",
    "        metrics['coverage'] = len(actual_types & expected_types) / len(expected_types)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Test data structures\n",
    "print(\"Advanced Bigraph Mapper components defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Deep Dive: Intelligent Anchor Selection\n",
    "\n",
    "### Key Innovation tá»« paper:\n",
    "> *\"CodeRAG can map sub-requirement and similar requirement nodes of the target requirement into their corresponding code nodes and successfully find supportive codes.\"*\n",
    "\n",
    "### Advanced Selection Features:\n",
    "1. **Multi-strategy Mapping**: Káº¿t há»£p multiple mapping approaches\n",
    "2. **Quality-based Filtering**: Filter anchors theo relevance vÃ  confidence\n",
    "3. **Diversity Enforcement**: Äáº£m báº£o diverse anchor types\n",
    "4. **Context-aware Scoring**: Adjust scores based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock requirement and code graphs for testing\n",
    "class MockRequirementGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.nodes = {}\n",
    "        self._create_mock_data()\n",
    "    \n",
    "    def _create_mock_data(self):\n",
    "        from dataclasses import dataclass\n",
    "        \n",
    "        @dataclass\n",
    "        class MockReqNode:\n",
    "            id: str\n",
    "            description: str\n",
    "            file_path: str\n",
    "            function_name: str\n",
    "            line_start: int = 1\n",
    "        \n",
    "        # Create mock requirement nodes\n",
    "        req_nodes = [\n",
    "            MockReqNode(\"req1\", \"Validate user input data\", \"utils.py\", \"validate_input\", 10),\n",
    "            MockReqNode(\"req2\", \"Clean and sanitize user data\", \"utils.py\", \"clean_data\", 20),\n",
    "            MockReqNode(\"req3\", \"Hash password securely\", \"auth.py\", \"hash_password\", 15),\n",
    "            MockReqNode(\"req4\", \"Verify password hash\", \"auth.py\", \"verify_password\", 25),\n",
    "            MockReqNode(\"req5\", \"Process user authentication\", \"auth.py\", \"authenticate_user\", 35)\n",
    "        ]\n",
    "        \n",
    "        for node in req_nodes:\n",
    "            self.nodes[node.id] = node\n",
    "            self.graph.add_node(node.id)\n",
    "        \n",
    "        # Add relationships\n",
    "        relationships = [\n",
    "            (\"req5\", \"req3\", \"parent-child\"),  # authenticate_user uses hash_password\n",
    "            (\"req5\", \"req4\", \"parent-child\"),  # authenticate_user uses verify_password\n",
    "            (\"req5\", \"req1\", \"parent-child\"),  # authenticate_user uses validate_input\n",
    "            (\"req1\", \"req2\", \"similarity\"),    # validate and clean are similar\n",
    "            (\"req3\", \"req4\", \"similarity\"),    # hash and verify are similar\n",
    "        ]\n",
    "        \n",
    "        for source, target, rel_type in relationships:\n",
    "            self.graph.add_edge(source, target, relation_type=rel_type, confidence=0.8)\n",
    "\n",
    "class MockDSCodeGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.nodes = {}\n",
    "        self._create_mock_data()\n",
    "    \n",
    "    def _create_mock_data(self):\n",
    "        from dataclasses import dataclass\n",
    "        \n",
    "        @dataclass\n",
    "        class MockCodeNode:\n",
    "            id: str\n",
    "            node_type: str\n",
    "            name: str\n",
    "            file_path: str\n",
    "            source_code: str\n",
    "            line_start: int = 1\n",
    "        \n",
    "        # Create mock code nodes\n",
    "        code_nodes = [\n",
    "            MockCodeNode(\"func:utils.py:validate_input\", \"Function\", \"validate_input\", \"utils.py\", \n",
    "                        \"def validate_input(data): return data is not None\", 10),\n",
    "            MockCodeNode(\"func:utils.py:clean_data\", \"Function\", \"clean_data\", \"utils.py\",\n",
    "                        \"def clean_data(data): return [x for x in data if x]\", 20),\n",
    "            MockCodeNode(\"func:utils.py:format_data\", \"Function\", \"format_data\", \"utils.py\",\n",
    "                        \"def format_data(data): return str(data).strip()\", 30),\n",
    "            MockCodeNode(\"func:auth.py:hash_password\", \"Function\", \"hash_password\", \"auth.py\",\n",
    "                        \"def hash_password(pwd): import hashlib; return hashlib.sha256(pwd.encode()).hexdigest()\", 15),\n",
    "            MockCodeNode(\"func:auth.py:verify_password\", \"Function\", \"verify_password\", \"auth.py\",\n",
    "                        \"def verify_password(pwd, hash_val): return hash_password(pwd) == hash_val\", 25),\n",
    "            MockCodeNode(\"func:auth.py:authenticate_user\", \"Function\", \"authenticate_user\", \"auth.py\",\n",
    "                        \"def authenticate_user(user, pwd): return verify_password(pwd, user.password_hash)\", 35),\n",
    "            MockCodeNode(\"func:auth.py:create_session\", \"Function\", \"create_session\", \"auth.py\",\n",
    "                        \"def create_session(user): return {'user_id': user.id, 'timestamp': time.now()}\", 45)\n",
    "        ]\n",
    "        \n",
    "        for node in code_nodes:\n",
    "            self.nodes[node.id] = node\n",
    "            self.graph.add_node(node.id)\n",
    "        \n",
    "        # Add call relationships\n",
    "        call_relationships = [\n",
    "            (\"func:auth.py:authenticate_user\", \"func:auth.py:verify_password\", \"call\"),\n",
    "            (\"func:auth.py:verify_password\", \"func:auth.py:hash_password\", \"call\"),\n",
    "        ]\n",
    "        \n",
    "        for source, target, edge_type in call_relationships:\n",
    "            self.graph.add_edge(source, target, edge_type=edge_type, confidence=0.9)\n",
    "    \n",
    "    def get_one_hop_neighbors(self, node_id: str, edge_types=None):\n",
    "        neighbors = []\n",
    "        for successor in self.graph.successors(node_id):\n",
    "            edge_data = self.graph.get_edge_data(node_id, successor)\n",
    "            if edge_types is None or edge_data.get('edge_type') in edge_types:\n",
    "                neighbors.append((successor, edge_data))\n",
    "        return neighbors\n",
    "\n",
    "# Create mock graphs\n",
    "mock_req_graph = MockRequirementGraph()\n",
    "mock_code_graph = MockDSCodeGraph()\n",
    "\n",
    "# Initialize advanced mapper\n",
    "mapper = AdvancedBigraphMapper(mock_req_graph, mock_code_graph)\n",
    "\n",
    "# Test mapping for authenticate_user requirement\n",
    "print(\"Testing Advanced Bigraph Mapping...\")\n",
    "mapping_result = mapper.map_requirement_to_anchors(\n",
    "    target_requirement_id=\"req5\",  # authenticate_user\n",
    "    max_anchors=8,\n",
    "    min_relevance=0.2\n",
    ")\n",
    "\n",
    "print(f\"\\nMapping Result Summary:\")\n",
    "print(f\"Target requirement: {mapping_result.target_requirement_id}\")\n",
    "print(f\"Total anchors: {len(mapping_result.anchors)}\")\n",
    "print(f\"\\nAnchors by type:\")\n",
    "for anchor_type, count in mapping_result.mapping_statistics['by_type'].items():\n",
    "    print(f\"  {anchor_type}: {count}\")\n",
    "\n",
    "print(f\"\\nQuality metrics:\")\n",
    "for metric, value in mapping_result.quality_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nTop 5 anchors:\")\n",
    "for i, anchor in enumerate(mapping_result.anchors[:5]):\n",
    "    code_node = mock_code_graph.nodes[anchor.code_node_id]\n",
    "    print(f\"  {i+1}. {code_node.name} ({anchor.anchor_type}) - Relevance: {anchor.relevance_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Anchor Quality Analysis vÃ  Optimization\n",
    "\n",
    "### Comprehensive analysis cá»§a anchor selection quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorQualityAnalyzer:\n",
    "    \"\"\"Comprehensive anchor quality analysis vÃ  optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, mapper: AdvancedBigraphMapper):\n",
    "        self.mapper = mapper\n",
    "        \n",
    "    def analyze_mapping_quality(self, mapping_results: List[MappingResult]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze quality across multiple mapping results\"\"\"\n",
    "        \n",
    "        analysis = {\n",
    "            'overall_metrics': {},\n",
    "            'anchor_type_analysis': {},\n",
    "            'mapping_method_analysis': {},\n",
    "            'quality_distribution': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        if not mapping_results:\n",
    "            return analysis\n",
    "        \n",
    "        # Collect all anchors\n",
    "        all_anchors = []\n",
    "        for result in mapping_results:\n",
    "            all_anchors.extend(result.anchors)\n",
    "        \n",
    "        # Overall metrics\n",
    "        relevance_scores = [a.relevance_score for a in all_anchors]\n",
    "        confidence_scores = [a.confidence_score for a in all_anchors]\n",
    "        \n",
    "        analysis['overall_metrics'] = {\n",
    "            'total_mappings': len(mapping_results),\n",
    "            'total_anchors': len(all_anchors),\n",
    "            'avg_anchors_per_mapping': len(all_anchors) / len(mapping_results),\n",
    "            'avg_relevance': np.mean(relevance_scores) if relevance_scores else 0,\n",
    "            'avg_confidence': np.mean(confidence_scores) if confidence_scores else 0,\n",
    "            'relevance_std': np.std(relevance_scores) if relevance_scores else 0,\n",
    "            'confidence_std': np.std(confidence_scores) if confidence_scores else 0\n",
    "        }\n",
    "        \n",
    "        # Anchor type analysis\n",
    "        type_analysis = defaultdict(lambda: {\n",
    "            'count': 0, 'avg_relevance': 0, 'avg_confidence': 0, 'scores': []\n",
    "        })\n",
    "        \n",
    "        for anchor in all_anchors:\n",
    "            type_analysis[anchor.anchor_type]['count'] += 1\n",
    "            type_analysis[anchor.anchor_type]['scores'].append({\n",
    "                'relevance': anchor.relevance_score,\n",
    "                'confidence': anchor.confidence_score\n",
    "            })\n",
    "        \n",
    "        for anchor_type, data in type_analysis.items():\n",
    "            scores = data['scores']\n",
    "            data['avg_relevance'] = np.mean([s['relevance'] for s in scores])\n",
    "            data['avg_confidence'] = np.mean([s['confidence'] for s in scores])\n",
    "            del data['scores']  # Remove raw scores to save space\n",
    "        \n",
    "        analysis['anchor_type_analysis'] = dict(type_analysis)\n",
    "        \n",
    "        # Mapping method analysis\n",
    "        method_analysis = defaultdict(lambda: {\n",
    "            'count': 0, 'avg_relevance': 0, 'avg_confidence': 0, 'success_rate': 0\n",
    "        })\n",
    "        \n",
    "        for anchor in all_anchors:\n",
    "            method = anchor.mapping_method\n",
    "            method_analysis[method]['count'] += 1\n",
    "            method_analysis[method]['avg_relevance'] += anchor.relevance_score\n",
    "            method_analysis[method]['avg_confidence'] += anchor.confidence_score\n",
    "            \n",
    "            if anchor.relevance_score >= 0.6:  # Successful mapping threshold\n",
    "                method_analysis[method]['success_rate'] += 1\n",
    "        \n",
    "        for method, data in method_analysis.items():\n",
    "            count = data['count']\n",
    "            if count > 0:\n",
    "                data['avg_relevance'] /= count\n",
    "                data['avg_confidence'] /= count\n",
    "                data['success_rate'] = data['success_rate'] / count\n",
    "        \n",
    "        analysis['mapping_method_analysis'] = dict(method_analysis)\n",
    "        \n",
    "        # Quality distribution analysis\n",
    "        quality_ranges = {\n",
    "            'high_quality': sum(1 for a in all_anchors if a.relevance_score >= 0.8),\n",
    "            'medium_quality': sum(1 for a in all_anchors if 0.5 <= a.relevance_score < 0.8),\n",
    "            'low_quality': sum(1 for a in all_anchors if a.relevance_score < 0.5)\n",
    "        }\n",
    "        \n",
    "        total = len(all_anchors)\n",
    "        analysis['quality_distribution'] = {\n",
    "            k: {'count': v, 'percentage': v/total*100 if total > 0 else 0}\n",
    "            for k, v in quality_ranges.items()\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(analysis)\n",
    "        analysis['recommendations'] = recommendations\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _generate_recommendations(self, analysis: Dict) -> List[str]:\n",
    "        \"\"\"Generate optimization recommendations based on analysis\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        overall = analysis['overall_metrics']\n",
    "        type_analysis = analysis['anchor_type_analysis']\n",
    "        method_analysis = analysis['mapping_method_analysis']\n",
    "        quality_dist = analysis['quality_distribution']\n",
    "        \n",
    "        # Overall quality recommendations\n",
    "        if overall['avg_relevance'] < 0.6:\n",
    "            recommendations.append(\"Overall relevance is low - consider tuning anchor weights or thresholds\")\n",
    "        \n",
    "        if overall['relevance_std'] > 0.3:\n",
    "            recommendations.append(\"High variance in relevance scores - review scoring consistency\")\n",
    "        \n",
    "        # Type-specific recommendations\n",
    "        for anchor_type, metrics in type_analysis.items():\n",
    "            if metrics['avg_relevance'] < 0.5:\n",
    "                recommendations.append(f\"Improve {anchor_type} anchor quality - current avg: {metrics['avg_relevance']:.3f}\")\n",
    "        \n",
    "        # Method-specific recommendations\n",
    "        best_method = max(method_analysis.items(), key=lambda x: x[1]['success_rate'])\n",
    "        worst_method = min(method_analysis.items(), key=lambda x: x[1]['success_rate'])\n",
    "        \n",
    "        if best_method[1]['success_rate'] - worst_method[1]['success_rate'] > 0.3:\n",
    "            recommendations.append(f\"Consider emphasizing {best_method[0]} method over {worst_method[0]}\")\n",
    "        \n",
    "        # Quality distribution recommendations\n",
    "        high_quality_pct = quality_dist['high_quality']['percentage']\n",
    "        if high_quality_pct < 30:\n",
    "            recommendations.append(f\"Only {high_quality_pct:.1f}% high-quality anchors - review selection criteria\")\n",
    "        \n",
    "        low_quality_pct = quality_dist['low_quality']['percentage']\n",
    "        if low_quality_pct > 40:\n",
    "            recommendations.append(f\"{low_quality_pct:.1f}% low-quality anchors - increase minimum relevance threshold\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def visualize_anchor_analysis(self, analysis: Dict, figsize=(16, 12)):\n",
    "        \"\"\"Visualize anchor quality analysis\"\"\"\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # 1. Anchor type performance\n",
    "        type_analysis = analysis['anchor_type_analysis']\n",
    "        if type_analysis:\n",
    "            types = list(type_analysis.keys())\n",
    "            relevance_scores = [type_analysis[t]['avg_relevance'] for t in types]\n",
    "            confidence_scores = [type_analysis[t]['avg_confidence'] for t in types]\n",
    "            \n",
    "            x = np.arange(len(types))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax1.bar(x - width/2, relevance_scores, width, label='Relevance', alpha=0.8)\n",
    "            ax1.bar(x + width/2, confidence_scores, width, label='Confidence', alpha=0.8)\n",
    "            ax1.set_xlabel('Anchor Type')\n",
    "            ax1.set_ylabel('Score')\n",
    "            ax1.set_title('Performance by Anchor Type')\n",
    "            ax1.set_xticks(x)\n",
    "            ax1.set_xticklabels(types, rotation=45, ha='right')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Mapping method success rates\n",
    "        method_analysis = analysis['mapping_method_analysis']\n",
    "        if method_analysis:\n",
    "            methods = list(method_analysis.keys())\n",
    "            success_rates = [method_analysis[m]['success_rate'] for m in methods]\n",
    "            counts = [method_analysis[m]['count'] for m in methods]\n",
    "            \n",
    "            # Color by success rate\n",
    "            colors = ['green' if sr >= 0.7 else 'orange' if sr >= 0.5 else 'red' for sr in success_rates]\n",
    "            \n",
    "            bars = ax2.bar(methods, success_rates, color=colors, alpha=0.8)\n",
    "            ax2.set_xlabel('Mapping Method')\n",
    "            ax2.set_ylabel('Success Rate')\n",
    "            ax2.set_title('Success Rate by Mapping Method')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add count labels\n",
    "            for bar, count in zip(bars, counts):\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # 3. Quality distribution\n",
    "        quality_dist = analysis['quality_distribution']\n",
    "        if quality_dist:\n",
    "            labels = list(quality_dist.keys())\n",
    "            sizes = [quality_dist[label]['count'] for label in labels]\n",
    "            colors = ['green', 'orange', 'red']\n",
    "            \n",
    "            wedges, texts, autotexts = ax3.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            ax3.set_title('Quality Distribution')\n",
    "        \n",
    "        # 4. Overall metrics summary\n",
    "        overall = analysis['overall_metrics']\n",
    "        if overall:\n",
    "            metrics = ['Avg Relevance', 'Avg Confidence', 'Anchors per Mapping']\n",
    "            values = [\n",
    "                overall['avg_relevance'],\n",
    "                overall['avg_confidence'],\n",
    "                overall['avg_anchors_per_mapping'] / 10  # Normalize for visualization\n",
    "            ]\n",
    "            \n",
    "            colors = ['green' if v >= 0.7 else 'orange' if v >= 0.5 else 'red' for v in values]\n",
    "            bars = ax4.bar(metrics, values, color=colors, alpha=0.8)\n",
    "            ax4.set_ylabel('Normalized Score')\n",
    "            ax4.set_title('Overall Quality Metrics')\n",
    "            ax4.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, values):\n",
    "                display_value = value * 10 if 'Anchors' in bar.get_x() else value\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'{display_value:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed analysis\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ANCHOR QUALITY ANALYSIS REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Overall Metrics:\")\n",
    "        for metric, value in overall.items():\n",
    "            print(f\"â€¢ {metric.replace('_', ' ').title()}: {value:.3f}\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Quality Distribution:\")\n",
    "        for quality, data in quality_dist.items():\n",
    "            print(f\"â€¢ {quality.replace('_', ' ').title()}: {data['count']} ({data['percentage']:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¡ Recommendations:\")\n",
    "        for i, rec in enumerate(analysis['recommendations'], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Test with multiple mappings\n",
    "test_requirements = [\"req1\", \"req3\", \"req5\"]\n",
    "mapping_results = []\n",
    "\n",
    "print(\"\\nTesting multiple requirement mappings...\")\n",
    "for req_id in test_requirements:\n",
    "    if req_id in mock_req_graph.nodes:\n",
    "        result = mapper.map_requirement_to_anchors(req_id, max_anchors=6, min_relevance=0.1)\n",
    "        mapping_results.append(result)\n",
    "        print(f\"Mapped {req_id}: {len(result.anchors)} anchors\")\n",
    "\n",
    "# Analyze quality\n",
    "analyzer = AnchorQualityAnalyzer(mapper)\n",
    "quality_analysis = analyzer.analyze_mapping_quality(mapping_results)\n",
    "analyzer.visualize_anchor_analysis(quality_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Comprehensive Testing vá»›i Mock Data\n",
    "\n",
    "### Test scenarios Ä‘á»ƒ validate bigraph mapping correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigraph_mapping_test_scenarios():\n",
    "    \"\"\"Create comprehensive test scenarios for bigraph mapping\"\"\"\n",
    "    \n",
    "    scenarios = {\n",
    "        'direct_mapping_test': {\n",
    "            'description': 'Test direct name-based mapping',\n",
    "            'target_req': 'req3',  # hash_password\n",
    "            'expected_anchors': {\n",
    "                'direct': ['hash_password'],\n",
    "                'similar': ['verify_password'],  # Similar functionality\n",
    "                'local': ['authenticate_user', 'create_session']  # Same file\n",
    "            },\n",
    "            'min_expected_total': 3\n",
    "        },\n",
    "        \n",
    "        'sub_requirement_mapping_test': {\n",
    "            'description': 'Test sub-requirement mapping',\n",
    "            'target_req': 'req5',  # authenticate_user (has sub-requirements)\n",
    "            'expected_anchors': {\n",
    "                'sub_requirement': ['hash_password', 'verify_password', 'validate_input'],\n",
    "                'local': ['create_session'],\n",
    "                'context_expanded': []  # May include related functions\n",
    "            },\n",
    "            'min_expected_total': 4\n",
    "        },\n",
    "        \n",
    "        'similarity_mapping_test': {\n",
    "            'description': 'Test similarity-based mapping',\n",
    "            'target_req': 'req1',  # validate_input (has similar requirement)\n",
    "            'expected_anchors': {\n",
    "                'direct': ['validate_input'],\n",
    "                'similar': ['clean_data'],  # Similar requirement\n",
    "                'local': ['format_data']  # Same file\n",
    "            },\n",
    "            'min_expected_total': 2\n",
    "        },\n",
    "        \n",
    "        'quality_threshold_test': {\n",
    "            'description': 'Test quality thresholding',\n",
    "            'target_req': 'req2',  # clean_data\n",
    "            'config': {'min_relevance': 0.7, 'max_anchors': 3},\n",
    "            'expected_behavior': 'Only high-quality anchors should be selected'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "def run_bigraph_mapping_test(scenario_name: str, scenario_data: Dict, mapper: AdvancedBigraphMapper) -> Dict:\n",
    "    \"\"\"Run a single bigraph mapping test scenario\"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸ§ª Testing: {scenario_name}\")\n",
    "    print(f\"Description: {scenario_data['description']}\")\n",
    "    \n",
    "    target_req = scenario_data['target_req']\n",
    "    config = scenario_data.get('config', {})\n",
    "    \n",
    "    # Run mapping\n",
    "    result = mapper.map_requirement_to_anchors(\n",
    "        target_requirement_id=target_req,\n",
    "        max_anchors=config.get('max_anchors', 8),\n",
    "        min_relevance=config.get('min_relevance', 0.2)\n",
    "    )\n",
    "    \n",
    "    test_result = {\n",
    "        'scenario': scenario_name,\n",
    "        'target_req': target_req,\n",
    "        'total_anchors': len(result.anchors),\n",
    "        'anchors_by_type': defaultdict(list),\n",
    "        'quality_metrics': result.quality_metrics,\n",
    "        'passed_checks': [],\n",
    "        'failed_checks': [],\n",
    "        'success_score': 0.0\n",
    "    }\n",
    "    \n",
    "    # Organize anchors by type\n",
    "    for anchor in result.anchors:\n",
    "        code_node = mapper.code_graph.nodes[anchor.code_node_id]\n",
    "        test_result['anchors_by_type'][anchor.anchor_type].append(code_node.name)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"â€¢ Total anchors: {test_result['total_anchors']}\")\n",
    "    for anchor_type, names in test_result['anchors_by_type'].items():\n",
    "        print(f\"â€¢ {anchor_type}: {names}\")\n",
    "    \n",
    "    # Run specific tests\n",
    "    if 'expected_anchors' in scenario_data:\n",
    "        expected = scenario_data['expected_anchors']\n",
    "        \n",
    "        # Check each expected type\n",
    "        for exp_type, exp_names in expected.items():\n",
    "            actual_names = test_result['anchors_by_type'].get(exp_type, [])\n",
    "            \n",
    "            # Check if expected anchors are found\n",
    "            found_expected = [name for name in exp_names if name in actual_names]\n",
    "            \n",
    "            if len(found_expected) >= len(exp_names) * 0.7:  # 70% threshold\n",
    "                test_result['passed_checks'].append(f\"{exp_type}: Found {len(found_expected)}/{len(exp_names)} expected\")\n",
    "            else:\n",
    "                test_result['failed_checks'].append(f\"{exp_type}: Only found {len(found_expected)}/{len(exp_names)} expected\")\n",
    "    \n",
    "    # Check minimum total anchors\n",
    "    if 'min_expected_total' in scenario_data:\n",
    "        min_expected = scenario_data['min_expected_total']\n",
    "        if test_result['total_anchors'] >= min_expected:\n",
    "            test_result['passed_checks'].append(f\"Minimum anchors: {test_result['total_anchors']} >= {min_expected}\")\n",
    "        else:\n",
    "            test_result['failed_checks'].append(f\"Minimum anchors: {test_result['total_anchors']} < {min_expected}\")\n",
    "    \n",
    "    # Check quality metrics\n",
    "    quality = result.quality_metrics\n",
    "    if quality.get('relevance', 0) >= 0.5:\n",
    "        test_result['passed_checks'].append(f\"Good relevance: {quality['relevance']:.3f}\")\n",
    "    else:\n",
    "        test_result['failed_checks'].append(f\"Low relevance: {quality['relevance']:.3f}\")\n",
    "    \n",
    "    if quality.get('diversity', 0) >= 0.3:\n",
    "        test_result['passed_checks'].append(f\"Good diversity: {quality['diversity']:.3f}\")\n",
    "    else:\n",
    "        test_result['failed_checks'].append(f\"Low diversity: {quality['diversity']:.3f}\")\n",
    "    \n",
    "    # Calculate success score\n",
    "    total_checks = len(test_result['passed_checks']) + len(test_result['failed_checks'])\n",
    "    if total_checks > 0:\n",
    "        test_result['success_score'] = len(test_result['passed_checks']) / total_checks\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nâœ… Passed checks: {len(test_result['passed_checks'])}\")\n",
    "    for check in test_result['passed_checks']:\n",
    "        print(f\"  â€¢ {check}\")\n",
    "    \n",
    "    if test_result['failed_checks']:\n",
    "        print(f\"\\nâŒ Failed checks: {len(test_result['failed_checks'])}\")\n",
    "        for check in test_result['failed_checks']:\n",
    "            print(f\"  â€¢ {check}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Success score: {test_result['success_score']:.1%}\")\n",
    "    \n",
    "    return test_result\n",
    "\n",
    "def run_comprehensive_bigraph_tests() -> List[Dict]:\n",
    "    \"\"\"Run all bigraph mapping test scenarios\"\"\"\n",
    "    \n",
    "    scenarios = create_bigraph_mapping_test_scenarios()\n",
    "    test_results = []\n",
    "    \n",
    "    for scenario_name, scenario_data in scenarios.items():\n",
    "        result = run_bigraph_mapping_test(scenario_name, scenario_data, mapper)\n",
    "        test_results.append(result)\n",
    "    \n",
    "    # Overall summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BIGRAPH MAPPING TEST SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_success = sum(r['success_score'] for r in test_results)\n",
    "    avg_success = total_success / len(test_results) if test_results else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Overall Results:\")\n",
    "    print(f\"â€¢ Total test scenarios: {len(test_results)}\")\n",
    "    print(f\"â€¢ Average success rate: {avg_success:.1%}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Individual Results:\")\n",
    "    for result in test_results:\n",
    "        status = \"âœ…\" if result['success_score'] >= 0.8 else \"âš ï¸\" if result['success_score'] >= 0.6 else \"âŒ\"\n",
    "        print(f\"{status} {result['scenario']}: {result['success_score']:.1%} ({result['total_anchors']} anchors)\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Success rates by scenario\n",
    "    scenario_names = [r['scenario'].replace('_test', '').replace('_', ' ').title() for r in test_results]\n",
    "    success_scores = [r['success_score'] for r in test_results]\n",
    "    \n",
    "    colors = ['green' if score >= 0.8 else 'orange' if score >= 0.6 else 'red' for score in success_scores]\n",
    "    bars1 = ax1.bar(scenario_names, success_scores, color=colors, alpha=0.8)\n",
    "    ax1.set_title('Test Success Rates by Scenario')\n",
    "    ax1.set_ylabel('Success Rate')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, score in zip(bars1, success_scores):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{score:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # Anchor counts by scenario\n",
    "    anchor_counts = [r['total_anchors'] for r in test_results]\n",
    "    bars2 = ax2.bar(scenario_names, anchor_counts, alpha=0.8, color='skyblue')\n",
    "    ax2.set_title('Total Anchors by Scenario')\n",
    "    ax2.set_ylabel('Number of Anchors')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars2, anchor_counts):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Run comprehensive tests\n",
    "test_results = run_comprehensive_bigraph_tests()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BIGRAPH MAPPING FOCUSED LEARNING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Key Learnings:\")\n",
    "print(\"1. Multi-strategy mapping improves anchor discovery\")\n",
    "print(\"2. Quality scoring helps select relevant anchors\")\n",
    "print(\"3. Diversity enforcement prevents anchor bias\")\n",
    "print(\"4. Context-aware filtering improves precision\")\n",
    "print(\"5. Comprehensive testing validates mapping correctness\")\n",
    "print(\"6. Local file anchors provide important context\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}