{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders trong LangChain\n",
    "\n",
    "## Giới thiệu\n",
    "\n",
    "Document Loaders là thành phần quan trọng trong LangChain, giúp:\n",
    "- **Tải dữ liệu** từ nhiều nguồn khác nhau (PDF, Web, CSV, Text, ...)\n",
    "- **Chuẩn hóa** dữ liệu thành format Document chuẩn\n",
    "- **Trích xuất metadata** hữu ích cho việc xử lý sau này\n",
    "- **Tích hợp** dễ dàng với các components khác (splitters, embeddings, vector stores)\n",
    "\n",
    "Trong notebook này, chúng ta sẽ học cách sử dụng các loaders phổ biến nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup môi trường và chuẩn bị dữ liệu mẫu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    WebBaseLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    DirectoryLoader,\n",
    "    JSONLoader\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục cho data samples\n",
    "data_dir = Path(\"sample_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Tạo file text mẫu\n",
    "sample_text = \"\"\"Giới thiệu về LangChain\n",
    "\n",
    "LangChain là một framework mạnh mẽ cho phát triển ứng dụng với Large Language Models (LLMs).\n",
    "Framework này cung cấp:\n",
    "\n",
    "1. Chains: Kết nối các components lại với nhau\n",
    "2. Agents: Tự động quyết định actions dựa trên input\n",
    "3. Memory: Lưu trữ context giữa các lần tương tác\n",
    "4. Callbacks: Theo dõi và debug applications\n",
    "\n",
    "LangChain hỗ trợ nhiều LLM providers như OpenAI, Anthropic, Cohere, và nhiều hơn nữa.\n",
    "\"\"\"\n",
    "\n",
    "with open(data_dir / \"langchain_intro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(\"✓ Đã tạo file text mẫu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo file CSV mẫu\n",
    "csv_data = {\n",
    "    \"product_name\": [\"iPhone 15\", \"Samsung S24\", \"Google Pixel 8\", \"OnePlus 12\"],\n",
    "    \"category\": [\"Smartphone\", \"Smartphone\", \"Smartphone\", \"Smartphone\"],\n",
    "    \"price_vnd\": [25000000, 22000000, 18000000, 16000000],\n",
    "    \"features\": [\n",
    "        \"A17 Pro chip, Dynamic Island, 48MP camera\",\n",
    "        \"Snapdragon 8 Gen 3, 200MP camera, S Pen support\",\n",
    "        \"Google Tensor G3, Best Android camera, 7 years update\",\n",
    "        \"Snapdragon 8 Gen 3, 100W charging, Hasselblad camera\"\n",
    "    ],\n",
    "    \"rating\": [4.8, 4.7, 4.6, 4.5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(csv_data)\n",
    "df.to_csv(data_dir / \"products.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✓ Đã tạo file CSV mẫu\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo file JSON mẫu\n",
    "json_data = {\n",
    "    \"company\": \"TechViet Solutions\",\n",
    "    \"founded\": 2020,\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"name\": \"Nguyễn Văn A\",\n",
    "            \"position\": \"CEO\",\n",
    "            \"department\": \"Management\",\n",
    "            \"skills\": [\"Leadership\", \"Strategy\", \"Business Development\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Trần Thị B\",\n",
    "            \"position\": \"CTO\",\n",
    "            \"department\": \"Technology\",\n",
    "            \"skills\": [\"Python\", \"Cloud Architecture\", \"AI/ML\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Lê Văn C\",\n",
    "            \"position\": \"Senior Developer\",\n",
    "            \"department\": \"Technology\",\n",
    "            \"skills\": [\"React\", \"Node.js\", \"MongoDB\"]\n",
    "        }\n",
    "    ],\n",
    "    \"products\": [\n",
    "        {\"name\": \"AI Assistant\", \"type\": \"SaaS\", \"users\": 10000},\n",
    "        {\"name\": \"Data Analytics Platform\", \"type\": \"Enterprise\", \"users\": 500}\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(data_dir / \"company_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✓ Đã tạo file JSON mẫu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo file Markdown mẫu\n",
    "markdown_content = \"\"\"# RAG System Architecture\n",
    "\n",
    "## Overview\n",
    "Retrieval-Augmented Generation (RAG) là một kỹ thuật kết hợp khả năng truy xuất thông tin với khả năng sinh văn bản của LLMs.\n",
    "\n",
    "## Components\n",
    "\n",
    "### 1. Document Loading\n",
    "- Load documents từ nhiều nguồn\n",
    "- Support nhiều formats: PDF, DOCX, HTML, etc.\n",
    "\n",
    "### 2. Text Splitting\n",
    "- Chia documents thành chunks nhỏ hơn\n",
    "- Overlap để maintain context\n",
    "\n",
    "### 3. Embedding\n",
    "- Convert text chunks thành vectors\n",
    "- Sử dụng models như OpenAI Embeddings, Sentence Transformers\n",
    "\n",
    "### 4. Vector Storage\n",
    "- Lưu trữ embeddings trong vector databases\n",
    "- Support similarity search\n",
    "\n",
    "### 5. Retrieval\n",
    "- Tìm kiếm relevant chunks dựa trên query\n",
    "- Ranking và filtering results\n",
    "\n",
    "### 6. Generation\n",
    "- Sử dụng retrieved context với LLM\n",
    "- Generate final response\n",
    "\n",
    "## Best Practices\n",
    "- Chunk size: 500-1000 tokens\n",
    "- Overlap: 10-20%\n",
    "- Top-k retrieval: 3-5 chunks\n",
    "\"\"\"\n",
    "\n",
    "with open(data_dir / \"rag_architecture.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown_content)\n",
    "\n",
    "print(\"✓ Đã tạo file Markdown mẫu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TextLoader - Loader cơ bản nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file\n",
    "text_loader = TextLoader(\n",
    "    file_path=str(data_dir / \"langchain_intro.txt\"),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "text_docs = text_loader.load()\n",
    "\n",
    "print(f\"Số lượng documents: {len(text_docs)}\")\n",
    "print(f\"\\nDocument đầu tiên:\")\n",
    "print(f\"- Content preview: {text_docs[0].page_content[:200]}...\")\n",
    "print(f\"- Metadata: {text_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextLoader với custom metadata\n",
    "class CustomTextLoader(TextLoader):\n",
    "    \"\"\"TextLoader với metadata tùy chỉnh\"\"\"\n",
    "    \n",
    "    def load(self):\n",
    "        documents = super().load()\n",
    "        # Thêm custom metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                \"type\": \"tutorial\",\n",
    "                \"language\": \"vietnamese\",\n",
    "                \"topic\": \"langchain\",\n",
    "                \"word_count\": len(doc.page_content.split())\n",
    "            })\n",
    "        return documents\n",
    "\n",
    "custom_loader = CustomTextLoader(str(data_dir / \"langchain_intro.txt\"), encoding=\"utf-8\")\n",
    "custom_docs = custom_loader.load()\n",
    "\n",
    "print(\"Document với custom metadata:\")\n",
    "print(f\"Metadata: {custom_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSVLoader - Load dữ liệu từ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CSV loading\n",
    "csv_loader = CSVLoader(\n",
    "    file_path=str(data_dir / \"products.csv\"),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "csv_docs = csv_loader.load()\n",
    "\n",
    "print(f\"Số lượng documents từ CSV: {len(csv_docs)}\")\n",
    "print(f\"\\nDocument đầu tiên:\")\n",
    "print(csv_docs[0].page_content)\n",
    "print(f\"\\nMetadata: {csv_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVLoader với custom settings\n",
    "custom_csv_loader = CSVLoader(\n",
    "    file_path=str(data_dir / \"products.csv\"),\n",
    "    encoding=\"utf-8\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "    },\n",
    "    source_column=\"product_name\"  # Sử dụng column này cho metadata source\n",
    ")\n",
    "\n",
    "custom_csv_docs = custom_csv_loader.load()\n",
    "\n",
    "# Format lại content cho dễ đọc\n",
    "for i, doc in enumerate(custom_csv_docs[:2]):\n",
    "    print(f\"\\n=== Product {i+1} ===\")\n",
    "    # Parse content thành dict để display đẹp hơn\n",
    "    lines = doc.page_content.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            print(f\"  {key.strip()}: {value.strip()}\")\n",
    "    print(f\"  Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSONLoader - Load dữ liệu từ JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONLoader với jq schema để extract specific data\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Load toàn bộ JSON\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    \"\"\"Custom function để tạo metadata\"\"\"\n",
    "    metadata[\"source\"] = \"company_database\"\n",
    "    metadata[\"type\"] = \"employee_record\" if \"position\" in record else \"company_info\"\n",
    "    return metadata\n",
    "\n",
    "# Loader cho employees\n",
    "employee_loader = JSONLoader(\n",
    "    file_path=str(data_dir / \"company_data.json\"),\n",
    "    jq_schema=\".employees[]\",  # Extract each employee\n",
    "    content_key=\"name\",  # Use name as main content\n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "employee_docs = employee_loader.load()\n",
    "\n",
    "print(f\"Số lượng employee documents: {len(employee_docs)}\")\n",
    "for doc in employee_docs:\n",
    "    print(f\"\\nEmployee: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONLoader cho complex extraction\n",
    "# Extract tất cả thông tin thành text\n",
    "full_json_loader = JSONLoader(\n",
    "    file_path=str(data_dir / \"company_data.json\"),\n",
    "    jq_schema=\".\",\n",
    "    text_content=True  # Convert entire JSON to text\n",
    ")\n",
    "\n",
    "full_docs = full_json_loader.load()\n",
    "print(f\"Full JSON as text (first 500 chars):\")\n",
    "print(full_docs[0].page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WebBaseLoader - Load từ websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebBaseLoader cơ bản\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Load một webpage\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=[\"https://python.langchain.com/docs/modules/data_connection/document_loaders/\"],\n",
    "    bs_kwargs={\"parse_only\": bs4.SoupStrainer(\"div\", {\"class\": \"markdown\"})}\n",
    ")\n",
    "\n",
    "try:\n",
    "    web_docs = web_loader.load()\n",
    "    print(f\"Loaded {len(web_docs)} documents from web\")\n",
    "    print(f\"\\nContent preview (first 500 chars):\")\n",
    "    print(web_docs[0].page_content[:500] + \"...\")\n",
    "    print(f\"\\nMetadata: {web_docs[0].metadata}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading webpage: {e}\")\n",
    "    print(\"Tip: Đảm bảo bạn có kết nối internet và URL hợp lệ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebBaseLoader với multiple URLs\n",
    "multiple_urls = [\n",
    "    \"https://python.langchain.com/docs/get_started/introduction\",\n",
    "    \"https://python.langchain.com/docs/get_started/installation\"\n",
    "]\n",
    "\n",
    "# Custom parser function\n",
    "def parse_langchain_docs(soup):\n",
    "    \"\"\"Extract main content từ LangChain docs\"\"\"\n",
    "    # Tìm main content area\n",
    "    main_content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", {\"class\": \"markdown\"})\n",
    "    if main_content:\n",
    "        # Remove code blocks để text dễ đọc hơn\n",
    "        for code in main_content.find_all([\"pre\", \"code\"]):\n",
    "            code.decompose()\n",
    "        return main_content.get_text(separator=\"\\n\", strip=True)\n",
    "    return soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Loader với custom parser\n",
    "custom_web_loader = WebBaseLoader(\n",
    "    web_paths=multiple_urls[:1],  # Load 1 URL để demo\n",
    "    bs_get_text_kwargs={\"separator\": \"\\n\", \"strip\": True}\n",
    ")\n",
    "\n",
    "try:\n",
    "    custom_web_docs = custom_web_loader.load()\n",
    "    if custom_web_docs:\n",
    "        print(f\"Loaded from {custom_web_docs[0].metadata['source']}\")\n",
    "        print(f\"Title: {custom_web_docs[0].metadata.get('title', 'N/A')}\")\n",
    "        print(f\"Content words: {len(custom_web_docs[0].page_content.split())}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Web loading requires internet connection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyPDFLoader - Load PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một PDF mẫu (nếu có pypdf)\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "    \n",
    "    # Tạo PDF mẫu\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=16)\n",
    "    pdf.cell(0, 10, txt=\"LangChain Tutorial\", ln=True, align='C')\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.ln(10)\n",
    "    \n",
    "    content = [\n",
    "        \"Chapter 1: Introduction to LangChain\",\n",
    "        \"\",\n",
    "        \"LangChain is a powerful framework for building LLM applications.\",\n",
    "        \"It provides modular components that can be combined in various ways.\",\n",
    "        \"\",\n",
    "        \"Key Features:\",\n",
    "        \"- Document loaders for various formats\",\n",
    "        \"- Text splitters for chunking\",\n",
    "        \"- Vector stores for embeddings\",\n",
    "        \"- Chains for complex workflows\"\n",
    "    ]\n",
    "    \n",
    "    for line in content:\n",
    "        pdf.cell(0, 10, txt=line, ln=True)\n",
    "    \n",
    "    pdf_path = data_dir / \"langchain_tutorial.pdf\"\n",
    "    pdf.output(str(pdf_path))\n",
    "    print(\"✓ Đã tạo file PDF mẫu\")\n",
    "    \n",
    "    # Load PDF\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    \n",
    "    pdf_loader = PyPDFLoader(str(pdf_path))\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    \n",
    "    print(f\"\\nSố pages loaded: {len(pdf_docs)}\")\n",
    "    for i, doc in enumerate(pdf_docs):\n",
    "        print(f\"\\nPage {i+1}:\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Cần install fpdf2 để tạo PDF mẫu: pip install fpdf2\")\n",
    "    print(\"Và pypdf để sử dụng PyPDFLoader: pip install pypdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Markdown file\n",
    "try:\n",
    "    from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "    \n",
    "    md_loader = UnstructuredMarkdownLoader(\n",
    "        str(data_dir / \"rag_architecture.md\"),\n",
    "        mode=\"elements\"  # hoặc \"single\" cho 1 document\n",
    "    )\n",
    "    \n",
    "    md_docs = md_loader.load()\n",
    "    \n",
    "    print(f\"Số lượng elements: {len(md_docs)}\")\n",
    "    \n",
    "    # Show different element types\n",
    "    element_types = set()\n",
    "    for doc in md_docs:\n",
    "        if 'category' in doc.metadata:\n",
    "            element_types.add(doc.metadata['category'])\n",
    "    \n",
    "    print(f\"\\nElement types found: {element_types}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    for i, doc in enumerate(md_docs[:3]):\n",
    "        print(f\"\\nElement {i+1}:\")\n",
    "        print(f\"Type: {doc.metadata.get('category', 'unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:100]}...\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Cần install unstructured: pip install unstructured\")\n",
    "    \n",
    "    # Fallback: simple markdown loader\n",
    "    print(\"\\nSử dụng TextLoader thay thế:\")\n",
    "    md_text_loader = TextLoader(str(data_dir / \"rag_architecture.md\"), encoding=\"utf-8\")\n",
    "    md_text_docs = md_text_loader.load()\n",
    "    print(f\"Loaded as text: {len(md_text_docs[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DirectoryLoader - Load toàn bộ thư mục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tất cả text files trong directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    str(data_dir),\n",
    "    glob=\"**/*.txt\",  # Pattern để tìm files\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "dir_docs = dir_loader.load()\n",
    "\n",
    "print(f\"Tìm thấy {len(dir_docs)} text files\")\n",
    "for doc in dir_docs:\n",
    "    print(f\"\\nFile: {doc.metadata['source']}\")\n",
    "    print(f\"Preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DirectoryLoader với multiple file types\n",
    "from langchain_community.document_loaders import TextLoader, CSVLoader\n",
    "\n",
    "# Tạo thêm vài files để test\n",
    "with open(data_dir / \"notes.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Đây là ghi chú về LangChain và RAG systems.\")\n",
    "\n",
    "# Loader cho mixed file types\n",
    "text_loader_mixed = DirectoryLoader(\n",
    "    str(data_dir),\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True  # Show loading progress\n",
    ")\n",
    "\n",
    "csv_loader_mixed = DirectoryLoader(\n",
    "    str(data_dir),\n",
    "    glob=\"**/*.csv\",\n",
    "    loader_cls=CSVLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Load all documents\n",
    "all_text_docs = text_loader_mixed.load()\n",
    "all_csv_docs = csv_loader_mixed.load()\n",
    "\n",
    "print(f\"Total text documents: {len(all_text_docs)}\")\n",
    "print(f\"Total CSV documents: {len(all_csv_docs)}\")\n",
    "print(f\"\\nTất cả files loaded:\")\n",
    "\n",
    "all_docs = all_text_docs + all_csv_docs\n",
    "for doc in all_docs:\n",
    "    source = Path(doc.metadata['source']).name\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Xử lý Documents sau khi load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitting cho documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load một document dài\n",
    "long_text = \"\"\"LangChain là một framework mạnh mẽ cho việc phát triển ứng dụng AI.\n",
    "\n",
    "Các thành phần chính của LangChain bao gồm:\n",
    "\n",
    "1. Models: Tích hợp với các LLM providers\n",
    "LangChain hỗ trợ nhiều model providers như OpenAI, Anthropic, Cohere, Hugging Face và nhiều hơn nữa.\n",
    "Mỗi provider có những ưu điểm riêng về hiệu suất, chi phí và tính năng.\n",
    "\n",
    "2. Prompts: Quản lý và tối ưu hóa prompts\n",
    "Prompt engineering là một phần quan trọng trong việc làm việc với LLMs.\n",
    "LangChain cung cấp các tools để tạo, quản lý và tối ưu hóa prompts.\n",
    "\n",
    "3. Memory: Lưu trữ conversation history\n",
    "Memory components cho phép lưu trữ và truy xuất thông tin từ các cuộc hội thoại trước.\n",
    "Điều này rất quan trọng cho việc xây dựng chatbots và các ứng dụng conversational.\n",
    "\n",
    "4. Chains: Kết nối các components\n",
    "Chains cho phép kết nối nhiều components lại với nhau để tạo ra workflows phức tạp.\n",
    "Ví dụ: load document -> split text -> create embeddings -> store in vector database.\n",
    "\n",
    "5. Agents: Tự động reasoning và action\n",
    "Agents có thể tự quyết định actions cần thực hiện dựa trên input.\n",
    "Chúng sử dụng LLMs để reasoning và có thể gọi các tools khác nhau.\n",
    "\"\"\"\n",
    "\n",
    "# Tạo document\n",
    "from langchain.schema import Document\n",
    "long_doc = Document(\n",
    "    page_content=long_text,\n",
    "    metadata={\"source\": \"manual\", \"topic\": \"langchain_overview\"}\n",
    ")\n",
    "\n",
    "# Split document\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents([long_doc])\n",
    "\n",
    "print(f\"Original document length: {len(long_doc.page_content)} characters\")\n",
    "print(f\"Number of chunks: {len(splits)}\")\n",
    "print(\"\\nChunks:\")\n",
    "for i, chunk in enumerate(splits[:3]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk.page_content)} chars):\")\n",
    "    print(f\"Content: {chunk.page_content}\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices và Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 1: Xử lý encoding\n",
    "def safe_load_text(file_path, encodings=['utf-8', 'latin-1', 'cp1252']):\n",
    "    \"\"\"Try multiple encodings để load text file\"\"\"\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding=encoding)\n",
    "            return loader.load()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise Exception(f\"Could not load file with any encoding: {encodings}\")\n",
    "\n",
    "# Best Practice 2: Add metadata during loading\n",
    "def enrich_documents(docs, additional_metadata):\n",
    "    \"\"\"Thêm metadata cho documents\"\"\"\n",
    "    for doc in docs:\n",
    "        doc.metadata.update(additional_metadata)\n",
    "        doc.metadata['char_count'] = len(doc.page_content)\n",
    "        doc.metadata['word_count'] = len(doc.page_content.split())\n",
    "        doc.metadata['load_time'] = pd.Timestamp.now().isoformat()\n",
    "    return docs\n",
    "\n",
    "# Best Practice 3: Validate loaded content\n",
    "def validate_documents(docs):\n",
    "    \"\"\"Kiểm tra documents sau khi load\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        # Check empty content\n",
    "        if not doc.page_content or doc.page_content.isspace():\n",
    "            issues.append(f\"Document {i} has empty content\")\n",
    "        \n",
    "        # Check metadata\n",
    "        if 'source' not in doc.metadata:\n",
    "            issues.append(f\"Document {i} missing source metadata\")\n",
    "        \n",
    "        # Check reasonable size\n",
    "        if len(doc.page_content) > 1000000:  # 1MB text\n",
    "            issues.append(f\"Document {i} unusually large: {len(doc.page_content)} chars\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Test best practices\n",
    "test_docs = text_loader.load()\n",
    "enriched_docs = enrich_documents(test_docs, {\"project\": \"langchain_tutorial\"})\n",
    "issues = validate_documents(enriched_docs)\n",
    "\n",
    "print(\"Enriched document metadata:\")\n",
    "print(enriched_docs[0].metadata)\n",
    "print(f\"\\nValidation issues found: {len(issues)}\")\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"- {issue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loader Selection Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm helper để chọn loader phù hợp\n",
    "def get_loader_for_file(file_path):\n",
    "    \"\"\"Tự động chọn loader dựa trên file extension\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    extension = file_path.suffix.lower()\n",
    "    \n",
    "    loader_map = {\n",
    "        '.txt': TextLoader,\n",
    "        '.csv': CSVLoader,\n",
    "        '.json': JSONLoader,\n",
    "        '.md': UnstructuredMarkdownLoader,\n",
    "        '.pdf': 'PyPDFLoader',  # String vì có thể chưa install\n",
    "        '.html': 'UnstructuredHTMLLoader',\n",
    "        '.docx': 'Docx2txtLoader',\n",
    "    }\n",
    "    \n",
    "    loader_class = loader_map.get(extension)\n",
    "    if not loader_class:\n",
    "        print(f\"No specific loader for {extension}, using TextLoader\")\n",
    "        return TextLoader\n",
    "    \n",
    "    if isinstance(loader_class, str):\n",
    "        print(f\"Loader {loader_class} requires additional installation\")\n",
    "        return TextLoader\n",
    "    \n",
    "    return loader_class\n",
    "\n",
    "# Test với các files trong sample_data\n",
    "print(\"Loader recommendations:\")\n",
    "for file in data_dir.glob(\"*\"):\n",
    "    if file.is_file():\n",
    "        loader = get_loader_for_file(file)\n",
    "        print(f\"- {file.name}: {loader.__name__ if hasattr(loader, '__name__') else loader}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng kết\n",
    "\n",
    "Trong notebook này, chúng ta đã học về Document Loaders:\n",
    "\n",
    "### 1. **Các loại Loader chính**:\n",
    "- **TextLoader**: Đơn giản, cho text files\n",
    "- **CSVLoader**: Structured data từ CSV\n",
    "- **JSONLoader**: Flexible với jq schema\n",
    "- **WebBaseLoader**: Scrape web content\n",
    "- **PyPDFLoader**: Extract từ PDF\n",
    "- **DirectoryLoader**: Batch loading\n",
    "\n",
    "### 2. **Key Concepts**:\n",
    "- Mỗi loader tạo ra `Document` objects\n",
    "- Documents có `page_content` và `metadata`\n",
    "- Metadata quan trọng cho filtering và retrieval\n",
    "\n",
    "### 3. **Best Practices**:\n",
    "- Handle encoding issues\n",
    "- Enrich metadata\n",
    "- Validate loaded content\n",
    "- Choose appropriate loader\n",
    "- Split large documents\n",
    "\n",
    "### 4. **Next Steps**:\n",
    "- Text Splitters để chunk documents\n",
    "- Embeddings để convert thành vectors\n",
    "- Vector Stores để lưu trữ và search\n",
    "\n",
    "Document Loaders là bước đầu tiên quan trọng trong pipeline RAG!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}