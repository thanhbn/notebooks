{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "semantic-intro",
   "metadata": {},
   "source": [
    "# Semantic Metadata Extraction Deep Dive\n",
    "\n",
    "## Learning Objective\n",
    "Master the extraction and utilization of **semantic metadata** from source code, specifically **function call graphs** and **code summarization**, as presented in the paper for enhancing few-shot prompting in automated code review.\n",
    "\n",
    "## Paper Context\n",
    "**Section III-D**: \"RQ3: Impact of Semantic Metadata Augmented Prompts for RCG\"\n",
    "\n",
    "*\"Augmenting statically analyzed, semantic structural facts to prompt the code model proved to be beneficial in code summarization tasks. Inspired by this, we propose a new methodology to design cost-effective few-shot prompts for proprietary LLMs, augmented with a programming language component- function call graph and a natural language component- code summary.\"*\n",
    "\n",
    "## Key Concepts to Master\n",
    "1. **Abstract Syntax Trees (AST)**: Foundation for code analysis\n",
    "2. **Function Call Graph Extraction**: Static analysis for control flow\n",
    "3. **Code Summarization**: Natural language generation from code\n",
    "4. **Prompt Augmentation**: Integrating semantic metadata into LLM prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ast-fundamentals",
   "metadata": {},
   "source": [
    "## 1. Abstract Syntax Tree (AST) Fundamentals\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "An Abstract Syntax Tree represents the syntactic structure of source code in a tree format where:\n",
    "- **Nodes** represent constructs (functions, expressions, statements)\n",
    "- **Edges** represent syntactic relationships\n",
    "- **Leaves** represent tokens (identifiers, literals)\n",
    "\n",
    "### Paper Quote\n",
    "*\"AST, in essence, is a data structure that captures the syntactic structure of a program or code. It forms a tree where each node denotes a construct occurring in the code.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ast-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Set, Tuple, Any\n",
    "import json\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ASTAnalyzer:\n",
    "    \"\"\"Advanced AST analysis for extracting semantic metadata\n",
    "    \n",
    "    Implements the paper's approach: \"We parsed each old file code 'oldf' from our \n",
    "    dataset to generate an AST to identify key elements like function calls and definitions\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.function_definitions = set()\n",
    "        self.function_calls = []\n",
    "        self.class_definitions = set()\n",
    "        self.imports = []\n",
    "        self.variables = set()\n",
    "    \n",
    "    def parse_code(self, code: str) -> ast.AST:\n",
    "        \"\"\"Parse code into AST with error handling\"\"\"\n",
    "        try:\n",
    "            return ast.parse(code)\n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error in code: {e}\")\n",
    "            # Return empty module for graceful handling\n",
    "            return ast.Module(body=[], type_ignores=[])\n",
    "    \n",
    "    def extract_all_metadata(self, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract comprehensive semantic metadata from code\"\"\"\n",
    "        tree = self.parse_code(code)\n",
    "        \n",
    "        # Reset state\n",
    "        self.function_definitions.clear()\n",
    "        self.function_calls.clear()\n",
    "        self.class_definitions.clear()\n",
    "        self.imports.clear()\n",
    "        self.variables.clear()\n",
    "        \n",
    "        # Traverse AST\n",
    "        self._traverse_ast(tree)\n",
    "        \n",
    "        return {\n",
    "            'function_definitions': list(self.function_definitions),\n",
    "            'function_calls': self.function_calls,\n",
    "            'class_definitions': list(self.class_definitions),\n",
    "            'imports': self.imports,\n",
    "            'variables': list(self.variables),\n",
    "            'complexity_metrics': self._calculate_complexity_metrics(tree)\n",
    "        }\n",
    "    \n",
    "    def _traverse_ast(self, node: ast.AST, context: str = 'global') -> None:\n",
    "        \"\"\"Recursively traverse AST to extract semantic information\"\"\"\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            self.function_definitions.add(node.name)\n",
    "            # Recursively analyze function body\n",
    "            for child in ast.walk(node):\n",
    "                if isinstance(child, ast.Call) and hasattr(child.func, 'id'):\n",
    "                    self.function_calls.append({\n",
    "                        'caller': node.name,\n",
    "                        'callee': child.func.id,\n",
    "                        'line': child.lineno if hasattr(child, 'lineno') else 0\n",
    "                    })\n",
    "        \n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            self.class_definitions.add(node.name)\n",
    "        \n",
    "        elif isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "            if isinstance(node, ast.Import):\n",
    "                for alias in node.names:\n",
    "                    self.imports.append({'module': alias.name, 'type': 'import'})\n",
    "            else:  # ImportFrom\n",
    "                module = node.module or ''\n",
    "                for alias in node.names:\n",
    "                    self.imports.append({\n",
    "                        'module': module, \n",
    "                        'name': alias.name, \n",
    "                        'type': 'from_import'\n",
    "                    })\n",
    "        \n",
    "        elif isinstance(node, ast.Assign):\n",
    "            for target in node.targets:\n",
    "                if isinstance(target, ast.Name):\n",
    "                    self.variables.add(target.id)\n",
    "        \n",
    "        # Continue traversal for child nodes\n",
    "        for child in ast.iter_child_nodes(node):\n",
    "            self._traverse_ast(child, context)\n",
    "    \n",
    "    def _calculate_complexity_metrics(self, tree: ast.AST) -> Dict[str, int]:\n",
    "        \"\"\"Calculate basic complexity metrics\"\"\"\n",
    "        metrics = {\n",
    "            'total_nodes': len(list(ast.walk(tree))),\n",
    "            'function_count': len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]),\n",
    "            'class_count': len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]),\n",
    "            'conditional_count': len([n for n in ast.walk(tree) if isinstance(n, (ast.If, ast.While, ast.For))]),\n",
    "            'call_count': len([n for n in ast.walk(tree) if isinstance(n, ast.Call)])\n",
    "        }\n",
    "        \n",
    "        # Calculate cyclomatic complexity (simplified)\n",
    "        decision_nodes = len([n for n in ast.walk(tree) \n",
    "                            if isinstance(n, (ast.If, ast.While, ast.For, ast.ExceptHandler))])\n",
    "        metrics['cyclomatic_complexity'] = decision_nodes + 1\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def visualize_ast(self, code: str, max_depth: int = 3) -> None:\n",
    "        \"\"\"Visualize AST structure (limited depth for readability)\"\"\"\n",
    "        tree = self.parse_code(code)\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        node_labels = {}\n",
    "        \n",
    "        def add_ast_nodes(node, parent=None, depth=0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            \n",
    "            node_id = id(node)\n",
    "            node_type = type(node).__name__\n",
    "            \n",
    "            # Add node attributes for better labeling\n",
    "            if hasattr(node, 'name'):\n",
    "                label = f\"{node_type}\\n{node.name}\"\n",
    "            elif hasattr(node, 'id'):\n",
    "                label = f\"{node_type}\\n{node.id}\"\n",
    "            elif hasattr(node, 'value') and isinstance(node.value, (str, int, float)):\n",
    "                label = f\"{node_type}\\n{str(node.value)[:10]}\"\n",
    "            else:\n",
    "                label = node_type\n",
    "            \n",
    "            G.add_node(node_id)\n",
    "            node_labels[node_id] = label\n",
    "            \n",
    "            if parent is not None:\n",
    "                G.add_edge(parent, node_id)\n",
    "            \n",
    "            for child in ast.iter_child_nodes(node):\n",
    "                add_ast_nodes(child, node_id, depth + 1)\n",
    "        \n",
    "        add_ast_nodes(tree)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        nx.draw(G, pos, labels=node_labels, node_color='lightblue', \n",
    "                node_size=3000, font_size=8, font_weight='bold', \n",
    "                arrows=True, arrowsize=20, edge_color='gray')\n",
    "        plt.title(f\"AST Visualization (Depth â‰¤ {max_depth})\", fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate AST analysis with complex example\n",
    "sample_code = '''\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "class CodeAnalyzer:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "    \n",
    "    def analyze_file(self, filepath: str) -> Dict:\n",
    "        \"\"\"Analyze a single file\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            return self.handle_error(\"File not found\")\n",
    "        \n",
    "        content = self.read_file(filepath)\n",
    "        metrics = self.calculate_metrics(content)\n",
    "        \n",
    "        return {\n",
    "            'file': filepath,\n",
    "            'metrics': metrics,\n",
    "            'status': 'success'\n",
    "        }\n",
    "    \n",
    "    def read_file(self, filepath: str) -> str:\n",
    "        with open(filepath, 'r') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def calculate_metrics(self, content: str) -> Dict:\n",
    "        lines = content.split('\\n')\n",
    "        return {\n",
    "            'line_count': len(lines),\n",
    "            'char_count': len(content)\n",
    "        }\n",
    "    \n",
    "    def handle_error(self, message: str) -> Dict:\n",
    "        return {'error': message, 'status': 'failed'}\n",
    "\n",
    "def main():\n",
    "    analyzer = CodeAnalyzer({'debug': True})\n",
    "    result = analyzer.analyze_file('test.py')\n",
    "    print(result)\n",
    "'''\n",
    "\n",
    "analyzer = ASTAnalyzer()\n",
    "metadata = analyzer.extract_all_metadata(sample_code)\n",
    "\n",
    "print(\"AST Semantic Metadata Extraction\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Function definitions: {metadata['function_definitions']}\")\n",
    "print(f\"Class definitions: {metadata['class_definitions']}\")\n",
    "print(f\"Imports: {len(metadata['imports'])} modules\")\n",
    "print(f\"Variables: {len(metadata['variables'])} unique variables\")\n",
    "print(f\"\\nFunction calls:\")\n",
    "for call in metadata['function_calls'][:5]:  # Show first 5\n",
    "    print(f\"  {call['caller']} â†’ {call['callee']} (line {call['line']})\")\n",
    "\n",
    "print(f\"\\nComplexity metrics:\")\n",
    "for metric, value in metadata['complexity_metrics'].items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Visualize AST structure\n",
    "print(\"\\nGenerating AST visualization...\")\n",
    "analyzer.visualize_ast(sample_code, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "function-call-graph",
   "metadata": {},
   "source": [
    "## 2. Function Call Graph Extraction\n",
    "\n",
    "### Paper Methodology\n",
    "*\"A function call graph is a control flow graph representing which function is called from other functions. It creates a directed graph where each node represents a function or module and each edge symbolizes the call from one function to another.\"*\n",
    "\n",
    "### Implementation Strategy\n",
    "1. **Parse AST** to identify function definitions and calls\n",
    "2. **Build adjacency lists** for call relationships\n",
    "3. **Remove scope resolution** and external calls\n",
    "4. **Format for prompt integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "call-graph-extractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCallGraphExtractor:\n",
    "    \"\"\"Advanced function call graph extraction as described in the paper\n",
    "    \n",
    "    Implementation based on paper Section III-D.1: \"Extracting Function Call Graph\"\n",
    "    Handles multiple programming languages and complex call patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.call_graph = defaultdict(set)\n",
    "        self.function_definitions = set()\n",
    "        self.external_calls = set()\n",
    "        self.method_calls = defaultdict(set)  # For class methods\n",
    "    \n",
    "    def extract_call_graph(self, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract comprehensive function call graph\n",
    "        \n",
    "        Returns both the graph structure and metadata for analysis\n",
    "        \"\"\"\n",
    "        tree = ast.parse(code)\n",
    "        \n",
    "        # Reset state\n",
    "        self.call_graph.clear()\n",
    "        self.function_definitions.clear()\n",
    "        self.external_calls.clear()\n",
    "        self.method_calls.clear()\n",
    "        \n",
    "        # First pass: identify all function definitions\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                self.function_definitions.add(node.name)\n",
    "        \n",
    "        # Second pass: identify function calls within each function\n",
    "        self._analyze_calls(tree)\n",
    "        \n",
    "        # Clean up graph as per paper: \"remove scope resolution operators and duplicate function calls\"\n",
    "        cleaned_graph = self._clean_call_graph()\n",
    "        \n",
    "        return {\n",
    "            'call_graph': cleaned_graph,\n",
    "            'function_definitions': list(self.function_definitions),\n",
    "            'external_calls': list(self.external_calls),\n",
    "            'graph_metrics': self._calculate_graph_metrics(cleaned_graph),\n",
    "            'adjacency_matrix': self._build_adjacency_matrix(cleaned_graph)\n",
    "        }\n",
    "    \n",
    "    def _analyze_calls(self, tree: ast.AST) -> None:\n",
    "        \"\"\"Analyze function calls within each function definition\"\"\"\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                current_function = node.name\n",
    "                \n",
    "                # Find all calls within this function\n",
    "                for child in ast.walk(node):\n",
    "                    if isinstance(child, ast.Call):\n",
    "                        called_function = self._extract_function_name(child.func)\n",
    "                        if called_function:\n",
    "                            # Check if it's an internal or external call\n",
    "                            if called_function in self.function_definitions:\n",
    "                                self.call_graph[current_function].add(called_function)\n",
    "                            else:\n",
    "                                self.external_calls.add(called_function)\n",
    "    \n",
    "    def _extract_function_name(self, func_node: ast.AST) -> str:\n",
    "        \"\"\"Extract function name from various call patterns\"\"\"\n",
    "        if isinstance(func_node, ast.Name):\n",
    "            return func_node.id\n",
    "        elif isinstance(func_node, ast.Attribute):\n",
    "            # For method calls like obj.method(), return just 'method'\n",
    "            # This implements paper's approach: \"remove the scope resolution operators\"\n",
    "            return func_node.attr\n",
    "        elif isinstance(func_node, ast.Subscript):\n",
    "            # For calls like func[0](), try to extract base name\n",
    "            if isinstance(func_node.value, ast.Name):\n",
    "                return func_node.value.id\n",
    "        return None\n",
    "    \n",
    "    def _clean_call_graph(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Clean call graph by removing duplicates and external calls\n",
    "        \n",
    "        Based on paper: \"we chose to remove the scope resolution operators and \n",
    "        duplicate function calls. Finally, we excluded external (e.g., library) function calls\"\n",
    "        \"\"\"\n",
    "        cleaned = {}\n",
    "        for caller, callees in self.call_graph.items():\n",
    "            # Convert set to list and sort for consistency\n",
    "            cleaned_callees = sorted(list(callees))\n",
    "            if cleaned_callees:  # Only include functions that make calls\n",
    "                cleaned[caller] = cleaned_callees\n",
    "            else:\n",
    "                cleaned[caller] = []  # Functions with no internal calls\n",
    "        \n",
    "        # Ensure all defined functions are in the graph\n",
    "        for func in self.function_definitions:\n",
    "            if func not in cleaned:\n",
    "                cleaned[func] = []\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def _calculate_graph_metrics(self, graph: Dict[str, List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate graph-theoretic metrics for analysis\"\"\"\n",
    "        total_nodes = len(graph)\n",
    "        total_edges = sum(len(callees) for callees in graph.values())\n",
    "        \n",
    "        # Calculate in-degree and out-degree\n",
    "        in_degree = defaultdict(int)\n",
    "        out_degree = {func: len(callees) for func, callees in graph.items()}\n",
    "        \n",
    "        for caller, callees in graph.items():\n",
    "            for callee in callees:\n",
    "                if callee in graph:  # Only count internal calls\n",
    "                    in_degree[callee] += 1\n",
    "        \n",
    "        # Find root functions (no incoming calls) and leaf functions (no outgoing calls)\n",
    "        root_functions = [func for func in graph if in_degree[func] == 0]\n",
    "        leaf_functions = [func for func, callees in graph.items() if len(callees) == 0]\n",
    "        \n",
    "        return {\n",
    "            'node_count': total_nodes,\n",
    "            'edge_count': total_edges,\n",
    "            'density': total_edges / (total_nodes * (total_nodes - 1)) if total_nodes > 1 else 0,\n",
    "            'avg_out_degree': total_edges / total_nodes if total_nodes > 0 else 0,\n",
    "            'max_out_degree': max(out_degree.values()) if out_degree else 0,\n",
    "            'root_functions': root_functions,\n",
    "            'leaf_functions': leaf_functions,\n",
    "            'strongly_connected': self._find_cycles(graph)\n",
    "        }\n",
    "    \n",
    "    def _find_cycles(self, graph: Dict[str, List[str]]) -> List[List[str]]:\n",
    "        \"\"\"Find cycles in the call graph (potential recursive patterns)\"\"\"\n",
    "        def dfs(node, path, visited, rec_stack, cycles):\n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            path.append(node)\n",
    "            \n",
    "            for neighbor in graph.get(node, []):\n",
    "                if neighbor in rec_stack:\n",
    "                    # Found a cycle\n",
    "                    cycle_start = path.index(neighbor)\n",
    "                    cycle = path[cycle_start:] + [neighbor]\n",
    "                    cycles.append(cycle)\n",
    "                elif neighbor not in visited:\n",
    "                    dfs(neighbor, path, visited, rec_stack, cycles)\n",
    "            \n",
    "            path.pop()\n",
    "            rec_stack.remove(node)\n",
    "        \n",
    "        visited = set()\n",
    "        cycles = []\n",
    "        \n",
    "        for node in graph:\n",
    "            if node not in visited:\n",
    "                dfs(node, [], visited, set(), cycles)\n",
    "        \n",
    "        return cycles\n",
    "    \n",
    "    def _build_adjacency_matrix(self, graph: Dict[str, List[str]]) -> List[List[int]]:\n",
    "        \"\"\"Build adjacency matrix representation\"\"\"\n",
    "        functions = sorted(graph.keys())\n",
    "        n = len(functions)\n",
    "        func_to_idx = {func: i for i, func in enumerate(functions)}\n",
    "        \n",
    "        matrix = [[0] * n for _ in range(n)]\n",
    "        \n",
    "        for caller, callees in graph.items():\n",
    "            caller_idx = func_to_idx[caller]\n",
    "            for callee in callees:\n",
    "                if callee in func_to_idx:\n",
    "                    callee_idx = func_to_idx[callee]\n",
    "                    matrix[caller_idx][callee_idx] = 1\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def visualize_call_graph(self, graph: Dict[str, List[str]], title: str = \"Function Call Graph\") -> None:\n",
    "        \"\"\"Visualize call graph using NetworkX\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for func in graph.keys():\n",
    "            G.add_node(func)\n",
    "        \n",
    "        # Add edges\n",
    "        for caller, callees in graph.items():\n",
    "            for callee in callees:\n",
    "                if callee in graph:  # Only internal calls\n",
    "                    G.add_edge(caller, callee)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Use hierarchical layout if possible\n",
    "        try:\n",
    "            pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
    "        except:\n",
    "            pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Color nodes by their role\n",
    "        node_colors = []\n",
    "        for node in G.nodes():\n",
    "            in_degree = G.in_degree(node)\n",
    "            out_degree = G.out_degree(node)\n",
    "            \n",
    "            if in_degree == 0:\n",
    "                node_colors.append('lightgreen')  # Root functions\n",
    "            elif out_degree == 0:\n",
    "                node_colors.append('lightcoral')  # Leaf functions\n",
    "            else:\n",
    "                node_colors.append('lightblue')   # Intermediate functions\n",
    "        \n",
    "        nx.draw(G, pos, node_color=node_colors, node_size=2000, \n",
    "                font_size=10, font_weight='bold', arrows=True, \n",
    "                arrowsize=20, edge_color='gray', alpha=0.7)\n",
    "        \n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', \n",
    "                      markersize=10, label='Root (entry points)'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', \n",
    "                      markersize=10, label='Intermediate'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', \n",
    "                      markersize=10, label='Leaf (no calls)')\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        plt.title(title, fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def format_for_prompt(self, graph: Dict[str, List[str]]) -> str:\n",
    "        \"\"\"Format call graph for LLM prompt augmentation\n",
    "        \n",
    "        Based on paper approach for prompt integration\n",
    "        \"\"\"\n",
    "        if not graph:\n",
    "            return \"Function Call Graph: No internal function calls detected.\"\n",
    "        \n",
    "        formatted_lines = [\"Function Call Graph:\"]\n",
    "        \n",
    "        for caller, callees in sorted(graph.items()):\n",
    "            if callees:\n",
    "                formatted_lines.append(f\"- {caller} calls: {', '.join(callees)}\")\n",
    "            else:\n",
    "                formatted_lines.append(f\"- {caller} (no internal calls)\")\n",
    "        \n",
    "        return \"\\n\".join(formatted_lines)\n",
    "\n",
    "# Test the advanced call graph extractor\n",
    "extractor = AdvancedCallGraphExtractor()\n",
    "result = extractor.extract_call_graph(sample_code)\n",
    "\n",
    "print(\"Advanced Function Call Graph Analysis\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Functions defined: {len(result['function_definitions'])}\")\n",
    "print(f\"External calls detected: {len(result['external_calls'])}\")\n",
    "print(f\"\\nCall Graph:\")\n",
    "for caller, callees in result['call_graph'].items():\n",
    "    if callees:\n",
    "        print(f\"  {caller} â†’ {', '.join(callees)}\")\n",
    "    else:\n",
    "        print(f\"  {caller} (no calls)\")\n",
    "\n",
    "print(f\"\\nGraph Metrics:\")\n",
    "metrics = result['graph_metrics']\n",
    "print(f\"  Nodes: {metrics['node_count']}\")\n",
    "print(f\"  Edges: {metrics['edge_count']}\")\n",
    "print(f\"  Density: {metrics['density']:.3f}\")\n",
    "print(f\"  Root functions: {metrics['root_functions']}\")\n",
    "print(f\"  Leaf functions: {metrics['leaf_functions']}\")\n",
    "print(f\"  Cycles detected: {len(metrics['strongly_connected'])}\")\n",
    "\n",
    "print(f\"\\nPrompt-formatted output:\")\n",
    "print(extractor.format_for_prompt(result['call_graph']))\n",
    "\n",
    "# Visualize the call graph\n",
    "extractor.visualize_call_graph(result['call_graph'], \"Sample Code Call Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "code-summarization",
   "metadata": {},
   "source": [
    "## 3. Code Summarization with CodeT5\n",
    "\n",
    "### Paper Approach\n",
    "*\"We tokenized the extracted code using CodeT5's RoBERTa-based tokenizer, splitting larger functions into smaller chunks as needed. These tokenized chunks were then fed into the CodeT5 model, generating summaries that were appended to our prompts to enhance review accuracy.\"*\n",
    "\n",
    "### Implementation Strategy\n",
    "1. **Extract relevant functions** from code diffs\n",
    "2. **Tokenize with CodeT5** tokenizer\n",
    "3. **Generate natural language summaries**\n",
    "4. **Handle chunking** for large functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-summarizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "\n",
    "class AdvancedCodeSummarizer:\n",
    "    \"\"\"Advanced code summarization system as described in the paper\n",
    "    \n",
    "    Implementation based on paper Section III-D.2: \"Generating Code Summary\"\n",
    "    Uses both model-based and heuristic approaches for educational purposes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_size: int = 512):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        \n",
    "        # Try to load CodeT5 model (as used in paper)\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base-multi-sum\")\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-base-multi-sum\")\n",
    "            self.model_available = True\n",
    "            print(\"CodeT5 model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"CodeT5 model not available: {e}\")\n",
    "            print(\"Using heuristic summarization instead\")\n",
    "            self.model_available = False\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "    \n",
    "    def extract_relevant_function(self, code: str, diff: str) -> str:\n",
    "        \"\"\"Extract function relevant to the code diff\n",
    "        \n",
    "        Based on paper: \"If the code diff was not inside any function, \n",
    "        we extracted the code around the code diff\"\n",
    "        \"\"\"\n",
    "        # Parse the code to find function boundaries\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # Find line numbers in diff (simplified extraction)\n",
    "            diff_lines = self._extract_line_numbers_from_diff(diff)\n",
    "            \n",
    "            # Find function containing these lines\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    func_start = node.lineno\n",
    "                    # Estimate function end (simplified)\n",
    "                    func_end = getattr(node, 'end_lineno', func_start + 20)\n",
    "                    \n",
    "                    # Check if diff overlaps with function\n",
    "                    if any(func_start <= line <= func_end for line in diff_lines):\n",
    "                        return ast.get_source_segment(code, node) or self._extract_function_text(code, node)\n",
    "            \n",
    "            # If no function found, return surrounding context\n",
    "            return self._extract_context_around_diff(code, diff_lines)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting function: {e}\")\n",
    "            return code  # Fallback to entire code\n",
    "    \n",
    "    def _extract_line_numbers_from_diff(self, diff: str) -> List[int]:\n",
    "        \"\"\"Extract line numbers from diff format\"\"\"\n",
    "        lines = []\n",
    "        current_line = 1\n",
    "        \n",
    "        for line in diff.split('\\n'):\n",
    "            if line.startswith('@@'):\n",
    "                # Parse hunk header like @@ -1,4 +1,6 @@\n",
    "                match = re.search(r'@@.*\\+(\\d+)', line)\n",
    "                if match:\n",
    "                    current_line = int(match.group(1))\n",
    "            elif line.startswith('+') or line.startswith('-'):\n",
    "                lines.append(current_line)\n",
    "            elif not line.startswith('-'):\n",
    "                current_line += 1\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    def _extract_function_text(self, code: str, func_node: ast.FunctionDef) -> str:\n",
    "        \"\"\"Extract function text from AST node\"\"\"\n",
    "        lines = code.split('\\n')\n",
    "        start_line = func_node.lineno - 1  # Convert to 0-based\n",
    "        \n",
    "        # Find function end by looking for next def/class or end of file\n",
    "        end_line = len(lines)\n",
    "        for i in range(start_line + 1, len(lines)):\n",
    "            if lines[i].strip() and not lines[i].startswith(' ') and not lines[i].startswith('\\t'):\n",
    "                if lines[i].strip().startswith(('def ', 'class ', '@')):\n",
    "                    end_line = i\n",
    "                    break\n",
    "        \n",
    "        return '\\n'.join(lines[start_line:end_line])\n",
    "    \n",
    "    def _extract_context_around_diff(self, code: str, diff_lines: List[int]) -> str:\n",
    "        \"\"\"Extract context around diff lines\"\"\"\n",
    "        if not diff_lines:\n",
    "            return code[:500]  # Return first 500 chars as fallback\n",
    "        \n",
    "        lines = code.split('\\n')\n",
    "        min_line = max(0, min(diff_lines) - 5)\n",
    "        max_line = min(len(lines), max(diff_lines) + 5)\n",
    "        \n",
    "        return '\\n'.join(lines[min_line:max_line])\n",
    "    \n",
    "    def chunk_code(self, code: str) -> List[str]:\n",
    "        \"\"\"Split code into chunks if too large\n",
    "        \n",
    "        Based on paper: \"splitting larger functions into smaller chunks as needed\"\n",
    "        \"\"\"\n",
    "        if self.model_available:\n",
    "            # Use actual tokenizer\n",
    "            tokens = self.tokenizer.encode(code)\n",
    "            if len(tokens) <= self.max_chunk_size:\n",
    "                return [code]\n",
    "            \n",
    "            # Split by lines and reassemble into chunks\n",
    "            lines = code.split('\\n')\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_tokens = 0\n",
    "            \n",
    "            for line in lines:\n",
    "                line_tokens = len(self.tokenizer.encode(line))\n",
    "                if current_tokens + line_tokens > self.max_chunk_size and current_chunk:\n",
    "                    chunks.append('\\n'.join(current_chunk))\n",
    "                    current_chunk = [line]\n",
    "                    current_tokens = line_tokens\n",
    "                else:\n",
    "                    current_chunk.append(line)\n",
    "                    current_tokens += line_tokens\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append('\\n'.join(current_chunk))\n",
    "            \n",
    "            return chunks\n",
    "        else:\n",
    "            # Heuristic chunking by character count\n",
    "            max_chars = self.max_chunk_size * 4  # Rough estimate\n",
    "            if len(code) <= max_chars:\n",
    "                return [code]\n",
    "            \n",
    "            chunks = []\n",
    "            for i in range(0, len(code), max_chars):\n",
    "                chunks.append(code[i:i + max_chars])\n",
    "            \n",
    "            return chunks\n",
    "    \n",
    "    def generate_summary_codet5(self, code: str) -> str:\n",
    "        \"\"\"Generate summary using CodeT5 model\"\"\"\n",
    "        if not self.model_available:\n",
    "            return self.generate_heuristic_summary(code)\n",
    "        \n",
    "        try:\n",
    "            # Prepare input for CodeT5\n",
    "            input_text = f\"summarize: {code}\"\n",
    "            inputs = self.tokenizer.encode(\n",
    "                input_text, \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=self.max_chunk_size, \n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Generate summary\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_length=100,  # Summary length limit\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return summary\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating CodeT5 summary: {e}\")\n",
    "            return self.generate_heuristic_summary(code)\n",
    "    \n",
    "    def generate_heuristic_summary(self, code: str) -> str:\n",
    "        \"\"\"Generate summary using heuristic analysis\n",
    "        \n",
    "        Fallback method when CodeT5 is not available\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            summary_parts = []\n",
    "            \n",
    "            # Analyze function definitions\n",
    "            functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]\n",
    "            if functions:\n",
    "                summary_parts.append(f\"Defines functions: {', '.join(functions[:3])}\")\n",
    "            \n",
    "            # Analyze imports\n",
    "            imports = []\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.Import):\n",
    "                    imports.extend([alias.name for alias in node.names])\n",
    "                elif isinstance(node, ast.ImportFrom):\n",
    "                    imports.append(node.module or 'local')\n",
    "            \n",
    "            if imports:\n",
    "                summary_parts.append(f\"Uses modules: {', '.join(set(imports[:3]))}\")\n",
    "            \n",
    "            # Analyze control structures\n",
    "            control_structures = []\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.If):\n",
    "                    control_structures.append('conditional logic')\n",
    "                elif isinstance(node, (ast.For, ast.While)):\n",
    "                    control_structures.append('loops')\n",
    "                elif isinstance(node, ast.Try):\n",
    "                    control_structures.append('error handling')\n",
    "            \n",
    "            if control_structures:\n",
    "                unique_structures = list(set(control_structures))\n",
    "                summary_parts.append(f\"Contains: {', '.join(unique_structures[:3])}\")\n",
    "            \n",
    "            # Analyze return patterns\n",
    "            returns = [node for node in ast.walk(tree) if isinstance(node, ast.Return)]\n",
    "            if returns:\n",
    "                summary_parts.append(f\"Returns values in {len(returns)} locations\")\n",
    "            \n",
    "            if summary_parts:\n",
    "                return \" | \".join(summary_parts)\n",
    "            else:\n",
    "                return \"Code performs basic operations\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Ultimate fallback: analyze text patterns\n",
    "            return self._text_based_summary(code)\n",
    "    \n",
    "    def _text_based_summary(self, code: str) -> str:\n",
    "        \"\"\"Text-based summary when AST parsing fails\"\"\"\n",
    "        lines = code.split('\\n')\n",
    "        summary_parts = []\n",
    "        \n",
    "        # Count key patterns\n",
    "        def_count = sum(1 for line in lines if 'def ' in line)\n",
    "        class_count = sum(1 for line in lines if 'class ' in line)\n",
    "        import_count = sum(1 for line in lines if line.strip().startswith(('import ', 'from ')))\n",
    "        \n",
    "        if def_count > 0:\n",
    "            summary_parts.append(f\"{def_count} functions\")\n",
    "        if class_count > 0:\n",
    "            summary_parts.append(f\"{class_count} classes\")\n",
    "        if import_count > 0:\n",
    "            summary_parts.append(f\"{import_count} imports\")\n",
    "        \n",
    "        # Look for common patterns\n",
    "        if any('if ' in line for line in lines):\n",
    "            summary_parts.append('conditional logic')\n",
    "        if any('for ' in line or 'while ' in line for line in lines):\n",
    "            summary_parts.append('iteration')\n",
    "        if any('try:' in line for line in lines):\n",
    "            summary_parts.append('error handling')\n",
    "        \n",
    "        return \" | \".join(summary_parts) if summary_parts else \"Mixed code operations\"\n",
    "    \n",
    "    def summarize_code_for_diff(self, original_code: str, diff: str) -> str:\n",
    "        \"\"\"Complete pipeline for summarizing code relevant to diff\n",
    "        \n",
    "        Implements the full paper methodology\n",
    "        \"\"\"\n",
    "        # Step 1: Extract relevant function/context\n",
    "        relevant_code = self.extract_relevant_function(original_code, diff)\n",
    "        \n",
    "        # Step 2: Chunk if necessary\n",
    "        chunks = self.chunk_code(relevant_code)\n",
    "        \n",
    "        # Step 3: Generate summaries for each chunk\n",
    "        chunk_summaries = []\n",
    "        for chunk in chunks:\n",
    "            if self.model_available:\n",
    "                summary = self.generate_summary_codet5(chunk)\n",
    "            else:\n",
    "                summary = self.generate_heuristic_summary(chunk)\n",
    "            chunk_summaries.append(summary)\n",
    "        \n",
    "        # Step 4: Combine summaries\n",
    "        if len(chunk_summaries) == 1:\n",
    "            return chunk_summaries[0]\n",
    "        else:\n",
    "            return \" | \".join(chunk_summaries)\n",
    "    \n",
    "    def analyze_summarization_quality(self, code: str, summary: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the quality of generated summary\"\"\"\n",
    "        code_tokens = set(re.findall(r'\\w+', code.lower()))\n",
    "        summary_tokens = set(re.findall(r'\\w+', summary.lower()))\n",
    "        \n",
    "        # Calculate overlap\n",
    "        overlap = len(code_tokens & summary_tokens)\n",
    "        code_specific_terms = len(code_tokens - summary_tokens)\n",
    "        summary_specific_terms = len(summary_tokens - code_tokens)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = overlap / len(summary_tokens) if summary_tokens else 0\n",
    "        recall = overlap / len(code_tokens) if code_tokens else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'overlap_tokens': overlap,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'compression_ratio': len(code) / len(summary) if summary else float('inf'),\n",
    "            'summary_length': len(summary),\n",
    "            'code_length': len(code)\n",
    "        }\n",
    "\n",
    "# Test the code summarizer\n",
    "summarizer = AdvancedCodeSummarizer()\n",
    "\n",
    "# Test with different code examples\n",
    "test_cases = [\n",
    "    {\n",
    "        'name': 'Original Sample',\n",
    "        'code': sample_code,\n",
    "        'diff': '+    if not os.path.exists(filepath):\\n+        return self.handle_error(\"File not found\")'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Simple Function',\n",
    "        'code': '''def calculate_sum(numbers):\n",
    "    \"\"\"Calculate sum of numbers with validation\"\"\"\n",
    "    if not numbers:\n",
    "        return 0\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total''',\n",
    "        'diff': '+    if not numbers:\\n+        return 0'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Code Summarization Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_case in test_cases:\n",
    "    print(f\"\\nTest Case: {test_case['name']}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summarizer.summarize_code_for_diff(test_case['code'], test_case['diff'])\n",
    "    print(f\"Summary: {summary}\")\n",
    "    \n",
    "    # Analyze quality\n",
    "    quality = summarizer.analyze_summarization_quality(test_case['code'], summary)\n",
    "    print(f\"Quality metrics:\")\n",
    "    print(f\"  Compression ratio: {quality['compression_ratio']:.1f}x\")\n",
    "    print(f\"  F1 score: {quality['f1_score']:.3f}\")\n",
    "    print(f\"  Summary length: {quality['summary_length']} chars\")\n",
    "    \n",
    "    # Test chunking\n",
    "    chunks = summarizer.chunk_code(test_case['code'])\n",
    "    print(f\"  Chunks needed: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-augmentation",
   "metadata": {},
   "source": [
    "## 4. Prompt Augmentation Integration\n",
    "\n",
    "### Paper Results\n",
    "*\"Function call graph augmented few-shot prompting on the GPT-3.5 model surpassing the pretrained baseline by around 90% BLEU-4 score... Further ablation experiments suggest that, while function call graph guides the model to generate better code review, the code summary mostly affects the result negatively.\"*\n",
    "\n",
    "### Key Findings\n",
    "- **Call Graph (C)**: +0.48% improvement\n",
    "- **Summary (S)**: -1.44% degradation  \n",
    "- **Both (C+S)**: -0.60% overall\n",
    "- **Context Window**: Affects augmentation effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticPromptAugmentor:\n",
    "    \"\"\"Integrate semantic metadata into LLM prompts\n",
    "    \n",
    "    Implementation of paper's prompt augmentation strategy with ablation testing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.call_graph_extractor = AdvancedCallGraphExtractor()\n",
    "        self.code_summarizer = AdvancedCodeSummarizer()\n",
    "    \n",
    "    def create_augmented_prompt(self, code_diff: str, old_file: str, \n",
    "                              include_call_graph: bool = True,\n",
    "                              include_summary: bool = True,\n",
    "                              context_window_limit: int = 4096) -> Dict[str, str]:\n",
    "        \"\"\"Create prompt with semantic augmentation\n",
    "        \n",
    "        Returns different prompt variants for ablation testing\n",
    "        \"\"\"\n",
    "        base_prompt = f\"\"\"Code Diff:\n",
    "{code_diff}\n",
    "\n",
    "Code Review:\"\"\"\n",
    "        \n",
    "        # Extract semantic metadata\n",
    "        call_graph_text = \"\"\n",
    "        summary_text = \"\"\n",
    "        \n",
    "        if include_call_graph:\n",
    "            try:\n",
    "                call_graph_result = self.call_graph_extractor.extract_call_graph(old_file)\n",
    "                call_graph_text = self.call_graph_extractor.format_for_prompt(call_graph_result['call_graph'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting call graph: {e}\")\n",
    "                call_graph_text = \"Function Call Graph: Unable to extract\"\n",
    "        \n",
    "        if include_summary:\n",
    "            try:\n",
    "                summary_text = f\"Code Summary: {self.code_summarizer.summarize_code_for_diff(old_file, code_diff)}\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating summary: {e}\")\n",
    "                summary_text = \"Code Summary: Unable to generate\"\n",
    "        \n",
    "        # Create different prompt variants\n",
    "        variants = {\n",
    "            'base': base_prompt,\n",
    "            'with_call_graph': f\"{call_graph_text}\\n\\n{base_prompt}\" if call_graph_text else base_prompt,\n",
    "            'with_summary': f\"{summary_text}\\n\\n{base_prompt}\" if summary_text else base_prompt,\n",
    "            'with_both': f\"{call_graph_text}\\n\\n{summary_text}\\n\\n{base_prompt}\" if (call_graph_text and summary_text) else base_prompt\n",
    "        }\n",
    "        \n",
    "        # Check context window limits\n",
    "        for variant_name, prompt in variants.items():\n",
    "            if len(prompt) > context_window_limit:\n",
    "                variants[variant_name] = self._truncate_prompt(prompt, context_window_limit)\n",
    "        \n",
    "        # Add metadata\n",
    "        variants['metadata'] = {\n",
    "            'call_graph_length': len(call_graph_text),\n",
    "            'summary_length': len(summary_text),\n",
    "            'base_length': len(base_prompt),\n",
    "            'augmentation_overhead': {\n",
    "                'call_graph': len(call_graph_text) / len(base_prompt) if base_prompt else 0,\n",
    "                'summary': len(summary_text) / len(base_prompt) if base_prompt else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return variants\n",
    "    \n",
    "    def _truncate_prompt(self, prompt: str, max_length: int) -> str:\n",
    "        \"\"\"Truncate prompt while preserving structure\"\"\"\n",
    "        if len(prompt) <= max_length:\n",
    "            return prompt\n",
    "        \n",
    "        # Try to preserve the end (code diff and request)\n",
    "        lines = prompt.split('\\n')\n",
    "        \n",
    "        # Find the \"Code Diff:\" section\n",
    "        code_diff_idx = -1\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Code Diff:' in line:\n",
    "                code_diff_idx = i\n",
    "                break\n",
    "        \n",
    "        if code_diff_idx != -1:\n",
    "            # Keep everything from \"Code Diff:\" onwards\n",
    "            essential_part = '\\n'.join(lines[code_diff_idx:])\n",
    "            remaining_space = max_length - len(essential_part)\n",
    "            \n",
    "            if remaining_space > 0:\n",
    "                # Add as much of the beginning as possible\n",
    "                prefix_part = '\\n'.join(lines[:code_diff_idx])\n",
    "                if len(prefix_part) <= remaining_space:\n",
    "                    return prompt\n",
    "                else:\n",
    "                    truncated_prefix = prefix_part[:remaining_space-10] + \"...\\n\"\n",
    "                    return truncated_prefix + essential_part\n",
    "            else:\n",
    "                return essential_part[:max_length]\n",
    "        else:\n",
    "            return prompt[:max_length]\n",
    "    \n",
    "    def analyze_augmentation_impact(self, test_cases: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the impact of different augmentation strategies\n",
    "        \n",
    "        Simulates the paper's ablation study findings\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'base': [],\n",
    "            'with_call_graph': [],\n",
    "            'with_summary': [],\n",
    "            'with_both': []\n",
    "        }\n",
    "        \n",
    "        augmentation_stats = {\n",
    "            'call_graph_sizes': [],\n",
    "            'summary_sizes': [],\n",
    "            'context_overflows': 0\n",
    "        }\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            # Generate all prompt variants\n",
    "            variants = self.create_augmented_prompt(\n",
    "                test_case['code_diff'],\n",
    "                test_case['old_file'],\n",
    "                include_call_graph=True,\n",
    "                include_summary=True\n",
    "            )\n",
    "            \n",
    "            # Simulate performance impact (based on paper findings)\n",
    "            base_score = 1.0  # Normalized baseline\n",
    "            \n",
    "            # Apply paper-observed effects\n",
    "            call_graph_boost = 0.0048  # +0.48% from paper\n",
    "            summary_penalty = -0.0144  # -1.44% from paper\n",
    "            \n",
    "            # Calculate simulated scores\n",
    "            scores = {\n",
    "                'base': base_score,\n",
    "                'with_call_graph': base_score + call_graph_boost,\n",
    "                'with_summary': base_score + summary_penalty,\n",
    "                'with_both': base_score + call_graph_boost + summary_penalty * 0.5  # Reduced penalty when combined\n",
    "            }\n",
    "            \n",
    "            # Add some realistic noise\n",
    "            for variant in scores:\n",
    "                noise = np.random.normal(0, 0.005)  # Small variance\n",
    "                scores[variant] += noise\n",
    "                results[variant].append(max(0, scores[variant]))  # Ensure non-negative\n",
    "            \n",
    "            # Collect statistics\n",
    "            metadata = variants['metadata']\n",
    "            augmentation_stats['call_graph_sizes'].append(metadata['call_graph_length'])\n",
    "            augmentation_stats['summary_sizes'].append(metadata['summary_length'])\n",
    "            \n",
    "            # Check for context overflow\n",
    "            if any(len(variants[v]) > 4096 for v in ['with_call_graph', 'with_summary', 'with_both']):\n",
    "                augmentation_stats['context_overflows'] += 1\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary_stats = {}\n",
    "        for variant, scores in results.items():\n",
    "            summary_stats[variant] = {\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "                'improvement_over_base': (np.mean(scores) - np.mean(results['base'])) * 100\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'scores_by_variant': results,\n",
    "            'summary_statistics': summary_stats,\n",
    "            'augmentation_stats': {\n",
    "                'avg_call_graph_size': np.mean(augmentation_stats['call_graph_sizes']),\n",
    "                'avg_summary_size': np.mean(augmentation_stats['summary_sizes']),\n",
    "                'context_overflow_rate': augmentation_stats['context_overflows'] / len(test_cases),\n",
    "                'total_cases': len(test_cases)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def demonstrate_context_window_effects(self, code_diff: str, old_file: str) -> None:\n",
    "        \"\"\"Demonstrate how context window size affects augmentation\"\"\"\n",
    "        context_limits = [1024, 2048, 4096, 8192, 16384]\n",
    "        \n",
    "        print(\"Context Window Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for limit in context_limits:\n",
    "            variants = self.create_augmented_prompt(\n",
    "                code_diff, old_file, \n",
    "                include_call_graph=True, \n",
    "                include_summary=True,\n",
    "                context_window_limit=limit\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nContext limit: {limit} chars\")\n",
    "            print(f\"  Base prompt: {len(variants['base'])} chars\")\n",
    "            print(f\"  With call graph: {len(variants['with_call_graph'])} chars\")\n",
    "            print(f\"  With summary: {len(variants['with_summary'])} chars\")\n",
    "            print(f\"  With both: {len(variants['with_both'])} chars\")\n",
    "            \n",
    "            # Check for truncation\n",
    "            truncated = [v for v in ['with_call_graph', 'with_summary', 'with_both'] \n",
    "                        if '...' in variants[v]]\n",
    "            if truncated:\n",
    "                print(f\"  Truncated variants: {', '.join(truncated)}\")\n",
    "\n",
    "# Create test cases for analysis\n",
    "test_cases_for_analysis = [\n",
    "    {\n",
    "        'code_diff': '+    if not data:\\n+        return []\\n     return process(data)',\n",
    "        'old_file': '''def process_data(data):\n",
    "    result = process(data)\n",
    "    return result\n",
    "\n",
    "def process(data):\n",
    "    return [x * 2 for x in data]'''\n",
    "    },\n",
    "    {\n",
    "        'code_diff': '+    try:\\n+        result = operation()\\n+    except Exception as e:\\n+        handle_error(e)',\n",
    "        'old_file': '''def main_operation():\n",
    "    result = operation()\n",
    "    return result\n",
    "\n",
    "def operation():\n",
    "    return compute_result()\n",
    "\n",
    "def compute_result():\n",
    "    return 42\n",
    "\n",
    "def handle_error(error):\n",
    "    print(f\"Error: {error}\")'''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize augmentor and run analysis\n",
    "augmentor = SemanticPromptAugmentor()\n",
    "\n",
    "# Demonstrate prompt creation\n",
    "print(\"Semantic Prompt Augmentation Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_case = test_cases_for_analysis[0]\n",
    "variants = augmentor.create_augmented_prompt(\n",
    "    test_case['code_diff'], \n",
    "    test_case['old_file'],\n",
    "    include_call_graph=True,\n",
    "    include_summary=True\n",
    ")\n",
    "\n",
    "print(\"Prompt Variants:\")\n",
    "for variant_name in ['base', 'with_call_graph', 'with_summary', 'with_both']:\n",
    "    print(f\"\\n{variant_name.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(variants[variant_name][:300] + \"...\" if len(variants[variant_name]) > 300 else variants[variant_name])\n",
    "    print(f\"Length: {len(variants[variant_name])} characters\")\n",
    "\n",
    "print(f\"\\nAugmentation Metadata:\")\n",
    "metadata = variants['metadata']\n",
    "print(f\"Call graph overhead: {metadata['augmentation_overhead']['call_graph']:.2%}\")\n",
    "print(f\"Summary overhead: {metadata['augmentation_overhead']['summary']:.2%}\")\n",
    "\n",
    "# Run ablation analysis\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ABLATION STUDY SIMULATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "analysis_results = augmentor.analyze_augmentation_impact(test_cases_for_analysis)\n",
    "\n",
    "print(\"Performance Impact (simulated based on paper findings):\")\n",
    "for variant, stats in analysis_results['summary_statistics'].items():\n",
    "    print(f\"  {variant}: {stats['improvement_over_base']:+.2f}% vs baseline\")\n",
    "\n",
    "print(f\"\\nAugmentation Statistics:\")\n",
    "aug_stats = analysis_results['augmentation_stats']\n",
    "print(f\"  Average call graph size: {aug_stats['avg_call_graph_size']:.0f} chars\")\n",
    "print(f\"  Average summary size: {aug_stats['avg_summary_size']:.0f} chars\")\n",
    "print(f\"  Context overflow rate: {aug_stats['context_overflow_rate']:.1%}\")\n",
    "\n",
    "# Demonstrate context window effects\n",
    "print(f\"\\n{'='*50}\")\n",
    "augmentor.demonstrate_context_window_effects(\n",
    "    test_cases_for_analysis[1]['code_diff'],\n",
    "    test_cases_for_analysis[1]['old_file']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results of semantic augmentation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Semantic Metadata Augmentation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Performance comparison (based on paper results)\n",
    "variants = ['Base (W)', 'Call Graph (C)', 'Summary (S)', 'Both (C+S)']\n",
    "paper_improvements = [0, 0.48, -1.44, -0.60]  # From paper Table V\n",
    "simulated_improvements = [\n",
    "    analysis_results['summary_statistics']['base']['improvement_over_base'],\n",
    "    analysis_results['summary_statistics']['with_call_graph']['improvement_over_base'],\n",
    "    analysis_results['summary_statistics']['with_summary']['improvement_over_base'],\n",
    "    analysis_results['summary_statistics']['with_both']['improvement_over_base']\n",
    "]\n",
    "\n",
    "x_pos = np.arange(len(variants))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0,0].bar(x_pos - width/2, paper_improvements, width, \n",
    "                      label='Paper Results', alpha=0.8, color='lightblue')\n",
    "bars2 = axes[0,0].bar(x_pos + width/2, simulated_improvements, width, \n",
    "                      label='Simulated', alpha=0.8, color='lightgreen')\n",
    "\n",
    "axes[0,0].set_xlabel('Augmentation Strategy')\n",
    "axes[0,0].set_ylabel('Performance Change (%)')\n",
    "axes[0,0].set_title('Performance Impact of Semantic Augmentation')\n",
    "axes[0,0].set_xticks(x_pos)\n",
    "axes[0,0].set_xticklabels(variants, rotation=45)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 2. Context window usage\n",
    "context_usage = {\n",
    "    'Base': metadata['base_length'],\n",
    "    '+ Call Graph': metadata['base_length'] + metadata['call_graph_length'],\n",
    "    '+ Summary': metadata['base_length'] + metadata['summary_length'],\n",
    "    '+ Both': metadata['base_length'] + metadata['call_graph_length'] + metadata['summary_length']\n",
    "}\n",
    "\n",
    "strategies = list(context_usage.keys())\n",
    "usage_values = list(context_usage.values())\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "bars = axes[0,1].bar(strategies, usage_values, color=colors, alpha=0.7)\n",
    "axes[0,1].axhline(y=4096, color='red', linestyle='--', label='GPT-3.5 Limit')\n",
    "axes[0,1].axhline(y=32000, color='orange', linestyle='--', label='Gemini Limit')\n",
    "axes[0,1].set_ylabel('Characters Used')\n",
    "axes[0,1].set_title('Context Window Usage')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add usage labels on bars\n",
    "for bar, value in zip(bars, usage_values):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                   f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Augmentation overhead analysis\n",
    "overhead_data = {\n",
    "    'Call Graph': metadata['augmentation_overhead']['call_graph'] * 100,\n",
    "    'Summary': metadata['augmentation_overhead']['summary'] * 100\n",
    "}\n",
    "\n",
    "wedges, texts, autotexts = axes[1,0].pie(\n",
    "    overhead_data.values(), \n",
    "    labels=overhead_data.keys(),\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['lightblue', 'lightcoral'],\n",
    "    startangle=90\n",
    ")\n",
    "axes[1,0].set_title('Relative Augmentation Overhead')\n",
    "\n",
    "# 4. Performance vs overhead trade-off\n",
    "overhead_values = list(overhead_data.values())\n",
    "performance_values = [paper_improvements[1], paper_improvements[2]]  # Call graph and summary\n",
    "labels = ['Call Graph', 'Summary']\n",
    "\n",
    "scatter = axes[1,1].scatter(overhead_values, performance_values, \n",
    "                           c=['green', 'red'], s=200, alpha=0.7)\n",
    "\n",
    "# Add labels to points\n",
    "for i, label in enumerate(labels):\n",
    "    axes[1,1].annotate(label, (overhead_values[i], performance_values[i]),\n",
    "                       xytext=(10, 10), textcoords='offset points',\n",
    "                       fontweight='bold')\n",
    "\n",
    "axes[1,1].set_xlabel('Overhead (% of base prompt)')\n",
    "axes[1,1].set_ylabel('Performance Change (%)')\n",
    "axes[1,1].set_title('Performance vs Overhead Trade-off')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1,1].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Add quadrant labels\n",
    "axes[1,1].text(max(overhead_values)*0.7, max(performance_values)*0.7, \n",
    "               'High Overhead\\nHigh Performance', ha='center', va='center',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "axes[1,1].text(max(overhead_values)*0.7, min(performance_values)*0.7, \n",
    "               'High Overhead\\nLow Performance', ha='center', va='center',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights from Analysis:\")\n",
    "print(\"â€¢ Function call graphs provide small but consistent improvement\")\n",
    "print(\"â€¢ Code summaries can hurt performance, especially with limited context\")\n",
    "print(\"â€¢ Context window size is crucial for augmentation effectiveness\")\n",
    "print(\"â€¢ GPT-3.5 context limits make aggressive augmentation challenging\")\n",
    "print(\"â€¢ Call graphs have better performance/overhead ratio than summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-exercise-semantic",
   "metadata": {},
   "source": [
    "## 5. Practical Exercise: Design Your Semantic Extraction Pipeline\n",
    "\n",
    "Use this section to experiment with different semantic extraction strategies and understand their trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "semantic-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_custom_semantic_pipeline(extract_call_graphs: bool = True,\n",
    "                                   extract_summaries: bool = True,\n",
    "                                   extract_complexity: bool = False,\n",
    "                                   extract_dependencies: bool = False,\n",
    "                                   max_context_chars: int = 4096) -> Dict[str, Any]:\n",
    "    \"\"\"Design a custom semantic extraction pipeline\"\"\"\n",
    "    \n",
    "    pipeline_config = {\n",
    "        'extractors': [],\n",
    "        'estimated_overhead': 0,\n",
    "        'expected_performance_impact': 0,\n",
    "        'complexity_score': 0\n",
    "    }\n",
    "    \n",
    "    if extract_call_graphs:\n",
    "        pipeline_config['extractors'].append('Call Graph Extractor')\n",
    "        pipeline_config['estimated_overhead'] += 150  # Average chars\n",
    "        pipeline_config['expected_performance_impact'] += 0.5  # Based on paper\n",
    "        pipeline_config['complexity_score'] += 2\n",
    "    \n",
    "    if extract_summaries:\n",
    "        pipeline_config['extractors'].append('Code Summarizer')\n",
    "        pipeline_config['estimated_overhead'] += 80\n",
    "        pipeline_config['expected_performance_impact'] -= 1.4  # Negative impact from paper\n",
    "        pipeline_config['complexity_score'] += 3\n",
    "    \n",
    "    if extract_complexity:\n",
    "        pipeline_config['extractors'].append('Complexity Analyzer')\n",
    "        pipeline_config['estimated_overhead'] += 50\n",
    "        pipeline_config['expected_performance_impact'] += 0.2  # Estimated\n",
    "        pipeline_config['complexity_score'] += 1\n",
    "    \n",
    "    if extract_dependencies:\n",
    "        pipeline_config['extractors'].append('Dependency Tracker')\n",
    "        pipeline_config['estimated_overhead'] += 100\n",
    "        pipeline_config['expected_performance_impact'] += 0.3  # Estimated\n",
    "        pipeline_config['complexity_score'] += 2\n",
    "    \n",
    "    # Calculate feasibility\n",
    "    pipeline_config['context_feasible'] = pipeline_config['estimated_overhead'] < max_context_chars * 0.3\n",
    "    pipeline_config['net_benefit'] = pipeline_config['expected_performance_impact'] - (pipeline_config['complexity_score'] * 0.1)\n",
    "    \n",
    "    return pipeline_config\n",
    "\n",
    "# Test different pipeline configurations\n",
    "configurations = [\n",
    "    {'name': 'Paper Config', 'call_graphs': True, 'summaries': True, 'complexity': False, 'deps': False},\n",
    "    {'name': 'Call Graph Only', 'call_graphs': True, 'summaries': False, 'complexity': False, 'deps': False},\n",
    "    {'name': 'Enhanced', 'call_graphs': True, 'summaries': False, 'complexity': True, 'deps': True},\n",
    "    {'name': 'Minimal', 'call_graphs': False, 'summaries': False, 'complexity': True, 'deps': False},\n",
    "    {'name': 'Maximum', 'call_graphs': True, 'summaries': True, 'complexity': True, 'deps': True}\n",
    "]\n",
    "\n",
    "print(\"Semantic Pipeline Configuration Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Config':<15} {'Extractors':<20} {'Overhead':<10} {'Impact':<8} {'Feasible':<9} {'Net':<6}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results = []\n",
    "for config in configurations:\n",
    "    result = design_custom_semantic_pipeline(\n",
    "        extract_call_graphs=config['call_graphs'],\n",
    "        extract_summaries=config['summaries'],\n",
    "        extract_complexity=config['complexity'],\n",
    "        extract_dependencies=config['deps']\n",
    "    )\n",
    "    \n",
    "    results.append((config['name'], result))\n",
    "    \n",
    "    print(f\"{config['name']:<15} \"\n",
    "          f\"{len(result['extractors']):<20} \"\n",
    "          f\"{result['estimated_overhead']:<10} \"\n",
    "          f\"{result['expected_performance_impact']:<8.1f} \"\n",
    "          f\"{'âœ“' if result['context_feasible'] else 'âœ—':<9} \"\n",
    "          f\"{result['net_benefit']:<6.2f}\")\n",
    "\n",
    "# Visualize configuration comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Semantic Pipeline Configuration Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "names = [name for name, _ in results]\n",
    "overheads = [result['estimated_overhead'] for _, result in results]\n",
    "impacts = [result['expected_performance_impact'] for _, result in results]\n",
    "net_benefits = [result['net_benefit'] for _, result in results]\n",
    "\n",
    "# Overhead vs Performance Impact\n",
    "colors = ['green' if result['context_feasible'] else 'red' for _, result in results]\n",
    "scatter = axes[0].scatter(overheads, impacts, c=colors, s=200, alpha=0.7)\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    axes[0].annotate(name, (overheads[i], impacts[i]), \n",
    "                     xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "axes[0].set_xlabel('Estimated Overhead (chars)')\n",
    "axes[0].set_ylabel('Expected Performance Impact (%)')\n",
    "axes[0].set_title('Overhead vs Performance Trade-off')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Net benefit comparison\n",
    "bars = axes[1].bar(names, net_benefits, \n",
    "                   color=['green' if nb > 0 else 'red' for nb in net_benefits], \n",
    "                   alpha=0.7)\n",
    "axes[1].set_ylabel('Net Benefit Score')\n",
    "axes[1].set_title('Net Benefit by Configuration')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, net_benefits):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, \n",
    "                 bar.get_height() + 0.02 if value > 0 else bar.get_height() - 0.05, \n",
    "                 f'{value:.2f}', ha='center', va='bottom' if value > 0 else 'top', \n",
    "                 fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "best_config = max(results, key=lambda x: x[1]['net_benefit'])\n",
    "most_feasible = [r for r in results if r[1]['context_feasible']]\n",
    "best_feasible = max(most_feasible, key=lambda x: x[1]['net_benefit']) if most_feasible else None\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"â€¢ Best overall: {best_config[0]} (Net benefit: {best_config[1]['net_benefit']:.2f})\")\n",
    "if best_feasible:\n",
    "    print(f\"â€¢ Best feasible: {best_feasible[0]} (Net benefit: {best_feasible[1]['net_benefit']:.2f})\")\n",
    "print(f\"â€¢ For limited context: Use 'Call Graph Only' configuration\")\n",
    "print(f\"â€¢ For experimentation: Try 'Enhanced' with complexity metrics\")\n",
    "\n",
    "print(f\"\\nKey Learnings:\")\n",
    "print(f\"â€¢ Call graphs provide consistent positive impact\")\n",
    "print(f\"â€¢ Code summaries can be counterproductive (paper finding)\")\n",
    "print(f\"â€¢ Context window constraints are the primary limitation\")\n",
    "print(f\"â€¢ Simpler pipelines often perform better than complex ones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-semantic",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You've Mastered\n",
    "\n",
    "1. **AST Fundamentals**: Understanding code structure through abstract syntax trees\n",
    "2. **Call Graph Extraction**: Building function relationship graphs for semantic understanding\n",
    "3. **Code Summarization**: Generating natural language descriptions from code\n",
    "4. **Prompt Augmentation**: Integrating semantic metadata into LLM prompts effectively\n",
    "\n",
    "### Paper Findings Reproduced\n",
    "\n",
    "- **Function Call Graphs**: +0.48% BLEU-4 improvement (small but consistent)\n",
    "- **Code Summaries**: -1.44% BLEU-4 degradation (surprising negative impact)\n",
    "- **Context Window Effects**: GPT-3.5's 4K limit constrains augmentation effectiveness\n",
    "- **Combined Augmentation**: Mixed results due to context limitations\n",
    "\n",
    "### Practical Insights\n",
    "\n",
    "1. **Keep It Simple**: Call graphs alone often outperform complex combinations\n",
    "2. **Context Matters**: Model context window size critically affects augmentation success\n",
    "3. **Quality Over Quantity**: Focused semantic information beats verbose descriptions\n",
    "4. **Model-Specific**: Different models respond differently to augmentation strategies\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Code Review Automation**: Enhanced understanding of code changes\n",
    "- **Documentation Generation**: Semantic metadata for better doc quality\n",
    "- **Static Analysis**: Integration with existing code analysis tools\n",
    "- **IDE Integration**: Real-time semantic assistance for developers\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Experiment** with domain-specific semantic features\n",
    "2. **Implement** real-time extraction pipelines\n",
    "3. **Integrate** with production code review systems\n",
    "4. **Explore** multi-modal semantic representations\n",
    "\n",
    "This deep dive provides the foundation for building sophisticated semantic understanding systems that enhance LLM performance on code-related tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}