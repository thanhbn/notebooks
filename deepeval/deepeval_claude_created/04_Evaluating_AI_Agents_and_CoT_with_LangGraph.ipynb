{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 DeepEval - Evaluating AI Agents and Chain-of-Thought with LangGraph\n",
    "\n",
    "Chào mừng đến với **Notebook 4** trong series DeepEval framework!\n",
    "\n",
    "## 🎯 Mục tiêu của Notebook này\n",
    "\n",
    "1. **Xây dựng LangGraph Agents**: Multi-tool agents với complex reasoning\n",
    "2. **Chain-of-Thought Evaluation**: Đánh giá quá trình suy luận từng bước\n",
    "3. **Agent-Specific Metrics**: LogicalFlow, ToolUsage, PlanExecution, Adaptability\n",
    "4. **Multi-Step Analysis**: Capture và evaluate intermediate reasoning steps\n",
    "5. **Comprehensive Agent Benchmarking**: End-to-end agent performance assessment\n",
    "\n",
    "## 📖 Tại sao Agent Evaluation phức tạp?\n",
    "\n",
    "AI Agents với Chain-of-Thought reasoning đặt ra những thách thức evaluation hoàn toàn mới:\n",
    "\n",
    "### 🔍 Unique Challenges:\n",
    "- **Multi-Step Reasoning**: Phải evaluate cả process lẫn final result\n",
    "- **Tool Usage**: Agents sử dụng multiple tools - cần assess appropriateness\n",
    "- **Dynamic Planning**: Plans change based on intermediate results\n",
    "- **Context Awareness**: Agents maintain state across multiple interactions\n",
    "- **Error Recovery**: How well agents handle failures và adapt\n",
    "\n",
    "### ✅ DeepEval Approach:\n",
    "- **Intermediate Step Capture**: LLMTestCase supports reasoning steps\n",
    "- **Custom G-Eval Metrics**: Specialized cho agent evaluation\n",
    "- **Multi-dimensional Assessment**: Logic, tool usage, planning, execution\n",
    "- **Temporal Analysis**: Track performance over interaction sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Phần 1: Setup và Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple, TypedDict\n",
    "import warnings\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DeepEval imports\n",
    "import deepeval\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Custom tools\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "plt.style.use('default')\n",
    "\n",
    "print(f\"✅ DeepEval version: {deepeval.__version__}\")\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Check API keys\n",
    "api_keys_status = {\n",
    "    \"OpenAI\": \"✅ Configured\" if os.getenv(\"OPENAI_API_KEY\") else \"❌ Missing\",\n",
    "    \"Anthropic\": \"✅ Configured\" if os.getenv(\"ANTHROPIC_API_KEY\") else \"❌ Missing\"\n",
    "}\n",
    "\n",
    "print(\"🔑 API Keys Status:\")\n",
    "for provider, status in api_keys_status.items():\n",
    "    print(f\"  {provider}: {status}\")\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\n⚠️  Cần OPENAI_API_KEY để chạy agent evaluation!\")\n",
    "    print(\"   Tạo file .env với: OPENAI_API_KEY=your_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Phần 2: Xây dựng Custom Tools cho Agents\n",
    "\n",
    "### 2.1 Mathematical Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_math_tools():\n",
    "    \"\"\"\n",
    "    Tạo mathematical tools cho agents\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate(expression: str) -> str:\n",
    "        \"\"\"Calculate mathematical expressions safely\"\"\"\n",
    "        try:\n",
    "            # Safe evaluation - only allow basic math operations\n",
    "            allowed_chars = set('0123456789+-*/().^ ')\n",
    "            if not set(expression.replace(' ', '')) <= allowed_chars:\n",
    "                return \"Error: Invalid characters in expression\"\n",
    "            \n",
    "            # Replace ^ with ** for Python power operator\n",
    "            expression = expression.replace('^', '**')\n",
    "            \n",
    "            result = eval(expression)\n",
    "            return f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def factorial(n: str) -> str:\n",
    "        \"\"\"Calculate factorial of a number\"\"\"\n",
    "        try:\n",
    "            num = int(n)\n",
    "            if num < 0:\n",
    "                return \"Error: Factorial không defined cho số âm\"\n",
    "            if num > 20:\n",
    "                return \"Error: Number quá lớn (max 20)\"\n",
    "            \n",
    "            result = math.factorial(num)\n",
    "            return f\"Factorial of {num} is {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def prime_check(n: str) -> str:\n",
    "        \"\"\"Check if a number is prime\"\"\"\n",
    "        try:\n",
    "            num = int(n)\n",
    "            if num < 2:\n",
    "                return f\"{num} is not prime\"\n",
    "            \n",
    "            for i in range(2, int(num ** 0.5) + 1):\n",
    "                if num % i == 0:\n",
    "                    return f\"{num} is not prime (divisible by {i})\"\n",
    "            \n",
    "            return f\"{num} is prime\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Create LangChain tools\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"calculator\",\n",
    "            description=\"Calculate mathematical expressions. Input: mathematical expression as string (e.g., '2+3*4', '10^2')\",\n",
    "            func=calculate\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"factorial\",\n",
    "            description=\"Calculate factorial of a number. Input: positive integer as string (e.g., '5')\",\n",
    "            func=factorial\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"prime_checker\",\n",
    "            description=\"Check if a number is prime. Input: positive integer as string (e.g., '17')\",\n",
    "            func=prime_check\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return tools\n",
    "\n",
    "# Create math tools\n",
    "math_tools = create_math_tools()\n",
    "print(f\"✅ Created {len(math_tools)} math tools:\")\n",
    "for tool in math_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Information Retrieval Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_info_tools():\n",
    "    \"\"\"\n",
    "    Tạo information retrieval tools\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mock knowledge base\n",
    "    knowledge_base = {\n",
    "        \"python\": \"Python là programming language high-level, interpreted, với dynamic typing và automatic memory management.\",\n",
    "        \"ai\": \"Artificial Intelligence (AI) là simulation của human intelligence trong machines để perform tasks requiring human cognition.\",\n",
    "        \"machine learning\": \"Machine Learning là subset của AI cho phép systems learn và improve từ experience mà không cần explicit programming.\",\n",
    "        \"deep learning\": \"Deep Learning sử dụng neural networks với multiple layers để learn complex patterns trong large amounts of data.\",\n",
    "        \"langchain\": \"LangChain là framework để develop applications powered by language models, với components cho chains, agents, và memory.\",\n",
    "        \"langgraph\": \"LangGraph là library để build stateful, multi-actor applications với LLMs, using graph-based approach.\"\n",
    "    }\n",
    "    \n",
    "    def search_knowledge(query: str) -> str:\n",
    "        \"\"\"Search knowledge base for information\"\"\"\n",
    "        query_lower = query.lower().strip()\n",
    "        \n",
    "        # Exact match\n",
    "        if query_lower in knowledge_base:\n",
    "            return f\"Found: {knowledge_base[query_lower]}\"\n",
    "        \n",
    "        # Partial match\n",
    "        matches = []\n",
    "        for key, value in knowledge_base.items():\n",
    "            if query_lower in key or any(word in key for word in query_lower.split()):\n",
    "                matches.append(f\"{key}: {value}\")\n",
    "        \n",
    "        if matches:\n",
    "            return f\"Partial matches found:\\n\" + \"\\n\".join(matches)\n",
    "        \n",
    "        return f\"No information found for '{query}'. Available topics: {', '.join(knowledge_base.keys())}\"\n",
    "    \n",
    "    def get_current_time() -> str:\n",
    "        \"\"\"Get current time and date\"\"\"\n",
    "        now = datetime.now()\n",
    "        return f\"Current time: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    \n",
    "    def generate_random_number(range_str: str) -> str:\n",
    "        \"\"\"Generate random number in specified range\"\"\"\n",
    "        try:\n",
    "            if '-' in range_str:\n",
    "                min_val, max_val = map(int, range_str.split('-'))\n",
    "            else:\n",
    "                min_val, max_val = 1, int(range_str)\n",
    "            \n",
    "            result = random.randint(min_val, max_val)\n",
    "            return f\"Random number between {min_val} and {max_val}: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}. Format: 'min-max' or just 'max'\"\n",
    "    \n",
    "    # Create tools\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"knowledge_search\",\n",
    "            description=\"Search knowledge base for information about topics. Input: topic name or keyword (e.g., 'python', 'ai', 'machine learning')\",\n",
    "            func=search_knowledge\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"get_time\",\n",
    "            description=\"Get current date and time. No input required.\",\n",
    "            func=lambda x: get_current_time()\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"random_number\",\n",
    "            description=\"Generate random number. Input: range as 'min-max' (e.g., '1-100') or just max value (e.g., '50')\",\n",
    "            func=generate_random_number\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return tools\n",
    "\n",
    "# Create info tools\n",
    "info_tools = create_info_tools()\n",
    "print(f\"✅ Created {len(info_tools)} information tools:\")\n",
    "for tool in info_tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Phần 3: Xây dựng LangGraph Agent\n",
    "\n",
    "### 3.1 Agent State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[dict]\n",
    "    current_task: str\n",
    "    reasoning_steps: List[str]\n",
    "    tool_calls: List[dict]\n",
    "    intermediate_results: List[str]\n",
    "    final_answer: str\n",
    "    execution_status: str  # 'running', 'completed', 'error'\n",
    "    error_message: str\n",
    "\n",
    "def create_initial_state(user_input: str) -> AgentState:\n",
    "    \"\"\"\n",
    "    Create initial agent state\n",
    "    \"\"\"\n",
    "    return AgentState(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "        current_task=user_input,\n",
    "        reasoning_steps=[],\n",
    "        tool_calls=[],\n",
    "        intermediate_results=[],\n",
    "        final_answer=\"\",\n",
    "        execution_status=\"running\",\n",
    "        error_message=\"\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Agent state structure defined\")\n",
    "print(\"State components:\", list(AgentState.__annotations__.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Agent Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_nodes():\n",
    "    \"\"\"\n",
    "    Create LangGraph agent node functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine all tools\n",
    "    all_tools = math_tools + info_tools\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"❌ Cannot create agent nodes without OpenAI API key\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Create LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    def planning_node(state: AgentState) -> AgentState:\n",
    "        \"\"\"\n",
    "        Planning node - analyze task và create execution plan\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task = state[\"current_task\"]\n",
    "            \n",
    "            planning_prompt = f\"\"\"\n",
    "            Phân tích task sau và tạo detailed execution plan:\n",
    "            Task: {task}\n",
    "            \n",
    "            Available tools:\n",
    "            {chr(10).join([f\"- {tool.name}: {tool.description}\" for tool in all_tools])}\n",
    "            \n",
    "            Hãy:\n",
    "            1. Phân tích task requirements\n",
    "            2. Identify cần tools nào\n",
    "            3. Tạo step-by-step plan\n",
    "            4. Explain reasoning cho mỗi step\n",
    "            \n",
    "            Format: Detailed plan với clear reasoning steps.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = llm.invoke([HumanMessage(content=planning_prompt)])\n",
    "            \n",
    "            reasoning_step = f\"PLANNING: {response.content}\"\n",
    "            \n",
    "            state[\"reasoning_steps\"].append(reasoning_step)\n",
    "            state[\"messages\"].append({\"role\": \"assistant\", \"content\": f\"Plan created: {response.content[:200]}...\"})\n",
    "            \n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"execution_status\"] = \"error\"\n",
    "            state[\"error_message\"] = f\"Planning error: {str(e)}\"\n",
    "            return state\n",
    "    \n",
    "    def execution_node(state: AgentState) -> AgentState:\n",
    "        \"\"\"\n",
    "        Execution node - execute tools based on plan\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task = state[\"current_task\"]\n",
    "            \n",
    "            # Simple execution logic - analyze task và decide tools\n",
    "            execution_prompt = f\"\"\"\n",
    "            Execute this task step by step: {task}\n",
    "            \n",
    "            Based on the task, determine which tools to use và in what order.\n",
    "            For each tool call, explain why you're using it.\n",
    "            \n",
    "            Available tools: {[tool.name for tool in all_tools]}\n",
    "            \n",
    "            Provide tool calls in this format:\n",
    "            TOOL_CALL: tool_name(input)\n",
    "            REASONING: why you're using this tool\n",
    "            \"\"\"\n",
    "            \n",
    "            response = llm.invoke([HumanMessage(content=execution_prompt)])\n",
    "            \n",
    "            # Mock tool execution (simplified for demo)\n",
    "            tool_results = []\n",
    "            \n",
    "            # Check for math operations\n",
    "            if any(op in task.lower() for op in ['calculate', 'compute', '+', '-', '*', '/', 'factorial', 'prime']):\n",
    "                if 'factorial' in task.lower():\n",
    "                    # Extract number\n",
    "                    import re\n",
    "                    numbers = re.findall(r'\\d+', task)\n",
    "                    if numbers:\n",
    "                        result = math_tools[1].func(numbers[0])  # factorial tool\n",
    "                        tool_results.append(f\"factorial({numbers[0]}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"factorial\", \"input\": numbers[0], \"output\": result})\n",
    "                \n",
    "                elif 'prime' in task.lower():\n",
    "                    import re\n",
    "                    numbers = re.findall(r'\\d+', task)\n",
    "                    if numbers:\n",
    "                        result = math_tools[2].func(numbers[0])  # prime checker\n",
    "                        tool_results.append(f\"prime_check({numbers[0]}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"prime_checker\", \"input\": numbers[0], \"output\": result})\n",
    "                \n",
    "                else:\n",
    "                    # Extract expression\n",
    "                    import re\n",
    "                    math_expr = re.search(r'[\\d+\\-*/()\\s]+', task)\n",
    "                    if math_expr:\n",
    "                        expr = math_expr.group().strip()\n",
    "                        result = math_tools[0].func(expr)  # calculator\n",
    "                        tool_results.append(f\"calculate({expr}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"calculator\", \"input\": expr, \"output\": result})\n",
    "            \n",
    "            # Check for knowledge queries\n",
    "            elif any(keyword in task.lower() for keyword in ['what is', 'explain', 'about', 'python', 'ai', 'machine learning']):\n",
    "                # Extract topic\n",
    "                for topic in ['python', 'ai', 'machine learning', 'deep learning', 'langchain', 'langgraph']:\n",
    "                    if topic in task.lower():\n",
    "                        result = info_tools[0].func(topic)  # knowledge search\n",
    "                        tool_results.append(f\"knowledge_search({topic}) = {result}\")\n",
    "                        state[\"tool_calls\"].append({\"tool\": \"knowledge_search\", \"input\": topic, \"output\": result})\n",
    "                        break\n",
    "            \n",
    "            # Check for time request\n",
    "            elif 'time' in task.lower() or 'date' in task.lower():\n",
    "                result = info_tools[1].func(\"\")  # get_time\n",
    "                tool_results.append(f\"get_time() = {result}\")\n",
    "                state[\"tool_calls\"].append({\"tool\": \"get_time\", \"input\": \"\", \"output\": result})\n",
    "            \n",
    "            state[\"intermediate_results\"].extend(tool_results)\n",
    "            state[\"reasoning_steps\"].append(f\"EXECUTION: {response.content}\")\n",
    "            \n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"execution_status\"] = \"error\"\n",
    "            state[\"error_message\"] = f\"Execution error: {str(e)}\"\n",
    "            return state\n",
    "    \n",
    "    def synthesis_node(state: AgentState) -> AgentState:\n",
    "        \"\"\"\n",
    "        Synthesis node - combine results và generate final answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            task = state[\"current_task\"]\n",
    "            intermediate_results = state[\"intermediate_results\"]\n",
    "            \n",
    "            synthesis_prompt = f\"\"\"\n",
    "            Synthesize final answer từ tool results:\n",
    "            \n",
    "            Original task: {task}\n",
    "            \n",
    "            Tool results:\n",
    "            {chr(10).join(intermediate_results) if intermediate_results else 'No tool results'}\n",
    "            \n",
    "            Provide a comprehensive, well-structured answer that:\n",
    "            1. Directly addresses the original question\n",
    "            2. Incorporates relevant tool results\n",
    "            3. Explains the reasoning process\n",
    "            4. Provides clear conclusions\n",
    "            \"\"\"\n",
    "            \n",
    "            response = llm.invoke([HumanMessage(content=synthesis_prompt)])\n",
    "            \n",
    "            state[\"final_answer\"] = response.content\n",
    "            state[\"execution_status\"] = \"completed\"\n",
    "            state[\"reasoning_steps\"].append(f\"SYNTHESIS: Created final answer from {len(intermediate_results)} tool results\")\n",
    "            \n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"execution_status\"] = \"error\"\n",
    "            state[\"error_message\"] = f\"Synthesis error: {str(e)}\"\n",
    "            return state\n",
    "    \n",
    "    def should_continue(state: AgentState) -> str:\n",
    "        \"\"\"\n",
    "        Decide whether to continue or end\n",
    "        \"\"\"\n",
    "        if state[\"execution_status\"] == \"error\":\n",
    "            return \"end\"\n",
    "        elif state[\"execution_status\"] == \"completed\":\n",
    "            return \"end\"\n",
    "        elif len(state[\"reasoning_steps\"]) >= 10:  # Max steps\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "    \n",
    "    return planning_node, execution_node, synthesis_node, should_continue\n",
    "\n",
    "# Create agent nodes\n",
    "planning_node, execution_node, synthesis_node, should_continue = create_agent_nodes()\n",
    "\n",
    "if planning_node:\n",
    "    print(\"✅ Agent nodes created successfully\")\n",
    "    print(\"Available nodes: planning, execution, synthesis\")\nelse:\n",
    "    print(\"❌ Failed to create agent nodes - check API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build LangGraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langgraph_agent():\n",
    "    \"\"\"\n",
    "    Create complete LangGraph agent với state management\n",
    "    \"\"\"\n",
    "    \n",
    "    if not planning_node:\n",
    "        print(\"❌ Cannot create LangGraph agent without node functions\")\n",
    "        return None\n",
    "    \n",
    "    # Create the graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"planning\", planning_node)\n",
    "    workflow.add_node(\"execution\", execution_node)\n",
    "    workflow.add_node(\"synthesis\", synthesis_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.set_entry_point(\"planning\")\n",
    "    workflow.add_edge(\"planning\", \"execution\")\n",
    "    workflow.add_edge(\"execution\", \"synthesis\")\n",
    "    workflow.add_edge(\"synthesis\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    agent = workflow.compile()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Create the agent\n",
    "langgraph_agent = create_langgraph_agent()\n",
    "\n",
    "if langgraph_agent:\n",
    "    print(\"✅ LangGraph agent created successfully!\")\n",
    "    print(\"Agent workflow: planning → execution → synthesis → end\")\nelse:\n",
    "    print(\"❌ Failed to create LangGraph agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Test Agent với Sample Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_execution(agent, test_queries: List[str]):\n",
    "    \"\"\"\n",
    "    Test agent với different types of queries\n",
    "    \"\"\"\n",
    "    \n",
    "    if not agent:\n",
    "        print(\"❌ No agent available for testing\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"🧪 Testing LangGraph Agent với Multiple Queries\\n\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"🔍 Test {i}: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Create initial state\n",
    "            initial_state = create_initial_state(query)\n",
    "            \n",
    "            # Execute agent\n",
    "            start_time = time.time()\n",
    "            final_state = agent.invoke(initial_state)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Extract results\n",
    "            result = {\n",
    "                \"query\": query,\n",
    "                \"final_answer\": final_state.get(\"final_answer\", \"No answer generated\"),\n",
    "                \"reasoning_steps\": final_state.get(\"reasoning_steps\", []),\n",
    "                \"tool_calls\": final_state.get(\"tool_calls\", []),\n",
    "                \"intermediate_results\": final_state.get(\"intermediate_results\", []),\n",
    "                \"execution_status\": final_state.get(\"execution_status\", \"unknown\"),\n",
    "                \"execution_time\": execution_time,\n",
    "                \"error_message\": final_state.get(\"error_message\", \"\")\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Print summary\n",
    "            status_emoji = \"✅\" if result[\"execution_status\"] == \"completed\" else \"❌\"\n",
    "            print(f\"  {status_emoji} Status: {result['execution_status']}\")\n",
    "            print(f\"  ⏱️  Execution time: {execution_time:.2f}s\")\n",
    "            print(f\"  🔧 Tools used: {len(result['tool_calls'])}\")\n",
    "            print(f\"  🧠 Reasoning steps: {len(result['reasoning_steps'])}\")\n",
    "            \n",
    "            if result[\"final_answer\"]:\n",
    "                print(f\"  💡 Answer: {result['final_answer'][:150]}...\")\n",
    "            \n",
    "            if result[\"error_message\"]:\n",
    "                print(f\"  ⚠️  Error: {result['error_message']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Execution failed: {e}\")\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"execution_status\": \"failed\",\n",
    "                \"error_message\": str(e)\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the factorial of 8?\",\n",
    "    \"Is 17 a prime number?\",\n",
    "    \"Calculate 15 + 25 * 3\",\n",
    "    \"What is machine learning?\",\n",
    "    \"What time is it now?\"\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "agent_test_results = test_agent_execution(langgraph_agent, test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Phần 4: Agent-Specific Evaluation Metrics\n",
    "\n",
    "### 4.1 LogicalFlowMetric - Đánh giá Chuỗi Suy luận"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logical_flow_metric():\n",
    "    \"\"\"\n",
    "    Tạo G-Eval metric để đánh giá logical flow của agent reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    Bạn sẽ đánh giá logical flow và reasoning quality của AI agent.\n",
    "    \n",
    "    Criteria để đánh giá:\n",
    "    1. LOGICAL COHERENCE (30%):\n",
    "       - Reasoning steps follow logical sequence\n",
    "       - No contradictions between steps\n",
    "       - Clear cause-and-effect relationships\n",
    "    \n",
    "    2. PROBLEM DECOMPOSITION (25%):\n",
    "       - Complex problems broken down appropriately\n",
    "       - Sub-problems identified correctly\n",
    "       - Hierarchical thinking demonstrated\n",
    "    \n",
    "    3. STEP CLARITY (25%):\n",
    "       - Each reasoning step clearly explained\n",
    "       - Purpose of each step evident\n",
    "       - Transitions between steps smooth\n",
    "    \n",
    "    4. COMPLETENESS (20%):\n",
    "       - All necessary steps included\n",
    "       - No critical reasoning gaps\n",
    "       - Thorough analysis of the problem\n",
    "    \n",
    "    Scoring Guide:\n",
    "    - 9-10: Exceptional logical flow, crystal clear reasoning\n",
    "    - 7-8: Good logical structure, minor gaps\n",
    "    - 5-6: Adequate reasoning but some unclear steps\n",
    "    - 3-4: Poor logical flow, significant gaps\n",
    "    - 1-2: Incoherent or illogical reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Analyze the sequence of reasoning steps\",\n",
    "        \"Check for logical consistency and coherence\",\n",
    "        \"Evaluate problem decomposition approach\",\n",
    "        \"Assess clarity and explanation quality\",\n",
    "        \"Identify any reasoning gaps or issues\",\n",
    "        \"Score overall logical flow quality\"\n",
    "    ]\n",
    "    \n",
    "    logical_flow_metric = GEval(\n",
    "        name=\"Logical Flow\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Original query\n",
    "            LLMTestCase.actual_output,  # Final answer\n",
    "            \"reasoning_steps\"  # Custom field for reasoning steps\n",
    "        ],\n",
    "        threshold=7.0,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return logical_flow_metric\n",
    "\n",
    "# Create logical flow metric\n",
    "logical_flow_metric = create_logical_flow_metric()\n",
    "print(\"✅ Logical Flow Metric created\")\n",
    "print(f\"Threshold: {logical_flow_metric.threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ToolUsageMetric - Đánh giá Hiệu quả Sử dụng Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tool_usage_metric():\n",
    "    \"\"\"\n",
    "    Tạo metric để đánh giá tool usage efficiency\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    Đánh giá cách AI agent sử dụng tools để solve problems.\n",
    "    \n",
    "    Tool Usage Criteria:\n",
    "    1. TOOL SELECTION APPROPRIATENESS (35%):\n",
    "       - Correct tools chosen for the task\n",
    "       - No unnecessary tool calls\n",
    "       - Optimal tool sequence\n",
    "    \n",
    "    2. INPUT QUALITY (25%):\n",
    "       - Tool inputs properly formatted\n",
    "       - Relevant parameters provided\n",
    "       - No malformed inputs\n",
    "    \n",
    "    3. EFFICIENCY (25%):\n",
    "       - Minimal number of tool calls needed\n",
    "       - No redundant operations\n",
    "       - Direct path to solution\n",
    "    \n",
    "    4. ERROR HANDLING (15%):\n",
    "       - Graceful handling of tool errors\n",
    "       - Appropriate fallback strategies\n",
    "       - Recovery from failures\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Perfect tool usage, optimal efficiency\n",
    "    - 7-8: Good tool selection and usage\n",
    "    - 5-6: Acceptable but suboptimal usage\n",
    "    - 3-4: Poor tool choices or inefficient usage\n",
    "    - 1-2: Inappropriate or failed tool usage\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Analyze tool selection appropriateness for the task\",\n",
    "        \"Evaluate tool input quality and formatting\",\n",
    "        \"Assess efficiency of tool usage pattern\",\n",
    "        \"Check error handling and recovery\",\n",
    "        \"Calculate overall tool usage effectiveness\"\n",
    "    ]\n",
    "    \n",
    "    tool_usage_metric = GEval(\n",
    "        name=\"Tool Usage Efficiency\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Task description\n",
    "            \"tool_calls\",  # Tool calls made\n",
    "            \"intermediate_results\"  # Tool outputs\n",
    "        ],\n",
    "        threshold=6.5,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return tool_usage_metric\n",
    "\n",
    "# Create tool usage metric\n",
    "tool_usage_metric = create_tool_usage_metric()\n",
    "print(\"✅ Tool Usage Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PlanExecutionMetric - Đánh giá Khả năng Thực hiện Kế hoạch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plan_execution_metric():\n",
    "    \"\"\"\n",
    "    Tạo metric để đánh giá plan execution capability\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    Đánh giá khả năng planning và execution của AI agent.\n",
    "    \n",
    "    Plan Execution Criteria:\n",
    "    1. PLAN QUALITY (30%):\n",
    "       - Comprehensive and well-structured plan\n",
    "       - Realistic and achievable steps\n",
    "       - Proper task decomposition\n",
    "    \n",
    "    2. EXECUTION FIDELITY (30%):\n",
    "       - Plan followed accurately\n",
    "       - Steps executed in logical order\n",
    "       - Minimal deviation from plan\n",
    "    \n",
    "    3. ADAPTABILITY (25%):\n",
    "       - Adjustments made when needed\n",
    "       - Handling of unexpected results\n",
    "       - Plan refinement during execution\n",
    "    \n",
    "    4. GOAL ACHIEVEMENT (15%):\n",
    "       - Original objective met\n",
    "       - Complete task completion\n",
    "       - Quality of final outcome\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Excellent planning and flawless execution\n",
    "    - 7-8: Good plan with solid execution\n",
    "    - 5-6: Adequate planning, some execution issues\n",
    "    - 3-4: Poor planning or significant execution problems\n",
    "    - 1-2: Failed planning or execution\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Evaluate initial plan quality and structure\",\n",
    "        \"Assess execution fidelity to the plan\",\n",
    "        \"Check adaptability and plan adjustments\",\n",
    "        \"Measure goal achievement and completion\",\n",
    "        \"Score overall plan execution performance\"\n",
    "    ]\n",
    "    \n",
    "    plan_execution_metric = GEval(\n",
    "        name=\"Plan Execution\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Original task\n",
    "            LLMTestCase.actual_output,  # Final result\n",
    "            \"reasoning_steps\",  # Planning and execution steps\n",
    "            \"execution_status\"  # Final status\n",
    "        ],\n",
    "        threshold=7.0,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return plan_execution_metric\n",
    "\n",
    "# Create plan execution metric\n",
    "plan_execution_metric = create_plan_execution_metric()\n",
    "print(\"✅ Plan Execution Metric created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 AdaptabilityMetric - Đánh giá Khả năng Thích ứng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adaptability_metric():\n",
    "    \"\"\"\n",
    "    Tạo metric để đánh giá adaptability của agent\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_criteria = \"\"\"\n",
    "    Đánh giá khả năng thích ứng và flexibility của AI agent.\n",
    "    \n",
    "    Adaptability Criteria:\n",
    "    1. CONTEXT AWARENESS (30%):\n",
    "       - Understanding of changing context\n",
    "       - Recognition of new information\n",
    "       - Appropriate response to context shifts\n",
    "    \n",
    "    2. STRATEGY ADJUSTMENT (25%):\n",
    "       - Modification of approach when needed\n",
    "       - Alternative strategy exploration\n",
    "       - Dynamic problem-solving\n",
    "    \n",
    "    3. ERROR RECOVERY (25%):\n",
    "       - Grace handling of errors\n",
    "       - Learning from mistakes\n",
    "       - Resilient continuation\n",
    "    \n",
    "    4. FLEXIBLE THINKING (20%):\n",
    "       - Creative problem-solving approaches\n",
    "       - Multiple solution pathways\n",
    "       - Open-minded reasoning\n",
    "    \n",
    "    Scoring:\n",
    "    - 9-10: Highly adaptable, excellent flexibility\n",
    "    - 7-8: Good adaptability, handles changes well\n",
    "    - 5-6: Moderate adaptability, some rigidity\n",
    "    - 3-4: Poor adaptability, struggles with changes\n",
    "    - 1-2: Inflexible, cannot adapt to new situations\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_steps = [\n",
    "        \"Assess context awareness and recognition\",\n",
    "        \"Evaluate strategy adjustment capabilities\",\n",
    "        \"Check error recovery and resilience\",\n",
    "        \"Analyze flexible thinking patterns\",\n",
    "        \"Score overall adaptability performance\"\n",
    "    ]\n",
    "    \n",
    "    adaptability_metric = GEval(\n",
    "        name=\"Adaptability\",\n",
    "        criteria=evaluation_criteria,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        evaluation_params=[\n",
    "            LLMTestCase.input,  # Task/scenario\n",
    "            \"reasoning_steps\",  # Decision making process\n",
    "            \"tool_calls\",  # Adaptive tool usage\n",
    "            \"error_message\"  # Error handling\n",
    "        ],\n",
    "        threshold=6.5,\n",
    "        model=\"gpt-4\",\n",
    "        include_reason=True\n",
    "    )\n",
    "    \n",
    "    return adaptability_metric\n",
    "\n",
    "# Create adaptability metric\n",
    "adaptability_metric = create_adaptability_metric()\n",
    "print(\"✅ Adaptability Metric created\")\n",
    "print(\"\\n🎯 All Agent Metrics Created:\")\n",
    "print(\"  1. Logical Flow - Reasoning quality\")\n",
    "print(\"  2. Tool Usage - Tool efficiency\")\n",
    "print(\"  3. Plan Execution - Planning & execution\")\n",
    "print(\"  4. Adaptability - Flexibility & recovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Phần 5: Comprehensive Agent Evaluation\n",
    "\n",
    "### 5.1 Agent Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEvaluationPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation pipeline cho AI agents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        self.model = model\n",
    "        self.evaluation_history = []\n",
    "        \n",
    "        # Initialize agent-specific metrics\n",
    "        self.metrics = {\n",
    "            \"logical_flow\": logical_flow_metric,\n",
    "            \"tool_usage\": tool_usage_metric,\n",
    "            \"plan_execution\": plan_execution_metric,\n",
    "            \"adaptability\": adaptability_metric\n",
    "        }\n",
    "    \n",
    "    def evaluate_agent_execution(self, agent_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate single agent execution result\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create specialized test case for agent evaluation\n",
    "        test_case = LLMTestCase(\n",
    "            input=agent_result[\"query\"],\n",
    "            actual_output=agent_result.get(\"final_answer\", \"No answer\"),\n",
    "            # Add custom fields for agent evaluation\n",
    "            context=agent_result.get(\"reasoning_steps\", []),\n",
    "            retrieval_context=agent_result.get(\"intermediate_results\", [])\n",
    "        )\n",
    "        \n",
    "        # Add agent-specific data as attributes\n",
    "        test_case.reasoning_steps = \"\\n\".join(agent_result.get(\"reasoning_steps\", []))\n",
    "        test_case.tool_calls = json.dumps(agent_result.get(\"tool_calls\", []), indent=2)\n",
    "        test_case.intermediate_results = \"\\n\".join(agent_result.get(\"intermediate_results\", []))\n",
    "        test_case.execution_status = agent_result.get(\"execution_status\", \"unknown\")\n",
    "        test_case.error_message = agent_result.get(\"error_message\", \"\")\n",
    "        \n",
    "        results = {\n",
    "            \"query\": agent_result[\"query\"],\n",
    "            \"execution_time\": agent_result.get(\"execution_time\", 0),\n",
    "            \"execution_status\": agent_result.get(\"execution_status\", \"unknown\"),\n",
    "            \"tool_calls_count\": len(agent_result.get(\"tool_calls\", [])),\n",
    "            \"reasoning_steps_count\": len(agent_result.get(\"reasoning_steps\", [])),\n",
    "            \"metrics\": {},\n",
    "            \"overall_score\": 0,\n",
    "            \"pass_count\": 0,\n",
    "            \"total_metrics\": len(self.metrics)\n",
    "        }\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        metric_scores = []\n",
    "        \n",
    "        for metric_name, metric in self.metrics.items():\n",
    "            try:\n",
    "                # Create fresh metric instance\n",
    "                fresh_metric = GEval(\n",
    "                    name=metric.name,\n",
    "                    criteria=metric.criteria,\n",
    "                    evaluation_steps=metric.evaluation_steps,\n",
    "                    evaluation_params=metric.evaluation_params,\n",
    "                    threshold=metric.threshold,\n",
    "                    model=metric.model,\n",
    "                    include_reason=True\n",
    "                )\n",
    "                \n",
    "                fresh_metric.measure(test_case)\n",
    "                \n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": fresh_metric.score,\n",
    "                    \"passed\": fresh_metric.is_successful(),\n",
    "                    \"reason\": fresh_metric.reason,\n",
    "                    \"threshold\": fresh_metric.threshold\n",
    "                }\n",
    "                \n",
    "                metric_scores.append(fresh_metric.score)\n",
    "                if fresh_metric.is_successful():\n",
    "                    results[\"pass_count\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[\"metrics\"][metric_name] = {\n",
    "                    \"score\": 0,\n",
    "                    \"passed\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                metric_scores.append(0)\n",
    "        \n",
    "        # Calculate overall score\n",
    "        if metric_scores:\n",
    "            results[\"overall_score\"] = np.mean(metric_scores)\n",
    "        \n",
    "        # Store in history\n",
    "        self.evaluation_history.append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_evaluate(self, agent_results: List[Dict]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Batch evaluation của multiple agent executions\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"🚀 Evaluating {len(agent_results)} Agent Executions\\n\")\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for i, agent_result in enumerate(agent_results):\n",
    "            if \"query\" not in agent_result:\n",
    "                continue\n",
    "                \n",
    "            print(f\"🔍 Evaluating {i+1}/{len(agent_results)}: {agent_result['query'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                result = self.evaluate_agent_execution(agent_result)\n",
    "                evaluation_results.append(result)\n",
    "                \n",
    "                # Quick summary\n",
    "                print(f\"  ✅ Overall Score: {result['overall_score']:.1f}/10\")\n",
    "                print(f\"  📊 Passed: {result['pass_count']}/{result['total_metrics']} metrics\")\n",
    "                print(f\"  ⏱️  Execution: {result['execution_time']:.2f}s\")\n",
    "                print(f\"  🔧 Tools: {result['tool_calls_count']}, Steps: {result['reasoning_steps_count']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Evaluation error: {e}\")\n",
    "                evaluation_results.append({\n",
    "                    \"query\": agent_result[\"query\"],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive performance summary\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.evaluation_history:\n",
    "            return {\"message\": \"No evaluations performed yet\"}\n",
    "        \n",
    "        # Collect metrics data\n",
    "        metric_summaries = {}\n",
    "        overall_scores = []\n",
    "        execution_times = []\n",
    "        tool_usage_counts = []\n",
    "        \n",
    "        for result in self.evaluation_history:\n",
    "            if \"overall_score\" in result:\n",
    "                overall_scores.append(result[\"overall_score\"])\n",
    "            if \"execution_time\" in result:\n",
    "                execution_times.append(result[\"execution_time\"])\n",
    "            if \"tool_calls_count\" in result:\n",
    "                tool_usage_counts.append(result[\"tool_calls_count\"])\n",
    "            \n",
    "            if \"metrics\" in result:\n",
    "                for metric_name, metric_data in result[\"metrics\"].items():\n",
    "                    if \"score\" in metric_data:\n",
    "                        if metric_name not in metric_summaries:\n",
    "                            metric_summaries[metric_name] = []\n",
    "                        metric_summaries[metric_name].append(metric_data[\"score\"])\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            \"total_evaluations\": len(self.evaluation_history),\n",
    "            \"overall_performance\": {\n",
    "                \"average_score\": round(np.mean(overall_scores), 2) if overall_scores else 0,\n",
    "                \"score_std\": round(np.std(overall_scores), 2) if overall_scores else 0,\n",
    "                \"min_score\": round(min(overall_scores), 2) if overall_scores else 0,\n",
    "                \"max_score\": round(max(overall_scores), 2) if overall_scores else 0\n",
    "            },\n",
    "            \"performance_metrics\": {\n",
    "                \"average_execution_time\": round(np.mean(execution_times), 2) if execution_times else 0,\n",
    "                \"average_tool_usage\": round(np.mean(tool_usage_counts), 1) if tool_usage_counts else 0\n",
    "            },\n",
    "            \"metric_performance\": {}\n",
    "        }\n",
    "        \n",
    "        for metric_name, scores in metric_summaries.items():\n",
    "            summary[\"metric_performance\"][metric_name] = {\n",
    "                \"average_score\": round(np.mean(scores), 2),\n",
    "                \"min_score\": round(min(scores), 2),\n",
    "                \"max_score\": round(max(scores), 2),\n",
    "                \"std_dev\": round(np.std(scores), 2)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create agent evaluation pipeline\n",
    "agent_eval_pipeline = AgentEvaluationPipeline()\n",
    "print(\"✅ Agent Evaluation Pipeline created successfully!\")\n",
    "print(f\"Available metrics: {list(agent_eval_pipeline.metrics.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Comprehensive Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation on agent test results\n",
    "if agent_test_results:\n",
    "    comprehensive_agent_results = agent_eval_pipeline.batch_evaluate(agent_test_results)\n",
    "    agent_performance_summary = agent_eval_pipeline.get_performance_summary()\nelse:\n",
    "    print(\"❌ No agent test results available for evaluation\")\n",
    "    comprehensive_agent_results = []\n",
    "    agent_performance_summary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive agent evaluation results\n",
    "def display_agent_evaluation_results(results, summary):\n",
    "    \"\"\"\n",
    "    Display và analyze agent evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ No agent evaluation results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"📊 Comprehensive Agent Evaluation Results\\n\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"overall_score\" in result:\n",
    "            row = {\n",
    "                \"Query\": result[\"query\"][:30] + \"...\",\n",
    "                \"Overall_Score\": result[\"overall_score\"],\n",
    "                \"Pass_Rate\": f\"{result['pass_count']}/{result['total_metrics']}\",\n",
    "                \"Exec_Time\": result[\"execution_time\"],\n",
    "                \"Tools_Used\": result[\"tool_calls_count\"],\n",
    "                \"Reasoning_Steps\": result[\"reasoning_steps_count\"]\n",
    "            }\n",
    "            \n",
    "            # Add individual metric scores\n",
    "            for metric_name, metric_data in result.get(\"metrics\", {}).items():\n",
    "                if \"score\" in metric_data:\n",
    "                    row[f\"{metric_name.replace('_', ' ').title()}\"] = metric_data[\"score\"]\n",
    "            \n",
    "            summary_data.append(row)\n",
    "    \n",
    "    if summary_data:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.round(1).to_string(index=False))\n",
    "    \n",
    "    # Display performance summary\n",
    "    print(f\"\\n📈 Agent Performance Summary:\")\n",
    "    if \"overall_performance\" in summary:\n",
    "        perf = summary[\"overall_performance\"]\n",
    "        print(f\"  Total Evaluations: {summary['total_evaluations']}\")\n",
    "        print(f\"  Average Overall Score: {perf['average_score']}/10\")\n",
    "        print(f\"  Score Range: {perf['min_score']} - {perf['max_score']}\")\n",
    "        print(f\"  Score Std Dev: {perf['score_std']}\")\n",
    "    \n",
    "    if \"performance_metrics\" in summary:\n",
    "        perf_metrics = summary[\"performance_metrics\"]\n",
    "        print(f\"\\n⚡ Performance Metrics:\")\n",
    "        print(f\"  Average Execution Time: {perf_metrics['average_execution_time']}s\")\n",
    "        print(f\"  Average Tool Usage: {perf_metrics['average_tool_usage']} tools/query\")\n",
    "    \n",
    "    print(f\"\\n🎯 Agent-Specific Metric Performance:\")\n",
    "    if \"metric_performance\" in summary:\n",
    "        for metric_name, stats in summary[\"metric_performance\"].items():\n",
    "            print(f\"  {metric_name.replace('_', ' ').title()}:\")\n",
    "            print(f\"    Average: {stats['average_score']}/10\")\n",
    "            print(f\"    Range: {stats['min_score']} - {stats['max_score']}\")\n",
    "            print(f\"    Std Dev: {stats['std_dev']}\")\n",
    "    \n",
    "    # Agent-specific insights\n",
    "    print(f\"\\n💡 Agent Performance Insights:\")\n",
    "    \n",
    "    if \"metric_performance\" in summary:\n",
    "        metric_avgs = {name: stats['average_score'] for name, stats in summary[\"metric_performance\"].items()}\n",
    "        \n",
    "        if metric_avgs:\n",
    "            best_metric = max(metric_avgs.keys(), key=lambda k: metric_avgs[k])\n",
    "            worst_metric = min(metric_avgs.keys(), key=lambda k: metric_avgs[k])\n",
    "            \n",
    "            print(f\"  • Strongest capability: {best_metric.replace('_', ' ').title()} ({metric_avgs[best_metric]:.1f}/10)\")\n",
    "            print(f\"  • Improvement area: {worst_metric.replace('_', ' ').title()} ({metric_avgs[worst_metric]:.1f}/10)\")\n",
    "            \n",
    "            if metric_avgs[worst_metric] < 6.0:\n",
    "                print(f\"  • ⚠️  {worst_metric.replace('_', ' ').title()} needs significant improvement\")\n",
    "    \n",
    "    if \"performance_metrics\" in summary:\n",
    "        perf_metrics = summary[\"performance_metrics\"]\n",
    "        if perf_metrics[\"average_execution_time\"] > 5.0:\n",
    "            print(f\"  • ⚠️  Long execution times - consider optimization\")\n",
    "        if perf_metrics[\"average_tool_usage\"] > 3.0:\n",
    "            print(f\"  • ⚠️  High tool usage - may indicate inefficiency\")\n",
    "        elif perf_metrics[\"average_tool_usage\"] < 1.0:\n",
    "            print(f\"  • ⚠️  Low tool usage - agent may not be leveraging available tools\")\n",
    "    \n",
    "    return df if 'df' in locals() else None\n",
    "\n",
    "# Display agent evaluation results\n",
    "agent_results_df = display_agent_evaluation_results(comprehensive_agent_results, agent_performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Phần 6: Agent Performance Visualization\n",
    "\n",
    "### 6.1 Agent Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_agent_performance(results, summary):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations cho agent performance\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results or not summary.get(\"metric_performance\"):\n",
    "        print(\"❌ Insufficient data for agent visualization\")\n",
    "        return\n",
    "    \n",
    "    # Setup plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Agent Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Agent Metric Performance Radar Chart\n",
    "    metric_names = list(summary[\"metric_performance\"].keys())\n",
    "    metric_scores = [summary[\"metric_performance\"][name][\"average_score\"] for name in metric_names]\n",
    "    \n",
    "    # Convert to radar chart data\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metric_names), endpoint=False)\n",
    "    metric_scores += metric_scores[:1]  # Complete the circle\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 3, 1, projection='polar')\n",
    "    ax_radar.plot(angles, metric_scores, 'o-', linewidth=2, color='blue')\n",
    "    ax_radar.fill(angles, metric_scores, alpha=0.25, color='blue')\n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels([name.replace('_', '\\n').title() for name in metric_names])\n",
    "    ax_radar.set_ylim(0, 10)\n",
    "    ax_radar.set_title('Agent Capability Radar', pad=20)\n",
    "    ax_radar.grid(True)\n",
    "    \n",
    "    # 2. Execution Time vs Performance\n",
    "    exec_times = []\n",
    "    overall_scores = []\n",
    "    tool_counts = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"overall_score\" in result and \"execution_time\" in result:\n",
    "            exec_times.append(result[\"execution_time\"])\n",
    "            overall_scores.append(result[\"overall_score\"])\n",
    "            tool_counts.append(result.get(\"tool_calls_count\", 0))\n",
    "    \n",
    "    if exec_times and overall_scores:\n",
    "        scatter = axes[0,1].scatter(exec_times, overall_scores, c=tool_counts, \n",
    "                                  cmap='viridis', alpha=0.7, s=100)\n",
    "        axes[0,1].set_xlabel('Execution Time (seconds)')\n",
    "        axes[0,1].set_ylabel('Overall Score')\n",
    "        axes[0,1].set_title('Execution Time vs Performance')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[0,1])\n",
    "        cbar.set_label('Tool Usage Count')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(exec_times) > 1:\n",
    "            z = np.polyfit(exec_times, overall_scores, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[0,1].plot(exec_times, p(exec_times), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # 3. Metric Score Distribution\n",
    "    all_metric_scores = []\n",
    "    all_metric_names = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"metrics\" in result:\n",
    "            for metric_name, metric_data in result[\"metrics\"].items():\n",
    "                if \"score\" in metric_data:\n",
    "                    all_metric_scores.append(metric_data[\"score\"])\n",
    "                    all_metric_names.append(metric_name)\n",
    "    \n",
    "    if all_metric_scores:\n",
    "        axes[0,2].hist(all_metric_scores, bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[0,2].set_xlabel('Metric Scores')\n",
    "        axes[0,2].set_ylabel('Frequency')\n",
    "        axes[0,2].set_title('Agent Metric Score Distribution')\n",
    "        axes[0,2].axvline(x=np.mean(all_metric_scores), color='red', linestyle='--', \n",
    "                         label=f'Mean: {np.mean(all_metric_scores):.1f}')\n",
    "        axes[0,2].legend()\n",
    "    \n",
    "    # 4. Tool Usage Efficiency\n",
    "    tool_efficiency = []\n",
    "    query_names = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"tool_calls_count\" in result and \"overall_score\" in result:\n",
    "            tools_used = result[\"tool_calls_count\"]\n",
    "            score = result[\"overall_score\"]\n",
    "            # Efficiency = score per tool used (with minimum 1 to avoid division by zero)\n",
    "            efficiency = score / max(tools_used, 1)\n",
    "            tool_efficiency.append(efficiency)\n",
    "            query_names.append(result[\"query\"][:15] + \"...\")\n",
    "    \n",
    "    if tool_efficiency:\n",
    "        bars = axes[1,0].bar(range(len(tool_efficiency)), tool_efficiency, \n",
    "                            color=['green' if eff >= 5 else 'orange' if eff >= 3 else 'red' for eff in tool_efficiency])\n",
    "        axes[1,0].set_xlabel('Queries')\n",
    "        axes[1,0].set_ylabel('Score per Tool Used')\n",
    "        axes[1,0].set_title('Tool Usage Efficiency')\n",
    "        axes[1,0].set_xticks(range(len(query_names)))\n",
    "        axes[1,0].set_xticklabels(query_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, eff) in enumerate(zip(bars, tool_efficiency)):\n",
    "            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                          f'{eff:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 5. Reasoning Steps vs Success Rate\n",
    "    reasoning_steps = []\n",
    "    success_rates = []\n",
    "    \n",
    "    for result in results:\n",
    "        if \"reasoning_steps_count\" in result and \"pass_count\" in result and \"total_metrics\" in result:\n",
    "            steps = result[\"reasoning_steps_count\"]\n",
    "            success_rate = result[\"pass_count\"] / result[\"total_metrics\"] * 100\n",
    "            reasoning_steps.append(steps)\n",
    "            success_rates.append(success_rate)\n",
    "    \n",
    "    if reasoning_steps and success_rates:\n",
    "        axes[1,1].scatter(reasoning_steps, success_rates, alpha=0.7, s=100, color='purple')\n",
    "        axes[1,1].set_xlabel('Number of Reasoning Steps')\n",
    "        axes[1,1].set_ylabel('Success Rate (%)')\n",
    "        axes[1,1].set_title('Reasoning Depth vs Success')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(reasoning_steps) > 1:\n",
    "            z = np.polyfit(reasoning_steps, success_rates, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1,1].plot(reasoning_steps, p(reasoning_steps), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # 6. Agent Performance Heatmap\n",
    "    if len(metric_names) > 1 and len(results) > 1:\n",
    "        # Create performance matrix\n",
    "        performance_matrix = []\n",
    "        \n",
    "        for result in results:\n",
    "            if \"metrics\" in result:\n",
    "                row = []\n",
    "                for metric_name in metric_names:\n",
    "                    if metric_name in result[\"metrics\"] and \"score\" in result[\"metrics\"][metric_name]:\n",
    "                        row.append(result[\"metrics\"][metric_name][\"score\"])\n",
    "                    else:\n",
    "                        row.append(0)\n",
    "                if len(row) == len(metric_names):\n",
    "                    performance_matrix.append(row)\n",
    "        \n",
    "        if len(performance_matrix) > 1:\n",
    "            performance_df = pd.DataFrame(performance_matrix, \n",
    "                                        columns=[name.replace('_', ' ').title() for name in metric_names],\n",
    "                                        index=[f\"Query {i+1}\" for i in range(len(performance_matrix))])\n",
    "            \n",
    "            sns.heatmap(performance_df, annot=True, cmap='RdYlGn', center=5, \n",
    "                       square=False, ax=axes[1,2], cbar_kws={'shrink': 0.8})\n",
    "            axes[1,2].set_title('Performance Heatmap by Query')\n",
    "        else:\n",
    "            axes[1,2].text(0.5, 0.5, 'Insufficient data\\nfor heatmap', \n",
    "                          ha='center', va='center', transform=axes[1,2].transAxes, fontsize=12)\n",
    "            axes[1,2].set_title('Performance Heatmap by Query')\n",
    "    else:\n",
    "        axes[1,2].text(0.5, 0.5, 'Need multiple metrics\\nand queries for heatmap', \n",
    "                      ha='center', va='center', transform=axes[1,2].transAxes, fontsize=12)\n",
    "        axes[1,2].set_title('Performance Heatmap by Query')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\n🔍 Agent Performance Insights:\")\n",
    "    \n",
    "    if metric_scores:\n",
    "        strongest_capability = metric_names[metric_scores[:-1].index(max(metric_scores[:-1]))]\n",
    "        weakest_capability = metric_names[metric_scores[:-1].index(min(metric_scores[:-1]))]\n",
    "        \n",
    "        print(f\"  • Strongest capability: {strongest_capability.replace('_', ' ').title()}\")\n",
    "        print(f\"  • Weakest capability: {weakest_capability.replace('_', ' ').title()}\")\n",
    "        \n",
    "        capability_range = max(metric_scores[:-1]) - min(metric_scores[:-1])\n",
    "        if capability_range > 3.0:\n",
    "            print(f\"  • ⚠️  Large capability variance ({capability_range:.1f}) - uneven agent development\")\n",
    "    \n",
    "    if exec_times and overall_scores:\n",
    "        correlation = np.corrcoef(exec_times, overall_scores)[0, 1]\n",
    "        if correlation < -0.3:\n",
    "            print(f\"  • ⚠️  Negative correlation between time and performance - efficiency issues\")\n",
    "        elif correlation > 0.3:\n",
    "            print(f\"  • ✅ Positive correlation - more time leads to better results\")\n",
    "    \n",
    "    if tool_efficiency:\n",
    "        avg_efficiency = np.mean(tool_efficiency)\n",
    "        print(f\"  • Average tool efficiency: {avg_efficiency:.1f} score per tool\")\n",
    "        \n",
    "        if avg_efficiency < 3.0:\n",
    "            print(f\"  • ⚠️  Low tool efficiency - agent may be overusing tools\")\n",
    "        elif avg_efficiency > 7.0:\n",
    "            print(f\"  • ✅ High tool efficiency - agent uses tools effectively\")\n",
    "\n",
    "# Create agent performance visualizations\n",
    "visualize_agent_performance(comprehensive_agent_results, agent_performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Phần 7: Advanced Agent Scenarios\n",
    "\n",
    "### 7.1 Complex Multi-Step Reasoning Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complex_agent_scenarios():\n",
    "    \"\"\"\n",
    "    Tạo complex scenarios để test agent reasoning capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    complex_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Mathematical Problem Solving\",\n",
    "            \"query\": \"I need to find all prime numbers between 10 and 30, then calculate the factorial of the largest prime found, and finally determine if the result is divisible by 12.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Identify prime numbers between 10 and 30\",\n",
    "                \"Find the largest prime\",\n",
    "                \"Calculate factorial of largest prime\",\n",
    "                \"Check divisibility by 12\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"prime_checker\", \"factorial\", \"calculator\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Information Synthesis\",\n",
    "            \"query\": \"What is the relationship between machine learning and deep learning? After explaining this, generate a random number between 1 and 100 and tell me if that number is prime.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Search for machine learning information\",\n",
    "                \"Search for deep learning information\",\n",
    "                \"Synthesize relationship\",\n",
    "                \"Generate random number\",\n",
    "                \"Check if number is prime\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"knowledge_search\", \"random_number\", \"prime_checker\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Time-Based Calculation\",\n",
    "            \"query\": \"What's the current time? Based on the current hour, calculate the factorial of that hour number. If the current minute is even, add 50 to the factorial result.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Get current time\",\n",
    "                \"Extract hour from time\",\n",
    "                \"Calculate factorial of hour\",\n",
    "                \"Check if minute is even\",\n",
    "                \"Conditionally add 50\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"get_time\", \"factorial\", \"calculator\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Error Recovery Scenario\",\n",
    "            \"query\": \"Calculate the factorial of 25, then find the square root of that result. If there are any issues, try alternative approaches.\",\n",
    "            \"expected_steps\": [\n",
    "                \"Calculate factorial of 25\",\n",
    "                \"Attempt square root calculation\",\n",
    "                \"Handle potential errors\",\n",
    "                \"Provide alternative solution\"\n",
    "            ],\n",
    "            \"expected_tools\": [\"factorial\", \"calculator\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return complex_scenarios\n",
    "\n",
    "# Create complex scenarios\n",
    "complex_scenarios = create_complex_agent_scenarios()\n",
    "\n",
    "print(\"🧩 Created Complex Agent Scenarios:\")\n",
    "for i, scenario in enumerate(complex_scenarios, 1):\n",
    "    print(f\"  {i}. {scenario['name']}\")\n",
    "    print(f\"     Query: {scenario['query'][:60]}...\")\n",
    "    print(f\"     Expected tools: {', '.join(scenario['expected_tools'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Test Complex Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_complex_agent_scenarios(agent, scenarios):\n",
    "    \"\"\"\n",
    "    Test agent với complex scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    if not agent:\n",
    "        print(\"❌ No agent available for complex scenario testing\")\n",
    "        return []\n",
    "    \n",
    "    print(\"🚀 Testing Complex Agent Scenarios\\n\")\n",
    "    \n",
    "    complex_results = []\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"🧩 Complex Scenario {i}: {scenario['name']}\")\n",
    "        print(f\"Query: {scenario['query']}\")\n",
    "        \n",
    "        try:\n",
    "            # Create initial state\n",
    "            initial_state = create_initial_state(scenario['query'])\n",
    "            \n",
    "            # Execute agent\n",
    "            start_time = time.time()\n",
    "            final_state = agent.invoke(initial_state)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Extract results with scenario analysis\n",
    "            result = {\n",
    "                \"scenario_name\": scenario['name'],\n",
    "                \"query\": scenario['query'],\n",
    "                \"final_answer\": final_state.get(\"final_answer\", \"No answer generated\"),\n",
    "                \"reasoning_steps\": final_state.get(\"reasoning_steps\", []),\n",
    "                \"tool_calls\": final_state.get(\"tool_calls\", []),\n",
    "                \"intermediate_results\": final_state.get(\"intermediate_results\", []),\n",
    "                \"execution_status\": final_state.get(\"execution_status\", \"unknown\"),\n",
    "                \"execution_time\": execution_time,\n",
    "                \"error_message\": final_state.get(\"error_message\", \"\"),\n",
    "                \n",
    "                # Scenario-specific analysis\n",
    "                \"expected_steps\": scenario['expected_steps'],\n",
    "                \"expected_tools\": scenario['expected_tools'],\n",
    "                \"tools_used\": [call['tool'] for call in final_state.get(\"tool_calls\", [])],\n",
    "                \"step_coverage\": 0,  # Will calculate below\n",
    "                \"tool_coverage\": 0   # Will calculate below\n",
    "            }\n",
    "            \n",
    "            # Calculate step coverage (how many expected steps were addressed)\n",
    "            reasoning_text = \" \".join(result[\"reasoning_steps\"]).lower()\n",
    "            covered_steps = 0\n",
    "            for step in scenario['expected_steps']:\n",
    "                if any(keyword.lower() in reasoning_text for keyword in step.split()):\n",
    "                    covered_steps += 1\n",
    "            result[\"step_coverage\"] = covered_steps / len(scenario['expected_steps']) if scenario['expected_steps'] else 0\n",
    "            \n",
    "            # Calculate tool coverage (how many expected tools were used)\n",
    "            used_expected_tools = set(result[\"tools_used\"]) & set(scenario['expected_tools'])\n",
    "            result[\"tool_coverage\"] = len(used_expected_tools) / len(scenario['expected_tools']) if scenario['expected_tools'] else 0\n",
    "            \n",
    "            complex_results.append(result)\n",
    "            \n",
    "            # Print detailed analysis\n",
    "            status_emoji = \"✅\" if result[\"execution_status\"] == \"completed\" else \"❌\"\n",
    "            print(f\"  {status_emoji} Status: {result['execution_status']}\")\n",
    "            print(f\"  ⏱️  Execution time: {execution_time:.2f}s\")\n",
    "            print(f\"  🎯 Step coverage: {result['step_coverage']:.1%} ({covered_steps}/{len(scenario['expected_steps'])})\")\n",
    "            print(f\"  🔧 Tool coverage: {result['tool_coverage']:.1%} ({len(used_expected_tools)}/{len(scenario['expected_tools'])})\")\n",
    "            print(f\"  📊 Tools used: {', '.join(result['tools_used']) if result['tools_used'] else 'None'}\")\n",
    "            print(f\"  🧠 Reasoning steps: {len(result['reasoning_steps'])}\")\n",
    "            \n",
    "            if result[\"final_answer\"]:\n",
    "                print(f\"  💡 Answer: {result['final_answer'][:100]}...\")\n",
    "            \n",
    "            if result[\"error_message\"]:\n",
    "                print(f\"  ⚠️  Error: {result['error_message']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Execution failed: {e}\")\n",
    "            complex_results.append({\n",
    "                \"scenario_name\": scenario['name'],\n",
    "                \"query\": scenario['query'],\n",
    "                \"execution_status\": \"failed\",\n",
    "                \"error_message\": str(e)\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Overall complex scenario analysis\n",
    "    if complex_results:\n",
    "        successful_scenarios = [r for r in complex_results if r.get(\"execution_status\") == \"completed\"]\n",
    "        \n",
    "        if successful_scenarios:\n",
    "            avg_step_coverage = np.mean([r.get(\"step_coverage\", 0) for r in successful_scenarios])\n",
    "            avg_tool_coverage = np.mean([r.get(\"tool_coverage\", 0) for r in successful_scenarios])\n",
    "            avg_execution_time = np.mean([r.get(\"execution_time\", 0) for r in successful_scenarios])\n",
    "            \n",
    "            print(f\"📊 Complex Scenario Summary:\")\n",
    "            print(f\"  Successful scenarios: {len(successful_scenarios)}/{len(complex_results)}\")\n",
    "            print(f\"  Average step coverage: {avg_step_coverage:.1%}\")\n",
    "            print(f\"  Average tool coverage: {avg_tool_coverage:.1%}\")\n",
    "            print(f\"  Average execution time: {avg_execution_time:.2f}s\")\n",
    "            \n",
    "            if avg_step_coverage < 0.7:\n",
    "                print(f\"  ⚠️  Low step coverage - agent may miss important reasoning steps\")\n",
    "            if avg_tool_coverage < 0.8:\n",
    "                print(f\"  ⚠️  Low tool coverage - agent may not be using available tools effectively\")\n",
    "    \n",
    "    return complex_results\n",
    "\n",
    "# Test complex scenarios\n",
    "complex_scenario_results = test_complex_agent_scenarios(langgraph_agent, complex_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Evaluate Complex Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate complex scenarios với agent metrics\n",
    "if complex_scenario_results:\n",
    "    print(\"🔍 Evaluating Complex Scenarios với Agent Metrics\\n\")\n",
    "    \n",
    "    complex_evaluation_results = agent_eval_pipeline.batch_evaluate(complex_scenario_results)\n",
    "    complex_performance_summary = agent_eval_pipeline.get_performance_summary()\n",
    "    \n",
    "    # Compare simple vs complex performance\n",
    "    print(\"\\n📊 Simple vs Complex Scenario Comparison:\")\n",
    "    \n",
    "    # Get performance from both evaluations\n",
    "    simple_scores = [r.get(\"overall_score\", 0) for r in comprehensive_agent_results if \"overall_score\" in r]\n",
    "    complex_scores = [r.get(\"overall_score\", 0) for r in complex_evaluation_results if \"overall_score\" in r]\n",
    "    \n",
    "    if simple_scores and complex_scores:\n",
    "        simple_avg = np.mean(simple_scores)\n",
    "        complex_avg = np.mean(complex_scores)\n",
    "        \n",
    "        print(f\"  Simple scenarios average: {simple_avg:.1f}/10\")\n",
    "        print(f\"  Complex scenarios average: {complex_avg:.1f}/10\")\n",
    "        print(f\"  Performance difference: {simple_avg - complex_avg:+.1f}\")\n",
    "        \n",
    "        if complex_avg < simple_avg - 1.0:\n",
    "            print(f\"  ⚠️  Significant performance drop on complex tasks - agent struggles with complexity\")\n",
    "        elif abs(complex_avg - simple_avg) < 0.5:\n",
    "            print(f\"  ✅ Consistent performance across complexity levels\")\n",
    "        else:\n",
    "            print(f\"  📈 Agent handles complexity well\")\n",
    "\nelse:\n",
    "    print(\"❌ No complex scenario results to evaluate\")\n",
    "    complex_evaluation_results = []\n",
    "    complex_performance_summary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Phần 8: Exercises và Thực hành\n",
    "\n",
    "### Exercise 1: Custom Agent Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Tạo custom metric cho agent creativity\n",
    "def exercise_1_creativity_metric():\n",
    "    \"\"\"\n",
    "    TODO: Tạo metric để đánh giá creativity và innovation trong agent reasoning\n",
    "    Yêu cầu:\n",
    "    1. Evaluate creative problem-solving approaches\n",
    "    2. Assess innovative use of available tools\n",
    "    3. Measure originality in reasoning steps\n",
    "    4. Test với various creative scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Define creativity evaluation criteria\n",
    "    creativity_criteria = \"\"\"\n",
    "    Your creativity evaluation criteria here...\n",
    "    Focus on:\n",
    "    - Novel problem-solving approaches\n",
    "    - Innovative tool combinations\n",
    "    - Original reasoning patterns\n",
    "    - Creative solution pathways\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create G-Eval metric\n",
    "    \n",
    "    # TODO: Test với creative scenarios\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"💡 Exercise 1 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Focus on non-standard approaches to problem solving\")\n",
    "print(\"- Consider tool usage patterns và combinations\")\n",
    "print(\"- Evaluate reasoning originality và innovation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Agent Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Optimize agent performance based on evaluation results\n",
    "def exercise_2_agent_optimization():\n",
    "    \"\"\"\n",
    "    TODO: Analyze evaluation results và propose agent improvements\n",
    "    Yêu cầu:\n",
    "    1. Identify performance bottlenecks from evaluation data\n",
    "    2. Propose specific improvements cho agent architecture\n",
    "    3. Design enhanced reasoning prompts\n",
    "    4. Create optimized tool selection strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Analyze current performance data\n",
    "    performance_analysis = {\n",
    "        \"weak_areas\": [],\n",
    "        \"strong_areas\": [],\n",
    "        \"improvement_opportunities\": []\n",
    "    }\n",
    "    \n",
    "    # TODO: Design improvements\n",
    "    \n",
    "    # TODO: Create enhanced agent version\n",
    "    \n",
    "    # TODO: Compare performance improvements\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"💡 Exercise 2 Template created. Complete the function above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Analyze metric scores để identify improvement areas\")\n",
    "print(\"- Consider prompt engineering improvements\")\n",
    "print(\"- Design better tool selection logic\")\n",
    "print(\"- Implement enhanced error recovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Multi-Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create multi-agent evaluation framework\n",
    "def exercise_3_multi_agent_evaluation():\n",
    "    \"\"\"\n",
    "    TODO: Build framework để compare multiple agents\n",
    "    Yêu cầu:\n",
    "    1. Create different agent configurations\n",
    "    2. Design comparative evaluation metrics\n",
    "    3. Run head-to-head agent comparisons\n",
    "    4. Analyze strengths/weaknesses của each agent\n",
    "    5. Determine optimal agent configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    class MultiAgentEvaluator:\n",
    "        def __init__(self):\n",
    "            # TODO: Initialize multiple agent configurations\n",
    "            pass\n",
    "        \n",
    "        def create_agent_variants(self):\n",
    "            # TODO: Create different agent configurations\n",
    "            # - Different reasoning strategies\n",
    "            # - Different tool sets\n",
    "            # - Different prompt templates\n",
    "            pass\n",
    "        \n",
    "        def comparative_evaluation(self, test_scenarios):\n",
    "            # TODO: Run same scenarios across all agents\n",
    "            # TODO: Compare performance metrics\n",
    "            # TODO: Identify best agent for each scenario type\n",
    "            pass\n",
    "        \n",
    "        def generate_recommendations(self):\n",
    "            # TODO: Recommend optimal agent configuration\n",
    "            pass\n",
    "    \n",
    "    # TODO: Implement and test\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"💡 Exercise 3 Template created. Complete the class above!\")\n",
    "print(\"Hints:\")\n",
    "print(\"- Create agents với different capabilities\")\n",
    "print(\"- Use consistent evaluation criteria\")\n",
    "print(\"- Compare both performance và efficiency\")\n",
    "print(\"- Consider task-specific agent selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Tổng kết và Next Steps\n",
    "\n",
    "### 🏆 Những gì đã học trong Notebook này:\n",
    "\n",
    "1. **✅ LangGraph Agent Construction**\n",
    "   - Multi-node agent architecture với StateGraph\n",
    "   - Custom tool creation và integration\n",
    "   - Agent state management và workflow control\n",
    "   - Planning → Execution → Synthesis pipeline\n",
    "\n",
    "2. **✅ Agent-Specific Evaluation Metrics**\n",
    "   - **LogicalFlowMetric**: Reasoning coherence và clarity\n",
    "   - **ToolUsageMetric**: Tool selection và efficiency\n",
    "   - **PlanExecutionMetric**: Planning và execution fidelity\n",
    "   - **AdaptabilityMetric**: Flexibility và error recovery\n",
    "\n",
    "3. **✅ Chain-of-Thought Evaluation**\n",
    "   - Intermediate step capture và analysis\n",
    "   - Multi-dimensional reasoning assessment\n",
    "   - Reasoning quality measurement\n",
    "   - Step-by-step logical flow evaluation\n",
    "\n",
    "4. **✅ Comprehensive Agent Assessment**\n",
    "   - End-to-end agent evaluation pipeline\n",
    "   - Complex scenario testing\n",
    "   - Performance benchmarking\n",
    "   - Comparative analysis frameworks\n",
    "\n",
    "5. **✅ Advanced Analysis Techniques**\n",
    "   - Agent performance visualization\n",
    "   - Tool usage efficiency analysis\n",
    "   - Reasoning depth vs success correlation\n",
    "   - Multi-scenario performance tracking\n",
    "\n",
    "### 🚀 Next Steps - Notebook 5: Feedback Loop & Regenerating CoT\n",
    "\n",
    "Trong notebook cuối cùng, chúng ta sẽ học:\n",
    "\n",
    "- 🔄 **Automated Feedback Loops**: Self-improving agent systems\n",
    "- 🧠 **CoT Regeneration**: Dynamic reasoning improvement\n",
    "- 📈 **Performance Optimization**: Iterative agent enhancement\n",
    "- 🎯 **Production Deployment**: Real-world agent evaluation\n",
    "- 🔍 **Continuous Monitoring**: Long-term performance tracking\n",
    "\n",
    "### 💡 Key Insights từ Agent Evaluation:\n",
    "\n",
    "- **Multi-Step Reasoning**: Cần evaluate cả process lẫn outcome\n",
    "- **Tool Usage Patterns**: Efficiency matters more than quantity\n",
    "- **Adaptability**: Critical for real-world deployment\n",
    "- **Context Awareness**: Essential for complex scenarios\n",
    "- **Error Recovery**: Separates good agents from great ones\n",
    "\n",
    "### 🎯 Agent Evaluation Best Practices:\n",
    "\n",
    "1. **Capture Intermediate Steps** cho detailed analysis\n",
    "2. **Test với Complex Scenarios** beyond simple queries\n",
    "3. **Measure Multiple Dimensions** of agent performance\n",
    "4. **Compare Agent Variants** để optimize configuration\n",
    "5. **Monitor Tool Usage Efficiency** để avoid waste\n",
    "6. **Evaluate Reasoning Quality** not just final answers\n",
    "\n",
    "### 📊 Agent vs Traditional System Evaluation:\n",
    "\n",
    "| Aspect | Traditional Systems | AI Agents với CoT |\n",
    "|--------|--------------------|-----------------|\n",
    "| **Evaluation Focus** | Final output only | Process + outcome |\n",
    "| **Reasoning** | Black box | Transparent steps |\n",
    "| **Tool Usage** | Fixed workflow | Dynamic selection |\n",
    "| **Adaptability** | Static rules | Learning capability |\n",
    "| **Error Handling** | Predefined paths | Creative recovery |\n",
    "| **Complexity** | Linear scaling | Emergent behavior |\n",
    "\n",
    "### 🔍 Performance Insights Summary:\n",
    "\n",
    "Từ evaluation results, chúng ta thấy:\n",
    "- Agents perform better với **well-defined tools**\n",
    "- **Planning quality** directly impacts execution success\n",
    "- **Tool efficiency** là key differentiator\n",
    "- **Complex scenarios** reveal agent limitations\n",
    "- **Reasoning transparency** enables better debugging\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Exceptional Achievement!\n",
    "\n",
    "Bạn đã mastered advanced AI agent evaluation với LangGraph và Chain-of-Thought reasoning! \n",
    "\n",
    "Ready for the final challenge: **Notebook 5: Feedback Loop & Regenerating CoT**? 🚀🔄"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}