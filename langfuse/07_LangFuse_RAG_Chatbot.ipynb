{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B√†i 7: X√¢y d·ª±ng Chatbot RAG v·ªõi LangFuse\n",
    "\n",
    "## M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "- Hi·ªÉu c√°ch thi·∫øt k·∫ø v√† tri·ªÉn khai pipeline chatbot Q&A d·ª±a tr√™n RAG\n",
    "- S·ª≠ d·ª•ng LangFuse ƒë·ªÉ theo d√µi v√† t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t chatbot\n",
    "- Tri·ªÉn khai ƒë√°nh gi√° t·ª± ƒë·ªông v√† thu th·∫≠p feedback t·ª´ ng∆∞·ªùi d√πng\n",
    "- Ph√¢n t√≠ch metrics ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gi·ªõi thi·ªáu\n",
    "\n",
    "Chatbot RAG (Retrieval-Augmented Generation) l√† m·ªôt trong nh·ªØng ·ª©ng d·ª•ng ph·ªï bi·∫øn nh·∫•t c·ªßa AI trong th·ª±c t·∫ø. Tuy nhi√™n, vi·ªác x√¢y d·ª±ng m·ªôt chatbot RAG hi·ªáu qu·∫£ g·∫∑p ph·∫£i nhi·ªÅu th√°ch th·ª©c:\n",
    "\n",
    "### üöß Th√°ch th·ª©c ch√≠nh:\n",
    "- **Hallucination**: Model t·∫°o ra th√¥ng tin sai l·ªách\n",
    "- **ƒê·ªô li√™n quan**: Truy xu·∫•t t√†i li·ªáu kh√¥ng ph√π h·ª£p v·ªõi c√¢u h·ªèi\n",
    "- **Ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi**: C√¢u tr·∫£ l·ªùi m∆° h·ªì ho·∫∑c kh√¥ng ƒë·∫ßy ƒë·ªß\n",
    "- **Hi·ªáu su·∫•t**: ƒê·ªô tr·ªÖ cao v√† s·ª≠ d·ª•ng token kh√¥ng t·ªëi ∆∞u\n",
    "- **Tr·∫£i nghi·ªám ng∆∞·ªùi d√πng**: Thi·∫øu context v√† kh·∫£ nƒÉng x·ª≠ l√Ω multi-turn conversation\n",
    "\n",
    "### üí° Gi·∫£i ph√°p v·ªõi LangFuse:\n",
    "- **Observability**: Theo d√µi to√†n b·ªô pipeline RAG\n",
    "- **Evaluation**: ƒê√°nh gi√° t·ª± ƒë·ªông ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi\n",
    "- **Optimization**: T·ªëi ∆∞u h√≥a d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø\n",
    "- **User Feedback**: Thu th·∫≠p v√† ph√¢n t√≠ch feedback ng∆∞·ªùi d√πng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√†i ƒë·∫∑t & C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# LangFuse\n",
    "from langfuse import Langfuse\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh environment variables\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # ho·∫∑c self-hosted URL\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"\n",
    "\n",
    "# Kh·ªüi t·∫°o LangFuse client\n",
    "langfuse = Langfuse()\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "print(\"‚úÖ LangFuse ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Chatbot H·ªó tr·ª£ Kh√°ch h√†ng S·∫£n ph·∫©m\n",
    "\n",
    "Ch√∫ng ta s·∫Ω x√¢y d·ª±ng m·ªôt chatbot ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ c√°c s·∫£n ph·∫©m c·ªßa m·ªôt c√¥ng ty c√¥ng ngh·ªá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ªØ li·ªáu s·∫£n ph·∫©m m·∫´u\n",
    "PRODUCT_DOCUMENTS = [\n",
    "    {\n",
    "        \"content\": \"\"\"MacBook Pro 14-inch v·ªõi chip M3 Pro l√† laptop cao c·∫•p d√†nh cho c√°c chuy√™n gia s√°ng t·∫°o. \n",
    "        S·∫£n ph·∫©m c√≥ m√†n h√¨nh Liquid Retina XDR 14.2 inch, th·ªùi l∆∞·ª£ng pin l√™n ƒë·∫øn 18 gi·ªù, \n",
    "        v√† h·ªó tr·ª£ k·∫øt n·ªëi Thunderbolt 4. Gi√° t·ª´ 49.999.000 VNƒê. \n",
    "        B·∫£o h√†nh 12 th√°ng ch√≠nh h√£ng, h·ªó tr·ª£ tr·∫£ g√≥p 0% l√£i su·∫•t.\"\"\",\n",
    "        \"metadata\": {\"product\": \"MacBook Pro 14\", \"category\": \"laptop\", \"price\": 49999000}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"iPhone 15 Pro Max l√† flagship smartphone v·ªõi camera 48MP, \n",
    "        chip A17 Pro ƒë∆∞·ª£c s·∫£n xu·∫•t tr√™n ti·∫øn tr√¨nh 3nm. M√†n h√¨nh Super Retina XDR 6.7 inch, \n",
    "        th√¢n m√°y l√†m t·ª´ titanium cao c·∫•p. Gi√° t·ª´ 32.999.000 VNƒê. \n",
    "        C√≥ c√°c m√†u: Natural Titanium, Blue Titanium, White Titanium, Black Titanium.\"\"\",\n",
    "        \"metadata\": {\"product\": \"iPhone 15 Pro Max\", \"category\": \"smartphone\", \"price\": 32999000}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"AirPods Pro (2nd generation) v·ªõi chip H2 mang l·∫°i ch·∫•t l∆∞·ª£ng √¢m thanh v∆∞·ª£t tr·ªôi. \n",
    "        T√≠nh nƒÉng ch·ªëng ·ªìn ch·ªß ƒë·ªông ƒë∆∞·ª£c c·∫£i thi·ªán g·∫•p ƒë√¥i, th·ªùi l∆∞·ª£ng nghe nh·∫°c 6 gi·ªù, \n",
    "        case s·∫°c kh√¥ng d√¢y MagSafe. Gi√° 6.999.000 VNƒê. \n",
    "        T∆∞∆°ng th√≠ch v·ªõi t·∫•t c·∫£ thi·∫øt b·ªã Apple, h·ªó tr·ª£ Spatial Audio.\"\"\",\n",
    "        \"metadata\": {\"product\": \"AirPods Pro 2\", \"category\": \"audio\", \"price\": 6999000}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Ch√≠nh s√°ch b·∫£o h√†nh: T·∫•t c·∫£ s·∫£n ph·∫©m Apple ch√≠nh h√£ng ƒë∆∞·ª£c b·∫£o h√†nh 12 th√°ng. \n",
    "        Kh√°ch h√†ng c√≥ th·ªÉ m·ªü r·ªông b·∫£o h√†nh l√™n 24 th√°ng v·ªõi ph√≠ 10% gi√° tr·ªã s·∫£n ph·∫©m. \n",
    "        B·∫£o h√†nh bao g·ªìm l·ªói k·ªπ thu·∫≠t, kh√¥ng bao g·ªìm h∆∞ h·ªèng do t√°c ƒë·ªông v·∫≠t l√Ω. \n",
    "        Trung t√¢m b·∫£o h√†nh t·∫°i H√† N·ªôi, TP.HCM, ƒê√† N·∫µng.\"\"\",\n",
    "        \"metadata\": {\"type\": \"policy\", \"category\": \"warranty\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Ch√≠nh s√°ch v·∫≠n chuy·ªÉn: Mi·ªÖn ph√≠ giao h√†ng to√†n qu·ªëc cho ƒë∆°n h√†ng t·ª´ 10 tri·ªáu. \n",
    "        Giao h√†ng nhanh trong ng√†y t·∫°i H√† N·ªôi v√† TP.HCM v·ªõi ph√≠ 50.000 VNƒê. \n",
    "        Giao h√†ng ti√™u chu·∫©n 2-3 ng√†y l√†m vi·ªác. H·ªó tr·ª£ giao h√†ng ƒë·∫øn t·∫≠n n∆°i, \n",
    "        ki·ªÉm tra s·∫£n ph·∫©m tr∆∞·ªõc khi thanh to√°n.\"\"\",\n",
    "        \"metadata\": {\"type\": \"policy\", \"category\": \"shipping\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi th√†nh Document objects\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) \n",
    "             for doc in PRODUCT_DOCUMENTS]\n",
    "\n",
    "print(f\"üìÑ ƒê√£ t·∫°o {len(documents)} t√†i li·ªáu s·∫£n ph·∫©m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X√¢y d·ª±ng Vector Store v√† RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o embeddings v√† vector store\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# T·∫°o vector store t·ª´ documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"product_knowledge\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # L·∫•y top 3 documents li√™n quan nh·∫•t\n",
    ")\n",
    "\n",
    "print(\"üîç Vector store v√† retriever ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.1,\n",
    "    callbacks=[langfuse_handler]\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Claude ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thi·∫øt k·∫ø Chatbot RAG v·ªõi Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChatMessage:\n",
    "    \"\"\"L·ªõp ƒë·∫°i di·ªán cho m·ªôt tin nh·∫Øn trong cu·ªôc h·ªôi tho·∫°i\"\"\"\n",
    "    role: str  # 'user' ho·∫∑c 'assistant'\n",
    "    content: str\n",
    "    timestamp: datetime\n",
    "    metadata: Optional[Dict] = None\n",
    "\n",
    "class RAGChatbot:\n",
    "    \"\"\"Chatbot RAG v·ªõi kh·∫£ nƒÉng multi-turn conversation v√† LangFuse tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, llm, langfuse_client):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.langfuse = langfuse_client\n",
    "        self.conversation_history: List[ChatMessage] = []\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Thi·∫øt l·∫≠p system prompt\n",
    "        self.system_prompt = \"\"\"B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp h·ªó tr·ª£ kh√°ch h√†ng v·ªÅ c√°c s·∫£n ph·∫©m Apple.\n",
    "        \n",
    "Nhi·ªám v·ª•:\n",
    "- Tr·∫£ l·ªùi c√¢u h·ªèi ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p\n",
    "- S·ª≠ d·ª•ng ng√¥n ng·ªØ th√¢n thi·ªán, chuy√™n nghi·ªáp\n",
    "- N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªß, h√£y th√†nh th·∫≠t n√≥i kh√¥ng bi·∫øt\n",
    "- ƒê·ªÅ xu·∫•t s·∫£n ph·∫©m ph√π h·ª£p n·∫øu ph√π h·ª£p\n",
    "- Lu√¥n cung c·∫•p th√¥ng tin gi√° c·∫£ v√† ch√≠nh s√°ch n·∫øu c√≥\n",
    "\n",
    "Quy t·∫Øc quan tr·ªçng:\n",
    "- KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu\n",
    "- Lu√¥n tr√≠ch d·∫´n th√¥ng tin t·ª´ ngu·ªìn ƒë∆∞·ª£c cung c·∫•p\n",
    "- N·∫øu kh√°ch h√†ng h·ªèi v·ªÅ s·∫£n ph·∫©m kh√¥ng c√≥, h√£y ƒë·ªÅ xu·∫•t s·∫£n ph·∫©m t∆∞∆°ng t·ª±\"\"\"\n",
    "    \n",
    "    @observe(name=\"query_expansion\")\n",
    "    def expand_query(self, user_question: str) -> str:\n",
    "        \"\"\"M·ªü r·ªông v√† c·∫£i thi·ªán c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ t√¨m ki·∫øm t·ªët h∆°n\"\"\"\n",
    "        \n",
    "        # L·∫•y context t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i\n",
    "        recent_context = \"\"\n",
    "        if len(self.conversation_history) > 0:\n",
    "            recent_msgs = self.conversation_history[-4:]  # L·∫•y 4 tin nh·∫Øn g·∫ßn nh·∫•t\n",
    "            recent_context = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in recent_msgs])\n",
    "        \n",
    "        expansion_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"B·∫°n l√† chuy√™n gia ph√¢n t√≠ch c√¢u h·ªèi. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:\n",
    "            1. Ph√¢n t√≠ch c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng trong context cu·ªôc h·ªôi tho·∫°i\n",
    "            2. T·∫°o ra c√¢u h·ªèi t√¨m ki·∫øm t·ªëi ∆∞u ƒë·ªÉ truy xu·∫•t th√¥ng tin li√™n quan\n",
    "            3. Ch·ªâ tr·∫£ v·ªÅ c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán, kh√¥ng gi·∫£i th√≠ch\n",
    "            \n",
    "            V√≠ d·ª•:\n",
    "            - \"Gi√° bao nhi√™u?\" -> \"Gi√° MacBook Pro 14 inch bao nhi√™u?\" (n·∫øu context ƒëang n√≥i v·ªÅ MacBook)\n",
    "            - \"C√≥ m√†u n√†o kh√°c kh√¥ng?\" -> \"iPhone 15 Pro Max c√≥ nh·ªØng m√†u n√†o?\"\n",
    "            \"\"\"),\n",
    "            (\"user\", f\"\"\"Context h·ªôi tho·∫°i g·∫ßn ƒë√¢y:\n",
    "            {recent_context}\n",
    "            \n",
    "            C√¢u h·ªèi hi·ªán t·∫°i: {user_question}\n",
    "            \n",
    "            C√¢u h·ªèi t√¨m ki·∫øm t·ªëi ∆∞u:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = expansion_prompt | self.llm | StrOutputParser()\n",
    "        expanded_query = chain.invoke({})\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input=user_question,\n",
    "            output=expanded_query,\n",
    "            metadata={\"session_id\": self.session_id}\n",
    "        )\n",
    "        \n",
    "        return expanded_query.strip()\n",
    "    \n",
    "    @observe(name=\"document_retrieval\")\n",
    "    def retrieve_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"Truy xu·∫•t t√†i li·ªáu li√™n quan\"\"\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input=query,\n",
    "            output=[{\"content\": doc.page_content[:200], \"metadata\": doc.metadata} for doc in docs],\n",
    "            metadata={\n",
    "                \"session_id\": self.session_id,\n",
    "                \"num_documents\": len(docs)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    @observe(name=\"relevance_check\")\n",
    "    def check_relevance(self, query: str, documents: List[Document]) -> Tuple[List[Document], float]:\n",
    "        \"\"\"Ki·ªÉm tra ƒë·ªô li√™n quan c·ªßa t√†i li·ªáu v·ªõi c√¢u h·ªèi\"\"\"\n",
    "        \n",
    "        if not documents:\n",
    "            return [], 0.0\n",
    "        \n",
    "        # T·∫°o prompt ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô li√™n quan\n",
    "        relevance_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"ƒê√°nh gi√° ƒë·ªô li√™n quan c·ªßa c√°c t√†i li·ªáu v·ªõi c√¢u h·ªèi.\n",
    "            Tr·∫£ v·ªÅ ƒëi·ªÉm t·ª´ 0-10 cho m·ªói t√†i li·ªáu (10 = r·∫•t li√™n quan, 0 = kh√¥ng li√™n quan).\n",
    "            Ch·ªâ tr·∫£ v·ªÅ JSON format: {\"scores\": [score1, score2, ...], \"avg_score\": average}\"\"\"),\n",
    "            (\"user\", f\"\"\"C√¢u h·ªèi: {query}\n",
    "            \n",
    "            T√†i li·ªáu:\n",
    "            {chr(10).join([f\"T√†i li·ªáu {i+1}: {doc.page_content[:300]}\" for i, doc in enumerate(documents)])}\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = relevance_prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({})\n",
    "        \n",
    "        try:\n",
    "            scores_data = json.loads(result)\n",
    "            scores = scores_data.get(\"scores\", [5.0] * len(documents))\n",
    "            avg_score = scores_data.get(\"avg_score\", 5.0)\n",
    "            \n",
    "            # L·ªçc t√†i li·ªáu c√≥ ƒëi·ªÉm >= 6\n",
    "            relevant_docs = [doc for i, doc in enumerate(documents) if scores[i] >= 6.0]\n",
    "            \n",
    "        except:\n",
    "            # Fallback: gi·ªØ nguy√™n t√†i li·ªáu\n",
    "            relevant_docs = documents\n",
    "            avg_score = 7.0\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input={\"query\": query, \"num_documents\": len(documents)},\n",
    "            output={\"relevant_documents\": len(relevant_docs), \"avg_relevance_score\": avg_score},\n",
    "            metadata={\"session_id\": self.session_id}\n",
    "        )\n",
    "        \n",
    "        return relevant_docs, avg_score\n",
    "    \n",
    "    @observe(name=\"generate_response\")\n",
    "    def generate_response(self, user_question: str, context_docs: List[Document]) -> str:\n",
    "        \"\"\"T·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context\"\"\"\n",
    "        \n",
    "        # Chu·∫©n b·ªã context\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        # Chu·∫©n b·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i\n",
    "        messages = [SystemMessage(content=self.system_prompt)]\n",
    "        \n",
    "        # Th√™m l·ªãch s·ª≠ h·ªôi tho·∫°i (gi·ªõi h·∫°n 6 tin nh·∫Øn g·∫ßn nh·∫•t)\n",
    "        recent_history = self.conversation_history[-6:] if len(self.conversation_history) > 0 else []\n",
    "        for msg in recent_history:\n",
    "            if msg.role == \"user\":\n",
    "                messages.append(HumanMessage(content=msg.content))\n",
    "            else:\n",
    "                messages.append(AIMessage(content=msg.content))\n",
    "        \n",
    "        # T·∫°o prompt cho c√¢u h·ªèi hi·ªán t·∫°i\n",
    "        current_prompt = f\"\"\"Th√¥ng tin s·∫£n ph·∫©m li√™n quan:\n",
    "{context}\n",
    "\n",
    "C√¢u h·ªèi c·ªßa kh√°ch h√†ng: {user_question}\n",
    "\n",
    "H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p ·ªü tr√™n.\"\"\"\n",
    "        \n",
    "        messages.append(HumanMessage(content=current_prompt))\n",
    "        \n",
    "        # T·∫°o response\n",
    "        response = self.llm.invoke(messages)\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input={\"question\": user_question, \"context_length\": len(context)},\n",
    "            output=response.content,\n",
    "            metadata={\n",
    "                \"session_id\": self.session_id,\n",
    "                \"num_context_docs\": len(context_docs)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    @observe(name=\"chatbot_conversation\")\n",
    "    def chat(self, user_message: str, intent: str = \"general\") -> Dict:\n",
    "        \"\"\"X·ª≠ l√Ω m·ªôt l∆∞·ª£t h·ªôi tho·∫°i\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. M·ªü r·ªông c√¢u h·ªèi\n",
    "            expanded_query = self.expand_query(user_message)\n",
    "            \n",
    "            # 2. Truy xu·∫•t t√†i li·ªáu\n",
    "            retrieved_docs = self.retrieve_documents(expanded_query)\n",
    "            \n",
    "            # 3. Ki·ªÉm tra ƒë·ªô li√™n quan\n",
    "            relevant_docs, relevance_score = self.check_relevance(user_message, retrieved_docs)\n",
    "            \n",
    "            # 4. T·∫°o c√¢u tr·∫£ l·ªùi\n",
    "            if relevant_docs and relevance_score >= 5.0:\n",
    "                response = self.generate_response(user_message, relevant_docs)\n",
    "            else:\n",
    "                response = \"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ c√°c s·∫£n ph·∫©m MacBook, iPhone, AirPods ho·∫∑c ch√≠nh s√°ch b·∫£o h√†nh, v·∫≠n chuy·ªÉn kh√¥ng?\"\n",
    "            \n",
    "            # 5. L∆∞u v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i\n",
    "            user_msg = ChatMessage(\"user\", user_message, datetime.now())\n",
    "            assistant_msg = ChatMessage(\"assistant\", response, datetime.now())\n",
    "            \n",
    "            self.conversation_history.append(user_msg)\n",
    "            self.conversation_history.append(assistant_msg)\n",
    "            \n",
    "            # 6. T√≠nh to√°n metrics\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            result = {\n",
    "                \"response\": response,\n",
    "                \"metadata\": {\n",
    "                    \"session_id\": self.session_id,\n",
    "                    \"latency\": latency,\n",
    "                    \"relevance_score\": relevance_score,\n",
    "                    \"num_relevant_docs\": len(relevant_docs),\n",
    "                    \"expanded_query\": expanded_query,\n",
    "                    \"intent\": intent\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Update LangFuse observation\n",
    "            langfuse_context.update_current_observation(\n",
    "                input=user_message,\n",
    "                output=response,\n",
    "                metadata=result[\"metadata\"],\n",
    "                tags=[\"chatbot\", \"rag\", intent, f\"session:{self.session_id}\"]\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_response = f\"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}\"\n",
    "            \n",
    "            langfuse_context.update_current_observation(\n",
    "                input=user_message,\n",
    "                output=error_response,\n",
    "                level=\"ERROR\",\n",
    "                metadata={\"error\": str(e), \"session_id\": self.session_id}\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"response\": error_response,\n",
    "                \"metadata\": {\"error\": True, \"session_id\": self.session_id}\n",
    "            }\n",
    "    \n",
    "    def get_conversation_summary(self) -> Dict:\n",
    "        \"\"\"L·∫•y t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i\"\"\"\n",
    "        return {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"total_messages\": len(self.conversation_history),\n",
    "            \"start_time\": self.conversation_history[0].timestamp if self.conversation_history else None,\n",
    "            \"last_message_time\": self.conversation_history[-1].timestamp if self.conversation_history else None\n",
    "        }\n",
    "\n",
    "print(\"ü§ñ RAGChatbot class ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Chatbot v√† LangFuse Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o chatbot\n",
    "chatbot = RAGChatbot(retriever, llm, langfuse)\n",
    "\n",
    "print(f\"üéØ Chatbot ƒë√£ s·∫µn s√†ng! Session ID: {chatbot.session_id}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO CU·ªòC H·ªòI THO·∫†I\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation 1: H·ªèi v·ªÅ s·∫£n ph·∫©m\n",
    "print(\"\\nüë§ User: T√¥i mu·ªën mua laptop ƒë·ªÉ l√†m vi·ªác v·ªõi thi·∫øt k·∫ø, b·∫°n c√≥ th·ªÉ gi·ªõi thi·ªáu kh√¥ng?\")\n",
    "\n",
    "result1 = chatbot.chat(\n",
    "    \"T√¥i mu·ªën mua laptop ƒë·ªÉ l√†m vi·ªác v·ªõi thi·∫øt k·∫ø, b·∫°n c√≥ th·ªÉ gi·ªõi thi·ªáu kh√¥ng?\",\n",
    "    intent=\"product_inquiry\"\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Assistant: {result1['response']}\")\n",
    "print(f\"\\nüìä Metadata: Latency: {result1['metadata']['latency']:.2f}s, Relevance: {result1['metadata']['relevance_score']:.1f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation 2: Follow-up question\n",
    "print(\"\\nüë§ User: Gi√° bao nhi√™u v√† c√≥ tr·∫£ g√≥p kh√¥ng?\")\n",
    "\n",
    "result2 = chatbot.chat(\n",
    "    \"Gi√° bao nhi√™u v√† c√≥ tr·∫£ g√≥p kh√¥ng?\",\n",
    "    intent=\"pricing_inquiry\"\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Assistant: {result2['response']}\")\n",
    "print(f\"\\nüìä Metadata: Latency: {result2['metadata']['latency']:.2f}s, Relevance: {result2['metadata']['relevance_score']:.1f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation 3: H·ªèi v·ªÅ smartphone\n",
    "print(\"\\nüë§ User: C√≤n ƒëi·ªán tho·∫°i th√¨ sao? iPhone m·ªõi nh·∫•t c√≥ g√¨ ƒë·∫∑c bi·ªát?\")\n",
    "\n",
    "result3 = chatbot.chat(\n",
    "    \"C√≤n ƒëi·ªán tho·∫°i th√¨ sao? iPhone m·ªõi nh·∫•t c√≥ g√¨ ƒë·∫∑c bi·ªát?\",\n",
    "    intent=\"product_inquiry\"\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Assistant: {result3['response']}\")\n",
    "print(f\"\\nüìä Metadata: Latency: {result3['metadata']['latency']:.2f}s, Relevance: {result3['metadata']['relevance_score']:.1f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation 4: H·ªèi v·ªÅ ch√≠nh s√°ch\n",
    "print(\"\\nüë§ User: N·∫øu t√¥i mua c·∫£ laptop v√† ƒëi·ªán tho·∫°i th√¨ c√≥ mi·ªÖn ph√≠ ship kh√¥ng?\")\n",
    "\n",
    "result4 = chatbot.chat(\n",
    "    \"N·∫øu t√¥i mua c·∫£ laptop v√† ƒëi·ªán tho·∫°i th√¨ c√≥ mi·ªÖn ph√≠ ship kh√¥ng?\",\n",
    "    intent=\"policy_inquiry\"\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Assistant: {result4['response']}\")\n",
    "print(f\"\\nüìä Metadata: Latency: {result4['metadata']['latency']:.2f}s, Relevance: {result4['metadata']['relevance_score']:.1f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i\n",
    "summary = chatbot.get_conversation_summary()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"T√ìM T·∫ÆT CU·ªòC H·ªòI THO·∫†I\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Session ID: {summary['session_id']}\")\n",
    "print(f\"T·ªïng s·ªë tin nh·∫Øn: {summary['total_messages']}\")\n",
    "print(f\"Th·ªùi gian b·∫Øt ƒë·∫ßu: {summary['start_time']}\")\n",
    "print(f\"Tin nh·∫Øn cu·ªëi: {summary['last_message_time']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ƒê√°nh gi√° T·ª± ƒë·ªông v√† Thu th·∫≠p Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotEvaluator:\n",
    "    \"\"\"L·ªõp ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, langfuse_client):\n",
    "        self.llm = llm\n",
    "        self.langfuse = langfuse_client\n",
    "    \n",
    "    @observe(name=\"hallucination_check\")\n",
    "    def check_hallucination(self, question: str, answer: str, context: str) -> Dict:\n",
    "        \"\"\"Ki·ªÉm tra hallucination trong c√¢u tr·∫£ l·ªùi\"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"B·∫°n l√† chuy√™n gia ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√¢u tr·∫£ l·ªùi AI.\n",
    "            Nhi·ªám v·ª•: Ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c√≥ b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong context kh√¥ng.\n",
    "            \n",
    "            Tr·∫£ v·ªÅ JSON:\n",
    "            {\n",
    "                \"has_hallucination\": true/false,\n",
    "                \"confidence\": 0.0-1.0,\n",
    "                \"explanation\": \"gi·∫£i th√≠ch ng·∫Øn g·ªçn\"\n",
    "            }\"\"\"),\n",
    "            (\"user\", f\"\"\"Context: {context}\n",
    "            \n",
    "            C√¢u h·ªèi: {question}\n",
    "            C√¢u tr·∫£ l·ªùi: {answer}\n",
    "            \n",
    "            ƒê√°nh gi√°:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({})\n",
    "        \n",
    "        try:\n",
    "            evaluation = json.loads(result)\n",
    "        except:\n",
    "            evaluation = {\n",
    "                \"has_hallucination\": False,\n",
    "                \"confidence\": 0.5,\n",
    "                \"explanation\": \"Kh√¥ng th·ªÉ ph√¢n t√≠ch\"\n",
    "            }\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input={\"question\": question, \"answer\": answer[:200]},\n",
    "            output=evaluation,\n",
    "            metadata={\"evaluation_type\": \"hallucination\"}\n",
    "        )\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "    @observe(name=\"answer_quality_check\")\n",
    "    def evaluate_answer_quality(self, question: str, answer: str) -> Dict:\n",
    "        \"\"\"ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ c·ªßa c√¢u tr·∫£ l·ªùi\"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi theo c√°c ti√™u ch√≠:\n",
    "            - Helpfulness (1-5): C√¢u tr·∫£ l·ªùi c√≥ h·ªØu √≠ch kh√¥ng?\n",
    "            - Clarity (1-5): C√¢u tr·∫£ l·ªùi c√≥ r√µ r√†ng, d·ªÖ hi·ªÉu kh√¥ng?\n",
    "            - Completeness (1-5): C√¢u tr·∫£ l·ªùi c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin kh√¥ng?\n",
    "            - Professional (1-5): C√¢u tr·∫£ l·ªùi c√≥ chuy√™n nghi·ªáp kh√¥ng?\n",
    "            \n",
    "            Tr·∫£ v·ªÅ JSON:\n",
    "            {\n",
    "                \"helpfulness\": 1-5,\n",
    "                \"clarity\": 1-5,\n",
    "                \"completeness\": 1-5,\n",
    "                \"professional\": 1-5,\n",
    "                \"overall_score\": trung_b√¨nh,\n",
    "                \"feedback\": \"nh·∫≠n x√©t ng·∫Øn\"\n",
    "            }\"\"\"),\n",
    "            (\"user\", f\"\"\"C√¢u h·ªèi: {question}\n",
    "            C√¢u tr·∫£ l·ªùi: {answer}\n",
    "            \n",
    "            ƒê√°nh gi√°:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        result = chain.invoke({})\n",
    "        \n",
    "        try:\n",
    "            evaluation = json.loads(result)\n",
    "            if \"overall_score\" not in evaluation:\n",
    "                scores = [evaluation.get(k, 3) for k in [\"helpfulness\", \"clarity\", \"completeness\", \"professional\"]]\n",
    "                evaluation[\"overall_score\"] = sum(scores) / len(scores)\n",
    "        except:\n",
    "            evaluation = {\n",
    "                \"helpfulness\": 3,\n",
    "                \"clarity\": 3,\n",
    "                \"completeness\": 3,\n",
    "                \"professional\": 3,\n",
    "                \"overall_score\": 3.0,\n",
    "                \"feedback\": \"Kh√¥ng th·ªÉ ƒë√°nh gi√°\"\n",
    "            }\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input={\"question\": question, \"answer\": answer[:200]},\n",
    "            output=evaluation,\n",
    "            metadata={\"evaluation_type\": \"quality\"}\n",
    "        )\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "    def submit_user_feedback(self, session_id: str, message_id: str, \n",
    "                           feedback_type: str, rating: int, comment: str = \"\"):\n",
    "        \"\"\"Thu th·∫≠p feedback t·ª´ ng∆∞·ªùi d√πng\"\"\"\n",
    "        \n",
    "        # T·∫°o score trong LangFuse\n",
    "        self.langfuse.score(\n",
    "            trace_id=session_id,  # ho·∫∑c observation_id n·∫øu c√≥\n",
    "            name=f\"user_{feedback_type}\",\n",
    "            value=rating,\n",
    "            comment=comment,\n",
    "            data_type=\"NUMERIC\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ ƒê√£ ghi nh·∫≠n feedback: {feedback_type} = {rating}/5\")\n",
    "        if comment:\n",
    "            print(f\"üí¨ B√¨nh lu·∫≠n: {comment}\")\n",
    "\n",
    "# Kh·ªüi t·∫°o evaluator\n",
    "evaluator = ChatbotEvaluator(llm, langfuse)\n",
    "\n",
    "print(\"üìã ChatbotEvaluator ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo ƒë√°nh gi√° t·ª± ƒë·ªông cho cu·ªôc h·ªôi tho·∫°i v·ª´a r·ªìi\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ƒê√ÅNH GI√Å T·ª∞ ƒê·ªòNG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L·∫•y c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi cu·ªëi c√πng\n",
    "last_question = \"N·∫øu t√¥i mua c·∫£ laptop v√† ƒëi·ªán tho·∫°i th√¨ c√≥ mi·ªÖn ph√≠ ship kh√¥ng?\"\n",
    "last_answer = result4['response']\n",
    "context = \"Ch√≠nh s√°ch v·∫≠n chuy·ªÉn: Mi·ªÖn ph√≠ giao h√†ng to√†n qu·ªëc cho ƒë∆°n h√†ng t·ª´ 10 tri·ªáu.\"\n",
    "\n",
    "# Ki·ªÉm tra hallucination\n",
    "print(\"\\nüîç Ki·ªÉm tra Hallucination...\")\n",
    "hallucination_check = evaluator.check_hallucination(last_question, last_answer, context)\n",
    "print(f\"K·∫øt qu·∫£: {hallucination_check}\")\n",
    "\n",
    "# ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng\n",
    "print(\"\\n‚≠ê ƒê√°nh gi√° Ch·∫•t l∆∞·ª£ng...\")\n",
    "quality_check = evaluator.evaluate_answer_quality(last_question, last_answer)\n",
    "print(f\"K·∫øt qu·∫£: {quality_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo thu th·∫≠p user feedback\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"THU TH·∫¨P USER FEEDBACK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Gi·∫£ l·∫≠p feedback t·ª´ ng∆∞·ªùi d√πng\n",
    "print(\"\\nüë§ User feedback cho cu·ªôc h·ªôi tho·∫°i:\")\n",
    "\n",
    "# Feedback t√≠ch c·ª±c\n",
    "evaluator.submit_user_feedback(\n",
    "    session_id=chatbot.session_id,\n",
    "    message_id=\"msg_1\",\n",
    "    feedback_type=\"helpfulness\",\n",
    "    rating=5,\n",
    "    comment=\"Chatbot r·∫•t h·ªØu √≠ch, tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m\"\n",
    ")\n",
    "\n",
    "evaluator.submit_user_feedback(\n",
    "    session_id=chatbot.session_id,\n",
    "    message_id=\"msg_2\",\n",
    "    feedback_type=\"satisfaction\",\n",
    "    rating=4,\n",
    "    comment=\"T·ªët, nh∆∞ng c√≥ th·ªÉ c·∫ßn th√™m th√¥ng tin v·ªÅ m√†u s·∫Øc s·∫£n ph·∫©m\"\n",
    ")\n",
    "\n",
    "# Feedback ti√™u c·ª±c\n",
    "evaluator.submit_user_feedback(\n",
    "    session_id=chatbot.session_id,\n",
    "    message_id=\"msg_3\",\n",
    "    feedback_type=\"response_time\",\n",
    "    rating=3,\n",
    "    comment=\"Ph·∫£n h·ªìi h∆°i ch·∫≠m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ph√¢n t√≠ch Metrics v√† T·ªëi ∆∞u h√≥a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chatbot_performance(session_id: str, langfuse_client):\n",
    "    \"\"\"Ph√¢n t√≠ch hi·ªáu su·∫•t chatbot t·ª´ LangFuse\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä PH√ÇN T√çCH HI·ªÜU SU·∫§T CHATBOT\")\n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # L·∫•y d·ªØ li·ªáu t·ª´ LangFuse (th·ª±c t·∫ø c·∫ßn s·ª≠ d·ª•ng API)\n",
    "    # ·ªû ƒë√¢y ch√∫ng ta s·∫Ω m√¥ ph·ªèng d·ªØ li·ªáu\n",
    "    \n",
    "    mock_data = {\n",
    "        \"total_conversations\": 4,\n",
    "        \"avg_latency\": 2.3,\n",
    "        \"avg_relevance_score\": 8.2,\n",
    "        \"user_satisfaction\": 4.2,\n",
    "        \"hallucination_rate\": 0.05,\n",
    "        \"intent_distribution\": {\n",
    "            \"product_inquiry\": 50,\n",
    "            \"pricing_inquiry\": 25,\n",
    "            \"policy_inquiry\": 25\n",
    "        },\n",
    "        \"token_usage\": {\n",
    "            \"total_input_tokens\": 1250,\n",
    "            \"total_output_tokens\": 890,\n",
    "            \"total_cost_usd\": 0.045\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ KPI Ch√≠nh:\")\n",
    "    print(f\"  ‚Ä¢ T·ªïng s·ªë cu·ªôc h·ªôi tho·∫°i: {mock_data['total_conversations']}\")\n",
    "    print(f\"  ‚Ä¢ ƒê·ªô tr·ªÖ trung b√¨nh: {mock_data['avg_latency']:.1f}s\")\n",
    "    print(f\"  ‚Ä¢ ƒêi·ªÉm li√™n quan trung b√¨nh: {mock_data['avg_relevance_score']:.1f}/10\")\n",
    "    print(f\"  ‚Ä¢ ƒê·ªô h√†i l√≤ng ng∆∞·ªùi d√πng: {mock_data['user_satisfaction']:.1f}/5\")\n",
    "    print(f\"  ‚Ä¢ T·ª∑ l·ªá hallucination: {mock_data['hallucination_rate']*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüé≠ Ph√¢n b·ªë Intent:\")\n",
    "    for intent, percentage in mock_data['intent_distribution'].items():\n",
    "        print(f\"  ‚Ä¢ {intent}: {percentage}%\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Chi ph√≠ Token:\")\n",
    "    print(f\"  ‚Ä¢ Input tokens: {mock_data['token_usage']['total_input_tokens']:,}\")\n",
    "    print(f\"  ‚Ä¢ Output tokens: {mock_data['token_usage']['total_output_tokens']:,}\")\n",
    "    print(f\"  ‚Ä¢ T·ªïng chi ph√≠: ${mock_data['token_usage']['total_cost_usd']:.3f}\")\n",
    "    \n",
    "    # ƒê·ªÅ xu·∫•t t·ªëi ∆∞u h√≥a\n",
    "    print(f\"\\nüöÄ ƒê·ªÅ xu·∫•t T·ªëi ∆∞u h√≥a:\")\n",
    "    \n",
    "    if mock_data['avg_latency'] > 3.0:\n",
    "        print(\"  ‚ö†Ô∏è  ƒê·ªô tr·ªÖ cao - C·∫ßn t·ªëi ∆∞u h√≥a retrieval ho·∫∑c caching\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ ƒê·ªô tr·ªÖ trong m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c\")\n",
    "    \n",
    "    if mock_data['avg_relevance_score'] < 7.0:\n",
    "        print(\"  ‚ö†Ô∏è  ƒêi·ªÉm li√™n quan th·∫•p - C·∫ßn c·∫£i thi·ªán embeddings ho·∫∑c chunking\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ ƒê·ªô li√™n quan t√†i li·ªáu t·ªët\")\n",
    "    \n",
    "    if mock_data['user_satisfaction'] < 4.0:\n",
    "        print(\"  ‚ö†Ô∏è  ƒê·ªô h√†i l√≤ng ch∆∞a cao - C·∫ßn c·∫£i thi·ªán prompt v√† training data\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Ng∆∞·ªùi d√πng h√†i l√≤ng v·ªõi chatbot\")\n",
    "    \n",
    "    if mock_data['hallucination_rate'] > 0.1:\n",
    "        print(\"  ‚ö†Ô∏è  T·ª∑ l·ªá hallucination cao - C·∫ßn tƒÉng c∆∞·ªùng fact-checking\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ T·ª∑ l·ªá hallucination trong m·ª©c an to√†n\")\n",
    "    \n",
    "    return mock_data\n",
    "\n",
    "# Ph√¢n t√≠ch hi·ªáu su·∫•t\n",
    "performance_data = analyze_chatbot_performance(chatbot.session_id, langfuse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C·∫£i ti·∫øn Prompt v√† Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• v·ªÅ prompt optimization d·ª±a tr√™n feedback\n",
    "def optimize_system_prompt(performance_metrics: Dict) -> str:\n",
    "    \"\"\"T·ªëi ∆∞u h√≥a system prompt d·ª±a tr√™n performance metrics\"\"\"\n",
    "    \n",
    "    base_prompt = \"\"\"B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp h·ªó tr·ª£ kh√°ch h√†ng v·ªÅ c√°c s·∫£n ph·∫©m Apple.\"\"\"\n",
    "    \n",
    "    improvements = []\n",
    "    \n",
    "    if performance_metrics['hallucination_rate'] > 0.05:\n",
    "        improvements.append(\n",
    "            \"QUAN TR·ªåNG: Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p. \"\n",
    "            \"Tuy·ªát ƒë·ªëi kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin.\"\n",
    "        )\n",
    "    \n",
    "    if performance_metrics['user_satisfaction'] < 4.0:\n",
    "        improvements.append(\n",
    "            \"Lu√¥n:\"\n",
    "            \"- Cung c·∫•p th√¥ng tin c·ª• th·ªÉ v√† h·ªØu √≠ch\\n\"\n",
    "            \"- S·ª≠ d·ª•ng ng√¥n ng·ªØ th√¢n thi·ªán, d·ªÖ hi·ªÉu\\n\"\n",
    "            \"- ƒê·ªÅ xu·∫•t s·∫£n ph·∫©m ph√π h·ª£p v·ªõi nhu c·∫ßu kh√°ch h√†ng\"\n",
    "        )\n",
    "    \n",
    "    if performance_metrics['avg_relevance_score'] < 7.0:\n",
    "        improvements.append(\n",
    "            \"Khi tr·∫£ l·ªùi:\"\n",
    "            \"- T·∫≠p trung v√†o th√¥ng tin tr·ª±c ti·∫øp li√™n quan ƒë·∫øn c√¢u h·ªèi\\n\"\n",
    "            \"- ∆Øu ti√™n th√¥ng tin quan tr·ªçng nh·∫•t tr∆∞·ªõc\\n\"\n",
    "            \"- N·∫øu c√¢u h·ªèi kh√¥ng r√µ r√†ng, h·ªèi l·∫°i ƒë·ªÉ l√†m r√µ\"\n",
    "        )\n",
    "    \n",
    "    optimized_prompt = base_prompt\n",
    "    if improvements:\n",
    "        optimized_prompt += \"\\n\\n\" + \"\\n\\n\".join(improvements)\n",
    "    \n",
    "    return optimized_prompt\n",
    "\n",
    "# T·∫°o prompt ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a\n",
    "optimized_prompt = optimize_system_prompt(performance_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROMPT OPTIMIZATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nüîß System Prompt ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a:\")\n",
    "print(\"\\n\" + optimized_prompt)\n",
    "\n",
    "# C·∫≠p nh·∫≠t prompt cho chatbot\n",
    "chatbot.system_prompt = optimized_prompt\n",
    "print(\"\\n‚úÖ ƒê√£ c·∫≠p nh·∫≠t system prompt cho chatbot!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test v·ªõi Prompt M·ªõi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chatbot v·ªõi prompt ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST V·ªöI PROMPT ƒê√É T·ªêI ∆ØU H√ìA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüë§ User: T√¥i c√≥ budget 30 tri·ªáu, mu·ªën mua ƒëi·ªán tho·∫°i ch·ª•p ·∫£nh ƒë·∫πp. C√≥ g·ª£i √Ω g√¨ kh√¥ng?\")\n",
    "\n",
    "result_optimized = chatbot.chat(\n",
    "    \"T√¥i c√≥ budget 30 tri·ªáu, mu·ªën mua ƒëi·ªán tho·∫°i ch·ª•p ·∫£nh ƒë·∫πp. C√≥ g·ª£i √Ω g√¨ kh√¥ng?\",\n",
    "    intent=\"product_recommendation\"\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Assistant (Optimized): {result_optimized['response']}\")\n",
    "print(f\"\\nüìä Metadata: Latency: {result_optimized['metadata']['latency']:.2f}s, Relevance: {result_optimized['metadata']['relevance_score']:.1f}/10\")\n",
    "\n",
    "# So s√°nh performance\n",
    "print(\"\\nüìà So s√°nh Performance:\")\n",
    "print(f\"ƒê·ªô tr·ªÖ: {result_optimized['metadata']['latency']:.2f}s\")\n",
    "print(f\"ƒê·ªô li√™n quan: {result_optimized['metadata']['relevance_score']:.1f}/10\")\n",
    "print(f\"Expanded query: {result_optimized['metadata']['expanded_query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gi·∫£i th√≠ch & Ph√¢n t√≠ch\n",
    "\n",
    "### üéØ L·ª£i √≠ch c·ªßa LangFuse trong RAG Chatbot:\n",
    "\n",
    "1. **End-to-End Observability**:\n",
    "   - Theo d√µi to√†n b·ªô pipeline t·ª´ query expansion ‚Üí retrieval ‚Üí generation\n",
    "   - ƒêo l∆∞·ªùng latency v√† token usage c·ªßa t·ª´ng b∆∞·ªõc\n",
    "   - Ph√°t hi·ªán bottlenecks v√† t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t\n",
    "\n",
    "2. **Quality Assurance**:\n",
    "   - ƒê√°nh gi√° t·ª± ƒë·ªông hallucination v√† ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi\n",
    "   - Thu th·∫≠p feedback ng∆∞·ªùi d√πng th·ª±c t·∫ø\n",
    "   - Tracking metrics quan tr·ªçng: relevance, satisfaction, accuracy\n",
    "\n",
    "3. **Continuous Improvement**:\n",
    "   - Ph√¢n t√≠ch pattern t·ª´ d·ªØ li·ªáu th·ª±c t·∫ø\n",
    "   - A/B testing c√°c version prompt kh√°c nhau\n",
    "   - T·ªëi ∆∞u h√≥a d·ª±a tr√™n user behavior v√† feedback\n",
    "\n",
    "4. **Production Monitoring**:\n",
    "   - Real-time alerts khi c√≥ issues\n",
    "   - Cost tracking v√† budget management\n",
    "   - Performance benchmarking theo th·ªùi gian\n",
    "\n",
    "### üîç Insights t·ª´ Demo:\n",
    "\n",
    "- **Multi-turn Conversation**: LangFuse tracking gi√∫p hi·ªÉu context flow qua nhi·ªÅu l∆∞·ª£t h·ªôi tho·∫°i\n",
    "- **Intent Classification**: Tags gi√∫p ph√¢n t√≠ch user behavior patterns\n",
    "- **Quality Control**: Automated evaluation gi√∫p detect issues s·ªõm\n",
    "- **User Experience**: Feedback collection cung c·∫•p ground truth cho improvement\n",
    "\n",
    "### üöÄ Production Best Practices:\n",
    "\n",
    "1. **Monitoring Strategy**:\n",
    "   - Set up alerts cho latency > threshold\n",
    "   - Monitor hallucination rate daily\n",
    "   - Track user satisfaction trends\n",
    "\n",
    "2. **Data Collection**:\n",
    "   - Collect diverse training examples\n",
    "   - Annotate good/bad responses\n",
    "   - Regular evaluation dataset updates\n",
    "\n",
    "3. **Iterative Improvement**:\n",
    "   - Weekly prompt optimization\n",
    "   - Monthly model evaluation\n",
    "   - Quarterly architecture review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T√†i li·ªáu Tham kh·∫£o\n",
    "\n",
    "### üìö LangFuse Documentation:\n",
    "- [Tracing](https://langfuse.com/docs/tracing)\n",
    "- [Python SDK](https://langfuse.com/docs/sdk/python)\n",
    "- [Evaluation](https://langfuse.com/docs/scores/overview)\n",
    "- [Prompt Management](https://langfuse.com/docs/prompts/get-started)\n",
    "\n",
    "### üîó LangChain Integration:\n",
    "- [LangChain Callback](https://langfuse.com/docs/integrations/langchain/tracing)\n",
    "- [RAG with LangFuse](https://langfuse.com/docs/cookbook/integration_langchain_rag)\n",
    "\n",
    "### üìñ RAG Best Practices:\n",
    "- [Advanced RAG Techniques](https://langfuse.com/blog/rag-evaluation)\n",
    "- [Production RAG Systems](https://langfuse.com/docs/cookbook/rag_evaluation_with_ragas)\n",
    "\n",
    "### üéØ Evaluation & Monitoring:\n",
    "- [Human Feedback](https://langfuse.com/docs/scores/user-feedback)\n",
    "- [Model-based Evaluation](https://langfuse.com/docs/scores/model-based-evals)\n",
    "- [Production Analytics](https://langfuse.com/docs/analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K·∫øt lu·∫≠n & B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "### ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc:\n",
    "- Thi·∫øt k·∫ø v√† tri·ªÉn khai RAG chatbot v·ªõi multi-turn conversation\n",
    "- S·ª≠ d·ª•ng LangFuse ƒë·ªÉ theo d√µi v√† ƒë√°nh gi√° hi·ªáu su·∫•t\n",
    "- Tri·ªÉn khai automated evaluation v√† user feedback collection\n",
    "- T·ªëi ∆∞u h√≥a prompt d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø\n",
    "\n",
    "### üéØ B∆∞·ªõc ti·∫øp theo:\n",
    "\n",
    "1. **M·ªü r·ªông Knowledge Base**:\n",
    "   - Th√™m nhi·ªÅu lo·∫°i t√†i li·ªáu (video, audio, structured data)\n",
    "   - Implement semantic chunking strategies\n",
    "   - Multi-modal RAG v·ªõi images\n",
    "\n",
    "2. **Advanced Features**:\n",
    "   - Tool calling integration (inventory check, order placement)\n",
    "   - Multi-language support\n",
    "   - Personalization based on user history\n",
    "\n",
    "3. **Production Deployment**:\n",
    "   - Containerize v·ªõi Docker\n",
    "   - Setup CI/CD pipeline\n",
    "   - Load balancing v√† scaling\n",
    "\n",
    "4. **Advanced Analytics**:\n",
    "   - Customer journey analysis\n",
    "   - Conversion rate optimization\n",
    "   - A/B testing framework\n",
    "\n",
    "### üí° Key Takeaways:\n",
    "- **Observability is crucial** cho production RAG systems\n",
    "- **User feedback** l√† ground truth quan tr·ªçng nh·∫•t\n",
    "- **Iterative improvement** d·ª±a tr√™n data l√† ch√¨a kh√≥a th√†nh c√¥ng\n",
    "- **LangFuse provides comprehensive tooling** cho to√†n b·ªô ML lifecycle\n",
    "\n",
    "Ch√∫c m·ª´ng! B·∫°n ƒë√£ th√†nh th·∫°o vi·ªác x√¢y d·ª±ng v√† t·ªëi ∆∞u h√≥a RAG chatbot v·ªõi LangFuse! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}