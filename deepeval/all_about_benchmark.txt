1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:the lack of reasoning-focused coding benchmarks and self-contained training
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:The first challenge is the lack of coding benchmarks that accurately assess LLMs’ reasoning abilities.
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:LiveCodeBench (Jain et al., 2024), a commonly used benchmark, addresses this by sourcing problems
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:for benchmarking, while those released earlier constitute the training set.
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:Finally, we developed the LeetCodeDataset, which features broad coverage, reliable benchmarking,
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarking strikes an effective balance between bias and variance.
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:evaluated them across four benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021),
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:(post-2024-07), the 2.6K-trained model underperformed on hard benchmarks. It suggests
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:Code Generation Benchmarks. Numerous benchmarks have been developed to evaluate the code
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:generation capabilities of LLMs. For foundational Python programming, widely used benchmarks
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarks by translating them into 18 other programming languages. As LLM capabilities
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:advance, many of these benchmarks are becoming too easy to assess modern models adequately. A
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:few specialized benchmarks focus on competitive programming challenges. APPS (Hendrycks et al.,
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:high-quality instructional data for coding tasks. In competitive programming benchmarks like
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:While our LeetCode dataset effectively benchmarks and fine-tunes code models, it has three key
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarking and supports longitudinal studies. This dataset comprehensively covers algorithms
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:can match the performance of those trained on 110K examples from previous benchmarks, demon-
1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarking neural code generation, 2022. URL https://arxiv.org/abs/2208.08227 .
1.3.2.AgentCoder. Multi-Agent-based Code Generation with Iterative Testing and Optimisation2312.13010v3.txt:series [4, 29] developed by OpenAI, have consistently set the benchmark for performance across
1.3.2.AgentCoder. Multi-Agent-based Code Generation with Iterative Testing and Optimisation2312.13010v3.txt:from codegeex.benchmark.execution import check_correctness
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:HumanEval benchmark [25]. To ensure the generalizability
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:benchmark as well. Specifically, we selected nine other
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:generated CoTs on code generation benchmarks (such as
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:manEval, HumanEval-plus, and OpenEval benchmarks by
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:Similarly, on the OpenEval benchmark, COTTON boosts the
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:experiments on a comprehensive set of benchmarks.
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:another dataset to benchmark the code generation
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:machine learning benchmark dataset for code understanding
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:code generation with multilingual benchmarking on humaneval-
1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:Standardizing and benchmarking interactive coding with execu-
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmarks.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2-
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:with test benchmarks. More broadly, this practice hinders scientific progress as other research teams cannot
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:assess and compare the performance of these models on a suite of code LLM benchmarks (Cassano et al.,
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:DeepSeekCoder-1.3B) on most benchmarks. Moreover, it matches or surpasses the performance of
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:code completion benchmarks for high-resource languages. However, StarCoder2-15B matches or
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:and Perl). Moreover, when we consider benchmarks that require models to reason about code
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:•APPS (train) (Hendrycks et al., 2021) is a popular text2code benchmark in Python with a train
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:•GSM8K (train) (Cobbe et al., 2021) is the train split of GSM8K, a popular evaluation benchmark
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:We apply several preprocessing steps, such as deduplication (§3.1), PII redaction (§3.2), benchmark decon-
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:To ensure the performance of StarCoder is not artificially inflated on our test benchmarks, we decontaminate
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:We evaluate the StarCoder2 models on a variety of benchmarks from the literature and compare them to
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:outperforms the extra-large models in several benchmarks.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmarks HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are two of the
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:most widely studied benchmarks for Code LLMs. Each benchmark has a few hundred programming problems.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Recently, Liu et al. (2023a) identified several issues with both benchmarks. (1) Most problems have inadequate
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:these problems. The resulting benchmarks (HumanEval+ and MBPP+) have 80 ×and 35×more tests than
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:the original benchmarks. For rigorous evaluation, we adopt the EvalPlus framework in this study.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:greedy decoding and report the mean pass@1 (mean success rate) for all problems in the benchmark.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:16.8% and 9.6% on these benchmarks.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Although EvalPlus makes HumanEval and MBPP far more robust, the problems in these benchmarks only
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark MultiPL-E (Cassano et al., 2023b) uses a suite of lightweight, rule-based compilers
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark with the same problems translated to different languages.26
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark DS-1000 (Lai et al., 2023) is a widely studied benchmark with 1,000 data science
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark HumanEvalFix (Muennighoff et al., 2024a) is a benchmark that tests a model’s
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:ability to identify and fix bugs in code. The benchmark supports six programming languages shown in
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Figure 12. Since it is not a code completion benchmark, most base models do poorly on HumanEvalFix
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:2024a; Zhuo et al., 2024; Longpre et al., 2023). We benchmarked the default HumanEvalFixTests subvariant;
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:via Ben Allal et al. (2022), and we denote this as the “Issue” prompt. We also benchmark StarCoder2 with
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark CanItEdit (Cassano et al., 2024) is a hand-crafted benchmark designed to evaluate
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:hidden test suite, and pass@1 is reported. The benchmark encompasses a variety of problems, from simple
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 13: Performance of instructional code editing on the CanItEdit benchmark (Cassano et al., 2024).
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:The results for non-StarCoder2 models are from the benchmark paper.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Hyperparameters We evaluate all sizes of StarCoder2 on the CanItEdit benchmark using the Issue prompt
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark. Following Cassano et al. (2024), we employ random sampling with a temperature of 0.2and a
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark We use the widely studied GSM8K benchmark (Cobbe et al., 2021), a set of
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 14: 8-shot accuracy on the GSM8K math-reasoning benchmark.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark CRUXEval (Gu et al., 2024) is a two-part benchmark consisting of 800samples
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:of the benchmark were generated by CodeLlama-34B and then filtered to remove complicated functions such
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Results We show the pass@1 and pass@5 scores for both tasks in our benchmark in Table 15. In terms of
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 15: Accuracy on the CRUXEval benchmark.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:precise samples in the benchmark were chosen from a larger set of samples, and the noise from choosing
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:which samples to include in the benchmark when using 800samples is about 1.5%. We make the following
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark StarCoder2 supports fill-in-the-middle (FIM), which is the ability to complete an
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:arbitrary span of code conditioned on both text before and after the insertion point. We use the benchmark
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:15B on this FIM benchmark. Unfortunately, StarCoder2-15B underperforms on FIM. Due to an implementa-
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:world scenarios. We evaluate models on repository-level code completion with two benchmarks: RepoBench
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark RepoBench (Liu et al., 2023b) is a live benchmark designed for evaluating code
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark CrossCodeEval (Ding et al., 2023) is a diverse and multilingual benchmark
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 19: Performance on the “Asleep at the Keyboard” benchmark.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark “Asleep at the Keyboard” is a benchmark designed for assessing security vulnera-
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmarks Bias in Open-ended Language Generation Dataset (BOLD) (Dhamala et al.,
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Vontimitta, Spencer Whitman, and Joshua Saxe. Purple llama CyberSecEval: A secure coding benchmark
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:and Abhinav Jangda. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:diverse and multilingual benchmark for cross-file code completion. In Thirty-seventh Conference on Neural
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:CRUXEval: a benchmark for code reasoning, understanding and execution. arXiv preprint , January 2024.
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:ume 1. Curran, 2021. URL https://datasets-benchmarks-proceedings .neurips.cc/paper/2021/hash/
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Daniel Fried, Sida Wang, and Tao Yu. DS-1000: A natural and reliable benchmark for data science code
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark. arXiv preprint arXiv:2210.07316 , 2022a. doi: 10 .48550/ARXIV .2210.07316. URL https:
1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark, stu-nicholls, sun-rpc, sun-source, sunsoft, supervisor, svndiff, swig, symphonysoft, synopsys-mit,
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:Synthetic Benchmark. Juliet Test Suite Boland and Black [2012] is a benchmark widely used to
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:its effectiveness. Specifically, we diff the sources/sinks labeled in the benchmark and the identified
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:identifying feasible paths. Due to the lack of explicit ground truth in the benchmark, we would
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:NoSynVal in the XSS and OSCI detection because the corresponding benchmark programs do not
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:the real-world benchmark SecBench.js
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:JavaScript benchmark SecBench.js Chow et al. [2023]. The Juliet Test Suite for C/C++ does not
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:den, Ben Hermann, and Fabio Massacci. Taintbench: Automatic real-world malware benchmarking
1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:script programs for several benchmark programs. Specifically, LLMDFA can wrongly interpret the
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:CodeT5+ across various benchmarks but also remains com-
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:2021a) and the MBPP (Austin et al., 2021) benchmarks.
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:benchmark (Lu et al., 2021) and also includes Hu-
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:2https://github.com/facebookresearch/fairseqTable 1: Overview of our evaluation benchmarks about test
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:on unit-tests provided in the benchmark using a single gen-
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:Baselines. We first benchmark AST-T5 against our own
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:We further benchmark AST-T5 against other language mod-
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:controlled experiments. Next, we benchmark AST-T5
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:models using the HumanEval benchmark and the MBPP
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:benchmark, respectively. Additional results on EvalPlus are
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:across multiple benchmarks at the same time. Moreover,
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:manEval benchmark, underscoring our model’s parameter
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:2021a) on the MBPP benchmark, showing the effectiveness
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:benchmark dataset for code understanding and generation.
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:Mia, M. M. Towards a big data curated benchmark of
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:We extend our evaluation to include EvalPlus (Liu et al., 2023), a more rigorous benchmark that enhances the original
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:Table 5: Performance of AST-T5 on HumanEval+ and MBPP+ benchmarks, compared with reported numbers of language
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:and MBXP benchmarks (Athiwaratkun et al., 2023). This analysis includes models such as BLOOM (BigScience, 2021),
1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:(2023). Our results show AST-T5’s superior performance across all benchmarks compared to the CodeGen-multi-350M.
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:HumanEval benchmark [25]. To ensure the generalizability
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:benchmark as well. Specifically, we selected nine other
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:generated CoTs on code generation benchmarks (such as
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:manEval, HumanEval-plus, and OpenEval benchmarks by
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:Similarly, on the OpenEval benchmark, COTTON boosts the
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:experiments on a comprehensive set of benchmarks.
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:another dataset to benchmark the code generation
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:machine learning benchmark dataset for code understanding
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:code generation with multilingual benchmarking on humaneval-
1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:Standardizing and benchmarking interactive coding with execu-
1.6.3. CodeAgent Paper- 2402.02172v5.txt:was benchmarked against established models like
1.6.3. CodeAgent Paper- 2402.02172v5.txt:Review, a challenging benchmark data, indicates a
2004.13820v2.txt:Table 1. Popular benchmark datasets for Semantic similarity
2004.13820v2.txt:the STS benchmark datasets and has shown better performance in 12 out of 19 chosen STS Datasets
2004.13820v2.txt:semantic similarity in the SICK dataset and the STS benchmark dataset when compared to
2004.13820v2.txt:Table 3. Pearson’s Correlation of various transformer-based models on STS benchmark dataset.
2004.13820v2.txt:and𝜃were tuned to values between 0 and 0.5 for different STS benchmark datasets. The
2004.13820v2.txt:ensemble model outperformed the STS benchmark unsupervised models in the 2017 SemEval
2004.13820v2.txt:series on various STS benchmark datasets.
2102.04664v2.txt:CodeXGLUE, a benchmark dataset to foster machine learning re-
2102.04664v2.txt:2https://evansdata.com/press/viewRelease.php?pressID=278It is commonly accepted that benchmarks have a significant impact
2102.04664v2.txt:establishing a benchmark dataset for code intelligence.
2102.04664v2.txt:benchmark suite that covers a wide range of tasks. The use of Ima-
2102.04664v2.txt:have shown that a diversified benchmark dataset has a significant
2102.04664v2.txt:learning benchmark dataset for program understanding and genera-
2102.04664v2.txt:3We plan to evolve the benchmark over time by extending to more tasks.To make it easy for participants, we provide three baseline mod-
2102.04664v2.txt:BigCloneBench is a widely used large code clone benchmark
2102.04664v2.txt:diversified benchmark dataset that can be applied to various code
2102.04664v2.txt:mad Mamun Mia. 2014. Towards a big data curated benchmark of inter-project
2102.04664v2.txt:Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural
2105.12655v2.txt:to benchmark and help accelerate research in AI techniques for a variety of crit-
2105.12655v2.txt:large benchmark datasets that are customized to their downstream use. See Figure 1 for a summary.
2105.12655v2.txt:POJ-104-like benchmark and used in some publications. This benchmark dataset, GCJ-297 [ 23],
2105.12655v2.txt:benchmark is approximately 10 times larger than POJ-104.
2105.12655v2.txt:CodeNet has a rich set of code samples, and the user can assemble a customized benchmark according
2105.12655v2.txt:to his/her need. Following POJ-104, we extracted benchmark datasets from CodeNet in C++, Python,
2105.12655v2.txt:and Java. The benchmark characteristics are shown in Table 6. For the C++ benchmarks, the number
2105.12655v2.txt:of problems and their solutions are chosen to make the benchmark challenging. The benchmarks are
2105.12655v2.txt:summarizes the SPT statistics for the four benchmarks.
2105.12655v2.txt:task, and a token inference task, using the four benchmark datasets (see Table 6) extracted from
2105.12655v2.txt:model is ﬁne-tuned on each benchmark.
2105.12655v2.txt:Table 7 summarizes the classiﬁcation accuracy for all models on all benchmarks. Despite the
2105.12655v2.txt:benchmarks.
2105.12655v2.txt:even though C-BERT is pre-trained with C programs, its performance on the two C++ benchmarks is
2105.12655v2.txt:Table 8 provides results of code classiﬁcation of all four benchmarks. The columns give the benchmark
2105.12655v2.txt:benchmark. The neural networks used for classiﬁcation of other benchmarks are similar to this one.
2105.12655v2.txt:C++1400 benchmark dataset is signiﬁcantly better than the potential 0.071% accuracy of random
2105.12655v2.txt:Table 9 shows results of code classiﬁcation on all four benchmarks by using the sequence-of-tokens
2105.12655v2.txt:representation. The columns give the benchmark name, the test accuracy, the number of training
2105.12655v2.txt:we choose for the C++1400 benchmark. It is a multi-layer convolutional neural network. It uses
2105.12655v2.txt:Using this network we get a test accuracy 93.71 0.18% for C++1400 benchmark dataset, which is
2105.12655v2.txt:used for classiﬁcation of other benchmarks are similar to the one shown in Figure 5. As we see in
2105.12655v2.txt:benchmark. Additionally, we experiment with the POJ-104 dataset, which contains code examples in
2105.12655v2.txt:languages share common tokens, we could apply the C-BERT model directly on the benchmarks.
2105.12655v2.txt:After pretraining, we ﬁne tune the model for ﬁve epochs on each benchmark, with a batch size 32 and
2105.12655v2.txt:Table 10 summarizes the accuracies C-BERT achives on the four CodeNet benchmarks as well as the
2105.12655v2.txt:The relatively low performance on C++ benchmarks is possibly related to the idiosyncrasies of the
2105.12655v2.txt:We conduct 6/2/2 random split for each of the 4 benchmarks: i.e., 60% training data, 20% testing
2105.12655v2.txt:data, and 20% validation data. We run ﬁve folds for each benchmark with early stop ”patience”
2105.12655v2.txt:are conducted on one NVIDIA V100 GPU. For large benchmarks such as C++1000 and C++1400, it
2105.12655v2.txt:Exploring further into the Java250 benchmark, Table 14 summarizes the MAP@R score with a variety
2105.12655v2.txt:Table 15 provides results of code similarity analysis on all four benchmarks. The columns give the
2105.12655v2.txt:benchmark name, the test accuracy, the number of training epochs, the number of samples in each
2105.12655v2.txt:Figure 7 shows the neural network used for code similarity analysis on the C++1400 benchmark. The
2105.12655v2.txt:neural networks used for code similarity analysis on other benchmarks are similar to this one. As we
2105.12655v2.txt:As we see in Table 15, the model accuracy is rather modest (<87%) for all benchmark datasets, which
2105.12655v2.txt:Table 16 provides results of code similarity analysis on all four benchmarks. The columns give the
2105.12655v2.txt:benchmark name, the test accuracy, the number of training epochs, the number of samples in each
2105.12655v2.txt:The neural network for the C++1400 benchmark is depicted in Figure 8. The siamese parts of the
2105.12655v2.txt:The network shows 96.56 0.07% test accuracy for C++1400 benchmark dataset. We consider this a
2105.12655v2.txt:many keywords. The neural networks used for code similarity analysis of other benchmarks are
2105.12655v2.txt:high memory requirement for computing MAP@R on the test set of CodeNet benchmarks, we had to
2105.12655v2.txt:Models trained on the CodeNet benchmark datasets can beneﬁt greatly from their high quality. To
2105.12655v2.txt:unseen test set. We train a popular BERT-like attention model on the C++1000 CodeNet benchmark
2105.12655v2.txt:only unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code
2105.12655v2.txt:Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
2105.12655v2.txt:Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
2105.12655v2.txt:Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
2105.12655v2.txt:Shengyu Fu, and Shujie Liu. CodeXGLUE: A machine learning benchmark dataset for code
2203.09095v2.txt:GitHub code review data, we create pre-training and benchmark
2203.09095v2.txt:We further evaluate our CodeReviewer model on our benchmark
2203.09095v2.txt:a high-quality benchmark dataset for evaluation in nine
2203.09095v2.txt:the benchmark dataset for the three downstream tasks. To prevent
2203.09095v2.txt:benchmark dataset. Other code repositories with [1,500,2,500)pull
2203.09095v2.txt:of the pre-training dataset. For benchmark datasets, details are
2203.09095v2.txt:Table 3: Statistics of benchmark datasets.
2203.09095v2.txt:training and a benchmark for evaluation on the three tasks. It is
2212.09132v1.txt:benchmark (Lu et al 2021) for code completion task.
2212.09132v1.txt:and establish benchmarks for tasks. The diversity of representations facilitates
2212.09132v1.txt:code completions from synthetic benchmarks and real-world completions. Al-
2212.09132v1.txt:though all forms of benchmarks are useful (Karmakar 2019), it found that
2212.09132v1.txt:synthetic benchmarks underweighted the frequencies of method completions
2212.09132v1.txt:sequences, as exhibited in specialized benchmarks (Tay et al 2020a).
2212.09132v1.txt:Karmakar A (2019) Establishing benchmarks for learning program representations. In: SAT-
2212.09132v1.txt:D, Tang D, et al (2021) Codexglue: A machine learning benchmark dataset for code
2212.09132v1.txt:Mir AM, Latoskinas E, Gousios G (2021) Manytypes4py: A benchmark python dataset for
2212.09132v1.txt:D (2020a) Long range arena: A benchmark for ecient transformers. arXiv preprint
2212.09132v1.txt:Wang K, Christodorescu M (2019) Coset: A benchmark for evaluating neural program em-
2305.06161v2.txt:benchmarks (Lai et al., 2022; Cassano et al., 2023; Pearce et al., 2022; Fried et al., 2022; Yee & Guha,
2305.06161v2.txt:positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the
2305.06161v2.txt:benchmarks are further described in Section 6.) To give an indication of the amount of data removed by
2305.06161v2.txt:et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation
2305.06161v2.txt:using a variety of benchmarks and tasks.
2305.06161v2.txt:code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks,
2305.06161v2.txt:for Python, Java, and C ++. CodeGeeX also includes its own multi-language benchmark suite,
2305.06161v2.txt:et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure
2305.06161v2.txt:performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science
2305.06161v2.txt:HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs
2305.06161v2.txt:using the pass@ kmetric (Chen et al., 2021): the total fraction of benchmark problems solved, where a
2305.06161v2.txt:1.StarCoder is the highest-performing open-access model on both benchmarks .
2305.06161v2.txt:representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al.,
2305.06161v2.txt:benchmark. Moreover, this is true across every kind of data science library.
2305.06161v2.txt:3.We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks
2305.06161v2.txt:does not always correlate with performance on the more realistic DS-1000 benchmarks . For example,
2305.06161v2.txt:models on a range of benchmarks.
2305.06161v2.txt:to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022)
2305.06161v2.txt:benchmark.
2305.06161v2.txt:Table 14: Performance on the ODEX benchmark by instruction languages and code domains: openproblems
2305.06161v2.txt:MBPP (Austin et al., 2021) Python benchmarks into 18 other programming languages as follows.
2305.06161v2.txt:MultiPL-E has a set of rule-based compilers that translate Python benchmarks to each target programming
2305.06161v2.txt:language. Each compiler expects a benchmark in the HumanEval format: 1) a natural language description
2305.06161v2.txt:have doctests) into a target language. Thus, MultiPL-E gives us a parallel set of benchmarks derived from
2305.06161v2.txt:language. MultiPL-E also omits three HumanEval benchmarks that do not fit the above format. These changes have a small
2305.06161v2.txt:Table 16: Performance on the Asleep at the Keyboard security benchmark (Pearce et al., 2022).
2305.06161v2.txt:TheAsleep at the Keyboard benchmark by Pearce et al. (2022) has 89 security-sensitive scenarios across
2305.06161v2.txt:OpenAI’s code-cushman-001. We use the original benchmarking methodology: generating 25 completions
2305.06161v2.txt:Table 17: Performance on single-line fill-in-the-middle on the FIM benchmark by Ben Allal et al. (2023).
2305.06161v2.txt:et al. (2020) benchmarks. We report both the overall F1 scores, which include trivial None-type prediction,
2305.06161v2.txt:left-to-right code completion. We evaluate StarCoderBase on four established FIM benchmarks below.
2305.06161v2.txt:Ben Allal et al. (2023) generalizes this benchmark to also support Java and JavaScript, using model-generated
2305.06161v2.txt:TypeScript. However, instead of measuring accuracy, they argue that benchmarks should measure how many
2305.06161v2.txt:use the Python subset of the CodeXGLUE code summarization benchmark (Lu et al., 2021). This benchmark
2305.06161v2.txt:Table 22: 8-shot accuracy on the GSM8K math-reasoning benchmark. Samples are generated with greedy
2305.06161v2.txt:this reasoning benchmark. However, its performance still lags behind LLaMA-33B (38.7%).
2305.06161v2.txt:Table 23: 5-shot accuracy on the MMLU language understanding benchmark.
2305.06161v2.txt:MMLU (Hendrycks et al., 2020) is a massive multitask language understanding benchmark, covering multiple-
2305.06161v2.txt:benchmarks that measure social bias and toxicity in model-produced text.15
2305.06161v2.txt:Table 27: Model results on natural language reasoning tasks in the HELM benchmark, with models ordered
2305.06161v2.txt:HELM benchmark does not include the CodeGen, CodeGeex, and LLaMA models. Therefore, we compare
2305.06161v2.txt:the HumanEval benchmark.
2305.06161v2.txt:English-only evaluations We evaluated the performance of StarCoder solely on English-based benchmarks
2305.06161v2.txt:and Abhinav Jangda. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation.
2305.06161v2.txt:Daniel Fried, Sida Wang, and Tao Yu. DS-1000: a natural and reliable benchmark for data science code
2305.06161v2.txt:CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. arXiv
2305.06161v2.txt:"-benchmark",
2305.06161v2.txt:We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several
2305.06161v2.txt:We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these
2308.10462v3.txt:challenging benchmark with test cases.
2308.10462v3.txt:evaluation datasets such as HumanEval [ 9] have extensively been used to benchmark code generation
2308.10462v3.txt:benchmarking neural code generation. IEEE Transactions on Software Engineering (2023).
2308.10462v3.txt:Daxin Jiang, Duyu Tang, et al .2021. Codexglue: A machine learning benchmark dataset for code understanding and
2309.03914v2.txt:natural and reliable benchmark for data science code generation. In International
2401.12954v1.txt:benchmarks,includingtheGameof24(Yaoetal .,2023a),Checkmate-in-OnefromtheBIG-Benchsuite(BIG-
2401.12954v1.txt:efficacyofthesepromptingmethodsacrossabroadsetoftasksandbenchmarks. Morerecentinnovations
2402.01035v2.txt:tokenizer, and close to Punct on coding benchmarks while
2402.01035v2.txt:eration with multilingual benchmarking on humaneval-x.
2402.11811v4.txt:benchmarks and six testing models.1
2402.11811v4.txt:various benchmarks (Cobbe et al., 2021; Suzgun
2402.11811v4.txt:downstream benchmarks and three diverse
2402.11811v4.txt:We include five benchmarks across two most com-
2402.11811v4.txt:benchmarks. MMLU covers 14k questions. For
2402.11811v4.txt:benchmarks. While differently, since both APE and
2402.11811v4.txt:“generation ” or “ multi-choice ” benchmarks takes
2402.11811v4.txt:report the accuracy score for all benchmarks.
2402.11811v4.txt:gains on different downstream generators acrossfive public benchmarks, shown in Table 2. The op-
2402.11811v4.txt:The number attached with the benchmark is the number of in-context examples (e.g., BBH (3) means 3-shot testing
2402.11811v4.txt:all benchmarks, except BBH, which is due to its
2402.11811v4.txt:downstream testing benchmarks, discussing the ef-
2402.11811v4.txt:Table 4: Examples from testing benchmarks.
2402.11811v4.txt:benchmark. In International Conference on Machine
2402.11811v4.txt:all benchmarks, except BBH. We suppose this may
2402.11811v4.txt:256 random optimized prompts for each benchmark.
2402.11811v4.txt:benchmark, manually checking with 256 random
2403.18350v2.txt:to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges
2403.18350v2.txt:potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of
2403.18350v2.txt:addressing the notable lack of benchmarks and baseline data. Additionally, we seek to explore the impact of semantic
2403.18350v2.txt:sorted by diminishing relevance to set a benchmark for the lowest achievable scores).
2403.19340v2.txt:moving contaminated data such as benchmark
2403.19340v2.txt:the elimination of benchmarks or other unintended
2403.19340v2.txt:benchmark and contamination for language mod-
2404.13506v2.txt:benchmarks such as Alpaca-Eval v1.0 and GLUE indicated
2404.13506v2.txt:MSR-VTT and ActivityNet benchmarks. Their research
2404.13506v2.txt:benchmark, this approach notably excels by updating merely
2404.13506v2.txt:managed to outperform the full fine-tuning benchmarks on
2404.13506v2.txt:Revisiting point cloud classification: A new benchmark
2405.18414v1.txt:highlight the advantages of the proposed G-RAG over state-of-the-art benchmarks, we conducted
2405.18414v1.txt:benchmark for question answering research. Transactions of the Association for Computational
2405.19250v1.txt:of the HumanEval benchmark rewritten by human experts
2405.19250v1.txt:on the HumanEval benchmark. Lastly, we discuss potential fu ture
2405.19250v1.txt:and designing a more realistic benchmark for Kotlin.
2405.19250v1.txt:generation of models can generate Kotlin code. To benchmark
2405.19250v1.txt:of the HumanEval benchmark, speciﬁcally adapted for Kotlin .
2405.19250v1.txt:programming languages. The ﬁndings from these benchmarks
2405.19250v1.txt:on the HumanEval benchmark for both languages.
2405.19250v1.txt:test-based benchmark involves reporting only the pass rate as
2405.19250v1.txt:code. Since the size of the HumanEval benchmark allows one
2405.19250v1.txt:HumanEval benchmark.
2405.19250v1.txt:train loss and downstream benchmark scores at no extra
2405.19250v1.txt:based benchmarking. In addition to its beneﬁts we describe
2405.19250v1.txt:and practitioners. Second, HumanEval benchmark is a well-
2405.19250v1.txt:established, popularr benchmark, which we further cleaned
2405.19250v1.txt:and improved. Our completion benchmark, however, was not
2405.19250v1.txt:the benchmark test dataset [25] (which is all the more possib le
2405.19250v1.txt:once the benchmarks become public), artiﬁcially inﬂating t he
2405.19250v1.txt:benchmarks.
2405.19250v1.txt:•More benchmarks . While in this work we suggest the
2405.19250v1.txt:most essential benchmark — HumanEval, it can hardly
2405.19250v1.txt:been a new generation of issue-based benchmarks like
2405.19250v1.txt:reliable benchmark for data science code generation,” in International
2406.04712v1.txt:quality of the AICoderEval benchmark.
2406.04712v1.txt:To address this challenge, we construct the AICoderEval dataset, a benchmark for AI-oriented
2406.04712v1.txt:benchmark. Our primary focus is on generating Python code files using GPT-4, streamlining the
2406.04712v1.txt:dataset size to around 2,000 code files. To construct our final benchmark, we further select a subset
2406.04712v1.txt:process ensures the quality and reliability of the AICoderEval benchmark.
2406.04712v1.txt:The AICoderEval benchmark is hosted on Hugging Face Datasets at https://huggingface.co/
2406.04712v1.txt:Table 1 presents the distribution of task categories within the AICoderEval benchmark. Natural
2406.04712v1.txt:domains showcases the breadth and depth of the AICoderEval benchmark.
2406.04712v1.txt:To ensure the quality and integrity of the AICoderEval benchmark, we will conduct both automated
2406.04712v1.txt:of the benchmark.
2406.04712v1.txt:AICoderEval, as depicted in figure 2. This framework can construct domain-specific tasks benchmark,
2406.04712v1.txt:for training and evaluation, and then fine-tunes a code generation model on the benchmark.
2406.11931v1.txt:length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves
2406.11931v1.txt:and Gemini 1.5 Pro in coding and math benchmarks.
2406.11931v1.txt:Figure 1|The Performance of DeepSeek-Coder-V2 on math and code benchmarks.
2406.11931v1.txt:to 37.2%) and MBPP (from 44.6% to 54.0%) benchmarks (Austin et al., 2021a; Chen et al.,
2406.11931v1.txt:•Code : Regarding code generation benchmark evaluation, DeepSeek-Coder-V2 demon-
2406.11931v1.txt:elementary benchmarks like GSM8K (Cobbe et al., 2021) and advanced competition-level
2406.11931v1.txt:benchmarks including MATH (Hendrycks et al., 2021), AIME (MAA, 2024), and Math
2406.11931v1.txt:on the MATH benchmark, nearly matching the state-of-the-art accuracy of 76.6% achieved
2406.11931v1.txt:49.0%) benchmarks, respectively. Further training the 1B model with 2T tokens led to additional
2406.11931v1.txt:et al., 2021b) benchmarks are commonly utilized for assessing the performance of code-generating
2406.11931v1.txt:abilities of models, we extended the HumanEval benchmark problems into seven additional
2406.11931v1.txt:For both benchmarks, we employed a greedy search strategy and recreated the baseline results
2406.11931v1.txt:benchmark (Shi et al., 2024) to estimate the effectiveness of DeepSeek-Coder-V2. LiveCodeBench
2406.11931v1.txt:benchmark contains 307 problems from the USA Computing Olympiad, along with high-quality
2406.11931v1.txt:Table 4|Performance on the LiveCodeBench (LCB) and USACO benchmarks.
2406.11931v1.txt:Table 4 showcases the performance of various language models on the two benchmarks.
2406.11931v1.txt:modify one method from this benchmark.
2406.11931v1.txt:SWE-bench is a comprehensive benchmark designed to evaluate the performance of large
2406.11931v1.txt:language models in addressing real-world software issues sourced from GitHub. The benchmark
2406.11931v1.txt:Aider’s code editing benchmark evaluates the LLM’s ability to modify Python source files,
2406.11931v1.txt:completing 133 distinct coding tasks. This benchmark not only tests the LLM’s coding skills but
2406.11931v1.txt:Table 7|Performances of different models on repair benchmarks. We do not evaluate
2406.11931v1.txt:To assess the code reasoning capabilities of our models, we utilize the CRUXEval benchmark.
2406.11931v1.txt:This benchmark comprises 800 Python functions paired with corresponding input-output
2406.11931v1.txt:8 presents the performance of various language models on the CruxEval benchmark, which
2406.11931v1.txt:Table 8|Performance of different models on the CruxEval benchmark.
2406.11931v1.txt:grade-school benchmark GSM8K (Cobbe et al., 2021), along with advanced competition-level
2406.11931v1.txt:benchmarks including MATH (Hendrycks et al., 2021), the American Invitational Mathematics
2406.11931v1.txt:accuracy of 75.7% on the MATH benchmark and 53.7% on Math Odyssey, comparable to the
2406.11931v1.txt:9The performance of DeepSeek-Coder-V2 on the four mathematical benchmarks was obtained with zero-shot
2406.11931v1.txt:bility, even surpassing DeepSeek-V2 on reasoning-related benchmarks. We compare DeepSeek-
2406.11931v1.txt:Coder-V2 Instruct with DeepSeek-V2 Chat on standard benchmarks, which covers both En-
2406.11931v1.txt:glish and Chinese benchmarks, including BigBench Hard (BBH) (Suzgun et al., 2022), MMLU
2406.11931v1.txt:Lite-Instruct outperforms DeepSeek-V2-Lite-Chat in benchmarks like BBH and Arena-Hard.
2406.11931v1.txt:These benchmarks place a high demand on the model’s reasoning ability, which DeepSeek-
2406.11931v1.txt:knowledge-intensive benchmarks like TriviaQA, primarily due to the relatively smaller amount
2406.11931v1.txt:soning benchmarks, particularly in Arena-Hard, which comprises a substantial proportion of
2406.11931v1.txt:slightly better results in benchmarks such as MT-bench (Zheng et al., 2023), AlpacaEval 2.0
2406.11931v1.txt:Although DeepSeek-Coder-V2 achieves impressive performance on standard benchmarks,
2406.11931v1.txt:J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering
2406.11931v1.txt:high-quality benchmarks: The arena-hard pipeline, April 2024. URL https://lmsys.org/
2406.11931v1.txt:nese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors,
2406.11931v1.txt:human-centric benchmark for evaluating foundation models. CoRR , abs/2304.06364, 2023.
2407.02485v1.txt:model with the state-of-the-art performance on RAG benchmarks. Specifically,
2407.02485v1.txt:models on nine knowledge-intensive benchmarks. In addition, it also performs
2407.02485v1.txt:comparably to GPT-4 on five RAG benchmarks in the biomedical domain without
2407.02485v1.txt:benchmarks for RAG, Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-
2407.02485v1.txt:we use the split from KILT benchmark (Petroni et al., 2021)2. (2) Fact verification , where we
2407.02485v1.txt:use FEVER (Thorne et al., 2018) from KILT benchmark. (3) Conversational QA (ConvQA) , we
2407.02485v1.txt:marked as “–”. We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct,
2407.02485v1.txt:many RAG benchmarks. RankRAG 8B is also competitive when compared with baseline models
2407.02485v1.txt:biomedical RAG benchmark. RankRAG and baselines use re-
2407.02485v1.txt:intensive benchmarks and demonstrate RankRAG significantly outperform all of them on nine
2407.02485v1.txt:general-domain and five biomedical benchmarks for RAG.
2407.02485v1.txt:hension benchmark requiring discrete reasoning over paragraphs. In NAACL , 2019.
2407.02485v1.txt:Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question answering
2407.02485v1.txt:V ., Maillard, J., Plachouras, V ., Rocktäschel, T., and Riedel, S. KILT: a benchmark for knowledge
2407.02485v1.txt:benchmark for zero-shot evaluation of information retrieval models. In NeurIPS , 2021.
2407.02485v1.txt:answering benchmark on a hybrid of tabular and textual content in finance. In ACL, 2021.
2407.02485v1.txt:evidence from Wikipedia, providing a benchmark for fact-checking systems.
2407.02485v1.txt:for NQ and TriviaQA, respectively. In contrast, the KILT benchmark (Petroni et al., 2021) utilizes
2407.08275v1.txt:and performance benchmarks such as the Massive Text Embedding
2407.08275v1.txt:models on benchmark datasets offer the simplified perspective of
2407.08275v1.txt:ding models, in contrast to performance benchmarks that require
2407.08275v1.txt:popular benchmark datasets to determine if similarities between
2407.08275v1.txt:on their performance on downstream tasks, with benchmarks such
2407.08275v1.txt:cluded in the benchmarks [ 8] or where the evaluation encompasses
2407.08275v1.txt:select five publicly available datasets from the BEIR benchmark [ 35].
2407.08275v1.txt:the smaller datasets from the benchmark. This approach allows us
2407.15462v4.txt:both the query-side and the item-side [ 6]. For common benchmark
2407.15462v4.txt:We benchmark MoL with the proposed load balancing loss L𝑀𝐼, on
2407.15462v4.txt:new state-of-the-art across common, heterogeneous benchmark
2407.15462v4.txt:[1][n. d.]. ANN Benchmarks. https://ann-benchmarks.com/. Accessed: 2024-08-06.
2409.10959v1.txt:GitHub code review dataset, which is now widely employed as the benchmark dataset [44, 46, 47, 65].
2409.10959v1.txt:Dataset selection. We use the CodeReviewer dataset provided by Li et al. [ 43], which is the benchmark dataset for the
2409.10959v1.txt:metric [ 55] with up to 4-gram matching to benchmark the new approaches in terms of accuracy against the test
2409.10959v1.txt:Associates Inc., Red Hook, NY, USA. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c16a5320fa475530d9583c34fd356ef5-
2410.20424v3.txt:Tang et al. (2024) introduces ML-Bench, a benchmark for language agents for machine learning
2411.10129v1.txt:“A survey of large language models for code: Evolution, benchmarking,
2411.10129v1.txt:torical data an appropriate benchmark for reviewer recommendation sys-tems?: A case study of the gerrit community,” in 2021 36th IEEE/ACM
2411.10129v1.txt:learning benchmark dataset for code understanding and generation,”
2501.03265v1.txt:development of unified standards, tools, benchmarks and
2502.02757v2.txt:to clean their widely-used CodeReviewer benchmark using
2502.02757v2.txt:benchmark (CodeReviewer [5]), we find that only 64% of
2502.02757v2.txt:benchmark are valid. Our approach using LLMs achieved
2502.02757v2.txt:stantial noise persists in current benchmark datasets including
2502.02757v2.txt:benchmarks in their study, our 32% specifically refers to the CodeReviewer
2502.02757v2.txt:benchmark, which was obtained from their replication package.
2502.02757v2.txt:benchmark dataset quality by providing more reliable test sets
2502.02757v2.txt:code: Evolution, benchmarking, and future trends,” ACM
2502.14862v1.txt:poor performance on standard benchmarks. Re-
2502.20273v4.txt:Figure 3 displays benchmark results for the UnigramLM
2502.20273v4.txt:phological benchmarks, while showing some fluctuations,
2503.13505v1.txt:benchmarks for instruction-following tasks, evaluating models
2503.13505v1.txt:cost-effective methods have been benchmarked [24], [61]. The
2503.13505v1.txt:K. Keutzer, and S. K. Upadhyay, “Routerbench: A benchmark for
2503.13505v1.txt:son, and M. Yurochkin, “LLM routing with benchmark datasets,” in
2504.10046v1.txt:gate the effectiveness of our framework on the DevEval benchmark
2504.10046v1.txt:searchers have developed specialized benchmarks like DevEval[ 18]
2504.10046v1.txt:Huang, and Yongbin Li. Evocodebench: An evolving code generation benchmark
2505.24581v1.txt:the MTEB benchmark. GATE leverages Ma-
2505.24581v1.txt:improvement on STS benchmarks, effectively
2505.24581v1.txt:benchmarking, and error analysis.
2505.24581v1.txt:benchmark for evaluating embedding models.Matryoshka Representation Learning (MRL) has
2505.24581v1.txt:mantic datasets, setting a new benchmark for Ara-
2505.24581v1.txt:small, serves as a comparative benchmark for
2505.24581v1.txt:Table 3: Performance comparison of Matryoshka models vs. their base counterparts on MTEB benchmarks.
2505.24581v1.txt:across Arabic NLP benchmarks.
2505.24581v1.txt:on Arabic STS benchmarks
2505.24581v1.txt:across the three Arabic STS benchmarks in MTEB.
2505.24581v1.txt:Figure 2: Performance comparison between Matryoshka models and larger models on MTEB Arabic benchmarks.
2505.24581v1.txt:comprehensive Arabic NLP benchmarks restricts abroader evaluation beyond STS tasks. Additionally,
2505.24581v1.txt:uations on MTEB benchmarks confirmed strong
2505.24581v1.txt:Future work will extend Arabic NLP benchmarks,
2505.24581v1.txt:benchmark. arXiv preprint arXiv:2210.07316 .
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:the lack of reasoning-focused coding benchmarks and self-contained training
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:The first challenge is the lack of coding benchmarks that accurately assess LLMs’ reasoning abilities.
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:LiveCodeBench (Jain et al., 2024), a commonly used benchmark, addresses this by sourcing problems
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:for benchmarking, while those released earlier constitute the training set.
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:Finally, we developed the LeetCodeDataset, which features broad coverage, reliable benchmarking,
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarking strikes an effective balance between bias and variance.
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:evaluated them across four benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021),
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:(post-2024-07), the 2.6K-trained model underperformed on hard benchmarks. It suggests
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:Code Generation Benchmarks. Numerous benchmarks have been developed to evaluate the code
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:generation capabilities of LLMs. For foundational Python programming, widely used benchmarks
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarks by translating them into 18 other programming languages. As LLM capabilities
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:advance, many of these benchmarks are becoming too easy to assess modern models adequately. A
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:few specialized benchmarks focus on competitive programming challenges. APPS (Hendrycks et al.,
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:high-quality instructional data for coding tasks. In competitive programming benchmarks like
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:While our LeetCode dataset effectively benchmarks and fine-tunes code models, it has three key
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarking and supports longitudinal studies. This dataset comprehensively covers algorithms
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:can match the performance of those trained on 110K examples from previous benchmarks, demon-
all_benchmark.md:1.2.3.LeetCodeDataset.A Temporal Dataset for Robust Evaluation and_2504.14655v1.txt:benchmarking neural code generation, 2022. URL https://arxiv.org/abs/2208.08227 .
all_benchmark.md:1.3.2.AgentCoder. Multi-Agent-based Code Generation with Iterative Testing and Optimisation2312.13010v3.txt:series [4, 29] developed by OpenAI, have consistently set the benchmark for performance across
all_benchmark.md:1.3.2.AgentCoder. Multi-Agent-based Code Generation with Iterative Testing and Optimisation2312.13010v3.txt:from codegeex.benchmark.execution import check_correctness
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:HumanEval benchmark [25]. To ensure the generalizability
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:benchmark as well. Specifically, we selected nine other
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:generated CoTs on code generation benchmarks (such as
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:manEval, HumanEval-plus, and OpenEval benchmarks by
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:Similarly, on the OpenEval benchmark, COTTON boosts the
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:experiments on a comprehensive set of benchmarks.
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:another dataset to benchmark the code generation
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:machine learning benchmark dataset for code understanding
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:code generation with multilingual benchmarking on humaneval-
all_benchmark.md:1.3.3. Chain-of-Thought in Neural Code Generation.2312.05562v2.txt:Standardizing and benchmarking interactive coding with execu-
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmarks.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2-
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:with test benchmarks. More broadly, this practice hinders scientific progress as other research teams cannot
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:assess and compare the performance of these models on a suite of code LLM benchmarks (Cassano et al.,
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:DeepSeekCoder-1.3B) on most benchmarks. Moreover, it matches or surpasses the performance of
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:code completion benchmarks for high-resource languages. However, StarCoder2-15B matches or
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:and Perl). Moreover, when we consider benchmarks that require models to reason about code
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:•APPS (train) (Hendrycks et al., 2021) is a popular text2code benchmark in Python with a train
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:•GSM8K (train) (Cobbe et al., 2021) is the train split of GSM8K, a popular evaluation benchmark
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:We apply several preprocessing steps, such as deduplication (§3.1), PII redaction (§3.2), benchmark decon-
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:To ensure the performance of StarCoder is not artificially inflated on our test benchmarks, we decontaminate
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:We evaluate the StarCoder2 models on a variety of benchmarks from the literature and compare them to
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:outperforms the extra-large models in several benchmarks.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmarks HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are two of the
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:most widely studied benchmarks for Code LLMs. Each benchmark has a few hundred programming problems.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Recently, Liu et al. (2023a) identified several issues with both benchmarks. (1) Most problems have inadequate
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:these problems. The resulting benchmarks (HumanEval+ and MBPP+) have 80 ×and 35×more tests than
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:the original benchmarks. For rigorous evaluation, we adopt the EvalPlus framework in this study.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:greedy decoding and report the mean pass@1 (mean success rate) for all problems in the benchmark.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:16.8% and 9.6% on these benchmarks.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Although EvalPlus makes HumanEval and MBPP far more robust, the problems in these benchmarks only
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark MultiPL-E (Cassano et al., 2023b) uses a suite of lightweight, rule-based compilers
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark with the same problems translated to different languages.26
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark DS-1000 (Lai et al., 2023) is a widely studied benchmark with 1,000 data science
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark HumanEvalFix (Muennighoff et al., 2024a) is a benchmark that tests a model’s
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:ability to identify and fix bugs in code. The benchmark supports six programming languages shown in
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Figure 12. Since it is not a code completion benchmark, most base models do poorly on HumanEvalFix
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:2024a; Zhuo et al., 2024; Longpre et al., 2023). We benchmarked the default HumanEvalFixTests subvariant;
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:via Ben Allal et al. (2022), and we denote this as the “Issue” prompt. We also benchmark StarCoder2 with
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark CanItEdit (Cassano et al., 2024) is a hand-crafted benchmark designed to evaluate
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:hidden test suite, and pass@1 is reported. The benchmark encompasses a variety of problems, from simple
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 13: Performance of instructional code editing on the CanItEdit benchmark (Cassano et al., 2024).
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:The results for non-StarCoder2 models are from the benchmark paper.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Hyperparameters We evaluate all sizes of StarCoder2 on the CanItEdit benchmark using the Issue prompt
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark. Following Cassano et al. (2024), we employ random sampling with a temperature of 0.2and a
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark We use the widely studied GSM8K benchmark (Cobbe et al., 2021), a set of
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 14: 8-shot accuracy on the GSM8K math-reasoning benchmark.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark CRUXEval (Gu et al., 2024) is a two-part benchmark consisting of 800samples
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:of the benchmark were generated by CodeLlama-34B and then filtered to remove complicated functions such
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Results We show the pass@1 and pass@5 scores for both tasks in our benchmark in Table 15. In terms of
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 15: Accuracy on the CRUXEval benchmark.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:precise samples in the benchmark were chosen from a larger set of samples, and the noise from choosing
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:which samples to include in the benchmark when using 800samples is about 1.5%. We make the following
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark StarCoder2 supports fill-in-the-middle (FIM), which is the ability to complete an
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:arbitrary span of code conditioned on both text before and after the insertion point. We use the benchmark
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:15B on this FIM benchmark. Unfortunately, StarCoder2-15B underperforms on FIM. Due to an implementa-
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:world scenarios. We evaluate models on repository-level code completion with two benchmarks: RepoBench
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark RepoBench (Liu et al., 2023b) is a live benchmark designed for evaluating code
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark CrossCodeEval (Ding et al., 2023) is a diverse and multilingual benchmark
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Table 19: Performance on the “Asleep at the Keyboard” benchmark.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmark “Asleep at the Keyboard” is a benchmark designed for assessing security vulnera-
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:About the benchmarks Bias in Open-ended Language Generation Dataset (BOLD) (Dhamala et al.,
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Vontimitta, Spencer Whitman, and Joshua Saxe. Purple llama CyberSecEval: A secure coding benchmark
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:and Abhinav Jangda. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:diverse and multilingual benchmark for cross-file code completion. In Thirty-seventh Conference on Neural
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:CRUXEval: a benchmark for code reasoning, understanding and execution. arXiv preprint , January 2024.
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:ume 1. Curran, 2021. URL https://datasets-benchmarks-proceedings .neurips.cc/paper/2021/hash/
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:Daniel Fried, Sida Wang, and Tao Yu. DS-1000: A natural and reliable benchmark for data science code
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark. arXiv preprint arXiv:2210.07316 , 2022a. doi: 10 .48550/ARXIV .2210.07316. URL https:
all_benchmark.md:1.3.4. StarCoder 2 and The Stack v2- The Next Generation. 2402.19173v1.txt:benchmark, stu-nicholls, sun-rpc, sun-source, sunsoft, supervisor, svndiff, swig, symphonysoft, synopsys-mit,
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:Synthetic Benchmark. Juliet Test Suite Boland and Black [2012] is a benchmark widely used to
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:its effectiveness. Specifically, we diff the sources/sinks labeled in the benchmark and the identified
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:identifying feasible paths. Due to the lack of explicit ground truth in the benchmark, we would
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:NoSynVal in the XSS and OSCI detection because the corresponding benchmark programs do not
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:the real-world benchmark SecBench.js
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:JavaScript benchmark SecBench.js Chow et al. [2023]. The Juliet Test Suite for C/C++ does not
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:den, Ben Hermann, and Fabio Massacci. Taintbench: Automatic real-world malware benchmarking
all_benchmark.md:1.4.2. LLMDFA- Analyzing Dataflow in Code with - 2402.10754v2.txt:script programs for several benchmark programs. Specifically, LLMDFA can wrongly interpret the
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:CodeT5+ across various benchmarks but also remains com-
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:2021a) and the MBPP (Austin et al., 2021) benchmarks.
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:benchmark (Lu et al., 2021) and also includes Hu-
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:2https://github.com/facebookresearch/fairseqTable 1: Overview of our evaluation benchmarks about test
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:on unit-tests provided in the benchmark using a single gen-
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:Baselines. We first benchmark AST-T5 against our own
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:We further benchmark AST-T5 against other language mod-
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:controlled experiments. Next, we benchmark AST-T5
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:models using the HumanEval benchmark and the MBPP
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:benchmark, respectively. Additional results on EvalPlus are
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:across multiple benchmarks at the same time. Moreover,
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:manEval benchmark, underscoring our model’s parameter
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:2021a) on the MBPP benchmark, showing the effectiveness
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:benchmark dataset for code understanding and generation.
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:Mia, M. M. Towards a big data curated benchmark of
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:We extend our evaluation to include EvalPlus (Liu et al., 2023), a more rigorous benchmark that enhances the original
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:Table 5: Performance of AST-T5 on HumanEval+ and MBPP+ benchmarks, compared with reported numbers of language
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:and MBXP benchmarks (Athiwaratkun et al., 2023). This analysis includes models such as BLOOM (BigScience, 2021),
all_benchmark.md:1.4.3. AST-T5- Structure-Aware Pretraining for Code Generation and Understanding - 2401.03003v4.txt:(2023). Our results show AST-T5’s superior performance across all benchmarks compared to the CodeGen-multi-350M.
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:HumanEval benchmark [25]. To ensure the generalizability
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:benchmark as well. Specifically, we selected nine other
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:generated CoTs on code generation benchmarks (such as
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:manEval, HumanEval-plus, and OpenEval benchmarks by
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:Similarly, on the OpenEval benchmark, COTTON boosts the
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:experiments on a comprehensive set of benchmarks.
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:another dataset to benchmark the code generation
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:machine learning benchmark dataset for code understanding
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:code generation with multilingual benchmarking on humaneval-
all_benchmark.md:1.6.2.Chain-of-Thought in Neural Code Generation-2312.05562v2.txt:Standardizing and benchmarking interactive coding with execu-
all_benchmark.md:1.6.3. CodeAgent Paper- 2402.02172v5.txt:was benchmarked against established models like
all_benchmark.md:1.6.3. CodeAgent Paper- 2402.02172v5.txt:Review, a challenging benchmark data, indicates a
all_benchmark.md:2004.13820v2.txt:Table 1. Popular benchmark datasets for Semantic similarity
all_benchmark.md:2004.13820v2.txt:the STS benchmark datasets and has shown better performance in 12 out of 19 chosen STS Datasets
all_benchmark.md:2004.13820v2.txt:semantic similarity in the SICK dataset and the STS benchmark dataset when compared to
all_benchmark.md:2004.13820v2.txt:Table 3. Pearson’s Correlation of various transformer-based models on STS benchmark dataset.
all_benchmark.md:2004.13820v2.txt:and𝜃were tuned to values between 0 and 0.5 for different STS benchmark datasets. The
all_benchmark.md:2004.13820v2.txt:ensemble model outperformed the STS benchmark unsupervised models in the 2017 SemEval
all_benchmark.md:2004.13820v2.txt:series on various STS benchmark datasets.
all_benchmark.md:2102.04664v2.txt:CodeXGLUE, a benchmark dataset to foster machine learning re-
all_benchmark.md:2102.04664v2.txt:2https://evansdata.com/press/viewRelease.php?pressID=278It is commonly accepted that benchmarks have a significant impact
all_benchmark.md:2102.04664v2.txt:establishing a benchmark dataset for code intelligence.
all_benchmark.md:2102.04664v2.txt:benchmark suite that covers a wide range of tasks. The use of Ima-
all_benchmark.md:2102.04664v2.txt:have shown that a diversified benchmark dataset has a significant
all_benchmark.md:2102.04664v2.txt:learning benchmark dataset for program understanding and genera-
all_benchmark.md:2102.04664v2.txt:3We plan to evolve the benchmark over time by extending to more tasks.To make it easy for participants, we provide three baseline mod-
all_benchmark.md:2102.04664v2.txt:BigCloneBench is a widely used large code clone benchmark
all_benchmark.md:2102.04664v2.txt:diversified benchmark dataset that can be applied to various code
all_benchmark.md:2102.04664v2.txt:mad Mamun Mia. 2014. Towards a big data curated benchmark of inter-project
all_benchmark.md:2102.04664v2.txt:Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural
all_benchmark.md:2105.12655v2.txt:to benchmark and help accelerate research in AI techniques for a variety of crit-
all_benchmark.md:2105.12655v2.txt:large benchmark datasets that are customized to their downstream use. See Figure 1 for a summary.
all_benchmark.md:2105.12655v2.txt:POJ-104-like benchmark and used in some publications. This benchmark dataset, GCJ-297 [ 23],
all_benchmark.md:2105.12655v2.txt:benchmark is approximately 10 times larger than POJ-104.
all_benchmark.md:2105.12655v2.txt:CodeNet has a rich set of code samples, and the user can assemble a customized benchmark according
all_benchmark.md:2105.12655v2.txt:to his/her need. Following POJ-104, we extracted benchmark datasets from CodeNet in C++, Python,
all_benchmark.md:2105.12655v2.txt:and Java. The benchmark characteristics are shown in Table 6. For the C++ benchmarks, the number
all_benchmark.md:2105.12655v2.txt:of problems and their solutions are chosen to make the benchmark challenging. The benchmarks are
all_benchmark.md:2105.12655v2.txt:summarizes the SPT statistics for the four benchmarks.
all_benchmark.md:2105.12655v2.txt:task, and a token inference task, using the four benchmark datasets (see Table 6) extracted from
all_benchmark.md:2105.12655v2.txt:model is ﬁne-tuned on each benchmark.
all_benchmark.md:2105.12655v2.txt:Table 7 summarizes the classiﬁcation accuracy for all models on all benchmarks. Despite the
all_benchmark.md:2105.12655v2.txt:benchmarks.
all_benchmark.md:2105.12655v2.txt:even though C-BERT is pre-trained with C programs, its performance on the two C++ benchmarks is
all_benchmark.md:2105.12655v2.txt:Table 8 provides results of code classiﬁcation of all four benchmarks. The columns give the benchmark
all_benchmark.md:2105.12655v2.txt:benchmark. The neural networks used for classiﬁcation of other benchmarks are similar to this one.
all_benchmark.md:2105.12655v2.txt:C++1400 benchmark dataset is signiﬁcantly better than the potential 0.071% accuracy of random
all_benchmark.md:2105.12655v2.txt:Table 9 shows results of code classiﬁcation on all four benchmarks by using the sequence-of-tokens
all_benchmark.md:2105.12655v2.txt:representation. The columns give the benchmark name, the test accuracy, the number of training
all_benchmark.md:2105.12655v2.txt:we choose for the C++1400 benchmark. It is a multi-layer convolutional neural network. It uses
all_benchmark.md:2105.12655v2.txt:Using this network we get a test accuracy 93.71 0.18% for C++1400 benchmark dataset, which is
all_benchmark.md:2105.12655v2.txt:used for classiﬁcation of other benchmarks are similar to the one shown in Figure 5. As we see in
all_benchmark.md:2105.12655v2.txt:benchmark. Additionally, we experiment with the POJ-104 dataset, which contains code examples in
all_benchmark.md:2105.12655v2.txt:languages share common tokens, we could apply the C-BERT model directly on the benchmarks.
all_benchmark.md:2105.12655v2.txt:After pretraining, we ﬁne tune the model for ﬁve epochs on each benchmark, with a batch size 32 and
all_benchmark.md:2105.12655v2.txt:Table 10 summarizes the accuracies C-BERT achives on the four CodeNet benchmarks as well as the
all_benchmark.md:2105.12655v2.txt:The relatively low performance on C++ benchmarks is possibly related to the idiosyncrasies of the
all_benchmark.md:2105.12655v2.txt:We conduct 6/2/2 random split for each of the 4 benchmarks: i.e., 60% training data, 20% testing
all_benchmark.md:2105.12655v2.txt:data, and 20% validation data. We run ﬁve folds for each benchmark with early stop ”patience”
all_benchmark.md:2105.12655v2.txt:are conducted on one NVIDIA V100 GPU. For large benchmarks such as C++1000 and C++1400, it
all_benchmark.md:2105.12655v2.txt:Exploring further into the Java250 benchmark, Table 14 summarizes the MAP@R score with a variety
all_benchmark.md:2105.12655v2.txt:Table 15 provides results of code similarity analysis on all four benchmarks. The columns give the
all_benchmark.md:2105.12655v2.txt:benchmark name, the test accuracy, the number of training epochs, the number of samples in each
all_benchmark.md:2105.12655v2.txt:Figure 7 shows the neural network used for code similarity analysis on the C++1400 benchmark. The
all_benchmark.md:2105.12655v2.txt:neural networks used for code similarity analysis on other benchmarks are similar to this one. As we
all_benchmark.md:2105.12655v2.txt:As we see in Table 15, the model accuracy is rather modest (<87%) for all benchmark datasets, which
all_benchmark.md:2105.12655v2.txt:Table 16 provides results of code similarity analysis on all four benchmarks. The columns give the
all_benchmark.md:2105.12655v2.txt:benchmark name, the test accuracy, the number of training epochs, the number of samples in each
all_benchmark.md:2105.12655v2.txt:The neural network for the C++1400 benchmark is depicted in Figure 8. The siamese parts of the
all_benchmark.md:2105.12655v2.txt:The network shows 96.56 0.07% test accuracy for C++1400 benchmark dataset. We consider this a
all_benchmark.md:2105.12655v2.txt:many keywords. The neural networks used for code similarity analysis of other benchmarks are
all_benchmark.md:2105.12655v2.txt:high memory requirement for computing MAP@R on the test set of CodeNet benchmarks, we had to
all_benchmark.md:2105.12655v2.txt:Models trained on the CodeNet benchmark datasets can beneﬁt greatly from their high quality. To
all_benchmark.md:2105.12655v2.txt:unseen test set. We train a popular BERT-like attention model on the C++1000 CodeNet benchmark
all_benchmark.md:2105.12655v2.txt:only unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code
all_benchmark.md:2105.12655v2.txt:Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
all_benchmark.md:2105.12655v2.txt:Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
all_benchmark.md:2105.12655v2.txt:Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
all_benchmark.md:2105.12655v2.txt:Shengyu Fu, and Shujie Liu. CodeXGLUE: A machine learning benchmark dataset for code
all_benchmark.md:2203.09095v2.txt:GitHub code review data, we create pre-training and benchmark
all_benchmark.md:2203.09095v2.txt:We further evaluate our CodeReviewer model on our benchmark
all_benchmark.md:2203.09095v2.txt:a high-quality benchmark dataset for evaluation in nine
all_benchmark.md:2203.09095v2.txt:the benchmark dataset for the three downstream tasks. To prevent
all_benchmark.md:2203.09095v2.txt:benchmark dataset. Other code repositories with [1,500,2,500)pull
all_benchmark.md:2203.09095v2.txt:of the pre-training dataset. For benchmark datasets, details are
all_benchmark.md:2203.09095v2.txt:Table 3: Statistics of benchmark datasets.
all_benchmark.md:2203.09095v2.txt:training and a benchmark for evaluation on the three tasks. It is
all_benchmark.md:2212.09132v1.txt:benchmark (Lu et al 2021) for code completion task.
all_benchmark.md:2212.09132v1.txt:and establish benchmarks for tasks. The diversity of representations facilitates
all_benchmark.md:2212.09132v1.txt:code completions from synthetic benchmarks and real-world completions. Al-
all_benchmark.md:2212.09132v1.txt:though all forms of benchmarks are useful (Karmakar 2019), it found that
all_benchmark.md:2212.09132v1.txt:synthetic benchmarks underweighted the frequencies of method completions
all_benchmark.md:2212.09132v1.txt:sequences, as exhibited in specialized benchmarks (Tay et al 2020a).
all_benchmark.md:2212.09132v1.txt:Karmakar A (2019) Establishing benchmarks for learning program representations. In: SAT-
all_benchmark.md:2212.09132v1.txt:D, Tang D, et al (2021) Codexglue: A machine learning benchmark dataset for code
all_benchmark.md:2212.09132v1.txt:Mir AM, Latoskinas E, Gousios G (2021) Manytypes4py: A benchmark python dataset for
all_benchmark.md:2212.09132v1.txt:D (2020a) Long range arena: A benchmark for ecient transformers. arXiv preprint
all_benchmark.md:2212.09132v1.txt:Wang K, Christodorescu M (2019) Coset: A benchmark for evaluating neural program em-
all_benchmark.md:2305.06161v2.txt:benchmarks (Lai et al., 2022; Cassano et al., 2023; Pearce et al., 2022; Fried et al., 2022; Yee & Guha,
all_benchmark.md:2305.06161v2.txt:positives we found during the evaluation on this benchmark. This modification boosted the F1 score of the
all_benchmark.md:2305.06161v2.txt:benchmarks are further described in Section 6.) To give an indication of the amount of data removed by
all_benchmark.md:2305.06161v2.txt:et al., 2021), and DS-1000 (Lai et al., 2022) evaluation benchmarks. Then we cover multi-language evaluation
all_benchmark.md:2305.06161v2.txt:using a variety of benchmarks and tasks.
all_benchmark.md:2305.06161v2.txt:code models, utilizing data parallelism and docker containers for execution. It supports several benchmarks,
all_benchmark.md:2305.06161v2.txt:for Python, Java, and C ++. CodeGeeX also includes its own multi-language benchmark suite,
all_benchmark.md:2305.06161v2.txt:et al., 2021), which are two widely used benchmarks of Python performance. However, we also measure
all_benchmark.md:2305.06161v2.txt:performance on DS-1000 (Lai et al., 2022), a code completion benchmark of 1,000 Python data science
all_benchmark.md:2305.06161v2.txt:HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021) are widely-used benchmarks for Code LLMs
all_benchmark.md:2305.06161v2.txt:using the pass@ kmetric (Chen et al., 2021): the total fraction of benchmark problems solved, where a
all_benchmark.md:2305.06161v2.txt:1.StarCoder is the highest-performing open-access model on both benchmarks .
all_benchmark.md:2305.06161v2.txt:representative of the code that most programmers write. In contrast, the DS-1000 benchmark (Lai et al.,
all_benchmark.md:2305.06161v2.txt:benchmark. Moreover, this is true across every kind of data science library.
all_benchmark.md:2305.06161v2.txt:3.We confirm the finding by Lai et al. (2022): model performance on HumanEval and MBPP benchmarks
all_benchmark.md:2305.06161v2.txt:does not always correlate with performance on the more realistic DS-1000 benchmarks . For example,
all_benchmark.md:2305.06161v2.txt:models on a range of benchmarks.
all_benchmark.md:2305.06161v2.txt:to generate code on a broader set of Python libraries, we use the ODEX benchmark (Wang et al., 2022)
all_benchmark.md:2305.06161v2.txt:benchmark.
all_benchmark.md:2305.06161v2.txt:Table 14: Performance on the ODEX benchmark by instruction languages and code domains: openproblems
all_benchmark.md:2305.06161v2.txt:MBPP (Austin et al., 2021) Python benchmarks into 18 other programming languages as follows.
all_benchmark.md:2305.06161v2.txt:MultiPL-E has a set of rule-based compilers that translate Python benchmarks to each target programming
all_benchmark.md:2305.06161v2.txt:language. Each compiler expects a benchmark in the HumanEval format: 1) a natural language description
all_benchmark.md:2305.06161v2.txt:have doctests) into a target language. Thus, MultiPL-E gives us a parallel set of benchmarks derived from
all_benchmark.md:2305.06161v2.txt:language. MultiPL-E also omits three HumanEval benchmarks that do not fit the above format. These changes have a small
all_benchmark.md:2305.06161v2.txt:Table 16: Performance on the Asleep at the Keyboard security benchmark (Pearce et al., 2022).
all_benchmark.md:2305.06161v2.txt:TheAsleep at the Keyboard benchmark by Pearce et al. (2022) has 89 security-sensitive scenarios across
all_benchmark.md:2305.06161v2.txt:OpenAI’s code-cushman-001. We use the original benchmarking methodology: generating 25 completions
all_benchmark.md:2305.06161v2.txt:Table 17: Performance on single-line fill-in-the-middle on the FIM benchmark by Ben Allal et al. (2023).
all_benchmark.md:2305.06161v2.txt:et al. (2020) benchmarks. We report both the overall F1 scores, which include trivial None-type prediction,
all_benchmark.md:2305.06161v2.txt:left-to-right code completion. We evaluate StarCoderBase on four established FIM benchmarks below.
all_benchmark.md:2305.06161v2.txt:Ben Allal et al. (2023) generalizes this benchmark to also support Java and JavaScript, using model-generated
all_benchmark.md:2305.06161v2.txt:TypeScript. However, instead of measuring accuracy, they argue that benchmarks should measure how many
all_benchmark.md:2305.06161v2.txt:use the Python subset of the CodeXGLUE code summarization benchmark (Lu et al., 2021). This benchmark
all_benchmark.md:2305.06161v2.txt:Table 22: 8-shot accuracy on the GSM8K math-reasoning benchmark. Samples are generated with greedy
all_benchmark.md:2305.06161v2.txt:this reasoning benchmark. However, its performance still lags behind LLaMA-33B (38.7%).
all_benchmark.md:2305.06161v2.txt:Table 23: 5-shot accuracy on the MMLU language understanding benchmark.
all_benchmark.md:2305.06161v2.txt:MMLU (Hendrycks et al., 2020) is a massive multitask language understanding benchmark, covering multiple-
all_benchmark.md:2305.06161v2.txt:benchmarks that measure social bias and toxicity in model-produced text.15
all_benchmark.md:2305.06161v2.txt:Table 27: Model results on natural language reasoning tasks in the HELM benchmark, with models ordered
all_benchmark.md:2305.06161v2.txt:HELM benchmark does not include the CodeGen, CodeGeex, and LLaMA models. Therefore, we compare
all_benchmark.md:2305.06161v2.txt:the HumanEval benchmark.
all_benchmark.md:2305.06161v2.txt:English-only evaluations We evaluated the performance of StarCoder solely on English-based benchmarks
all_benchmark.md:2305.06161v2.txt:and Abhinav Jangda. MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation.
all_benchmark.md:2305.06161v2.txt:Daniel Fried, Sida Wang, and Tao Yu. DS-1000: a natural and reliable benchmark for data science code
all_benchmark.md:2305.06161v2.txt:CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. arXiv
all_benchmark.md:2305.06161v2.txt:"-benchmark",
all_benchmark.md:2305.06161v2.txt:We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several
all_benchmark.md:2305.06161v2.txt:We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these
all_benchmark.md:2308.10462v3.txt:challenging benchmark with test cases.
all_benchmark.md:2308.10462v3.txt:evaluation datasets such as HumanEval [ 9] have extensively been used to benchmark code generation
all_benchmark.md:2308.10462v3.txt:benchmarking neural code generation. IEEE Transactions on Software Engineering (2023).
all_benchmark.md:2308.10462v3.txt:Daxin Jiang, Duyu Tang, et al .2021. Codexglue: A machine learning benchmark dataset for code understanding and
all_benchmark.md:2309.03914v2.txt:natural and reliable benchmark for data science code generation. In International
all_benchmark.md:2401.12954v1.txt:benchmarks,includingtheGameof24(Yaoetal .,2023a),Checkmate-in-OnefromtheBIG-Benchsuite(BIG-
all_benchmark.md:2401.12954v1.txt:efficacyofthesepromptingmethodsacrossabroadsetoftasksandbenchmarks. Morerecentinnovations
all_benchmark.md:2402.01035v2.txt:tokenizer, and close to Punct on coding benchmarks while
all_benchmark.md:2402.01035v2.txt:eration with multilingual benchmarking on humaneval-x.
all_benchmark.md:2402.11811v4.txt:benchmarks and six testing models.1
all_benchmark.md:2402.11811v4.txt:various benchmarks (Cobbe et al., 2021; Suzgun
all_benchmark.md:2402.11811v4.txt:downstream benchmarks and three diverse
all_benchmark.md:2402.11811v4.txt:We include five benchmarks across two most com-
all_benchmark.md:2402.11811v4.txt:benchmarks. MMLU covers 14k questions. For
all_benchmark.md:2402.11811v4.txt:benchmarks. While differently, since both APE and
all_benchmark.md:2402.11811v4.txt:“generation ” or “ multi-choice ” benchmarks takes
all_benchmark.md:2402.11811v4.txt:report the accuracy score for all benchmarks.
all_benchmark.md:2402.11811v4.txt:gains on different downstream generators acrossfive public benchmarks, shown in Table 2. The op-
all_benchmark.md:2402.11811v4.txt:The number attached with the benchmark is the number of in-context examples (e.g., BBH (3) means 3-shot testing
all_benchmark.md:2402.11811v4.txt:all benchmarks, except BBH, which is due to its
all_benchmark.md:2402.11811v4.txt:downstream testing benchmarks, discussing the ef-
all_benchmark.md:2402.11811v4.txt:Table 4: Examples from testing benchmarks.
all_benchmark.md:2402.11811v4.txt:benchmark. In International Conference on Machine
all_benchmark.md:2402.11811v4.txt:all benchmarks, except BBH. We suppose this may
all_benchmark.md:2402.11811v4.txt:256 random optimized prompts for each benchmark.
all_benchmark.md:2402.11811v4.txt:benchmark, manually checking with 256 random
all_benchmark.md:2403.18350v2.txt:to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges
all_benchmark.md:2403.18350v2.txt:potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of
all_benchmark.md:2403.18350v2.txt:addressing the notable lack of benchmarks and baseline data. Additionally, we seek to explore the impact of semantic
all_benchmark.md:2403.18350v2.txt:sorted by diminishing relevance to set a benchmark for the lowest achievable scores).
all_benchmark.md:2403.19340v2.txt:moving contaminated data such as benchmark
all_benchmark.md:2403.19340v2.txt:the elimination of benchmarks or other unintended
all_benchmark.md:2403.19340v2.txt:benchmark and contamination for language mod-
all_benchmark.md:2404.13506v2.txt:benchmarks such as Alpaca-Eval v1.0 and GLUE indicated
all_benchmark.md:2404.13506v2.txt:MSR-VTT and ActivityNet benchmarks. Their research
all_benchmark.md:2404.13506v2.txt:benchmark, this approach notably excels by updating merely
all_benchmark.md:2404.13506v2.txt:managed to outperform the full fine-tuning benchmarks on
all_benchmark.md:2404.13506v2.txt:Revisiting point cloud classification: A new benchmark
all_benchmark.md:2405.18414v1.txt:highlight the advantages of the proposed G-RAG over state-of-the-art benchmarks, we conducted
all_benchmark.md:2405.18414v1.txt:benchmark for question answering research. Transactions of the Association for Computational
all_benchmark.md:2405.19250v1.txt:of the HumanEval benchmark rewritten by human experts
all_benchmark.md:2405.19250v1.txt:on the HumanEval benchmark. Lastly, we discuss potential fu ture
all_benchmark.md:2405.19250v1.txt:and designing a more realistic benchmark for Kotlin.
all_benchmark.md:2405.19250v1.txt:generation of models can generate Kotlin code. To benchmark
all_benchmark.md:2405.19250v1.txt:of the HumanEval benchmark, speciﬁcally adapted for Kotlin .
all_benchmark.md:2405.19250v1.txt:programming languages. The ﬁndings from these benchmarks
all_benchmark.md:2405.19250v1.txt:on the HumanEval benchmark for both languages.
all_benchmark.md:2405.19250v1.txt:test-based benchmark involves reporting only the pass rate as
all_benchmark.md:2405.19250v1.txt:code. Since the size of the HumanEval benchmark allows one
all_benchmark.md:2405.19250v1.txt:HumanEval benchmark.
all_benchmark.md:2405.19250v1.txt:train loss and downstream benchmark scores at no extra
all_benchmark.md:2405.19250v1.txt:based benchmarking. In addition to its beneﬁts we describe
all_benchmark.md:2405.19250v1.txt:and practitioners. Second, HumanEval benchmark is a well-
all_benchmark.md:2405.19250v1.txt:established, popularr benchmark, which we further cleaned
all_benchmark.md:2405.19250v1.txt:and improved. Our completion benchmark, however, was not
all_benchmark.md:2405.19250v1.txt:the benchmark test dataset [25] (which is all the more possib le
all_benchmark.md:2405.19250v1.txt:once the benchmarks become public), artiﬁcially inﬂating t he
all_benchmark.md:2405.19250v1.txt:benchmarks.
all_benchmark.md:2405.19250v1.txt:•More benchmarks . While in this work we suggest the
all_benchmark.md:2405.19250v1.txt:most essential benchmark — HumanEval, it can hardly
all_benchmark.md:2405.19250v1.txt:been a new generation of issue-based benchmarks like
all_benchmark.md:2405.19250v1.txt:reliable benchmark for data science code generation,” in International
all_benchmark.md:2406.04712v1.txt:quality of the AICoderEval benchmark.
all_benchmark.md:2406.04712v1.txt:To address this challenge, we construct the AICoderEval dataset, a benchmark for AI-oriented
all_benchmark.md:2406.04712v1.txt:benchmark. Our primary focus is on generating Python code files using GPT-4, streamlining the
all_benchmark.md:2406.04712v1.txt:dataset size to around 2,000 code files. To construct our final benchmark, we further select a subset
all_benchmark.md:2406.04712v1.txt:process ensures the quality and reliability of the AICoderEval benchmark.
all_benchmark.md:2406.04712v1.txt:The AICoderEval benchmark is hosted on Hugging Face Datasets at https://huggingface.co/
all_benchmark.md:2406.04712v1.txt:Table 1 presents the distribution of task categories within the AICoderEval benchmark. Natural
all_benchmark.md:2406.04712v1.txt:domains showcases the breadth and depth of the AICoderEval benchmark.
all_benchmark.md:2406.04712v1.txt:To ensure the quality and integrity of the AICoderEval benchmark, we will conduct both automated
all_benchmark.md:2406.04712v1.txt:of the benchmark.
all_benchmark.md:2406.04712v1.txt:AICoderEval, as depicted in figure 2. This framework can construct domain-specific tasks benchmark,
all_benchmark.md:2406.04712v1.txt:for training and evaluation, and then fine-tunes a code generation model on the benchmark.
all_benchmark.md:2406.11931v1.txt:length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves
all_benchmark.md:2406.11931v1.txt:and Gemini 1.5 Pro in coding and math benchmarks.
all_benchmark.md:2406.11931v1.txt:Figure 1|The Performance of DeepSeek-Coder-V2 on math and code benchmarks.
all_benchmark.md:2406.11931v1.txt:to 37.2%) and MBPP (from 44.6% to 54.0%) benchmarks (Austin et al., 2021a; Chen et al.,
all_benchmark.md:2406.11931v1.txt:•Code : Regarding code generation benchmark evaluation, DeepSeek-Coder-V2 demon-
all_benchmark.md:2406.11931v1.txt:elementary benchmarks like GSM8K (Cobbe et al., 2021) and advanced competition-level
all_benchmark.md:2406.11931v1.txt:benchmarks including MATH (Hendrycks et al., 2021), AIME (MAA, 2024), and Math
all_benchmark.md:2406.11931v1.txt:on the MATH benchmark, nearly matching the state-of-the-art accuracy of 76.6% achieved
all_benchmark.md:2406.11931v1.txt:49.0%) benchmarks, respectively. Further training the 1B model with 2T tokens led to additional
all_benchmark.md:2406.11931v1.txt:et al., 2021b) benchmarks are commonly utilized for assessing the performance of code-generating
all_benchmark.md:2406.11931v1.txt:abilities of models, we extended the HumanEval benchmark problems into seven additional
all_benchmark.md:2406.11931v1.txt:For both benchmarks, we employed a greedy search strategy and recreated the baseline results
all_benchmark.md:2406.11931v1.txt:benchmark (Shi et al., 2024) to estimate the effectiveness of DeepSeek-Coder-V2. LiveCodeBench
all_benchmark.md:2406.11931v1.txt:benchmark contains 307 problems from the USA Computing Olympiad, along with high-quality
all_benchmark.md:2406.11931v1.txt:Table 4|Performance on the LiveCodeBench (LCB) and USACO benchmarks.
all_benchmark.md:2406.11931v1.txt:Table 4 showcases the performance of various language models on the two benchmarks.
all_benchmark.md:2406.11931v1.txt:modify one method from this benchmark.
all_benchmark.md:2406.11931v1.txt:SWE-bench is a comprehensive benchmark designed to evaluate the performance of large
all_benchmark.md:2406.11931v1.txt:language models in addressing real-world software issues sourced from GitHub. The benchmark
all_benchmark.md:2406.11931v1.txt:Aider’s code editing benchmark evaluates the LLM’s ability to modify Python source files,
all_benchmark.md:2406.11931v1.txt:completing 133 distinct coding tasks. This benchmark not only tests the LLM’s coding skills but
all_benchmark.md:2406.11931v1.txt:Table 7|Performances of different models on repair benchmarks. We do not evaluate
all_benchmark.md:2406.11931v1.txt:To assess the code reasoning capabilities of our models, we utilize the CRUXEval benchmark.
all_benchmark.md:2406.11931v1.txt:This benchmark comprises 800 Python functions paired with corresponding input-output
all_benchmark.md:2406.11931v1.txt:8 presents the performance of various language models on the CruxEval benchmark, which
all_benchmark.md:2406.11931v1.txt:Table 8|Performance of different models on the CruxEval benchmark.
all_benchmark.md:2406.11931v1.txt:grade-school benchmark GSM8K (Cobbe et al., 2021), along with advanced competition-level
all_benchmark.md:2406.11931v1.txt:benchmarks including MATH (Hendrycks et al., 2021), the American Invitational Mathematics
all_benchmark.md:2406.11931v1.txt:accuracy of 75.7% on the MATH benchmark and 53.7% on Math Odyssey, comparable to the
all_benchmark.md:2406.11931v1.txt:9The performance of DeepSeek-Coder-V2 on the four mathematical benchmarks was obtained with zero-shot
all_benchmark.md:2406.11931v1.txt:bility, even surpassing DeepSeek-V2 on reasoning-related benchmarks. We compare DeepSeek-
all_benchmark.md:2406.11931v1.txt:Coder-V2 Instruct with DeepSeek-V2 Chat on standard benchmarks, which covers both En-
all_benchmark.md:2406.11931v1.txt:glish and Chinese benchmarks, including BigBench Hard (BBH) (Suzgun et al., 2022), MMLU
all_benchmark.md:2406.11931v1.txt:Lite-Instruct outperforms DeepSeek-V2-Lite-Chat in benchmarks like BBH and Arena-Hard.
all_benchmark.md:2406.11931v1.txt:These benchmarks place a high demand on the model’s reasoning ability, which DeepSeek-
all_benchmark.md:2406.11931v1.txt:knowledge-intensive benchmarks like TriviaQA, primarily due to the relatively smaller amount
all_benchmark.md:2406.11931v1.txt:soning benchmarks, particularly in Arena-Hard, which comprises a substantial proportion of
all_benchmark.md:2406.11931v1.txt:slightly better results in benchmarks such as MT-bench (Zheng et al., 2023), AlpacaEval 2.0
all_benchmark.md:2406.11931v1.txt:Although DeepSeek-Coder-V2 achieves impressive performance on standard benchmarks,
all_benchmark.md:2406.11931v1.txt:J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering
all_benchmark.md:2406.11931v1.txt:high-quality benchmarks: The arena-hard pipeline, April 2024. URL https://lmsys.org/
all_benchmark.md:2406.11931v1.txt:nese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors,
all_benchmark.md:2406.11931v1.txt:human-centric benchmark for evaluating foundation models. CoRR , abs/2304.06364, 2023.
all_benchmark.md:2407.02485v1.txt:model with the state-of-the-art performance on RAG benchmarks. Specifically,
all_benchmark.md:2407.02485v1.txt:models on nine knowledge-intensive benchmarks. In addition, it also performs
all_benchmark.md:2407.02485v1.txt:comparably to GPT-4 on five RAG benchmarks in the biomedical domain without
all_benchmark.md:2407.02485v1.txt:benchmarks for RAG, Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-
all_benchmark.md:2407.02485v1.txt:we use the split from KILT benchmark (Petroni et al., 2021)2. (2) Fact verification , where we
all_benchmark.md:2407.02485v1.txt:use FEVER (Thorne et al., 2018) from KILT benchmark. (3) Conversational QA (ConvQA) , we
all_benchmark.md:2407.02485v1.txt:marked as “–”. We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct,
all_benchmark.md:2407.02485v1.txt:many RAG benchmarks. RankRAG 8B is also competitive when compared with baseline models
all_benchmark.md:2407.02485v1.txt:biomedical RAG benchmark. RankRAG and baselines use re-
all_benchmark.md:2407.02485v1.txt:intensive benchmarks and demonstrate RankRAG significantly outperform all of them on nine
all_benchmark.md:2407.02485v1.txt:general-domain and five biomedical benchmarks for RAG.
all_benchmark.md:2407.02485v1.txt:hension benchmark requiring discrete reasoning over paragraphs. In NAACL , 2019.
all_benchmark.md:2407.02485v1.txt:Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question answering
all_benchmark.md:2407.02485v1.txt:V ., Maillard, J., Plachouras, V ., Rocktäschel, T., and Riedel, S. KILT: a benchmark for knowledge
all_benchmark.md:2407.02485v1.txt:benchmark for zero-shot evaluation of information retrieval models. In NeurIPS , 2021.
all_benchmark.md:2407.02485v1.txt:answering benchmark on a hybrid of tabular and textual content in finance. In ACL, 2021.
all_benchmark.md:2407.02485v1.txt:evidence from Wikipedia, providing a benchmark for fact-checking systems.
all_benchmark.md:2407.02485v1.txt:for NQ and TriviaQA, respectively. In contrast, the KILT benchmark (Petroni et al., 2021) utilizes
all_benchmark.md:2407.08275v1.txt:and performance benchmarks such as the Massive Text Embedding
all_benchmark.md:2407.08275v1.txt:models on benchmark datasets offer the simplified perspective of
all_benchmark.md:2407.08275v1.txt:ding models, in contrast to performance benchmarks that require
all_benchmark.md:2407.08275v1.txt:popular benchmark datasets to determine if similarities between
all_benchmark.md:2407.08275v1.txt:on their performance on downstream tasks, with benchmarks such
all_benchmark.md:2407.08275v1.txt:cluded in the benchmarks [ 8] or where the evaluation encompasses
all_benchmark.md:2407.08275v1.txt:select five publicly available datasets from the BEIR benchmark [ 35].
all_benchmark.md:2407.08275v1.txt:the smaller datasets from the benchmark. This approach allows us
all_benchmark.md:2407.15462v4.txt:both the query-side and the item-side [ 6]. For common benchmark
all_benchmark.md:2407.15462v4.txt:We benchmark MoL with the proposed load balancing loss L𝑀𝐼, on
all_benchmark.md:2407.15462v4.txt:new state-of-the-art across common, heterogeneous benchmark
all_benchmark.md:2407.15462v4.txt:[1][n. d.]. ANN Benchmarks. https://ann-benchmarks.com/. Accessed: 2024-08-06.
all_benchmark.md:2409.10959v1.txt:GitHub code review dataset, which is now widely employed as the benchmark dataset [44, 46, 47, 65].
all_benchmark.md:2409.10959v1.txt:Dataset selection. We use the CodeReviewer dataset provided by Li et al. [ 43], which is the benchmark dataset for the
all_benchmark.md:2409.10959v1.txt:metric [ 55] with up to 4-gram matching to benchmark the new approaches in terms of accuracy against the test
all_benchmark.md:2409.10959v1.txt:Associates Inc., Red Hook, NY, USA. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c16a5320fa475530d9583c34fd356ef5-
all_benchmark.md:2410.20424v3.txt:Tang et al. (2024) introduces ML-Bench, a benchmark for language agents for machine learning
all_benchmark.md:2411.10129v1.txt:“A survey of large language models for code: Evolution, benchmarking,
all_benchmark.md:2411.10129v1.txt:torical data an appropriate benchmark for reviewer recommendation sys-tems?: A case study of the gerrit community,” in 2021 36th IEEE/ACM
all_benchmark.md:2411.10129v1.txt:learning benchmark dataset for code understanding and generation,”
all_benchmark.md:2501.03265v1.txt:development of unified standards, tools, benchmarks and
all_benchmark.md:2502.02757v2.txt:to clean their widely-used CodeReviewer benchmark using
all_benchmark.md:2502.02757v2.txt:benchmark (CodeReviewer [5]), we find that only 64% of
all_benchmark.md:2502.02757v2.txt:benchmark are valid. Our approach using LLMs achieved
all_benchmark.md:2502.02757v2.txt:stantial noise persists in current benchmark datasets including
all_benchmark.md:2502.02757v2.txt:benchmarks in their study, our 32% specifically refers to the CodeReviewer
all_benchmark.md:2502.02757v2.txt:benchmark, which was obtained from their replication package.
all_benchmark.md:2502.02757v2.txt:benchmark dataset quality by providing more reliable test sets
all_benchmark.md:2502.02757v2.txt:code: Evolution, benchmarking, and future trends,” ACM
all_benchmark.md:2502.14862v1.txt:poor performance on standard benchmarks. Re-
all_benchmark.md:2502.20273v4.txt:Figure 3 displays benchmark results for the UnigramLM
all_benchmark.md:2502.20273v4.txt:phological benchmarks, while showing some fluctuations,
all_benchmark.md:2503.13505v1.txt:benchmarks for instruction-following tasks, evaluating models
all_benchmark.md:2503.13505v1.txt:cost-effective methods have been benchmarked [24], [61]. The
all_benchmark.md:2503.13505v1.txt:K. Keutzer, and S. K. Upadhyay, “Routerbench: A benchmark for
all_benchmark.md:2503.13505v1.txt:son, and M. Yurochkin, “LLM routing with benchmark datasets,” in
all_benchmark.md:2504.10046v1.txt:gate the effectiveness of our framework on the DevEval benchmark
all_benchmark.md:2504.10046v1.txt:searchers have developed specialized benchmarks like DevEval[ 18]
all_benchmark.md:2504.10046v1.txt:Huang, and Yongbin Li. Evocodebench: An evolving code generation benchmark
all_benchmark.md:2505.24581v1.txt:the MTEB benchmark. GATE leverages Ma-
all_benchmark.md:2505.24581v1.txt:improvement on STS benchmarks, effectively
all_benchmark.md:2505.24581v1.txt:benchmarking, and error analysis.
all_benchmark.md:2505.24581v1.txt:benchmark for evaluating embedding models.Matryoshka Representation Learning (MRL) has
all_benchmark.md:2505.24581v1.txt:mantic datasets, setting a new benchmark for Ara-
all_benchmark.md:2505.24581v1.txt:small, serves as a comparative benchmark for
all_benchmark.md:2505.24581v1.txt:Table 3: Performance comparison of Matryoshka models vs. their base counterparts on MTEB benchmarks.
all_benchmark.md:2505.24581v1.txt:across Arabic NLP benchmarks.
all_benchmark.md:2505.24581v1.txt:on Arabic STS benchmarks
all_benchmark.md:2505.24581v1.txt:across the three Arabic STS benchmarks in MTEB.
all_benchmark.md:2505.24581v1.txt:Figure 2: Performance comparison between Matryoshka models and larger models on MTEB Arabic benchmarks.
all_benchmark.md:2505.24581v1.txt:comprehensive Arabic NLP benchmarks restricts abroader evaluation beyond STS tasks. Additionally,
all_benchmark.md:2505.24581v1.txt:uations on MTEB benchmarks confirmed strong
all_benchmark.md:2505.24581v1.txt:Future work will extend Arabic NLP benchmarks,
all_benchmark.md:2505.24581v1.txt:benchmark. arXiv preprint arXiv:2210.07316 .
