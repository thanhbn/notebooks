{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - LangFuse Evaluations & Prompt Management\n",
    "\n",
    "## üìñ M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "\n",
    "Notebook n√†y s·∫Ω h∆∞·ªõng d·∫´n b·∫°n:\n",
    "- Hi·ªÉu c√°ch th·ª©c ƒë√°nh gi√° (evaluation) h·ªá th·ªëng LLM v·ªõi LangFuse\n",
    "- Th·ª±c hi·ªán ƒë√°nh gi√° th·ªß c√¥ng (human feedback) v√† t·ª± ƒë·ªông\n",
    "- Qu·∫£n l√Ω prompt versions th√¥ng qua LangFuse Prompt Management\n",
    "- Theo d√µi hi·ªáu su·∫•t v√† c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng model qua th·ªùi gian\n",
    "- T√≠ch h·ª£p evaluation workflow v√†o quy tr√¨nh ph√°t tri·ªÉn LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü Gi·ªõi thi·ªáu\n",
    "\n",
    "### T·∫ßm quan tr·ªçng c·ªßa Evaluation trong LLM Development\n",
    "\n",
    "ƒê√°nh gi√° l√† m·ªôt ph·∫ßn thi·∫øt y·∫øu trong ph√°t tri·ªÉn ·ª©ng d·ª•ng LLM v√¨:\n",
    "\n",
    "1. **ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng**: Ki·ªÉm tra ƒë·ªô ch√≠nh x√°c v√† ph√π h·ª£p c·ªßa output\n",
    "2. **Theo d√µi hi·ªáu su·∫•t**: Gi√°m s√°t s·ª± thay ƒë·ªïi ch·∫•t l∆∞·ª£ng theo th·ªùi gian\n",
    "3. **A/B Testing**: So s√°nh hi·ªáu qu·∫£ c·ªßa c√°c prompt/model kh√°c nhau\n",
    "4. **Compliance & Safety**: ƒê·∫£m b·∫£o output tu√¢n th·ªß quy ƒë·ªãnh v√† an to√†n\n",
    "\n",
    "### LangFuse Evaluation Features\n",
    "\n",
    "- **Human Feedback**: Thu th·∫≠p ƒë√°nh gi√° t·ª´ ng∆∞·ªùi d√πng th·ª±c t·∫ø\n",
    "- **Automated Scoring**: ƒê√°nh gi√° t·ª± ƒë·ªông d·ª±a tr√™n rule-based ho·∫∑c model-based\n",
    "- **Prompt Management**: Version control cho prompts v·ªõi A/B testing\n",
    "- **Analytics Dashboard**: Visualize evaluation metrics v√† trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è C√†i ƒë·∫∑t & C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t dependencies c·∫ßn thi·∫øt\n",
    "!pip install langfuse langchain-anthropic python-dotenv uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langfuse import Langfuse\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o LangFuse client\n",
    "langfuse = Langfuse(\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    ")\n",
    "\n",
    "# Kh·ªüi t·∫°o ChatAnthropic\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"üöÄ ƒê√£ kh·ªüi t·∫°o LangFuse v√† ChatAnthropic\")\n",
    "print(f\"üìä LangFuse Host: {os.getenv('LANGFUSE_HOST', 'http://localhost:3000')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç V√≠ d·ª• 1: Ghi nh·∫≠n ƒë√°nh gi√° th·ªß c√¥ng (Human Feedback)\n",
    "\n",
    "Human feedback l√† c√°ch quan tr·ªçng nh·∫•t ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng output c·ªßa LLM v√¨ ch·ªâ con ng∆∞·ªùi m·ªõi c√≥ th·ªÉ ƒë√°nh gi√° ƒë∆∞·ª£c context, tone, v√† appropriateness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def generate_customer_response(customer_query: str, context: str = \"\") -> str:\n",
    "    \"\"\"T·∫°o ph·∫£n h·ªìi cho kh√°ch h√†ng d·ª±a tr√™n query v√† context\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"B·∫°n l√† m·ªôt chuy√™n vi√™n h·ªó tr·ª£ kh√°ch h√†ng chuy√™n nghi·ªáp. \n",
    "    H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng m·ªôt c√°ch th√¢n thi·ªán, ch√≠nh x√°c v√† h·ªØu √≠ch.\n",
    "    N·∫øu c√≥ th√¥ng tin context, h√£y s·ª≠ d·ª•ng ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ph√π h·ª£p nh·∫•t.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=f\"Context: {context}\\n\\nC√¢u h·ªèi kh√°ch h√†ng: {customer_query}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test customer service responses\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"T√¥i mu·ªën ƒë·ªïi tr·∫£ s·∫£n ph·∫©m ƒë√£ mua 3 ng√†y tr∆∞·ªõc\",\n",
    "        \"context\": \"Ch√≠nh s√°ch ƒë·ªïi tr·∫£: 7 ng√†y, s·∫£n ph·∫©m c√≤n nguy√™n seal\",\n",
    "        \"trace_id\": str(uuid.uuid4())\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"L√†m sao ƒë·ªÉ theo d√µi ƒë∆°n h√†ng c·ªßa t√¥i?\",\n",
    "        \"context\": \"H·ªá th·ªëng tracking c√≥ s·∫µn tr√™n website, c·∫ßn m√£ ƒë∆°n h√†ng\",\n",
    "        \"trace_id\": str(uuid.uuid4())\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"S·∫£n ph·∫©m n√†y c√≥ b·∫£o h√†nh bao l√¢u?\",\n",
    "        \"context\": \"ƒêi·ªán tho·∫°i iPhone: b·∫£o h√†nh 12 th√°ng ch√≠nh h√£ng\",\n",
    "        \"trace_id\": str(uuid.uuid4())\n",
    "    }\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for test_case in test_queries:\n",
    "    # Set trace context\n",
    "    langfuse_context.configure(\n",
    "        trace_id=test_case[\"trace_id\"],\n",
    "        session_id=\"customer_support_evaluation\",\n",
    "        user_id=\"evaluator_001\"\n",
    "    )\n",
    "    \n",
    "    response = generate_customer_response(\n",
    "        customer_query=test_case[\"query\"],\n",
    "        context=test_case[\"context\"]\n",
    "    )\n",
    "    \n",
    "    responses.append({\n",
    "        \"query\": test_case[\"query\"],\n",
    "        \"response\": response,\n",
    "        \"trace_id\": test_case[\"trace_id\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"‚ùì Query: {test_case['query']}\")\n",
    "    print(f\"ü§ñ Response: {response}\")\n",
    "    print(f\"üîó Trace ID: {test_case['trace_id']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate human feedback collection\n",
    "def collect_human_feedback(trace_id: str, response_text: str, query: str):\n",
    "    \"\"\"Thu th·∫≠p feedback t·ª´ ng∆∞·ªùi ƒë√°nh gi√°\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìã ƒê√°nh gi√° ph·∫£n h·ªìi cho: {query}\")\n",
    "    print(f\"ü§ñ Ph·∫£n h·ªìi: {response_text}\")\n",
    "    print(\"\\nüìä Ti√™u ch√≠ ƒë√°nh gi√°:\")\n",
    "    print(\"1. Helpfulness (1-5): M·ª©c ƒë·ªô h·ªØu √≠ch\")\n",
    "    print(\"2. Accuracy (1-5): ƒê·ªô ch√≠nh x√°c\")\n",
    "    print(\"3. Friendliness (1-5): ƒê·ªô th√¢n thi·ªán\")\n",
    "    print(\"4. Overall (1-5): ƒê√°nh gi√° t·ªïng th·ªÉ\")\n",
    "    \n",
    "    # Simulate evaluator scores (trong th·ª±c t·∫ø s·∫Ω input t·ª´ UI)\n",
    "    import random\n",
    "    scores = {\n",
    "        \"helpfulness\": random.randint(3, 5),\n",
    "        \"accuracy\": random.randint(3, 5),\n",
    "        \"friendliness\": random.randint(3, 5),\n",
    "        \"overall\": random.randint(3, 5)\n",
    "    }\n",
    "    \n",
    "    # T·∫°o comment m·∫´u\n",
    "    comments = [\n",
    "        \"Ph·∫£n h·ªìi r·∫•t chuy√™n nghi·ªáp v√† ƒë·∫ßy ƒë·ªß th√¥ng tin\",\n",
    "        \"C·∫ßn c·∫£i thi·ªán th√™m v·ªÅ ƒë·ªô chi ti·∫øt\",\n",
    "        \"Tone r·∫•t ph√π h·ª£p v·ªõi kh√°ch h√†ng\",\n",
    "        \"Ch√≠nh x√°c v√† h·ªØu √≠ch\"\n",
    "    ]\n",
    "    \n",
    "    feedback_comment = random.choice(comments)\n",
    "    \n",
    "    return scores, feedback_comment\n",
    "\n",
    "# Thu th·∫≠p feedback cho t·∫•t c·∫£ responses\n",
    "for response_data in responses:\n",
    "    scores, comment = collect_human_feedback(\n",
    "        response_data[\"trace_id\"],\n",
    "        response_data[\"response\"],\n",
    "        response_data[\"query\"]\n",
    "    )\n",
    "    \n",
    "    # Ghi feedback v√†o LangFuse\n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"helpfulness\",\n",
    "        value=scores[\"helpfulness\"],\n",
    "        comment=f\"Human feedback: {comment}\"\n",
    "    )\n",
    "    \n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"accuracy\",\n",
    "        value=scores[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"friendliness\",\n",
    "        value=scores[\"friendliness\"]\n",
    "    )\n",
    "    \n",
    "    langfuse.score(\n",
    "        trace_id=response_data[\"trace_id\"],\n",
    "        name=\"overall_rating\",\n",
    "        value=scores[\"overall\"],\n",
    "        comment=comment\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ ghi feedback cho trace: {response_data['trace_id'][:8]}...\")\n",
    "    print(f\"üìä Scores: {scores}\")\n",
    "    print(f\"üí¨ Comment: {comment}\\n\")\n",
    "\n",
    "print(\"üéâ Ho√†n th√†nh thu th·∫≠p human feedback!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ V√≠ d·ª• 2: ƒê√°nh gi√° t·ª± ƒë·ªông (Automated Evaluation)\n",
    "\n",
    "Automated evaluation cho ph√©p scale vi·ªác ƒë√°nh gi√° v√† ƒë·∫£m b·∫£o consistency. Ch√∫ng ta s·∫Ω implement c√°c evaluator cho JSON format, safety, v√† content quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Format Evaluator\n",
    "def evaluate_json_format(response_text: str) -> Dict:\n",
    "    \"\"\"Ki·ªÉm tra xem response c√≥ ƒë√∫ng format JSON kh√¥ng\"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(response_text)\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"passed\": True,\n",
    "            \"reason\": \"Valid JSON format\",\n",
    "            \"details\": {\"keys\": list(parsed.keys()) if isinstance(parsed, dict) else \"array\"}\n",
    "        }\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Invalid JSON: {str(e)}\",\n",
    "            \"details\": {\"error\": str(e)}\n",
    "        }\n",
    "\n",
    "# Safety Evaluator\n",
    "def evaluate_safety(response_text: str) -> Dict:\n",
    "    \"\"\"Ki·ªÉm tra content safety (simplified version)\"\"\"\n",
    "    unsafe_keywords = [\n",
    "        \"hack\", \"phishing\", \"scam\", \"illegal\", \"virus\",\n",
    "        \"password\", \"credit card\", \"sensitive\"\n",
    "    ]\n",
    "    \n",
    "    found_unsafe = []\n",
    "    for keyword in unsafe_keywords:\n",
    "        if keyword.lower() in response_text.lower():\n",
    "            found_unsafe.append(keyword)\n",
    "    \n",
    "    if found_unsafe:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Unsafe keywords detected: {found_unsafe}\",\n",
    "            \"details\": {\"unsafe_keywords\": found_unsafe}\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"passed\": True,\n",
    "            \"reason\": \"No unsafe content detected\",\n",
    "            \"details\": {\"checked_keywords\": len(unsafe_keywords)}\n",
    "        }\n",
    "\n",
    "# Length Evaluator\n",
    "def evaluate_response_length(response_text: str, min_length: int = 50, max_length: int = 500) -> Dict:\n",
    "    \"\"\"Ki·ªÉm tra ƒë·ªô d√†i ph·∫£n h·ªìi c√≥ ph√π h·ª£p kh√¥ng\"\"\"\n",
    "    length = len(response_text)\n",
    "    \n",
    "    if length < min_length:\n",
    "        return {\n",
    "            \"score\": 0.3,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Response too short: {length} chars (min: {min_length})\",\n",
    "            \"details\": {\"length\": length, \"min_required\": min_length}\n",
    "        }\n",
    "    elif length > max_length:\n",
    "        return {\n",
    "            \"score\": 0.7,\n",
    "            \"passed\": False,\n",
    "            \"reason\": f\"Response too long: {length} chars (max: {max_length})\",\n",
    "            \"details\": {\"length\": length, \"max_allowed\": max_length}\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"passed\": True,\n",
    "            \"reason\": f\"Appropriate length: {length} chars\",\n",
    "            \"details\": {\"length\": length, \"min\": min_length, \"max\": max_length}\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c automated evaluators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def generate_product_info(product_name: str, format_type: str = \"json\") -> str:\n",
    "    \"\"\"T·∫°o th√¥ng tin s·∫£n ph·∫©m theo format ƒë∆∞·ª£c y√™u c·∫ßu\"\"\"\n",
    "    \n",
    "    if format_type == \"json\":\n",
    "        system_prompt = \"\"\"B·∫°n l√† m·ªôt AI assistant chuy√™n cung c·∫•p th√¥ng tin s·∫£n ph·∫©m.\n",
    "        H√£y tr·∫£ v·ªÅ th√¥ng tin s·∫£n ph·∫©m d∆∞·ªõi d·∫°ng JSON v·ªõi c√°c field: name, price, description, features, availability.\n",
    "        Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng c√≥ text kh√°c.\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"\"\"B·∫°n l√† m·ªôt AI assistant chuy√™n cung c·∫•p th√¥ng tin s·∫£n ph·∫©m.\n",
    "        H√£y m√¥ t·∫£ s·∫£n ph·∫©m m·ªôt c√°ch chi ti·∫øt v√† h·∫•p d·∫´n.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=f\"Cung c·∫•p th√¥ng tin v·ªÅ s·∫£n ph·∫©m: {product_name}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# Test v·ªõi c√°c s·∫£n ph·∫©m kh√°c nhau\n",
    "test_products = [\n",
    "    {\"name\": \"iPhone 15 Pro\", \"format\": \"json\", \"trace_id\": str(uuid.uuid4())},\n",
    "    {\"name\": \"MacBook Air M2\", \"format\": \"json\", \"trace_id\": str(uuid.uuid4())},\n",
    "    {\"name\": \"AirPods Pro\", \"format\": \"text\", \"trace_id\": str(uuid.uuid4())}\n",
    "]\n",
    "\n",
    "product_responses = []\n",
    "\n",
    "for product in test_products:\n",
    "    langfuse_context.configure(\n",
    "        trace_id=product[\"trace_id\"],\n",
    "        session_id=\"product_info_evaluation\",\n",
    "        user_id=\"system_test\"\n",
    "    )\n",
    "    \n",
    "    response = generate_product_info(\n",
    "        product_name=product[\"name\"],\n",
    "        format_type=product[\"format\"]\n",
    "    )\n",
    "    \n",
    "    product_responses.append({\n",
    "        \"product\": product[\"name\"],\n",
    "        \"format\": product[\"format\"],\n",
    "        \"response\": response,\n",
    "        \"trace_id\": product[\"trace_id\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"üì± Product: {product['name']} ({product['format']})\")\n",
    "    print(f\"üîó Trace: {product['trace_id']}\")\n",
    "    print(f\"üìù Response: {response[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù V√≠ d·ª• 3: Prompt Management v·ªõi LangFuse\n",
    "\n",
    "Prompt Management cho ph√©p version control prompts, A/B test different versions, v√† theo d√µi performance c·ªßa t·ª´ng prompt version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define c√°c prompt versions cho customer support\n",
    "customer_support_prompts = {\n",
    "    \"v1_basic\": {\n",
    "        \"content\": \"\"\"B·∫°n l√† m·ªôt nh√¢n vi√™n h·ªó tr·ª£ kh√°ch h√†ng. \n",
    "H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng m·ªôt c√°ch l·ªãch s·ª± v√† h·ªØu √≠ch.\n",
    "\n",
    "C√¢u h·ªèi: {question}\n",
    "Context: {context}\"\"\",\n",
    "        \"labels\": [\"customer-support\", \"basic\"]\n",
    "    },\n",
    "    \n",
    "    \"v2_detailed\": {\n",
    "        \"content\": \"\"\"B·∫°n l√† m·ªôt chuy√™n vi√™n h·ªó tr·ª£ kh√°ch h√†ng chuy√™n nghi·ªáp c·ªßa c√¥ng ty.\n",
    "Khi tr·∫£ l·ªùi kh√°ch h√†ng, h√£y:\n",
    "1. Th·ªÉ hi·ªán s·ª± ƒë·ªìng c·∫£m v√† hi·ªÉu bi·∫øt\n",
    "2. Cung c·∫•p th√¥ng tin ch√≠nh x√°c v√† chi ti·∫øt\n",
    "3. ƒê∆∞a ra c√°c b∆∞·ªõc h√†nh ƒë·ªông c·ª• th·ªÉ n·∫øu c·∫ßn\n",
    "4. K·∫øt th√∫c b·∫±ng vi·ªác h·ªèi th√™m n·∫øu kh√°ch h√†ng c·∫ßn h·ªó tr·ª£ g√¨ kh√°c\n",
    "\n",
    "Context c√≥ s·∫µn: {context}\n",
    "C√¢u h·ªèi c·ªßa kh√°ch h√†ng: {question}\n",
    "\n",
    "Ph·∫£n h·ªìi c·ªßa b·∫°n:\"\"\",\n",
    "        \"labels\": [\"customer-support\", \"detailed\", \"structured\"]\n",
    "    },\n",
    "    \n",
    "    \"v3_empathetic\": {\n",
    "        \"content\": \"\"\"B·∫°n l√† m·ªôt chuy√™n vi√™n h·ªó tr·ª£ kh√°ch h√†ng gi√†u kinh nghi·ªám v√† r·∫•t quan t√¢m ƒë·∫øn tr·∫£i nghi·ªám kh√°ch h√†ng.\n",
    "\n",
    "Nguy√™n t·∫Øc ph·∫£n h·ªìi:\n",
    "- Lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác th·ª´a nh·∫≠n v√† ƒë·ªìng c·∫£m v·ªõi t√¨nh hu·ªëng c·ªßa kh√°ch h√†ng\n",
    "- S·ª≠ d·ª•ng ng√¥n ng·ªØ ·∫•m √°p, th√¢n thi·ªán nh∆∞ng v·∫´n chuy√™n nghi·ªáp\n",
    "- Gi·∫£i th√≠ch r√µ r√†ng c√°c policy v√† quy tr√¨nh\n",
    "- ƒê∆∞a ra c√°c l·ª±a ch·ªçn v√† gi·∫£i ph√°p thay th·∫ø n·∫øu c√≥ th·ªÉ\n",
    "- ƒê·∫£m b·∫£o kh√°ch h√†ng c·∫£m th·∫•y ƒë∆∞·ª£c l·∫Øng nghe v√† quan t√¢m\n",
    "\n",
    "Th√¥ng tin tham kh·∫£o: {context}\n",
    "C√¢u h·ªèi/V·∫•n ƒë·ªÅ c·ªßa kh√°ch h√†ng: {question}\n",
    "\n",
    "H√£y tr·∫£ l·ªùi m·ªôt c√°ch chu ƒë√°o v√† h·ªó tr·ª£ t·ªët nh·∫•t:\"\"\",\n",
    "        \"labels\": [\"customer-support\", \"empathetic\", \"advanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìù Defined {len(customer_support_prompts)} prompt versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def respond_with_prompt_version(question: str, context: str, prompt_version: str):\n",
    "    \"\"\"S·ª≠ d·ª•ng specific prompt version ƒë·ªÉ tr·∫£ l·ªùi\"\"\"\n",
    "    \n",
    "    # Get prompt content\n",
    "    prompt_content = customer_support_prompts[prompt_version][\"content\"]\n",
    "    \n",
    "    # Format prompt v·ªõi variables\n",
    "    formatted_prompt = prompt_content.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    # G·ªçi LLM\n",
    "    messages = [HumanMessage(content=formatted_prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Log prompt version ƒë∆∞·ª£c s·ª≠ d·ª•ng\n",
    "    langfuse_context.update_current_trace(\n",
    "        metadata={\n",
    "            \"prompt_version\": prompt_version,\n",
    "            \"prompt_name\": \"customer_support_response\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "print(\"‚úÖ ƒê√£ setup prompt management functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Test c√°c prompt versions\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"question\": \"T√¥i ƒë·∫∑t h√†ng 1 tu·∫ßn r·ªìi m√† ch∆∞a nh·∫≠n ƒë∆∞·ª£c, r·∫•t th·∫•t v·ªçng!\",\n",
    "        \"context\": \"ƒê∆°n h√†ng #12345, ship date d·ª± ki·∫øn: 3-5 ng√†y, hi·ªán t·∫°i ƒë√£ 7 ng√†y\",\n",
    "        \"scenario_id\": \"delayed_delivery\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"S·∫£n ph·∫©m t√¥i nh·∫≠n ƒë∆∞·ª£c b·ªã l·ªói, mu·ªën ƒë·ªïi tr·∫£\",\n",
    "        \"context\": \"Ch√≠nh s√°ch: ƒë·ªïi tr·∫£ trong 7 ng√†y, c·∫ßn h√≥a ƒë∆°n v√† s·∫£n ph·∫©m nguy√™n v·∫πn\",\n",
    "        \"scenario_id\": \"defective_product\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test t·ª´ng prompt version v·ªõi m·ªói scenario\n",
    "comparison_results = []\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(f\"\\nüß™ Testing scenario: {scenario['scenario_id']}\")\n",
    "    print(f\"‚ùì Question: {scenario['question']}\")\n",
    "    \n",
    "    scenario_results = {\n",
    "        \"scenario_id\": scenario[\"scenario_id\"],\n",
    "        \"question\": scenario[\"question\"],\n",
    "        \"responses\": {}\n",
    "    }\n",
    "    \n",
    "    for version in [\"v1_basic\", \"v2_detailed\", \"v3_empathetic\"]:\n",
    "        trace_id = str(uuid.uuid4())\n",
    "        \n",
    "        langfuse_context.configure(\n",
    "            trace_id=trace_id,\n",
    "            session_id=f\"prompt_ab_test_{scenario['scenario_id']}\",\n",
    "            user_id=\"prompt_tester\",\n",
    "            tags=[\"ab_test\", f\"prompt_{version}\", scenario[\"scenario_id\"]]\n",
    "        )\n",
    "        \n",
    "        response = respond_with_prompt_version(\n",
    "            question=scenario[\"question\"],\n",
    "            context=scenario[\"context\"],\n",
    "            prompt_version=version\n",
    "        )\n",
    "        \n",
    "        scenario_results[\"responses\"][version] = {\n",
    "            \"response\": response,\n",
    "            \"trace_id\": trace_id,\n",
    "            \"length\": len(response)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìù {version.upper()}:\")\n",
    "        print(f\"Response ({len(response)} chars): {response[:150]}...\")\n",
    "        print(f\"Trace: {trace_id}\")\n",
    "    \n",
    "    comparison_results.append(scenario_results)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nüéØ Ho√†n th√†nh A/B testing c√°c prompt versions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Gi·∫£i th√≠ch & Ph√¢n t√≠ch\n",
    "\n",
    "### Truy c·∫≠p LangFuse Dashboard\n",
    "\n",
    "ƒê·ªÉ xem v√† ph√¢n t√≠ch k·∫øt qu·∫£ evaluation:\n",
    "\n",
    "1. **M·ªü LangFuse UI**: Truy c·∫≠p `http://localhost:3000` (ho·∫∑c URL c·ªßa LangFuse instance)\n",
    "\n",
    "2. **Xem Traces**: \n",
    "   - Navigate ƒë·∫øn \"Traces\" tab\n",
    "   - Filter theo session_id ƒë·ªÉ xem specific evaluation runs\n",
    "   - Click v√†o individual traces ƒë·ªÉ xem chi ti·∫øt\n",
    "\n",
    "3. **Ph√¢n t√≠ch Scores**:\n",
    "   - Trong \"Scores\" tab, xem distribution c·ªßa c√°c evaluation metrics\n",
    "   - So s√°nh performance gi·ªØa c√°c prompt versions\n",
    "   - Track trends theo th·ªùi gian\n",
    "\n",
    "4. **Prompt Management**:\n",
    "   - Trong \"Prompts\" tab, xem c√°c versions ƒë√£ t·∫°o\n",
    "   - Compare performance c·ªßa different versions\n",
    "   - Deploy prompts t·ªët nh·∫•t v√†o production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush all data to LangFuse\n",
    "langfuse.flush()\n",
    "print(\"‚úÖ All evaluation data has been sent to LangFuse!\")\n",
    "print(\"üìä Check LangFuse Dashboard at http://localhost:3000 for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ K·∫øt lu·∫≠n & B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "### Nh·ªØng g√¨ ƒë√£ h·ªçc ƒë∆∞·ª£c\n",
    "\n",
    "1. **Human Feedback Collection**: C√°ch thu th·∫≠p v√† ghi nh·∫≠n feedback t·ª´ ng∆∞·ªùi d√πng th·ª±c t·∫ø\n",
    "2. **Automated Evaluation**: Implement c√°c evaluator t·ª± ƒë·ªông cho format, safety, v√† quality\n",
    "3. **Prompt Management**: Version control v√† A/B testing cho prompts\n",
    "4. **Performance Analysis**: Ph√¢n t√≠ch v√† so s√°nh hi·ªáu su·∫•t c·ªßa c√°c approaches kh√°c nhau\n",
    "5. **LangFuse Integration**: S·ª≠ d·ª•ng LangFuse ƒë·ªÉ tracking v√† visualization\n",
    "\n",
    "### Best Practices ƒë√£ √°p d·ª•ng\n",
    "\n",
    "- **Structured Evaluation**: ƒê·ªãnh nghƒ©a r√µ c√°c ti√™u ch√≠ ƒë√°nh gi√°\n",
    "- **Automated + Human**: K·∫øt h·ª£p automation v·ªõi human judgment\n",
    "- **Version Control**: Theo d√µi changes v√† performance c·ªßa prompts\n",
    "- **Continuous Monitoring**: Setup pipeline ƒë·ªÉ evaluation li√™n t·ª•c\n",
    "\n",
    "### B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "1. **Production Deployment**: \n",
    "   - Deploy prompt version t·ªët nh·∫•t v√†o production\n",
    "   - Implement gradual rollout strategy\n",
    "\n",
    "2. **Automated Pipeline**:\n",
    "   - Setup CI/CD cho prompt testing\n",
    "   - Automated regression testing\n",
    "\n",
    "3. **Advanced Evaluation**:\n",
    "   - Model-based evaluation (using LLM as judge)\n",
    "   - Multi-dimensional scoring\n",
    "   - Real-time feedback collection\n",
    "\n",
    "4. **Scaling**:\n",
    "   - Evaluation cho multiple use cases\n",
    "   - Cross-model comparison\n",
    "   - Performance optimization\n",
    "\n",
    "### Resources cho h·ªçc th√™m\n",
    "\n",
    "- [LangFuse Documentation](https://langfuse.com/docs)\n",
    "- [LLM Evaluation Best Practices](https://langfuse.com/guides/evaluation)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [A/B Testing for AI Systems](https://langfuse.com/guides/ab-testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}