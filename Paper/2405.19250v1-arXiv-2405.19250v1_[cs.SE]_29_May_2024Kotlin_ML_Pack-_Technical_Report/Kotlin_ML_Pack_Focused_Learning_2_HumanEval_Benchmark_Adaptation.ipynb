{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: HumanEval Benchmark Adaptation for Kotlin\n",
    "\n",
    "## Learning Objective\n",
    "Understand the challenges and solutions in adapting code generation benchmarks across programming languages, focusing on the HumanEval adaptation for Kotlin described in Section IV.\n",
    "\n",
    "## Paper Reference\n",
    "- **Section**: IV - Kotlin Evaluation\n",
    "- **Key Challenge**: Existing Kotlin HumanEval had faulty prompts and tests\n",
    "- **Solution**: Human experts rewrote HumanEval from scratch for Kotlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Why Cross-Language Benchmarks Fail\n",
    "\n",
    "### 1.1 Issues Found in Existing Kotlin HumanEval\n",
    "\n",
    "From Section IV.A, the paper identifies two major categories of issues:\n",
    "\n",
    "1. **Type System Mismatches**: Generic variable types preventing method usage\n",
    "2. **Numerical Precision Differences**: Rounding differences between Python and Kotlin\n",
    "\n",
    "Let's explore these with concrete examples from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install deepeval langchain pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Case Study: Type System Issues\n",
    "\n",
    "The paper mentions: \"too generic variable type in the Kotlin function signature... cannot apply many built-in Kotlin methods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HumanEvalProblem:\n",
    "    \"\"\"Represents a HumanEval problem with Python and Kotlin versions\"\"\"\n",
    "    task_id: int\n",
    "    description: str\n",
    "    python_signature: str\n",
    "    kotlin_signature_bad: str  # Problematic version\n",
    "    kotlin_signature_good: str  # Fixed version\n",
    "    test_cases: List[Dict]\n",
    "\n",
    "# Example: Array sorting problem demonstrating type issues\n",
    "type_issue_example = HumanEvalProblem(\n",
    "    task_id=1,\n",
    "    description=\"Sort an array in ascending order\",\n",
    "    python_signature=\"def sort_array(arr: List[int]) -> List[int]:\",\n",
    "    kotlin_signature_bad=\"fun sortArray(arr: Array<Any>): Array<Any>\",  # TOO GENERIC!\n",
    "    kotlin_signature_good=\"fun sortArray(arr: IntArray): IntArray\",  # SPECIFIC TYPE\n",
    "    test_cases=[\n",
    "        {\"input\": [3, 1, 4, 1, 5], \"expected\": [1, 1, 3, 4, 5]},\n",
    "        {\"input\": [9, 2, 6, 5, 3], \"expected\": [2, 3, 5, 6, 9]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Type System Issue Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Task: {type_issue_example.description}\")\n",
    "print(f\"\\nPython signature:\\n  {type_issue_example.python_signature}\")\n",
    "print(f\"\\nProblematic Kotlin signature:\\n  {type_issue_example.kotlin_signature_bad}\")\n",
    "print(f\"  ❌ Problem: Cannot use sort() method on Array<Any>!\")\n",
    "print(f\"\\nFixed Kotlin signature:\\n  {type_issue_example.kotlin_signature_good}\")\n",
    "print(f\"  ✅ Solution: Use specific type (IntArray) that supports sorting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Demonstrating the Type System Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create Kotlin code snippets to demonstrate the issue\n",
    "kotlin_bad_code = \"\"\"\n",
    "// This will NOT compile!\n",
    "fun sortArray(arr: Array<Any>): Array<Any> {\n",
    "    return arr.sorted().toTypedArray()  // Error: No sorted() for Array<Any>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kotlin_good_code = \"\"\"\n",
    "// This works correctly\n",
    "fun sortArray(arr: IntArray): IntArray {\n",
    "    return arr.sorted().toIntArray()  // Works: IntArray has sorted()\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create a validator to check Kotlin compilation\n",
    "class KotlinValidator:\n",
    "    @staticmethod\n",
    "    def validate_code(code: str, test_code: str = \"\") -> Dict[str, any]:\n",
    "        \"\"\"Validate Kotlin code compilation\"\"\"\n",
    "        full_code = f\"\"\"\n",
    "{code}\n",
    "\n",
    "fun main() {{\n",
    "    {test_code}\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        # Create temporary file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.kt', delete=False) as f:\n",
    "            f.write(full_code)\n",
    "            kotlin_file = f.name\n",
    "        \n",
    "        try:\n",
    "            # Try to compile\n",
    "            result = subprocess.run(\n",
    "                ['kotlinc', kotlin_file, '-d', tempfile.gettempdir()],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=10\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'compiles': result.returncode == 0,\n",
    "                'error': result.stderr if result.returncode != 0 else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'compiles': False, 'error': str(e)}\n",
    "        finally:\n",
    "            if os.path.exists(kotlin_file):\n",
    "                os.remove(kotlin_file)\n",
    "\n",
    "# Note: In a real environment with kotlinc installed, this would show compilation errors\n",
    "print(\"\\nKotlin Code Examples:\")\n",
    "print(\"\\nBad (Generic Type):\")\n",
    "print(kotlin_bad_code)\n",
    "print(\"\\nGood (Specific Type):\")\n",
    "print(kotlin_good_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Case Study: Numerical Precision Issues\n",
    "\n",
    "The paper provides a specific example from HumanEval task #2 about floating-point precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HumanEval Task #2 from the paper\n",
    "precision_issue_example = HumanEvalProblem(\n",
    "    task_id=2,\n",
    "    description=\"\"\"Given a positive floating point number, it can be decomposed into \n",
    "integer part (largest integer smaller than given number) and decimals \n",
    "(leftover part always smaller than 1). Return the decimal part of the number.\"\"\",\n",
    "    python_signature=\"def truncate_number(number: float) -> float:\",\n",
    "    kotlin_signature_bad=\"fun truncate(number: Double): Double\",\n",
    "    kotlin_signature_good=\"fun truncate(number: Double): Double\",\n",
    "    test_cases=[\n",
    "        {\"input\": 3.5, \"expected\": 0.5},\n",
    "        {\"input\": 1.25, \"expected\": 0.25},\n",
    "        {\"input\": 123.45, \"expected\": 0.45}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The problematic Kotlin solution from the paper\n",
    "kotlin_truncate_simple = \"\"\"\n",
    "fun truncate(number: Double): Double {\n",
    "    return number - Math.floor(number)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Demonstrate the precision issue\n",
    "import math\n",
    "\n",
    "def demonstrate_precision_issue():\n",
    "    \"\"\"Show how floating-point precision can cause test failures\"\"\"\n",
    "    test_values = [3.5, 1.25, 123.45, 10.999999999999998]\n",
    "    \n",
    "    print(\"Floating-Point Precision Issues:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Input':<20} {'Python Result':<20} {'Kotlin Result':<20} {'Difference':<20}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for value in test_values:\n",
    "        # Python implementation\n",
    "        python_result = value - math.floor(value)\n",
    "        \n",
    "        # Simulate Kotlin's slightly different precision\n",
    "        # In reality, this comes from JVM vs Python float handling\n",
    "        kotlin_result = value - math.floor(value)\n",
    "        if value == 10.999999999999998:\n",
    "            # Simulate precision error\n",
    "            kotlin_result += 1e-15\n",
    "        \n",
    "        difference = abs(python_result - kotlin_result)\n",
    "        \n",
    "        print(f\"{value:<20} {python_result:<20.15f} {kotlin_result:<20.15f} {difference:<20.2e}\")\n",
    "        \n",
    "        if difference > 1e-8:\n",
    "            print(f\"  ❌ Would fail with strict equality check!\")\n",
    "        else:\n",
    "            print(f\"  ✅ Within acceptable tolerance\")\n",
    "\n",
    "demonstrate_precision_issue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Solution: Human Expert Rewrite\n",
    "\n",
    "From Section IV.A: \"All HumanEval solutions and tests in Kotlin were written by an expert competitive programmer with six years of experience in Kotlin, and independently reviewed by a programmer with four years of experience.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanEvalAdapter:\n",
    "    \"\"\"Demonstrates the process of adapting HumanEval problems to Kotlin\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.adaptation_rules = [\n",
    "            \"Use specific types instead of generic Any\",\n",
    "            \"Handle floating-point comparisons with tolerance\",\n",
    "            \"Adapt to Kotlin idioms (e.g., IntArray vs Array<Int>)\",\n",
    "            \"Consider null safety in function signatures\",\n",
    "            \"Use appropriate collection types\"\n",
    "        ]\n",
    "    \n",
    "    def adapt_problem(self, problem: HumanEvalProblem) -> Dict[str, str]:\n",
    "        \"\"\"Adapt a HumanEval problem from Python to Kotlin\"\"\"\n",
    "        return {\n",
    "            \"description\": self._adapt_description(problem.description),\n",
    "            \"signature\": self._adapt_signature(problem.python_signature),\n",
    "            \"tests\": self._adapt_tests(problem.test_cases),\n",
    "            \"solution_template\": self._create_solution_template(problem)\n",
    "        }\n",
    "    \n",
    "    def _adapt_description(self, description: str) -> str:\n",
    "        \"\"\"Adapt problem description for Kotlin context\"\"\"\n",
    "        # Add Kotlin-specific notes if needed\n",
    "        kotlin_notes = \"\\n\\nNote: Use Kotlin's standard library functions where appropriate.\"\n",
    "        return description + kotlin_notes\n",
    "    \n",
    "    def _adapt_signature(self, python_sig: str) -> str:\n",
    "        \"\"\"Convert Python signature to Kotlin\"\"\"\n",
    "        # Mapping of Python types to Kotlin types\n",
    "        type_mapping = {\n",
    "            \"List[int]\": \"IntArray\",\n",
    "            \"List[str]\": \"List<String>\",\n",
    "            \"Dict[str, int]\": \"Map<String, Int>\",\n",
    "            \"float\": \"Double\",\n",
    "            \"int\": \"Int\",\n",
    "            \"str\": \"String\",\n",
    "            \"bool\": \"Boolean\"\n",
    "        }\n",
    "        \n",
    "        kotlin_sig = python_sig.replace(\"def \", \"fun \")\n",
    "        for py_type, kt_type in type_mapping.items():\n",
    "            kotlin_sig = kotlin_sig.replace(py_type, kt_type)\n",
    "        \n",
    "        return kotlin_sig\n",
    "    \n",
    "    def _adapt_tests(self, test_cases: List[Dict]) -> str:\n",
    "        \"\"\"Generate Kotlin test code with proper assertions\"\"\"\n",
    "        test_code = \"// Test cases\\n\"\n",
    "        \n",
    "        for i, test in enumerate(test_cases):\n",
    "            test_code += f\"\"\"\n",
    "// Test {i + 1}\n",
    "val result{i} = solution({test['input']})\n",
    "assert(result{i} == {test['expected']}) {{ \"Test {i + 1} failed\" }}\n",
    "\"\"\"\n",
    "        return test_code\n",
    "    \n",
    "    def _create_solution_template(self, problem: HumanEvalProblem) -> str:\n",
    "        \"\"\"Create a Kotlin solution template\"\"\"\n",
    "        return f\"\"\"\n",
    "{problem.kotlin_signature_good} {{\n",
    "    // TODO: Implement the solution\n",
    "    TODO(\"Not yet implemented\")\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Example adaptation\n",
    "adapter = HumanEvalAdapter()\n",
    "adapted = adapter.adapt_problem(type_issue_example)\n",
    "\n",
    "print(\"Adapted HumanEval Problem:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in adapted.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Setup from Section IV.B\n",
    "\n",
    "The paper describes a specific evaluation setup with multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "class ErrorType(Enum):\n",
    "    \"\"\"Error types from Section IV.B\"\"\"\n",
    "    COMPILATION_ERROR = \"compilation_error\"\n",
    "    RUNTIME_ERROR = \"runtime_error\"\n",
    "    TEST_FAILURE = \"test_failure\"\n",
    "    TIMEOUT_ERROR = \"timeout_error\"\n",
    "    SUCCESS = \"success\"\n",
    "\n",
    "class KotlinHumanEvalMetrics:\n",
    "    \"\"\"Implements the evaluation metrics from Section IV.B\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def evaluate_generation(self, generated_code: str, test_harness: str) -> Dict:\n",
    "        \"\"\"Evaluate a single code generation\"\"\"\n",
    "        # Combine generated code with test harness\n",
    "        full_code = f\"\"\"\n",
    "{generated_code}\n",
    "\n",
    "fun main() {{\n",
    "    {test_harness}\n",
    "    println(\"All tests passed!\")\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        # Simulate evaluation (in real implementation, would compile and run)\n",
    "        # For demonstration, we'll use mock results\n",
    "        import random\n",
    "        \n",
    "        # Simulate different error types based on paper's statistics\n",
    "        rand = random.random()\n",
    "        if rand < 0.15:  # ~15% compilation errors\n",
    "            error_type = ErrorType.COMPILATION_ERROR\n",
    "        elif rand < 0.20:  # ~5% runtime errors\n",
    "            error_type = ErrorType.RUNTIME_ERROR\n",
    "        elif rand < 0.50:  # ~30% test failures\n",
    "            error_type = ErrorType.TEST_FAILURE\n",
    "        elif rand < 0.51:  # ~1% timeout\n",
    "            error_type = ErrorType.TIMEOUT_ERROR\n",
    "        else:  # ~49% success\n",
    "            error_type = ErrorType.SUCCESS\n",
    "        \n",
    "        result = {\n",
    "            'error_type': error_type,\n",
    "            'passed': error_type == ErrorType.SUCCESS,\n",
    "            'compilation_error': error_type == ErrorType.COMPILATION_ERROR,\n",
    "            'runtime_error': error_type == ErrorType.RUNTIME_ERROR,\n",
    "            'test_failure': error_type == ErrorType.TEST_FAILURE,\n",
    "            'timeout': error_type == ErrorType.TIMEOUT_ERROR\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate all metrics from Section IV.B\"\"\"\n",
    "        if not self.results:\n",
    "            return {}\n",
    "        \n",
    "        total = len(self.results)\n",
    "        \n",
    "        metrics = {\n",
    "            'pass_at_1': sum(r['passed'] for r in self.results) / total * 100,\n",
    "            'compilation_error_rate': sum(r['compilation_error'] for r in self.results) / total * 100,\n",
    "            'runtime_error_rate': sum(r['runtime_error'] for r in self.results) / total * 100,\n",
    "            'test_error_rate': sum(r['test_failure'] for r in self.results) / total * 100,\n",
    "            'timeout_error_rate': sum(r['timeout'] for r in self.results) / total * 100\n",
    "        }\n",
    "        \n",
    "        # Syntax error rate = compilation + runtime errors (as per paper)\n",
    "        metrics['syntax_error_rate'] = (\n",
    "            metrics['compilation_error_rate'] + \n",
    "            metrics['runtime_error_rate']\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Simulate evaluation of 100 problems\n",
    "evaluator = KotlinHumanEvalMetrics()\n",
    "for _ in range(100):\n",
    "    evaluator.evaluate_generation(\"fun solution() { /* generated */ }\", \"// tests\")\n",
    "\n",
    "metrics = evaluator.calculate_metrics()\n",
    "\n",
    "print(\"Evaluation Metrics (Simulated):\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:<25}: {value:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation Setup Details\n",
    "\n",
    "Section IV.B provides specific details about the generation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KotlinGenerationConfig:\n",
    "    \"\"\"Configuration for Kotlin code generation based on Section IV.B\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # From the paper\n",
    "        self.prompt_template = \"You are an expert Kotlin programmer...\"\n",
    "        self.generation_strategy = \"greedy\"  # Greedy generation\n",
    "        self.min_tokens = 128\n",
    "        self.max_tokens = 256\n",
    "        self.early_stop_sequence = \"\\n}\\n\"  # End of Kotlin method\n",
    "        self.remove_comments = True\n",
    "        self.handle_prompt_repetition = True\n",
    "    \n",
    "    def preprocess_prompt(self, problem_description: str, signature: str) -> str:\n",
    "        \"\"\"Prepare the prompt for generation\"\"\"\n",
    "        return f\"\"\"{self.prompt_template}\n",
    "\n",
    "Problem: {problem_description}\n",
    "\n",
    "Complete the following Kotlin function:\n",
    "{signature} {{\n",
    "    // Your implementation here\n",
    "\"\"\"\n",
    "    \n",
    "    def postprocess_generation(self, generated: str, original_signature: str) -> str:\n",
    "        \"\"\"Post-process generated code as per paper's approach\"\"\"\n",
    "        lines = generated.split('\\n')\n",
    "        processed_lines = []\n",
    "        \n",
    "        # Remove comments if configured\n",
    "        if self.remove_comments:\n",
    "            lines = [line for line in lines if not line.strip().startswith('//')]\n",
    "        \n",
    "        # Handle prompt repetition\n",
    "        if self.handle_prompt_repetition:\n",
    "            # Find first function definition\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.strip().startswith('fun '):\n",
    "                    # Remove this line and everything before\n",
    "                    lines = lines[i+1:]\n",
    "                    break\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "# Example usage\n",
    "config = KotlinGenerationConfig()\n",
    "\n",
    "# Example problem\n",
    "problem = \"Return the sum of two integers\"\n",
    "signature = \"fun sum(a: Int, b: Int): Int\"\n",
    "\n",
    "prompt = config.preprocess_prompt(problem, signature)\n",
    "print(\"Generation Prompt:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt)\n",
    "\n",
    "# Simulate generation\n",
    "mock_generation = \"\"\"\n",
    "fun sum(a: Int, b: Int): Int {\n",
    "    // This function adds two numbers\n",
    "    return a + b\n",
    "}\n",
    "\n",
    "// Additional generated content\n",
    "fun test() {\n",
    "    println(sum(1, 2))\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "processed = config.postprocess_generation(mock_generation, signature)\n",
    "print(\"\\nProcessed Generation:\")\n",
    "print(\"=\" * 50)\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Python vs Kotlin Performance\n",
    "\n",
    "From Figure 1 in the paper, we can analyze the performance gap between languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Figure 1 in the paper\n",
    "model_performance = {\n",
    "    'model': ['GPT-4-turbo', 'Deepseek-coder-33B-instruct', 'Deepseek-coder-6.7B-instruct',\n",
    "              'GPT-3.5-turbo', 'CodeLlama-70B-Instruct-hf', 'CodeQwen1.5-7B',\n",
    "              'Meta-Llama-3-8B-Instruct', 'Deepseek-coder-6.7B-base',\n",
    "              'CodeLlama-13b-Instruct-hf', 'Deepseek-coder-1.3B-instruct'],\n",
    "    'kotlin_score': [73, 63, 49, 62, 58, 44, 43, 41, 35, 29],\n",
    "    'python_score': [81, 75, 61, 73, 67, 55, 52, 48, 43, 36]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(model_performance)\n",
    "df['performance_gap'] = df['python_score'] - df['kotlin_score']\n",
    "df['gap_percentage'] = (df['performance_gap'] / df['python_score']) * 100\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart comparing scores\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, df['kotlin_score'], width, label='Kotlin', color='orange')\n",
    "bars2 = ax1.bar(x + width/2, df['python_score'], width, label='Python', color='blue')\n",
    "\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('HumanEval Score')\n",
    "ax1.set_title('Kotlin vs Python HumanEval Performance')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df['model'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance gap analysis\n",
    "ax2.barh(df['model'], df['gap_percentage'], color='red', alpha=0.7)\n",
    "ax2.set_xlabel('Performance Gap (%)')\n",
    "ax2.set_title('Python Advantage over Kotlin (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nPerformance Gap Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Average Kotlin Score: {df['kotlin_score'].mean():.1f}\")\n",
    "print(f\"Average Python Score: {df['python_score'].mean():.1f}\")\n",
    "print(f\"Average Performance Gap: {df['performance_gap'].mean():.1f} points\")\n",
    "print(f\"Average Gap Percentage: {df['gap_percentage'].mean():.1f}%\")\n",
    "print(f\"\\nConclusion: Models consistently perform better on Python than Kotlin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Cross-Language Benchmark Adaptation\n",
    "\n",
    "Based on the paper's findings, here are key principles for adapting benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkAdaptationGuidelines:\n",
    "    \"\"\"Best practices learned from the Kotlin HumanEval adaptation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.guidelines = {\n",
    "            \"Type Systems\": {\n",
    "                \"principle\": \"Respect language-specific type systems\",\n",
    "                \"example\": \"Use IntArray instead of Array<Any> in Kotlin\",\n",
    "                \"impact\": \"Prevents compilation errors and enables proper method usage\"\n",
    "            },\n",
    "            \"Numerical Precision\": {\n",
    "                \"principle\": \"Account for floating-point differences between languages\",\n",
    "                \"example\": \"Use tolerance-based comparisons instead of exact equality\",\n",
    "                \"impact\": \"Avoids false test failures due to precision differences\"\n",
    "            },\n",
    "            \"Idioms\": {\n",
    "                \"principle\": \"Adapt to language-specific idioms and conventions\",\n",
    "                \"example\": \"Use Kotlin's when expression instead of if-else chains\",\n",
    "                \"impact\": \"Makes benchmarks more representative of real code\"\n",
    "            },\n",
    "            \"Expert Review\": {\n",
    "                \"principle\": \"Have language experts review all adaptations\",\n",
    "                \"example\": \"6-year Kotlin expert + 4-year reviewer in the paper\",\n",
    "                \"impact\": \"Ensures high-quality, idiomatic benchmark problems\"\n",
    "            },\n",
    "            \"Test Equivalence\": {\n",
    "                \"principle\": \"Ensure tests are functionally equivalent, not literally translated\",\n",
    "                \"example\": \"Adapt test assertions to language-specific testing patterns\",\n",
    "                \"impact\": \"Maintains benchmark validity across languages\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def validate_adaptation(self, original_problem: dict, adapted_problem: dict) -> List[str]:\n",
    "        \"\"\"Validate that an adaptation follows best practices\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check for generic types\n",
    "        if \"Any\" in adapted_problem.get(\"signature\", \"\"):\n",
    "            issues.append(\"Uses generic 'Any' type - consider more specific types\")\n",
    "        \n",
    "        # Check for exact floating-point comparisons\n",
    "        if \"==\" in adapted_problem.get(\"tests\", \"\") and \"Double\" in adapted_problem.get(\"signature\", \"\"):\n",
    "            issues.append(\"Uses exact equality for floating-point - consider tolerance\")\n",
    "        \n",
    "        # Check for language idioms\n",
    "        if \"ArrayList\" in adapted_problem.get(\"signature\", \"\") and \"mutableListOf\" not in adapted_problem.get(\"signature\", \"\"):\n",
    "            issues.append(\"Consider using Kotlin's mutableListOf() idiom\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def display_guidelines(self):\n",
    "        \"\"\"Display adaptation guidelines in a structured format\"\"\"\n",
    "        print(\"Benchmark Adaptation Guidelines:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for category, details in self.guidelines.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(f\"  Principle: {details['principle']}\")\n",
    "            print(f\"  Example: {details['example']}\")\n",
    "            print(f\"  Impact: {details['impact']}\")\n",
    "\n",
    "guidelines = BenchmarkAdaptationGuidelines()\n",
    "guidelines.display_guidelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Implementing a Complete HumanEval Problem\n",
    "\n",
    "Let's implement a complete example showing the full adaptation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteHumanEvalExample:\n",
    "    \"\"\"Complete example of HumanEval problem adaptation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.problem = {\n",
    "            \"task_id\": 100,\n",
    "            \"description\": \"\"\"Given a list of integers, return a list of only the even integers.\n",
    "            If there are no even integers, return an empty list.\"\"\",\n",
    "            \"python\": {\n",
    "                \"signature\": \"def filter_even(numbers: List[int]) -> List[int]:\",\n",
    "                \"solution\": \"\"\"def filter_even(numbers: List[int]) -> List[int]:\n",
    "    return [n for n in numbers if n % 2 == 0]\"\"\",\n",
    "                \"tests\": [\n",
    "                    \"assert filter_even([1, 2, 3, 4, 5]) == [2, 4]\",\n",
    "                    \"assert filter_even([1, 3, 5, 7]) == []\",\n",
    "                    \"assert filter_even([2, 4, 6, 8]) == [2, 4, 6, 8]\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_kotlin_adaptation(self) -> dict:\n",
    "        \"\"\"Create proper Kotlin adaptation following paper's guidelines\"\"\"\n",
    "        kotlin_adaptation = {\n",
    "            \"signature\": \"fun filterEven(numbers: IntArray): IntArray\",\n",
    "            \"solution\": \"\"\"fun filterEven(numbers: IntArray): IntArray {\n",
    "    return numbers.filter { it % 2 == 0 }.toIntArray()\n",
    "}\"\"\",\n",
    "            \"prompt\": f\"\"\"You are an expert Kotlin programmer.\n",
    "\n",
    "{self.problem['description']}\n",
    "\n",
    "Complete the following Kotlin function:\n",
    "fun filterEven(numbers: IntArray): IntArray {{\n",
    "    // Your implementation here\n",
    "\"\"\",\n",
    "            \"tests\": \"\"\"// Test cases\n",
    "fun main() {\n",
    "    // Test 1\n",
    "    val result1 = filterEven(intArrayOf(1, 2, 3, 4, 5))\n",
    "    assert(result1.contentEquals(intArrayOf(2, 4))) { \"Test 1 failed\" }\n",
    "    \n",
    "    // Test 2\n",
    "    val result2 = filterEven(intArrayOf(1, 3, 5, 7))\n",
    "    assert(result2.isEmpty()) { \"Test 2 failed\" }\n",
    "    \n",
    "    // Test 3\n",
    "    val result3 = filterEven(intArrayOf(2, 4, 6, 8))\n",
    "    assert(result3.contentEquals(intArrayOf(2, 4, 6, 8))) { \"Test 3 failed\" }\n",
    "    \n",
    "    println(\"All tests passed!\")\n",
    "}\"\"\",\n",
    "            \"key_adaptations\": [\n",
    "                \"Used IntArray instead of List<Int> for better performance\",\n",
    "                \"Used contentEquals() for array comparison (Kotlin idiom)\",\n",
    "                \"Added proper error messages to assertions\",\n",
    "                \"Followed Kotlin naming conventions (camelCase)\"\n",
    "            ]\n",
    "        }\n",
    "        return kotlin_adaptation\n",
    "    \n",
    "    def demonstrate_evaluation(self):\n",
    "        \"\"\"Show how the adapted problem would be evaluated\"\"\"\n",
    "        kotlin = self.create_kotlin_adaptation()\n",
    "        \n",
    "        print(\"Complete HumanEval Adaptation Example:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nProblem: {self.problem['description']}\")\n",
    "        print(f\"\\nPython signature: {self.problem['python']['signature']}\")\n",
    "        print(f\"Kotlin signature: {kotlin['signature']}\")\n",
    "        \n",
    "        print(\"\\nKotlin Solution:\")\n",
    "        print(kotlin['solution'])\n",
    "        \n",
    "        print(\"\\nKotlin Tests:\")\n",
    "        print(kotlin['tests'])\n",
    "        \n",
    "        print(\"\\nKey Adaptations Made:\")\n",
    "        for i, adaptation in enumerate(kotlin['key_adaptations'], 1):\n",
    "            print(f\"{i}. {adaptation}\")\n",
    "\n",
    "example = CompleteHumanEvalExample()\n",
    "example.demonstrate_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### Main Lessons from the Paper\n",
    "\n",
    "1. **Direct Translation Fails**: You cannot simply translate syntax between languages\n",
    "2. **Type Systems Matter**: Generic types prevent proper method usage\n",
    "3. **Precision Issues**: Floating-point handling differs between language runtimes\n",
    "4. **Expert Review Essential**: Language experts catch subtle issues automated tools miss\n",
    "5. **Metrics Beyond Pass Rate**: Syntax error rate provides additional insights\n",
    "\n",
    "### Impact on Results\n",
    "\n",
    "The proper adaptation enabled meaningful evaluation of Kotlin code generation, revealing:\n",
    "- Significant performance gaps between Python and Kotlin\n",
    "- The effectiveness of different fine-tuning approaches\n",
    "- The importance of language-specific training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Error type distribution from Table II\n",
    "error_types = ['Pass', 'Test Fail', 'Compilation Error', 'Runtime Error', 'Timeout']\n",
    "baseline_rates = [26.09, 50.31, 19.25, 3.73, 0.62]  # CodeLlama-7B Base\n",
    "improved_rates = [42.24, 37.89, 17.39, 1.86, 0.62]  # CodeLlama-7B KExercises\n",
    "\n",
    "x = np.arange(len(error_types))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, baseline_rates, width, label='Baseline', color='lightcoral')\n",
    "ax1.bar(x + width/2, improved_rates, width, label='After Fine-tuning', color='lightgreen')\n",
    "ax1.set_ylabel('Rate (%)')\n",
    "ax1.set_title('Error Distribution: Before vs After Fine-tuning')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(error_types, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Key metrics improvement\n",
    "metrics = ['Pass Rate', 'Syntax Error Rate', 'Completion Rate']\n",
    "improvements = [16.15, -3.73, -0.044]  # Positive is better for pass rate, negative for error rates\n",
    "colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "\n",
    "ax2.barh(metrics, improvements, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Change (percentage points)')\n",
    "ax2.set_title('Metric Improvements with KExercises Fine-tuning')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"Proper benchmark adaptation is crucial for meaningful cross-language evaluation.\")\n",
    "print(\"The Kotlin HumanEval rewrite enabled accurate assessment of model capabilities.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}