{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Focus: Contrastive Triplet Learning\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Master contrastive learning principles and triplet loss formulation\n",
    "- Understand hard negative mining strategies for Arabic text\n",
    "- Implement advanced negative sampling techniques\n",
    "- Explore InfoNCE limitations and solutions\n",
    "- Develop effective triplet dataset construction methods\n",
    "\n",
    "## üìö Paper Context\n",
    "**From GATE Paper (Section 1):**\n",
    "> *\"At the heart of many highly effective embedding models lies contrastive learning, a paradigm that optimizes the quality of representation by pulling semantically similar (positive) samples closer while pushing dissimilar (negative) samples apart. Despite the versatility and success of contrastive learning, most existing text embedding pipelines rely on InfoNCE loss with in-batch negative samples, achieving robust representations predominantly by using large batch sizes and numerous negative samples.\"*\n",
    "\n",
    "**Key Challenges Identified:**\n",
    "1. **InfoNCE Limitations**: Not sufficient for all downstream tasks\n",
    "2. **Sentence-level Tasks**: STS benefits less from InfoNCE-based training\n",
    "3. **Fine-grained Similarity**: InfoNCE struggles with subtle semantic differences\n",
    "4. **Negative Sampling**: Quality of negatives crucial for learning\n",
    "5. **Arabic Specificity**: Need for Arabic-tailored contrastive learning\n",
    "\n",
    "## üîë GATE's Innovation\n",
    "GATE uses **Arabic NLI triplet datasets** with curated hard negatives, moving beyond simple InfoNCE to capture fine-grained semantic relationships crucial for Arabic text understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup for Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for contrastive learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced sampling and mining\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"üî• Contrastive Learning Environment Ready!\")\n",
    "print(f\"üì± Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üéØ Focus: Advanced contrastive and triplet learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Mathematical Foundation: Contrastive Learning\n",
    "\n",
    "### Understanding InfoNCE and Triplet Loss Formulations\n",
    "Let's implement and analyze the mathematical foundations of contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLearningFoundation:\n",
    "    \"\"\"Mathematical foundation and analysis of contrastive learning\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07, margin=0.5):\n",
    "        self.temperature = temperature\n",
    "        self.margin = margin\n",
    "        \n",
    "    def infonce_loss(self, anchor, positive, negatives, return_components=False):\n",
    "        \"\"\"\n",
    "        Compute InfoNCE loss\n",
    "        \n",
    "        InfoNCE: L = -log(exp(sim(a,p)/œÑ) / (exp(sim(a,p)/œÑ) + Œ£ exp(sim(a,n)/œÑ)))\n",
    "        \n",
    "        Args:\n",
    "            anchor: [batch_size, dim] anchor embeddings\n",
    "            positive: [batch_size, dim] positive embeddings  \n",
    "            negatives: [batch_size, num_negatives, dim] negative embeddings\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
    "        \n",
    "        # Compute negative similarities\n",
    "        neg_sims = []\n",
    "        for i in range(batch_size):\n",
    "            anchor_expanded = anchor[i:i+1].expand(negatives.size(1), -1)\n",
    "            neg_sim = F.cosine_similarity(anchor_expanded, negatives[i], dim=1) / self.temperature\n",
    "            neg_sims.append(neg_sim)\n",
    "        \n",
    "        # Compute InfoNCE loss\n",
    "        losses = []\n",
    "        for i in range(batch_size):\n",
    "            numerator = torch.exp(pos_sim[i])\n",
    "            denominator = numerator + torch.sum(torch.exp(neg_sims[i]))\n",
    "            loss = -torch.log(numerator / denominator)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        final_loss = torch.stack(losses).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': [neg_sim.detach() for neg_sim in neg_sims],\n",
    "                'individual_losses': torch.stack(losses).detach()\n",
    "            }\n",
    "            return final_loss, components\n",
    "        \n",
    "        return final_loss\n",
    "    \n",
    "    def triplet_loss(self, anchor, positive, negative, return_components=False):\n",
    "        \"\"\"\n",
    "        Compute triplet loss with margin\n",
    "        \n",
    "        Triplet: L = max(0, margin + sim(a,n) - sim(a,p))\n",
    "        \n",
    "        Args:\n",
    "            anchor: [batch_size, dim] anchor embeddings\n",
    "            positive: [batch_size, dim] positive embeddings\n",
    "            negative: [batch_size, dim] negative embeddings\n",
    "        \"\"\"\n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1)\n",
    "        neg_sim = F.cosine_similarity(anchor, negative, dim=1)\n",
    "        \n",
    "        # Triplet loss with margin\n",
    "        loss = torch.clamp(self.margin + neg_sim - pos_sim, min=0.0).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': neg_sim.detach(),\n",
    "                'margin_violations': (neg_sim - pos_sim + self.margin).detach()\n",
    "            }\n",
    "            return loss, components\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def multiple_negatives_ranking_loss(self, anchor, positive, negatives, return_components=False):\n",
    "        \"\"\"\n",
    "        Multiple Negatives Ranking Loss (used in sentence-transformers)\n",
    "        \n",
    "        Similar to InfoNCE but designed for ranking tasks\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        \n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
    "        \n",
    "        # Compute all negative similarities\n",
    "        all_neg_sims = []\n",
    "        for i in range(batch_size):\n",
    "            for j in range(negatives.size(1)):\n",
    "                neg_sim = F.cosine_similarity(\n",
    "                    anchor[i:i+1], negatives[i, j:j+1], dim=1\n",
    "                ) / self.temperature\n",
    "                all_neg_sims.append(neg_sim)\n",
    "        \n",
    "        # Combine positive and negative similarities\n",
    "        all_sims = torch.cat([pos_sim] + all_neg_sims)\n",
    "        \n",
    "        # Create labels (first batch_size are positives)\n",
    "        labels = torch.arange(batch_size).to(anchor.device)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        logits = all_sims.view(batch_size, -1)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'logits': logits.detach(),\n",
    "                'labels': labels.detach(),\n",
    "                'positive_similarities': pos_sim.detach()\n",
    "            }\n",
    "            return loss, components\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def demonstrate_loss_behaviors(self):\n",
    "        \"\"\"Demonstrate different loss function behaviors\"\"\"\n",
    "        print(\"üßÆ Contrastive Loss Function Analysis\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Create sample embeddings\n",
    "        batch_size, dim, num_negatives = 4, 128, 3\n",
    "        \n",
    "        anchor = torch.randn(batch_size, dim)\n",
    "        positive = torch.randn(batch_size, dim)\n",
    "        negatives = torch.randn(batch_size, num_negatives, dim)\n",
    "        \n",
    "        # Make positives more similar to anchors\n",
    "        positive = 0.7 * anchor + 0.3 * positive\n",
    "        \n",
    "        # Make some negatives harder (more similar to anchor)\n",
    "        negatives[:, 0] = 0.3 * anchor + 0.7 * negatives[:, 0]  # Hard negatives\n",
    "        \n",
    "        print(f\"üìä Sample Setup:\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Embedding dimension: {dim}\")\n",
    "        print(f\"   Number of negatives: {num_negatives}\")\n",
    "        print(f\"   Temperature: {self.temperature}\")\n",
    "        print(f\"   Margin: {self.margin}\")\n",
    "        \n",
    "        # Test InfoNCE\n",
    "        print(f\"\\nüîç InfoNCE Loss Analysis:\")\n",
    "        infonce_loss, infonce_components = self.infonce_loss(\n",
    "            anchor, positive, negatives, return_components=True\n",
    "        )\n",
    "        print(f\"   Loss value: {infonce_loss.item():.4f}\")\n",
    "        print(f\"   Positive similarities: {infonce_components['positive_similarities'].tolist()}\")\n",
    "        \n",
    "        # Test Triplet Loss\n",
    "        print(f\"\\nüîç Triplet Loss Analysis:\")\n",
    "        triplet_loss, triplet_components = self.triplet_loss(\n",
    "            anchor, positive, negatives[:, 0], return_components=True  # Use first negative\n",
    "        )\n",
    "        print(f\"   Loss value: {triplet_loss.item():.4f}\")\n",
    "        print(f\"   Positive similarities: {triplet_components['positive_similarities'].tolist()}\")\n",
    "        print(f\"   Negative similarities: {triplet_components['negative_similarities'].tolist()}\")\n",
    "        print(f\"   Margin violations: {triplet_components['margin_violations'].tolist()}\")\n",
    "        \n",
    "        # Test Multiple Negatives Ranking\n",
    "        print(f\"\\nüîç Multiple Negatives Ranking Loss:\")\n",
    "        mnr_loss, mnr_components = self.multiple_negatives_ranking_loss(\n",
    "            anchor, positive, negatives, return_components=True\n",
    "        )\n",
    "        print(f\"   Loss value: {mnr_loss.item():.4f}\")\n",
    "        print(f\"   Logits shape: {mnr_components['logits'].shape}\")\n",
    "        \n",
    "        return {\n",
    "            'infonce': (infonce_loss, infonce_components),\n",
    "            'triplet': (triplet_loss, triplet_components),\n",
    "            'mnr': (mnr_loss, mnr_components)\n",
    "        }\n",
    "    \n",
    "    def analyze_temperature_effects(self):\n",
    "        \"\"\"Analyze effects of temperature on contrastive learning\"\"\"\n",
    "        print(\"\\nüå°Ô∏è Temperature Effects on Contrastive Learning\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Sample similarities\n",
    "        similarities = torch.tensor([0.9, 0.7, 0.5, 0.3, 0.1, -0.1])\n",
    "        temperatures = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "        \n",
    "        print(f\"üìä Raw similarities: {similarities.tolist()}\")\n",
    "        print(f\"\\nTemperature effects on softmax distribution:\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            scaled = similarities / temp\n",
    "            softmax_probs = F.softmax(scaled, dim=0)\n",
    "            \n",
    "            print(f\"\\n   œÑ = {temp:4.2f}:\")\n",
    "            print(f\"      Scaled: {scaled.tolist()}\")\n",
    "            print(f\"      Softmax: {softmax_probs.tolist()}\")\n",
    "            print(f\"      Max prob: {torch.max(softmax_probs).item():.3f}\")\n",
    "            print(f\"      Entropy: {-torch.sum(softmax_probs * torch.log(softmax_probs + 1e-8)).item():.3f}\")\n",
    "            \n",
    "            # Concentration analysis\n",
    "            concentration = (softmax_probs[0] / torch.sum(softmax_probs[1:])).item()\n",
    "            print(f\"      Concentration ratio: {concentration:.3f}\")\n",
    "        \n",
    "        return temperatures, similarities\n",
    "\n",
    "# Initialize foundation and run analysis\n",
    "contrastive_foundation = ContrastiveLearningFoundation(temperature=0.07, margin=0.5)\n",
    "loss_analysis = contrastive_foundation.demonstrate_loss_behaviors()\n",
    "temperature_analysis = contrastive_foundation.analyze_temperature_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Hard Negative Mining Strategies\n",
    "\n",
    "### Advanced Negative Sampling for Arabic Text\n",
    "The quality of negative samples is crucial for effective contrastive learning. Let's implement sophisticated mining strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ArabicTriplet:\n",
    "    \"\"\"Data structure for Arabic text triplets\"\"\"\n",
    "    anchor: str\n",
    "    positive: str\n",
    "    negative: str\n",
    "    anchor_embedding: Optional[torch.Tensor] = None\n",
    "    positive_embedding: Optional[torch.Tensor] = None\n",
    "    negative_embedding: Optional[torch.Tensor] = None\n",
    "    difficulty: float = 0.0  # Mining difficulty score\n",
    "    category: str = \"general\"  # Semantic category\n",
    "\n",
    "class HardNegativeMiner:\n",
    "    \"\"\"Advanced hard negative mining for Arabic text\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=768, similarity_threshold=0.7):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        \n",
    "        # Strategies for negative mining\n",
    "        self.mining_strategies = {\n",
    "            'random': self.random_negative_mining,\n",
    "            'hard': self.hard_negative_mining,\n",
    "            'semi_hard': self.semi_hard_negative_mining,\n",
    "            'cluster_based': self.cluster_based_mining,\n",
    "            'semantic': self.semantic_category_mining\n",
    "        }\n",
    "    \n",
    "    def create_mock_arabic_corpus(self):\n",
    "        \"\"\"Create mock Arabic text corpus for demonstration\"\"\"\n",
    "        \n",
    "        corpus = {\n",
    "            'education': [\n",
    "                \"ÿßŸÑÿ∑ÿßŸÑÿ® ŸäÿØÿ±ÿ≥ ŸÅŸä ÿßŸÑŸÖŸÉÿ™ÿ®ÿ© ÿ®ÿ¨ÿØ Ÿàÿßÿ¨ÿ™ŸáÿßÿØ\",\n",
    "                \"ÿßŸÑŸÖÿπŸÑŸÖ Ÿäÿ¥ÿ±ÿ≠ ÿßŸÑÿØÿ±ÿ≥ ŸÑŸÑÿ∑ŸÑÿßÿ® ŸÅŸä ÿßŸÑŸÅÿµŸÑ\",\n",
    "                \"ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿ™ŸÇÿØŸÖ ÿ®ÿ±ÿßŸÖÿ¨ ÿ™ÿπŸÑŸäŸÖŸäÿ© ŸÖÿ™ŸÜŸàÿπÿ© ŸàŸÖÿ™ŸÖŸäÿ≤ÿ©\",\n",
    "                \"ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿπŸÑŸÖŸä Ÿäÿ≥ÿßŸáŸÖ ŸÅŸä ÿ™ÿ∑ŸàŸäÿ± ÿßŸÑŸÖÿπÿ±ŸÅÿ©\",\n",
    "                \"ÿßŸÑÿ™ÿπŸÑŸäŸÖ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ÿ£ÿµÿ®ÿ≠ ÿ∂ÿ±Ÿàÿ±ÿ© ŸÅŸä ÿßŸÑÿπÿµÿ± ÿßŸÑÿ≠ÿØŸäÿ´\"\n",
    "            ],\n",
    "            'technology': [\n",
    "                \"ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä Ÿäÿ∫Ÿäÿ± ŸÖÿ≥ÿ™ŸÇÿ®ŸÑ ÿßŸÑÿ™ŸÉŸÜŸàŸÑŸàÿ¨Ÿäÿß\",\n",
    "                \"ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® ÿ£ÿØÿßÿ© ŸÖŸáŸÖÿ© ŸÅŸä ÿßŸÑÿπŸÖŸÑ ŸàÿßŸÑÿ≠Ÿäÿßÿ©\",\n",
    "                \"ÿßŸÑÿ•ŸÜÿ™ÿ±ŸÜÿ™ Ÿäÿ±ÿ®ÿ∑ ÿßŸÑÿπÿßŸÑŸÖ ÿ®ÿ®ÿπÿ∂Ÿá ÿßŸÑÿ®ÿπÿ∂\",\n",
    "                \"ÿßŸÑÿ®ÿ±ŸÖÿ¨Ÿäÿßÿ™ ÿ™ÿ≥ŸáŸÑ ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑŸÖŸáÿßŸÖ ÿßŸÑŸäŸàŸÖŸäÿ©\",\n",
    "                \"ÿßŸÑŸáŸàÿßÿ™ŸÅ ÿßŸÑÿ∞ŸÉŸäÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ™ŸÇŸÜŸäÿßÿ™ ŸÖÿ™ŸÇÿØŸÖÿ©\"\n",
    "            ],\n",
    "            'health': [\n",
    "                \"ÿßŸÑÿ±Ÿäÿßÿ∂ÿ© ÿ™ÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ÿµÿ≠ÿ© ÿßŸÑÿ¨ÿ≥ŸÖ ŸàÿßŸÑÿπŸÇŸÑ\",\n",
    "                \"ÿßŸÑÿ™ÿ∫ÿ∞Ÿäÿ© ÿßŸÑÿµÿ≠Ÿäÿ© ÿ£ÿ≥ÿßÿ≥ ÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑÿ≥ŸÑŸäŸÖÿ©\",\n",
    "                \"ÿßŸÑÿ∑ÿ®Ÿäÿ® ŸäÿπÿßŸÑÿ¨ ÿßŸÑŸÖÿ±ÿ∂Ÿâ ÿ®ÿÆÿ®ÿ±ÿ© ŸàŸÖŸáÿßÿ±ÿ©\",\n",
    "                \"ÿßŸÑŸÖÿ≥ÿ™ÿ¥ŸÅŸâ ŸäŸÇÿØŸÖ ÿÆÿØŸÖÿßÿ™ ÿ∑ÿ®Ÿäÿ© ŸÖÿ™ÿ∑Ÿàÿ±ÿ©\",\n",
    "                \"ÿßŸÑŸàŸÇÿßŸäÿ© ÿÆŸäÿ± ŸÖŸÜ ÿßŸÑÿπŸÑÿßÿ¨ ŸÅŸä ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ÿ≠ŸàÿßŸÑ\"\n",
    "            ],\n",
    "            'culture': [\n",
    "                \"ÿßŸÑÿ´ŸÇÿßŸÅÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ™ÿ±ÿßÿ´ ÿ∫ŸÜŸä ŸàŸÖÿ™ŸÜŸàÿπ\",\n",
    "                \"ÿßŸÑÿ¥ÿπÿ± ÿßŸÑÿπÿ±ÿ®Ÿä ŸÑŸá ÿ™ÿßÿ±ŸäÿÆ ÿ∑ŸàŸäŸÑ ŸàŸÖÿ¨ŸäÿØ\",\n",
    "                \"ÿßŸÑŸÅŸÜŸàŸÜ ÿßŸÑÿ™ÿ±ÿßÿ´Ÿäÿ© ÿ™ÿπÿ®ÿ± ÿπŸÜ ŸáŸàŸäÿ© ÿßŸÑÿ¥ÿπŸàÿ®\",\n",
    "                \"ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÑÿ∫ÿ© ÿßŸÑÿ∂ÿßÿØ ŸàÿßŸÑÿ®ŸäÿßŸÜ\",\n",
    "                \"ÿßŸÑÿ£ÿØÿ® ÿßŸÑÿπÿ±ÿ®Ÿä Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÉŸÜŸàÿ≤ ŸÖŸÜ ÿßŸÑŸÖÿπÿ±ŸÅÿ©\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return corpus\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Generate mock embeddings for texts\"\"\"\n",
    "        embeddings = torch.randn(len(texts), self.embedding_dim)\n",
    "        \n",
    "        # Add some semantic structure (texts with similar words get similar embeddings)\n",
    "        for i, text1 in enumerate(texts):\n",
    "            words1 = set(text1.split())\n",
    "            for j, text2 in enumerate(texts):\n",
    "                if i != j:\n",
    "                    words2 = set(text2.split())\n",
    "                    overlap = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "                    \n",
    "                    # Make embeddings more similar based on word overlap\n",
    "                    if overlap > 0.2:\n",
    "                        embeddings[j] = 0.3 * embeddings[i] + 0.7 * embeddings[j]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def random_negative_mining(self, anchor_text: str, positive_text: str, \n",
    "                              corpus: Dict[str, List[str]], num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Random negative sampling (baseline)\"\"\"\n",
    "        all_texts = []\n",
    "        for category_texts in corpus.values():\n",
    "            all_texts.extend(category_texts)\n",
    "        \n",
    "        # Remove anchor and positive from candidates\n",
    "        candidates = [text for text in all_texts if text not in [anchor_text, positive_text]]\n",
    "        \n",
    "        return random.sample(candidates, min(num_negatives, len(candidates)))\n",
    "    \n",
    "    def hard_negative_mining(self, anchor_embedding: torch.Tensor, positive_embedding: torch.Tensor,\n",
    "                            candidate_embeddings: torch.Tensor, candidate_texts: List[str],\n",
    "                            num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Hard negative mining - select most similar negatives\"\"\"\n",
    "        # Compute similarities with anchor\n",
    "        similarities = F.cosine_similarity(\n",
    "            anchor_embedding.unsqueeze(0), candidate_embeddings, dim=1\n",
    "        )\n",
    "        \n",
    "        # Get indices of most similar (hardest) negatives\n",
    "        _, hard_indices = torch.topk(similarities, min(num_negatives, len(similarities)))\n",
    "        \n",
    "        return [candidate_texts[idx] for idx in hard_indices.tolist()]\n",
    "    \n",
    "    def semi_hard_negative_mining(self, anchor_embedding: torch.Tensor, positive_embedding: torch.Tensor,\n",
    "                                 candidate_embeddings: torch.Tensor, candidate_texts: List[str],\n",
    "                                 num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Semi-hard negative mining - negatives closer to anchor than positive\"\"\"\n",
    "        # Compute similarities\n",
    "        anchor_pos_sim = F.cosine_similarity(anchor_embedding, positive_embedding, dim=0)\n",
    "        anchor_neg_sims = F.cosine_similarity(\n",
    "            anchor_embedding.unsqueeze(0), candidate_embeddings, dim=1\n",
    "        )\n",
    "        \n",
    "        # Semi-hard condition: similarity(anchor, negative) > similarity(anchor, positive)\n",
    "        semi_hard_mask = anchor_neg_sims > anchor_pos_sim\n",
    "        semi_hard_indices = torch.where(semi_hard_mask)[0]\n",
    "        \n",
    "        if len(semi_hard_indices) == 0:\n",
    "            # Fallback to hard negatives if no semi-hard found\n",
    "            return self.hard_negative_mining(\n",
    "                anchor_embedding, positive_embedding, candidate_embeddings, candidate_texts, num_negatives\n",
    "            )\n",
    "        \n",
    "        # Select from semi-hard negatives\n",
    "        selected_indices = semi_hard_indices[\n",
    "            torch.randperm(len(semi_hard_indices))[:min(num_negatives, len(semi_hard_indices))]\n",
    "        ]\n",
    "        \n",
    "        return [candidate_texts[idx] for idx in selected_indices.tolist()]\n",
    "    \n",
    "    def cluster_based_mining(self, anchor_embedding: torch.Tensor, positive_embedding: torch.Tensor,\n",
    "                            candidate_embeddings: torch.Tensor, candidate_texts: List[str],\n",
    "                            num_negatives: int = 5, num_clusters: int = 3) -> List[str]:\n",
    "        \"\"\"Cluster-based negative mining for diverse negatives\"\"\"\n",
    "        # Perform clustering on candidate embeddings\n",
    "        if len(candidate_embeddings) < num_clusters:\n",
    "            return self.random_negative_mining(None, None, {'all': candidate_texts}, num_negatives)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(candidate_embeddings.numpy())\n",
    "        \n",
    "        # Sample negatives from different clusters\n",
    "        negatives = []\n",
    "        negatives_per_cluster = max(1, num_negatives // num_clusters)\n",
    "        \n",
    "        for cluster_id in range(num_clusters):\n",
    "            cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "            if len(cluster_indices) > 0:\n",
    "                selected = np.random.choice(\n",
    "                    cluster_indices, \n",
    "                    min(negatives_per_cluster, len(cluster_indices)), \n",
    "                    replace=False\n",
    "                )\n",
    "                negatives.extend([candidate_texts[idx] for idx in selected])\n",
    "        \n",
    "        return negatives[:num_negatives]\n",
    "    \n",
    "    def semantic_category_mining(self, anchor_text: str, positive_text: str,\n",
    "                                corpus: Dict[str, List[str]], num_negatives: int = 5) -> List[str]:\n",
    "        \"\"\"Semantic category-based mining - avoid same category negatives\"\"\"\n",
    "        # Find anchor category\n",
    "        anchor_category = None\n",
    "        for category, texts in corpus.items():\n",
    "            if anchor_text in texts:\n",
    "                anchor_category = category\n",
    "                break\n",
    "        \n",
    "        # Sample from different categories\n",
    "        negatives = []\n",
    "        for category, texts in corpus.items():\n",
    "            if category != anchor_category:\n",
    "                available = [text for text in texts if text not in [anchor_text, positive_text]]\n",
    "                if available:\n",
    "                    negatives.extend(random.sample(available, min(2, len(available))))\n",
    "        \n",
    "        return negatives[:num_negatives]\n",
    "    \n",
    "    def create_arabic_triplets(self, corpus: Dict[str, List[str]], \n",
    "                              strategy: str = 'hard', num_triplets: int = 20) -> List[ArabicTriplet]:\n",
    "        \"\"\"Create Arabic triplets using specified mining strategy\"\"\"\n",
    "        print(f\"üéØ Creating Arabic Triplets with {strategy} mining\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        triplets = []\n",
    "        all_texts = []\n",
    "        \n",
    "        # Collect all texts\n",
    "        for category_texts in corpus.values():\n",
    "            all_texts.extend(category_texts)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        all_embeddings = self.generate_embeddings(all_texts)\n",
    "        text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n",
    "        \n",
    "        triplet_count = 0\n",
    "        \n",
    "        # Create triplets for each category\n",
    "        for category, texts in corpus.items():\n",
    "            if len(texts) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Create positive pairs within category\n",
    "            for i in range(len(texts)):\n",
    "                for j in range(i + 1, len(texts)):\n",
    "                    if triplet_count >= num_triplets:\n",
    "                        break\n",
    "                        \n",
    "                    anchor_text = texts[i]\n",
    "                    positive_text = texts[j]\n",
    "                    \n",
    "                    # Get embeddings\n",
    "                    anchor_emb = text_to_embedding[anchor_text]\n",
    "                    positive_emb = text_to_embedding[positive_text]\n",
    "                    \n",
    "                    # Mine negatives based on strategy\n",
    "                    if strategy in ['hard', 'semi_hard', 'cluster_based']:\n",
    "                        # Get candidate negatives (exclude same category for diversity)\n",
    "                        candidate_texts = []\n",
    "                        candidate_embeddings = []\n",
    "                        \n",
    "                        for other_category, other_texts in corpus.items():\n",
    "                            if other_category != category:\n",
    "                                candidate_texts.extend(other_texts)\n",
    "                                candidate_embeddings.extend([\n",
    "                                    text_to_embedding[text] for text in other_texts\n",
    "                                ])\n",
    "                        \n",
    "                        if candidate_embeddings:\n",
    "                            candidate_embeddings = torch.stack(candidate_embeddings)\n",
    "                            negatives = self.mining_strategies[strategy](\n",
    "                                anchor_emb, positive_emb, candidate_embeddings, candidate_texts, 1\n",
    "                            )\n",
    "                        else:\n",
    "                            negatives = self.random_negative_mining(anchor_text, positive_text, corpus, 1)\n",
    "                    else:\n",
    "                        negatives = self.mining_strategies[strategy](anchor_text, positive_text, corpus, 1)\n",
    "                    \n",
    "                    if negatives:\n",
    "                        negative_text = negatives[0]\n",
    "                        negative_emb = text_to_embedding[negative_text]\n",
    "                        \n",
    "                        # Calculate difficulty score\n",
    "                        pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=0).item()\n",
    "                        neg_sim = F.cosine_similarity(anchor_emb, negative_emb, dim=0).item()\n",
    "                        difficulty = neg_sim - pos_sim + 1.0  # Higher = more difficult\n",
    "                        \n",
    "                        triplet = ArabicTriplet(\n",
    "                            anchor=anchor_text,\n",
    "                            positive=positive_text,\n",
    "                            negative=negative_text,\n",
    "                            anchor_embedding=anchor_emb,\n",
    "                            positive_embedding=positive_emb,\n",
    "                            negative_embedding=negative_emb,\n",
    "                            difficulty=difficulty,\n",
    "                            category=category\n",
    "                        )\n",
    "                        \n",
    "                        triplets.append(triplet)\n",
    "                        triplet_count += 1\n",
    "                        \n",
    "                        print(f\"   Triplet {triplet_count}: {category} (difficulty: {difficulty:.3f})\")\n",
    "                        print(f\"      Anchor: {anchor_text[:50]}...\")\n",
    "                        print(f\"      Positive: {positive_text[:50]}...\")\n",
    "                        print(f\"      Negative: {negative_text[:50]}...\")\n",
    "                        print(f\"      Pos sim: {pos_sim:.3f}, Neg sim: {neg_sim:.3f}\")\n",
    "                        print()\n",
    "                \n",
    "                if triplet_count >= num_triplets:\n",
    "                    break\n",
    "            \n",
    "            if triplet_count >= num_triplets:\n",
    "                break\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(triplets)} Arabic triplets using {strategy} mining\")\n",
    "        return triplets\n",
    "    \n",
    "    def analyze_mining_strategies(self, corpus: Dict[str, List[str]]) -> Dict[str, List[ArabicTriplet]]:\n",
    "        \"\"\"Compare different mining strategies\"\"\"\n",
    "        print(\"\\nüîç Comparing Mining Strategies\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        strategies_results = {}\n",
    "        \n",
    "        for strategy in ['random', 'hard', 'semi_hard', 'semantic']:\n",
    "            print(f\"\\nüìä Testing {strategy} strategy:\")\n",
    "            triplets = self.create_arabic_triplets(corpus, strategy, num_triplets=5)\n",
    "            strategies_results[strategy] = triplets\n",
    "            \n",
    "            # Analyze strategy characteristics\n",
    "            if triplets:\n",
    "                difficulties = [t.difficulty for t in triplets]\n",
    "                avg_difficulty = np.mean(difficulties)\n",
    "                std_difficulty = np.std(difficulties)\n",
    "                \n",
    "                print(f\"   Average difficulty: {avg_difficulty:.3f}\")\n",
    "                print(f\"   Difficulty std: {std_difficulty:.3f}\")\n",
    "                print(f\"   Difficulty range: {min(difficulties):.3f} - {max(difficulties):.3f}\")\n",
    "        \n",
    "        return strategies_results\n",
    "\n",
    "# Initialize miner and create corpus\n",
    "hard_negative_miner = HardNegativeMiner()\n",
    "arabic_corpus = hard_negative_miner.create_mock_arabic_corpus()\n",
    "\n",
    "print(\"üìö Arabic Corpus Created:\")\n",
    "for category, texts in arabic_corpus.items():\n",
    "    print(f\"   {category}: {len(texts)} texts\")\n",
    "\n",
    "# Test different mining strategies\n",
    "mining_comparison = hard_negative_miner.analyze_mining_strategies(arabic_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Advanced Triplet Training Framework\n",
    "\n",
    "### Complete Implementation for Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicTripletTrainer:\n",
    "    \"\"\"Advanced triplet training framework for Arabic text embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=768, margin=0.5, temperature=0.07, \n",
    "                 mining_strategy='hard', curriculum_learning=True):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "        self.mining_strategy = mining_strategy\n",
    "        self.curriculum_learning = curriculum_learning\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'losses': [],\n",
    "            'difficulties': [],\n",
    "            'positive_similarities': [],\n",
    "            'negative_similarities': [],\n",
    "            'margin_violations': []\n",
    "        }\n",
    "        \n",
    "        # Curriculum learning parameters\n",
    "        self.curriculum_stages = {\n",
    "            'easy': {'difficulty_threshold': 0.3, 'epochs': 2},\n",
    "            'medium': {'difficulty_threshold': 0.6, 'epochs': 3},\n",
    "            'hard': {'difficulty_threshold': 1.0, 'epochs': 5}\n",
    "        }\n",
    "        \n",
    "    def triplet_loss_with_hard_negatives(self, anchor, positive, negative, \n",
    "                                        return_components=False):\n",
    "        \"\"\"\n",
    "        Advanced triplet loss with multiple negatives and adaptive margin\n",
    "        \"\"\"\n",
    "        # Compute similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1)\n",
    "        neg_sim = F.cosine_similarity(anchor, negative, dim=1)\n",
    "        \n",
    "        # Adaptive margin based on positive similarity\n",
    "        adaptive_margin = self.margin * (1.0 - pos_sim.detach())\n",
    "        \n",
    "        # Triplet loss with adaptive margin\n",
    "        loss = torch.clamp(adaptive_margin + neg_sim - pos_sim, min=0.0)\n",
    "        \n",
    "        # Add hard negative penalty (encourage diversity)\n",
    "        hard_negative_penalty = torch.clamp(neg_sim - 0.8, min=0.0) * 0.1\n",
    "        \n",
    "        total_loss = (loss + hard_negative_penalty).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'base_loss': loss.mean().item(),\n",
    "                'hard_penalty': hard_negative_penalty.mean().item(),\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': neg_sim.detach(),\n",
    "                'adaptive_margins': adaptive_margin.detach(),\n",
    "                'margin_violations': (neg_sim - pos_sim + adaptive_margin).detach()\n",
    "            }\n",
    "            return total_loss, components\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def contrastive_loss_with_temperature(self, anchor, positive, negatives,\n",
    "                                         return_components=False):\n",
    "        \"\"\"\n",
    "        InfoNCE-style loss with learnable temperature\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        \n",
    "        # Compute positive similarities\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
    "        \n",
    "        # Compute negative similarities\n",
    "        neg_sims = []\n",
    "        for i in range(batch_size):\n",
    "            if negatives.dim() == 3:  # [batch_size, num_negatives, dim]\n",
    "                anchor_expanded = anchor[i:i+1].expand(negatives.size(1), -1)\n",
    "                neg_sim = F.cosine_similarity(anchor_expanded, negatives[i], dim=1) / self.temperature\n",
    "            else:  # [batch_size, dim] - single negative per sample\n",
    "                neg_sim = F.cosine_similarity(anchor[i:i+1], negatives[i:i+1], dim=1) / self.temperature\n",
    "            neg_sims.append(neg_sim)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        losses = []\n",
    "        for i in range(batch_size):\n",
    "            numerator = torch.exp(pos_sim[i])\n",
    "            denominator = numerator + torch.sum(torch.exp(neg_sims[i]))\n",
    "            loss = -torch.log(numerator / (denominator + 1e-8))\n",
    "            losses.append(loss)\n",
    "        \n",
    "        total_loss = torch.stack(losses).mean()\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'positive_similarities': pos_sim.detach(),\n",
    "                'negative_similarities': [neg_sim.detach() for neg_sim in neg_sims],\n",
    "                'individual_losses': torch.stack(losses).detach(),\n",
    "                'temperature': self.temperature\n",
    "            }\n",
    "            return total_loss, components\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def curriculum_training_step(self, triplets: List[ArabicTriplet], \n",
    "                                current_stage: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform curriculum learning training step\n",
    "        \"\"\"\n",
    "        stage_config = self.curriculum_stages[current_stage]\n",
    "        \n",
    "        # Filter triplets by difficulty\n",
    "        filtered_triplets = [\n",
    "            t for t in triplets \n",
    "            if t.difficulty <= stage_config['difficulty_threshold']\n",
    "        ]\n",
    "        \n",
    "        if not filtered_triplets:\n",
    "            print(f\"‚ö†Ô∏è No triplets found for {current_stage} stage\")\n",
    "            return {'loss': 0.0, 'num_samples': 0}\n",
    "        \n",
    "        print(f\"\\nüìö Training Stage: {current_stage}\")\n",
    "        print(f\"   Difficulty threshold: {stage_config['difficulty_threshold']}\")\n",
    "        print(f\"   Available triplets: {len(filtered_triplets)}\")\n",
    "        \n",
    "        # Prepare batch data\n",
    "        anchors = torch.stack([t.anchor_embedding for t in filtered_triplets])\n",
    "        positives = torch.stack([t.positive_embedding for t in filtered_triplets])\n",
    "        negatives = torch.stack([t.negative_embedding for t in filtered_triplets])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, components = self.triplet_loss_with_hard_negatives(\n",
    "            anchors, positives, negatives, return_components=True\n",
    "        )\n",
    "        \n",
    "        # Record training history\n",
    "        self.training_history['losses'].append(loss.item())\n",
    "        self.training_history['difficulties'].extend([t.difficulty for t in filtered_triplets])\n",
    "        self.training_history['positive_similarities'].extend(\n",
    "            components['positive_similarities'].tolist()\n",
    "        )\n",
    "        self.training_history['negative_similarities'].extend(\n",
    "            components['negative_similarities'].tolist()\n",
    "        )\n",
    "        self.training_history['margin_violations'].extend(\n",
    "            components['margin_violations'].tolist()\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        pos_sim_mean = components['positive_similarities'].mean().item()\n",
    "        neg_sim_mean = components['negative_similarities'].mean().item()\n",
    "        margin_violations = (components['margin_violations'] > 0).float().mean().item()\n",
    "        \n",
    "        results = {\n",
    "            'loss': loss.item(),\n",
    "            'base_loss': components['base_loss'],\n",
    "            'hard_penalty': components['hard_penalty'],\n",
    "            'num_samples': len(filtered_triplets),\n",
    "            'pos_sim_mean': pos_sim_mean,\n",
    "            'neg_sim_mean': neg_sim_mean,\n",
    "            'margin_violations': margin_violations,\n",
    "            'avg_difficulty': np.mean([t.difficulty for t in filtered_triplets])\n",
    "        }\n",
    "        \n",
    "        print(f\"   Loss: {loss.item():.4f}\")\n",
    "        print(f\"   Positive similarity: {pos_sim_mean:.3f}\")\n",
    "        print(f\"   Negative similarity: {neg_sim_mean:.3f}\")\n",
    "        print(f\"   Margin violations: {margin_violations:.1%}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def full_curriculum_training(self, triplets: List[ArabicTriplet]) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Complete curriculum learning training\n",
    "        \"\"\"\n",
    "        print(\"üéì Starting Curriculum Learning Training\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for stage in ['easy', 'medium', 'hard']:\n",
    "            stage_results = []\n",
    "            \n",
    "            for epoch in range(self.curriculum_stages[stage]['epochs']):\n",
    "                print(f\"\\nüîÑ Epoch {epoch + 1}/{self.curriculum_stages[stage]['epochs']}\")\n",
    "                \n",
    "                epoch_result = self.curriculum_training_step(triplets, stage)\n",
    "                epoch_result['stage'] = stage\n",
    "                epoch_result['epoch'] = epoch + 1\n",
    "                \n",
    "                stage_results.append(epoch_result)\n",
    "            \n",
    "            all_results[stage] = stage_results\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def analyze_training_dynamics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyze training dynamics and convergence\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä Training Dynamics Analysis\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        if not self.training_history['losses']:\n",
    "            print(\"‚ö†Ô∏è No training history available\")\n",
    "            return {}\n",
    "        \n",
    "        losses = self.training_history['losses']\n",
    "        pos_sims = self.training_history['positive_similarities']\n",
    "        neg_sims = self.training_history['negative_similarities']\n",
    "        violations = self.training_history['margin_violations']\n",
    "        \n",
    "        analysis = {\n",
    "            'loss_trend': 'decreasing' if losses[-1] < losses[0] else 'increasing',\n",
    "            'loss_reduction': (losses[0] - losses[-1]) / losses[0] * 100,\n",
    "            'avg_loss': np.mean(losses),\n",
    "            'loss_std': np.std(losses),\n",
    "            'final_loss': losses[-1],\n",
    "            'avg_pos_similarity': np.mean(pos_sims),\n",
    "            'avg_neg_similarity': np.mean(neg_sims),\n",
    "            'similarity_gap': np.mean(pos_sims) - np.mean(neg_sims),\n",
    "            'violation_rate': np.mean([v > 0 for v in violations]),\n",
    "            'training_stability': 1.0 / (1.0 + np.std(losses[-5:]) if len(losses) >= 5 else np.std(losses))\n",
    "        }\n",
    "        \n",
    "        print(f\"üìà Loss Trend: {analysis['loss_trend']}\")\n",
    "        print(f\"üìâ Loss Reduction: {analysis['loss_reduction']:.1f}%\")\n",
    "        print(f\"üéØ Final Loss: {analysis['final_loss']:.4f}\")\n",
    "        print(f\"‚úÖ Positive Similarity: {analysis['avg_pos_similarity']:.3f}\")\n",
    "        print(f\"‚ùå Negative Similarity: {analysis['avg_neg_similarity']:.3f}\")\n",
    "        print(f\"üìè Similarity Gap: {analysis['similarity_gap']:.3f}\")\n",
    "        print(f\"‚ö†Ô∏è Violation Rate: {analysis['violation_rate']:.1%}\")\n",
    "        print(f\"üîí Training Stability: {analysis['training_stability']:.3f}\")\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Test triplet training with different strategies\n",
    "def test_triplet_training_strategies():\n",
    "    \"\"\"Test different triplet training strategies\"\"\"\n",
    "    print(\"\\nüß™ Testing Triplet Training Strategies\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    strategies = ['hard', 'semi_hard', 'random']\n",
    "    results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\nüéØ Testing {strategy} strategy:\")\n",
    "        \n",
    "        # Create triplets with this strategy\n",
    "        triplets = hard_negative_miner.create_arabic_triplets(\n",
    "            arabic_corpus, strategy=strategy, num_triplets=10\n",
    "        )\n",
    "        \n",
    "        if triplets:\n",
    "            # Initialize trainer\n",
    "            trainer = ArabicTripletTrainer(\n",
    "                mining_strategy=strategy,\n",
    "                curriculum_learning=True\n",
    "            )\n",
    "            \n",
    "            # Run curriculum training\n",
    "            training_results = trainer.full_curriculum_training(triplets)\n",
    "            \n",
    "            # Analyze training dynamics\n",
    "            dynamics = trainer.analyze_training_dynamics()\n",
    "            \n",
    "            results[strategy] = {\n",
    "                'training_results': training_results,\n",
    "                'dynamics': dynamics,\n",
    "                'num_triplets': len(triplets)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive testing\n",
    "training_strategy_results = test_triplet_training_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Advanced Visualization and Analysis\n",
    "\n",
    "### Understanding Contrastive Learning Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_contrastive_learning_analysis():\n",
    "    \"\"\"Create comprehensive visualizations of contrastive learning\"\"\"\n",
    "    \n",
    "    # Create comprehensive subplot layout\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Mining Strategy Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    if mining_comparison:\n",
    "        strategies = list(mining_comparison.keys())\n",
    "        avg_difficulties = []\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            triplets = mining_comparison[strategy]\n",
    "            if triplets:\n",
    "                avg_difficulties.append(np.mean([t.difficulty for t in triplets]))\n",
    "            else:\n",
    "                avg_difficulties.append(0)\n",
    "        \n",
    "        bars = ax1.bar(strategies, avg_difficulties, alpha=0.8, \n",
    "                      color=['skyblue', 'orange', 'green', 'red'])\n",
    "        ax1.set_title('Average Difficulty by Mining Strategy', fontweight='bold')\n",
    "        ax1.set_ylabel('Average Difficulty Score')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, avg_difficulties):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 2: Loss Function Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    if loss_analysis:\n",
    "        loss_types = ['InfoNCE', 'Triplet', 'MNR']\n",
    "        loss_values = [\n",
    "            loss_analysis['infonce'][0].item(),\n",
    "            loss_analysis['triplet'][0].item(),\n",
    "            loss_analysis['mnr'][0].item()\n",
    "        ]\n",
    "        \n",
    "        ax2.bar(loss_types, loss_values, alpha=0.8, color=['blue', 'green', 'purple'])\n",
    "        ax2.set_title('Loss Function Comparison', fontweight='bold')\n",
    "        ax2.set_ylabel('Loss Value')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Training Strategy Performance\n",
    "    ax3 = axes[0, 2]\n",
    "    if training_strategy_results:\n",
    "        strategies = list(training_strategy_results.keys())\n",
    "        final_losses = []\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            dynamics = training_strategy_results[strategy]['dynamics']\n",
    "            if 'final_loss' in dynamics:\n",
    "                final_losses.append(dynamics['final_loss'])\n",
    "            else:\n",
    "                final_losses.append(0)\n",
    "        \n",
    "        ax3.bar(strategies, final_losses, alpha=0.8, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "        ax3.set_title('Final Training Loss by Strategy', fontweight='bold')\n",
    "        ax3.set_ylabel('Final Loss')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Similarity Gap Analysis\n",
    "    ax4 = axes[1, 0]\n",
    "    if training_strategy_results:\n",
    "        strategies = list(training_strategy_results.keys())\n",
    "        similarity_gaps = []\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            dynamics = training_strategy_results[strategy]['dynamics']\n",
    "            if 'similarity_gap' in dynamics:\n",
    "                similarity_gaps.append(dynamics['similarity_gap'])\n",
    "            else:\n",
    "                similarity_gaps.append(0)\n",
    "        \n",
    "        ax4.bar(strategies, similarity_gaps, alpha=0.8, color=['gold', 'silver', 'bronze'])\n",
    "        ax4.set_title('Positive-Negative Similarity Gap', fontweight='bold')\n",
    "        ax4.set_ylabel('Similarity Gap')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Temperature Effects Visualization\n",
    "    ax5 = axes[1, 1]\n",
    "    if temperature_analysis:\n",
    "        temperatures, similarities = temperature_analysis\n",
    "        \n",
    "        # Show softmax distribution for different temperatures\n",
    "        for i, temp in enumerate(temperatures[:3]):  # Show first 3 temperatures\n",
    "            scaled = similarities / temp\n",
    "            softmax_probs = F.softmax(scaled, dim=0)\n",
    "            \n",
    "            ax5.plot(range(len(softmax_probs)), softmax_probs.numpy(), \n",
    "                    'o-', label=f'œÑ={temp}', alpha=0.8)\n",
    "        \n",
    "        ax5.set_title('Temperature Effects on Softmax', fontweight='bold')\n",
    "        ax5.set_xlabel('Similarity Rank')\n",
    "        ax5.set_ylabel('Softmax Probability')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Training Dynamics\n",
    "    ax6 = axes[1, 2]\n",
    "    if training_strategy_results:\n",
    "        for strategy, results in training_strategy_results.items():\n",
    "            if 'dynamics' in results and 'training_stability' in results['dynamics']:\n",
    "                stability = results['dynamics']['training_stability']\n",
    "                violation_rate = results['dynamics']['violation_rate']\n",
    "                \n",
    "                ax6.scatter(violation_rate, stability, s=100, alpha=0.7, label=strategy)\n",
    "        \n",
    "        ax6.set_title('Training Stability vs Violation Rate', fontweight='bold')\n",
    "        ax6.set_xlabel('Margin Violation Rate')\n",
    "        ax6.set_ylabel('Training Stability')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_contrastive_learning_insights():\n",
    "    \"\"\"Generate insights from contrastive learning analysis\"\"\"\n",
    "    print(\"\\nüí° Contrastive Learning Insights\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Mining strategy insights\n",
    "    if mining_comparison:\n",
    "        print(\"\\nüéØ Mining Strategy Analysis:\")\n",
    "        \n",
    "        for strategy, triplets in mining_comparison.items():\n",
    "            if triplets:\n",
    "                avg_difficulty = np.mean([t.difficulty for t in triplets])\n",
    "                difficulty_std = np.std([t.difficulty for t in triplets])\n",
    "                \n",
    "                print(f\"   {strategy:>12}: avg_difficulty={avg_difficulty:.3f}, std={difficulty_std:.3f}\")\n",
    "                \n",
    "                # Strategy characteristics\n",
    "                if strategy == 'hard':\n",
    "                    print(f\"                 ‚Üí Best for challenging models, may cause training instability\")\n",
    "                elif strategy == 'semi_hard':\n",
    "                    print(f\"                 ‚Üí Balanced approach, good convergence properties\")\n",
    "                elif strategy == 'random':\n",
    "                    print(f\"                 ‚Üí Baseline approach, stable but slower learning\")\n",
    "                elif strategy == 'semantic':\n",
    "                    print(f\"                 ‚Üí Category-aware, good for structured datasets\")\n",
    "    \n",
    "    # Training strategy insights\n",
    "    if training_strategy_results:\n",
    "        print(\"\\nüöÇ Training Strategy Performance:\")\n",
    "        \n",
    "        best_strategy = None\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        for strategy, results in training_strategy_results.items():\n",
    "            dynamics = results['dynamics']\n",
    "            if 'similarity_gap' in dynamics:\n",
    "                score = dynamics['similarity_gap'] * dynamics['training_stability']\n",
    "                \n",
    "                print(f\"   {strategy:>12}: gap={dynamics['similarity_gap']:.3f}, stability={dynamics['training_stability']:.3f}, score={score:.3f}\")\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_strategy = strategy\n",
    "        \n",
    "        if best_strategy:\n",
    "            print(f\"\\nüèÜ Best Strategy: {best_strategy} (score: {best_score:.3f})\")\n",
    "    \n",
    "    # Loss function insights\n",
    "    if loss_analysis:\n",
    "        print(\"\\nüßÆ Loss Function Characteristics:\")\n",
    "        \n",
    "        infonce_loss = loss_analysis['infonce'][0].item()\n",
    "        triplet_loss = loss_analysis['triplet'][0].item()\n",
    "        mnr_loss = loss_analysis['mnr'][0].item()\n",
    "        \n",
    "        print(f\"   InfoNCE: {infonce_loss:.4f} ‚Üí Good for large-scale contrastive learning\")\n",
    "        print(f\"   Triplet: {triplet_loss:.4f} ‚Üí Direct optimization of similarity relationships\")\n",
    "        print(f\"   MNR: {mnr_loss:.4f} ‚Üí Balanced approach for ranking tasks\")\n",
    "        \n",
    "        # Recommend best loss\n",
    "        losses = {'InfoNCE': infonce_loss, 'Triplet': triplet_loss, 'MNR': mnr_loss}\n",
    "        best_loss = min(losses, key=losses.get)\n",
    "        print(f\"\\nüí° Recommended loss: {best_loss} (lowest value: {losses[best_loss]:.4f})\")\n",
    "\n",
    "def practical_implementation_recommendations():\n",
    "    \"\"\"Provide practical recommendations for contrastive learning\"\"\"\n",
    "    print(\"\\nüõ†Ô∏è Practical Implementation Recommendations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    recommendations = {\n",
    "        \"üéØ Negative Mining Strategy\": [\n",
    "            \"Start with semi-hard negatives for stable training\",\n",
    "            \"Use hard negatives only after initial convergence\",\n",
    "            \"Implement dynamic difficulty adjustment\",\n",
    "            \"Mix strategies during training for robustness\"\n",
    "        ],\n",
    "        \"üå°Ô∏è Temperature Optimization\": [\n",
    "            \"Start with œÑ=0.07 and adjust based on loss behavior\",\n",
    "            \"Lower temperature (0.01-0.05) for sharper distributions\",\n",
    "            \"Higher temperature (0.1-0.5) for smoother learning\",\n",
    "            \"Use learnable temperature when possible\"\n",
    "        ],\n",
    "        \"üìö Curriculum Learning\": [\n",
    "            \"Begin with easy negatives (low similarity to anchor)\",\n",
    "            \"Gradually increase difficulty as model learns\",\n",
    "            \"Monitor margin violation rates as difficulty indicator\",\n",
    "            \"Adjust stage thresholds based on convergence\"\n",
    "        ],\n",
    "        \"üîß Training Optimization\": [\n",
    "            \"Use batch sizes of 64-256 for good negative sampling\",\n",
    "            \"Implement gradient clipping for training stability\",\n",
    "            \"Monitor positive/negative similarity gap\",\n",
    "            \"Apply data augmentation for Arabic text variation\"\n",
    "        ],\n",
    "        \"üìä Evaluation Metrics\": [\n",
    "            \"Track similarity gap as primary metric\",\n",
    "            \"Monitor margin violation rates for difficulty\",\n",
    "            \"Use retrieval metrics (MRR, nDCG) for validation\",\n",
    "            \"Test on diverse Arabic dialects for robustness\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, tips in recommendations.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"   ‚Ä¢ {tip}\")\n",
    "\n",
    "# Run comprehensive analysis\n",
    "visualize_contrastive_learning_analysis()\n",
    "generate_contrastive_learning_insights()\n",
    "practical_implementation_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Insights and Learning Takeaways\n",
    "\n",
    "### Mastering Contrastive Learning for Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_contrastive_learning_mastery():\n",
    "    \"\"\"Comprehensive summary of contrastive learning mastery\"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        \"üßÆ Mathematical Mastery\": [\n",
    "            \"InfoNCE optimizes ranking through temperature-scaled softmax\",\n",
    "            \"Triplet loss directly optimizes similarity relationships with margin\",\n",
    "            \"Temperature scaling controls distribution sharpness and learning speed\",\n",
    "            \"Multiple negatives ranking balances classification and similarity objectives\",\n",
    "            \"Adaptive margins improve training stability and convergence\"\n",
    "        ],\n",
    "        \"üéØ Mining Strategy Excellence\": [\n",
    "            \"Hard negatives accelerate learning but may cause instability\",\n",
    "            \"Semi-hard negatives provide optimal balance of challenge and stability\",\n",
    "            \"Cluster-based mining ensures negative diversity across semantic space\",\n",
    "            \"Semantic category mining leverages domain structure effectively\",\n",
    "            \"Dynamic difficulty adjustment enables curriculum learning\"\n",
    "        ],\n",
    "        \"üìö Curriculum Learning Innovation\": [\n",
    "            \"Progressive difficulty increases model robustness gradually\",\n",
    "            \"Stage-based training prevents mode collapse in early phases\",\n",
    "            \"Difficulty thresholds should align with model capacity\",\n",
    "            \"Margin violation rates indicate optimal difficulty progression\",\n",
    "            \"Multi-stage approach improves final performance significantly\"\n",
    "        ],\n",
    "        \"üöÄ Training Optimization\": [\n",
    "            \"Batch size affects negative sampling quality and diversity\",\n",
    "            \"Gradient clipping essential for hard negative training stability\",\n",
    "            \"Learning rate scheduling improves convergence in later stages\",\n",
    "            \"Mixed precision training enables larger batch sizes efficiently\",\n",
    "            \"Regular evaluation prevents overfitting to training negatives\"\n",
    "        ],\n",
    "        \"üî¨ Arabic-Specific Adaptations\": [\n",
    "            \"Morphological variations require careful negative selection\",\n",
    "            \"Dialectal differences enhance negative diversity naturally\",\n",
    "            \"Root-pattern relationships inform semantic similarity mining\",\n",
    "            \"Cross-dialectal triplets improve generalization\",\n",
    "            \"Arabic-specific augmentations increase training robustness\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"üéì Contrastive Learning Mastery\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, points in insights.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for point in points:\n",
    "            print(f\"   ‚Ä¢ {point}\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def connection_to_gate_innovation():\n",
    "    \"\"\"Connect insights to GATE's contrastive learning innovations\"\"\"\n",
    "    print(\"\\nüîó Connection to GATE's Innovation\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    print(\"üìã How GATE Advances Contrastive Learning:\")\n",
    "    \n",
    "    gate_innovations = {\n",
    "        \"Beyond InfoNCE Limitations\": [\n",
    "            \"Identifies InfoNCE insufficiency for sentence-level tasks\",\n",
    "            \"Implements task-specific loss functions for better performance\",\n",
    "            \"Uses Arabic NLI triplets for fine-grained similarity learning\"\n",
    "        ],\n",
    "        \"Arabic-Tailored Negative Mining\": [\n",
    "            \"Curates hard negatives from Arabic NLI datasets\",\n",
    "            \"Leverages linguistic structure for better negative selection\",\n",
    "            \"Addresses Arabic-specific semantic challenges\"\n",
    "        ],\n",
    "        \"Hybrid Training Integration\": [\n",
    "            \"Combines contrastive learning with classification objectives\",\n",
    "            \"Balances similarity learning with semantic understanding\",\n",
    "            \"Integrates with Matryoshka representation learning\"\n",
    "        ],\n",
    "        \"Performance Achievements\": [\n",
    "            \"20-25% improvement over larger models (OpenAI)\",\n",
    "            \"State-of-the-art Arabic STS performance on MTEB\",\n",
    "            \"Robust performance across multiple embedding dimensions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for innovation, details in gate_innovations.items():\n",
    "        print(f\"\\nüéØ {innovation}:\")\n",
    "        for detail in details:\n",
    "            print(f\"   ‚úì {detail}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Result: GATE demonstrates that thoughtful contrastive learning\")\n",
    "    print(f\"   design can outperform larger, more resource-intensive models\")\n",
    "\n",
    "def advanced_research_directions():\n",
    "    \"\"\"Outline advanced research directions in contrastive learning\"\"\"\n",
    "    print(\"\\nüî¨ Advanced Research Directions\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    directions = {\n",
    "        \"üß¨ Meta-Learning for Mining\": [\n",
    "            \"Learn optimal negative sampling strategies automatically\",\n",
    "            \"Adapt mining difficulty based on model performance\",\n",
    "            \"Personalize negative selection for different domains\"\n",
    "        ],\n",
    "        \"üåç Cross-Lingual Contrastive Learning\": [\n",
    "            \"Multi-lingual triplet construction and evaluation\",\n",
    "            \"Zero-shot transfer through cross-lingual negatives\",\n",
    "            \"Language-agnostic similarity learning frameworks\"\n",
    "        ],\n",
    "        \"üé≠ Multi-Modal Integration\": [\n",
    "            \"Text-image contrastive learning for Arabic content\",\n",
    "            \"Audio-text alignment for Arabic speech processing\",\n",
    "            \"Multi-modal negative mining strategies\"\n",
    "        ],\n",
    "        \"‚ö° Efficiency Optimizations\": [\n",
    "            \"Approximate negative sampling for large-scale training\",\n",
    "            \"Hierarchical negative caching mechanisms\",\n",
    "            \"Dynamic batch construction for optimal mining\"\n",
    "        ],\n",
    "        \"üéØ Task-Specific Adaptations\": [\n",
    "            \"Question-answering focused contrastive objectives\",\n",
    "            \"Document retrieval optimized negative mining\",\n",
    "            \"Conversational AI contrastive learning strategies\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for direction, aspects in directions.items():\n",
    "        print(f\"\\n{direction}:\")\n",
    "        for aspect in aspects:\n",
    "            print(f\"   ‚Ä¢ {aspect}\")\n",
    "\n",
    "# Generate comprehensive insights\n",
    "contrastive_insights = summarize_contrastive_learning_mastery()\n",
    "connection_to_gate_innovation()\n",
    "advanced_research_directions()\n",
    "\n",
    "print(\"\\nüéì Learning Completion Summary\")\n",
    "print(\"=\" * 35)\n",
    "print(\"‚úÖ Contrastive learning mathematics thoroughly mastered\")\n",
    "print(\"‚úÖ Advanced negative mining strategies implemented\")\n",
    "print(\"‚úÖ Curriculum learning framework developed\")\n",
    "print(\"‚úÖ Arabic-specific adaptations understood\")\n",
    "print(\"‚úÖ Training optimization techniques learned\")\n",
    "print(\"‚úÖ Connection to GATE's innovations established\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   ‚Ä¢ Apply contrastive learning to your domain\")\n",
    "print(\"   ‚Ä¢ Experiment with novel negative mining strategies\")\n",
    "print(\"   ‚Ä¢ Integrate with multi-modal learning systems\")\n",
    "print(\"   ‚Ä¢ Contribute to Arabic NLP research advancement\")\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You have completed all GATE focused learning notebooks:\")\n",
    "print(\"   1. ‚úÖ Matryoshka Representation Learning\")\n",
    "print(\"   2. ‚úÖ Hybrid Loss Architecture\")\n",
    "print(\"   3. ‚úÖ Arabic NLP Challenges\")\n",
    "print(\"   4. ‚úÖ Contrastive Triplet Learning\")\n",
    "print(\"\\nüåü You are now equipped to implement and extend GATE's innovations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}