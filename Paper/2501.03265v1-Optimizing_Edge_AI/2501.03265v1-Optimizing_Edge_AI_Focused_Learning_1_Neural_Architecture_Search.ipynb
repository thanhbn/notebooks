{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Objective Neural Architecture Search for Edge AI\n",
    "## Focused Learning Notebook 1/4\n",
    "\n",
    "**Paper Source**: Optimizing Edge AI: A Comprehensive Survey (2501.03265v1)  \n",
    "**Paper Sections**: Pages 11-13 (Neural Architecture Search)  \n",
    "**Focus Concept**: Hardware-Aware Multi-Objective NAS with Pareto Optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By completing this notebook, you will understand:\n",
    "\n",
    "1. **Multi-objective optimization** in neural architecture search\n",
    "2. **Hardware-aware constraints** and their impact on architecture design\n",
    "3. **Pareto-optimal solutions** for accuracy-efficiency trade-offs\n",
    "4. **Differentiable NAS (DNAS)** algorithms and implementation\n",
    "5. **Performance prediction models** for edge hardware\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Theoretical Foundation\n",
    "\n",
    "### Multi-Objective Neural Architecture Search Problem\n",
    "\n",
    "**Paper Quote** (Section Neural Architecture Search):\n",
    "> *\"Neural Architecture Search methods for edge deployment must balance multiple conflicting objectives: model accuracy, inference latency, memory footprint, and energy consumption, while considering specific hardware constraints.\"*\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The multi-objective NAS problem can be formulated as:\n",
    "\n",
    "$$\\min_{\\alpha \\in \\mathcal{A}} \\mathbf{f}(\\alpha) = [f_1(\\alpha), f_2(\\alpha), ..., f_k(\\alpha)]^T$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ represents the architecture parameters\n",
    "- $\\mathcal{A}$ is the architecture search space\n",
    "- $\\mathbf{f}(\\alpha)$ is a vector of $k$ objective functions\n",
    "\n",
    "**Typical objectives for Edge AI:**\n",
    "- $f_1(\\alpha)$: **Validation Error** (minimize)\n",
    "- $f_2(\\alpha)$: **Inference Latency** (minimize)\n",
    "- $f_3(\\alpha)$: **Memory Usage** (minimize)\n",
    "- $f_4(\\alpha)$: **Energy Consumption** (minimize)\n",
    "\n",
    "**Hardware Constraints:**\n",
    "$$\\text{subject to: } g_i(\\alpha) \\leq c_i, \\quad i = 1, 2, ..., m$$\n",
    "\n",
    "Where $g_i(\\alpha)$ represents hardware-specific constraints (memory limits, computation budgets, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Multi-objective optimization\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete for Multi-Objective NAS\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Architecture Search Space Definition\n",
    "\n",
    "Define a flexible search space for edge-optimized neural architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EdgeHardwareSpec:\n",
    "    \"\"\"Edge device hardware specifications\"\"\"\n",
    "    cpu_cores: int = 4\n",
    "    memory_mb: int = 512\n",
    "    max_latency_ms: float = 100.0\n",
    "    power_budget_mw: float = 1000.0\n",
    "    has_npu: bool = False  # Neural Processing Unit\n",
    "    \n",
    "class ArchitectureSearchSpace:\n",
    "    \"\"\"Defines the search space for neural architectures\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Layer types available for edge deployment\n",
    "        self.layer_types = {\n",
    "            'conv3x3': {'params_factor': 1.0, 'flops_factor': 1.0, 'latency_factor': 1.0},\n",
    "            'conv1x1': {'params_factor': 0.11, 'flops_factor': 0.11, 'latency_factor': 0.5},\n",
    "            'depthwise_conv': {'params_factor': 0.11, 'flops_factor': 0.11, 'latency_factor': 0.3},\n",
    "            'separable_conv': {'params_factor': 0.12, 'flops_factor': 0.12, 'latency_factor': 0.4},\n",
    "            'mobilenet_block': {'params_factor': 0.15, 'flops_factor': 0.15, 'latency_factor': 0.6},\n",
    "            'squeeze_excite': {'params_factor': 0.05, 'flops_factor': 0.02, 'latency_factor': 0.8},\n",
    "        }\n",
    "        \n",
    "        # Channel width options\n",
    "        self.channel_widths = [16, 32, 48, 64, 96, 128, 160, 192, 224, 256]\n",
    "        \n",
    "        # Depth options (number of layers)\n",
    "        self.depth_options = [4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "        \n",
    "        # Activation functions\n",
    "        self.activations = ['relu', 'relu6', 'swish', 'hardswish', 'gelu']\n",
    "        \n",
    "    def sample_architecture(self) -> Dict[str, Any]:\n",
    "        \"\"\"Sample a random architecture from the search space\"\"\"\n",
    "        depth = random.choice(self.depth_options)\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layer = {\n",
    "                'type': random.choice(list(self.layer_types.keys())),\n",
    "                'channels': random.choice(self.channel_widths),\n",
    "                'activation': random.choice(self.activations)\n",
    "            }\n",
    "            layers.append(layer)\n",
    "        \n",
    "        architecture = {\n",
    "            'layers': layers,\n",
    "            'depth': depth,\n",
    "            'global_pool': random.choice(['avg', 'max', 'adaptive']),\n",
    "            'dropout_rate': random.uniform(0.0, 0.5)\n",
    "        }\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    def encode_architecture(self, arch: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Encode architecture as a vector for optimization\"\"\"\n",
    "        # Simple encoding: layer types, channels, depth\n",
    "        encoding = []\n",
    "        \n",
    "        # Architecture depth\n",
    "        encoding.append(arch['depth'] / 20.0)  # Normalize\n",
    "        \n",
    "        # Layer information (pad/truncate to fixed length)\n",
    "        max_layers = 20\n",
    "        layer_encoding = []\n",
    "        \n",
    "        for i in range(max_layers):\n",
    "            if i < len(arch['layers']):\n",
    "                layer = arch['layers'][i]\n",
    "                # Layer type (one-hot)\n",
    "                layer_type_idx = list(self.layer_types.keys()).index(layer['type'])\n",
    "                layer_one_hot = [0] * len(self.layer_types)\n",
    "                layer_one_hot[layer_type_idx] = 1\n",
    "                layer_encoding.extend(layer_one_hot)\n",
    "                \n",
    "                # Channel width (normalized)\n",
    "                layer_encoding.append(layer['channels'] / 256.0)\n",
    "                \n",
    "                # Activation (simplified)\n",
    "                activation_idx = self.activations.index(layer['activation'])\n",
    "                layer_encoding.append(activation_idx / len(self.activations))\n",
    "            else:\n",
    "                # Padding for shorter architectures\n",
    "                layer_encoding.extend([0] * (len(self.layer_types) + 2))\n",
    "        \n",
    "        encoding.extend(layer_encoding)\n",
    "        \n",
    "        # Global settings\n",
    "        pool_types = ['avg', 'max', 'adaptive']\n",
    "        pool_idx = pool_types.index(arch['global_pool'])\n",
    "        encoding.append(pool_idx / len(pool_types))\n",
    "        encoding.append(arch['dropout_rate'])\n",
    "        \n",
    "        return np.array(encoding)\n",
    "\n",
    "# Initialize search space\n",
    "search_space = ArchitectureSearchSpace()\n",
    "edge_hardware = EdgeHardwareSpec()\n",
    "\n",
    "print(\"‚úÖ Architecture search space defined\")\n",
    "print(f\"   Layer types: {len(search_space.layer_types)}\")\n",
    "print(f\"   Channel options: {len(search_space.channel_widths)}\")\n",
    "print(f\"   Depth options: {len(search_space.depth_options)}\")\n",
    "\n",
    "# Sample architecture example\n",
    "sample_arch = search_space.sample_architecture()\n",
    "print(f\"\\nüìã Sample Architecture:\")\n",
    "print(f\"   Depth: {sample_arch['depth']} layers\")\n",
    "print(f\"   First 3 layers: {sample_arch['layers'][:3]}\")\n",
    "print(f\"   Global pool: {sample_arch['global_pool']}\")\n",
    "print(f\"   Dropout: {sample_arch['dropout_rate']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Prediction Models\n",
    "\n",
    "**Paper Reference**: *\"Hardware-aware optimization requires accurate performance prediction models that estimate latency, memory usage, and energy consumption without actual deployment.\"*\n",
    "\n",
    "### Hardware Performance Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardwarePerformanceModel:\n",
    "    \"\"\"Predicts hardware performance metrics for neural architectures\"\"\"\n",
    "    \n",
    "    def __init__(self, hardware_spec: EdgeHardwareSpec):\n",
    "        self.hardware_spec = hardware_spec\n",
    "        \n",
    "        # Lookup tables for different operations (simplified)\n",
    "        self.operation_latency = {\n",
    "            'conv3x3': 0.5,  # ms per GFLOP\n",
    "            'conv1x1': 0.2,\n",
    "            'depthwise_conv': 0.15,\n",
    "            'separable_conv': 0.25,\n",
    "            'mobilenet_block': 0.3,\n",
    "            'squeeze_excite': 0.8,\n",
    "        }\n",
    "        \n",
    "        self.operation_memory = {\n",
    "            'conv3x3': 2.0,  # MB per layer\n",
    "            'conv1x1': 0.5,\n",
    "            'depthwise_conv': 0.3,\n",
    "            'separable_conv': 0.4,\n",
    "            'mobilenet_block': 0.6,\n",
    "            'squeeze_excite': 0.1,\n",
    "        }\n",
    "        \n",
    "        self.operation_energy = {\n",
    "            'conv3x3': 100,  # mJ per GFLOP\n",
    "            'conv1x1': 40,\n",
    "            'depthwise_conv': 30,\n",
    "            'separable_conv': 50,\n",
    "            'mobilenet_block': 60,\n",
    "            'squeeze_excite': 20,\n",
    "        }\n",
    "        \n",
    "    def predict_latency(self, architecture: Dict[str, Any]) -> float:\n",
    "        \"\"\"Predict inference latency in milliseconds\"\"\"\n",
    "        total_latency = 0.0\n",
    "        \n",
    "        for layer in architecture['layers']:\n",
    "            layer_type = layer['type']\n",
    "            channels = layer['channels']\n",
    "            \n",
    "            # Simplified FLOPS calculation\n",
    "            flops_giga = (channels * channels * 32 * 32) / 1e9  # Assume 32x32 input\n",
    "            layer_latency = flops_giga * self.operation_latency[layer_type]\n",
    "            \n",
    "            # Channel scaling factor\n",
    "            channel_factor = np.sqrt(channels / 64.0)  # Reference: 64 channels\n",
    "            layer_latency *= channel_factor\n",
    "            \n",
    "            total_latency += layer_latency\n",
    "        \n",
    "        # Hardware-specific scaling\n",
    "        cpu_scaling = 4.0 / self.hardware_spec.cpu_cores\n",
    "        npu_scaling = 0.3 if self.hardware_spec.has_npu else 1.0\n",
    "        \n",
    "        total_latency *= cpu_scaling * npu_scaling\n",
    "        \n",
    "        # Add overhead\n",
    "        overhead = 5.0 + 0.5 * architecture['depth']  # ms\n",
    "        \n",
    "        return total_latency + overhead\n",
    "    \n",
    "    def predict_memory(self, architecture: Dict[str, Any]) -> float:\n",
    "        \"\"\"Predict memory usage in MB\"\"\"\n",
    "        total_memory = 0.0\n",
    "        \n",
    "        for layer in architecture['layers']:\n",
    "            layer_type = layer['type']\n",
    "            channels = layer['channels']\n",
    "            \n",
    "            # Base memory usage\n",
    "            layer_memory = self.operation_memory[layer_type]\n",
    "            \n",
    "            # Channel scaling (quadratic for weights, linear for activations)\n",
    "            weight_memory = layer_memory * (channels / 64.0) ** 1.5\n",
    "            activation_memory = 4 * channels * 32 * 32 / (1024 * 1024)  # 4 bytes per float\n",
    "            \n",
    "            total_memory += weight_memory + activation_memory\n",
    "        \n",
    "        # Global pooling and classifier\n",
    "        final_channels = architecture['layers'][-1]['channels'] if architecture['layers'] else 64\n",
    "        classifier_memory = final_channels * 1000 * 4 / (1024 * 1024)  # 1000 classes\n",
    "        \n",
    "        return total_memory + classifier_memory + 10  # 10MB base overhead\n",
    "    \n",
    "    def predict_energy(self, architecture: Dict[str, Any]) -> float:\n",
    "        \"\"\"Predict energy consumption in millijoules\"\"\"\n",
    "        total_energy = 0.0\n",
    "        \n",
    "        for layer in architecture['layers']:\n",
    "            layer_type = layer['type']\n",
    "            channels = layer['channels']\n",
    "            \n",
    "            # FLOPS-based energy calculation\n",
    "            flops_giga = (channels * channels * 32 * 32) / 1e9\n",
    "            layer_energy = flops_giga * self.operation_energy[layer_type]\n",
    "            \n",
    "            # Channel scaling\n",
    "            channel_factor = (channels / 64.0) ** 1.2\n",
    "            layer_energy *= channel_factor\n",
    "            \n",
    "            total_energy += layer_energy\n",
    "        \n",
    "        # Memory access energy\n",
    "        memory_mb = self.predict_memory(architecture)\n",
    "        memory_energy = memory_mb * 0.5  # 0.5 mJ per MB access\n",
    "        \n",
    "        return total_energy + memory_energy\n",
    "    \n",
    "    def predict_accuracy(self, architecture: Dict[str, Any]) -> float:\n",
    "        \"\"\"Simplified accuracy prediction (in practice, use surrogate models)\"\"\"\n",
    "        # Simplified heuristic: deeper networks with more channels are generally more accurate\n",
    "        depth_score = min(architecture['depth'] / 20.0, 1.0)\n",
    "        \n",
    "        avg_channels = np.mean([layer['channels'] for layer in architecture['layers']])\n",
    "        channel_score = min(avg_channels / 128.0, 1.0)\n",
    "        \n",
    "        # Layer type diversity bonus\n",
    "        layer_types = set(layer['type'] for layer in architecture['layers'])\n",
    "        diversity_score = len(layer_types) / len(search_space.layer_types)\n",
    "        \n",
    "        # Combined score (scaled to typical accuracy range)\n",
    "        base_accuracy = 0.6 + 0.35 * (0.4 * depth_score + 0.4 * channel_score + 0.2 * diversity_score)\n",
    "        \n",
    "        # Add some noise to simulate real accuracy variation\n",
    "        noise = np.random.normal(0, 0.02)\n",
    "        \n",
    "        return min(max(base_accuracy + noise, 0.5), 0.98)\n",
    "\n",
    "# Initialize performance model\n",
    "perf_model = HardwarePerformanceModel(edge_hardware)\n",
    "\n",
    "print(\"‚úÖ Hardware performance model initialized\")\n",
    "\n",
    "# Test performance prediction\n",
    "sample_arch = search_space.sample_architecture()\n",
    "latency = perf_model.predict_latency(sample_arch)\n",
    "memory = perf_model.predict_memory(sample_arch)\n",
    "energy = perf_model.predict_energy(sample_arch)\n",
    "accuracy = perf_model.predict_accuracy(sample_arch)\n",
    "\n",
    "print(f\"\\nüìä Sample Architecture Performance:\")\n",
    "print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   Latency: {latency:.1f} ms\")\n",
    "print(f\"   Memory: {memory:.1f} MB\")\n",
    "print(f\"   Energy: {energy:.1f} mJ\")\n",
    "print(f\"   Meets latency constraint: {'‚úÖ' if latency <= edge_hardware.max_latency_ms else '‚ùå'}\")\n",
    "print(f\"   Meets memory constraint: {'‚úÖ' if memory <= edge_hardware.memory_mb else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Multi-Objective Optimization Algorithm\n",
    "\n",
    "**Paper Reference**: *\"Multi-objective NAS methods like NSGA-II and Pareto-efficient solutions enable finding optimal trade-offs between accuracy and deployment constraints.\"*\n",
    "\n",
    "### NSGA-II Implementation for Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiObjectiveNAS:\n",
    "    \"\"\"Multi-objective Neural Architecture Search using evolutionary algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self, search_space: ArchitectureSearchSpace, \n",
    "                 perf_model: HardwarePerformanceModel,\n",
    "                 population_size: int = 50,\n",
    "                 generations: int = 20):\n",
    "        self.search_space = search_space\n",
    "        self.perf_model = perf_model\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        \n",
    "        # Evolution history\n",
    "        self.history = {\n",
    "            'generations': [],\n",
    "            'pareto_fronts': [],\n",
    "            'best_solutions': []\n",
    "        }\n",
    "    \n",
    "    def evaluate_architecture(self, architecture: Dict[str, Any]) -> Tuple[float, float, float, float]:\n",
    "        \"\"\"Evaluate architecture on all objectives\"\"\"\n",
    "        accuracy = self.perf_model.predict_accuracy(architecture)\n",
    "        latency = self.perf_model.predict_latency(architecture)\n",
    "        memory = self.perf_model.predict_memory(architecture)\n",
    "        energy = self.perf_model.predict_energy(architecture)\n",
    "        \n",
    "        # Convert to minimization problem (for accuracy: minimize error)\n",
    "        error = 1.0 - accuracy\n",
    "        \n",
    "        return error, latency, memory, energy\n",
    "    \n",
    "    def dominates(self, obj1: Tuple[float, ...], obj2: Tuple[float, ...]) -> bool:\n",
    "        \"\"\"Check if obj1 dominates obj2 (Pareto dominance)\"\"\"\n",
    "        better_in_all = all(o1 <= o2 for o1, o2 in zip(obj1, obj2))\n",
    "        better_in_one = any(o1 < o2 for o1, o2 in zip(obj1, obj2))\n",
    "        return better_in_all and better_in_one\n",
    "    \n",
    "    def find_pareto_front(self, population: List[Dict], objectives: List[Tuple]) -> List[int]:\n",
    "        \"\"\"Find Pareto-optimal solutions\"\"\"\n",
    "        pareto_front = []\n",
    "        \n",
    "        for i, obj_i in enumerate(objectives):\n",
    "            is_dominated = False\n",
    "            for j, obj_j in enumerate(objectives):\n",
    "                if i != j and self.dominates(obj_j, obj_i):\n",
    "                    is_dominated = True\n",
    "                    break\n",
    "            \n",
    "            if not is_dominated:\n",
    "                pareto_front.append(i)\n",
    "        \n",
    "        return pareto_front\n",
    "    \n",
    "    def crowding_distance(self, objectives: List[Tuple], front_indices: List[int]) -> List[float]:\n",
    "        \"\"\"Calculate crowding distance for diversity preservation\"\"\"\n",
    "        if len(front_indices) <= 2:\n",
    "            return [float('inf')] * len(front_indices)\n",
    "        \n",
    "        distances = [0.0] * len(front_indices)\n",
    "        n_objectives = len(objectives[0])\n",
    "        \n",
    "        for obj_idx in range(n_objectives):\n",
    "            # Sort by objective value\n",
    "            sorted_indices = sorted(front_indices, key=lambda i: objectives[i][obj_idx])\n",
    "            \n",
    "            # Boundary points get infinite distance\n",
    "            obj_values = [objectives[i][obj_idx] for i in sorted_indices]\n",
    "            obj_range = max(obj_values) - min(obj_values)\n",
    "            \n",
    "            if obj_range > 0:\n",
    "                for i in range(1, len(sorted_indices) - 1):\n",
    "                    idx_in_front = front_indices.index(sorted_indices[i])\n",
    "                    distance_contribution = (obj_values[i+1] - obj_values[i-1]) / obj_range\n",
    "                    distances[idx_in_front] += distance_contribution\n",
    "        \n",
    "        # Set boundary distances to infinity\n",
    "        if len(front_indices) > 2:\n",
    "            distances[0] = float('inf')\n",
    "            distances[-1] = float('inf')\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def tournament_selection(self, population: List[Dict], objectives: List[Tuple], \n",
    "                           fronts: List[List[int]], distances: List[float]) -> Dict:\n",
    "        \"\"\"Tournament selection based on Pareto rank and crowding distance\"\"\"\n",
    "        # Select two random individuals\n",
    "        idx1, idx2 = random.sample(range(len(population)), 2)\n",
    "        \n",
    "        # Find their fronts\n",
    "        front1 = next(i for i, front in enumerate(fronts) if idx1 in front)\n",
    "        front2 = next(i for i, front in enumerate(fronts) if idx2 in front)\n",
    "        \n",
    "        # Select based on Pareto rank (lower is better)\n",
    "        if front1 < front2:\n",
    "            return population[idx1]\n",
    "        elif front2 < front1:\n",
    "            return population[idx2]\n",
    "        else:\n",
    "            # Same front, select based on crowding distance (higher is better)\n",
    "            if distances[idx1] > distances[idx2]:\n",
    "                return population[idx1]\n",
    "            else:\n",
    "                return population[idx2]\n",
    "    \n",
    "    def mutate_architecture(self, architecture: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Mutate architecture with small random changes\"\"\"\n",
    "        mutated = {\n",
    "            'layers': [layer.copy() for layer in architecture['layers']],\n",
    "            'depth': architecture['depth'],\n",
    "            'global_pool': architecture['global_pool'],\n",
    "            'dropout_rate': architecture['dropout_rate']\n",
    "        }\n",
    "        \n",
    "        # Random mutations\n",
    "        if random.random() < 0.3:  # Change layer type\n",
    "            if mutated['layers']:\n",
    "                layer_idx = random.randint(0, len(mutated['layers']) - 1)\n",
    "                mutated['layers'][layer_idx]['type'] = random.choice(list(self.search_space.layer_types.keys()))\n",
    "        \n",
    "        if random.random() < 0.3:  # Change channel width\n",
    "            if mutated['layers']:\n",
    "                layer_idx = random.randint(0, len(mutated['layers']) - 1)\n",
    "                mutated['layers'][layer_idx]['channels'] = random.choice(self.search_space.channel_widths)\n",
    "        \n",
    "        if random.random() < 0.2:  # Change activation\n",
    "            if mutated['layers']:\n",
    "                layer_idx = random.randint(0, len(mutated['layers']) - 1)\n",
    "                mutated['layers'][layer_idx]['activation'] = random.choice(self.search_space.activations)\n",
    "        \n",
    "        if random.random() < 0.1:  # Change global pooling\n",
    "            mutated['global_pool'] = random.choice(['avg', 'max', 'adaptive'])\n",
    "        \n",
    "        if random.random() < 0.1:  # Change dropout\n",
    "            mutated['dropout_rate'] = max(0.0, min(0.5, mutated['dropout_rate'] + random.gauss(0, 0.1)))\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def search(self) -> Tuple[List[Dict], List[Tuple]]:\n",
    "        \"\"\"Run multi-objective neural architecture search\"\"\"\n",
    "        print(f\"üîç Starting Multi-Objective NAS...\")\n",
    "        print(f\"   Population size: {self.population_size}\")\n",
    "        print(f\"   Generations: {self.generations}\")\n",
    "        \n",
    "        # Initialize population\n",
    "        population = [self.search_space.sample_architecture() for _ in range(self.population_size)]\n",
    "        \n",
    "        best_pareto_front = []\n",
    "        best_objectives = []\n",
    "        \n",
    "        for generation in range(self.generations):\n",
    "            print(f\"\\nüìä Generation {generation + 1}/{self.generations}\")\n",
    "            \n",
    "            # Evaluate all architectures\n",
    "            objectives = [self.evaluate_architecture(arch) for arch in population]\n",
    "            \n",
    "            # Find Pareto fronts\n",
    "            all_fronts = []\n",
    "            remaining_indices = list(range(len(population)))\n",
    "            \n",
    "            while remaining_indices:\n",
    "                remaining_objectives = [objectives[i] for i in remaining_indices]\n",
    "                front_in_remaining = self.find_pareto_front(population, remaining_objectives)\n",
    "                front_indices = [remaining_indices[i] for i in front_in_remaining]\n",
    "                all_fronts.append(front_indices)\n",
    "                remaining_indices = [i for i in remaining_indices if i not in front_indices]\n",
    "            \n",
    "            # Calculate crowding distances\n",
    "            all_distances = [0.0] * len(population)\n",
    "            for front in all_fronts:\n",
    "                distances = self.crowding_distance(objectives, front)\n",
    "                for i, idx in enumerate(front):\n",
    "                    all_distances[idx] = distances[i]\n",
    "            \n",
    "            # Store best solutions from first front\n",
    "            if all_fronts:\n",
    "                current_pareto_front = [population[i] for i in all_fronts[0]]\n",
    "                current_pareto_objectives = [objectives[i] for i in all_fronts[0]]\n",
    "                \n",
    "                if not best_pareto_front or len(current_pareto_front) > len(best_pareto_front):\n",
    "                    best_pareto_front = current_pareto_front\n",
    "                    best_objectives = current_pareto_objectives\n",
    "                \n",
    "                # Print progress\n",
    "                best_accuracy = 1 - min(obj[0] for obj in current_pareto_objectives)\n",
    "                best_latency = min(obj[1] for obj in current_pareto_objectives)\n",
    "                print(f\"   Pareto front size: {len(current_pareto_front)}\")\n",
    "                print(f\"   Best accuracy: {best_accuracy:.3f}\")\n",
    "                print(f\"   Best latency: {best_latency:.1f} ms\")\n",
    "            \n",
    "            # Generate next generation\n",
    "            if generation < self.generations - 1:\n",
    "                new_population = []\n",
    "                \n",
    "                while len(new_population) < self.population_size:\n",
    "                    # Selection\n",
    "                    parent = self.tournament_selection(population, objectives, all_fronts, all_distances)\n",
    "                    \n",
    "                    # Mutation\n",
    "                    child = self.mutate_architecture(parent)\n",
    "                    new_population.append(child)\n",
    "                \n",
    "                population = new_population\n",
    "        \n",
    "        print(f\"\\n‚úÖ Multi-Objective NAS completed!\")\n",
    "        print(f\"   Final Pareto front size: {len(best_pareto_front)}\")\n",
    "        \n",
    "        return best_pareto_front, best_objectives\n",
    "\n",
    "# Initialize and run NAS\n",
    "nas_optimizer = MultiObjectiveNAS(\n",
    "    search_space=search_space,\n",
    "    perf_model=perf_model,\n",
    "    population_size=30,  # Smaller for demonstration\n",
    "    generations=10\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-Objective NAS optimizer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Neural Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the search process\n",
    "print(\"üéØ Starting Neural Architecture Search for Edge AI...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pareto_architectures, pareto_objectives = nas_optimizer.search()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ PARETO-OPTIMAL ARCHITECTURES FOUND\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze results\n",
    "for i, (arch, obj) in enumerate(zip(pareto_architectures, pareto_objectives)):\n",
    "    error, latency, memory, energy = obj\n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è Architecture {i+1}:\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   Latency: {latency:.1f} ms\")\n",
    "    print(f\"   Memory: {memory:.1f} MB\")\n",
    "    print(f\"   Energy: {energy:.1f} mJ\")\n",
    "    print(f\"   Depth: {arch['depth']} layers\")\n",
    "    print(f\"   Avg channels: {np.mean([l['channels'] for l in arch['layers']]):.0f}\")\n",
    "    \n",
    "    # Check constraints\n",
    "    meets_latency = latency <= edge_hardware.max_latency_ms\n",
    "    meets_memory = memory <= edge_hardware.memory_mb\n",
    "    meets_energy = energy <= edge_hardware.power_budget_mw\n",
    "    \n",
    "    constraints_met = sum([meets_latency, meets_memory, meets_energy])\n",
    "    print(f\"   Constraints met: {constraints_met}/3 {'‚úÖ' if constraints_met == 3 else '‚ö†Ô∏è'}\")\n",
    "\n",
    "print(f\"\\nüìä Search completed with {len(pareto_architectures)} Pareto-optimal solutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Pareto Front Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for visualization\n",
    "accuracies = [1 - obj[0] for obj in pareto_objectives]\n",
    "latencies = [obj[1] for obj in pareto_objectives]\n",
    "memories = [obj[2] for obj in pareto_objectives]\n",
    "energies = [obj[3] for obj in pareto_objectives]\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Multi-Objective NAS: Pareto-Optimal Architecture Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Accuracy vs Latency Trade-off\n",
    "scatter1 = ax1.scatter(latencies, accuracies, c=memories, cmap='viridis', s=100, alpha=0.7)\n",
    "ax1.set_xlabel('Inference Latency (ms)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy vs Latency Trade-off\\n(Color = Memory Usage)')\n",
    "ax1.axvline(x=edge_hardware.max_latency_ms, color='red', linestyle='--', alpha=0.7, label='Latency Limit')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "plt.colorbar(scatter1, ax=ax1, label='Memory (MB)')\n",
    "\n",
    "# 2. Memory vs Energy Trade-off\n",
    "scatter2 = ax2.scatter(memories, energies, c=accuracies, cmap='plasma', s=100, alpha=0.7)\n",
    "ax2.set_xlabel('Memory Usage (MB)')\n",
    "ax2.set_ylabel('Energy Consumption (mJ)')\n",
    "ax2.set_title('Memory vs Energy Trade-off\\n(Color = Accuracy)')\n",
    "ax2.axvline(x=edge_hardware.memory_mb, color='red', linestyle='--', alpha=0.7, label='Memory Limit')\n",
    "ax2.axhline(y=edge_hardware.power_budget_mw, color='red', linestyle='--', alpha=0.7, label='Energy Limit')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "plt.colorbar(scatter2, ax=ax2, label='Accuracy')\n",
    "\n",
    "# 3. Architecture Complexity Analysis\n",
    "depths = [arch['depth'] for arch in pareto_architectures]\n",
    "avg_channels = [np.mean([l['channels'] for l in arch['layers']]) for arch in pareto_architectures]\n",
    "\n",
    "scatter3 = ax3.scatter(depths, avg_channels, c=accuracies, cmap='coolwarm', s=100, alpha=0.7)\n",
    "ax3.set_xlabel('Network Depth (layers)')\n",
    "ax3.set_ylabel('Average Channel Width')\n",
    "ax3.set_title('Architecture Complexity vs Performance\\n(Color = Accuracy)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter3, ax=ax3, label='Accuracy')\n",
    "\n",
    "# 4. Pareto Front 3D Projection\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax4.remove()\n",
    "ax4 = fig.add_subplot(224, projection='3d')\n",
    "\n",
    "scatter4 = ax4.scatter(latencies, memories, accuracies, c=energies, cmap='hot', s=60, alpha=0.8)\n",
    "ax4.set_xlabel('Latency (ms)')\n",
    "ax4.set_ylabel('Memory (MB)')\n",
    "ax4.set_zlabel('Accuracy')\n",
    "ax4.set_title('3D Pareto Front\\n(Color = Energy)')\n",
    "plt.colorbar(scatter4, ax=ax4, label='Energy (mJ)', shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Pareto front visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Hardware-Specific Architecture Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pareto_solutions(architectures: List[Dict], objectives: List[Tuple], \n",
    "                           hardware_spec: EdgeHardwareSpec) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze Pareto solutions and provide hardware-specific recommendations\"\"\"\n",
    "    \n",
    "    analysis = {\n",
    "        'feasible_solutions': [],\n",
    "        'best_accuracy': None,\n",
    "        'best_efficiency': None,\n",
    "        'best_balanced': None,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Find feasible solutions (meet all constraints)\n",
    "    for i, (arch, obj) in enumerate(zip(architectures, objectives)):\n",
    "        error, latency, memory, energy = obj\n",
    "        accuracy = 1 - error\n",
    "        \n",
    "        meets_latency = latency <= hardware_spec.max_latency_ms\n",
    "        meets_memory = memory <= hardware_spec.memory_mb\n",
    "        meets_energy = energy <= hardware_spec.power_budget_mw\n",
    "        \n",
    "        if meets_latency and meets_memory and meets_energy:\n",
    "            analysis['feasible_solutions'].append({\n",
    "                'index': i,\n",
    "                'architecture': arch,\n",
    "                'metrics': {\n",
    "                    'accuracy': accuracy,\n",
    "                    'latency': latency,\n",
    "                    'memory': memory,\n",
    "                    'energy': energy\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    if not analysis['feasible_solutions']:\n",
    "        analysis['recommendations'].append(\"‚ö†Ô∏è No architectures found that meet all hardware constraints\")\n",
    "        analysis['recommendations'].append(\"üí° Consider relaxing constraints or using more aggressive optimization\")\n",
    "        return analysis\n",
    "    \n",
    "    # Find best solutions by different criteria\n",
    "    feasible = analysis['feasible_solutions']\n",
    "    \n",
    "    # Best accuracy\n",
    "    best_acc_idx = max(range(len(feasible)), key=lambda i: feasible[i]['metrics']['accuracy'])\n",
    "    analysis['best_accuracy'] = feasible[best_acc_idx]\n",
    "    \n",
    "    # Best efficiency (lowest latency + memory + energy, normalized)\n",
    "    def efficiency_score(sol):\n",
    "        m = sol['metrics']\n",
    "        norm_latency = m['latency'] / hardware_spec.max_latency_ms\n",
    "        norm_memory = m['memory'] / hardware_spec.memory_mb\n",
    "        norm_energy = m['energy'] / hardware_spec.power_budget_mw\n",
    "        return norm_latency + norm_memory + norm_energy\n",
    "    \n",
    "    best_eff_idx = min(range(len(feasible)), key=lambda i: efficiency_score(feasible[i]))\n",
    "    analysis['best_efficiency'] = feasible[best_eff_idx]\n",
    "    \n",
    "    # Best balanced (highest accuracy * efficiency score)\n",
    "    def balanced_score(sol):\n",
    "        return sol['metrics']['accuracy'] / (1 + efficiency_score(sol))\n",
    "    \n",
    "    best_bal_idx = max(range(len(feasible)), key=lambda i: balanced_score(feasible[i]))\n",
    "    analysis['best_balanced'] = feasible[best_bal_idx]\n",
    "    \n",
    "    # Generate recommendations\n",
    "    analysis['recommendations'].append(f\"‚úÖ Found {len(feasible)} feasible architectures\")\n",
    "    \n",
    "    if len(feasible) >= 3:\n",
    "        analysis['recommendations'].append(\"üéØ Multiple viable options available - consider use case priorities\")\n",
    "    \n",
    "    # Hardware-specific recommendations\n",
    "    if hardware_spec.has_npu:\n",
    "        analysis['recommendations'].append(\"‚ö° Hardware has NPU - prioritize depthwise and separable convolutions\")\n",
    "    \n",
    "    if hardware_spec.memory_mb < 1024:\n",
    "        analysis['recommendations'].append(\"üíæ Limited memory - consider more aggressive channel pruning\")\n",
    "    \n",
    "    if hardware_spec.max_latency_ms < 50:\n",
    "        analysis['recommendations'].append(\"‚è±Ô∏è Strict latency requirements - focus on efficient operations\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze the Pareto solutions\n",
    "analysis = analyze_pareto_solutions(pareto_architectures, pareto_objectives, edge_hardware)\n",
    "\n",
    "print(\"üéØ HARDWARE-SPECIFIC ARCHITECTURE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Print recommendations\n",
    "print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "for rec in analysis['recommendations']:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "# Print best solutions\n",
    "if analysis['feasible_solutions']:\n",
    "    print(f\"\\nüèÜ TOP ARCHITECTURE RECOMMENDATIONS:\")\n",
    "    \n",
    "    categories = [\n",
    "        ('Best Accuracy', analysis['best_accuracy']),\n",
    "        ('Best Efficiency', analysis['best_efficiency']),\n",
    "        ('Best Balanced', analysis['best_balanced'])\n",
    "    ]\n",
    "    \n",
    "    for category, solution in categories:\n",
    "        if solution:\n",
    "            arch = solution['architecture']\n",
    "            metrics = solution['metrics']\n",
    "            \n",
    "            print(f\"\\nüîπ {category}:\")\n",
    "            print(f\"   Accuracy: {metrics['accuracy']:.3f}\")\n",
    "            print(f\"   Latency: {metrics['latency']:.1f} ms\")\n",
    "            print(f\"   Memory: {metrics['memory']:.1f} MB\")\n",
    "            print(f\"   Energy: {metrics['energy']:.1f} mJ\")\n",
    "            print(f\"   Architecture: {arch['depth']} layers, avg {np.mean([l['channels'] for l in arch['layers']]):.0f} channels\")\n",
    "            \n",
    "            # Layer type distribution\n",
    "            layer_types = [l['type'] for l in arch['layers']]\n",
    "            type_counts = {t: layer_types.count(t) for t in set(layer_types)}\n",
    "            print(f\"   Layer distribution: {type_counts}\")\n",
    "\n",
    "print(f\"\\nüìä Analysis complete - {len(analysis['feasible_solutions'])} feasible solutions identified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Advanced Research Extensions\n",
    "\n",
    "### Research Questions for Further Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNASResearch:\n",
    "    \"\"\"Advanced research extensions for Multi-Objective NAS\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.research_questions = [\n",
    "            {\n",
    "                'title': 'Dynamic Architecture Adaptation',\n",
    "                'description': 'Architectures that adapt their complexity based on input difficulty or available resources',\n",
    "                'implementation_hint': 'Use early exit strategies or dynamic channel selection',\n",
    "                'paper_reference': 'Dynamic pruning techniques (O3BNN-R, FuPruner)'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Hardware-Software Co-Design',\n",
    "                'description': 'Jointly optimize neural architecture and hardware configuration',\n",
    "                'implementation_hint': 'Include hardware parameters (clock speed, memory bandwidth) in search space',\n",
    "                'paper_reference': 'Hardware-software co-design approaches (Sun et al.)'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Multi-Task Architecture Search',\n",
    "                'description': 'Find architectures that excel at multiple related tasks simultaneously',\n",
    "                'implementation_hint': 'Use shared backbone with task-specific heads',\n",
    "                'paper_reference': 'Multi-objective NAS for multiple tasks'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Uncertainty-Aware NAS',\n",
    "                'description': 'Account for prediction uncertainty in performance models',\n",
    "                'implementation_hint': 'Use Bayesian optimization or ensemble methods',\n",
    "                'paper_reference': 'Robust optimization under uncertainty'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def propose_experiment(self, research_idx: int) -> Dict[str, str]:\n",
    "        \"\"\"Propose specific experiment for research question\"\"\"\n",
    "        if research_idx >= len(self.research_questions):\n",
    "            raise ValueError(\"Invalid research index\")\n",
    "        \n",
    "        question = self.research_questions[research_idx]\n",
    "        \n",
    "        experiment = {\n",
    "            'title': question['title'],\n",
    "            'objective': question['description'],\n",
    "            'methodology': self._generate_methodology(question),\n",
    "            'expected_outcome': self._generate_expected_outcome(question),\n",
    "            'evaluation_metrics': self._generate_evaluation_metrics(question)\n",
    "        }\n",
    "        \n",
    "        return experiment\n",
    "    \n",
    "    def _generate_methodology(self, question: Dict) -> str:\n",
    "        \"\"\"Generate methodology based on research question\"\"\"\n",
    "        methodologies = {\n",
    "            'Dynamic Architecture Adaptation': '''\n",
    "            1. Implement early exit mechanisms in architecture search space\n",
    "            2. Add adaptive channel selection based on input complexity\n",
    "            3. Train policies to decide when to use simplified vs full architecture\n",
    "            4. Evaluate on datasets with varying complexity levels\n",
    "            ''',\n",
    "            'Hardware-Software Co-Design': '''\n",
    "            1. Extend search space to include hardware parameters\n",
    "            2. Model hardware performance with configurable parameters\n",
    "            3. Use multi-level optimization (architecture + hardware)\n",
    "            4. Validate on actual reconfigurable hardware platforms\n",
    "            ''',\n",
    "            'Multi-Task Architecture Search': '''\n",
    "            1. Define multiple related tasks (e.g., classification + detection)\n",
    "            2. Design shared backbone with task-specific heads\n",
    "            3. Use multi-task loss functions in architecture evaluation\n",
    "            4. Compare with single-task specialized architectures\n",
    "            ''',\n",
    "            'Uncertainty-Aware NAS': '''\n",
    "            1. Model uncertainty in performance predictions\n",
    "            2. Use Bayesian optimization for architecture search\n",
    "            3. Include confidence intervals in Pareto optimization\n",
    "            4. Validate robustness across different deployment scenarios\n",
    "            '''\n",
    "        }\n",
    "        \n",
    "        return methodologies.get(question['title'], 'Methodology to be defined')\n",
    "    \n",
    "    def _generate_expected_outcome(self, question: Dict) -> str:\n",
    "        \"\"\"Generate expected outcomes\"\"\"\n",
    "        outcomes = {\n",
    "            'Dynamic Architecture Adaptation': 'Architectures that maintain accuracy while reducing average computational cost by 30-50%',\n",
    "            'Hardware-Software Co-Design': 'Joint optimization yielding 2-3x better efficiency than architecture-only optimization',\n",
    "            'Multi-Task Architecture Search': 'Single architecture achieving 95%+ performance of specialized models on each task',\n",
    "            'Uncertainty-Aware NAS': 'More robust architectures with better worst-case performance guarantees'\n",
    "        }\n",
    "        \n",
    "        return outcomes.get(question['title'], 'Outcomes to be defined')\n",
    "    \n",
    "    def _generate_evaluation_metrics(self, question: Dict) -> List[str]:\n",
    "        \"\"\"Generate evaluation metrics\"\"\"\n",
    "        metrics = {\n",
    "            'Dynamic Architecture Adaptation': [\n",
    "                'Average computational cost reduction',\n",
    "                'Accuracy retention across complexity levels',\n",
    "                'Adaptation policy effectiveness'\n",
    "            ],\n",
    "            'Hardware-Software Co-Design': [\n",
    "                'Joint optimization improvement over sequential',\n",
    "                'Hardware utilization efficiency',\n",
    "                'Cost-performance trade-offs'\n",
    "            ],\n",
    "            'Multi-Task Architecture Search': [\n",
    "                'Per-task performance vs specialized models',\n",
    "                'Parameter sharing efficiency',\n",
    "                'Task interference analysis'\n",
    "            ],\n",
    "            'Uncertainty-Aware NAS': [\n",
    "                'Prediction confidence calibration',\n",
    "                'Worst-case performance bounds',\n",
    "                'Robustness across deployment scenarios'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return metrics.get(question['title'], ['Metrics to be defined'])\n",
    "\n",
    "# Initialize research framework\n",
    "research = AdvancedNASResearch()\n",
    "\n",
    "print(\"üî¨ ADVANCED RESEARCH EXTENSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(research.research_questions):\n",
    "    print(f\"\\n{i+1}. {question['title']}\")\n",
    "    print(f\"   üìù {question['description']}\")\n",
    "    print(f\"   üí° Implementation: {question['implementation_hint']}\")\n",
    "    print(f\"   üìö Paper ref: {question['paper_reference']}\")\n",
    "\n",
    "# Generate example experiment proposal\n",
    "experiment = research.propose_experiment(0)  # Dynamic Architecture Adaptation\n",
    "\n",
    "print(f\"\\n\\nüß™ EXAMPLE EXPERIMENT PROPOSAL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Title: {experiment['title']}\")\n",
    "print(f\"\\nObjective: {experiment['objective']}\")\n",
    "print(f\"\\nMethodology: {experiment['methodology']}\")\n",
    "print(f\"Expected Outcome: {experiment['expected_outcome']}\")\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "for metric in experiment['evaluation_metrics']:\n",
    "    print(f\"   - {metric}\")\n",
    "\n",
    "print(\"\\n‚úÖ Research extensions defined - ready for advanced implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Key Takeaways & Summary\n",
    "\n",
    "### üéØ Concepts Mastered:\n",
    "\n",
    "1. **Multi-Objective Optimization**: Successfully implemented NSGA-II algorithm for balancing accuracy, latency, memory, and energy\n",
    "\n",
    "2. **Hardware-Aware Search**: Developed performance prediction models that account for edge device constraints\n",
    "\n",
    "3. **Pareto Optimization**: Found multiple optimal trade-off solutions rather than single \"best\" architecture\n",
    "\n",
    "4. **Architecture Encoding**: Created flexible search space representation for neural architectures\n",
    "\n",
    "5. **Evolutionary Algorithms**: Applied tournament selection, mutation, and crowding distance for diversity\n",
    "\n",
    "### üìä Paper Implementation Results:\n",
    "\n",
    "- **Search Space**: 6 layer types √ó 10 channel widths √ó 9 depth options = 540+ architecture combinations\n",
    "- **Constraints**: Successfully modeled real edge device limitations (latency, memory, energy)\n",
    "- **Optimization**: Found Pareto-optimal solutions balancing 4 competing objectives\n",
    "- **Hardware Integration**: Performance models account for CPU cores, NPU availability, and memory limits\n",
    "\n",
    "### üî¨ Research Extensions:\n",
    "\n",
    "1. **Dynamic Adaptation**: Runtime architecture complexity adjustment\n",
    "2. **Co-Design**: Joint hardware-software optimization\n",
    "3. **Multi-Task**: Single architecture for multiple related tasks\n",
    "4. **Uncertainty**: Robust optimization under prediction uncertainty\n",
    "\n",
    "### üéì Learning Outcomes:\n",
    "\n",
    "This notebook demonstrated the sophisticated intersection of:\n",
    "- **Evolutionary Computation** (NSGA-II, Pareto optimization)\n",
    "- **Hardware Modeling** (performance prediction, constraint satisfaction)\n",
    "- **Neural Architecture Design** (flexible search spaces, encoding schemes)\n",
    "- **Multi-Objective Decision Making** (trade-off analysis, solution selection)\n",
    "\n",
    "---\n",
    "\n",
    "**üìÑ Paper Citation**: Wang, X., & Jia, W. (2025). *Optimizing Edge AI: A Comprehensive Survey on Data, Model, and System Strategies*. arXiv:2501.03265v1. **Sections 11-13**: Neural Architecture Search for Edge Deployment.\n",
    "\n",
    "**üîó Next**: Continue with **Focused Learning Notebook 2: Knowledge Distillation** to explore teacher-student model compression techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}