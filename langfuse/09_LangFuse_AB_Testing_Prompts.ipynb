{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09_LangFuse_AB_Testing_Prompts\n",
    "\n",
    "## M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "\n",
    "Hi·ªÉu c√°ch thi·∫øt l·∫≠p, ch·∫°y v√† ph√¢n t√≠ch c√°c th·ª≠ nghi·ªám A/B cho prompt b·∫±ng LangFuse ƒë·ªÉ t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t LLM.\n",
    "\n",
    "## T·∫°i sao A/B Testing cho Prompts quan tr·ªçng?\n",
    "\n",
    "- **T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t**: So s√°nh c√°c phi√™n b·∫£n prompt kh√°c nhau ƒë·ªÉ t√¨m ra c√°ch vi·∫øt t·ªët nh·∫•t\n",
    "- **Ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n d·ªØ li·ªáu**: S·ª≠ d·ª•ng metrics th·ª±c t·∫ø thay v√¨ c·∫£m t√≠nh\n",
    "- **C·∫£i thi·ªán li√™n t·ª•c**: Th·ª≠ nghi·ªám v√† c·∫£i ti·∫øn prompt theo th·ªùi gian\n",
    "- **Gi·∫£m bias**: Tr√°nh vi·ªác ch·ªçn prompt d·ª±a tr√™n √Ω ki·∫øn ch·ªß quan\n",
    "\n",
    "## C√°ch LangFuse h·ªó tr·ª£ A/B Testing\n",
    "\n",
    "LangFuse cung c·∫•p:\n",
    "- **Tagging system**: G·∫Øn th·∫ª c√°c trace theo phi√™n b·∫£n prompt\n",
    "- **Metrics tracking**: Theo d√µi latency, token usage, cost\n",
    "- **Dashboard ph√¢n t√≠ch**: So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c phi√™n b·∫£n\n",
    "- **Evaluation system**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng output t·ª± ƒë·ªông v√† th·ªß c√¥ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C√†i ƒë·∫∑t & C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "# Ki·ªÉm tra API keys\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable is required\")\n",
    "\n",
    "if not os.getenv(\"LANGFUSE_SECRET_KEY\") or not os.getenv(\"LANGFUSE_PUBLIC_KEY\"):\n",
    "    raise ValueError(\"LANGFUSE_SECRET_KEY and LANGFUSE_PUBLIC_KEY environment variables are required\")\n",
    "\n",
    "print(\"‚úÖ Environment variables configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kh·ªüi t·∫°o LangFuse v√† LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o LangFuse\n",
    "langfuse = Langfuse()\n",
    "\n",
    "# Kh·ªüi t·∫°o LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LangFuse v√† ChatAnthropic initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ƒê·ªãnh nghƒ©a Use Case: A/B Testing cho Prompt T√≥m t·∫Øt VƒÉn b·∫£n\n",
    "\n",
    "Ch√∫ng ta s·∫Ω th·ª≠ nghi·ªám hai phi√™n b·∫£n prompt kh√°c nhau cho vi·ªác t√≥m t·∫Øt vƒÉn b·∫£n:\n",
    "- **Prompt A**: Phong c√°ch tr·ª±c ti·∫øp, ng·∫Øn g·ªçn\n",
    "- **Prompt B**: Phong c√°ch c√≥ c·∫•u tr√∫c, chi ti·∫øt h∆°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a hai phi√™n b·∫£n prompt\n",
    "PROMPT_A = \"\"\"\n",
    "H√£y t√≥m t·∫Øt vƒÉn b·∫£n sau trong 2-3 c√¢u ng·∫Øn g·ªçn:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_B = \"\"\"\n",
    "B·∫°n l√† m·ªôt chuy√™n gia t√≥m t·∫Øt vƒÉn b·∫£n. H√£y ph√¢n t√≠ch v√† t√≥m t·∫Øt vƒÉn b·∫£n sau theo c·∫•u tr√∫c:\n",
    "\n",
    "**Ch·ªß ƒë·ªÅ ch√≠nh:** [1 c√¢u]\n",
    "**ƒêi·ªÉm quan tr·ªçng:** [2-3 ƒëi·ªÉm ch√≠nh]\n",
    "**K·∫øt lu·∫≠n:** [1 c√¢u]\n",
    "\n",
    "VƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# T·∫°o prompt templates\n",
    "template_a = ChatPromptTemplate.from_template(PROMPT_A)\n",
    "template_b = ChatPromptTemplate.from_template(PROMPT_B)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a hai phi√™n b·∫£n prompt A v√† B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. D·ªØ li·ªáu Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ªØ li·ªáu m·∫´u ƒë·ªÉ test\n",
    "test_texts = [\n",
    "    \"\"\"\n",
    "Tr√≠ tu·ªá nh√¢n t·∫°o (AI) ƒëang thay ƒë·ªïi c√°ch ch√∫ng ta l√†m vi·ªác v√† s·ªëng. \n",
    "T·ª´ vi·ªác t·ª± ƒë·ªông h√≥a c√°c t√°c v·ª• ƒë∆°n gi·∫£n ƒë·∫øn vi·ªác h·ªó tr·ª£ ra quy·∫øt ƒë·ªãnh ph·ª©c t·∫°p, \n",
    "AI mang l·∫°i nhi·ªÅu l·ª£i √≠ch. Tuy nhi√™n, c≈©ng c√≥ nh·ªØng lo ng·∫°i v·ªÅ vi·ªác m·∫•t vi·ªác l√†m \n",
    "v√† v·∫•n ƒë·ªÅ ƒë·∫°o ƒë·ª©c trong vi·ªác s·ª≠ d·ª•ng AI. ƒê·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa AI, \n",
    "ch√∫ng ta c·∫ßn ph√°t tri·ªÉn k·ªπ nƒÉng ph√π h·ª£p v√† x√¢y d·ª±ng khung ph√°p l√Ω ph√π h·ª£p.\n",
    "    \"\"\".strip(),\n",
    "    \n",
    "    \"\"\"\n",
    "Bi·∫øn ƒë·ªïi kh√≠ h·∫≠u l√† m·ªôt trong nh·ªØng th√°ch th·ª©c l·ªõn nh·∫•t c·ªßa th·∫ø k·ª∑ 21. \n",
    "Nhi·ªát ƒë·ªô to√†n c·∫ßu tƒÉng do ho·∫°t ƒë·ªông c·ªßa con ng∆∞·ªùi, ƒë·∫∑c bi·ªát l√† vi·ªác ƒë·ªët nhi√™n li·ªáu h√≥a th·∫°ch. \n",
    "ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn bƒÉng tan, m·ª±c n∆∞·ªõc bi·ªÉn d√¢ng v√† th·ªùi ti·∫øt c·ª±c ƒëoan. \n",
    "ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y, c·∫ßn c√≥ s·ª± h·ª£p t√°c qu·ªëc t·∫ø, \n",
    "ƒë·∫ßu t∆∞ v√†o nƒÉng l∆∞·ª£ng t√°i t·∫°o v√† thay ƒë·ªïi l·ªëi s·ªëng b·ªÅn v·ªØng.\n",
    "    \"\"\".strip(),\n",
    "    \n",
    "    \"\"\"\n",
    "Gi√°o d·ª•c tr·ª±c tuy·∫øn ƒë√£ tr·ªü n√™n ph·ªï bi·∫øn, ƒë·∫∑c bi·ªát sau ƒë·∫°i d·ªãch COVID-19. \n",
    "N√≥ mang l·∫°i s·ª± linh ho·∫°t v·ªÅ th·ªùi gian v√† ƒë·ªãa ƒëi·ªÉm h·ªçc t·∫≠p, \n",
    "gi√∫p nhi·ªÅu ng∆∞·ªùi ti·∫øp c·∫≠n ƒë∆∞·ª£c ki·∫øn th·ª©c ch·∫•t l∆∞·ª£ng. \n",
    "Tuy nhi√™n, gi√°o d·ª•c tr·ª±c tuy·∫øn c≈©ng c√≥ nh·ªØng h·∫°n ch·∫ø nh∆∞ thi·∫øu t∆∞∆°ng t√°c tr·ª±c ti·∫øp, \n",
    "c·∫ßn k·ª∑ lu·∫≠t t·ª± h·ªçc cao v√† v·∫•n ƒë·ªÅ c√¥ng ngh·ªá. \n",
    "T∆∞∆°ng lai gi√°o d·ª•c c√≥ th·ªÉ l√† s·ª± k·∫øt h·ª£p gi·ªØa tr·ª±c tuy·∫øn v√† tr·ª±c ti·∫øp.\n",
    "    \"\"\".strip()\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Chu·∫©n b·ªã {len(test_texts)} vƒÉn b·∫£n test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tri·ªÉn khai A/B Testing v·ªõi LangFuse Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ab_test_with_tracking(text: str, prompt_version: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ch·∫°y m·ªôt test v·ªõi tracking ƒë·∫ßy ƒë·ªß b·∫±ng LangFuse\n",
    "    \"\"\"\n",
    "    # T·∫°o callback handler v·ªõi tags\n",
    "    callback_handler = CallbackHandler(\n",
    "        tags=[f\"prompt_version_{prompt_version}\", \"ab_testing\", \"summarization\"],\n",
    "        user_id=\"ab_test_user\",\n",
    "        session_id=f\"ab_test_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    \n",
    "    # Ch·ªçn prompt template d·ª±a tr√™n version\n",
    "    if prompt_version == \"A\":\n",
    "        template = template_a\n",
    "    else:\n",
    "        template = template_b\n",
    "    \n",
    "    # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # T·∫°o chain v·ªõi callback\n",
    "        chain = template | llm\n",
    "        \n",
    "        # G·ªçi LLM\n",
    "        result = chain.invoke(\n",
    "            {\"text\": text},\n",
    "            config={\"callbacks\": [callback_handler]}\n",
    "        )\n",
    "        \n",
    "        # T√≠nh th·ªùi gian x·ª≠ l√Ω\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"prompt_version\": prompt_version,\n",
    "            \"input_text\": text,\n",
    "            \"output\": result.content,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"success\": True,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        return {\n",
    "            \"prompt_version\": prompt_version,\n",
    "            \"input_text\": text,\n",
    "            \"output\": None,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a function run_ab_test_with_tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ch·∫°y A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y A/B test cho t·∫•t c·∫£ c√°c vƒÉn b·∫£n\n",
    "results = []\n",
    "\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu ch·∫°y A/B testing...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"\\nüìù Test vƒÉn b·∫£n #{i+1}\")\n",
    "    \n",
    "    # Test v·ªõi c·∫£ hai phi√™n b·∫£n prompt\n",
    "    for version in [\"A\", \"B\"]:\n",
    "        print(f\"  ‚è≥ ƒêang test Prompt {version}...\")\n",
    "        \n",
    "        result = run_ab_test_with_tracking(text, version)\n",
    "        results.append(result)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(f\"  ‚úÖ Prompt {version}: {result['processing_time']:.2f}s\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Prompt {version}: Error - {result['error']}\")\n",
    "        \n",
    "        # T·∫°m d·ª´ng ƒë·ªÉ tr√°nh rate limiting\n",
    "        time.sleep(1)\n",
    "\n",
    "print(f\"\\nüéâ Ho√†n th√†nh A/B testing v·ªõi {len(results)} k·∫øt qu·∫£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ph√¢n t√≠ch K·∫øt qu·∫£ A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "def analyze_ab_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Ph√¢n t√≠ch k·∫øt qu·∫£ A/B testing\n",
    "    \"\"\"\n",
    "    # Ph√¢n chia k·∫øt qu·∫£ theo prompt version\n",
    "    results_a = [r for r in results if r[\"prompt_version\"] == \"A\" and r[\"success\"]]\n",
    "    results_b = [r for r in results if r[\"prompt_version\"] == \"B\" and r[\"success\"]]\n",
    "    \n",
    "    print(\"üìä PH√ÇN T√çCH K·∫æT QU·∫¢ A/B TESTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Th·ªëng k√™ c∆° b·∫£n\n",
    "    print(f\"\\nüìà Th·ªëng k√™ c∆° b·∫£n:\")\n",
    "    print(f\"  ‚Ä¢ Prompt A: {len(results_a)} tests th√†nh c√¥ng\")\n",
    "    print(f\"  ‚Ä¢ Prompt B: {len(results_b)} tests th√†nh c√¥ng\")\n",
    "    \n",
    "    if results_a and results_b:\n",
    "        # So s√°nh th·ªùi gian x·ª≠ l√Ω\n",
    "        avg_time_a = sum(r[\"processing_time\"] for r in results_a) / len(results_a)\n",
    "        avg_time_b = sum(r[\"processing_time\"] for r in results_b) / len(results_b)\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh:\")\n",
    "        print(f\"  ‚Ä¢ Prompt A: {avg_time_a:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Prompt B: {avg_time_b:.2f}s\")\n",
    "        \n",
    "        if avg_time_a < avg_time_b:\n",
    "            print(f\"  üèÜ Prompt A nhanh h∆°n {avg_time_b - avg_time_a:.2f}s\")\n",
    "        else:\n",
    "            print(f\"  üèÜ Prompt B nhanh h∆°n {avg_time_a - avg_time_b:.2f}s\")\n",
    "        \n",
    "        # So s√°nh ƒë·ªô d√†i output\n",
    "        avg_len_a = sum(len(r[\"output\"]) for r in results_a) / len(results_a)\n",
    "        avg_len_b = sum(len(r[\"output\"]) for r in results_b) / len(results_b)\n",
    "        \n",
    "        print(f\"\\nüìè ƒê·ªô d√†i output trung b√¨nh:\")\n",
    "        print(f\"  ‚Ä¢ Prompt A: {avg_len_a:.0f} k√Ω t·ª±\")\n",
    "        print(f\"  ‚Ä¢ Prompt B: {avg_len_b:.0f} k√Ω t·ª±\")\n",
    "    \n",
    "    return results_a, results_b\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "results_a, results_b = analyze_ab_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hi·ªÉn th·ªã K·∫øt qu·∫£ Chi ti·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt ƒë·ªÉ so s√°nh ch·∫•t l∆∞·ª£ng\n",
    "print(\"üîç SO S√ÅNH CHI TI·∫æT K·∫æT QU·∫¢\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(len(results_a), len(results_b))):\n",
    "    print(f\"\\nüìù VƒÉn b·∫£n #{i+1}:\")\n",
    "    print(f\"Input: {results_a[i]['input_text'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nüÖ∞Ô∏è Prompt A Output ({results_a[i]['processing_time']:.2f}s):\")\n",
    "    print(f\"{results_a[i]['output']}\")\n",
    "    \n",
    "    print(f\"\\nüÖ±Ô∏è Prompt B Output ({results_b[i]['processing_time']:.2f}s):\")\n",
    "    print(f\"{results_b[i]['output']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Th√™m Manual Evaluation v·ªõi LangFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th√™m evaluation scores th·ªß c√¥ng cho m·ªôt s·ªë k·∫øt qu·∫£\n",
    "def add_manual_evaluation(result: Dict[str, Any], quality_score: float, relevance_score: float):\n",
    "    \"\"\"\n",
    "    Th√™m ƒë√°nh gi√° th·ªß c√¥ng v√†o LangFuse\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # T·∫°o evaluation entry\n",
    "        langfuse.score(\n",
    "            name=\"quality_score\",\n",
    "            value=quality_score,\n",
    "            comment=f\"Manual quality evaluation for prompt version {result['prompt_version']}\"\n",
    "        )\n",
    "        \n",
    "        langfuse.score(\n",
    "            name=\"relevance_score\", \n",
    "            value=relevance_score,\n",
    "            comment=f\"Manual relevance evaluation for prompt version {result['prompt_version']}\"\n",
    "        )\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding evaluation: {e}\")\n",
    "        return False\n",
    "\n",
    "# Th√™m m·ªôt s·ªë ƒë√°nh gi√° m·∫´u\n",
    "print(\"üìã Th√™m manual evaluations...\")\n",
    "\n",
    "# ƒê√°nh gi√° m·∫´u (trong th·ª±c t·∫ø s·∫Ω do con ng∆∞·ªùi ƒë√°nh gi√°)\n",
    "sample_evaluations = [\n",
    "    {\"quality\": 8.5, \"relevance\": 9.0},  # Prompt A - Text 1\n",
    "    {\"quality\": 9.2, \"relevance\": 8.8},  # Prompt B - Text 1\n",
    "    {\"quality\": 7.8, \"relevance\": 8.5},  # Prompt A - Text 2\n",
    "    {\"quality\": 8.9, \"relevance\": 9.1},  # Prompt B - Text 2\n",
    "]\n",
    "\n",
    "for i, eval_data in enumerate(sample_evaluations[:len(results)]):\n",
    "    if i < len(results):\n",
    "        success = add_manual_evaluation(\n",
    "            results[i], \n",
    "            eval_data[\"quality\"], \n",
    "            eval_data[\"relevance\"]\n",
    "        )\n",
    "        if success:\n",
    "            print(f\"  ‚úÖ Added evaluation for result #{i+1}\")\n",
    "\n",
    "print(\"‚úÖ Manual evaluations added to LangFuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. T·ªïng h·ª£p v√† K·∫øt lu·∫≠n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªïng h·ª£p k·∫øt qu·∫£ cu·ªëi c√πng\n",
    "def generate_final_report(results_a: List[Dict], results_b: List[Dict]):\n",
    "    \"\"\"\n",
    "    T·∫°o b√°o c√°o t·ªïng h·ª£p A/B testing\n",
    "    \"\"\"\n",
    "    print(\"üìä B√ÅO C√ÅO T·ªîNG H·ª¢P A/B TESTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not results_a or not results_b:\n",
    "        print(\"‚ùå Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ so s√°nh\")\n",
    "        return\n",
    "    \n",
    "    # Metrics comparison\n",
    "    avg_time_a = sum(r[\"processing_time\"] for r in results_a) / len(results_a)\n",
    "    avg_time_b = sum(r[\"processing_time\"] for r in results_b) / len(results_b)\n",
    "    avg_len_a = sum(len(r[\"output\"]) for r in results_a) / len(results_a)\n",
    "    avg_len_b = sum(len(r[\"output\"]) for r in results_b) / len(results_b)\n",
    "    \n",
    "    print(f\"\\nüîç K·∫øt qu·∫£ so s√°nh:\")\n",
    "    print(f\"\\n‚è±Ô∏è T·ªëc ƒë·ªô:\")\n",
    "    if avg_time_a < avg_time_b:\n",
    "        print(f\"  üèÜ Prompt A th·∫Øng (nhanh h∆°n {avg_time_b - avg_time_a:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"  üèÜ Prompt B th·∫Øng (nhanh h∆°n {avg_time_a - avg_time_b:.2f}s)\")\n",
    "    \n",
    "    print(f\"\\nüìè ƒê·ªô chi ti·∫øt:\")\n",
    "    if avg_len_b > avg_len_a:\n",
    "        print(f\"  üèÜ Prompt B chi ti·∫øt h∆°n ({avg_len_b - avg_len_a:.0f} k√Ω t·ª±)\")\n",
    "    else:\n",
    "        print(f\"  üèÜ Prompt A ng·∫Øn g·ªçn h∆°n ({avg_len_a - avg_len_b:.0f} k√Ω t·ª±)\")\n",
    "    \n",
    "    print(f\"\\nüéØ Khuy·∫øn ngh·ªã:\")\n",
    "    if avg_time_a < avg_time_b and avg_len_a > 0:\n",
    "        print(\"  ‚Ä¢ Prompt A ph√π h·ª£p khi c·∫ßn t·ªëc ƒë·ªô v√† t√≠nh ng·∫Øn g·ªçn\")\n",
    "        print(\"  ‚Ä¢ Prompt B ph√π h·ª£p khi c·∫ßn ƒë·ªô chi ti·∫øt v√† c·∫•u tr√∫c\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Prompt B c√≥ v·∫ª to√†n di·ªán h∆°n v·ªõi c·∫•u tr√∫c r√µ r√†ng\")\n",
    "        print(\"  ‚Ä¢ Prompt A ph√π h·ª£p cho use case c·∫ßn t√≥m t·∫Øt nhanh\")\n",
    "    \n",
    "    print(f\"\\nüí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng LangFuse Dashboard:\")\n",
    "    print(\"  1. Truy c·∫≠p LangFuse dashboard\")\n",
    "    print(\"  2. Filter theo tags: 'prompt_version_A' v√† 'prompt_version_B'\")\n",
    "    print(\"  3. So s√°nh metrics: latency, token usage, costs\")\n",
    "    print(\"  4. Xem detailed traces ƒë·ªÉ hi·ªÉu chi ti·∫øt\")\n",
    "    print(\"  5. Ph√¢n t√≠ch evaluation scores ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng\")\n",
    "\n",
    "# T·∫°o b√°o c√°o cu·ªëi c√πng\n",
    "generate_final_report(results_a, results_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Automated Evaluation v·ªõi LangFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• v·ªÅ automated evaluation\n",
    "def automated_evaluation(output_text: str, input_text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° t·ª± ƒë·ªông ch·∫•t l∆∞·ª£ng output\n",
    "    (ƒê√¢y l√† v√≠ d·ª• ƒë∆°n gi·∫£n, trong th·ª±c t·∫ø c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c models chuy√™n d·ª•ng)\n",
    "    \"\"\"\n",
    "    # Metrics ƒë∆°n gi·∫£n\n",
    "    compression_ratio = len(output_text) / len(input_text)\n",
    "    \n",
    "    # ƒê√°nh gi√° d·ª±a tr√™n ƒë·ªô n√©n v√† structure\n",
    "    structure_score = 8.0 if \"**\" in output_text else 6.0  # C√≥ structure formatting\n",
    "    conciseness_score = max(1.0, 10.0 - (compression_ratio * 10))  # Penalty for too long\n",
    "    \n",
    "    return {\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"structure_score\": structure_score,\n",
    "        \"conciseness_score\": conciseness_score\n",
    "    }\n",
    "\n",
    "# Ch·∫°y automated evaluation cho m·ªôt s·ªë k·∫øt qu·∫£\n",
    "print(\"ü§ñ AUTOMATED EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, result in enumerate(results[:4]):  # Evaluate first 4 results\n",
    "    if result[\"success\"]:\n",
    "        eval_scores = automated_evaluation(result[\"output\"], result[\"input_text\"])\n",
    "        \n",
    "        print(f\"\\nResult #{i+1} (Prompt {result['prompt_version']}):\")\n",
    "        print(f\"  üìä Compression ratio: {eval_scores['compression_ratio']:.2f}\")\n",
    "        print(f\"  üèóÔ∏è Structure score: {eval_scores['structure_score']:.1f}/10\")\n",
    "        print(f\"  ‚úÇÔ∏è Conciseness score: {eval_scores['conciseness_score']:.1f}/10\")\n",
    "        \n",
    "        # C√≥ th·ªÉ g·ª≠i scores n√†y l√™n LangFuse\n",
    "        try:\n",
    "            langfuse.score(\n",
    "                name=\"automated_structure\",\n",
    "                value=eval_scores['structure_score'],\n",
    "                comment=f\"Automated structure evaluation for prompt {result['prompt_version']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not upload score: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Automated evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. T√†i li·ªáu Tham kh·∫£o\n",
    "\n",
    "### LangFuse Documentation\n",
    "- **A/B Testing Guide**: https://langfuse.com/docs/experimentation\n",
    "- **Prompt Management**: https://langfuse.com/docs/prompts\n",
    "- **Evaluation & Scoring**: https://langfuse.com/docs/scores\n",
    "- **Tags & Filtering**: https://langfuse.com/docs/tracing-features/tags\n",
    "- **Dashboard Analytics**: https://langfuse.com/docs/analytics\n",
    "\n",
    "### Best Practices\n",
    "- **Experimentation Best Practices**: https://langfuse.com/docs/experimentation/best-practices\n",
    "- **Prompt Engineering**: https://langfuse.com/docs/prompts/best-practices\n",
    "- **LLM Observability**: https://langfuse.com/docs/tracing\n",
    "\n",
    "### Integration Guides\n",
    "- **LangChain Integration**: https://langfuse.com/docs/integrations/langchain\n",
    "- **Python SDK**: https://langfuse.com/docs/sdk/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. K·∫øt lu·∫≠n & B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "### Nh·ªØng g√¨ ƒë√£ h·ªçc ƒë∆∞·ª£c:\n",
    "\n",
    "‚úÖ **Thi·∫øt l·∫≠p A/B Testing cho Prompts**\n",
    "- ƒê·ªãnh nghƒ©a nhi·ªÅu phi√™n b·∫£n prompt cho c√πng m·ªôt t√°c v·ª•\n",
    "- S·ª≠ d·ª•ng tags ƒë·ªÉ ph√¢n bi·ªát c√°c phi√™n b·∫£n trong LangFuse\n",
    "- Tracking metrics nh∆∞ latency, token usage, output quality\n",
    "\n",
    "‚úÖ **Ph√¢n t√≠ch d·ªØ li·ªáu v·ªõi LangFuse**\n",
    "- So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c prompt versions\n",
    "- S·ª≠ d·ª•ng dashboard ƒë·ªÉ visualize k·∫øt qu·∫£\n",
    "- Manual v√† automated evaluation\n",
    "\n",
    "‚úÖ **Ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n d·ªØ li·ªáu**\n",
    "- Ch·ªçn prompt version t·ªëi ∆∞u d·ª±a tr√™n metrics th·ª±c t·∫ø\n",
    "- Hi·ªÉu trade-offs gi·ªØa t·ªëc ƒë·ªô v√† ch·∫•t l∆∞·ª£ng\n",
    "- C√¢n nh·∫Øc use case specific requirements\n",
    "\n",
    "### B∆∞·ªõc ti·∫øp theo:\n",
    "\n",
    "üöÄ **N√¢ng cao A/B Testing**\n",
    "- Test nhi·ªÅu prompt variations h∆°n (A/B/C/D testing)\n",
    "- Th·ª≠ nghi·ªám v·ªõi different model parameters (temperature, max_tokens)\n",
    "- A/B test different LLM models\n",
    "\n",
    "üéØ **Automated Evaluation Pipeline**\n",
    "- T√≠ch h·ª£p evaluation models chuy√™n d·ª•ng\n",
    "- Set up continuous evaluation cho production prompts\n",
    "- Build automated decision making based on evaluation results\n",
    "\n",
    "üìä **Production Monitoring**\n",
    "- Monitor prompt performance in production\n",
    "- Set up alerts cho performance degradation\n",
    "- Implement gradual rollout strategies\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Always measure**: ƒê·ª´ng d·ª±a v√†o c·∫£m t√≠nh, h√£y ƒëo l∆∞·ªùng th·ª±c t·∫ø\n",
    "2. **Use proper tagging**: Tags gi√∫p filter v√† ph√¢n t√≠ch d·ªØ li·ªáu d·ªÖ d√†ng\n",
    "3. **Consider multiple metrics**: Kh√¥ng ch·ªâ accuracy, m√† c√≤n latency, cost, user experience\n",
    "4. **Iterative improvement**: A/B testing l√† qu√° tr√¨nh li√™n t·ª•c, kh√¥ng ph·∫£i one-time activity\n",
    "\n",
    "LangFuse l√†m cho vi·ªác A/B testing prompts tr·ªü n√™n ƒë∆°n gi·∫£n v√† c√≥ h·ªá th·ªëng, gi√∫p b·∫°n t·ªëi ∆∞u h√≥a LLM applications m·ªôt c√°ch khoa h·ªçc! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}