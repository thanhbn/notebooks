{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Simulating Different Review Treatments\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the three review treatments: **MCR**, **ACR**, and **CCR**\n",
    "2. Implement realistic simulations of each treatment using **LangGraph**\n",
    "3. Model reviewer behavior under different conditions\n",
    "4. Analyze treatment effects on review outcomes\n",
    "\n",
    "## Paper Context\n",
    "**Section Reference**: Section II-B (Code Review Treatments) and Section II-C (Experimental Setup)\n",
    "\n",
    "**Treatment Definitions**:\n",
    "- **MCR (Manual Code Review)**: Classic review without automation\n",
    "- **ACR (Automated Code Review)**: ChatGPT-generated review as starting point  \n",
    "- **CCR (Comprehensive Code Review)**: \"Perfect\" review identifying all injected issues\n",
    "\n",
    "**Key Quote**:\n",
    "> \"The third treatment simulates an ideal, hypothetical scenario where the automated code review is able to identify all quality issues in a given code.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Optional, Set, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "# LangGraph components\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-notebook')\n",
    "colors = sns.color_palette(\"husl\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core data structures based on paper\n",
    "\n",
    "class Treatment(Enum):\n",
    "    MCR = \"Manual Code Review\"\n",
    "    ACR = \"Automated Code Review\"\n",
    "    CCR = \"Comprehensive Code Review\"\n",
    "\n",
    "@dataclass\n",
    "class InjectedIssue:\n",
    "    \"\"\"Represents an issue injected into code (from Table I)\"\"\"\n",
    "    id: str\n",
    "    description: str\n",
    "    file: str\n",
    "    line: int\n",
    "    type: str  # evolvability or functional\n",
    "    severity: str  # low, medium, high\n",
    "    example_code: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ReviewAction:\n",
    "    \"\"\"Represents an action taken during review\"\"\"\n",
    "    timestamp: datetime\n",
    "    action_type: str  # open_file, read_code, write_comment, verify_issue, etc.\n",
    "    file: Optional[str] = None\n",
    "    line_range: Optional[Tuple[int, int]] = None\n",
    "    duration_seconds: float = 0\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass \n",
    "class ReviewSession:\n",
    "    \"\"\"Complete review session data\"\"\"\n",
    "    treatment: Treatment\n",
    "    reviewer_id: str\n",
    "    program: str\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    actions: List[ReviewAction] = field(default_factory=list)\n",
    "    initial_comments: List[Dict] = field(default_factory=list)\n",
    "    final_comments: List[Dict] = field(default_factory=list)\n",
    "    confidence_score: Optional[int] = None  # 1-5 scale\n",
    "    issues_found: Set[str] = field(default_factory=set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulating Code Programs with Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgramSimulator:\n",
    "    \"\"\"Simulates programs with injected issues based on paper's approach\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Issue distribution from paper: 78% evolvability, 22% functional\n",
    "        self.issue_type_probs = {'evolvability': 0.78, 'functional': 0.22}\n",
    "        \n",
    "        # Programs from Table I\n",
    "        self.programs = {\n",
    "            'maze-generator': {'loc': 75, 'issues': 2},\n",
    "            'number-conversion': {'loc': 81, 'issues': 2},\n",
    "            'stopwatch': {'loc': 258, 'issues': 4},\n",
    "            'tic-tac-toe': {'loc': 121, 'issues': 7},\n",
    "            'todo-list': {'loc': 198, 'issues': 3},\n",
    "            'word-utils': {'loc': 426, 'issues': 7}\n",
    "        }\n",
    "    \n",
    "    def create_program(self, name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Create a program with injected issues\"\"\"\n",
    "        \n",
    "        if name not in self.programs:\n",
    "            raise ValueError(f\"Unknown program: {name}\")\n",
    "        \n",
    "        prog_info = self.programs[name]\n",
    "        issues = []\n",
    "        \n",
    "        # Generate issues based on paper's distribution\n",
    "        for i in range(prog_info['issues']):\n",
    "            issue_type = np.random.choice(\n",
    "                list(self.issue_type_probs.keys()),\n",
    "                p=list(self.issue_type_probs.values())\n",
    "            )\n",
    "            \n",
    "            # Severity distribution (estimated from paper)\n",
    "            if issue_type == 'functional':\n",
    "                severity = np.random.choice(['medium', 'high'], p=[0.4, 0.6])\n",
    "            else:\n",
    "                severity = np.random.choice(['low', 'medium', 'high'], p=[0.5, 0.35, 0.15])\n",
    "            \n",
    "            issue = InjectedIssue(\n",
    "                id=f\"{name}-issue-{i+1}\",\n",
    "                description=self._generate_issue_description(issue_type, severity),\n",
    "                file=f\"{name}.py\",\n",
    "                line=np.random.randint(1, prog_info['loc']),\n",
    "                type=issue_type,\n",
    "                severity=severity\n",
    "            )\n",
    "            issues.append(issue)\n",
    "        \n",
    "        return {\n",
    "            'name': name,\n",
    "            'loc': prog_info['loc'],\n",
    "            'files': [f\"{name}.py\", f\"test_{name}.py\"],\n",
    "            'injected_issues': issues,\n",
    "            'code_complexity': self._estimate_complexity(prog_info['loc'])\n",
    "        }\n",
    "    \n",
    "    def _generate_issue_description(self, issue_type: str, severity: str) -> str:\n",
    "        \"\"\"Generate realistic issue descriptions\"\"\"\n",
    "        \n",
    "        templates = {\n",
    "            'evolvability': {\n",
    "                'low': [\n",
    "                    \"Missing documentation for method\",\n",
    "                    \"Variable name could be more descriptive\",\n",
    "                    \"Code duplication could be refactored\"\n",
    "                ],\n",
    "                'medium': [\n",
    "                    \"Method is too long and complex\",\n",
    "                    \"Poor separation of concerns\",\n",
    "                    \"String concatenation in loop (performance)\"\n",
    "                ],\n",
    "                'high': [\n",
    "                    \"Architectural issue: tight coupling\",\n",
    "                    \"Major performance bottleneck\",\n",
    "                    \"Critical maintainability issue\"\n",
    "                ]\n",
    "            },\n",
    "            'functional': {\n",
    "                'medium': [\n",
    "                    \"Missing input validation\",\n",
    "                    \"Edge case not handled\",\n",
    "                    \"Potential null pointer exception\"\n",
    "                ],\n",
    "                'high': [\n",
    "                    \"Logic error in algorithm\",\n",
    "                    \"Incorrect calculation result\",\n",
    "                    \"Race condition in concurrent code\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return np.random.choice(templates.get(issue_type, {}).get(severity, [\"Generic issue\"]))\n",
    "    \n",
    "    def _estimate_complexity(self, loc: int) -> str:\n",
    "        \"\"\"Estimate program complexity based on LOC\"\"\"\n",
    "        if loc < 100:\n",
    "            return 'low'\n",
    "        elif loc < 300:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "\n",
    "# Create simulator and example program\n",
    "program_sim = ProgramSimulator()\n",
    "example_program = program_sim.create_program('number-conversion')\n",
    "\n",
    "print(f\"Created program: {example_program['name']}\")\n",
    "print(f\"Lines of code: {example_program['loc']}\")\n",
    "print(f\"Injected issues: {len(example_program['injected_issues'])}\")\n",
    "print(\"\\nIssue breakdown:\")\n",
    "for issue in example_program['injected_issues']:\n",
    "    print(f\"  - {issue.severity.upper()}: {issue.description} (line {issue.line})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treatment-Specific Review Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreatmentSimulator:\n",
    "    \"\"\"Simulates different review treatments based on paper findings\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Key statistics from paper\n",
    "        self.acr_detection_rate = 0.42  # ACR finds 42% of injected issues\n",
    "        self.acr_keep_rate = 0.89       # 89% of ACR suggestions kept\n",
    "        self.mcr_detection_rate = 0.50  # MCR finds 50% of injected issues\n",
    "        \n",
    "    def generate_mcr_review(self, program: Dict[str, Any]) -> List[Dict]:\n",
    "        \"\"\"Generate manual code review (no automation)\"\"\"\n",
    "        \n",
    "        comments = []\n",
    "        injected_issues = program['injected_issues']\n",
    "        \n",
    "        # MCR finds ~50% of injected issues\n",
    "        found_issues = np.random.choice(\n",
    "            injected_issues,\n",
    "            size=int(len(injected_issues) * self.mcr_detection_rate),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for issue in found_issues:\n",
    "            # Human reviewers write more concise comments\n",
    "            comment = {\n",
    "                'id': f\"mcr-{issue.id}\",\n",
    "                'issue_id': issue.id,\n",
    "                'text': self._humanize_comment(issue.description),\n",
    "                'file': issue.file,\n",
    "                'line': issue.line,\n",
    "                'severity': issue.severity,\n",
    "                'author': 'human'\n",
    "            }\n",
    "            comments.append(comment)\n",
    "        \n",
    "        # Add some additional non-injected issues\n",
    "        if np.random.random() < 0.3:  # 30% chance\n",
    "            extra_comment = {\n",
    "                'id': f\"mcr-extra-{len(comments)+1}\",\n",
    "                'issue_id': None,\n",
    "                'text': \"Consider adding unit tests for this method\",\n",
    "                'file': program['files'][0],\n",
    "                'line': np.random.randint(1, program['loc']),\n",
    "                'severity': 'low',\n",
    "                'author': 'human'\n",
    "            }\n",
    "            comments.append(extra_comment)\n",
    "        \n",
    "        return comments\n",
    "    \n",
    "    def generate_acr_review(self, program: Dict[str, Any]) -> List[Dict]:\n",
    "        \"\"\"Generate automated code review (ChatGPT-style)\"\"\"\n",
    "        \n",
    "        comments = []\n",
    "        injected_issues = program['injected_issues']\n",
    "        \n",
    "        # ACR finds ~42% of injected issues\n",
    "        # But biased towards low-severity issues\n",
    "        weights = []\n",
    "        for issue in injected_issues:\n",
    "            if issue.severity == 'low':\n",
    "                weights.append(3.0)\n",
    "            elif issue.severity == 'medium':\n",
    "                weights.append(1.5)\n",
    "            else:  # high\n",
    "                weights.append(0.5)\n",
    "        \n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        n_found = int(len(injected_issues) * self.acr_detection_rate)\n",
    "        found_issues = np.random.choice(\n",
    "            injected_issues,\n",
    "            size=n_found,\n",
    "            replace=False,\n",
    "            p=weights\n",
    "        )\n",
    "        \n",
    "        for issue in found_issues:\n",
    "            # LLM comments are more verbose\n",
    "            comment = {\n",
    "                'id': f\"acr-{issue.id}\",\n",
    "                'issue_id': issue.id,\n",
    "                'text': self._llm_style_comment(issue.description),\n",
    "                'file': issue.file,\n",
    "                'line': issue.line,\n",
    "                'severity': issue.severity,\n",
    "                'author': 'llm'\n",
    "            }\n",
    "            comments.append(comment)\n",
    "        \n",
    "        # ACR tends to add more low-severity suggestions\n",
    "        extra_suggestions = [\n",
    "            \"Consider using more descriptive variable names for clarity\",\n",
    "            \"This method could benefit from additional documentation\",\n",
    "            \"The logic is sound, though it might benefit from comments\",\n",
    "            \"Consider extracting this logic into a separate method\"\n",
    "        ]\n",
    "        \n",
    "        n_extra = np.random.poisson(3)  # Average 3 extra suggestions\n",
    "        for i in range(min(n_extra, len(extra_suggestions))):\n",
    "            comment = {\n",
    "                'id': f\"acr-extra-{i+1}\",\n",
    "                'issue_id': None,\n",
    "                'text': extra_suggestions[i],\n",
    "                'file': program['files'][0],\n",
    "                'line': np.random.randint(1, program['loc']),\n",
    "                'severity': 'low',\n",
    "                'author': 'llm'\n",
    "            }\n",
    "            comments.append(comment)\n",
    "        \n",
    "        return comments\n",
    "    \n",
    "    def generate_ccr_review(self, program: Dict[str, Any]) -> List[Dict]:\n",
    "        \"\"\"Generate comprehensive review (all injected issues found)\"\"\"\n",
    "        \n",
    "        comments = []\n",
    "        \n",
    "        # CCR finds ALL injected issues\n",
    "        for issue in program['injected_issues']:\n",
    "            # Rephrased by LLM to seem natural\n",
    "            comment = {\n",
    "                'id': f\"ccr-{issue.id}\",\n",
    "                'issue_id': issue.id,\n",
    "                'text': self._rephrase_for_ccr(issue.description, issue.severity),\n",
    "                'file': issue.file,\n",
    "                'line': issue.line,\n",
    "                'severity': issue.severity,\n",
    "                'author': 'comprehensive'\n",
    "            }\n",
    "            comments.append(comment)\n",
    "        \n",
    "        return comments\n",
    "    \n",
    "    def _humanize_comment(self, text: str) -> str:\n",
    "        \"\"\"Make comment sound more human\"\"\"\n",
    "        return text  # Simplified for demo\n",
    "    \n",
    "    def _llm_style_comment(self, text: str) -> str:\n",
    "        \"\"\"Make comment sound like LLM output\"\"\"\n",
    "        prefixes = [\n",
    "            \"I noticed that \",\n",
    "            \"It appears that \",\n",
    "            \"Consider addressing: \",\n",
    "            \"There's an opportunity to improve: \"\n",
    "        ]\n",
    "        return np.random.choice(prefixes) + text.lower()\n",
    "    \n",
    "    def _rephrase_for_ccr(self, text: str, severity: str) -> str:\n",
    "        \"\"\"Rephrase for comprehensive review\"\"\"\n",
    "        severity_prefix = {\n",
    "            'low': \"Minor issue: \",\n",
    "            'medium': \"Important: \",\n",
    "            'high': \"Critical issue: \"\n",
    "        }\n",
    "        return severity_prefix.get(severity, \"\") + text\n",
    "\n",
    "# Generate reviews for all treatments\n",
    "treatment_sim = TreatmentSimulator()\n",
    "\n",
    "mcr_comments = treatment_sim.generate_mcr_review(example_program)\n",
    "acr_comments = treatment_sim.generate_acr_review(example_program)\n",
    "ccr_comments = treatment_sim.generate_ccr_review(example_program)\n",
    "\n",
    "print(\"Generated reviews:\")\n",
    "print(f\"  MCR: {len(mcr_comments)} comments\")\n",
    "print(f\"  ACR: {len(acr_comments)} comments\")\n",
    "print(f\"  CCR: {len(ccr_comments)} comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangGraph-based Review Process Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for LangGraph\n",
    "class ReviewState(TypedDict):\n",
    "    \"\"\"State for the review process workflow\"\"\"\n",
    "    treatment: str\n",
    "    program: Dict[str, Any]\n",
    "    initial_comments: List[Dict]\n",
    "    current_file: Optional[str]\n",
    "    reviewed_lines: Set[Tuple[str, int]]\n",
    "    final_comments: List[Dict]\n",
    "    actions: List[ReviewAction]\n",
    "    time_elapsed: float\n",
    "    confidence: Optional[int]\n",
    "    stage: str\n",
    "\n",
    "class ReviewProcessSimulator:\n",
    "    \"\"\"Simulates the complete review process using LangGraph\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.treatment_sim = TreatmentSimulator()\n",
    "        self.graph = self._build_graph()\n",
    "        \n",
    "        # Time parameters from paper (in seconds)\n",
    "        self.time_params = {\n",
    "            'MCR': {'mean': 42*60, 'std': 10*60},\n",
    "            'ACR': {'mean': 56*60, 'std': 15*60},\n",
    "            'CCR': {'mean': 57*60, 'std': 12*60}\n",
    "        }\n",
    "    \n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build the review process workflow\"\"\"\n",
    "        \n",
    "        workflow = StateGraph(ReviewState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"initialize\", self._initialize_review)\n",
    "        workflow.add_node(\"generate_initial\", self._generate_initial_review)\n",
    "        workflow.add_node(\"open_file\", self._open_file)\n",
    "        workflow.add_node(\"review_code\", self._review_code)\n",
    "        workflow.add_node(\"process_comments\", self._process_comments)\n",
    "        workflow.add_node(\"finalize\", self._finalize_review)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"initialize\", \"generate_initial\")\n",
    "        workflow.add_edge(\"generate_initial\", \"open_file\")\n",
    "        workflow.add_edge(\"open_file\", \"review_code\")\n",
    "        workflow.add_edge(\"review_code\", \"process_comments\")\n",
    "        \n",
    "        # Conditional edge: continue reviewing or finalize\n",
    "        workflow.add_conditional_edges(\n",
    "            \"process_comments\",\n",
    "            self._should_continue,\n",
    "            {\n",
    "                \"continue\": \"open_file\",\n",
    "                \"finish\": \"finalize\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"finalize\", END)\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"initialize\")\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def _initialize_review(self, state: ReviewState) -> ReviewState:\n",
    "        \"\"\"Initialize review session\"\"\"\n",
    "        state['reviewed_lines'] = set()\n",
    "        state['final_comments'] = []\n",
    "        state['actions'] = []\n",
    "        state['time_elapsed'] = 0\n",
    "        state['stage'] = 'initialized'\n",
    "        \n",
    "        # Log action\n",
    "        action = ReviewAction(\n",
    "            timestamp=datetime.now(),\n",
    "            action_type='start_review',\n",
    "            details={'treatment': state['treatment'], 'program': state['program']['name']}\n",
    "        )\n",
    "        state['actions'].append(action)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _generate_initial_review(self, state: ReviewState) -> ReviewState:\n",
    "        \"\"\"Generate initial automated review if applicable\"\"\"\n",
    "        \n",
    "        treatment = Treatment[state['treatment']]\n",
    "        \n",
    "        if treatment == Treatment.MCR:\n",
    "            state['initial_comments'] = []\n",
    "        elif treatment == Treatment.ACR:\n",
    "            state['initial_comments'] = self.treatment_sim.generate_acr_review(state['program'])\n",
    "        else:  # CCR\n",
    "            state['initial_comments'] = self.treatment_sim.generate_ccr_review(state['program'])\n",
    "        \n",
    "        # Log action\n",
    "        action = ReviewAction(\n",
    "            timestamp=datetime.now(),\n",
    "            action_type='generate_initial_review',\n",
    "            duration_seconds=np.random.uniform(1, 3),\n",
    "            details={'num_comments': len(state['initial_comments'])}\n",
    "        )\n",
    "        state['actions'].append(action)\n",
    "        state['time_elapsed'] += action.duration_seconds\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _open_file(self, state: ReviewState) -> ReviewState:\n",
    "        \"\"\"Simulate opening a file for review\"\"\"\n",
    "        \n",
    "        # Choose file to review\n",
    "        if state['initial_comments'] and np.random.random() < 0.8:\n",
    "            # 80% chance to follow automated suggestions\n",
    "            suggested_files = list(set(c['file'] for c in state['initial_comments']))\n",
    "            state['current_file'] = np.random.choice(suggested_files)\n",
    "        else:\n",
    "            # Random file selection\n",
    "            state['current_file'] = np.random.choice(state['program']['files'])\n",
    "        \n",
    "        # Log action\n",
    "        action = ReviewAction(\n",
    "            timestamp=datetime.now(),\n",
    "            action_type='open_file',\n",
    "            file=state['current_file'],\n",
    "            duration_seconds=np.random.uniform(2, 5)\n",
    "        )\n",
    "        state['actions'].append(action)\n",
    "        state['time_elapsed'] += action.duration_seconds\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _review_code(self, state: ReviewState) -> ReviewState:\n",
    "        \"\"\"Simulate reviewing code in current file\"\"\"\n",
    "        \n",
    "        treatment = Treatment[state['treatment']]\n",
    "        \n",
    "        # Different behavior based on treatment\n",
    "        if treatment == Treatment.MCR:\n",
    "            # Manual review: systematic exploration\n",
    "            lines_to_review = np.random.randint(20, 50)\n",
    "            review_time = np.random.normal(60, 20)  # 1 minute average\n",
    "        else:\n",
    "            # Automated-assisted: focused on suggested lines\n",
    "            suggested_lines = [\n",
    "                c['line'] for c in state['initial_comments'] \n",
    "                if c['file'] == state['current_file']\n",
    "            ]\n",
    "            lines_to_review = len(suggested_lines) + np.random.randint(5, 15)\n",
    "            review_time = np.random.normal(45, 15)  # Slightly faster\n",
    "        \n",
    "        # Mark lines as reviewed\n",
    "        for _ in range(lines_to_review):\n",
    "            line = np.random.randint(1, state['program']['loc'])\n",
    "            state['reviewed_lines'].add((state['current_file'], line))\n",
    "        \n",
    "        # Log action\n",
    "        action = ReviewAction(\n",
    "            timestamp=datetime.now(),\n",
    "            action_type='review_code',\n",
    "            file=state['current_file'],\n",
    "            duration_seconds=review_time,\n",
    "            details={'lines_reviewed': lines_to_review}\n",
    "        )\n",
    "        state['actions'].append(action)\n",
    "        state['time_elapsed'] += action.duration_seconds\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _process_comments(self, state: ReviewState) -> ReviewState:\n",
    "        \"\"\"Process and finalize comments\"\"\"\n",
    "        \n",
    "        treatment = Treatment[state['treatment']]\n",
    "        \n",
    "        if treatment == Treatment.MCR:\n",
    "            # Generate manual comments\n",
    "            manual_comments = self.treatment_sim.generate_mcr_review(state['program'])\n",
    "            state['final_comments'].extend(manual_comments)\n",
    "        else:\n",
    "            # Process initial comments (ACR/CCR)\n",
    "            for comment in state['initial_comments']:\n",
    "                # 89% keep rate for automated comments\n",
    "                if np.random.random() < 0.89:\n",
    "                    state['final_comments'].append(comment)\n",
    "            \n",
    "            # Small chance to add new issues\n",
    "            if np.random.random() < 0.1:\n",
    "                # Find an issue not in initial comments\n",
    "                found_issue_ids = {c['issue_id'] for c in state['initial_comments'] if c['issue_id']}\n",
    "                remaining = [\n",
    "                    issue for issue in state['program']['injected_issues']\n",
    "                    if issue.id not in found_issue_ids\n",
    "                ]\n",
    "                if remaining:\n",
    "                    new_issue = np.random.choice(remaining)\n",
    "                    comment = {\n",
    "                        'id': f\"manual-{new_issue.id}\",\n",
    "                        'issue_id': new_issue.id,\n",
    "                        'text': f\"Additionally found: {new_issue.description}\",\n",
    "                        'file': new_issue.file,\n",
    "                        'line': new_issue.line,\n",
    "                        'severity': new_issue.severity,\n",
    "                        'author': 'human'\n",
    "                    }\n",
    "                    state['final_comments'].append(comment)\n",
    "        \n",
    "        # Log action\n",
    "        write_time = len(state['final_comments']) * np.random.uniform(20, 40)\n",
    "        action = ReviewAction(\n",
    "            timestamp=datetime.now(),\n",
    "            action_type='write_comments',\n",
    "            duration_seconds=write_time,\n",
    "            details={'num_comments': len(state['final_comments'])}\n",
    "        )\n",
    "        state['actions'].append(action)\n",
    "        state['time_elapsed'] += action.duration_seconds\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _should_continue(self, state: ReviewState) -> str:\n",
    "        \"\"\"Decide whether to continue reviewing\"\"\"\n",
    "        \n",
    "        # Check time budget\n",
    "        treatment = state['treatment']\n",
    "        time_budget = self.time_params[treatment]['mean']\n",
    "        \n",
    "        if state['time_elapsed'] > time_budget * 0.8:\n",
    "            return \"finish\"\n",
    "        \n",
    "        # Check if all files reviewed\n",
    "        reviewed_files = set(f for f, _ in state['reviewed_lines'])\n",
    "        if len(reviewed_files) >= len(state['program']['files']):\n",
    "            return \"finish\"\n",
    "        \n",
    "        return \"continue\"\n",
    "    \n",
    "    def _finalize_review(self, state: ReviewState) -> ReviewState:\n",
    "        \"\"\"Finalize the review session\"\"\"\n",
    "        \n",
    "        # Calculate confidence (no significant difference between treatments)\n",
    "        state['confidence'] = int(np.random.normal(3.6, 0.8))\n",
    "        state['confidence'] = max(1, min(5, state['confidence']))\n",
    "        \n",
    "        # Log final action\n",
    "        action = ReviewAction(\n",
    "            timestamp=datetime.now(),\n",
    "            action_type='finalize_review',\n",
    "            duration_seconds=np.random.uniform(30, 60),\n",
    "            details={\n",
    "                'total_comments': len(state['final_comments']),\n",
    "                'confidence': state['confidence'],\n",
    "                'total_time': state['time_elapsed']\n",
    "            }\n",
    "        )\n",
    "        state['actions'].append(action)\n",
    "        state['time_elapsed'] += action.duration_seconds\n",
    "        \n",
    "        state['stage'] = 'completed'\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def run_review(self, treatment: str, program: Dict[str, Any]) -> ReviewState:\n",
    "        \"\"\"Run a complete review simulation\"\"\"\n",
    "        \n",
    "        initial_state = ReviewState(\n",
    "            treatment=treatment,\n",
    "            program=program,\n",
    "            initial_comments=[],\n",
    "            current_file=None,\n",
    "            reviewed_lines=set(),\n",
    "            final_comments=[],\n",
    "            actions=[],\n",
    "            time_elapsed=0,\n",
    "            confidence=None,\n",
    "            stage='pending'\n",
    "        )\n",
    "        \n",
    "        return self.graph.invoke(initial_state)\n",
    "\n",
    "# Initialize simulator\n",
    "process_sim = ReviewProcessSimulator()\n",
    "print(\"Review Process Simulator initialized with LangGraph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running Complete Treatment Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations for all treatments\n",
    "results = {}\n",
    "\n",
    "for treatment in ['MCR', 'ACR', 'CCR']:\n",
    "    print(f\"\\nRunning {treatment} simulation...\")\n",
    "    \n",
    "    # Run the review\n",
    "    result = process_sim.run_review(treatment, example_program)\n",
    "    results[treatment] = result\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"  Time: {result['time_elapsed']/60:.1f} minutes\")\n",
    "    print(f\"  Comments: {len(result['final_comments'])}\")\n",
    "    print(f\"  Confidence: {result['confidence']}/5\")\n",
    "    print(f\"  Files reviewed: {len(set(f for f, _ in result['reviewed_lines']))}\")\n",
    "    print(f\"  Lines reviewed: {len(result['reviewed_lines'])}\")\n",
    "\n",
    "print(\"\\nAll simulations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing Treatment Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_treatment_differences(results: Dict[str, ReviewState]):\n",
    "    \"\"\"Analyze key differences between treatments\"\"\"\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = []\n",
    "    \n",
    "    for treatment, state in results.items():\n",
    "        # Count issues found\n",
    "        found_issue_ids = {c['issue_id'] for c in state['final_comments'] if c['issue_id']}\n",
    "        all_issue_ids = {issue.id for issue in state['program']['injected_issues']}\n",
    "        \n",
    "        # Time breakdown\n",
    "        time_by_action = defaultdict(float)\n",
    "        for action in state['actions']:\n",
    "            time_by_action[action.action_type] += action.duration_seconds\n",
    "        \n",
    "        metrics.append({\n",
    "            'Treatment': treatment,\n",
    "            'Total Time (min)': state['time_elapsed'] / 60,\n",
    "            'Comments': len(state['final_comments']),\n",
    "            'Injected Issues Found': len(found_issue_ids),\n",
    "            'Detection Rate': len(found_issue_ids) / len(all_issue_ids) * 100,\n",
    "            'Lines Reviewed': len(state['reviewed_lines']),\n",
    "            'Confidence': state['confidence'],\n",
    "            'Time Reading (%)': time_by_action['review_code'] / state['time_elapsed'] * 100,\n",
    "            'Time Writing (%)': time_by_action['write_comments'] / state['time_elapsed'] * 100\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Time comparison\n",
    "    ax = axes[0, 0]\n",
    "    df.plot(x='Treatment', y='Total Time (min)', kind='bar', ax=ax, color=colors[:3])\n",
    "    ax.set_title('Review Time by Treatment')\n",
    "    ax.set_ylabel('Time (minutes)')\n",
    "    ax.legend().remove()\n",
    "    \n",
    "    # Add paper reference line\n",
    "    ax.axhline(y=42, color='red', linestyle='--', alpha=0.5, label='MCR avg (paper)')\n",
    "    ax.axhline(y=56, color='blue', linestyle='--', alpha=0.5, label='ACR avg (paper)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Detection rate\n",
    "    ax = axes[0, 1]\n",
    "    df.plot(x='Treatment', y='Detection Rate', kind='bar', ax=ax, color=colors[3:6])\n",
    "    ax.set_title('Issue Detection Rate')\n",
    "    ax.set_ylabel('Detection Rate (%)')\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.legend().remove()\n",
    "    \n",
    "    # 3. Comments vs Coverage\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(df['Comments'], df['Lines Reviewed'], s=100, c=colors[:3])\n",
    "    for i, txt in enumerate(df['Treatment']):\n",
    "        ax.annotate(txt, (df['Comments'].iloc[i], df['Lines Reviewed'].iloc[i]), \n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    ax.set_xlabel('Number of Comments')\n",
    "    ax.set_ylabel('Lines Reviewed')\n",
    "    ax.set_title('Review Verbosity vs Coverage')\n",
    "    \n",
    "    # 4. Time allocation\n",
    "    ax = axes[1, 1]\n",
    "    time_data = df[['Treatment', 'Time Reading (%)', 'Time Writing (%)']].set_index('Treatment')\n",
    "    time_data.plot(kind='bar', stacked=True, ax=ax, color=['skyblue', 'lightcoral'])\n",
    "    ax.set_title('Time Allocation by Treatment')\n",
    "    ax.set_ylabel('Percentage of Total Time')\n",
    "    ax.legend(title='Activity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze differences\n",
    "analysis_df = analyze_treatment_differences(results)\n",
    "print(\"\\nTreatment Analysis Summary:\")\n",
    "print(analysis_df.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Behavioral Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_behavioral_patterns(results: Dict[str, ReviewState]):\n",
    "    \"\"\"Analyze reviewer behavior patterns across treatments\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    for idx, (treatment, state) in enumerate(results.items()):\n",
    "        # Extract action timeline\n",
    "        actions = state['actions']\n",
    "        start_time = actions[0].timestamp\n",
    "        \n",
    "        # Create timeline data\n",
    "        timeline_data = []\n",
    "        cumulative_time = 0\n",
    "        \n",
    "        for action in actions:\n",
    "            timeline_data.append({\n",
    "                'time': cumulative_time / 60,  # Convert to minutes\n",
    "                'action': action.action_type,\n",
    "                'duration': action.duration_seconds / 60\n",
    "            })\n",
    "            cumulative_time += action.duration_seconds\n",
    "        \n",
    "        # Plot timeline\n",
    "        ax = axes[0, idx]\n",
    "        \n",
    "        # Color mapping for actions\n",
    "        action_colors = {\n",
    "            'start_review': 'green',\n",
    "            'generate_initial_review': 'yellow',\n",
    "            'open_file': 'blue',\n",
    "            'review_code': 'orange',\n",
    "            'write_comments': 'red',\n",
    "            'finalize_review': 'purple'\n",
    "        }\n",
    "        \n",
    "        y_pos = 0\n",
    "        for item in timeline_data:\n",
    "            color = action_colors.get(item['action'], 'gray')\n",
    "            ax.barh(y_pos, item['duration'], left=item['time'], \n",
    "                   color=color, alpha=0.7, height=0.8)\n",
    "            y_pos += 1\n",
    "        \n",
    "        ax.set_xlabel('Time (minutes)')\n",
    "        ax.set_ylabel('Action Sequence')\n",
    "        ax.set_title(f'{treatment} Review Timeline')\n",
    "        ax.set_ylim(-0.5, len(timeline_data) - 0.5)\n",
    "        \n",
    "        # Create legend for first plot\n",
    "        if idx == 0:\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [Patch(facecolor=color, label=action.replace('_', ' ').title()) \n",
    "                             for action, color in action_colors.items()]\n",
    "            ax.legend(handles=legend_elements, loc='upper left', \n",
    "                     bbox_to_anchor=(1.05, 1), fontsize=8)\n",
    "        \n",
    "        # Plot coverage heatmap\n",
    "        ax = axes[1, idx]\n",
    "        \n",
    "        # Create coverage matrix\n",
    "        max_line = state['program']['loc']\n",
    "        coverage_matrix = np.zeros((len(state['program']['files']), max_line // 10 + 1))\n",
    "        \n",
    "        for file, line in state['reviewed_lines']:\n",
    "            file_idx = state['program']['files'].index(file)\n",
    "            line_bucket = line // 10\n",
    "            if line_bucket < coverage_matrix.shape[1]:\n",
    "                coverage_matrix[file_idx, line_bucket] += 1\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(coverage_matrix, cmap='YlOrRd', aspect='auto')\n",
    "        ax.set_xlabel('Code Sections (10 lines each)')\n",
    "        ax.set_ylabel('Files')\n",
    "        ax.set_title(f'{treatment} Coverage Heatmap')\n",
    "        ax.set_yticks(range(len(state['program']['files'])))\n",
    "        ax.set_yticklabels([f.replace('.py', '') for f in state['program']['files']])\n",
    "        \n",
    "        # Add colorbar for last plot\n",
    "        if idx == 2:\n",
    "            cbar = plt.colorbar(im, ax=ax)\n",
    "            cbar.set_label('Review Intensity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze behavior patterns\n",
    "analyze_behavioral_patterns(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comment Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_comment_quality(results: Dict[str, ReviewState]):\n",
    "    \"\"\"Analyze the quality characteristics of comments across treatments\"\"\"\n",
    "    \n",
    "    comment_analysis = []\n",
    "    \n",
    "    for treatment, state in results.items():\n",
    "        for comment in state['final_comments']:\n",
    "            # Calculate comment metrics\n",
    "            word_count = len(comment['text'].split())\n",
    "            \n",
    "            # Check for actionability\n",
    "            action_keywords = ['should', 'must', 'need', 'consider', 'fix', 'change', 'update']\n",
    "            is_actionable = any(keyword in comment['text'].lower() for keyword in action_keywords)\n",
    "            \n",
    "            # Check for specificity\n",
    "            has_line_ref = 'line' in comment['text'].lower()\n",
    "            has_code_ref = '`' in comment['text'] or 'code' in comment['text'].lower()\n",
    "            \n",
    "            comment_analysis.append({\n",
    "                'Treatment': treatment,\n",
    "                'Severity': comment['severity'],\n",
    "                'Word Count': word_count,\n",
    "                'Actionable': is_actionable,\n",
    "                'Has Line Ref': has_line_ref,\n",
    "                'Has Code Ref': has_code_ref,\n",
    "                'Author': comment['author'],\n",
    "                'Is Injected': comment['issue_id'] is not None\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    comment_df = pd.DataFrame(comment_analysis)\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Word count distribution\n",
    "    ax = axes[0, 0]\n",
    "    for treatment in ['MCR', 'ACR', 'CCR']:\n",
    "        data = comment_df[comment_df['Treatment'] == treatment]['Word Count']\n",
    "        ax.hist(data, alpha=0.6, label=treatment, bins=15)\n",
    "    ax.set_xlabel('Word Count')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Comment Length Distribution')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Severity distribution\n",
    "    ax = axes[0, 1]\n",
    "    severity_pivot = comment_df.pivot_table(\n",
    "        index='Treatment', \n",
    "        columns='Severity', \n",
    "        values='Word Count', \n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    severity_pivot.plot(kind='bar', ax=ax, color=['lightgreen', 'orange', 'red'])\n",
    "    ax.set_title('Severity Distribution by Treatment')\n",
    "    ax.set_ylabel('Number of Comments')\n",
    "    \n",
    "    # 3. Quality metrics\n",
    "    ax = axes[1, 0]\n",
    "    quality_metrics = comment_df.groupby('Treatment').agg({\n",
    "        'Actionable': 'mean',\n",
    "        'Has Line Ref': 'mean',\n",
    "        'Has Code Ref': 'mean'\n",
    "    }) * 100  # Convert to percentage\n",
    "    \n",
    "    quality_metrics.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Comment Quality Metrics')\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.legend(title='Metric')\n",
    "    \n",
    "    # 4. Detection accuracy\n",
    "    ax = axes[1, 1]\n",
    "    detection_data = []\n",
    "    for treatment in ['MCR', 'ACR', 'CCR']:\n",
    "        treatment_comments = comment_df[comment_df['Treatment'] == treatment]\n",
    "        injected_found = treatment_comments['Is Injected'].sum()\n",
    "        total_comments = len(treatment_comments)\n",
    "        precision = injected_found / total_comments if total_comments > 0 else 0\n",
    "        \n",
    "        detection_data.append({\n",
    "            'Treatment': treatment,\n",
    "            'Precision': precision,\n",
    "            'Total Comments': total_comments,\n",
    "            'Injected Found': injected_found\n",
    "        })\n",
    "    \n",
    "    detection_df = pd.DataFrame(detection_data)\n",
    "    \n",
    "    x = range(len(detection_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar([i - width/2 for i in x], detection_df['Injected Found'], \n",
    "                   width, label='Injected Issues', color='darkgreen')\n",
    "    bars2 = ax.bar([i + width/2 for i in x], \n",
    "                   detection_df['Total Comments'] - detection_df['Injected Found'], \n",
    "                   width, label='Other Comments', color='lightgray')\n",
    "    \n",
    "    ax.set_xlabel('Treatment')\n",
    "    ax.set_ylabel('Number of Comments')\n",
    "    ax.set_title('Comment Composition')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(detection_df['Treatment'])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add precision values on top\n",
    "    for i, (bar1, bar2, precision) in enumerate(zip(bars1, bars2, detection_df['Precision'])):\n",
    "        height = bar1.get_height() + bar2.get_height()\n",
    "        ax.text(i, height + 0.5, f'{precision:.0%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comment_df\n",
    "\n",
    "# Analyze comment quality\n",
    "comment_analysis_df = analyze_comment_quality(results)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nComment Quality Summary:\")\n",
    "summary = comment_analysis_df.groupby('Treatment').agg({\n",
    "    'Word Count': ['mean', 'std'],\n",
    "    'Actionable': 'mean',\n",
    "    'Is Injected': ['sum', 'count']\n",
    "}).round(2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Simulating Multiple Reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_multiple_reviewers(n_reviewers: int = 10, program_name: str = 'number-conversion'):\n",
    "    \"\"\"Simulate multiple reviewers to observe variability\"\"\"\n",
    "    \n",
    "    # Create program\n",
    "    program = program_sim.create_program(program_name)\n",
    "    \n",
    "    # Store results for each treatment\n",
    "    multi_results = {treatment: [] for treatment in ['MCR', 'ACR', 'CCR']}\n",
    "    \n",
    "    print(f\"Simulating {n_reviewers} reviewers for each treatment...\")\n",
    "    \n",
    "    for treatment in ['MCR', 'ACR', 'CCR']:\n",
    "        for i in range(n_reviewers):\n",
    "            # Run review\n",
    "            result = process_sim.run_review(treatment, program)\n",
    "            \n",
    "            # Extract key metrics\n",
    "            found_issue_ids = {c['issue_id'] for c in result['final_comments'] if c['issue_id']}\n",
    "            \n",
    "            metrics = {\n",
    "                'reviewer_id': i + 1,\n",
    "                'time_minutes': result['time_elapsed'] / 60,\n",
    "                'num_comments': len(result['final_comments']),\n",
    "                'issues_found': len(found_issue_ids),\n",
    "                'detection_rate': len(found_issue_ids) / len(program['injected_issues']),\n",
    "                'confidence': result['confidence'],\n",
    "                'lines_reviewed': len(result['reviewed_lines'])\n",
    "            }\n",
    "            \n",
    "            multi_results[treatment].append(metrics)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    dfs = {}\n",
    "    for treatment, results in multi_results.items():\n",
    "        dfs[treatment] = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualize variability\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Time variability\n",
    "    ax = axes[0, 0]\n",
    "    time_data = [dfs[t]['time_minutes'].values for t in ['MCR', 'ACR', 'CCR']]\n",
    "    bp = ax.boxplot(time_data, labels=['MCR', 'ACR', 'CCR'], patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors[:3]):\n",
    "        patch.set_facecolor(color)\n",
    "    ax.set_ylabel('Time (minutes)')\n",
    "    ax.set_title('Review Time Variability')\n",
    "    \n",
    "    # Add paper means\n",
    "    paper_means = [42, 56, 57]\n",
    "    for i, mean in enumerate(paper_means):\n",
    "        ax.axhline(y=mean, xmin=i/3-0.1, xmax=i/3+0.2, \n",
    "                  color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # 2. Detection rate variability\n",
    "    ax = axes[0, 1]\n",
    "    detection_data = [dfs[t]['detection_rate'].values * 100 for t in ['MCR', 'ACR', 'CCR']]\n",
    "    bp = ax.boxplot(detection_data, labels=['MCR', 'ACR', 'CCR'], patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors[3:6]):\n",
    "        patch.set_facecolor(color)\n",
    "    ax.set_ylabel('Detection Rate (%)')\n",
    "    ax.set_title('Issue Detection Variability')\n",
    "    ax.set_ylim(0, 110)\n",
    "    \n",
    "    # 3. Scatter: Time vs Detection\n",
    "    ax = axes[1, 0]\n",
    "    for treatment, color in zip(['MCR', 'ACR', 'CCR'], colors[:3]):\n",
    "        df = dfs[treatment]\n",
    "        ax.scatter(df['time_minutes'], df['detection_rate'] * 100, \n",
    "                  alpha=0.6, s=100, label=treatment, color=color)\n",
    "    ax.set_xlabel('Time (minutes)')\n",
    "    ax.set_ylabel('Detection Rate (%)')\n",
    "    ax.set_title('Time-Efficiency Trade-off')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 4. Confidence distribution\n",
    "    ax = axes[1, 1]\n",
    "    confidence_data = pd.DataFrame({\n",
    "        treatment: dfs[treatment]['confidence'].value_counts(normalize=True).sort_index()\n",
    "        for treatment in ['MCR', 'ACR', 'CCR']\n",
    "    }).fillna(0)\n",
    "    \n",
    "    confidence_data.T.plot(kind='bar', ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "    ax.set_xlabel('Treatment')\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.set_title('Confidence Score Distribution')\n",
    "    ax.legend(title='Confidence', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    for treatment in ['MCR', 'ACR', 'CCR']:\n",
    "        df = dfs[treatment]\n",
    "        print(f\"\\n{treatment}:\")\n",
    "        print(f\"  Time: {df['time_minutes'].mean():.1f}  {df['time_minutes'].std():.1f} min\")\n",
    "        print(f\"  Detection: {df['detection_rate'].mean()*100:.1f}  {df['detection_rate'].std()*100:.1f}%\")\n",
    "        print(f\"  Comments: {df['num_comments'].mean():.1f}  {df['num_comments'].std():.1f}\")\n",
    "        print(f\"  Confidence: {df['confidence'].mean():.2f}  {df['confidence'].std():.2f}\")\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "# Run multi-reviewer simulation\n",
    "multi_reviewer_results = simulate_multiple_reviewers(n_reviewers=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Implementation Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = {\n",
    "    \"Treatment Characteristics\": [\n",
    "        \"MCR: High variability in coverage, balanced severity detection\",\n",
    "        \"ACR: Biased towards low-severity issues, influences reviewer focus\",\n",
    "        \"CCR: Perfect recall but doesn't improve reviewer confidence\",\n",
    "        \"All treatments show similar confidence levels (~3.6/5)\"\n",
    "    ],\n",
    "    \n",
    "    \"Behavioral Differences\": [\n",
    "        \"MCR reviewers explore code more systematically\",\n",
    "        \"ACR/CCR reviewers focus on suggested locations (anchoring bias)\",\n",
    "        \"89% of automated suggestions are kept in final review\",\n",
    "        \"Limited exploration beyond automated suggestions in ACR/CCR\"\n",
    "    ],\n",
    "    \n",
    "    \"Time and Efficiency\": [\n",
    "        \"No time savings with automated assistance (MCR: 42min, ACR: 56min)\",\n",
    "        \"Time spent verifying automated comments offsets exploration savings\",\n",
    "        \"CCR shows diminishing returns despite perfect issue identification\",\n",
    "        \"High inter-reviewer variability in all treatments\"\n",
    "    ],\n",
    "    \n",
    "    \"Implementation with LangGraph\": [\n",
    "        \"State management captures complete review process\",\n",
    "        \"Conditional edges model decision points effectively\",\n",
    "        \"Action tracking enables detailed behavioral analysis\",\n",
    "        \"Flexible framework for testing new treatments\"\n",
    "    ],\n",
    "    \n",
    "    \"Practical Recommendations\": [\n",
    "        \"Consider hybrid approaches that combine MCR exploration with ACR assistance\",\n",
    "        \"Focus automation on high-severity issue detection\",\n",
    "        \"Design interfaces that encourage exploration beyond suggested issues\",\n",
    "        \"Track behavioral metrics to identify and mitigate biases\",\n",
    "        \"Use A/B testing to evaluate new treatment variations\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS: Simulating Different Review Treatments\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category, items in insights.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"   {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"This simulation framework demonstrates how different code review treatments\")\n",
    "print(\"affect reviewer behavior and outcomes. The key finding that automated\")\n",
    "print(\"assistance doesn't save time but changes focus patterns has important\")\n",
    "print(\"implications for tool design and deployment strategies.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}