{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIPO Focused Learning: Model-Agnostic Optimization\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "Notebook nÃ y táº­p trung vÃ o kháº£ nÄƒng **Model-Agnostic** cá»§a FIPO - má»™t trong nhá»¯ng Ä‘Ã³ng gÃ³p quan trá»ng nháº¥t:\n",
    "\n",
    "1. Hiá»ƒu sá»± khÃ¡c biá»‡t giá»¯a model-specific vÃ  model-agnostic optimization\n",
    "2. PhÃ¢n tÃ­ch cÃ¡ch FIPO hoáº¡t Ä‘á»™ng vá»›i cÃ¡c downstream generators khÃ¡c nhau\n",
    "3. So sÃ¡nh hiá»‡u quáº£ trÃªn nhiá»u model sizes vÃ  architectures\n",
    "4. Implement strategies cho cross-model optimization\n",
    "\n",
    "## ðŸ“š Paper References\n",
    "\n",
    "- **Section 2.1**: Task Formulation (model-agnostic approach)\n",
    "- **Section 3.2**: Experimental Results across models\n",
    "- **Table 2**: Performance on various downstream generators\n",
    "- **Figure 4**: Comparison across 6 different LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Model-Agnostic vs Model-Specific\n",
    "\n",
    "### 1.1 The Generalization Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
    "import networkx as nx\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimization_approaches():\n",
    "    \"\"\"Compare model-specific vs model-agnostic approaches\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Left: Model-specific (Ad-hoc APO)\n",
    "    ax1.set_title(\"Model-Specific Optimization\\n(Traditional APO)\", \n",
    "                 fontsize=16, weight='bold', pad=20)\n",
    "    \n",
    "    # Draw in-box testing loop\n",
    "    in_box_rect = FancyBboxPatch((0.1, 0.6), 0.8, 0.3, \n",
    "                                boxstyle=\"round,pad=0.05\",\n",
    "                                facecolor='lightcoral', \n",
    "                                edgecolor='darkred',\n",
    "                                linewidth=2)\n",
    "    ax1.add_patch(in_box_rect)\n",
    "    ax1.text(0.5, 0.75, \"In-box Generator\\n(e.g., GPT-3.5)\", \n",
    "            ha='center', va='center', fontsize=12, weight='bold')\n",
    "    \n",
    "    # Draw optimization dependency\n",
    "    ax1.arrow(0.5, 0.6, 0, -0.15, head_width=0.05, head_length=0.03, \n",
    "             fc='black', ec='black')\n",
    "    ax1.text(0.52, 0.5, \"Depends on\", ha='left', fontsize=10)\n",
    "    \n",
    "    # Optimizer\n",
    "    opt_rect = FancyBboxPatch((0.2, 0.2), 0.6, 0.2,\n",
    "                             boxstyle=\"round,pad=0.05\",\n",
    "                             facecolor='lightblue',\n",
    "                             edgecolor='darkblue',\n",
    "                             linewidth=2)\n",
    "    ax1.add_patch(opt_rect)\n",
    "    ax1.text(0.5, 0.3, \"API Optimizer\\n(e.g., GPT-4)\", \n",
    "            ha='center', va='center', fontsize=12, weight='bold')\n",
    "    \n",
    "    # Show poor generalization\n",
    "    out_models = [(0.05, 0.02, \"Model A\"), (0.35, 0.02, \"Model B\"), \n",
    "                 (0.65, 0.02, \"Model C\")]\n",
    "    for x, y, label in out_models:\n",
    "        rect = Rectangle((x, y), 0.25, 0.08, \n",
    "                        facecolor='lightgray', edgecolor='gray')\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(x + 0.125, y + 0.04, label, ha='center', va='center', \n",
    "                fontsize=9)\n",
    "        ax1.text(x + 0.125, y - 0.03, \"âŒ\", ha='center', fontsize=12)\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Right: Model-agnostic (FIPO)\n",
    "    ax2.set_title(\"Model-Agnostic Optimization\\n(FIPO)\", \n",
    "                 fontsize=16, weight='bold', pad=20)\n",
    "    \n",
    "    # Local optimizer\n",
    "    local_rect = FancyBboxPatch((0.2, 0.6), 0.6, 0.3,\n",
    "                               boxstyle=\"round,pad=0.05\",\n",
    "                               facecolor='lightgreen',\n",
    "                               edgecolor='darkgreen',\n",
    "                               linewidth=2)\n",
    "    ax2.add_patch(local_rect)\n",
    "    ax2.text(0.5, 0.75, \"Local FIPO Optimizer\\n(Trained Offline)\", \n",
    "            ha='center', va='center', fontsize=12, weight='bold')\n",
    "    \n",
    "    # No dependency arrow - independent!\n",
    "    ax2.text(0.5, 0.5, \"âœ¨ Independent âœ¨\", ha='center', fontsize=12, \n",
    "            style='italic', color='darkgreen')\n",
    "    \n",
    "    # Show good generalization\n",
    "    out_models = [(0.05, 0.3, \"Model A\"), (0.35, 0.3, \"Model B\"), \n",
    "                 (0.65, 0.3, \"Model C\"),\n",
    "                 (0.05, 0.15, \"Model D\"), (0.35, 0.15, \"Model E\"), \n",
    "                 (0.65, 0.15, \"Model F\")]\n",
    "    \n",
    "    for x, y, label in out_models:\n",
    "        rect = Rectangle((x, y), 0.25, 0.08, \n",
    "                        facecolor='lightgreen', edgecolor='green')\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(x + 0.125, y + 0.04, label, ha='center', va='center', \n",
    "                fontsize=9)\n",
    "        ax2.text(x + 0.125, y - 0.05, \"âœ“\", ha='center', fontsize=14, \n",
    "                color='green')\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_optimization_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mathematical Formulation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_mathematical_difference():\n",
    "    \"\"\"Explain mathematical difference between approaches\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Traditional Ad-hoc APO\n",
    "    ax1.text(0.5, 0.9, \"Traditional Ad-hoc APO (Equations 6-7)\", \n",
    "            transform=ax1.transAxes, ha='center', fontsize=16, weight='bold')\n",
    "    \n",
    "    equations_adhoc = [\n",
    "        r\"$\\hat{x}_t^{oi+1} = \\arg\\max_{M_{o-api}} p(\\hat{x}_t^{oi+1} | x_t^{oi}, \\hat{y}_t^{oi})$\",\n",
    "        r\"$\\hat{y}_t^{oi} = \\arg\\max_{M_{g-in}} p(\\hat{y}_t^{oi} | x_t^{oi})$\",\n",
    "        \"\",\n",
    "        \"âš ï¸ Requires in-box testing response $\\hat{y}_t^{oi}$\",\n",
    "        \"âš ï¸ Tied to specific $M_{g-in}$ generator\"\n",
    "    ]\n",
    "    \n",
    "    y_pos = 0.7\n",
    "    for eq in equations_adhoc:\n",
    "        if eq.startswith(\"$\"):\n",
    "            ax1.text(0.5, y_pos, eq, transform=ax1.transAxes, ha='center', \n",
    "                    fontsize=14, fontfamily='serif')\n",
    "        elif eq.startswith(\"âš ï¸\"):\n",
    "            ax1.text(0.5, y_pos, eq, transform=ax1.transAxes, ha='center', \n",
    "                    fontsize=12, color='red')\n",
    "        y_pos -= 0.12\n",
    "    \n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # FIPO Approach\n",
    "    ax2.text(0.5, 0.9, \"FIPO Model-Agnostic (Equations 1, 5)\", \n",
    "            transform=ax2.transAxes, ha='center', fontsize=16, weight='bold')\n",
    "    \n",
    "    equations_fipo = [\n",
    "        \"Training:\",\n",
    "        r\"$\\hat{x}_o = \\arg\\max_{M_{o-local}} p(\\hat{x}_o | x_n, [\\hat{y}_n], [y_n])$\",\n",
    "        \"\",\n",
    "        \"Testing:\",\n",
    "        r\"$\\hat{x}_t^o = \\arg\\max_{M_o} p(\\hat{x}_t^o | x_t^n)$\",\n",
    "        \"\",\n",
    "        \"âœ… No dependency on specific generator\",\n",
    "        \"âœ… Works with any downstream $M_g$\"\n",
    "    ]\n",
    "    \n",
    "    y_pos = 0.7\n",
    "    for eq in equations_fipo:\n",
    "        if eq in [\"Training:\", \"Testing:\"]:\n",
    "            ax2.text(0.1, y_pos, eq, transform=ax2.transAxes, \n",
    "                    fontsize=12, weight='bold')\n",
    "        elif eq.startswith(\"$\"):\n",
    "            ax2.text(0.5, y_pos, eq, transform=ax2.transAxes, ha='center', \n",
    "                    fontsize=14, fontfamily='serif')\n",
    "        elif eq.startswith(\"âœ…\"):\n",
    "            ax2.text(0.5, y_pos, eq, transform=ax2.transAxes, ha='center', \n",
    "                    fontsize=12, color='green')\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explain_mathematical_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Model Performance Analysis\n",
    "\n",
    "### 2.1 FIPO Performance Across Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPerformance:\n",
    "    \"\"\"Performance data for a model\"\"\"\n",
    "    model_name: str\n",
    "    model_size: str\n",
    "    naive_scores: Dict[str, float]\n",
    "    optimized_scores: Dict[str, float]\n",
    "    \n",
    "    def get_improvement(self, benchmark: str) -> float:\n",
    "        \"\"\"Calculate improvement percentage\"\"\"\n",
    "        naive = self.naive_scores.get(benchmark, 0)\n",
    "        optimized = self.optimized_scores.get(benchmark, 0)\n",
    "        if naive == 0:\n",
    "            return 0\n",
    "        return ((optimized - naive) / naive) * 100\n",
    "\n",
    "# Load performance data from Table 2\n",
    "def load_fipo_results() -> List[ModelPerformance]:\n",
    "    \"\"\"Load FIPO results from paper\"\"\"\n",
    "    \n",
    "    models_data = [\n",
    "        ModelPerformance(\n",
    "            model_name=\"Llama2-7B\",\n",
    "            model_size=\"7B\",\n",
    "            naive_scores={\"GSM8K\": 8.89, \"BBH\": 31.21, \"PiQA\": 62.78, \n",
    "                         \"CosmosQA\": 43.09, \"MMLU\": 46.58},\n",
    "            optimized_scores={\"GSM8K\": 11.70, \"BBH\": 33.50, \"PiQA\": 69.37, \n",
    "                            \"CosmosQA\": 52.11, \"MMLU\": 54.56}\n",
    "        ),\n",
    "        ModelPerformance(\n",
    "            model_name=\"Tulu2-13B\",\n",
    "            model_size=\"13B\",\n",
    "            naive_scores={\"GSM8K\": 39.06, \"BBH\": 36.49, \"PiQA\": 76.62, \n",
    "                         \"CosmosQA\": 55.13, \"MMLU\": 57.43},\n",
    "            optimized_scores={\"GSM8K\": 40.17, \"BBH\": 40.26, \"PiQA\": 78.58, \n",
    "                            \"CosmosQA\": 57.68, \"MMLU\": 59.10}\n",
    "        ),\n",
    "        ModelPerformance(\n",
    "            model_name=\"Baichuan2-13B\",\n",
    "            model_size=\"13B\",\n",
    "            naive_scores={\"GSM8K\": 46.81, \"BBH\": 37.95, \"PiQA\": 68.56, \n",
    "                         \"CosmosQA\": 51.88, \"MMLU\": 57.46},\n",
    "            optimized_scores={\"GSM8K\": 48.12, \"BBH\": 39.95, \"PiQA\": 74.77, \n",
    "                            \"CosmosQA\": 56.88, \"MMLU\": 58.32}\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return models_data\n",
    "\n",
    "def visualize_cross_model_performance():\n",
    "    \"\"\"Visualize FIPO performance across models\"\"\"\n",
    "    \n",
    "    models_data = load_fipo_results()\n",
    "    benchmarks = [\"GSM8K\", \"BBH\", \"PiQA\", \"CosmosQA\", \"MMLU\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot for each benchmark\n",
    "    for idx, benchmark in enumerate(benchmarks):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        model_names = [m.model_name for m in models_data]\n",
    "        naive_scores = [m.naive_scores[benchmark] for m in models_data]\n",
    "        optimized_scores = [m.optimized_scores[benchmark] for m in models_data]\n",
    "        improvements = [m.get_improvement(benchmark) for m in models_data]\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, naive_scores, width, label='Naive', \n",
    "                       color='lightcoral', alpha=0.7)\n",
    "        bars2 = ax.bar(x + width/2, optimized_scores, width, label='FIPO', \n",
    "                       color='lightgreen', alpha=0.7)\n",
    "        \n",
    "        # Add improvement percentages\n",
    "        for i, (imp, opt_score) in enumerate(zip(improvements, optimized_scores)):\n",
    "            ax.text(i, opt_score + 1, f'+{imp:.1f}%', ha='center', \n",
    "                   fontsize=9, color='darkgreen', weight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Models')\n",
    "        ax.set_ylabel('Score (%)')\n",
    "        ax.set_title(f'{benchmark} Performance', fontsize=14, weight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names, rotation=45)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Overall improvement summary\n",
    "    ax = axes[5]\n",
    "    \n",
    "    # Calculate average improvements\n",
    "    avg_improvements = []\n",
    "    for model in models_data:\n",
    "        avg_imp = np.mean([model.get_improvement(b) for b in benchmarks])\n",
    "        avg_improvements.append(avg_imp)\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']  # Different colors for each model\n",
    "    bars = ax.bar(range(len(models_data)), avg_improvements, color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, imp) in enumerate(zip(bars, avg_improvements)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "               f'{imp:.1f}%', ha='center', va='bottom', fontsize=12, weight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Average Improvement (%)')\n",
    "    ax.set_title('Average Improvement Across All Benchmarks', fontsize=14, weight='bold')\n",
    "    ax.set_xticks(range(len(models_data)))\n",
    "    ax.set_xticklabels([m.model_name for m in models_data])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('FIPO Performance Across Different Models', fontsize=18, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_cross_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extended Analysis with More Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_extended_models():\n",
    "    \"\"\"Analyze FIPO on extended model set from Figure 4\"\"\"\n",
    "    \n",
    "    # Data from Figure 4 (6 BBH tasks average)\n",
    "    extended_results = pd.DataFrame({\n",
    "        'Model': ['Llama2-7B', 'Tulu2-7B', 'Llama3-70B-Instruct', \n",
    "                 'Qwen2-72B-Instruct', 'GPT-3.5-turbo', 'GPT-4o'],\n",
    "        'Size': ['7B', '7B', '70B', '72B', 'Unknown', 'Unknown'],\n",
    "        'Type': ['Open', 'Open', 'Open', 'Open', 'Proprietary', 'Proprietary'],\n",
    "        'Naive': [46.8, 44.7, 66.6, 68.0, 51.3, 76.2],\n",
    "        'APE': [49.2, 48.9, 67.6, 68.7, 68.1, 79.7],\n",
    "        'PromptAgent': [54.1, 52.4, 67.7, 72.2, 79.0, 82.0],\n",
    "        'GPT-4': [53.1, 54.7, 66.5, 69.1, 68.0, 81.3],\n",
    "        'FIPO': [56.7, 55.4, 69.2, 73.1, 73.2, 84.4]\n",
    "    })\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Left: Performance comparison\n",
    "    methods = ['Naive', 'APE', 'PromptAgent', 'GPT-4', 'FIPO']\n",
    "    colors = ['gray', 'lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "    \n",
    "    x = np.arange(len(extended_results))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "        offset = (i - 2) * width\n",
    "        bars = ax1.bar(x + offset, extended_results[method], width, \n",
    "                       label=method, color=color, alpha=0.8)\n",
    "        \n",
    "        # Highlight FIPO\n",
    "        if method == 'FIPO':\n",
    "            for bar in bars:\n",
    "                bar.set_edgecolor('black')\n",
    "                bar.set_linewidth(2)\n",
    "    \n",
    "    ax1.set_xlabel('Models', fontsize=12)\n",
    "    ax1.set_ylabel('Performance (%)', fontsize=12)\n",
    "    ax1.set_title('Performance Comparison Across Methods', fontsize=14, weight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(extended_results['Model'], rotation=45, ha='right')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Right: Improvement analysis\n",
    "    improvements = pd.DataFrame()\n",
    "    for method in methods[1:]:\n",
    "        improvements[method] = ((extended_results[method] - extended_results['Naive']) / \n",
    "                               extended_results['Naive'] * 100)\n",
    "    \n",
    "    # Group by model type\n",
    "    model_types = {\n",
    "        '7B Models': ['Llama2-7B', 'Tulu2-7B'],\n",
    "        '70B+ Models': ['Llama3-70B-Instruct', 'Qwen2-72B-Instruct'],\n",
    "        'Proprietary': ['GPT-3.5-turbo', 'GPT-4o']\n",
    "    }\n",
    "    \n",
    "    y_pos = 0\n",
    "    y_labels = []\n",
    "    \n",
    "    for group_name, models in model_types.items():\n",
    "        for model in models:\n",
    "            if model in extended_results['Model'].values:\n",
    "                idx = extended_results[extended_results['Model'] == model].index[0]\n",
    "                \n",
    "                # Plot improvements\n",
    "                for j, method in enumerate(['APE', 'PromptAgent', 'GPT-4', 'FIPO']):\n",
    "                    imp = improvements.loc[idx, method]\n",
    "                    color = colors[j+1]\n",
    "                    \n",
    "                    bar = ax2.barh(y_pos, imp, height=0.2, \n",
    "                                  left=j*25, color=color, alpha=0.8)\n",
    "                    \n",
    "                    # Add value\n",
    "                    ax2.text(j*25 + imp/2, y_pos, f'{imp:.1f}%', \n",
    "                            ha='center', va='center', fontsize=8)\n",
    "                \n",
    "                y_labels.append(model)\n",
    "                y_pos += 1\n",
    "        \n",
    "        # Add group separator\n",
    "        if group_name != 'Proprietary':\n",
    "            ax2.axhline(y_pos - 0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax2.set_yticks(range(len(y_labels)))\n",
    "    ax2.set_yticklabels(y_labels)\n",
    "    ax2.set_xlabel('Improvement over Naive (%)', fontsize=12)\n",
    "    ax2.set_title('Improvement Analysis by Model Type', fontsize=14, weight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add method labels\n",
    "    for j, method in enumerate(['APE', 'PromptAgent', 'GPT-4', 'FIPO']):\n",
    "        ax2.text(j*25 + 12.5, -0.8, method, ha='center', fontsize=10, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nðŸ“Š Model-Agnostic Performance Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for group_name, models in model_types.items():\n",
    "        group_data = extended_results[extended_results['Model'].isin(models)]\n",
    "        if not group_data.empty:\n",
    "            avg_fipo_imp = ((group_data['FIPO'] - group_data['Naive']) / \n",
    "                           group_data['Naive'] * 100).mean()\n",
    "            print(f\"{group_name}: Average FIPO improvement = {avg_fipo_imp:.1f}%\")\n",
    "    \n",
    "    print(\"\\nðŸ” Key Insights:\")\n",
    "    print(\"â€¢ FIPO consistently outperforms other methods\")\n",
    "    print(\"â€¢ Larger improvements on smaller models (7B)\")\n",
    "    print(\"â€¢ Works effectively on both open and proprietary models\")\n",
    "\n",
    "analyze_extended_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model-Agnostic Design Principles\n",
    "\n",
    "### 3.1 Key Design Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAgnosticOptimizer:\n",
    "    \"\"\"Demonstrates model-agnostic design principles\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.design_principles = {\n",
    "            \"No In-Box Dependency\": {\n",
    "                \"description\": \"Optimizer trained offline without specific generator\",\n",
    "                \"benefit\": \"Works with any model at inference time\",\n",
    "                \"implementation\": \"Use diverse training data, not model-specific\"\n",
    "            },\n",
    "            \"Universal Meta-Template\": {\n",
    "                \"description\": \"Template adapts to presence/absence of responses\",\n",
    "                \"benefit\": \"Flexible across training and inference\",\n",
    "                \"implementation\": \"Optional components in template\"\n",
    "            },\n",
    "            \"Task-Focused Optimization\": {\n",
    "                \"description\": \"Optimize task instruction, not model behavior\",\n",
    "                \"benefit\": \"Generalizes across model architectures\",\n",
    "                \"implementation\": \"Focus on clarity, structure, specificity\"\n",
    "            },\n",
    "            \"Dataset Diversification\": {\n",
    "                \"description\": \"8 format types reduce model-specific bias\",\n",
    "                \"benefit\": \"Robust to different model capabilities\",\n",
    "                \"implementation\": \"2Ã—2Ã—2 diversification strategy\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def visualize_principles(self):\n",
    "        \"\"\"Visualize design principles\"\"\"\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        \n",
    "        # Create network graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Central node\n",
    "        G.add_node(\"Model-Agnostic\\nFIPO\", size=3000, color='gold')\n",
    "        \n",
    "        # Principle nodes\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "        for i, (principle, details) in enumerate(self.design_principles.items()):\n",
    "            G.add_node(principle, size=2000, color=colors[i])\n",
    "            G.add_edge(\"Model-Agnostic\\nFIPO\", principle)\n",
    "            \n",
    "            # Add benefit nodes\n",
    "            benefit_node = f\"Benefit {i+1}\"\n",
    "            G.add_node(benefit_node, size=1000, color='lightgray')\n",
    "            G.add_edge(principle, benefit_node)\n",
    "        \n",
    "        # Layout\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Draw nodes\n",
    "        node_colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "        node_sizes = [G.nodes[node].get('size', 1500) for node in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                              node_size=node_sizes, alpha=0.8)\n",
    "        nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5, width=2)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        \n",
    "        ax.set_title(\"Model-Agnostic Design Principles\", fontsize=18, weight='bold', pad=20)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show detailed explanation\n",
    "        print(\"\\nðŸ—ï¸ Model-Agnostic Design Principles:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for principle, details in self.design_principles.items():\n",
    "            print(f\"\\nðŸ“Œ {principle}\")\n",
    "            print(f\"   Description: {details['description']}\")\n",
    "            print(f\"   Benefit: {details['benefit']}\")\n",
    "            print(f\"   Implementation: {details['implementation']}\")\n",
    "\n",
    "optimizer = ModelAgnosticOptimizer()\n",
    "optimizer.visualize_principles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparison with Model-Specific Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_optimization_strategies():\n",
    "    \"\"\"Compare different optimization strategies\"\"\"\n",
    "    \n",
    "    strategies = pd.DataFrame({\n",
    "        'Approach': ['APE', 'PromptAgent', 'Direct GPT-4', 'FIPO'],\n",
    "        'Type': ['Model-Specific', 'Model-Specific', 'Model-Specific', 'Model-Agnostic'],\n",
    "        'Training_Required': ['No', 'No', 'No', 'Yes'],\n",
    "        'API_Dependency': ['Yes', 'Yes', 'Yes', 'No'],\n",
    "        'Privacy_Safe': ['No', 'No', 'No', 'Yes'],\n",
    "        'Generalization': ['Poor', 'Poor', 'Poor', 'Excellent'],\n",
    "        'Cost_Per_Optimization': ['$5', '$5', '$4', '$0'],\n",
    "        'Speed': ['2h', '2h', '1h', '30s']\n",
    "    })\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Left: Feature comparison\n",
    "    features = ['Training_Required', 'API_Dependency', 'Privacy_Safe']\n",
    "    \n",
    "    # Convert to binary for visualization\n",
    "    binary_data = np.zeros((len(strategies), len(features)))\n",
    "    for i, approach in enumerate(strategies['Approach']):\n",
    "        for j, feature in enumerate(features):\n",
    "            value = strategies.loc[i, feature]\n",
    "            binary_data[i, j] = 1 if value == 'Yes' else 0\n",
    "    \n",
    "    im = ax1.imshow(binary_data, cmap='RdYlGn', aspect='auto')\n",
    "    \n",
    "    # Set ticks\n",
    "    ax1.set_xticks(np.arange(len(features)))\n",
    "    ax1.set_yticks(np.arange(len(strategies)))\n",
    "    ax1.set_xticklabels(features, rotation=45, ha='right')\n",
    "    ax1.set_yticklabels(strategies['Approach'])\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(strategies)):\n",
    "        for j in range(len(features)):\n",
    "            text = ax1.text(j, i, 'âœ“' if binary_data[i, j] else 'âœ—',\n",
    "                           ha='center', va='center', fontsize=16,\n",
    "                           color='white' if binary_data[i, j] else 'black')\n",
    "    \n",
    "    ax1.set_title('Feature Comparison', fontsize=14, weight='bold')\n",
    "    \n",
    "    # Right: Performance vs Cost\n",
    "    # Extract numeric values\n",
    "    costs = [5, 5, 4, 0]  # Dollar amounts\n",
    "    speeds = [120, 120, 60, 0.5]  # Minutes\n",
    "    \n",
    "    # Create bubble chart\n",
    "    colors = ['red', 'orange', 'yellow', 'green']\n",
    "    sizes = [100, 100, 100, 200]  # FIPO is larger\n",
    "    \n",
    "    for i, approach in enumerate(strategies['Approach']):\n",
    "        ax2.scatter(costs[i], speeds[i], s=sizes[i]*5, \n",
    "                   c=[colors[i]], alpha=0.6, edgecolors='black', linewidth=2)\n",
    "        ax2.annotate(approach, (costs[i], speeds[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    ax2.set_xlabel('Cost per Optimization ($)', fontsize=12)\n",
    "    ax2.set_ylabel('Time per Optimization (minutes)', fontsize=12)\n",
    "    ax2.set_title('Cost vs Speed Analysis', fontsize=14, weight='bold')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add ideal zone\n",
    "    ideal_rect = Rectangle((0, 0), 1, 1, linewidth=2, \n",
    "                          edgecolor='green', facecolor='green', alpha=0.1)\n",
    "    ax2.add_patch(ideal_rect)\n",
    "    ax2.text(0.5, 0.5, 'Ideal\\nZone', ha='center', va='center', \n",
    "            fontsize=12, color='green', weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\nðŸ“Š Strategy Comparison Summary:\")\n",
    "    print(strategies.to_string(index=False))\n",
    "\n",
    "compare_optimization_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Model-Agnostic Optimization\n",
    "\n",
    "### 4.1 Cross-Model Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModelTester:\n",
    "    \"\"\"Framework for testing FIPO across different models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_models = [\n",
    "            {\"name\": \"Small-7B\", \"size\": 7, \"type\": \"decoder\"},\n",
    "            {\"name\": \"Medium-13B\", \"size\": 13, \"type\": \"decoder\"},\n",
    "            {\"name\": \"Large-70B\", \"size\": 70, \"type\": \"decoder\"},\n",
    "            {\"name\": \"Instruction-Tuned\", \"size\": 13, \"type\": \"instruction\"},\n",
    "            {\"name\": \"Chat-Model\", \"size\": 7, \"type\": \"chat\"},\n",
    "            {\"name\": \"Multilingual\", \"size\": 13, \"type\": \"multilingual\"}\n",
    "        ]\n",
    "        \n",
    "        self.test_prompts = [\n",
    "            {\"task\": \"Math\", \"naive\": \"Calculate 15% of 200\"},\n",
    "            {\"task\": \"Reasoning\", \"naive\": \"Why does ice float?\"},\n",
    "            {\"task\": \"Creative\", \"naive\": \"Write a haiku about AI\"},\n",
    "            {\"task\": \"Factual\", \"naive\": \"What is quantum computing?\"}\n",
    "        ]\n",
    "    \n",
    "    def simulate_optimization(self, prompt: str, model_type: str) -> Tuple[str, float]:\n",
    "        \"\"\"Simulate FIPO optimization\"\"\"\n",
    "        \n",
    "        # Base optimization (model-agnostic)\n",
    "        optimized = f\"Please carefully and systematically {prompt.lower()}. \"\n",
    "        optimized += \"Provide a clear, step-by-step explanation.\"\n",
    "        \n",
    "        # Simulate performance improvement\n",
    "        base_improvement = np.random.uniform(5, 15)\n",
    "        \n",
    "        # Model-specific adjustments (but FIPO doesn't know this!)\n",
    "        model_factors = {\n",
    "            \"decoder\": 1.0,\n",
    "            \"instruction\": 1.2,\n",
    "            \"chat\": 1.1,\n",
    "            \"multilingual\": 0.9\n",
    "        }\n",
    "        \n",
    "        improvement = base_improvement * model_factors.get(model_type, 1.0)\n",
    "        \n",
    "        return optimized, improvement\n",
    "    \n",
    "    def run_cross_model_test(self):\n",
    "        \"\"\"Run optimization across all models\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for model in self.test_models:\n",
    "            for prompt_data in self.test_prompts:\n",
    "                optimized, improvement = self.simulate_optimization(\n",
    "                    prompt_data[\"naive\"], model[\"type\"]\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"model\": model[\"name\"],\n",
    "                    \"model_size\": model[\"size\"],\n",
    "                    \"model_type\": model[\"type\"],\n",
    "                    \"task\": prompt_data[\"task\"],\n",
    "                    \"improvement\": improvement\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def visualize_results(self, results_df: pd.DataFrame):\n",
    "        \"\"\"Visualize cross-model results\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Heatmap of improvements\n",
    "        ax = axes[0, 0]\n",
    "        pivot_data = results_df.pivot(index='model', columns='task', values='improvement')\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax)\n",
    "        ax.set_title('Improvement Heatmap (Model Ã— Task)', fontsize=14, weight='bold')\n",
    "        \n",
    "        # 2. Model size vs improvement\n",
    "        ax = axes[0, 1]\n",
    "        avg_by_size = results_df.groupby('model_size')['improvement'].mean()\n",
    "        ax.plot(avg_by_size.index, avg_by_size.values, 'o-', linewidth=3, \n",
    "               markersize=10, color='darkblue')\n",
    "        ax.set_xlabel('Model Size (B parameters)')\n",
    "        ax.set_ylabel('Average Improvement (%)')\n",
    "        ax.set_title('Model Size vs Improvement', fontsize=14, weight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Task-wise performance\n",
    "        ax = axes[1, 0]\n",
    "        task_avg = results_df.groupby('task')['improvement'].agg(['mean', 'std'])\n",
    "        x = range(len(task_avg))\n",
    "        ax.bar(x, task_avg['mean'], yerr=task_avg['std'], \n",
    "              capsize=5, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(task_avg.index)\n",
    "        ax.set_ylabel('Average Improvement (%)')\n",
    "        ax.set_title('Task-wise Performance', fontsize=14, weight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 4. Model type analysis\n",
    "        ax = axes[1, 1]\n",
    "        type_avg = results_df.groupby('model_type')['improvement'].mean().sort_values()\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(type_avg)))\n",
    "        bars = ax.barh(range(len(type_avg)), type_avg.values, color=colors)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, type_avg.values)):\n",
    "            ax.text(value + 0.2, i, f'{value:.1f}%', va='center')\n",
    "        \n",
    "        ax.set_yticks(range(len(type_avg)))\n",
    "        ax.set_yticklabels(type_avg.index)\n",
    "        ax.set_xlabel('Average Improvement (%)')\n",
    "        ax.set_title('Performance by Model Type', fontsize=14, weight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.suptitle('FIPO Cross-Model Testing Results', fontsize=18, weight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run cross-model testing\n",
    "tester = CrossModelTester()\n",
    "results = tester.run_cross_model_test()\n",
    "tester.visualize_results(results)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Cross-Model Testing Summary:\")\n",
    "print(f\"Total configurations tested: {len(results)}\")\n",
    "print(f\"Average improvement: {results['improvement'].mean():.1f}%\")\n",
    "print(f\"Std deviation: {results['improvement'].std():.1f}%\")\n",
    "print(\"\\nâœ… FIPO shows consistent improvement across all model types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Guidelines\n",
    "\n",
    "### 5.1 Best Practices for Model-Agnostic Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_practices_guide():\n",
    "    \"\"\"Create visual guide for best practices\"\"\"\n",
    "    \n",
    "    best_practices = {\n",
    "        \"Training Phase\": [\n",
    "            \"Use diverse model outputs for training data\",\n",
    "            \"Apply 8-type format diversification\",\n",
    "            \"Train with both small and large model responses\",\n",
    "            \"Include multilingual examples if possible\"\n",
    "        ],\n",
    "        \"Optimization Focus\": [\n",
    "            \"Optimize clarity and structure, not model quirks\",\n",
    "            \"Use universal improvement patterns\",\n",
    "            \"Avoid model-specific terminology\",\n",
    "            \"Focus on task understanding\"\n",
    "        ],\n",
    "        \"Testing Strategy\": [\n",
    "            \"Test on models not seen during training\",\n",
    "            \"Evaluate across different model sizes\",\n",
    "            \"Include both base and instruction-tuned models\",\n",
    "            \"Measure relative improvement, not absolute scores\"\n",
    "        ],\n",
    "        \"Deployment\": [\n",
    "            \"Use same optimizer for all downstream models\",\n",
    "            \"No need for model-specific fine-tuning\",\n",
    "            \"Monitor performance across model updates\",\n",
    "            \"Maintain single optimization pipeline\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(0.5, 0.95, \"Model-Agnostic Optimization Best Practices\", \n",
    "           ha='center', fontsize=20, weight='bold', transform=ax.transAxes)\n",
    "    \n",
    "    # Create quadrants\n",
    "    colors = ['#FFE5E5', '#E5F5FF', '#E5FFE5', '#FFF5E5']\n",
    "    positions = [(0.02, 0.5), (0.52, 0.5), (0.02, 0.05), (0.52, 0.05)]\n",
    "    \n",
    "    for i, (category, practices) in enumerate(best_practices.items()):\n",
    "        x, y = positions[i]\n",
    "        \n",
    "        # Category box\n",
    "        rect = FancyBboxPatch((x, y), 0.46, 0.4, \n",
    "                             boxstyle=\"round,pad=0.02\",\n",
    "                             facecolor=colors[i],\n",
    "                             edgecolor='gray',\n",
    "                             linewidth=2,\n",
    "                             transform=ax.transAxes)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Category title\n",
    "        ax.text(x + 0.23, y + 0.35, category, \n",
    "               ha='center', fontsize=14, weight='bold',\n",
    "               transform=ax.transAxes)\n",
    "        \n",
    "        # Practices\n",
    "        for j, practice in enumerate(practices):\n",
    "            ax.text(x + 0.03, y + 0.28 - j*0.07, f\"â€¢ {practice}\", \n",
    "                   fontsize=11, transform=ax.transAxes,\n",
    "                   wrap=True)\n",
    "    \n",
    "    # Add central insight\n",
    "    center_text = \"Key Insight:\\nFIPO optimizes prompts,\\nnot model behavior\"\n",
    "    ax.text(0.5, 0.48, center_text, ha='center', va='center',\n",
    "           fontsize=12, weight='bold', style='italic',\n",
    "           bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='yellow', alpha=0.3),\n",
    "           transform=ax.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_best_practices_guide()\n",
    "\n",
    "# Implementation example\n",
    "print(\"\\nðŸ’» Implementation Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "# Model-agnostic FIPO usage\n",
    "\n",
    "# 1. Load trained FIPO optimizer (same for all models)\n",
    "fipo_optimizer = load_fipo_model(\"path/to/fipo_weights\")\n",
    "\n",
    "# 2. Optimize prompt (no model info needed)\n",
    "naive_prompt = \"Explain quantum computing\"\n",
    "optimized_prompt = fipo_optimizer.optimize(naive_prompt)\n",
    "\n",
    "# 3. Use with ANY model\n",
    "model_a_response = model_a.generate(optimized_prompt)\n",
    "model_b_response = model_b.generate(optimized_prompt)\n",
    "model_c_response = model_c.generate(optimized_prompt)\n",
    "\n",
    "# All models benefit from the same optimization!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Key Takeaways\n",
    "\n",
    "### Core Model-Agnostic Concepts:\n",
    "\n",
    "1. **Independence from Generators**:\n",
    "   - No in-box testing required\n",
    "   - Trained offline once, used everywhere\n",
    "   - No API dependencies\n",
    "\n",
    "2. **Universal Optimization**:\n",
    "   - Focus on task clarity, not model quirks\n",
    "   - Works across model sizes (7B to 70B+)\n",
    "   - Effective for both open and proprietary models\n",
    "\n",
    "3. **Performance Insights**:\n",
    "   - Average 6.37% improvement on 7B models\n",
    "   - Average 2.13% improvement on 13B models\n",
    "   - Consistent gains across all benchmarks\n",
    "   - Larger relative gains on smaller models\n",
    "\n",
    "4. **Practical Benefits**:\n",
    "   - Single optimization pipeline for all models\n",
    "   - Privacy-preserving (no external APIs)\n",
    "   - Cost-effective ($0 per optimization)\n",
    "   - Fast inference (30 seconds vs hours)\n",
    "\n",
    "### Key Success Factors:\n",
    "\n",
    "- **Diverse Training Data**: Not tied to specific model outputs\n",
    "- **Format Diversification**: 8 types reduce model bias\n",
    "- **Task-Focused Design**: Optimize instructions, not behaviors\n",
    "- **Offline Training**: Complete independence from test models\n",
    "\n",
    "FIPO's model-agnostic design represents a paradigm shift from traditional prompt optimization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}