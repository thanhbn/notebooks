{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Low-Rank Adaptation (LoRA) Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the mathematical foundations of Low-Rank Adaptation (LoRA)\n",
    "- Implement LoRA from scratch to gain deep insights into its mechanics\n",
    "- Explore the trade-offs between rank, scaling factors, and performance\n",
    "- Compare LoRA with other PEFT methods through hands-on implementation\n",
    "\n",
    "## Paper Reference\n",
    "This notebook explores concepts from the paper \"Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications\" (arXiv:2404.13506v2).\n",
    "\n",
    "Specifically, we focus on Section 3 which highlights that LoRA is one of the most effective PEFT methods across various applications:\n",
    "\n",
    "> \"Our analysis reveals that Low-Rank Adaptation (LoRA) fine-tunes a minimal number of parameters, thus enabling the recalibration of training weights on a single GPU.\" (Section 5, Page 6)\n",
    "\n",
    "The paper also notes that LoRA has been successfully applied in multiple domains:\n",
    "\n",
    "> \"In medical imaging, a combination of methods (e.g., BitFit + LoRA) may be effective\" (Section 3.3, Page 3)\n",
    "\n",
    "> \"For PPI prediction, PEFT models even outperform traditional methods... Low-Rank Adaptation at 0.81 percent compared to the full model's 100 percent...\" (Section 3.4, Page 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to LoRA\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method introduced by Hu et al. (2021) that updates pre-trained model weights by adding low-rank decomposition matrices. The key insight behind LoRA is that the weight updates during fine-tuning have a low \"intrinsic rank\".\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "In LoRA, instead of directly updating a weight matrix $W \\in \\mathbb{R}^{d \\times k}$, we freeze $W$ and introduce a low-rank decomposition:\n",
    "\n",
    "$$W' = W + \\Delta W = W + BA$$\n",
    "\n",
    "where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$\n",
    "- $r \\ll \\min(d,k)$ is the rank\n",
    "\n",
    "This results in significantly fewer trainable parameters, as we only train the matrices $A$ and $B$ while keeping the original weights $W$ frozen.\n",
    "\n",
    "During the forward pass, the LoRA-augmented weight computation is:\n",
    "\n",
    "$$h = Wx + \\Delta W x = Wx + BAx$$\n",
    "\n",
    "To further control the contribution of the adaptation, a scaling factor $\\alpha$ is introduced:\n",
    "\n",
    "$$h = Wx + \\frac{\\alpha}{r}BAx$$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter that scales the contribution of the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch transformers datasets peft matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing LoRA from Scratch\n",
    "\n",
    "Let's implement LoRA from scratch to understand its inner workings. We'll start by creating a basic linear layer with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"Linear layer with LoRA adaptation\"\"\"\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Original linear layer (frozen)\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        # Freeze the weights of the original linear layer\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Scaling factor\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize LoRA matrices\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Original forward pass\n",
    "        orig_output = self.linear(x)\n",
    "        \n",
    "        # LoRA forward pass\n",
    "        # Apply A matrix, then dropout, then B matrix\n",
    "        lora_output = self.dropout(F.linear(x, self.lora_A))  # [batch_size, rank]\n",
    "        lora_output = F.linear(lora_output, self.lora_B.t())  # [batch_size, out_features]\n",
    "        \n",
    "        # Scale LoRA output\n",
    "        lora_output = lora_output * self.scaling\n",
    "        \n",
    "        # Combine outputs\n",
    "        return orig_output + lora_output\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        return self.lora_A.numel() + self.lora_B.numel()\n",
    "    \n",
    "    def get_total_parameters(self):\n",
    "        \"\"\"Count the total number of parameters (trainable + frozen)\"\"\"\n",
    "        return self.linear.weight.numel() + self.lora_A.numel() + self.lora_B.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a simple neural network with and without LoRA to compare their parameter count and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple neural network with two linear layers\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SimpleNN_LoRA(nn.Module):\n",
    "    \"\"\"Simple neural network with LoRA-adapted linear layers\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, rank=8, alpha=16, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = LoRALinear(input_dim, hidden_dim, rank, alpha, dropout)\n",
    "        self.fc2 = LoRALinear(hidden_dim, output_dim, rank, alpha, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        return self.fc1.get_trainable_parameters() + self.fc2.get_trainable_parameters()\n",
    "    \n",
    "    def get_total_parameters(self):\n",
    "        \"\"\"Count the total number of parameters (trainable + frozen)\"\"\"\n",
    "        return self.fc1.get_total_parameters() + self.fc2.get_total_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to count the number of trainable parameters in a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_total_parameters(model):\n",
    "    \"\"\"Count the total number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a synthetic classification dataset and compare the standard model with the LoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "def create_synthetic_data(n_samples=1000, n_features=20, n_classes=2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create a random weight matrix for classification\n",
    "    W = np.random.randn(n_features, n_classes)\n",
    "    \n",
    "    # Compute logits and apply softmax\n",
    "    logits = X @ W\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    y = np.argmax(probs, axis=1)\n",
    "    \n",
    "    # Add some noise to make it more challenging\n",
    "    noise_idx = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "    y[noise_idx] = 1 - y[noise_idx]  # Flip labels for these samples\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create datasets\n",
    "X_train, y_train = create_synthetic_data(n_samples=800, n_features=100, n_classes=2)\n",
    "X_test, y_test = create_synthetic_data(n_samples=200, n_features=100, n_classes=2)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Parameter Counts and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "\n",
    "# Create models\n",
    "standard_model = SimpleNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "lora_model = SimpleNN_LoRA(input_dim, hidden_dim, output_dim, rank=8, alpha=16, dropout=0.1).to(device)\n",
    "\n",
    "# Count parameters\n",
    "standard_params = count_parameters(standard_model)\n",
    "standard_total_params = count_total_parameters(standard_model)\n",
    "lora_params = count_parameters(lora_model)\n",
    "lora_total_params = count_total_parameters(lora_model)\n",
    "\n",
    "# Print parameter counts\n",
    "print(f\"Standard model - Trainable parameters: {standard_params:,} (100.00%)\")\n",
    "print(f\"LoRA model - Trainable parameters: {lora_params:,} ({lora_params / standard_params * 100:.2f}%)\")\n",
    "print(f\"LoRA model - Total parameters (including frozen): {lora_total_params:,}\")\n",
    "print(f\"Parameter reduction: {(1 - lora_params / standard_params) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train both models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, test_dataloader, optimizer, num_epochs=10, model_name=\"Model\"):\n",
    "    \"\"\"Train and evaluate a model\"\"\"\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training stats\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_dataloader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"{model_name} - Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%\")\n",
    "    \n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "# Define optimizers\n",
    "standard_optimizer = AdamW(standard_model.parameters(), lr=0.001)\n",
    "lora_optimizer = AdamW(lora_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the standard model\n",
    "print(\"Training the standard model...\")\n",
    "standard_losses, standard_accuracies = train_model(\n",
    "    standard_model, \n",
    "    train_dataloader, \n",
    "    test_dataloader, \n",
    "    standard_optimizer, \n",
    "    num_epochs=10, \n",
    "    model_name=\"Standard Model\"\n",
    ")\n",
    "\n",
    "# Train the LoRA model\n",
    "print(\"\\nTraining the LoRA model...\")\n",
    "lora_losses, lora_accuracies = train_model(\n",
    "    lora_model, \n",
    "    train_dataloader, \n",
    "    test_dataloader, \n",
    "    lora_optimizer, \n",
    "    num_epochs=10, \n",
    "    model_name=\"LoRA Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training progress and compare the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(standard_losses, label='Standard Model')\n",
    "plt.plot(lora_losses, label='LoRA Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(standard_accuracies, label='Standard Model')\n",
    "plt.plot(lora_accuracies, label='LoRA Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring LoRA Hyperparameters\n",
    "\n",
    "Let's explore how different LoRA hyperparameters (rank and alpha) affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora_with_hyperparams(input_dim, hidden_dim, output_dim, rank, alpha, train_dataloader, test_dataloader):\n",
    "    \"\"\"Train a LoRA model with specified hyperparameters\"\"\"\n",
    "    model = SimpleNN_LoRA(input_dim, hidden_dim, output_dim, rank=rank, alpha=alpha, dropout=0.1).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Get parameter counts\n",
    "    trainable_params = count_parameters(model)\n",
    "    total_params = count_total_parameters(model)\n",
    "    param_percentage = trainable_params / count_parameters(standard_model) * 100\n",
    "    \n",
    "    print(f\"LoRA(rank={rank}, alpha={alpha}) - Trainable parameters: {trainable_params:,} ({param_percentage:.2f}%)\")\n",
    "    \n",
    "    # Train for fewer epochs to speed up experimentation\n",
    "    _, accuracies = train_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        test_dataloader, \n",
    "        optimizer, \n",
    "        num_epochs=5, \n",
    "        model_name=f\"LoRA(r={rank}, α={alpha})\"\n",
    "    )\n",
    "    \n",
    "    # Return the final accuracy and parameter percentage\n",
    "    return accuracies[-1], param_percentage\n",
    "\n",
    "# Define a range of ranks and alphas to explore\n",
    "ranks = [1, 2, 4, 8, 16, 32]\n",
    "alphas = [1, 4, 16, 32, 64]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train models with different hyperparameters\n",
    "for rank in ranks:\n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nTraining LoRA model with rank={rank}, alpha={alpha}...\")\n",
    "        accuracy, param_percentage = train_lora_with_hyperparams(\n",
    "            input_dim, hidden_dim, output_dim, rank, alpha, train_dataloader, test_dataloader\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'rank': rank,\n",
    "            'alpha': alpha,\n",
    "            'accuracy': accuracy,\n",
    "            'param_percentage': param_percentage\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results of our hyperparameter exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a pivot table for the heatmap\n",
    "pivot_acc = results_df.pivot(index='rank', columns='alpha', values='accuracy')\n",
    "\n",
    "# Plot heatmap of accuracy\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_acc, annot=True, fmt='.1f', cmap='viridis')\n",
    "plt.title('Test Accuracy (%) for Different LoRA Hyperparameters')\n",
    "plt.xlabel('Alpha (α)')\n",
    "plt.ylabel('Rank (r)')\n",
    "plt.show()\n",
    "\n",
    "# Plot parameter percentage vs. accuracy for different ranks\n",
    "plt.figure(figsize=(12, 6))\n",
    "for rank in ranks:\n",
    "    rank_data = results_df[results_df['rank'] == rank]\n",
    "    plt.plot(rank_data['param_percentage'], rank_data['accuracy'], marker='o', label=f'rank={rank}')\n",
    "\n",
    "plt.axhline(y=standard_accuracies[-1], color='r', linestyle='--', label='Standard Model')\n",
    "plt.xlabel('Parameter Percentage (%)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Parameter Efficiency vs. Accuracy for Different LoRA Ranks')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA for Transformer Models\n",
    "\n",
    "In practice, LoRA is most commonly applied to transformer models, particularly for fine-tuning large language models. Let's see how to apply LoRA to a transformer model using the `peft` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load a pre-trained BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "# Count parameters in the full model\n",
    "full_params = count_parameters(model)\n",
    "print(f\"Full BERT model - Trainable parameters: {full_params:,}\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank of the LoRA matrices\n",
    "    lora_alpha=16,  # scaling factor\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # which modules to apply LoRA to\n",
    "    lora_dropout=0.1,  # dropout probability\n",
    "    bias=\"none\",  # whether to train bias parameters\n",
    "    task_type=TaskType.SEQ_CLS  # task type (sequence classification)\n",
    ")\n",
    "\n",
    "# Create the LoRA model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count trainable parameters\n",
    "lora_params = count_parameters(lora_model)\n",
    "print(f\"LoRA BERT model - Trainable parameters: {lora_params:,} ({lora_params/full_params*100:.2f}%)\")\n",
    "print(f\"Parameter reduction: {(1 - lora_params/full_params)*100:.2f}%\")\n",
    "\n",
    "# Print the model structure to see the LoRA layers\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis of LoRA across Different Applications\n",
    "\n",
    "Based on the paper, let's analyze how LoRA performs across different application domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application domains and parameter percentages from the paper\n",
    "applications = [\n",
    "    'Commonsense Reasoning', \n",
    "    'Arithmetic Reasoning',\n",
    "    'Video Text Generation',\n",
    "    'Medical Imaging',\n",
    "    'Protein Models',\n",
    "    'Code Review',\n",
    "    'Speech Synthesis'\n",
    "]\n",
    "\n",
    "# Parameter percentages for LoRA (from the paper)\n",
    "lora_percentages = [0.83, 0.83, 0.81, 0.81, 0.81, 1.0, 0.8]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(applications, lora_percentages, color='skyblue')\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', label='1% threshold')\n",
    "plt.title('LoRA Parameter Percentages Across Different Applications')\n",
    "plt.xlabel('Application Domain')\n",
    "plt.ylabel('Parameters (% of full model)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 2.0)  # Set a reasonable y-axis limit\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add data labels on the bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{height:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Other PEFT Methods\n",
    "\n",
    "Let's visualize the comparison of LoRA with other PEFT methods mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT methods and their parameter percentages (from Table 1 and 2 in the paper)\n",
    "methods = ['PrefT', 'AdapterS', 'AdapterP', 'LoRA', 'DoRA (half)', 'DoRA', 'LoReFT']\n",
    "param_percentages_7b = [0.110, 0.990, 3.540, 0.830, 0.430, 0.840, 0.031]  # For LLaMA-7B\n",
    "param_percentages_13b = [0.030, 0.800, 2.890, 0.670, 0.350, 0.680, 0.025]  # For LLaMA-13B\n",
    "\n",
    "# Create a bar chart for parameter percentages\n",
    "plt.figure(figsize=(14, 6))\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, param_percentages_7b, width, label='LLaMA-7B')\n",
    "plt.bar(x + width/2, param_percentages_13b, width, label='LLaMA-13B')\n",
    "\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', label='1% threshold')\n",
    "plt.title('Parameter Percentages of Different PEFT Methods')\n",
    "plt.xlabel('PEFT Method')\n",
    "plt.ylabel('Parameters (% of full model)')\n",
    "plt.xticks(x, methods)\n",
    "plt.yscale('log')  # Log scale to better visualize the differences\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Accuracies from Table 1 in the paper (Commonsense Reasoning)\n",
    "accuracies_7b = [64.6, 70.8, 72.3, 74.7, 77.5, 78.1, 80.2]  # For LLaMA-7B\n",
    "accuracies_13b = [68.4, 79.5, 81.5, 80.5, 80.8, 81.5, 83.3]  # For LLaMA-13B\n",
    "\n",
    "# Create a bar chart for accuracies\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width/2, accuracies_7b, width, label='LLaMA-7B')\n",
    "plt.bar(x + width/2, accuracies_13b, width, label='LLaMA-13B')\n",
    "\n",
    "plt.axhline(y=77.0, color='g', linestyle='--', label='ChatGPT')\n",
    "plt.title('Average Accuracy of Different PEFT Methods on Commonsense Reasoning')\n",
    "plt.xlabel('PEFT Method')\n",
    "plt.ylabel('Average Accuracy (%)')\n",
    "plt.xticks(x, methods)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of parameter percentage vs. accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(param_percentages_7b, accuracies_7b, s=100, marker='o', label='LLaMA-7B')\n",
    "plt.scatter(param_percentages_13b, accuracies_13b, s=100, marker='s', label='LLaMA-13B')\n",
    "\n",
    "# Annotate each point with the method name\n",
    "for i, method in enumerate(methods):\n",
    "    plt.annotate(method, (param_percentages_7b[i], accuracies_7b[i]), \n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.annotate(method, (param_percentages_13b[i], accuracies_13b[i]), \n",
    "                 textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
    "\n",
    "plt.axhline(y=77.0, color='g', linestyle='--', label='ChatGPT')\n",
    "plt.title('Parameter Efficiency vs. Accuracy for Different PEFT Methods')\n",
    "plt.xlabel('Parameters (% of full model)')\n",
    "plt.ylabel('Average Accuracy (%)')\n",
    "plt.xscale('log')  # Log scale for better visualization\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Key Takeaways\n",
    "\n",
    "From our experiments and analysis, we can draw several key insights about LoRA and its applications:\n",
    "\n",
    "1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters (typically to less than 1% of the full model), which leads to lower memory usage and faster training.\n",
    "\n",
    "2. **Performance Trade-off**: Despite the drastic reduction in trainable parameters, LoRA can achieve performance comparable to full fine-tuning in many cases, especially when the rank and scaling factor are appropriately chosen.\n",
    "\n",
    "3. **Hyperparameter Sensitivity**: The performance of LoRA depends on the choice of hyperparameters, particularly the rank (r) and scaling factor (alpha). Higher ranks generally lead to better performance but at the cost of more parameters.\n",
    "\n",
    "4. **Application Versatility**: As shown in the paper, LoRA has been successfully applied across various domains, including commonsense reasoning, medical imaging, protein modeling, and speech synthesis, with consistently low parameter overhead (around 0.8-1%).\n",
    "\n",
    "5. **Comparative Advantage**: While newer methods like LoReFT may achieve better parameter efficiency in some tasks, LoRA offers a good balance between performance and parameter efficiency across a wide range of applications.\n",
    "\n",
    "6. **Integration with Transformers**: LoRA is particularly well-suited for transformer-based models, as it can be selectively applied to specific attention modules (query, key, value), further optimizing the parameter-performance trade-off.\n",
    "\n",
    "These insights align with the paper's conclusion that LoRA is one of the most effective PEFT methods, offering a good balance between computational efficiency and performance across diverse applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Balne, C. C. S., Bhaduri, S., Roy, T., Jain, V., & Chadha, A. (2024). Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications. arXiv:2404.13506v2.\n",
    "\n",
    "2. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\n",
    "\n",
    "3. Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). ReFT: Representation finetuning for language models. arXiv preprint arXiv:2401.13622."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}