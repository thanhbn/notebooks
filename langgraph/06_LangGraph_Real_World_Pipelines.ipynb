{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_LangGraph_Real_World_Pipelines - Xây dựng Pipeline AI Thực tế\n",
    "\n",
    "## 🎯 Mục tiêu học tập\n",
    "\n",
    "Trong notebook này, chúng ta sẽ học cách:\n",
    "- Hiểu tại sao LangGraph phù hợp cho các pipeline AI phức tạp\n",
    "- Xây dựng pipeline phân tích và tóm tắt tài liệu (RAG nâng cao)\n",
    "- Tạo hệ thống hỗ trợ quyết định với nhiều bước tương tác\n",
    "- Quản lý trạng thái và xử lý lỗi trong pipeline thực tế\n",
    "- Áp dụng conditional logic và branching phức tạp\n",
    "\n",
    "## 📋 Tổng quan\n",
    "\n",
    "### Tại sao LangGraph cho Pipeline AI?\n",
    "\n",
    "**LangGraph vs Chains đơn giản:**\n",
    "- **Quản lý trạng thái**: Theo dõi thông tin qua nhiều bước\n",
    "- **Điều hướng có điều kiện**: Rẽ nhánh dựa trên kết quả trung gian\n",
    "- **Khả năng mở rộng**: Dễ dàng thêm/bớt các bước xử lý\n",
    "- **Debugging**: Theo dõi và kiểm soát từng bước thực thi\n",
    "- **Error handling**: Xử lý lỗi và retry logic tốt hơn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Cài đặt & Cấu hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra và cài đặt các thư viện cần thiết\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✅ {package} đã được cài đặt\")\n",
    "    except ImportError:\n",
    "        print(f\"⚠️ Đang cài đặt {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Cài đặt các thư viện cần thiết\n",
    "packages = [\n",
    "    \"langgraph\",\n",
    "    \"langchain-anthropic\", \n",
    "    \"langchain-community\",\n",
    "    \"langchain-core\",\n",
    "    \"requests\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"python-dotenv\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Kiểm tra API key\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"⚠️ Vui lòng đặt ANTHROPIC_API_KEY trong file .env\")\n",
    "    ANTHROPIC_API_KEY = input(\"Nhập ANTHROPIC_API_KEY: \")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "else:\n",
    "    print(\"✅ ANTHROPIC_API_KEY đã được cấu hình\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "from typing import TypedDict, List, Dict, Any, Optional, Annotated\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"✅ Đã import thành công tất cả thư viện cần thiết\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Setup LLM và Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "print(\"✅ Đã khởi tạo ChatAnthropic\")\n",
    "\n",
    "# Test kết nối\n",
    "try:\n",
    "    test_response = llm.invoke([HumanMessage(content=\"Xin chào, bạn có thể trả lời bằng tiếng Việt không?\")])\n",
    "    print(f\"🔗 Kết nối thành công: {test_response.content[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi kết nối: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions cho document processing\n",
    "def extract_text_from_url(url: str) -> str:\n",
    "    \"\"\"Trích xuất văn bản từ URL\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Loại bỏ script và style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # Lấy text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Làm sạch text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text[:5000]  # Giới hạn để tránh quá dài\n",
    "    except Exception as e:\n",
    "        return f\"Lỗi khi trích xuất từ {url}: {str(e)}\"\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Chia text thành các đoạn nhỏ với overlap\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Tìm điểm ngắt tự nhiên (dấu câu hoặc khoảng trắng)\n",
    "        if end < len(text):\n",
    "            for i in range(end, max(start + chunk_size - 200, start), -1):\n",
    "                if text[i] in '.!?\\n':\n",
    "                    end = i + 1\n",
    "                    break\n",
    "                elif text[i] == ' ':\n",
    "                    end = i\n",
    "                    break\n",
    "        \n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap\n",
    "        \n",
    "        if start >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"✅ Đã định nghĩa utility functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Pipeline 1: Phân tích & Tóm tắt Tài liệu (RAG Nâng cao)\n",
    "\n",
    "Pipeline này sẽ:\n",
    "1. **Tải tài liệu** từ URL hoặc text\n",
    "2. **Chia đoạn** thông minh\n",
    "3. **Truy xuất thông tin** liên quan\n",
    "4. **Đánh giá độ liên quan** của thông tin\n",
    "5. **Tóm tắt** hoặc **trả lời câu hỏi** dựa trên thông tin đã lọc\n",
    "6. **Xử lý feedback** và cải thiện kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa State cho Document Analysis Pipeline\n",
    "class DocumentAnalysisState(TypedDict):\n",
    "    # Input\n",
    "    query: str\n",
    "    document_source: str  # URL hoặc text\n",
    "    \n",
    "    # Intermediate states\n",
    "    raw_content: str\n",
    "    chunks: List[str]\n",
    "    relevant_chunks: List[str]\n",
    "    relevance_scores: List[float]\n",
    "    \n",
    "    # Output\n",
    "    analysis_result: str\n",
    "    confidence_score: float\n",
    "    suggestions: List[str]\n",
    "    \n",
    "    # Control flow\n",
    "    needs_refinement: bool\n",
    "    error_message: Optional[str]\n",
    "    step_count: int\n",
    "\n",
    "print(\"✅ Đã định nghĩa DocumentAnalysisState\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node functions cho Document Analysis Pipeline\n",
    "\n",
    "def load_document(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"Tải tài liệu từ nguồn\"\"\"\n",
    "    print(f\"📄 Đang tải tài liệu từ: {state['document_source'][:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        if state['document_source'].startswith(('http://', 'https://')):\n",
    "            content = extract_text_from_url(state['document_source'])\n",
    "        else:\n",
    "            content = state['document_source']\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'raw_content': content,\n",
    "            'error_message': None,\n",
    "            'step_count': state.get('step_count', 0) + 1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            'error_message': f\"Lỗi tải tài liệu: {str(e)}\",\n",
    "            'step_count': state.get('step_count', 0) + 1\n",
    "        }\n",
    "\n",
    "def chunk_document(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"Chia tài liệu thành các đoạn nhỏ\"\"\"\n",
    "    print(\"✂️ Đang chia tài liệu thành các đoạn...\")\n",
    "    \n",
    "    if state.get('error_message'):\n",
    "        return state\n",
    "    \n",
    "    chunks = chunk_text(state['raw_content'])\n",
    "    print(f\"📋 Đã tạo {len(chunks)} đoạn văn bản\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        'chunks': chunks,\n",
    "        'step_count': state['step_count'] + 1\n",
    "    }\n",
    "\n",
    "def retrieve_relevant_chunks(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"Truy xuất các đoạn liên quan đến query\"\"\"\n",
    "    print(f\"🔍 Đang tìm kiếm thông tin liên quan đến: '{state['query']}'\")\n",
    "    \n",
    "    if state.get('error_message'):\n",
    "        return state\n",
    "    \n",
    "    # Sử dụng LLM để đánh giá mức độ liên quan\n",
    "    relevance_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Bạn là một chuyên gia đánh giá mức độ liên quan của thông tin.\n",
    "        \n",
    "Query: {query}\n",
    "Đoạn văn bản: {chunk}\n",
    "\n",
    "Hãy đánh giá mức độ liên quan của đoạn văn bản này với query trên thang điểm từ 0 đến 1.\n",
    "Chỉ trả về một số thập phân từ 0.0 đến 1.0, không giải thích thêm.\n",
    "Ví dụ: 0.8\"\"\"\n",
    "    )\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    relevance_scores = []\n",
    "    \n",
    "    for chunk in state['chunks']:\n",
    "        try:\n",
    "            score_response = llm.invoke(\n",
    "                relevance_prompt.format_messages(query=state['query'], chunk=chunk[:500])\n",
    "            )\n",
    "            \n",
    "            # Trích xuất điểm số\n",
    "            score_text = score_response.content.strip()\n",
    "            score = float(re.findall(r'\\d+\\.\\d+|\\d+', score_text)[0])\n",
    "            \n",
    "            if score > 0.3:  # Ngưỡng liên quan\n",
    "                relevant_chunks.append(chunk)\n",
    "                relevance_scores.append(score)\n",
    "                \n",
    "        except (ValueError, IndexError):\n",
    "            # Nếu không thể parse điểm số, bỏ qua chunk này\n",
    "            continue\n",
    "    \n",
    "    # Sắp xếp theo điểm số giảm dần\n",
    "    sorted_pairs = sorted(zip(relevant_chunks, relevance_scores), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if sorted_pairs:\n",
    "        relevant_chunks, relevance_scores = zip(*sorted_pairs)\n",
    "        relevant_chunks = list(relevant_chunks[:5])  # Top 5 chunks\n",
    "        relevance_scores = list(relevance_scores[:5])\n",
    "    else:\n",
    "        relevant_chunks = []\n",
    "        relevance_scores = []\n",
    "    \n",
    "    print(f\"✅ Tìm được {len(relevant_chunks)} đoạn liên quan\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        'relevant_chunks': relevant_chunks,\n",
    "        'relevance_scores': relevance_scores,\n",
    "        'step_count': state['step_count'] + 1\n",
    "    }\n",
    "\n",
    "def analyze_and_summarize(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"Phân tích và tóm tắt thông tin\"\"\"\n",
    "    print(\"🧠 Đang phân tích và tóm tắt thông tin...\")\n",
    "    \n",
    "    if state.get('error_message'):\n",
    "        return state\n",
    "    \n",
    "    if not state['relevant_chunks']:\n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_result': \"Không tìm thấy thông tin liên quan đến câu hỏi của bạn trong tài liệu.\",\n",
    "            'confidence_score': 0.0,\n",
    "            'suggestions': [\"Thử với câu hỏi khác\", \"Kiểm tra lại tài liệu nguồn\"],\n",
    "            'needs_refinement': True,\n",
    "            'step_count': state['step_count'] + 1\n",
    "        }\n",
    "    \n",
    "    # Tạo context từ các chunks liên quan\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Đoạn {i+1} (độ liên quan: {score:.2f}):\\n{chunk}\"\n",
    "        for i, (chunk, score) in enumerate(zip(state['relevant_chunks'], state['relevance_scores']))\n",
    "    ])\n",
    "    \n",
    "    analysis_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Bạn là một chuyên gia phân tích tài liệu. Hãy phân tích và trả lời câu hỏi dựa trên thông tin được cung cấp.\n",
    "\n",
    "Câu hỏi: {query}\n",
    "\n",
    "Thông tin từ tài liệu:\n",
    "{context}\n",
    "\n",
    "Hãy:\n",
    "1. Trả lời câu hỏi một cách chi tiết và chính xác\n",
    "2. Trích dẫn thông tin từ tài liệu\n",
    "3. Đánh giá độ tin cậy của câu trả lời (0-1)\n",
    "4. Đưa ra 2-3 gợi ý để tìm hiểu thêm\n",
    "\n",
    "Định dạng trả lời:\n",
    "**Câu trả lời:**\n",
    "[Câu trả lời chi tiết]\n",
    "\n",
    "**Độ tin cậy:** [0.0-1.0]\n",
    "\n",
    "**Gợi ý thêm:**\n",
    "- [Gợi ý 1]\n",
    "- [Gợi ý 2]\n",
    "- [Gợi ý 3]\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            analysis_prompt.format_messages(query=state['query'], context=context)\n",
    "        )\n",
    "        \n",
    "        result = response.content\n",
    "        \n",
    "        # Trích xuất confidence score\n",
    "        confidence_match = re.search(r'\\*\\*Độ tin cậy:\\*\\*\\s*([0-9.]+)', result)\n",
    "        confidence_score = float(confidence_match.group(1)) if confidence_match else 0.5\n",
    "        \n",
    "        # Trích xuất suggestions\n",
    "        suggestions_section = re.search(r'\\*\\*Gợi ý thêm:\\*\\*\\s*(.+)', result, re.DOTALL)\n",
    "        suggestions = []\n",
    "        if suggestions_section:\n",
    "            suggestion_lines = suggestions_section.group(1).strip().split('\\n')\n",
    "            suggestions = [line.strip('- ').strip() for line in suggestion_lines if line.strip().startswith('-')]\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_result': result,\n",
    "            'confidence_score': confidence_score,\n",
    "            'suggestions': suggestions,\n",
    "            'needs_refinement': confidence_score < 0.6,\n",
    "            'step_count': state['step_count'] + 1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            'error_message': f\"Lỗi phân tích: {str(e)}\",\n",
    "            'step_count': state['step_count'] + 1\n",
    "        }\n",
    "\n",
    "print(\"✅ Đã định nghĩa các node functions cho Document Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo Document Analysis Graph\n",
    "def create_document_analysis_graph():\n",
    "    \"\"\"Tạo workflow graph cho phân tích tài liệu\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(DocumentAnalysisState)\n",
    "    \n",
    "    # Thêm các nodes\n",
    "    workflow.add_node(\"load_document\", load_document)\n",
    "    workflow.add_node(\"chunk_document\", chunk_document)\n",
    "    workflow.add_node(\"retrieve_relevant\", retrieve_relevant_chunks)\n",
    "    workflow.add_node(\"analyze_summarize\", analyze_and_summarize)\n",
    "    \n",
    "    # Định nghĩa flow\n",
    "    workflow.add_edge(START, \"load_document\")\n",
    "    \n",
    "    # Conditional edge sau load_document\n",
    "    def should_continue_after_load(state: DocumentAnalysisState) -> str:\n",
    "        if state.get('error_message'):\n",
    "            return END\n",
    "        return \"chunk_document\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"load_document\",\n",
    "        should_continue_after_load,\n",
    "        {\"chunk_document\": \"chunk_document\", END: END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"chunk_document\", \"retrieve_relevant\")\n",
    "    workflow.add_edge(\"retrieve_relevant\", \"analyze_summarize\")\n",
    "    \n",
    "    # Conditional edge sau analyze_summarize\n",
    "    def should_refine(state: DocumentAnalysisState) -> str:\n",
    "        if state.get('needs_refinement') and state.get('step_count', 0) < 5:\n",
    "            return \"retrieve_relevant\"  # Retry với threshold thấp hơn\n",
    "        return END\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze_summarize\",\n",
    "        should_refine,\n",
    "        {\"retrieve_relevant\": \"retrieve_relevant\", END: END}\n",
    "    )\n",
    "    \n",
    "    # Compile graph\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Tạo graph\n",
    "doc_analysis_app = create_document_analysis_graph()\n",
    "print(\"✅ Đã tạo Document Analysis Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Document Analysis Pipeline\n",
    "def test_document_analysis():\n",
    "    \"\"\"Test pipeline phân tích tài liệu\"\"\"\n",
    "    \n",
    "    # Sample document\n",
    "    sample_doc = \"\"\"\n",
    "    Trí tuệ nhân tạo (AI) đang thay đổi cách chúng ta làm việc và sống. \n",
    "    Machine Learning là một nhánh quan trọng của AI, cho phép máy tính học từ dữ liệu mà không cần lập trình cụ thể.\n",
    "    \n",
    "    Deep Learning, một phần của Machine Learning, sử dụng mạng neural nhân tạo để xử lý dữ liệu phức tạp.\n",
    "    Các ứng dụng của AI bao gồm:\n",
    "    - Xử lý ngôn ngữ tự nhiên (NLP)\n",
    "    - Computer Vision\n",
    "    - Robotics\n",
    "    - Autonomous vehicles\n",
    "    \n",
    "    LangChain là một framework mạnh mẽ để xây dựng ứng dụng AI sử dụng Large Language Models (LLMs).\n",
    "    LangGraph mở rộng LangChain bằng cách cung cấp khả năng tạo workflow phức tạp với state management.\n",
    "    \n",
    "    Trong tương lai, AI sẽ tiếp tục phát triển và tích hợp sâu hơn vào cuộc sống hàng ngày.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"LangGraph là gì và có tác dụng gì?\",\n",
    "            \"document_source\": sample_doc\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Các ứng dụng của AI có gì?\",\n",
    "            \"document_source\": sample_doc\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🧪 TEST CASE {i+1}: {test_case['query']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Tạo initial state\n",
    "        initial_state = {\n",
    "            \"query\": test_case['query'],\n",
    "            \"document_source\": test_case['document_source'],\n",
    "            \"step_count\": 0,\n",
    "            \"needs_refinement\": False\n",
    "        }\n",
    "        \n",
    "        # Chạy pipeline\n",
    "        config = {\"configurable\": {\"thread_id\": f\"test_{i}\"}}\n",
    "        \n",
    "        try:\n",
    "            # Execute graph\n",
    "            result = doc_analysis_app.invoke(initial_state, config)\n",
    "            \n",
    "            # Display results\n",
    "            if result.get('error_message'):\n",
    "                print(f\"❌ Lỗi: {result['error_message']}\")\n",
    "            else:\n",
    "                print(f\"📊 Kết quả phân tích:\")\n",
    "                print(f\"📈 Độ tin cậy: {result.get('confidence_score', 0):.2f}\")\n",
    "                print(f\"📝 Kết quả:\\n{result.get('analysis_result', 'Không có kết quả')}\")\n",
    "                \n",
    "                if result.get('suggestions'):\n",
    "                    print(f\"\\n💡 Gợi ý:\")\n",
    "                    for suggestion in result['suggestions']:\n",
    "                        print(f\"   • {suggestion}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Lỗi thực thi: {str(e)}\")\n",
    "\n",
    "# Chạy test\n",
    "test_document_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Pipeline 2: Hệ thống Hỗ trợ Quyết định (Decision Support Agent)\n",
    "\n",
    "Pipeline này sẽ:\n",
    "1. **Phân tích yêu cầu** của người dùng\n",
    "2. **Tìm kiếm thông tin** từ nhiều nguồn\n",
    "3. **Phân tích dữ liệu** và so sánh các lựa chọn\n",
    "4. **Đề xuất giải pháp** với lý do\n",
    "5. **Tương tác với người dùng** để làm rõ\n",
    "6. **Đưa ra quyết định cuối cùng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa State cho Decision Support Pipeline\n",
    "class DecisionSupportState(TypedDict):\n",
    "    # Input\n",
    "    user_request: str\n",
    "    context: Dict[str, Any]  # Thông tin bổ sung từ user\n",
    "    \n",
    "    # Analysis phase\n",
    "    request_type: str  # \"comparison\", \"recommendation\", \"analysis\", \"planning\"\n",
    "    key_factors: List[str]\n",
    "    information_needs: List[str]\n",
    "    \n",
    "    # Research phase\n",
    "    collected_info: Dict[str, Any]\n",
    "    sources: List[str]\n",
    "    \n",
    "    # Decision phase\n",
    "    options: List[Dict[str, Any]]\n",
    "    analysis_results: Dict[str, Any]\n",
    "    recommendations: List[str]\n",
    "    \n",
    "    # Interaction phase\n",
    "    clarifications_needed: List[str]\n",
    "    user_feedback: Optional[str]\n",
    "    \n",
    "    # Output\n",
    "    final_decision: str\n",
    "    reasoning: str\n",
    "    confidence_level: float\n",
    "    next_steps: List[str]\n",
    "    \n",
    "    # Control\n",
    "    needs_clarification: bool\n",
    "    is_complete: bool\n",
    "    iteration_count: int\n",
    "\n",
    "print(\"✅ Đã định nghĩa DecisionSupportState\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node functions cho Decision Support Pipeline\n",
    "\n",
    "def analyze_request(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Phân tích yêu cầu của người dùng\"\"\"\n",
    "    print(f\"🔍 Đang phân tích yêu cầu: {state['user_request'][:100]}...\")\n",
    "    \n",
    "    analysis_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Bạn là một chuyên gia phân tích yêu cầu. Hãy phân tích yêu cầu sau và trả lời theo định dạng JSON.\n",
    "\n",
    "Yêu cầu: {request}\n",
    "Context: {context}\n",
    "\n",
    "Hãy xác định:\n",
    "1. Loại yêu cầu (comparison/recommendation/analysis/planning)\n",
    "2. Các yếu tố quan trọng cần xem xét\n",
    "3. Thông tin cần thu thập\n",
    "\n",
    "Trả lời theo định dạng JSON:\n",
    "{{\n",
    "    \"request_type\": \"loại yêu cầu\",\n",
    "    \"key_factors\": [\"yếu tố 1\", \"yếu tố 2\", \"yếu tố 3\"],\n",
    "    \"information_needs\": [\"thông tin cần 1\", \"thông tin cần 2\"]\n",
    "}}\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            analysis_prompt.format_messages(\n",
    "                request=state['user_request'],\n",
    "                context=json.dumps(state.get('context', {}), ensure_ascii=False)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result_text = response.content.strip()\n",
    "        json_match = re.search(r'\\{[^}]+\\}', result_text, re.DOTALL)\n",
    "        \n",
    "        if json_match:\n",
    "            result_json = json.loads(json_match.group())\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                'request_type': result_json.get('request_type', 'analysis'),\n",
    "                'key_factors': result_json.get('key_factors', []),\n",
    "                'information_needs': result_json.get('information_needs', []),\n",
    "                'iteration_count': state.get('iteration_count', 0) + 1\n",
    "            }\n",
    "        else:\n",
    "            # Fallback nếu không parse được JSON\n",
    "            return {\n",
    "                **state,\n",
    "                'request_type': 'analysis',\n",
    "                'key_factors': ['Chất lượng', 'Chi phí', 'Thời gian'],\n",
    "                'information_needs': ['Thông tin cơ bản', 'So sánh lựa chọn'],\n",
    "                'iteration_count': state.get('iteration_count', 0) + 1\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi phân tích yêu cầu: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            'request_type': 'analysis',\n",
    "            'key_factors': ['Yếu tố chính'],\n",
    "            'information_needs': ['Thông tin cần thiết'],\n",
    "            'iteration_count': state.get('iteration_count', 0) + 1\n",
    "        }\n",
    "\n",
    "def collect_information(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Thu thập thông tin cần thiết\"\"\"\n",
    "    print(f\"📊 Đang thu thập thông tin cho {len(state['information_needs'])} nhu cầu\")\n",
    "    \n",
    "    # Mô phỏng việc thu thập thông tin từ nhiều nguồn\n",
    "    collected_info = {}\n",
    "    sources = []\n",
    "    \n",
    "    research_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Bạn là một nhà nghiên cứu chuyên nghiệp. Hãy cung cấp thông tin chi tiết về:\n",
    "\n",
    "Chủ đề nghiên cứu: {topic}\n",
    "Yêu cầu gốc: {original_request}\n",
    "Yếu tố quan trọng: {key_factors}\n",
    "\n",
    "Hãy cung cấp:\n",
    "1. Thông tin chi tiết và cập nhật\n",
    "2. Ưu điểm và nhược điểm\n",
    "3. Các lựa chọn có thể\n",
    "4. Đánh giá dựa trên các yếu tố quan trọng\n",
    "\n",
    "Định dạng trả lời:\n",
    "**Thông tin tổng quan:**\n",
    "[Thông tin chi tiết]\n",
    "\n",
    "**Các lựa chọn:**\n",
    "1. [Lựa chọn 1]: [Mô tả và đánh giá]\n",
    "2. [Lựa chọn 2]: [Mô tả và đánh giá]\n",
    "\n",
    "**Phân tích theo yếu tố:**\n",
    "[Phân tích dựa trên key_factors]\"\"\"\n",
    "    )\n",
    "    \n",
    "    for info_need in state['information_needs']:\n",
    "        try:\n",
    "            response = llm.invoke(\n",
    "                research_prompt.format_messages(\n",
    "                    topic=info_need,\n",
    "                    original_request=state['user_request'],\n",
    "                    key_factors=', '.join(state['key_factors'])\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            collected_info[info_need] = response.content\n",
    "            sources.append(f\"Nghiên cứu nội bộ - {info_need}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Lỗi thu thập thông tin cho {info_need}: {e}\")\n",
    "            collected_info[info_need] = f\"Không thể thu thập thông tin cho {info_need}\"\n",
    "    \n",
    "    print(f\"✅ Đã thu thập thông tin từ {len(sources)} nguồn\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        'collected_info': collected_info,\n",
    "        'sources': sources,\n",
    "        'iteration_count': state['iteration_count'] + 1\n",
    "    }\n",
    "\n",
    "def analyze_and_recommend(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Phân tích thông tin và đưa ra khuyến nghị\"\"\"\n",
    "    print(\"🧠 Đang phân tích thông tin và tạo khuyến nghị...\")\n",
    "    \n",
    "    # Tổng hợp thông tin\n",
    "    all_info = \"\\n\\n\".join([\n",
    "        f\"**{topic}:**\\n{info}\"\n",
    "        for topic, info in state['collected_info'].items()\n",
    "    ])\n",
    "    \n",
    "    recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Bạn là một chuyên gia tư vấn quyết định. Dựa trên thông tin đã thu thập, hãy đưa ra phân tích và khuyến nghị.\n",
    "\n",
    "Yêu cầu gốc: {request}\n",
    "Loại yêu cầu: {request_type}\n",
    "Yếu tố quan trọng: {key_factors}\n",
    "\n",
    "Thông tin đã thu thập:\n",
    "{collected_info}\n",
    "\n",
    "Hãy cung cấp:\n",
    "1. Phân tích chi tiết các lựa chọn\n",
    "2. So sánh ưu nhược điểm\n",
    "3. Khuyến nghị cụ thể với lý do\n",
    "4. Đánh giá độ tin cậy (0-1)\n",
    "5. Các câu hỏi cần làm rõ thêm (nếu có)\n",
    "\n",
    "Định dạng:\n",
    "**PHÂN TÍCH:**\n",
    "[Phân tích chi tiết]\n",
    "\n",
    "**KHUYẾN NGHỊ:**\n",
    "[Khuyến nghị cụ thể]\n",
    "\n",
    "**LÝ DO:**\n",
    "[Lý do cho khuyến nghị]\n",
    "\n",
    "**ĐỘ TIN CẬY:** [0.0-1.0]\n",
    "\n",
    "**CÂU HỎI LÀM RÕ:**\n",
    "- [Câu hỏi 1]\n",
    "- [Câu hỏi 2]\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            recommendation_prompt.format_messages(\n",
    "                request=state['user_request'],\n",
    "                request_type=state['request_type'],\n",
    "                key_factors=', '.join(state['key_factors']),\n",
    "                collected_info=all_info\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result = response.content\n",
    "        \n",
    "        # Trích xuất confidence level\n",
    "        confidence_match = re.search(r'\\*\\*ĐỘ TIN CẬY:\\*\\*\\s*([0-9.]+)', result)\n",
    "        confidence_level = float(confidence_match.group(1)) if confidence_match else 0.7\n",
    "        \n",
    "        # Trích xuất clarifications\n",
    "        clarification_section = re.search(r'\\*\\*CÂU HỎI LÀM RÕ:\\*\\*\\s*(.+)', result, re.DOTALL)\n",
    "        clarifications = []\n",
    "        if clarification_section:\n",
    "            clarification_lines = clarification_section.group(1).strip().split('\\n')\n",
    "            clarifications = [line.strip('- ').strip() for line in clarification_lines if line.strip().startswith('-')]\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_results': {'full_analysis': result},\n",
    "            'confidence_level': confidence_level,\n",
    "            'clarifications_needed': clarifications,\n",
    "            'needs_clarification': len(clarifications) > 0 and confidence_level < 0.8,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi phân tích: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_results': {'error': str(e)},\n",
    "            'confidence_level': 0.3,\n",
    "            'needs_clarification': True,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "\n",
    "def finalize_decision(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Hoàn thiện quyết định cuối cùng\"\"\"\n",
    "    print(\"✅ Đang hoàn thiện quyết định cuối cùng...\")\n",
    "    \n",
    "    final_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Dựa trên phân tích đã thực hiện, hãy đưa ra quyết định cuối cùng và kế hoạch hành động.\n",
    "\n",
    "Yêu cầu gốc: {request}\n",
    "Phân tích đã thực hiện: {analysis}\n",
    "Độ tin cậy: {confidence}\n",
    "\n",
    "Hãy cung cấp:\n",
    "1. Quyết định cuối cùng rõ ràng\n",
    "2. Lý do chi tiết\n",
    "3. Các bước tiếp theo cần thực hiện\n",
    "\n",
    "Định dạng:\n",
    "**QUYẾT ĐỊNH CUỐI CÙNG:**\n",
    "[Quyết định cụ thể]\n",
    "\n",
    "**LÝ DO:**\n",
    "[Lý do chi tiết]\n",
    "\n",
    "**CÁC BƯỚC TIẾP THEO:**\n",
    "1. [Bước 1]\n",
    "2. [Bước 2]\n",
    "3. [Bước 3]\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            final_prompt.format_messages(\n",
    "                request=state['user_request'],\n",
    "                analysis=state['analysis_results'].get('full_analysis', 'Không có phân tích'),\n",
    "                confidence=state['confidence_level']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result = response.content\n",
    "        \n",
    "        # Trích xuất next steps\n",
    "        steps_section = re.search(r'\\*\\*CÁC BƯỚC TIẾP THEO:\\*\\*\\s*(.+)', result, re.DOTALL)\n",
    "        next_steps = []\n",
    "        if steps_section:\n",
    "            step_lines = steps_section.group(1).strip().split('\\n')\n",
    "            next_steps = [re.sub(r'^\\d+\\.\\s*', '', line.strip()) for line in step_lines if re.match(r'^\\d+\\.', line.strip())]\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'final_decision': result,\n",
    "            'reasoning': result,\n",
    "            'next_steps': next_steps,\n",
    "            'is_complete': True,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            'final_decision': f\"Lỗi hoàn thiện quyết định: {str(e)}\",\n",
    "            'is_complete': True,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "\n",
    "print(\"✅ Đã định nghĩa các node functions cho Decision Support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo Decision Support Graph\n",
    "def create_decision_support_graph():\n",
    "    \"\"\"Tạo workflow graph cho hỗ trợ quyết định\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(DecisionSupportState)\n",
    "    \n",
    "    # Thêm các nodes\n",
    "    workflow.add_node(\"analyze_request\", analyze_request)\n",
    "    workflow.add_node(\"collect_info\", collect_information)\n",
    "    workflow.add_node(\"analyze_recommend\", analyze_and_recommend)\n",
    "    workflow.add_node(\"finalize\", finalize_decision)\n",
    "    \n",
    "    # Định nghĩa flow\n",
    "    workflow.add_edge(START, \"analyze_request\")\n",
    "    workflow.add_edge(\"analyze_request\", \"collect_info\")\n",
    "    workflow.add_edge(\"collect_info\", \"analyze_recommend\")\n",
    "    \n",
    "    # Conditional edge sau analyze_recommend\n",
    "    def should_clarify(state: DecisionSupportState) -> str:\n",
    "        if (state.get('needs_clarification') and \n",
    "            state.get('iteration_count', 0) < 3):\n",
    "            return \"collect_info\"  # Collect more info\n",
    "        return \"finalize\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze_recommend\",\n",
    "        should_clarify,\n",
    "        {\"collect_info\": \"collect_info\", \"finalize\": \"finalize\"}\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"finalize\", END)\n",
    "    \n",
    "    # Compile graph\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Tạo graph\n",
    "decision_support_app = create_decision_support_graph()\n",
    "print(\"✅ Đã tạo Decision Support Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Decision Support Pipeline\n",
    "def test_decision_support():\n",
    "    \"\"\"Test pipeline hỗ trợ quyết định\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"user_request\": \"Tôi muốn học một ngôn ngữ lập trình mới để phát triển ứng dụng AI. Nên chọn Python hay JavaScript?\",\n",
    "            \"context\": {\n",
    "                \"current_skills\": [\"HTML\", \"CSS\", \"basic programming\"],\n",
    "                \"goal\": \"AI application development\",\n",
    "                \"time_available\": \"3-6 months\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"user_request\": \"Công ty tôi cần chọn giữa việc xây dựng chatbot nội bộ hay thuê dịch vụ bên ngoài. Giúp tôi quyết định.\",\n",
    "            \"context\": {\n",
    "                \"company_size\": \"50-100 employees\",\n",
    "                \"budget\": \"moderate\",\n",
    "                \"technical_team\": \"limited\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🤖 DECISION SUPPORT TEST {i+1}\")\n",
    "        print(f\"📋 Yêu cầu: {test_case['user_request']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Tạo initial state\n",
    "        initial_state = {\n",
    "            \"user_request\": test_case['user_request'],\n",
    "            \"context\": test_case['context'],\n",
    "            \"iteration_count\": 0,\n",
    "            \"needs_clarification\": False,\n",
    "            \"is_complete\": False\n",
    "        }\n",
    "        \n",
    "        # Chạy pipeline\n",
    "        config = {\"configurable\": {\"thread_id\": f\"decision_test_{i}\"}}\n",
    "        \n",
    "        try:\n",
    "            # Execute graph\n",
    "            result = decision_support_app.invoke(initial_state, config)\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n📊 KẾT QUẢ PHÂN TÍCH:\")\n",
    "            print(f\"🎯 Loại yêu cầu: {result.get('request_type', 'N/A')}\")\n",
    "            print(f\"🔑 Yếu tố quan trọng: {', '.join(result.get('key_factors', []))}\")\n",
    "            print(f\"📈 Độ tin cậy: {result.get('confidence_level', 0):.2f}\")\n",
    "            \n",
    "            if result.get('final_decision'):\n",
    "                print(f\"\\n✅ QUYẾT ĐỊNH CUỐI CÙNG:\")\n",
    "                print(result['final_decision'])\n",
    "            \n",
    "            if result.get('next_steps'):\n",
    "                print(f\"\\n📋 CÁC BƯỚC TIẾP THEO:\")\n",
    "                for j, step in enumerate(result['next_steps'], 1):\n",
    "                    print(f\"   {j}. {step}\")\n",
    "            \n",
    "            if result.get('clarifications_needed'):\n",
    "                print(f\"\\n❓ CÂU HỎI CẦN LÀM RÕ:\")\n",
    "                for clarification in result['clarifications_needed']:\n",
    "                    print(f\"   • {clarification}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Lỗi thực thi: {str(e)}\")\n",
    "\n",
    "# Chạy test\n",
    "test_decision_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Phân tích Cấu trúc và Hiệu suất Pipeline\n",
    "\n",
    "### 🔄 Cấu trúc Graph\n",
    "\n",
    "Hãy phân tích cấu trúc của các pipeline chúng ta đã tạo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phân tích cấu trúc Graph\n",
    "def analyze_graph_structure():\n",
    "    \"\"\"Phân tích và hiển thị cấu trúc của các graph\"\"\"\n",
    "    \n",
    "    print(\"📊 PHÂN TÍCH CẤU TRÚC PIPELINE\\n\")\n",
    "    \n",
    "    # Document Analysis Pipeline\n",
    "    print(\"🔍 DOCUMENT ANALYSIS PIPELINE:\")\n",
    "    print(\"├── START\")\n",
    "    print(\"├── load_document (Tải tài liệu)\")\n",
    "    print(\"│   ├── Success → chunk_document\")\n",
    "    print(\"│   └── Error → END\")\n",
    "    print(\"├── chunk_document (Chia đoạn)\")\n",
    "    print(\"├── retrieve_relevant (Truy xuất liên quan)\")\n",
    "    print(\"├── analyze_summarize (Phân tích & tóm tắt)\")\n",
    "    print(\"│   ├── needs_refinement → retrieve_relevant (retry)\")\n",
    "    print(\"│   └── complete → END\")\n",
    "    print(\"└── END\\n\")\n",
    "    \n",
    "    # Decision Support Pipeline\n",
    "    print(\"🤖 DECISION SUPPORT PIPELINE:\")\n",
    "    print(\"├── START\")\n",
    "    print(\"├── analyze_request (Phân tích yêu cầu)\")\n",
    "    print(\"├── collect_info (Thu thập thông tin)\")\n",
    "    print(\"├── analyze_recommend (Phân tích & khuyến nghị)\")\n",
    "    print(\"│   ├── needs_clarification → collect_info (retry)\")\n",
    "    print(\"│   └── ready → finalize\")\n",
    "    print(\"├── finalize (Hoàn thiện quyết định)\")\n",
    "    print(\"└── END\\n\")\n",
    "    \n",
    "    # Key Features\n",
    "    print(\"🔑 TÍNH NĂNG QUAN TRỌNG:\")\n",
    "    print(\"• State Management: Theo dõi trạng thái qua các bước\")\n",
    "    print(\"• Conditional Routing: Rẽ nhánh dựa trên kết quả\")\n",
    "    print(\"• Error Handling: Xử lý lỗi và retry logic\")\n",
    "    print(\"• Memory Persistence: Lưu trạng thái với MemorySaver\")\n",
    "    print(\"• Iterative Refinement: Cải thiện kết quả qua nhiều lần thực hiện\")\n",
    "    \n",
    "    # Benefits\n",
    "    print(\"\\n✅ LỢI ÍCH CỦA LANGGRAPH:\")\n",
    "    print(\"• Modularity: Dễ dàng thêm/bớt/thay đổi các bước\")\n",
    "    print(\"• Debugging: Theo dõi từng bước thực thi\")\n",
    "    print(\"• Scalability: Mở rộng pipeline phức tạp\")\n",
    "    print(\"• Reliability: Xử lý lỗi và retry tốt hơn\")\n",
    "    print(\"• Flexibility: Thay đổi luồng dựa trên điều kiện\")\n",
    "\n",
    "analyze_graph_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Best Practices và Tối ưu hóa\n",
    "\n",
    "### Các nguyên tắc quan trọng khi xây dựng pipeline thực tế:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices cho LangGraph Pipelines\n",
    "def demonstrate_best_practices():\n",
    "    \"\"\"Minh họa các best practices\"\"\"\n",
    "    \n",
    "    print(\"🏆 BEST PRACTICES CHO LANGGRAPH PIPELINES\\n\")\n",
    "    \n",
    "    print(\"1. 📋 STATE DESIGN:\")\n",
    "    print(\"   • Sử dụng TypedDict cho type safety\")\n",
    "    print(\"   • Bao gồm control fields (error_message, step_count)\")\n",
    "    print(\"   • Thiết kế state tối thiểu nhưng đầy đủ\")\n",
    "    print(\"   • Sử dụng Optional cho các field không bắt buộc\\n\")\n",
    "    \n",
    "    print(\"2. 🔄 ERROR HANDLING:\")\n",
    "    print(\"   • Luôn có try-catch trong các node functions\")\n",
    "    print(\"   • Truyền error thông qua state\")\n",
    "    print(\"   • Conditional routing dựa trên error status\")\n",
    "    print(\"   • Graceful degradation khi gặp lỗi\\n\")\n",
    "    \n",
    "    print(\"3. 🎯 CONDITIONAL LOGIC:\")\n",
    "    print(\"   • Sử dụng conditional edges cho branching\")\n",
    "    print(\"   • Đặt điều kiện dựa trên business logic\")\n",
    "    print(\"   • Tránh vòng lặp vô hạn với iteration counters\")\n",
    "    print(\"   • Default paths cho các trường hợp edge cases\\n\")\n",
    "    \n",
    "    print(\"4. 🚀 PERFORMANCE:\")\n",
    "    print(\"   • Sử dụng MemorySaver cho persistent state\")\n",
    "    print(\"   • Batch processing khi có thể\")\n",
    "    print(\"   • Caching kết quả trung gian\")\n",
    "    print(\"   • Parallel execution cho independent tasks\\n\")\n",
    "    \n",
    "    print(\"5. 🧪 TESTING:\")\n",
    "    print(\"   • Test từng node function riêng biệt\")\n",
    "    print(\"   • Integration tests cho toàn bộ pipeline\")\n",
    "    print(\"   • Mock external dependencies\")\n",
    "    print(\"   • Test các edge cases và error conditions\\n\")\n",
    "    \n",
    "    print(\"6. 📊 MONITORING:\")\n",
    "    print(\"   • Log state changes tại mỗi node\")\n",
    "    print(\"   • Track execution time và performance\")\n",
    "    print(\"   • Monitor success/failure rates\")\n",
    "    print(\"   • Alert trên các lỗi quan trọng\")\n",
    "\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Debugging và Monitoring\n",
    "\n",
    "### Cách debug và monitor pipeline hiệu quả:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging utilities\n",
    "def create_debug_pipeline():\n",
    "    \"\"\"Tạo pipeline với debugging capabilities\"\"\"\n",
    "    \n",
    "    class DebugState(TypedDict):\n",
    "        input_data: str\n",
    "        processed_data: str\n",
    "        result: str\n",
    "        debug_info: Dict[str, Any]\n",
    "        execution_log: List[str]\n",
    "    \n",
    "    def debug_node_1(state: DebugState) -> DebugState:\n",
    "        \"\"\"Node với debugging\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Actual processing\n",
    "            processed = f\"Processed: {state['input_data']}\"\n",
    "            \n",
    "            # Debug info\n",
    "            end_time = datetime.now()\n",
    "            execution_time = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            debug_info = state.get('debug_info', {})\n",
    "            debug_info['node_1'] = {\n",
    "                'execution_time': execution_time,\n",
    "                'input_length': len(state['input_data']),\n",
    "                'output_length': len(processed),\n",
    "                'timestamp': start_time.isoformat()\n",
    "            }\n",
    "            \n",
    "            execution_log = state.get('execution_log', [])\n",
    "            execution_log.append(f\"Node 1 executed in {execution_time:.3f}s\")\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                'processed_data': processed,\n",
    "                'debug_info': debug_info,\n",
    "                'execution_log': execution_log\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_log = state.get('execution_log', [])\n",
    "            execution_log.append(f\"Node 1 failed: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                'debug_info': state.get('debug_info', {}),\n",
    "                'execution_log': execution_log\n",
    "            }\n",
    "    \n",
    "    def debug_node_2(state: DebugState) -> DebugState:\n",
    "        \"\"\"Node 2 với debugging\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result = f\"Final: {state['processed_data']}\"\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        debug_info = state.get('debug_info', {})\n",
    "        debug_info['node_2'] = {\n",
    "            'execution_time': execution_time,\n",
    "            'timestamp': start_time.isoformat()\n",
    "        }\n",
    "        \n",
    "        execution_log = state.get('execution_log', [])\n",
    "        execution_log.append(f\"Node 2 executed in {execution_time:.3f}s\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'result': result,\n",
    "            'debug_info': debug_info,\n",
    "            'execution_log': execution_log\n",
    "        }\n",
    "    \n",
    "    # Create graph\n",
    "    workflow = StateGraph(DebugState)\n",
    "    workflow.add_node(\"node_1\", debug_node_1)\n",
    "    workflow.add_node(\"node_2\", debug_node_2)\n",
    "    \n",
    "    workflow.add_edge(START, \"node_1\")\n",
    "    workflow.add_edge(\"node_1\", \"node_2\")\n",
    "    workflow.add_edge(\"node_2\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Test debug pipeline\n",
    "def test_debug_pipeline():\n",
    "    \"\"\"Test debug pipeline\"\"\"\n",
    "    print(\"🔍 TESTING DEBUG PIPELINE\\n\")\n",
    "    \n",
    "    debug_app = create_debug_pipeline()\n",
    "    \n",
    "    initial_state = {\n",
    "        \"input_data\": \"Test data for debugging\",\n",
    "        \"debug_info\": {},\n",
    "        \"execution_log\": []\n",
    "    }\n",
    "    \n",
    "    result = debug_app.invoke(initial_state)\n",
    "    \n",
    "    print(\"📊 DEBUG INFORMATION:\")\n",
    "    print(f\"Input: {result['input_data']}\")\n",
    "    print(f\"Result: {result['result']}\")\n",
    "    \n",
    "    print(\"\\n⏱️ EXECUTION LOG:\")\n",
    "    for log_entry in result['execution_log']:\n",
    "        print(f\"  • {log_entry}\")\n",
    "    \n",
    "    print(\"\\n🔧 DEBUG INFO:\")\n",
    "    for node, info in result['debug_info'].items():\n",
    "        print(f\"  {node}:\")\n",
    "        for key, value in info.items():\n",
    "            print(f\"    - {key}: {value}\")\n",
    "\n",
    "test_debug_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Tài liệu Tham khảo\n",
    "\n",
    "### Các nguồn tài liệu quan trọng để tìm hiểu thêm:\n",
    "\n",
    "1. **LangGraph Official Documentation**\n",
    "   - [LangGraph Introduction](https://langchain-ai.github.io/langgraph/)\n",
    "   - [How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/)\n",
    "   - [API Reference](https://langchain-ai.github.io/langgraph/reference/)\n",
    "\n",
    "2. **Advanced Patterns**\n",
    "   - [Conditional Edges](https://langchain-ai.github.io/langgraph/how-tos/branching/)\n",
    "   - [State Management](https://langchain-ai.github.io/langgraph/how-tos/state-model/)\n",
    "   - [Error Handling](https://langchain-ai.github.io/langgraph/how-tos/error-handling/)\n",
    "\n",
    "3. **Real-world Examples**\n",
    "   - [Multi-agent Systems](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/)\n",
    "   - [RAG Applications](https://langchain-ai.github.io/langgraph/tutorials/rag/)\n",
    "   - [Chatbot Implementation](https://langchain-ai.github.io/langgraph/tutorials/chatbots/)\n",
    "\n",
    "4. **Integration Guides**\n",
    "   - [LangSmith Integration](https://langchain-ai.github.io/langgraph/how-tos/langsmith/)\n",
    "   - [Anthropic Models](https://python.langchain.com/docs/integrations/chat/anthropic/)\n",
    "   - [Memory and Persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Kết luận và Bước tiếp theo\n",
    "\n",
    "### 📝 Tóm tắt những gì đã học:\n",
    "\n",
    "1. **Pipeline Architecture**: Cách thiết kế và xây dựng pipeline AI có cấu trúc\n",
    "2. **State Management**: Quản lý trạng thái phức tạp qua nhiều bước\n",
    "3. **Conditional Logic**: Rẽ nhánh và điều hướng dựa trên kết quả\n",
    "4. **Error Handling**: Xử lý lỗi và retry logic hiệu quả\n",
    "5. **Real-world Applications**: Ứng dụng thực tế với RAG và Decision Support\n",
    "\n",
    "### 🚀 Bước tiếp theo:\n",
    "\n",
    "1. **Thực hành**: Tự tạo pipeline cho use case riêng của bạn\n",
    "2. **Tối ưu hóa**: Áp dụng các best practices và monitoring\n",
    "3. **Mở rộng**: Thêm các tính năng như async processing, parallel execution\n",
    "4. **Production**: Deploy pipeline với proper infrastructure\n",
    "5. **Integration**: Tích hợp với các hệ thống khác trong organization\n",
    "\n",
    "### 💡 Gợi ý cho dự án tiếp theo:\n",
    "\n",
    "- **Content Generation Pipeline**: Multi-step content creation và review\n",
    "- **Data Analysis Pipeline**: Automated data processing và insights\n",
    "- **Customer Service Agent**: Intelligent routing và response system\n",
    "- **Research Assistant**: Automated research và report generation\n",
    "- **Code Review Pipeline**: Automated code analysis và suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final message\n",
    "print(\"🎉 HOÀN THÀNH NOTEBOOK 06 - LANGGRAPH REAL WORLD PIPELINES\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Bạn đã học được:\")\n",
    "print(\"   • Xây dựng pipeline RAG nâng cao\")\n",
    "print(\"   • Tạo hệ thống hỗ trợ quyết định\")\n",
    "print(\"   • Quản lý state và conditional logic\")\n",
    "print(\"   • Best practices và debugging\")\n",
    "print(\"\\n🚀 Hãy tiếp tục khám phá và xây dựng các pipeline phức tạp hơn!\")\n",
    "print(\"📚 Tham khảo tài liệu chính thức để tìm hiểu thêm các tính năng nâng cao.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}