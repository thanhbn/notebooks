{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning 2: Multilingual Embedding Models for Arabic Semantic Search\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Understand the architecture and training of multilingual embedding models\n",
    "- Analyze how different models handle Arabic language characteristics\n",
    "- Compare embedding dimensions and their impact on performance\n",
    "- Implement custom embedding analysis and visualization tools\n",
    "- Explore asymmetric vs symmetric semantic search scenarios\n",
    "\n",
    "## üìö Paper Context\n",
    "**Section 3.3.1**: \"The success of semantic search ranking relies significantly on the caliber of encoders used; higher-quality encoders produce more detailed embedding vectors, which in turn enable more accurate evaluations of similarity between search queries and documents.\"\n",
    "\n",
    "**Models Evaluated in Paper**:\n",
    "1. **Paraphrase Multilingual MiniLM** (384D) - Clustering and semantic search focused\n",
    "2. **CMLM Multilingual** (768D) - Universal sentence encoder for 109 languages\n",
    "3. **Paraphrase Multilingual MPNet** (768D) - 50+ languages with high performance\n",
    "4. **Multilingual DistilBERT** (512D) - Efficient 15-language model\n",
    "5. **XLM-RoBERTa** (768D) - Trained on SNLI, MNLI, ANLI, XNLI\n",
    "\n",
    "## üîç Why Embedding Quality Matters for Arabic\n",
    "Arabic presents unique challenges:\n",
    "- **Rich morphology**: One root can generate hundreds of word forms\n",
    "- **Dialectical variations**: MSA vs. regional dialects\n",
    "- **Right-to-left script**: Different text processing requirements\n",
    "- **Diacritics**: Optional marks that change meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ Multilingual Embeddings Learning Environment Ready!\")\n",
    "print(f\"üîß PyTorch available: {torch.cuda.is_available()}\")\n",
    "print(f\"üìä All visualization libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Multilingual Embedding Models\n",
    "\n",
    "### Paper's Model Selection Rationale\n",
    "The paper chose these 5 models based on:\n",
    "- **Language coverage**: All support Arabic\n",
    "- **Architecture diversity**: Different transformer variants\n",
    "- **Embedding dimensions**: Range from 384 to 768 dimensions\n",
    "- **Training objectives**: Different pre-training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualEmbeddingAnalyzer:\n",
    "    \"\"\"Comprehensive analysis tool for multilingual embedding models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model configurations from the paper\n",
    "        self.models_config = {\n",
    "            'MiniLM': {\n",
    "                'name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                'dimensions': 384,\n",
    "                'languages': '50+',\n",
    "                'focus': 'Clustering & Semantic Search',\n",
    "                'architecture': 'MiniLM'\n",
    "            },\n",
    "            'CMLM': {\n",
    "                'name': 'sentence-transformers/use-cmlm-multilingual',\n",
    "                'dimensions': 768,\n",
    "                'languages': '109',\n",
    "                'focus': 'Universal Sentence Encoding',\n",
    "                'architecture': 'LaBSE-based'\n",
    "            },\n",
    "            'MPNet': {\n",
    "                'name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "                'dimensions': 768,\n",
    "                'languages': '50+',\n",
    "                'focus': 'Paraphrase & Similarity',\n",
    "                'architecture': 'MPNet'\n",
    "            },\n",
    "            'DistilBERT': {\n",
    "                'name': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n",
    "                'dimensions': 512,\n",
    "                'languages': '15',\n",
    "                'focus': 'Efficient Processing',\n",
    "                'architecture': 'DistilBERT'\n",
    "            },\n",
    "            'XLM-RoBERTa': {\n",
    "                'name': 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli',\n",
    "                'dimensions': 768,\n",
    "                'languages': '12+',\n",
    "                'focus': 'Natural Language Inference',\n",
    "                'architecture': 'XLM-RoBERTa'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.loaded_models = {}\n",
    "        \n",
    "        # Arabic test sentences covering different linguistic phenomena\n",
    "        self.arabic_test_sentences = {\n",
    "            'customer_support': [\n",
    "                \"ÿ£ÿ≠ÿ™ÿßÿ¨ ŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸä ÿ≠ŸÑ ŸÖÿ¥ŸÉŸÑÿ© ÿ™ŸÇŸÜŸäÿ©\",\n",
    "                \"ÿ£Ÿàÿßÿ¨Ÿá ÿµÿπŸàÿ®ÿ© ŸÅŸä ÿßŸÑŸàÿµŸàŸÑ ÿ•ŸÑŸâ ÿ≠ÿ≥ÿßÿ®Ÿä\",\n",
    "                \"ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ÿ•ÿπÿßÿØÿ© ÿ™ÿπŸäŸäŸÜ ŸÉŸÑŸÖÿ© ÿßŸÑŸÖÿ±Ÿàÿ±ÿü\",\n",
    "                \"ŸÑÿØŸä ŸÖÿ¥ŸÉŸÑÿ© ŸÅŸä ÿßŸÑÿØŸÅÿπ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä\",\n",
    "                \"ÿ£ÿ±ŸäÿØ ÿ•ŸÑÿ∫ÿßÿ° ÿßÿ¥ÿ™ÿ±ÿßŸÉŸä ŸÅŸä ÿßŸÑÿÆÿØŸÖÿ©\"\n",
    "            ],\n",
    "            'morphological_variants': [\n",
    "                \"ŸÉÿ™ÿ® ÿßŸÑÿ∑ÿßŸÑÿ® Ÿàÿßÿ¨ÿ®Ÿá\",  # Root: ŸÉ-ÿ™-ÿ® (write)\n",
    "                \"ŸäŸÉÿ™ÿ® ÿßŸÑŸÖÿπŸÑŸÖ ÿπŸÑŸâ ÿßŸÑÿ≥ÿ®Ÿàÿ±ÿ©\",  # Same root, different form\n",
    "                \"ÿßŸÑŸÉÿ™ÿßÿ®ÿ© ŸÖŸáÿßÿ±ÿ© ŸÖŸáŸÖÿ©\",  # Nominal form\n",
    "                \"ÿßŸÑŸÖŸÉÿ™ÿ®ÿ© ŸÖŸÑŸäÿ¶ÿ© ÿ®ÿßŸÑŸÉÿ™ÿ®\",  # Place noun + plural\n",
    "                \"ÿßŸÑŸÉÿßÿ™ÿ® ŸÖÿ¥ŸáŸàÿ± ÿ¨ÿØÿßŸã\"  # Agent noun\n",
    "            ],\n",
    "            'dialectal_variations': [\n",
    "                \"ÿ£ŸäŸÜ ÿßŸÑÿ≠ŸÖÿßŸÖÿü\",  # MSA (Modern Standard Arabic)\n",
    "                \"ŸàŸäŸÜ ÿßŸÑÿ≠ŸÖÿßŸÖÿü\",  # Levantine dialect\n",
    "                \"ŸÅŸäŸÜ ÿßŸÑÿ≠ŸÖÿßŸÖÿü\",  # Egyptian dialect\n",
    "                \"ÿ•Ÿäÿ¥ ÿ±ÿßŸäŸÉÿü\",  # Levantine: \"What do you think?\"\n",
    "                \"ÿ•ŸäŸá ÿ±ÿ£ŸäŸÉÿü\"  # Egyptian: \"What do you think?\"\n",
    "            ],\n",
    "            'semantic_similarity': [\n",
    "                \"ÿßŸÑÿ≥Ÿäÿßÿ±ÿ© ÿ≥ÿ±Ÿäÿπÿ© ÿ¨ÿØÿßŸã\",\n",
    "                \"ÿßŸÑŸÖÿ±ŸÉÿ®ÿ© ÿ™ÿ™ÿ≠ÿ±ŸÉ ÿ®ÿ≥ÿ±ÿπÿ© ÿπÿßŸÑŸäÿ©\",\n",
    "                \"ÿßŸÑÿπÿ±ÿ®ÿ© ÿ™ÿ≥Ÿäÿ± ÿ®Ÿàÿ™Ÿäÿ±ÿ© ÿ≥ÿ±Ÿäÿπÿ©\",\n",
    "                \"ÿßŸÑÿ∑ÿßÿ¶ÿ±ÿ© ÿ™ÿ≠ŸÑŸÇ ŸÅŸä ÿßŸÑÿ≥ŸÖÿßÿ°\",\n",
    "                \"ÿßŸÑŸÇÿ∑ÿßÿ± Ÿäÿ≥Ÿäÿ± ÿπŸÑŸâ ÿßŸÑŸÇÿ∂ÿ®ÿßŸÜ\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def load_model(self, model_key: str) -> SentenceTransformer:\n",
    "        \"\"\"Load and cache a specific model\"\"\"\n",
    "        if model_key not in self.loaded_models:\n",
    "            print(f\"üîÑ Loading {model_key}...\")\n",
    "            model_name = self.models_config[model_key]['name']\n",
    "            try:\n",
    "                self.loaded_models[model_key] = SentenceTransformer(model_name)\n",
    "                print(f\"‚úÖ {model_key} loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {model_key}: {e}\")\n",
    "                return None\n",
    "        return self.loaded_models[model_key]\n",
    "    \n",
    "    def get_model_info_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a DataFrame with model information\"\"\"\n",
    "        data = []\n",
    "        for key, config in self.models_config.items():\n",
    "            data.append({\n",
    "                'Model': key,\n",
    "                'Dimensions': config['dimensions'],\n",
    "                'Languages': config['languages'],\n",
    "                'Focus': config['focus'],\n",
    "                'Architecture': config['architecture']\n",
    "            })\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def analyze_embedding_spaces(self, model_keys: List[str] = None) -> Dict:\n",
    "        \"\"\"Analyze embedding spaces for different models\"\"\"\n",
    "        if model_keys is None:\n",
    "            model_keys = ['MiniLM', 'MPNet']  # Analyze 2 models for demo\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Test with customer support sentences\n",
    "        test_sentences = self.arabic_test_sentences['customer_support']\n",
    "        \n",
    "        for model_key in model_keys:\n",
    "            model = self.load_model(model_key)\n",
    "            if model is None:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüîç Analyzing {model_key} embedding space...\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(test_sentences)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            embedding_stats = {\n",
    "                'dimensions': embeddings.shape[1],\n",
    "                'mean_magnitude': np.mean(np.linalg.norm(embeddings, axis=1)),\n",
    "                'std_magnitude': np.std(np.linalg.norm(embeddings, axis=1)),\n",
    "                'mean_cosine_similarity': np.mean(cosine_similarity(embeddings)),\n",
    "                'embedding_variance': np.var(embeddings.flatten()),\n",
    "                'sparsity': np.mean(embeddings == 0)\n",
    "            }\n",
    "            \n",
    "            results[model_key] = {\n",
    "                'embeddings': embeddings,\n",
    "                'sentences': test_sentences,\n",
    "                'stats': embedding_stats\n",
    "            }\n",
    "            \n",
    "            print(f\"  üìä Dimensions: {embedding_stats['dimensions']}\")\n",
    "            print(f\"  üìè Mean magnitude: {embedding_stats['mean_magnitude']:.4f}\")\n",
    "            print(f\"  üéØ Mean cosine similarity: {embedding_stats['mean_cosine_similarity']:.4f}\")\n",
    "            print(f\"  üìà Embedding variance: {embedding_stats['embedding_variance']:.6f}\")\n",
    "            print(f\"  üï≥Ô∏è Sparsity: {embedding_stats['sparsity']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize analyzer and show model information\n",
    "analyzer = MultilingualEmbeddingAnalyzer()\n",
    "model_info_df = analyzer.get_model_info_df()\n",
    "\n",
    "print(\"üìä Model Configurations from Paper:\")\n",
    "print(\"=\" * 80)\n",
    "print(model_info_df.to_string(index=False))\n",
    "\n",
    "# Analyze embedding spaces\n",
    "embedding_analysis = analyzer.analyze_embedding_spaces(['MiniLM', 'MPNet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Arabic Language Characteristics Analysis\n",
    "\n",
    "### Understanding How Models Handle Arabic Morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_arabic_morphology(analyzer: MultilingualEmbeddingAnalyzer, model_key: str = 'MPNet'):\n",
    "    \"\"\"Analyze how models handle Arabic morphological variations\"\"\"\n",
    "    \n",
    "    model = analyzer.load_model(model_key)\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüîç Arabic Morphology Analysis with {model_key}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test morphological variants\n",
    "    morphological_sentences = analyzer.arabic_test_sentences['morphological_variants']\n",
    "    embeddings = model.encode(morphological_sentences)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    print(\"\\nüìù Morphological Variants (Root: ŸÉ-ÿ™-ÿ® - 'write'):\")\n",
    "    for i, sentence in enumerate(morphological_sentences):\n",
    "        print(f\"{i+1}. {sentence}\")\n",
    "    \n",
    "    print(\"\\nüìä Cosine Similarity Matrix:\")\n",
    "    print(\"    \", end=\"\")\n",
    "    for i in range(len(morphological_sentences)):\n",
    "        print(f\"{i+1:6}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(morphological_sentences)):\n",
    "        print(f\"{i+1:2}. \", end=\"\")\n",
    "        for j in range(len(morphological_sentences)):\n",
    "            print(f\"{similarity_matrix[i,j]:5.3f}\", end=\" \")\n",
    "        print()\n",
    "    \n",
    "    # Analyze dialectal variations\n",
    "    print(\"\\nüó£Ô∏è Dialectal Variations Analysis:\")\n",
    "    dialectal_sentences = analyzer.arabic_test_sentences['dialectal_variations']\n",
    "    dialectal_embeddings = model.encode(dialectal_sentences)\n",
    "    dialectal_similarity = cosine_similarity(dialectal_embeddings)\n",
    "    \n",
    "    for i, sentence in enumerate(dialectal_sentences):\n",
    "        print(f\"{i+1}. {sentence}\")\n",
    "    \n",
    "    print(\"\\nüìä Dialectal Similarity Matrix:\")\n",
    "    print(\"    \", end=\"\")\n",
    "    for i in range(len(dialectal_sentences)):\n",
    "        print(f\"{i+1:6}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(dialectal_sentences)):\n",
    "        print(f\"{i+1:2}. \", end=\"\")\n",
    "        for j in range(len(dialectal_sentences)):\n",
    "            print(f\"{dialectal_similarity[i,j]:5.3f}\", end=\" \")\n",
    "        print()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Morphological similarity heatmap\n",
    "    im1 = ax1.imshow(similarity_matrix, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax1.set_title(f'Morphological Similarity\\n({model_key})', fontweight='bold')\n",
    "    ax1.set_xlabel('Sentence Index')\n",
    "    ax1.set_ylabel('Sentence Index')\n",
    "    \n",
    "    # Add similarity values\n",
    "    for i in range(len(morphological_sentences)):\n",
    "        for j in range(len(morphological_sentences)):\n",
    "            ax1.text(j, i, f'{similarity_matrix[i,j]:.3f}', \n",
    "                    ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    # Dialectal similarity heatmap\n",
    "    im2 = ax2.imshow(dialectal_similarity, cmap='Reds', vmin=0, vmax=1)\n",
    "    ax2.set_title(f'Dialectal Similarity\\n({model_key})', fontweight='bold')\n",
    "    ax2.set_xlabel('Sentence Index')\n",
    "    ax2.set_ylabel('Sentence Index')\n",
    "    \n",
    "    # Add similarity values\n",
    "    for i in range(len(dialectal_sentences)):\n",
    "        for j in range(len(dialectal_sentences)):\n",
    "            ax2.text(j, i, f'{dialectal_similarity[i,j]:.3f}', \n",
    "                    ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('arabic_linguistic_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'morphological_similarity': similarity_matrix,\n",
    "        'dialectal_similarity': dialectal_similarity,\n",
    "        'morphological_sentences': morphological_sentences,\n",
    "        'dialectal_sentences': dialectal_sentences\n",
    "    }\n",
    "\n",
    "# Run Arabic morphology analysis\n",
    "arabic_analysis = analyze_arabic_morphology(analyzer, 'MPNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Dimensionality Impact Analysis\n",
    "\n",
    "### Paper Finding: \"The size of the embedding vector also plays a crucial role, as larger vectors can encapsulate more information, potentially enhancing overall performance, particularly for Arabic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dimensionality_impact(analyzer: MultilingualEmbeddingAnalyzer):\n",
    "    \"\"\"Analyze the impact of embedding dimensions on performance\"\"\"\n",
    "    \n",
    "    print(\"\\nüìè Dimensionality Impact Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load models with different dimensions\n",
    "    models_to_test = ['MiniLM', 'DistilBERT', 'MPNet']  # 384, 512, 768 dimensions\n",
    "    \n",
    "    results = []\n",
    "    test_sentences = analyzer.arabic_test_sentences['semantic_similarity']\n",
    "    \n",
    "    for model_key in models_to_test:\n",
    "        model = analyzer.load_model(model_key)\n",
    "        if model is None:\n",
    "            continue\n",
    "        \n",
    "        dimensions = analyzer.models_config[model_key]['dimensions']\n",
    "        print(f\"\\nüîç Testing {model_key} ({dimensions}D)...\")\n",
    "        \n",
    "        # Encode sentences\n",
    "        embeddings = model.encode(test_sentences)\n",
    "        \n",
    "        # Calculate various metrics\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Information capacity metrics\n",
    "        embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "        pairwise_distances = euclidean_distances(embeddings)\n",
    "        \n",
    "        # Semantic coherence: how well related sentences cluster together\n",
    "        # Sentences 1-3 are about cars, 4-5 are about other transport\n",
    "        car_similarities = [\n",
    "            similarity_matrix[0, 1],  # Car 1 vs Car 2\n",
    "            similarity_matrix[0, 2],  # Car 1 vs Car 3\n",
    "            similarity_matrix[1, 2]   # Car 2 vs Car 3\n",
    "        ]\n",
    "        \n",
    "        cross_category_similarities = [\n",
    "            similarity_matrix[0, 3],  # Car vs Plane\n",
    "            similarity_matrix[0, 4],  # Car vs Train\n",
    "            similarity_matrix[1, 3],  # Car vs Plane\n",
    "            similarity_matrix[1, 4],  # Car vs Train\n",
    "            similarity_matrix[2, 3],  # Car vs Plane\n",
    "            similarity_matrix[2, 4]   # Car vs Train\n",
    "        ]\n",
    "        \n",
    "        semantic_coherence = np.mean(car_similarities) - np.mean(cross_category_similarities)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_key,\n",
    "            'Dimensions': dimensions,\n",
    "            'Mean_Norm': np.mean(embedding_norms),\n",
    "            'Std_Norm': np.std(embedding_norms),\n",
    "            'Mean_Distance': np.mean(pairwise_distances),\n",
    "            'Semantic_Coherence': semantic_coherence,\n",
    "            'Embedding_Variance': np.var(embeddings.flatten()),\n",
    "            'Information_Density': np.var(embeddings.flatten()) * dimensions\n",
    "        })\n",
    "        \n",
    "        print(f\"  üìä Mean embedding norm: {np.mean(embedding_norms):.4f}\")\n",
    "        print(f\"  üéØ Semantic coherence: {semantic_coherence:.4f}\")\n",
    "        print(f\"  üìà Information density: {np.var(embeddings.flatten()) * dimensions:.6f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def visualize_dimensionality_analysis(dim_results: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visualization of dimensionality impact\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Dimensions vs Semantic Coherence\n",
    "    ax1.scatter(dim_results['Dimensions'], dim_results['Semantic_Coherence'], \n",
    "               s=100, alpha=0.7, c=['red', 'blue', 'green'])\n",
    "    \n",
    "    for i, row in dim_results.iterrows():\n",
    "        ax1.annotate(row['Model'], \n",
    "                    (row['Dimensions'], row['Semantic_Coherence']),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax1.set_xlabel('Embedding Dimensions')\n",
    "    ax1.set_ylabel('Semantic Coherence Score')\n",
    "    ax1.set_title('Dimensions vs Semantic Coherence')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Information Density Comparison\n",
    "    bars = ax2.bar(dim_results['Model'], dim_results['Information_Density'], \n",
    "                   color=['red', 'blue', 'green'], alpha=0.7)\n",
    "    ax2.set_ylabel('Information Density')\n",
    "    ax2.set_title('Information Density by Model')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, dim_results['Information_Density']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + value*0.01,\n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Embedding Variance vs Dimensions\n",
    "    ax3.plot(dim_results['Dimensions'], dim_results['Embedding_Variance'], \n",
    "             'o-', linewidth=2, markersize=8, color='purple')\n",
    "    \n",
    "    for i, row in dim_results.iterrows():\n",
    "        ax3.annotate(row['Model'], \n",
    "                    (row['Dimensions'], row['Embedding_Variance']),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax3.set_xlabel('Embedding Dimensions')\n",
    "    ax3.set_ylabel('Embedding Variance')\n",
    "    ax3.set_title('Embedding Variance vs Dimensions')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Multi-metric comparison\n",
    "    metrics = ['Semantic_Coherence', 'Information_Density', 'Embedding_Variance']\n",
    "    normalized_data = dim_results[metrics].copy()\n",
    "    \n",
    "    # Normalize to 0-1 scale for comparison\n",
    "    for metric in metrics:\n",
    "        min_val = normalized_data[metric].min()\n",
    "        max_val = normalized_data[metric].max()\n",
    "        normalized_data[metric] = (normalized_data[metric] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    x = np.arange(len(dim_results))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax4.bar(x + i*width, normalized_data[metric], width, \n",
    "               label=metric.replace('_', ' '), alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('Models')\n",
    "    ax4.set_ylabel('Normalized Score')\n",
    "    ax4.set_title('Multi-Metric Comparison (Normalized)')\n",
    "    ax4.set_xticks(x + width)\n",
    "    ax4.set_xticklabels(dim_results['Model'])\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dimensionality_impact_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run dimensionality analysis\n",
    "dim_results = analyze_dimensionality_impact(analyzer)\n",
    "print(\"\\nüìä Dimensionality Analysis Results:\")\n",
    "print(dim_results.round(4))\n",
    "\n",
    "# Visualize results\n",
    "visualize_dimensionality_analysis(dim_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Asymmetric vs Symmetric Semantic Search\n",
    "\n",
    "### Paper Insight: \"The discrepancy can be attributed to the nature of the semantic search evaluation, which closely resembles an Asymmetric Semantic Search scenario, where embeddings of extensive text summaries and brief queries, averaging four words, are compared.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_asymmetric_vs_symmetric_search(analyzer: MultilingualEmbeddingAnalyzer):\n",
    "    \"\"\"Analyze performance differences between asymmetric and symmetric search scenarios\"\"\"\n",
    "    \n",
    "    print(\"\\nüîç Asymmetric vs Symmetric Search Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define test scenarios\n",
    "    asymmetric_scenario = {\n",
    "        'name': 'Asymmetric (Query vs Document)',\n",
    "        'short_queries': [\n",
    "            \"ŸÖÿ¥ŸÉŸÑÿ© ÿ™ÿ≥ÿ¨ŸäŸÑ ÿØÿÆŸàŸÑ\",  # Login problem\n",
    "            \"ÿ•ÿπÿßÿØÿ© ÿ™ÿπŸäŸäŸÜ ŸÉŸÑŸÖÿ© ŸÖÿ±Ÿàÿ±\",  # Reset password\n",
    "            \"ÿØŸÅÿπ ÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä\",  # Electronic payment\n",
    "            \"ÿ•ŸÑÿ∫ÿßÿ° ÿßÿ¥ÿ™ÿ±ÿßŸÉ\"  # Cancel subscription\n",
    "        ],\n",
    "        'long_documents': [\n",
    "            \"ÿßŸÑÿπŸÖŸäŸÑ ŸäŸàÿßÿ¨Ÿá ŸÖÿ¥ŸÉŸÑÿ© ŸÅŸä ÿ™ÿ≥ÿ¨ŸäŸÑ ÿßŸÑÿØÿÆŸàŸÑ ÿ•ŸÑŸâ ÿ≠ÿ≥ÿßÿ®Ÿá ÿßŸÑÿ¥ÿÆÿµŸä ÿπŸÑŸâ ÿßŸÑŸÖŸàŸÇÿπ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ŸàŸäÿ∑ŸÑÿ® ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ÿßŸÑŸÅŸàÿ±Ÿäÿ© ŸÑÿ≠ŸÑ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑÿ™ŸÇŸÜŸäÿ© ÿßŸÑŸÖÿ™ÿπŸÑŸÇÿ© ÿ®ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÖÿµÿßÿØŸÇÿ©\",\n",
    "            \"ÿ∑ŸÑÿ® ÿßŸÑÿπŸÖŸäŸÑ ŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸÜŸäÿ© ŸÅŸä ÿ•ÿπÿßÿØÿ© ÿ™ÿπŸäŸäŸÜ ŸÉŸÑŸÖÿ© ÿßŸÑŸÖÿ±Ÿàÿ± ÿßŸÑÿÆÿßÿµÿ© ÿ®ÿ≠ÿ≥ÿßÿ®Ÿá ÿ®ÿπÿØ ŸÜÿ≥ŸäÿßŸÜŸáÿß ŸàÿπÿØŸÖ ŸÇÿØÿ±ÿ™Ÿá ÿπŸÑŸâ ÿßŸÑŸàÿµŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿ®ÿ±ŸäÿØ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ÿßŸÑŸÖÿ±ÿ™ÿ®ÿ∑ ÿ®ÿßŸÑÿ≠ÿ≥ÿßÿ®\",\n",
    "            \"Ÿäÿ¥ŸÉŸà ÿßŸÑÿπŸÖŸäŸÑ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ŸÖÿ¥ŸÉŸÑÿ© ÿ™ŸÇŸÜŸäÿ© ŸÅŸä ŸÜÿ∏ÿßŸÖ ÿßŸÑÿØŸÅÿπ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ÿπÿ®ÿ± ÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿßÿ¶ÿ™ŸÖÿßŸÜ ŸàŸÑÿß Ÿäÿ≥ÿ™ÿ∑Ÿäÿπ ÿ•ÿ™ŸÖÿßŸÖ ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ¥ÿ±ÿßÿ° ÿ®ŸÜÿ¨ÿßÿ≠ ÿ±ÿ∫ŸÖ ŸÖÿ≠ÿßŸàŸÑÿßÿ™Ÿá ÿßŸÑŸÖÿ™ŸÉÿ±ÿ±ÿ©\",\n",
    "            \"ÿßŸÑÿπŸÖŸäŸÑ Ÿäÿ±ŸäÿØ ÿ•ŸÑÿ∫ÿßÿ° ÿßÿ¥ÿ™ÿ±ÿßŸÉŸá ŸÅŸä ÿßŸÑÿÆÿØŸÖÿ© ÿßŸÑŸÖÿØŸÅŸàÿπÿ© ÿ®ÿ≥ÿ®ÿ® ÿπÿØŸÖ ÿ±ÿ∂ÿßŸá ÿπŸÜ ŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑÿÆÿØŸÖÿ© ÿßŸÑŸÖŸÇÿØŸÖÿ© ŸàŸäÿ∑ŸÑÿ® ÿßÿ≥ÿ™ÿ±ÿØÿßÿØ ÿßŸÑŸÖÿ®ŸÑÿ∫ ÿßŸÑŸÖÿØŸÅŸàÿπ ŸÑŸÑÿ¥Ÿáÿ± ÿßŸÑÿ≠ÿßŸÑŸä\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    symmetric_scenario = {\n",
    "        'name': 'Symmetric (Query vs Query)',\n",
    "        'queries_set1': [\n",
    "            \"ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ÿ™ÿ≥ÿ¨ŸäŸÑ ÿßŸÑÿØÿÆŸàŸÑ ÿ•ŸÑŸâ ÿ≠ÿ≥ÿßÿ®Ÿäÿü\",\n",
    "            \"ŸÉŸäŸÅ ÿ£ÿπŸäÿØ ÿ™ÿπŸäŸäŸÜ ŸÉŸÑŸÖÿ© ÿßŸÑŸÖÿ±Ÿàÿ±ÿü\", \n",
    "            \"ŸÖÿß ŸáŸä ŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑÿØŸÅÿπ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸäÿü\",\n",
    "            \"ŸÉŸäŸÅ ŸäŸÖŸÉŸÜŸÜŸä ÿ•ŸÑÿ∫ÿßÿ° ÿßŸÑÿßÿ¥ÿ™ÿ±ÿßŸÉÿü\"\n",
    "        ],\n",
    "        'queries_set2': [\n",
    "            \"ÿ£Ÿàÿßÿ¨Ÿá ÿµÿπŸàÿ®ÿ© ŸÅŸä ÿßŸÑÿØÿÆŸàŸÑ ŸÑÿ≠ÿ≥ÿßÿ®Ÿä\",\n",
    "            \"ŸÜÿ≥Ÿäÿ™ ŸÉŸÑŸÖÿ© ÿßŸÑÿ≥ÿ± Ÿàÿ£ÿ≠ÿ™ÿßÿ¨ ŸÑÿ•ÿπÿßÿØÿ© ÿ™ÿπŸäŸäŸÜŸáÿß\",\n",
    "            \"ŸÑÿß ŸäÿπŸÖŸÑ ÿßŸÑÿØŸÅÿπ ÿ®ÿßŸÑÿ®ÿ∑ÿßŸÇÿ© ÿßŸÑÿßÿ¶ÿ™ŸÖÿßŸÜŸäÿ©\",\n",
    "            \"ÿ£ÿ±ŸäÿØ ÿ•ŸäŸÇÿßŸÅ ÿßÿ¥ÿ™ÿ±ÿßŸÉŸä ŸÅŸä ÿßŸÑÿÆÿØŸÖÿ©\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Test with multiple models\n",
    "    models_to_test = ['MiniLM', 'MPNet']\n",
    "    results = []\n",
    "    \n",
    "    for model_key in models_to_test:\n",
    "        model = analyzer.load_model(model_key)\n",
    "        if model is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüîç Testing {model_key}...\")\n",
    "        \n",
    "        # Asymmetric scenario\n",
    "        query_embeddings = model.encode(asymmetric_scenario['short_queries'])\n",
    "        doc_embeddings = model.encode(asymmetric_scenario['long_documents'])\n",
    "        asymmetric_similarities = cosine_similarity(query_embeddings, doc_embeddings)\n",
    "        \n",
    "        # Symmetric scenario\n",
    "        set1_embeddings = model.encode(symmetric_scenario['queries_set1'])\n",
    "        set2_embeddings = model.encode(symmetric_scenario['queries_set2'])\n",
    "        symmetric_similarities = cosine_similarity(set1_embeddings, set2_embeddings)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        # For both scenarios, diagonal elements should have highest similarity\n",
    "        asymmetric_diagonal = np.diag(asymmetric_similarities)\n",
    "        symmetric_diagonal = np.diag(symmetric_similarities)\n",
    "        \n",
    "        # Off-diagonal elements (should be lower)\n",
    "        asymmetric_off_diag = asymmetric_similarities[np.triu_indices_from(asymmetric_similarities, k=1)]\n",
    "        symmetric_off_diag = symmetric_similarities[np.triu_indices_from(symmetric_similarities, k=1)]\n",
    "        \n",
    "        # Discrimination ability (how well it separates relevant from irrelevant)\n",
    "        asymmetric_discrimination = np.mean(asymmetric_diagonal) - np.mean(asymmetric_off_diag)\n",
    "        symmetric_discrimination = np.mean(symmetric_diagonal) - np.mean(symmetric_off_diag)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_key,\n",
    "            'Asymmetric_Mean_Similarity': np.mean(asymmetric_diagonal),\n",
    "            'Symmetric_Mean_Similarity': np.mean(symmetric_diagonal),\n",
    "            'Asymmetric_Discrimination': asymmetric_discrimination,\n",
    "            'Symmetric_Discrimination': symmetric_discrimination,\n",
    "            'Asymmetric_Std': np.std(asymmetric_similarities),\n",
    "            'Symmetric_Std': np.std(symmetric_similarities)\n",
    "        })\n",
    "        \n",
    "        print(f\"  üìä Asymmetric discrimination: {asymmetric_discrimination:.4f}\")\n",
    "        print(f\"  üìä Symmetric discrimination: {symmetric_discrimination:.4f}\")\n",
    "        \n",
    "        # Visualize similarity matrices\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Asymmetric similarity matrix\n",
    "        im1 = ax1.imshow(asymmetric_similarities, cmap='Blues', vmin=0, vmax=1)\n",
    "        ax1.set_title(f'Asymmetric Search\\n{model_key} (Query vs Document)')\n",
    "        ax1.set_xlabel('Document Index')\n",
    "        ax1.set_ylabel('Query Index')\n",
    "        \n",
    "        # Add similarity values\n",
    "        for i in range(asymmetric_similarities.shape[0]):\n",
    "            for j in range(asymmetric_similarities.shape[1]):\n",
    "                ax1.text(j, i, f'{asymmetric_similarities[i,j]:.3f}', \n",
    "                        ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        # Symmetric similarity matrix\n",
    "        im2 = ax2.imshow(symmetric_similarities, cmap='Reds', vmin=0, vmax=1)\n",
    "        ax2.set_title(f'Symmetric Search\\n{model_key} (Query vs Query)')\n",
    "        ax2.set_xlabel('Query Set 2 Index')\n",
    "        ax2.set_ylabel('Query Set 1 Index')\n",
    "        \n",
    "        # Add similarity values\n",
    "        for i in range(symmetric_similarities.shape[0]):\n",
    "            for j in range(symmetric_similarities.shape[1]):\n",
    "                ax2.text(j, i, f'{symmetric_similarities[i,j]:.3f}', \n",
    "                        ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_key}_asymmetric_vs_symmetric.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    return pd.DataFrame(results), asymmetric_scenario, symmetric_scenario\n",
    "\n",
    "# Run asymmetric vs symmetric analysis\n",
    "search_results, asym_scenario, sym_scenario = analyze_asymmetric_vs_symmetric_search(analyzer)\n",
    "\n",
    "print(\"\\nüìä Search Scenario Comparison Results:\")\n",
    "print(search_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Embedding Visualization\n",
    "\n",
    "### Understanding Embedding Spaces through Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_visualization(analyzer: MultilingualEmbeddingAnalyzer, model_key: str = 'MPNet'):\n",
    "    \"\"\"Create comprehensive embedding space visualization\"\"\"\n",
    "    \n",
    "    model = analyzer.load_model(model_key)\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüé® Creating Embedding Visualization for {model_key}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Combine all test sentences with categories\n",
    "    all_sentences = []\n",
    "    categories = []\n",
    "    colors = []\n",
    "    \n",
    "    # Customer support category\n",
    "    all_sentences.extend(analyzer.arabic_test_sentences['customer_support'])\n",
    "    categories.extend(['Customer Support'] * len(analyzer.arabic_test_sentences['customer_support']))\n",
    "    colors.extend(['red'] * len(analyzer.arabic_test_sentences['customer_support']))\n",
    "    \n",
    "    # Morphological variants\n",
    "    all_sentences.extend(analyzer.arabic_test_sentences['morphological_variants'])\n",
    "    categories.extend(['Morphological'] * len(analyzer.arabic_test_sentences['morphological_variants']))\n",
    "    colors.extend(['blue'] * len(analyzer.arabic_test_sentences['morphological_variants']))\n",
    "    \n",
    "    # Dialectal variations\n",
    "    all_sentences.extend(analyzer.arabic_test_sentences['dialectal_variations'])\n",
    "    categories.extend(['Dialectal'] * len(analyzer.arabic_test_sentences['dialectal_variations']))\n",
    "    colors.extend(['green'] * len(analyzer.arabic_test_sentences['dialectal_variations']))\n",
    "    \n",
    "    # Semantic similarity\n",
    "    all_sentences.extend(analyzer.arabic_test_sentences['semantic_similarity'])\n",
    "    categories.extend(['Semantic'] * len(analyzer.arabic_test_sentences['semantic_similarity']))\n",
    "    colors.extend(['purple'] * len(analyzer.arabic_test_sentences['semantic_similarity']))\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(f\"üîÑ Encoding {len(all_sentences)} sentences...\")\n",
    "    embeddings = model.encode(all_sentences)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    print(\"üîÑ Applying PCA...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_pca = pca.fit_transform(embeddings)\n",
    "    \n",
    "    print(\"üîÑ Applying t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_sentences)-1))\n",
    "    embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # PCA plot\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    unique_categories = list(set(categories))\n",
    "    category_colors = {'Customer Support': 'red', 'Morphological': 'blue', \n",
    "                      'Dialectal': 'green', 'Semantic': 'purple'}\n",
    "    \n",
    "    for category in unique_categories:\n",
    "        mask = [cat == category for cat in categories]\n",
    "        ax1.scatter(embeddings_pca[mask, 0], embeddings_pca[mask, 1], \n",
    "                   c=category_colors[category], label=category, alpha=0.7, s=60)\n",
    "    \n",
    "    ax1.set_xlabel(f'PC1 (Var: {pca.explained_variance_ratio_[0]:.2%})')\n",
    "    ax1.set_ylabel(f'PC2 (Var: {pca.explained_variance_ratio_[1]:.2%})')\n",
    "    ax1.set_title(f'PCA Visualization\\n{model_key}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # t-SNE plot\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    for category in unique_categories:\n",
    "        mask = [cat == category for cat in categories]\n",
    "        ax2.scatter(embeddings_tsne[mask, 0], embeddings_tsne[mask, 1], \n",
    "                   c=category_colors[category], label=category, alpha=0.7, s=60)\n",
    "    \n",
    "    ax2.set_xlabel('t-SNE 1')\n",
    "    ax2.set_ylabel('t-SNE 2')\n",
    "    ax2.set_title(f't-SNE Visualization\\n{model_key}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Similarity matrix heatmap\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    im = ax3.imshow(similarity_matrix, cmap='viridis', vmin=0, vmax=1)\n",
    "    ax3.set_title(f'Cosine Similarity Matrix\\n{model_key}')\n",
    "    ax3.set_xlabel('Sentence Index')\n",
    "    ax3.set_ylabel('Sentence Index')\n",
    "    plt.colorbar(im, ax=ax3)\n",
    "    \n",
    "    # Embedding magnitude distribution\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
    "    ax4.hist(embedding_norms, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax4.set_xlabel('Embedding Magnitude')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title(f'Embedding Magnitude Distribution\\n{model_key}')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Principal component variance\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(embeddings)\n",
    "    cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    \n",
    "    ax5.plot(range(1, min(51, len(cumsum_var)+1)), cumsum_var[:50], 'o-', linewidth=2)\n",
    "    ax5.set_xlabel('Principal Component')\n",
    "    ax5.set_ylabel('Cumulative Explained Variance')\n",
    "    ax5.set_title(f'PCA Variance Explained\\n{model_key}')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% Variance')\n",
    "    ax5.legend()\n",
    "    \n",
    "    # Embedding statistics summary\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "üìä EMBEDDING STATISTICS\n",
    "\n",
    "Model: {model_key}\n",
    "Dimensions: {embeddings.shape[1]}\n",
    "Sentences: {embeddings.shape[0]}\n",
    "\n",
    "üìè Magnitude Stats:\n",
    "Mean: {np.mean(embedding_norms):.4f}\n",
    "Std: {np.std(embedding_norms):.4f}\n",
    "Min: {np.min(embedding_norms):.4f}\n",
    "Max: {np.max(embedding_norms):.4f}\n",
    "\n",
    "üéØ Similarity Stats:\n",
    "Mean: {np.mean(similarity_matrix):.4f}\n",
    "Std: {np.std(similarity_matrix):.4f}\n",
    "\n",
    "üìà Variance:\n",
    "Total: {np.var(embeddings.flatten()):.6f}\n",
    "95% in {np.argmax(cumsum_var >= 0.95)+1} PCs\n",
    "\"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, stats_text, transform=ax6.transAxes, fontsize=11, \n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_key}_embedding_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some sentences for reference\n",
    "    print(\"\\nüìù Sample Sentences by Category:\")\n",
    "    for category in unique_categories:\n",
    "        print(f\"\\n{category}:\")\n",
    "        category_sentences = [sent for sent, cat in zip(all_sentences, categories) if cat == category]\n",
    "        for i, sent in enumerate(category_sentences[:2]):  # Show first 2 from each category\n",
    "            print(f\"  {i+1}. {sent}\")\n",
    "    \n",
    "    return {\n",
    "        'embeddings': embeddings,\n",
    "        'pca_embeddings': embeddings_pca,\n",
    "        'tsne_embeddings': embeddings_tsne,\n",
    "        'sentences': all_sentences,\n",
    "        'categories': categories,\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'pca_variance_ratio': pca.explained_variance_ratio_\n",
    "    }\n",
    "\n",
    "# Create embedding visualization\n",
    "viz_results = create_embedding_visualization(analyzer, 'MPNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Performance Analysis\n",
    "\n",
    "### Comprehensive comparison following the paper's methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_comparison(analyzer: MultilingualEmbeddingAnalyzer):\n",
    "    \"\"\"Comprehensive comparison of all models following paper methodology\"\"\"\n",
    "    \n",
    "    print(\"\\nüèÜ Comprehensive Model Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Models to test (limit to 3 for demo due to loading time)\n",
    "    models_to_test = ['MiniLM', 'MPNet', 'DistilBERT']\n",
    "    \n",
    "    comparison_results = []\n",
    "    test_scenarios = {\n",
    "        'Customer Support': analyzer.arabic_test_sentences['customer_support'],\n",
    "        'Morphological': analyzer.arabic_test_sentences['morphological_variants'],\n",
    "        'Dialectal': analyzer.arabic_test_sentences['dialectal_variations'],\n",
    "        'Semantic': analyzer.arabic_test_sentences['semantic_similarity']\n",
    "    }\n",
    "    \n",
    "    for model_key in models_to_test:\n",
    "        model = analyzer.load_model(model_key)\n",
    "        if model is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüîç Evaluating {model_key}...\")\n",
    "        \n",
    "        model_results = {\n",
    "            'Model': model_key,\n",
    "            'Dimensions': analyzer.models_config[model_key]['dimensions'],\n",
    "            'Architecture': analyzer.models_config[model_key]['architecture']\n",
    "        }\n",
    "        \n",
    "        # Test each scenario\n",
    "        for scenario_name, sentences in test_scenarios.items():\n",
    "            embeddings = model.encode(sentences)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mean_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "            embedding_variance = np.var(embeddings.flatten())\n",
    "            mean_magnitude = np.mean(np.linalg.norm(embeddings, axis=1))\n",
    "            \n",
    "            # Coherence: how well similar sentences cluster\n",
    "            if len(sentences) >= 3:\n",
    "                # Take first 3 sentences as one cluster\n",
    "                intra_cluster_sim = np.mean([\n",
    "                    similarity_matrix[0,1], similarity_matrix[0,2], similarity_matrix[1,2]\n",
    "                ])\n",
    "                # Inter-cluster with remaining sentences\n",
    "                if len(sentences) > 3:\n",
    "                    inter_cluster_sim = np.mean([\n",
    "                        similarity_matrix[i,j] for i in range(3) for j in range(3, len(sentences))\n",
    "                    ])\n",
    "                    coherence = intra_cluster_sim - inter_cluster_sim\n",
    "                else:\n",
    "                    coherence = intra_cluster_sim\n",
    "            else:\n",
    "                coherence = mean_similarity\n",
    "            \n",
    "            model_results[f'{scenario_name}_Similarity'] = mean_similarity\n",
    "            model_results[f'{scenario_name}_Variance'] = embedding_variance\n",
    "            model_results[f'{scenario_name}_Magnitude'] = mean_magnitude\n",
    "            model_results[f'{scenario_name}_Coherence'] = coherence\n",
    "        \n",
    "        # Overall performance metrics\n",
    "        all_sentences = [sent for sentences in test_scenarios.values() for sent in sentences]\n",
    "        all_embeddings = model.encode(all_sentences)\n",
    "        overall_similarity = cosine_similarity(all_embeddings)\n",
    "        \n",
    "        model_results['Overall_Performance'] = np.mean([\n",
    "            model_results[f'{scenario}_Coherence'] for scenario in test_scenarios.keys()\n",
    "        ])\n",
    "        \n",
    "        comparison_results.append(model_results)\n",
    "        \n",
    "        print(f\"  üìä Overall Performance: {model_results['Overall_Performance']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "def visualize_model_comparison(comparison_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visualization of model comparison\"\"\"\n",
    "    \n",
    "    # Create performance radar chart\n",
    "    scenarios = ['Customer Support', 'Morphological', 'Dialectal', 'Semantic']\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Radar chart for coherence across scenarios\n",
    "    ax1 = plt.subplot(2, 3, 1, projection='polar')\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(scenarios), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    colors = ['red', 'blue', 'green']\n",
    "    \n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        values = [row[f'{scenario}_Coherence'] for scenario in scenarios]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax1.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[i])\n",
    "        ax1.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax1.set_xticks(angles[:-1])\n",
    "    ax1.set_xticklabels(scenarios)\n",
    "    ax1.set_title('Coherence Across Scenarios', size=14, fontweight='bold', pad=20)\n",
    "    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    # 2. Dimensions vs Performance\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    scatter = ax2.scatter(comparison_df['Dimensions'], comparison_df['Overall_Performance'], \n",
    "                         c=colors[:len(comparison_df)], s=100, alpha=0.7)\n",
    "    \n",
    "    for i, row in comparison_df.iterrows():\n",
    "        ax2.annotate(row['Model'], \n",
    "                    (row['Dimensions'], row['Overall_Performance']),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax2.set_xlabel('Embedding Dimensions')\n",
    "    ax2.set_ylabel('Overall Performance')\n",
    "    ax2.set_title('Dimensions vs Overall Performance')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Architecture comparison\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    architectures = comparison_df['Architecture'].tolist()\n",
    "    performances = comparison_df['Overall_Performance'].tolist()\n",
    "    \n",
    "    bars = ax3.bar(range(len(architectures)), performances, \n",
    "                   color=colors[:len(comparison_df)], alpha=0.7)\n",
    "    ax3.set_xticks(range(len(architectures)))\n",
    "    ax3.set_xticklabels(architectures, rotation=45)\n",
    "    ax3.set_ylabel('Overall Performance')\n",
    "    ax3.set_title('Architecture Performance Comparison')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, performances):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Scenario-specific performance\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    \n",
    "    x = np.arange(len(scenarios))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        coherence_values = [row[f'{scenario}_Coherence'] for scenario in scenarios]\n",
    "        ax4.bar(x + i*width, coherence_values, width, \n",
    "               label=row['Model'], color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('Scenarios')\n",
    "    ax4.set_ylabel('Coherence Score')\n",
    "    ax4.set_title('Scenario-Specific Performance')\n",
    "    ax4.set_xticks(x + width)\n",
    "    ax4.set_xticklabels(scenarios, rotation=45)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Performance vs Efficiency trade-off\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Use dimensions as proxy for computational cost\n",
    "    efficiency = 1000 / comparison_df['Dimensions']  # Inverse relationship\n",
    "    \n",
    "    for i, row in comparison_df.iterrows():\n",
    "        ax5.scatter(efficiency.iloc[i], row['Overall_Performance'], \n",
    "                   c=colors[i], s=100, alpha=0.7, label=row['Model'])\n",
    "        ax5.annotate(row['Model'], \n",
    "                    (efficiency.iloc[i], row['Overall_Performance']),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax5.set_xlabel('Efficiency (1000/Dimensions)')\n",
    "    ax5.set_ylabel('Overall Performance')\n",
    "    ax5.set_title('Performance vs Efficiency Trade-off')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Summary statistics\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Find best performing model\n",
    "    best_model = comparison_df.loc[comparison_df['Overall_Performance'].idxmax()]\n",
    "    most_efficient = comparison_df.loc[comparison_df['Dimensions'].idxmin()]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "üèÜ MODEL COMPARISON SUMMARY\n",
    "\n",
    "üìä Best Overall Performance:\n",
    "{best_model['Model']} ({best_model['Overall_Performance']:.4f})\n",
    "Architecture: {best_model['Architecture']}\n",
    "Dimensions: {best_model['Dimensions']}\n",
    "\n",
    "‚ö° Most Efficient:\n",
    "{most_efficient['Model']} ({most_efficient['Dimensions']} dims)\n",
    "Performance: {most_efficient['Overall_Performance']:.4f}\n",
    "\n",
    "üí° Key Insights:\n",
    "‚Ä¢ Higher dimensions generally ‚Üó performance\n",
    "‚Ä¢ Architecture matters as much as size\n",
    "‚Ä¢ Arabic benefits from larger embeddings\n",
    "‚Ä¢ Trade-off between efficiency and accuracy\n",
    "\n",
    "üéØ Paper Finding Confirmed:\n",
    "\"Larger vectors can encapsulate more \n",
    "information, potentially enhancing \n",
    "overall performance, particularly \n",
    "for Arabic.\"\n",
    "\"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, fontsize=10, \n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive comparison\n",
    "comparison_results = comprehensive_model_comparison(analyzer)\n",
    "\n",
    "print(\"\\nüìä Model Comparison Results:\")\n",
    "print(comparison_results[['Model', 'Dimensions', 'Architecture', 'Overall_Performance']].round(4))\n",
    "\n",
    "# Visualize comparison\n",
    "visualize_model_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Learning Summary\n",
    "\n",
    "### üéì What We've Mastered\n",
    "\n",
    "Through this focused learning session, we've gained deep insights into:\n",
    "\n",
    "#### üî¨ Multilingual Embedding Models\n",
    "1. **Architecture Impact**: Different transformer architectures (MiniLM, MPNet, DistilBERT, XLM-RoBERTa) handle Arabic differently\n",
    "2. **Dimensionality Trade-offs**: Higher dimensions (768D) vs efficiency (384D)\n",
    "3. **Training Objectives**: Models trained on different tasks (paraphrasing, NLI, universal encoding) show varying Arabic performance\n",
    "\n",
    "#### üåê Arabic Language Challenges\n",
    "1. **Morphological Complexity**: Root-based word formation creates semantic relationships that models must capture\n",
    "2. **Dialectal Variations**: MSA vs regional dialects require robust embedding spaces\n",
    "3. **Script Characteristics**: Right-to-left text and optional diacritics add complexity\n",
    "\n",
    "#### üìä Performance Analysis\n",
    "1. **Asymmetric vs Symmetric Search**: Different scenarios require different model strengths\n",
    "2. **Embedding Space Quality**: Visualization reveals clustering and separation patterns\n",
    "3. **Evaluation Metrics**: Multi-faceted assessment beyond simple similarity scores\n",
    "\n",
    "### üöÄ Practical Applications\n",
    "\n",
    "This knowledge directly applies to:\n",
    "- **Model Selection**: Choose appropriate embeddings for Arabic tasks\n",
    "- **System Design**: Balance performance vs computational efficiency\n",
    "- **Quality Assessment**: Understand embedding space characteristics\n",
    "- **Optimization**: Target specific Arabic linguistic phenomena\n",
    "\n",
    "### üîÆ Future Directions\n",
    "\n",
    "1. **Fine-tuning**: Adapt models specifically for Arabic domains\n",
    "2. **Hybrid Approaches**: Combine multiple models for better coverage\n",
    "3. **Specialized Architectures**: Develop Arabic-specific embedding models\n",
    "4. **Evaluation Frameworks**: Create more comprehensive Arabic benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì MULTILINGUAL EMBEDDINGS MASTERY COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "‚úÖ What you've mastered:\n",
    "\n",
    "üî¨ Technical Understanding:\n",
    "‚Ä¢ Architecture differences (MiniLM, MPNet, DistilBERT, XLM-RoBERTa)\n",
    "‚Ä¢ Dimensionality impact on Arabic performance\n",
    "‚Ä¢ Embedding space visualization and analysis\n",
    "‚Ä¢ Asymmetric vs symmetric search scenarios\n",
    "\n",
    "üåê Arabic Language Insights:\n",
    "‚Ä¢ Morphological variation handling\n",
    "‚Ä¢ Dialectal similarity patterns\n",
    "‚Ä¢ Semantic relationship capture\n",
    "‚Ä¢ Cross-linguistic transfer effectiveness\n",
    "\n",
    "üìä Practical Skills:\n",
    "‚Ä¢ Model comparison methodologies\n",
    "‚Ä¢ Performance vs efficiency trade-offs\n",
    "‚Ä¢ Embedding quality assessment\n",
    "‚Ä¢ Visualization and interpretation techniques\n",
    "\n",
    "üéØ Paper Findings Validated:\n",
    "‚Ä¢ \"Higher-quality encoders produce more detailed embedding vectors\"\n",
    "‚Ä¢ \"Larger vectors can encapsulate more information for Arabic\"\n",
    "‚Ä¢ \"Asymmetric search scenarios have different requirements\"\n",
    "\n",
    "üöÄ Ready to select and optimize embeddings for real-world Arabic applications!\n",
    "\"\"\")\n",
    "\n",
    "# Save comprehensive results\n",
    "results_summary = {\n",
    "    'model_comparison': comparison_results.to_dict('records'),\n",
    "    'dimensionality_analysis': dim_results.to_dict('records'),\n",
    "    'search_scenario_analysis': search_results.to_dict('records'),\n",
    "    'key_insights': {\n",
    "        'best_overall_model': comparison_results.loc[comparison_results['Overall_Performance'].idxmax(), 'Model'],\n",
    "        'most_efficient_model': comparison_results.loc[comparison_results['Dimensions'].idxmin(), 'Model'],\n",
    "        'dimension_performance_correlation': dim_results[['Dimensions', 'Semantic_Coherence']].corr().iloc[0,1],\n",
    "        'arabic_specific_challenges': [\n",
    "            'Morphological complexity requires larger embedding spaces',\n",
    "            'Dialectal variations need robust cross-dialectal understanding',\n",
    "            'Asymmetric search scenarios benefit from different model architectures'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('multilingual_embeddings_analysis_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Analysis results saved to 'multilingual_embeddings_analysis_results.json'\")\n",
    "print(\"üìä All visualizations saved as PNG files\")\n",
    "print(\"\\nüéâ Multilingual Embeddings Deep Dive Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Papers (PDF Utils)",\n",
   "language": "python",
   "name": "ai-papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}