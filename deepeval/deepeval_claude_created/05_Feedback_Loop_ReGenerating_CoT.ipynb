{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: VÃ²ng Láº·p Pháº£n Há»“i & TÃ¡i Táº¡o CoT\n",
    "\n",
    "**Má»¥c tiÃªu**: XÃ¢y dá»±ng vÃ²ng láº·p tá»± Ä‘á»™ng cáº£i thiá»‡n dá»±a trÃªn feedback tá»« DeepEval\n",
    "\n",
    "Trong notebook nÃ y, chÃºng ta sáº½ há»c cÃ¡ch:\n",
    "1. XÃ¢y dá»±ng automated feedback pipeline vá»›i 5 giai Ä‘oáº¡n\n",
    "2. Triá»ƒn khai advanced feedback strategies\n",
    "3. Táº¡o pattern recognition cho failures\n",
    "4. á»¨ng dá»¥ng thá»±c tiá»…n vá»›i real-world case studies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ CÃ i Äáº·t vÃ  Import\n",
    "\n",
    "TrÆ°á»›c tiÃªn, hÃ£y import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t vÃ  thiáº¿t láº­p mÃ´i trÆ°á»ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate, assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    HallucinationMetric,\n",
    "    GEval,\n",
    "    BaseMetric\n",
    ")\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… CÃ¡c thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c import thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Thiáº¿t Láº­p API Keys\n",
    "\n",
    "Äáº£m báº£o báº¡n Ä‘Ã£ thiáº¿t láº­p cÃ¡c API keys cáº§n thiáº¿t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra API keys\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âš ï¸ ChÆ°a thiáº¿t láº­p OPENAI_API_KEY\")\n",
    "    print(\"HÃ£y thiáº¿t láº­p: export OPENAI_API_KEY='your-api-key'\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Custom Metrics cho CoT Evaluation\n",
    "\n",
    "Äáº§u tiÃªn, chÃºng ta sáº½ táº¡o cÃ¡c custom metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Chain-of-Thought reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalFlowMetric(BaseMetric):\n",
    "    \"\"\"ÄÃ¡nh giÃ¡ chuá»—i suy luáº­n logic trong CoT\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.7):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        ÄÃ¡nh giÃ¡ tÃ­nh logic vÃ  máº¡ch láº¡c cá»§a chuá»—i suy luáº­n sau Ä‘Ã¢y:\n",
    "        \n",
    "        CÃ¢u há»i: {test_case.input}\n",
    "        Chuá»—i suy luáº­n: {test_case.actual_output}\n",
    "        \n",
    "        TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡:\n",
    "        1. TÃ­nh logic: CÃ¡c bÆ°á»›c cÃ³ theo thá»© tá»± há»£p lÃ½ khÃ´ng?\n",
    "        2. TÃ­nh máº¡ch láº¡c: CÃ³ má»‘i liÃªn káº¿t rÃµ rÃ ng giá»¯a cÃ¡c bÆ°á»›c?\n",
    "        3. TÃ­nh Ä‘áº§y Ä‘á»§: CÃ³ bá» qua bÆ°á»›c quan trá»ng nÃ o khÃ´ng?\n",
    "        4. TÃ­nh chÃ­nh xÃ¡c: Káº¿t luáº­n cÃ³ phÃ¹ há»£p vá»›i quÃ¡ trÃ¬nh suy luáº­n?\n",
    "        \n",
    "        Cho Ä‘iá»ƒm tá»« 0-10 (10 = hoÃ n háº£o, 0 = hoÃ n toÃ n thiáº¿u logic).\n",
    "        Chá»‰ tráº£ vá» sá»‘ Ä‘iá»ƒm, khÃ´ng giáº£i thÃ­ch.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke(evaluation_prompt)\n",
    "        try:\n",
    "            score = float(response.content.strip()) / 10.0\n",
    "            self.score = score\n",
    "            self.reason = f\"Logic flow score: {score:.2f}\"\n",
    "            self.success = score >= self.threshold\n",
    "            return score\n",
    "        except ValueError:\n",
    "            self.score = 0.0\n",
    "            self.reason = \"KhÃ´ng thá»ƒ Ä‘Ã¡nh giÃ¡ logic flow\"\n",
    "            self.success = False\n",
    "            return 0.0\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "class StepCompletenessMetric(BaseMetric):\n",
    "    \"\"\"ÄÃ¡nh giÃ¡ tÃ­nh Ä‘áº§y Ä‘á»§ cá»§a cÃ¡c bÆ°á»›c trong CoT\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.8):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        ÄÃ¡nh giÃ¡ tÃ­nh Ä‘áº§y Ä‘á»§ cá»§a cÃ¡c bÆ°á»›c giáº£i quyáº¿t váº¥n Ä‘á»:\n",
    "        \n",
    "        Váº¥n Ä‘á»: {test_case.input}\n",
    "        CÃ¡c bÆ°á»›c Ä‘Æ°á»£c thá»±c hiá»‡n: {test_case.actual_output}\n",
    "        \n",
    "        TiÃªu chÃ­:\n",
    "        1. CÃ³ xÃ¡c Ä‘á»‹nh rÃµ váº¥n Ä‘á»?\n",
    "        2. CÃ³ phÃ¢n tÃ­ch cÃ¡c yáº¿u tá»‘ liÃªn quan?\n",
    "        3. CÃ³ Ä‘á» xuáº¥t giáº£i phÃ¡p cá»¥ thá»ƒ?\n",
    "        4. CÃ³ kiá»ƒm tra tÃ­nh kháº£ thi?\n",
    "        5. CÃ³ káº¿t luáº­n rÃµ rÃ ng?\n",
    "        \n",
    "        Cho Ä‘iá»ƒm tá»« 0-10 dá»±a trÃªn sá»‘ bÆ°á»›c thiáº¿t yáº¿u Ä‘Æ°á»£c thá»±c hiá»‡n.\n",
    "        Chá»‰ tráº£ vá» sá»‘ Ä‘iá»ƒm.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke(evaluation_prompt)\n",
    "        try:\n",
    "            score = float(response.content.strip()) / 10.0\n",
    "            self.score = score\n",
    "            self.reason = f\"Step completeness: {score:.2f}\"\n",
    "            self.success = score >= self.threshold\n",
    "            return score\n",
    "        except ValueError:\n",
    "            self.score = 0.0\n",
    "            self.reason = \"KhÃ´ng thá»ƒ Ä‘Ã¡nh giÃ¡ step completeness\"\n",
    "            self.success = False\n",
    "            return 0.0\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "print(\"âœ… Custom metrics Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Core Feedback Pipeline Classes\n",
    "\n",
    "BÃ¢y giá» chÃºng ta sáº½ táº¡o cÃ¡c class cá»‘t lÃµi cho feedback pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeedbackResult:\n",
    "    \"\"\"LÆ°u trá»¯ káº¿t quáº£ cá»§a má»™t láº§n feedback\"\"\"\n",
    "    iteration: int\n",
    "    original_prompt: str\n",
    "    enhanced_prompt: str\n",
    "    original_output: str\n",
    "    enhanced_output: str\n",
    "    original_scores: Dict[str, float]\n",
    "    enhanced_scores: Dict[str, float]\n",
    "    improvement: Dict[str, float]\n",
    "    feedback_used: List[str]\n",
    "    timestamp: str\n",
    "    success: bool\n",
    "\n",
    "@dataclass\n",
    "class FailurePattern:\n",
    "    \"\"\"MÃ´ táº£ pattern cá»§a failure\"\"\"\n",
    "    pattern_type: str\n",
    "    description: str\n",
    "    frequency: int\n",
    "    suggested_fix: str\n",
    "    examples: List[str]\n",
    "\n",
    "class FeedbackExtractor:\n",
    "    \"\"\"TrÃ­ch xuáº¥t feedback tá»« failed evaluations\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def extract_feedback(self, test_case: LLMTestCase, failed_metrics: List[BaseMetric]) -> List[str]:\n",
    "        \"\"\"TrÃ­ch xuáº¥t feedback tá»« cÃ¡c metrics bá»‹ fail\"\"\"\n",
    "        feedback_list = []\n",
    "        \n",
    "        for metric in failed_metrics:\n",
    "            if hasattr(metric, 'reason') and metric.reason:\n",
    "                # Táº¡o feedback chi tiáº¿t hÆ¡n\n",
    "                detailed_feedback = self._generate_detailed_feedback(\n",
    "                    test_case, metric\n",
    "                )\n",
    "                feedback_list.append(detailed_feedback)\n",
    "        \n",
    "        return feedback_list\n",
    "    \n",
    "    def _generate_detailed_feedback(self, test_case: LLMTestCase, metric: BaseMetric) -> str:\n",
    "        \"\"\"Táº¡o feedback chi tiáº¿t cho metric cá»¥ thá»ƒ\"\"\"\n",
    "        feedback_prompt = f\"\"\"\n",
    "        Má»™t AI agent Ä‘Ã£ tháº¥t báº¡i trong Ä‘Ã¡nh giÃ¡ sau:\n",
    "        \n",
    "        Input: {test_case.input}\n",
    "        Output: {test_case.actual_output}\n",
    "        Metric: {metric.__class__.__name__}\n",
    "        Reason: {getattr(metric, 'reason', 'Unknown')}\n",
    "        Score: {getattr(metric, 'score', 'Unknown')}\n",
    "        \n",
    "        HÃ£y phÃ¢n tÃ­ch váº¥n Ä‘á» vÃ  Ä‘Æ°a ra feedback cá»¥ thá»ƒ Ä‘á»ƒ cáº£i thiá»‡n:\n",
    "        1. Váº¥n Ä‘á» chÃ­nh lÃ  gÃ¬?\n",
    "        2. LÃ m tháº¿ nÃ o Ä‘á»ƒ sá»­a?\n",
    "        3. VÃ­ dá»¥ cá»¥ thá»ƒ vá» cÃ¡ch cáº£i thiá»‡n?\n",
    "        \n",
    "        Tráº£ lá»i ngáº¯n gá»n, táº­p trung vÃ o hÃ nh Ä‘á»™ng cá»¥ thá»ƒ.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(feedback_prompt)\n",
    "        return response.content.strip()\n",
    "\n",
    "class PromptEnhancer:\n",
    "    \"\"\"Cáº£i thiá»‡n prompt dá»±a trÃªn feedback\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def enhance_prompt(self, original_prompt: str, feedback_list: List[str]) -> str:\n",
    "        \"\"\"Cáº£i thiá»‡n prompt dá»±a trÃªn feedback\"\"\"\n",
    "        enhancement_prompt = f\"\"\"\n",
    "        Báº¡n lÃ  má»™t chuyÃªn gia prompt engineering. HÃ£y cáº£i thiá»‡n prompt sau dá»±a trÃªn feedback:\n",
    "        \n",
    "        PROMPT Gá»C:\n",
    "        {original_prompt}\n",
    "        \n",
    "        FEEDBACK:\n",
    "        {chr(10).join(f\"- {fb}\" for fb in feedback_list)}\n",
    "        \n",
    "        HÃ£y táº¡o prompt má»›i:\n",
    "        1. Giá»¯ nguyÃªn Ã½ nghÄ©a chÃ­nh\n",
    "        2. TÃ­ch há»£p feedback Ä‘á»ƒ kháº¯c phá»¥c váº¥n Ä‘á»\n",
    "        3. ThÃªm hÆ°á»›ng dáº«n cá»¥ thá»ƒ cho Chain-of-Thought\n",
    "        4. Äáº£m báº£o prompt rÃµ rÃ ng vÃ  cÃ³ cáº¥u trÃºc\n",
    "        \n",
    "        CHá»ˆ TRáº¢ Vá»€ PROMPT Má»šI, KHÃ”NG GIáº¢I THÃCH.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(enhancement_prompt)\n",
    "        return response.content.strip()\n",
    "\n",
    "print(\"âœ… Feedback pipeline classes Ä‘Ã£ Ä‘Æ°á»£c táº¡o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Automated Feedback Pipeline\n",
    "\n",
    "ÄÃ¢y lÃ  pháº§n cá»‘t lÃµi - pipeline tá»± Ä‘á»™ng vá»›i 5 giai Ä‘oáº¡n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatedFeedbackPipeline:\n",
    "    \"\"\"Pipeline tá»± Ä‘á»™ng cho feedback loop\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_iterations: int = 3):\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.feedback_extractor = FeedbackExtractor(llm)\n",
    "        self.prompt_enhancer = PromptEnhancer(llm)\n",
    "        self.results: List[FeedbackResult] = []\n",
    "        self.failure_patterns: List[FailurePattern] = []\n",
    "    \n",
    "    def run_pipeline(self, \n",
    "                    initial_prompt: str, \n",
    "                    test_input: str,\n",
    "                    metrics: List[BaseMetric],\n",
    "                    context: Optional[str] = None) -> FeedbackResult:\n",
    "        \"\"\"Cháº¡y toÃ n bá»™ pipeline feedback\"\"\"\n",
    "        \n",
    "        current_prompt = initial_prompt\n",
    "        iteration = 0\n",
    "        \n",
    "        print(f\"ğŸš€ Báº¯t Ä‘áº§u Automated Feedback Pipeline\")\n",
    "        print(f\"ğŸ“ Test input: {test_input[:100]}...\")\n",
    "        print(f\"ğŸ“Š Sá»‘ metrics: {len(metrics)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        while iteration < self.max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"\\nğŸ”„ ITERATION {iteration}\")\n",
    "            \n",
    "            # Phase 1: Execution\n",
    "            print(\"1ï¸âƒ£ Execution Phase...\")\n",
    "            output = self._execute_prompt(current_prompt, test_input)\n",
    "            \n",
    "            # Phase 2: Evaluation\n",
    "            print(\"2ï¸âƒ£ Evaluation Phase...\")\n",
    "            test_case = LLMTestCase(\n",
    "                input=test_input,\n",
    "                actual_output=output,\n",
    "                context=[context] if context else None\n",
    "            )\n",
    "            \n",
    "            scores, failed_metrics = self._evaluate_test_case(test_case, metrics)\n",
    "            \n",
    "            # Kiá»ƒm tra náº¿u Ä‘Ã£ pass táº¥t cáº£ metrics\n",
    "            if not failed_metrics:\n",
    "                print(\"âœ… Táº¥t cáº£ metrics Ä‘Ã£ pass!\")\n",
    "                success_result = FeedbackResult(\n",
    "                    iteration=iteration,\n",
    "                    original_prompt=initial_prompt,\n",
    "                    enhanced_prompt=current_prompt,\n",
    "                    original_output=\"N/A\",\n",
    "                    enhanced_output=output,\n",
    "                    original_scores={},\n",
    "                    enhanced_scores=scores,\n",
    "                    improvement=scores,\n",
    "                    feedback_used=[],\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    success=True\n",
    "                )\n",
    "                self.results.append(success_result)\n",
    "                return success_result\n",
    "            \n",
    "            # Phase 3: Feedback Extraction\n",
    "            print(\"3ï¸âƒ£ Feedback Extraction Phase...\")\n",
    "            feedback_list = self.feedback_extractor.extract_feedback(test_case, failed_metrics)\n",
    "            print(f\"   ğŸ“‹ Extracted {len(feedback_list)} feedback items\")\n",
    "            \n",
    "            # Phase 4: Prompt Enhancement\n",
    "            print(\"4ï¸âƒ£ Prompt Enhancement Phase...\")\n",
    "            enhanced_prompt = self.prompt_enhancer.enhance_prompt(current_prompt, feedback_list)\n",
    "            \n",
    "            # Phase 5: Re-execution\n",
    "            print(\"5ï¸âƒ£ Re-execution Phase...\")\n",
    "            enhanced_output = self._execute_prompt(enhanced_prompt, test_input)\n",
    "            \n",
    "            # ÄÃ¡nh giÃ¡ output má»›i\n",
    "            enhanced_test_case = LLMTestCase(\n",
    "                input=test_input,\n",
    "                actual_output=enhanced_output,\n",
    "                context=[context] if context else None\n",
    "            )\n",
    "            \n",
    "            enhanced_scores, enhanced_failed = self._evaluate_test_case(enhanced_test_case, metrics)\n",
    "            \n",
    "            # TÃ­nh improvement\n",
    "            improvement = self._calculate_improvement(scores, enhanced_scores)\n",
    "            \n",
    "            # LÆ°u káº¿t quáº£\n",
    "            result = FeedbackResult(\n",
    "                iteration=iteration,\n",
    "                original_prompt=current_prompt if iteration == 1 else initial_prompt,\n",
    "                enhanced_prompt=enhanced_prompt,\n",
    "                original_output=output,\n",
    "                enhanced_output=enhanced_output,\n",
    "                original_scores=scores,\n",
    "                enhanced_scores=enhanced_scores,\n",
    "                improvement=improvement,\n",
    "                feedback_used=feedback_list,\n",
    "                timestamp=datetime.now().isoformat(),\n",
    "                success=len(enhanced_failed) == 0\n",
    "            )\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            # In káº¿t quáº£ iteration\n",
    "            self._print_iteration_summary(result)\n",
    "            \n",
    "            # Update prompt cho iteration tiáº¿p theo\n",
    "            current_prompt = enhanced_prompt\n",
    "            \n",
    "            if result.success:\n",
    "                print(\"âœ… Pipeline hoÃ n thÃ nh thÃ nh cÃ´ng!\")\n",
    "                return result\n",
    "        \n",
    "        print(f\"âš ï¸ ÄÃ£ Ä‘áº¡t max iterations ({self.max_iterations})\")\n",
    "        return self.results[-1] if self.results else None\n",
    "    \n",
    "    def _execute_prompt(self, prompt: str, test_input: str) -> str:\n",
    "        \"\"\"Thá»±c hiá»‡n prompt vá»›i input\"\"\"\n",
    "        full_prompt = f\"{prompt}\\n\\nInput: {test_input}\\n\\nOutput:\"\n",
    "        response = self.llm.invoke(full_prompt)\n",
    "        return response.content.strip()\n",
    "    \n",
    "    def _evaluate_test_case(self, test_case: LLMTestCase, metrics: List[BaseMetric]) -> Tuple[Dict[str, float], List[BaseMetric]]:\n",
    "        \"\"\"ÄÃ¡nh giÃ¡ test case vá»›i cÃ¡c metrics\"\"\"\n",
    "        scores = {}\n",
    "        failed_metrics = []\n",
    "        \n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                score = metric.measure(test_case)\n",
    "                scores[metric.__class__.__name__] = score\n",
    "                \n",
    "                if not metric.is_successful():\n",
    "                    failed_metrics.append(metric)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating {metric.__class__.__name__}: {e}\")\n",
    "                scores[metric.__class__.__name__] = 0.0\n",
    "                failed_metrics.append(metric)\n",
    "        \n",
    "        return scores, failed_metrics\n",
    "    \n",
    "    def _calculate_improvement(self, original_scores: Dict[str, float], enhanced_scores: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"TÃ­nh toÃ¡n má»©c cáº£i thiá»‡n\"\"\"\n",
    "        improvement = {}\n",
    "        for metric_name in original_scores:\n",
    "            if metric_name in enhanced_scores:\n",
    "                improvement[metric_name] = enhanced_scores[metric_name] - original_scores[metric_name]\n",
    "        return improvement\n",
    "    \n",
    "    def _print_iteration_summary(self, result: FeedbackResult):\n",
    "        \"\"\"In tÃ³m táº¯t iteration\"\"\"\n",
    "        print(f\"\\nğŸ“Š ITERATION {result.iteration} SUMMARY:\")\n",
    "        print(f\"   âœ… Success: {result.success}\")\n",
    "        \n",
    "        print(\"   ğŸ“ˆ Improvements:\")\n",
    "        for metric, improvement in result.improvement.items():\n",
    "            direction = \"ğŸ“ˆ\" if improvement > 0 else \"ğŸ“‰\" if improvement < 0 else \"â¡ï¸\"\n",
    "            print(f\"      {direction} {metric}: {improvement:+.3f}\")\n",
    "        \n",
    "        print(f\"   ğŸ’¬ Feedback items: {len(result.feedback_used)}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"âœ… Automated Feedback Pipeline Ä‘Ã£ Ä‘Æ°á»£c táº¡o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Thá»±c HÃ nh: Complex Problem Solving vá»›i Feedback Loop\n",
    "\n",
    "BÃ¢y giá» chÃºng ta sáº½ test pipeline vá»›i má»™t bÃ i toÃ¡n phá»©c táº¡p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o test case phá»©c táº¡p\n",
    "complex_problem = \"\"\"\n",
    "Má»™t cÃ´ng ty cÃ´ng nghá»‡ cÃ³ 200 nhÃ¢n viÃªn Ä‘ang gáº·p váº¥n Ä‘á» vá» hiá»‡u suáº¥t lÃ m viá»‡c tá»« xa. \n",
    "Kháº£o sÃ¡t cho tháº¥y: 60% nhÃ¢n viÃªn cáº£m tháº¥y thiáº¿u káº¿t ná»‘i vá»›i Ä‘á»“ng nghiá»‡p, \n",
    "45% bÃ¡o cÃ¡o khÃ³ khÄƒn trong viá»‡c quáº£n lÃ½ thá»i gian, vÃ  30% nÃ³i ráº±ng há» thiáº¿u \n",
    "Ä‘á»™ng lá»±c. Chi phÃ­ váº­n hÃ nh vÄƒn phÃ²ng Ä‘Ã£ giáº£m 40% nhÆ°ng doanh thu giáº£m 15%.\n",
    "\n",
    "HÃ£y phÃ¢n tÃ­ch váº¥n Ä‘á» vÃ  Ä‘á» xuáº¥t giáº£i phÃ¡p toÃ n diá»‡n.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt ban Ä‘áº§u (cá»‘ tÃ¬nh lÃ m Ä‘Æ¡n giáº£n Ä‘á»ƒ test feedback)\n",
    "initial_prompt = \"\"\"\n",
    "Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n doanh nghiá»‡p. HÃ£y phÃ¢n tÃ­ch váº¥n Ä‘á» vÃ  Ä‘Æ°a ra giáº£i phÃ¡p.\n",
    "Suy nghÄ© tá»«ng bÆ°á»›c má»™t cÃ¡ch logic.\n",
    "\"\"\"\n",
    "\n",
    "# Táº¡o metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡\n",
    "metrics = [\n",
    "    LogicalFlowMetric(threshold=0.7),\n",
    "    StepCompletenessMetric(threshold=0.8),\n",
    "    AnswerRelevancyMetric(threshold=0.8)\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ Test case Ä‘Ã£ Ä‘Æ°á»£c táº¡o!\")\n",
    "print(f\"Problem: {complex_problem[:100]}...\")\n",
    "print(f\"Initial prompt: {initial_prompt[:100]}...\")\n",
    "print(f\"Metrics: {[m.__class__.__name__ for m in metrics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cháº¡y Feedback Pipeline\n",
    "pipeline = AutomatedFeedbackPipeline(llm, max_iterations=3)\n",
    "\n",
    "print(\"ğŸš€ Báº¯t Ä‘áº§u cháº¡y Automated Feedback Pipeline...\\n\")\n",
    "\n",
    "final_result = pipeline.run_pipeline(\n",
    "    initial_prompt=initial_prompt,\n",
    "    test_input=complex_problem,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ PIPELINE HOÃ€N THÃ€NH!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š PhÃ¢n TÃ­ch Káº¿t Quáº£ Pipeline\n",
    "\n",
    "HÃ£y phÃ¢n tÃ­ch chi tiáº¿t káº¿t quáº£ cá»§a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhÃ¢n tÃ­ch káº¿t quáº£ chi tiáº¿t\n",
    "def analyze_pipeline_results(pipeline: AutomatedFeedbackPipeline):\n",
    "    \"\"\"PhÃ¢n tÃ­ch chi tiáº¿t káº¿t quáº£ pipeline\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š PHÃ‚N TÃCH Káº¾T QUáº¢ PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not pipeline.results:\n",
    "        print(\"âŒ KhÃ´ng cÃ³ káº¿t quáº£ Ä‘á»ƒ phÃ¢n tÃ­ch\")\n",
    "        return\n",
    "    \n",
    "    # Tá»•ng quan\n",
    "    total_iterations = len(pipeline.results)\n",
    "    final_success = pipeline.results[-1].success\n",
    "    \n",
    "    print(f\"ğŸ”¢ Tá»•ng sá»‘ iterations: {total_iterations}\")\n",
    "    print(f\"âœ… Káº¿t quáº£ cuá»‘i: {'ThÃ nh cÃ´ng' if final_success else 'Tháº¥t báº¡i'}\")\n",
    "    print()\n",
    "    \n",
    "    # PhÃ¢n tÃ­ch tá»«ng iteration\n",
    "    for i, result in enumerate(pipeline.results, 1):\n",
    "        print(f\"ğŸ”„ ITERATION {i}:\")\n",
    "        print(f\"   ğŸ“… Timestamp: {result.timestamp}\")\n",
    "        print(f\"   âœ… Success: {result.success}\")\n",
    "        \n",
    "        # Scores\n",
    "        print(\"   ğŸ“Š Scores:\")\n",
    "        if result.original_scores:\n",
    "            for metric, score in result.original_scores.items():\n",
    "                enhanced_score = result.enhanced_scores.get(metric, score)\n",
    "                improvement = result.improvement.get(metric, 0)\n",
    "                print(f\"      {metric}: {score:.3f} â†’ {enhanced_score:.3f} ({improvement:+.3f})\")\n",
    "        else:\n",
    "            for metric, score in result.enhanced_scores.items():\n",
    "                print(f\"      {metric}: {score:.3f}\")\n",
    "        \n",
    "        # Feedback sá»­ dá»¥ng\n",
    "        print(f\"   ğŸ’¬ Feedback items: {len(result.feedback_used)}\")\n",
    "        for j, feedback in enumerate(result.feedback_used[:2], 1):  # Chá»‰ hiá»ƒn thá»‹ 2 feedback Ä‘áº§u\n",
    "            print(f\"      {j}. {feedback[:100]}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # PhÃ¢n tÃ­ch improvement tá»•ng thá»ƒ\n",
    "    if len(pipeline.results) > 1:\n",
    "        print(\"ğŸ“ˆ Tá»”NG Káº¾T IMPROVEMENT:\")\n",
    "        first_result = pipeline.results[0]\n",
    "        last_result = pipeline.results[-1]\n",
    "        \n",
    "        for metric in first_result.enhanced_scores:\n",
    "            if metric in last_result.enhanced_scores:\n",
    "                initial_score = first_result.original_scores.get(metric, first_result.enhanced_scores[metric])\n",
    "                final_score = last_result.enhanced_scores[metric]\n",
    "                total_improvement = final_score - initial_score\n",
    "                print(f\"   {metric}: {initial_score:.3f} â†’ {final_score:.3f} ({total_improvement:+.3f})\")\n",
    "\n",
    "# Cháº¡y phÃ¢n tÃ­ch\n",
    "analyze_pipeline_results(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ So SÃ¡nh Prompt vÃ  Output\n",
    "\n",
    "HÃ£y so sÃ¡nh prompt vÃ  output ban Ä‘áº§u vá»›i káº¿t quáº£ cuá»‘i cÃ¹ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sÃ¡nh prompt vÃ  output\n",
    "def compare_prompts_and_outputs(pipeline: AutomatedFeedbackPipeline):\n",
    "    \"\"\"So sÃ¡nh prompt vÃ  output qua cÃ¡c iterations\"\"\"\n",
    "    \n",
    "    if not pipeline.results:\n",
    "        print(\"âŒ KhÃ´ng cÃ³ káº¿t quáº£ Ä‘á»ƒ so sÃ¡nh\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ” SO SÃNH PROMPT VÃ€ OUTPUT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prompt evolution\n",
    "    print(\"ğŸ“ EVOLUTION Cá»¦A PROMPT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, result in enumerate(pipeline.results, 1):\n",
    "        print(f\"\\nğŸ”„ ITERATION {i} PROMPT:\")\n",
    "        prompt_to_show = result.enhanced_prompt if i == 1 else result.enhanced_prompt\n",
    "        print(f\"   {prompt_to_show}\")\n",
    "        print()\n",
    "    \n",
    "    # Output comparison\n",
    "    print(\"\\nğŸ“„ SO SÃNH OUTPUT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, result in enumerate(pipeline.results, 1):\n",
    "        print(f\"\\nğŸ”„ ITERATION {i}:\")\n",
    "        \n",
    "        if result.original_output != \"N/A\":\n",
    "            print(f\"   ğŸ“¤ Original Output ({len(result.original_output)} chars):\")\n",
    "            print(f\"   {result.original_output[:200]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"   ğŸ“¥ Enhanced Output ({len(result.enhanced_output)} chars):\")\n",
    "        print(f\"   {result.enhanced_output[:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Final comparison\n",
    "    if len(pipeline.results) > 1:\n",
    "        first_result = pipeline.results[0]\n",
    "        last_result = pipeline.results[-1]\n",
    "        \n",
    "        print(\"\\nğŸ FINAL COMPARISON:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        initial_output = first_result.original_output if first_result.original_output != \"N/A\" else first_result.enhanced_output\n",
    "        final_output = last_result.enhanced_output\n",
    "        \n",
    "        print(f\"ğŸ“Š Length comparison: {len(initial_output)} â†’ {len(final_output)} chars\")\n",
    "        print(f\"ğŸ“ˆ Length change: {len(final_output) - len(initial_output):+d} chars\")\n",
    "\n",
    "# Cháº¡y so sÃ¡nh\n",
    "compare_prompts_and_outputs(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Advanced Feedback Strategies\n",
    "\n",
    "BÃ¢y giá» chÃºng ta sáº½ triá»ƒn khai cÃ¡c strategies nÃ¢ng cao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedFeedbackAnalyzer:\n",
    "    \"\"\"PhÃ¢n tÃ­ch feedback nÃ¢ng cao vá»›i pattern recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.failure_patterns = defaultdict(list)\n",
    "        self.improvement_history = []\n",
    "    \n",
    "    def analyze_failure_patterns(self, pipeline_results: List[FeedbackResult]) -> List[FailurePattern]:\n",
    "        \"\"\"PhÃ¢n tÃ­ch patterns cá»§a failures\"\"\"\n",
    "        \n",
    "        print(\"ğŸ” ANALYZING FAILURE PATTERNS...\")\n",
    "        \n",
    "        # Collect all feedback\n",
    "        all_feedback = []\n",
    "        for result in pipeline_results:\n",
    "            all_feedback.extend(result.feedback_used)\n",
    "        \n",
    "        if not all_feedback:\n",
    "            print(\"   â„¹ï¸ KhÃ´ng cÃ³ feedback Ä‘á»ƒ phÃ¢n tÃ­ch\")\n",
    "            return []\n",
    "        \n",
    "        # Categorize feedback\n",
    "        categories = self._categorize_feedback(all_feedback)\n",
    "        \n",
    "        # Create failure patterns\n",
    "        patterns = []\n",
    "        for category, feedback_items in categories.items():\n",
    "            if len(feedback_items) > 1:  # Pattern náº¿u xuáº¥t hiá»‡n > 1 láº§n\n",
    "                pattern = FailurePattern(\n",
    "                    pattern_type=category,\n",
    "                    description=f\"Recurring issue with {category.lower()}\",\n",
    "                    frequency=len(feedback_items),\n",
    "                    suggested_fix=self._generate_pattern_fix(category, feedback_items),\n",
    "                    examples=feedback_items[:3]  # Top 3 examples\n",
    "                )\n",
    "                patterns.append(pattern)\n",
    "        \n",
    "        # Print patterns\n",
    "        self._print_failure_patterns(patterns)\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _categorize_feedback(self, feedback_list: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"PhÃ¢n loáº¡i feedback theo categories\"\"\"\n",
    "        \n",
    "        categories = defaultdict(list)\n",
    "        \n",
    "        # Define keywords for categories\n",
    "        category_keywords = {\n",
    "            \"Logic Flow\": [\"logic\", \"flow\", \"sequence\", \"order\", \"reasoning\", \"step\"],\n",
    "            \"Completeness\": [\"missing\", \"incomplete\", \"lack\", \"absent\", \"need more\"],\n",
    "            \"Relevance\": [\"relevant\", \"related\", \"connection\", \"context\", \"topic\"],\n",
    "            \"Structure\": [\"structure\", \"organization\", \"format\", \"layout\", \"arrange\"],\n",
    "            \"Clarity\": [\"clear\", \"unclear\", \"confusing\", \"ambiguous\", \"vague\"]\n",
    "        }\n",
    "        \n",
    "        for feedback in feedback_list:\n",
    "            feedback_lower = feedback.lower()\n",
    "            categorized = False\n",
    "            \n",
    "            for category, keywords in category_keywords.items():\n",
    "                if any(keyword in feedback_lower for keyword in keywords):\n",
    "                    categories[category].append(feedback)\n",
    "                    categorized = True\n",
    "                    break\n",
    "            \n",
    "            if not categorized:\n",
    "                categories[\"Other\"].append(feedback)\n",
    "        \n",
    "        return dict(categories)\n",
    "    \n",
    "    def _generate_pattern_fix(self, category: str, feedback_items: List[str]) -> str:\n",
    "        \"\"\"Táº¡o suggested fix cho pattern\"\"\"\n",
    "        \n",
    "        fix_templates = {\n",
    "            \"Logic Flow\": \"Add explicit step-by-step reasoning with clear transitions\",\n",
    "            \"Completeness\": \"Include comprehensive analysis covering all aspects\",\n",
    "            \"Relevance\": \"Ensure all points directly address the main question\",\n",
    "            \"Structure\": \"Use clear headings and organized sections\",\n",
    "            \"Clarity\": \"Simplify language and provide clear explanations\"\n",
    "        }\n",
    "        \n",
    "        return fix_templates.get(category, \"Review and improve based on feedback\")\n",
    "    \n",
    "    def _print_failure_patterns(self, patterns: List[FailurePattern]):\n",
    "        \"\"\"In failure patterns\"\"\"\n",
    "        \n",
    "        if not patterns:\n",
    "            print(\"   âœ… KhÃ´ng phÃ¡t hiá»‡n failure patterns láº·p láº¡i\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   ğŸ” PhÃ¡t hiá»‡n {len(patterns)} failure patterns:\")\n",
    "        print()\n",
    "        \n",
    "        for i, pattern in enumerate(patterns, 1):\n",
    "            print(f\"   {i}. ğŸ“Š {pattern.pattern_type}\")\n",
    "            print(f\"      ğŸ“ˆ Frequency: {pattern.frequency}\")\n",
    "            print(f\"      ğŸ“‹ Description: {pattern.description}\")\n",
    "            print(f\"      ğŸ’¡ Suggested fix: {pattern.suggested_fix}\")\n",
    "            print(f\"      ğŸ“ Example: {pattern.examples[0][:100]}...\")\n",
    "            print()\n",
    "    \n",
    "    def generate_meta_feedback(self, pipeline_results: List[FeedbackResult]) -> str:\n",
    "        \"\"\"Táº¡o meta-feedback vá» toÃ n bá»™ pipeline\"\"\"\n",
    "        \n",
    "        if not pipeline_results:\n",
    "            return \"No results to analyze\"\n",
    "        \n",
    "        meta_prompt = f\"\"\"\n",
    "        PhÃ¢n tÃ­ch toÃ n bá»™ quÃ¡ trÃ¬nh feedback loop vá»›i {len(pipeline_results)} iterations:\n",
    "        \n",
    "        RESULTS SUMMARY:\n",
    "        {self._format_results_summary(pipeline_results)}\n",
    "        \n",
    "        HÃ£y Ä‘Æ°a ra meta-feedback vá»:\n",
    "        1. Hiá»‡u quáº£ cá»§a feedback loop\n",
    "        2. Patterns trong quÃ¡ trÃ¬nh cáº£i thiá»‡n\n",
    "        3. Äiá»ƒm máº¡nh vÃ  Ä‘iá»ƒm yáº¿u\n",
    "        4. Äá» xuáº¥t Ä‘á»ƒ tá»‘i Æ°u pipeline\n",
    "        \n",
    "        Tráº£ lá»i ngáº¯n gá»n, táº­p trung vÃ o insights há»¯u Ã­ch.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(meta_prompt)\n",
    "        return response.content.strip()\n",
    "    \n",
    "    def _format_results_summary(self, results: List[FeedbackResult]) -> str:\n",
    "        \"\"\"Format tÃ³m táº¯t káº¿t quáº£\"\"\"\n",
    "        summary_lines = []\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            success_status = \"âœ…\" if result.success else \"âŒ\"\n",
    "            avg_improvement = np.mean(list(result.improvement.values())) if result.improvement else 0\n",
    "            summary_lines.append(\n",
    "                f\"Iteration {i}: {success_status} Success={result.success}, \"\n",
    "                f\"Avg_Improvement={avg_improvement:.3f}, \"\n",
    "                f\"Feedback_Count={len(result.feedback_used)}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\".join(summary_lines)\n",
    "\n",
    "print(\"âœ… Advanced Feedback Analyzer Ä‘Ã£ Ä‘Æ°á»£c táº¡o!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cháº¡y advanced analysis\n",
    "advanced_analyzer = AdvancedFeedbackAnalyzer(llm)\n",
    "\n",
    "print(\"ğŸ” ADVANCED FEEDBACK ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PhÃ¢n tÃ­ch failure patterns\n",
    "failure_patterns = advanced_analyzer.analyze_failure_patterns(pipeline.results)\n",
    "\n",
    "print(\"\\nğŸ§  META-FEEDBACK:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Táº¡o meta-feedback\n",
    "meta_feedback = advanced_analyzer.generate_meta_feedback(pipeline.results)\n",
    "print(meta_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Performance Tracking vÃ  Visualization\n",
    "\n",
    "Táº¡o dashboard Ä‘á»ƒ theo dÃµi performance qua cÃ¡c iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "def create_performance_dashboard(pipeline_results: List[FeedbackResult]):\n",
    "    \"\"\"Táº¡o dashboard hiá»ƒn thá»‹ performance\"\"\"\n",
    "    \n",
    "    if not pipeline_results:\n",
    "        print(\"âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ visualize\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    iterations = list(range(1, len(pipeline_results) + 1))\n",
    "    \n",
    "    # Collect metrics data\n",
    "    metrics_data = defaultdict(list)\n",
    "    success_data = []\n",
    "    \n",
    "    for result in pipeline_results:\n",
    "        # Use enhanced scores (final scores for each iteration)\n",
    "        for metric_name, score in result.enhanced_scores.items():\n",
    "            metrics_data[metric_name].append(score)\n",
    "        \n",
    "        success_data.append(1 if result.success else 0)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ğŸš€ Feedback Pipeline Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Metrics scores over iterations\n",
    "    ax1 = axes[0, 0]\n",
    "    for metric_name, scores in metrics_data.items():\n",
    "        if len(scores) == len(iterations):\n",
    "            ax1.plot(iterations, scores, marker='o', linewidth=2, label=metric_name)\n",
    "    \n",
    "    ax1.set_title('ğŸ“ˆ Metrics Scores Over Iterations')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Success rate\n",
    "    ax2 = axes[0, 1]\n",
    "    colors = ['red' if s == 0 else 'green' for s in success_data]\n",
    "    ax2.bar(iterations, success_data, color=colors, alpha=0.7)\n",
    "    ax2.set_title('âœ… Success Rate by Iteration')\n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Success (1) / Failure (0)')\n",
    "    ax2.set_ylim(0, 1.2)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Improvement trends\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    improvement_data = defaultdict(list)\n",
    "    for result in pipeline_results:\n",
    "        for metric_name, improvement in result.improvement.items():\n",
    "            improvement_data[metric_name].append(improvement)\n",
    "    \n",
    "    for metric_name, improvements in improvement_data.items():\n",
    "        if len(improvements) == len(iterations):\n",
    "            ax3.bar([i + 0.1 * list(improvement_data.keys()).index(metric_name) for i in iterations], \n",
    "                   improvements, width=0.15, label=metric_name, alpha=0.8)\n",
    "    \n",
    "    ax3.set_title('ğŸ“Š Improvement per Iteration')\n",
    "    ax3.set_xlabel('Iteration')\n",
    "    ax3.set_ylabel('Improvement Score')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 4: Feedback count per iteration\n",
    "    ax4 = axes[1, 1]\n",
    "    feedback_counts = [len(result.feedback_used) for result in pipeline_results]\n",
    "    ax4.bar(iterations, feedback_counts, color='purple', alpha=0.7)\n",
    "    ax4.set_title('ğŸ’¬ Feedback Items per Iteration')\n",
    "    ax4.set_xlabel('Iteration')\n",
    "    ax4.set_ylabel('Number of Feedback Items')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nğŸ“Š PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    final_success_rate = sum(success_data) / len(success_data)\n",
    "    print(f\"âœ… Overall success rate: {final_success_rate:.1%}\")\n",
    "    \n",
    "    if metrics_data:\n",
    "        print(\"\\nğŸ“ˆ Final Metrics Scores:\")\n",
    "        for metric_name, scores in metrics_data.items():\n",
    "            if scores:\n",
    "                initial_score = scores[0]\n",
    "                final_score = scores[-1]\n",
    "                improvement = final_score - initial_score\n",
    "                print(f\"   {metric_name}: {initial_score:.3f} â†’ {final_score:.3f} ({improvement:+.3f})\")\n",
    "    \n",
    "    total_feedback = sum(feedback_counts)\n",
    "    avg_feedback = total_feedback / len(feedback_counts) if feedback_counts else 0\n",
    "    print(f\"\\nğŸ’¬ Total feedback items: {total_feedback}\")\n",
    "    print(f\"ğŸ’¬ Average feedback per iteration: {avg_feedback:.1f}\")\n",
    "\n",
    "# Táº¡o dashboard\n",
    "create_performance_dashboard(pipeline.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Real-World Case Study: Code Review Agent\n",
    "\n",
    "BÃ¢y giá» chÃºng ta sáº½ Ã¡p dá»¥ng feedback loop cho má»™t use case thá»±c táº¿ - Code Review Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world case study: Code Review Agent\n",
    "code_review_problem = \"\"\"\n",
    "HÃ£y review Ä‘oáº¡n code Python sau vÃ  Ä‘Æ°a ra feedback chi tiáº¿t:\n",
    "\n",
    "```python\n",
    "def process_user_data(data):\n",
    "    users = []\n",
    "    for item in data:\n",
    "        if item['age'] > 18:\n",
    "            user = {\n",
    "                'name': item['name'],\n",
    "                'email': item['email'],\n",
    "                'age': item['age']\n",
    "            }\n",
    "            users.append(user)\n",
    "    return users\n",
    "\n",
    "def send_emails(users):\n",
    "    import smtplib\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.login('admin@company.com', 'password123')\n",
    "    \n",
    "    for user in users:\n",
    "        message = f\"Hello {user['name']}, welcome!\"\n",
    "        server.sendmail('admin@company.com', user['email'], message)\n",
    "    \n",
    "    server.quit()\n",
    "```\n",
    "\n",
    "Táº­p trung vÃ o: security issues, error handling, code quality, performance.\n",
    "\"\"\"\n",
    "\n",
    "# Code review prompt (cá»‘ tÃ¬nh Ä‘Æ¡n giáº£n)\n",
    "code_review_initial_prompt = \"\"\"\n",
    "Báº¡n lÃ  má»™t senior developer. HÃ£y review code vÃ  Ä‘Æ°a ra feedback.\n",
    "HÃ£y suy nghÄ© tá»«ng bÆ°á»›c má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng.\n",
    "\"\"\"\n",
    "\n",
    "# Custom metrics cho code review\n",
    "class SecurityAnalysisMetric(BaseMetric):\n",
    "    \"\"\"ÄÃ¡nh giÃ¡ kháº£ nÄƒng phÃ¢n tÃ­ch security issues\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.8):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        security_keywords = [\n",
    "            'password', 'hardcoded', 'credential', 'authentication',\n",
    "            'security', 'vulnerability', 'sensitive', 'encryption',\n",
    "            'sql injection', 'xss', 'csrf', 'validation'\n",
    "        ]\n",
    "        \n",
    "        output_lower = test_case.actual_output.lower()\n",
    "        security_mentions = sum(1 for keyword in security_keywords if keyword in output_lower)\n",
    "        \n",
    "        # Check if mentions hardcoded password specifically\n",
    "        has_password_issue = 'password123' in test_case.actual_output or 'hardcoded' in output_lower\n",
    "        \n",
    "        # Score based on security awareness\n",
    "        score = min(1.0, (security_mentions * 0.2) + (0.5 if has_password_issue else 0))\n",
    "        \n",
    "        self.score = score\n",
    "        self.reason = f\"Security analysis score: {score:.2f} (mentions: {security_mentions})\"  \n",
    "        self.success = score >= self.threshold\n",
    "        return score\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "class ErrorHandlingMetric(BaseMetric):\n",
    "    \"\"\"ÄÃ¡nh giÃ¡ kháº£ nÄƒng phÃ¡t hiá»‡n thiáº¿u error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.7):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        error_keywords = [\n",
    "            'try', 'except', 'error', 'exception', 'handle',\n",
    "            'validation', 'check', 'null', 'none', 'empty'\n",
    "        ]\n",
    "        \n",
    "        output_lower = test_case.actual_output.lower()\n",
    "        error_mentions = sum(1 for keyword in error_keywords if keyword in output_lower)\n",
    "        \n",
    "        # Specific issues that should be caught\n",
    "        catches_key_error = 'keyerror' in output_lower or 'key error' in output_lower\n",
    "        mentions_validation = 'validation' in output_lower or 'validate' in output_lower\n",
    "        \n",
    "        score = min(1.0, (error_mentions * 0.15) + \n",
    "                   (0.3 if catches_key_error else 0) + \n",
    "                   (0.2 if mentions_validation else 0))\n",
    "        \n",
    "        self.score = score\n",
    "        self.reason = f\"Error handling analysis: {score:.2f} (mentions: {error_mentions})\"\n",
    "        self.success = score >= self.threshold\n",
    "        return score\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "# Code review metrics\n",
    "code_review_metrics = [\n",
    "    LogicalFlowMetric(threshold=0.7),\n",
    "    StepCompletenessMetric(threshold=0.8),\n",
    "    SecurityAnalysisMetric(threshold=0.8),\n",
    "    ErrorHandlingMetric(threshold=0.7)\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ Code Review Case Study Ä‘Ã£ Ä‘Æ°á»£c setup!\")\n",
    "print(f\"Problem: Code review for security and quality issues\")\n",
    "print(f\"Metrics: {[m.__class__.__name__ for m in code_review_metrics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cháº¡y Code Review Pipeline\n",
    "code_review_pipeline = AutomatedFeedbackPipeline(llm, max_iterations=4)\n",
    "\n",
    "print(\"ğŸ” CHáº Y CODE REVIEW FEEDBACK PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "code_review_result = code_review_pipeline.run_pipeline(\n",
    "    initial_prompt=code_review_initial_prompt,\n",
    "    test_input=code_review_problem,\n",
    "    metrics=code_review_metrics\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ CODE REVIEW PIPELINE HOÃ€N THÃ€NH!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhÃ¢n tÃ­ch káº¿t quáº£ Code Review\n",
    "print(\"ğŸ“Š PHÃ‚N TÃCH Káº¾T QUáº¢ CODE REVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "analyze_pipeline_results(code_review_pipeline)\n",
    "\n",
    "print(\"\\nğŸ“‹ SO SÃNH CODE REVIEW OUTPUT:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if code_review_pipeline.results:\n",
    "    final_result = code_review_pipeline.results[-1]\n",
    "    \n",
    "    print(\"ğŸ¯ FINAL CODE REVIEW OUTPUT:\")\n",
    "    print(final_result.enhanced_output)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ FINAL SCORES:\")\n",
    "    for metric, score in final_result.enhanced_scores.items():\n",
    "        status = \"âœ…\" if score >= 0.7 else \"âŒ\"\n",
    "        print(f\"   {status} {metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o dashboard cho Code Review Pipeline\n",
    "print(\"ğŸ“Š CODE REVIEW PERFORMANCE DASHBOARD\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "create_performance_dashboard(code_review_pipeline.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Adaptive Threshold Adjustment\n",
    "\n",
    "Triá»ƒn khai tÃ­nh nÄƒng tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh threshold dá»±a trÃªn performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveThresholdManager:\n",
    "    \"\"\"Quáº£n lÃ½ adaptive threshold cho metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_thresholds: Dict[str, float]):\n",
    "        self.initial_thresholds = initial_thresholds.copy()\n",
    "        self.current_thresholds = initial_thresholds.copy()\n",
    "        self.performance_history = defaultdict(list)\n",
    "        self.adjustment_history = defaultdict(list)\n",
    "    \n",
    "    def update_thresholds(self, pipeline_results: List[FeedbackResult]) -> Dict[str, float]:\n",
    "        \"\"\"Cáº­p nháº­t thresholds dá»±a trÃªn performance history\"\"\"\n",
    "        \n",
    "        if not pipeline_results:\n",
    "            return self.current_thresholds\n",
    "        \n",
    "        # Collect performance data\n",
    "        for result in pipeline_results:\n",
    "            for metric_name, score in result.enhanced_scores.items():\n",
    "                self.performance_history[metric_name].append(score)\n",
    "        \n",
    "        # Adjust thresholds\n",
    "        new_thresholds = {}\n",
    "        \n",
    "        for metric_name, scores in self.performance_history.items():\n",
    "            if len(scores) < 2:\n",
    "                new_thresholds[metric_name] = self.current_thresholds.get(metric_name, 0.7)\n",
    "                continue\n",
    "            \n",
    "            # Calculate statistics\n",
    "            avg_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            recent_scores = scores[-3:]  # Last 3 scores\n",
    "            recent_avg = np.mean(recent_scores)\n",
    "            \n",
    "            current_threshold = self.current_thresholds.get(metric_name, 0.7)\n",
    "            \n",
    "            # Adjustment logic\n",
    "            if recent_avg > current_threshold + 0.1:  # Consistently performing well\n",
    "                new_threshold = min(0.9, current_threshold + 0.05)  # Increase threshold\n",
    "                adjustment_reason = \"Performance consistently high\"\n",
    "            elif recent_avg < current_threshold - 0.1:  # Consistently underperforming\n",
    "                new_threshold = max(0.3, current_threshold - 0.05)  # Decrease threshold\n",
    "                adjustment_reason = \"Performance consistently low\"\n",
    "            else:\n",
    "                new_threshold = current_threshold  # Keep current\n",
    "                adjustment_reason = \"Performance stable\"\n",
    "            \n",
    "            new_thresholds[metric_name] = new_threshold\n",
    "            \n",
    "            # Record adjustment\n",
    "            if new_threshold != current_threshold:\n",
    "                self.adjustment_history[metric_name].append({\n",
    "                    'old_threshold': current_threshold,\n",
    "                    'new_threshold': new_threshold,\n",
    "                    'reason': adjustment_reason,\n",
    "                    'avg_score': avg_score,\n",
    "                    'recent_avg': recent_avg\n",
    "                })\n",
    "        \n",
    "        self.current_thresholds = new_thresholds\n",
    "        return new_thresholds\n",
    "    \n",
    "    def print_threshold_adjustments(self):\n",
    "        \"\"\"In lá»‹ch sá»­ Ä‘iá»u chá»‰nh threshold\"\"\"\n",
    "        \n",
    "        print(\"ğŸ¯ ADAPTIVE THRESHOLD ADJUSTMENTS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if not any(self.adjustment_history.values()):\n",
    "            print(\"   â„¹ï¸ ChÆ°a cÃ³ Ä‘iá»u chá»‰nh threshold nÃ o\")\n",
    "            return\n",
    "        \n",
    "        for metric_name, adjustments in self.adjustment_history.items():\n",
    "            if adjustments:\n",
    "                print(f\"\\nğŸ“Š {metric_name}:\")\n",
    "                for i, adj in enumerate(adjustments, 1):\n",
    "                    direction = \"ğŸ“ˆ\" if adj['new_threshold'] > adj['old_threshold'] else \"ğŸ“‰\"\n",
    "                    print(f\"   {i}. {direction} {adj['old_threshold']:.3f} â†’ {adj['new_threshold']:.3f}\")\n",
    "                    print(f\"      Reason: {adj['reason']}\")\n",
    "                    print(f\"      Recent avg: {adj['recent_avg']:.3f}\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ CURRENT THRESHOLDS:\")\n",
    "        for metric_name, threshold in self.current_thresholds.items():\n",
    "            initial = self.initial_thresholds.get(metric_name, threshold)\n",
    "            change = threshold - initial\n",
    "            direction = \"ğŸ“ˆ\" if change > 0 else \"ğŸ“‰\" if change < 0 else \"â¡ï¸\"\n",
    "            print(f\"   {direction} {metric_name}: {initial:.3f} â†’ {threshold:.3f} ({change:+.3f})\")\n",
    "\n",
    "# Test Adaptive Threshold Manager\n",
    "initial_thresholds = {\n",
    "    'LogicalFlowMetric': 0.7,\n",
    "    'StepCompletenessMetric': 0.8,\n",
    "    'SecurityAnalysisMetric': 0.8,\n",
    "    'ErrorHandlingMetric': 0.7\n",
    "}\n",
    "\n",
    "threshold_manager = AdaptiveThresholdManager(initial_thresholds)\n",
    "\n",
    "# Update vá»›i káº¿t quáº£ tá»« code review pipeline\n",
    "updated_thresholds = threshold_manager.update_thresholds(code_review_pipeline.results)\n",
    "\n",
    "# Print adjustments\n",
    "threshold_manager.print_threshold_adjustments()\n",
    "\n",
    "print(\"\\nâœ… Adaptive Threshold Manager hoÃ n thÃ nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Export vÃ  LÆ°u Trá»¯ Káº¿t Quáº£\n",
    "\n",
    "Cuá»‘i cÃ¹ng, chÃºng ta sáº½ export káº¿t quáº£ Ä‘á»ƒ phÃ¢n tÃ­ch sau nÃ y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackResultsExporter:\n",
    "    \"\"\"Export feedback results cho analysis vÃ  reporting\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.export_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    def export_to_json(self, pipeline_results: List[FeedbackResult], filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export káº¿t quáº£ ra JSON file\"\"\"\n",
    "        \n",
    "        if not filename:\n",
    "            filename = f\"feedback_results_{self.export_timestamp}.json\"\n",
    "        \n",
    "        # Convert to serializable format\n",
    "        export_data = {\n",
    "            'export_timestamp': self.export_timestamp,\n",
    "            'total_iterations': len(pipeline_results),\n",
    "            'overall_success': pipeline_results[-1].success if pipeline_results else False,\n",
    "            'results': [asdict(result) for result in pipeline_results]\n",
    "        }\n",
    "        \n",
    "        # Write to file\n",
    "        filepath = f\"data/{filename}\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Exported results to {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def export_to_csv(self, pipeline_results: List[FeedbackResult], filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export metrics data ra CSV\"\"\"\n",
    "        \n",
    "        if not filename:\n",
    "            filename = f\"feedback_metrics_{self.export_timestamp}.csv\"\n",
    "        \n",
    "        # Prepare data for DataFrame\n",
    "        rows = []\n",
    "        \n",
    "        for result in pipeline_results:\n",
    "            base_row = {\n",
    "                'iteration': result.iteration,\n",
    "                'timestamp': result.timestamp,\n",
    "                'success': result.success,\n",
    "                'feedback_count': len(result.feedback_used)\n",
    "            }\n",
    "            \n",
    "            # Add metric scores\n",
    "            for metric_name, score in result.enhanced_scores.items():\n",
    "                base_row[f'{metric_name}_score'] = score\n",
    "            \n",
    "            # Add improvements\n",
    "            for metric_name, improvement in result.improvement.items():\n",
    "                base_row[f'{metric_name}_improvement'] = improvement\n",
    "            \n",
    "            rows.append(base_row)\n",
    "        \n",
    "        # Create DataFrame and save\n",
    "        df = pd.DataFrame(rows)\n",
    "        filepath = f\"data/{filename}\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        df.to_csv(filepath, index=False)\n",
    "        \n",
    "        print(f\"âœ… Exported metrics to {filepath}\")\n",
    "        print(f\"ğŸ“Š Data shape: {df.shape}\")\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    def generate_summary_report(self, pipeline_results: List[FeedbackResult]) -> str:\n",
    "        \"\"\"Táº¡o summary report\"\"\"\n",
    "        \n",
    "        if not pipeline_results:\n",
    "            return \"No results to report\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"ğŸš€ FEEDBACK PIPELINE SUMMARY REPORT\",\n",
    "            \"=\" * 50,\n",
    "            f\"ğŸ“… Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"ğŸ”„ Total Iterations: {len(pipeline_results)}\",\n",
    "            f\"âœ… Final Success: {pipeline_results[-1].success}\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        # Performance summary\n",
    "        report_lines.extend([\n",
    "            \"ğŸ“Š PERFORMANCE SUMMARY:\",\n",
    "            \"-\" * 30\n",
    "        ])\n",
    "        \n",
    "        # Collect all metrics\n",
    "        all_metrics = set()\n",
    "        for result in pipeline_results:\n",
    "            all_metrics.update(result.enhanced_scores.keys())\n",
    "        \n",
    "        for metric_name in sorted(all_metrics):\n",
    "            scores = []\n",
    "            for result in pipeline_results:\n",
    "                if metric_name in result.enhanced_scores:\n",
    "                    scores.append(result.enhanced_scores[metric_name])\n",
    "            \n",
    "            if scores:\n",
    "                initial_score = scores[0]\n",
    "                final_score = scores[-1]\n",
    "                avg_score = np.mean(scores)\n",
    "                improvement = final_score - initial_score\n",
    "                \n",
    "                direction = \"ğŸ“ˆ\" if improvement > 0 else \"ğŸ“‰\" if improvement < 0 else \"â¡ï¸\"\n",
    "                report_lines.append(\n",
    "                    f\"   {direction} {metric_name}:\"\n",
    "                )\n",
    "                report_lines.append(\n",
    "                    f\"      Initial: {initial_score:.3f} | Final: {final_score:.3f} | \"\n",
    "                    f\"Avg: {avg_score:.3f} | Change: {improvement:+.3f}\"\n",
    "                )\n",
    "        \n",
    "        # Feedback analysis\n",
    "        total_feedback = sum(len(result.feedback_used) for result in pipeline_results)\n",
    "        avg_feedback = total_feedback / len(pipeline_results)\n",
    "        \n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"ğŸ’¬ FEEDBACK ANALYSIS:\",\n",
    "            \"-\" * 30,\n",
    "            f\"   Total feedback items: {total_feedback}\",\n",
    "            f\"   Average per iteration: {avg_feedback:.1f}\",\n",
    "        ])\n",
    "        \n",
    "        # Success progression\n",
    "        success_progression = [result.success for result in pipeline_results]\n",
    "        success_rate = sum(success_progression) / len(success_progression)\n",
    "        \n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"âœ… SUCCESS ANALYSIS:\",\n",
    "            \"-\" * 30,\n",
    "            f\"   Success rate: {success_rate:.1%}\",\n",
    "            f\"   Success progression: {' â†’ '.join('âœ…' if s else 'âŒ' for s in success_progression)}\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# Export results\n",
    "exporter = FeedbackResultsExporter()\n",
    "\n",
    "print(\"ğŸ“¤ EXPORTING FEEDBACK RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Export JSON\n",
    "json_file = exporter.export_to_json(code_review_pipeline.results)\n",
    "\n",
    "# Export CSV\n",
    "csv_file = exporter.export_to_csv(code_review_pipeline.results)\n",
    "\n",
    "# Generate and print report\n",
    "print(\"\\nğŸ“‹ SUMMARY REPORT:\")\n",
    "print(\"-\" * 40)\n",
    "summary_report = exporter.generate_summary_report(code_review_pipeline.results)\n",
    "print(summary_report)\n",
    "\n",
    "# Save report to file\n",
    "report_filename = f\"data/feedback_report_{exporter.export_timestamp}.txt\"\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nâœ… Report saved to {report_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Tá»•ng Káº¿t vÃ  BÃ i Táº­p\n",
    "\n",
    "### ğŸ“š Kiáº¿n Thá»©c ÄÃ£ Há»c\n",
    "\n",
    "Trong notebook nÃ y, chÃºng ta Ä‘Ã£ há»c:\n",
    "\n",
    "1. **Automated Feedback Pipeline**: 5-phase execution cycle\n",
    "2. **Advanced Feedback Strategies**: Pattern recognition, meta-feedback\n",
    "3. **Adaptive Thresholds**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh dá»±a trÃªn performance\n",
    "4. **Real-world Applications**: Code Review Agent use case\n",
    "5. **Performance Tracking**: Visualization vÃ  analysis\n",
    "6. **Export vÃ  Reporting**: LÆ°u trá»¯ káº¿t quáº£ cho phÃ¢n tÃ­ch sau\n",
    "\n",
    "### ğŸ¯ BÃ i Táº­p Thá»±c HÃ nh\n",
    "\n",
    "1. **TÃ¹y chá»‰nh Metrics**: Táº¡o custom metrics cho domain cá»¥ thá»ƒ\n",
    "2. **Multi-agent Evaluation**: Ãp dá»¥ng cho complex agent workflows\n",
    "3. **Production Integration**: TÃ­ch há»£p vÃ o CI/CD pipeline\n",
    "4. **A/B Testing**: So sÃ¡nh different feedback strategies\n",
    "\n",
    "### ğŸš€ á»¨ng Dá»¥ng Thá»±c Tiá»…n\n",
    "\n",
    "Framework nÃ y cÃ³ thá»ƒ Ã¡p dá»¥ng cho:\n",
    "- **Code Review Automation**\n",
    "- **Content Generation Quality Control**\n",
    "- **Customer Support Bot Improvement**\n",
    "- **Educational Content Assessment**\n",
    "- **Legal Document Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ChÃºc má»«ng!** Báº¡n Ä‘Ã£ hoÃ n thÃ nh series DeepEval comprehensive framework!\n",
    "\n",
    "Tiáº¿p theo, hÃ£y Ã¡p dá»¥ng nhá»¯ng kiáº¿n thá»©c nÃ y vÃ o dá»± Ã¡n thá»±c táº¿ cá»§a báº¡n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration - Mini exercise\n",
    "print(\"ğŸ¯ MINI EXERCISE: Táº¡o Feedback Loop cho Domain cá»§a Báº¡n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "ğŸ“ CHALLENGE:\n",
    "HÃ£y táº¡o má»™t feedback loop cho má»™t use case cá»¥ thá»ƒ trong domain cá»§a báº¡n.\n",
    "\n",
    "STEPS:\n",
    "1. XÃ¡c Ä‘á»‹nh problem domain (VD: Email writing, SQL query generation, etc.)\n",
    "2. Táº¡o 2-3 custom metrics phÃ¹ há»£p\n",
    "3. Thiáº¿t káº¿ initial prompt (cá»‘ tÃ¬nh Ä‘Æ¡n giáº£n)\n",
    "4. Cháº¡y feedback pipeline\n",
    "5. PhÃ¢n tÃ­ch káº¿t quáº£\n",
    "\n",
    "Tá»ª KHÃ“A QUAN TRá»ŒNG:\n",
    "- Domain-specific metrics\n",
    "- Iterative improvement\n",
    "- Performance tracking\n",
    "- Real-world applicability\n",
    "\n",
    "ğŸ’¡ TIP: Báº¯t Ä‘áº§u vá»›i use case Ä‘Æ¡n giáº£n, sau Ä‘Ã³ má»Ÿ rá»™ng complexity!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸš€ Báº¡n Ä‘Ã£ sáºµn sÃ ng Ã¡p dá»¥ng DeepEval vÃ o dá»± Ã¡n thá»±c táº¿!\")\n",
    "print(\"âœ¨ Good luck vÃ  happy coding! âœ¨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}