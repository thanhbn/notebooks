{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Dataset Quality Filtering with LLM Classifiers\n",
    "\n",
    "## Learning Objective\n",
    "Master the pairwise comparison technique for dataset quality filtering using LLM classifiers, as described in Section III.B of the Kotlin ML Pack paper.\n",
    "\n",
    "## Paper Reference\n",
    "- **Section**: III.B - KStack-clean: Learning the code quality\n",
    "- **Key Insight**: Small curated datasets (25K examples) can outperform large uncurated ones (4M files)\n",
    "- **Technique**: LLM-based pairwise comparison for quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Foundation\n",
    "\n",
    "### 1.1 The Problem\n",
    "From the paper: \"Using curated datasets to fine-tune a model can provide larger improvements than fine-tuning it on a bigger corpus of non-curated data.\"\n",
    "\n",
    "### 1.2 The Solution: Pairwise Quality Scoring\n",
    "\n",
    "The paper introduces a clever scoring formula:\n",
    "\n",
    "$$s(f) = \\frac{(s(f,c)_A - s(f,c)_B) + (s(c,f)_B - s(c,f)_A)}{2}$$\n",
    "\n",
    "Where:\n",
    "- $f$ is the file being evaluated\n",
    "- $c$ is a comparison file\n",
    "- $s(f,c)_A$ is the probability of choosing A when $f$ is labeled A and $c$ is labeled B\n",
    "- The formula accounts for ordering bias by testing both directions\n",
    "\n",
    "### 1.3 Three-Pass Approximation\n",
    "Since comparing all pairs is $O(n^2)$ complexity, the paper uses a clever three-pass approach:\n",
    "1. **Pass 1**: Compare random sample against dataset\n",
    "2. **Pass 2**: Compare highest-scored file from Pass 1 against dataset\n",
    "3. **Pass 3**: Compare highest-scored file from Pass 2 against dataset\n",
    "4. **Final Score**: Average of Pass 2 and Pass 3 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai numpy pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of Pairwise Quality Classifier\n",
    "\n",
    "Let's implement the exact algorithm from the paper with detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CodeFile:\n",
    "    \"\"\"Represents a code file for quality assessment\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    score: float = 0.0\n",
    "    comparisons: int = 0\n",
    "\n",
    "class PairwiseQualityClassifier:\n",
    "    \"\"\"\n",
    "    Implements the pairwise quality classification from Section III.B.\n",
    "    \n",
    "    Key innovation: Uses bidirectional comparison to eliminate ordering bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\", use_mock: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: LLM to use for comparison\n",
    "            use_mock: If True, use mock responses for demonstration\n",
    "        \"\"\"\n",
    "        self.use_mock = use_mock\n",
    "        if not use_mock:\n",
    "            self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "        \n",
    "        # Prompt directly from the paper\n",
    "        self.prompt_template = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"You are evaluating Kotlin code quality.\"),\n",
    "            HumanMessage(content=\"\"\"Compare these two Kotlin code files and determine which has \n",
    "greater educational value for learning algorithms in Kotlin.\n",
    "\n",
    "File A:\n",
    "{file_a}\n",
    "\n",
    "File B:\n",
    "{file_b}\n",
    "\n",
    "Which file (A or B) has higher educational value? Respond with only 'A' or 'B'.\"\"\")\n",
    "        ])\n",
    "    \n",
    "    def _mock_compare(self, file_a: str, file_b: str) -> str:\n",
    "        \"\"\"Mock comparison based on code quality heuristics\"\"\"\n",
    "        # Simple heuristics for demonstration\n",
    "        score_a = (\n",
    "            len(file_a.split('\\n'))  # More lines\n",
    "            + file_a.count('fun ')    # More functions\n",
    "            + file_a.count('//')      # More comments\n",
    "            + file_a.count('class ')  # More classes\n",
    "        )\n",
    "        score_b = (\n",
    "            len(file_b.split('\\n'))\n",
    "            + file_b.count('fun ')\n",
    "            + file_b.count('//')\n",
    "            + file_b.count('class ')\n",
    "        )\n",
    "        # Add some randomness to simulate LLM uncertainty\n",
    "        score_a += random.gauss(0, 2)\n",
    "        score_b += random.gauss(0, 2)\n",
    "        \n",
    "        return 'A' if score_a > score_b else 'B'\n",
    "    \n",
    "    def compare_files(self, file_a: str, file_b: str) -> str:\n",
    "        \"\"\"Compare two files and return which is better ('A' or 'B')\"\"\"\n",
    "        if self.use_mock:\n",
    "            return self._mock_compare(file_a, file_b)\n",
    "        else:\n",
    "            messages = self.prompt_template.format_messages(file_a=file_a, file_b=file_b)\n",
    "            response = self.llm(messages)\n",
    "            return response.content.strip()\n",
    "    \n",
    "    def calculate_bidirectional_score(self, file_f: str, file_c: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate score using the paper's formula to eliminate ordering bias.\n",
    "        \n",
    "        Formula: s(f) = [(s(f,c)_A - s(f,c)_B) + (s(c,f)_B - s(c,f)_A)] / 2\n",
    "        \"\"\"\n",
    "        # First comparison: f as A, c as B\n",
    "        result1 = self.compare_files(file_f, file_c)\n",
    "        score_fc = 1.0 if result1 == 'A' else 0.0\n",
    "        \n",
    "        # Second comparison: c as A, f as B (reversed)\n",
    "        result2 = self.compare_files(file_c, file_f)\n",
    "        score_cf = 1.0 if result2 == 'B' else 0.0\n",
    "        \n",
    "        # Apply the formula\n",
    "        bidirectional_score = (score_fc + score_cf) / 2\n",
    "        \n",
    "        return bidirectional_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Three-Pass Scoring Algorithm\n",
    "\n",
    "The paper's key insight: instead of $O(n^2)$ comparisons, use three strategic passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreePassScorer:\n",
    "    \"\"\"\n",
    "    Implements the three-pass approximation algorithm from Section III.B.\n",
    "    \n",
    "    This reduces complexity from O(n²) to O(3n) while maintaining quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifier: PairwiseQualityClassifier):\n",
    "        self.classifier = classifier\n",
    "        self.pass_results = []\n",
    "    \n",
    "    def score_against_reference(self, files: List[CodeFile], reference: CodeFile) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Score all files against a reference file.\n",
    "        \n",
    "        This is one 'pass' in the three-pass algorithm.\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"Scoring against {reference.id}\"):\n",
    "            if file.id == reference.id:\n",
    "                scores[file.id] = 0.5  # Neutral score for self-comparison\n",
    "            else:\n",
    "                score = self.classifier.calculate_bidirectional_score(\n",
    "                    file.content, \n",
    "                    reference.content\n",
    "                )\n",
    "                scores[file.id] = score\n",
    "                file.comparisons += 1\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def three_pass_scoring(self, files: List[CodeFile], sample_size: int = None) -> List[CodeFile]:\n",
    "        \"\"\"\n",
    "        Perform the three-pass scoring algorithm from the paper.\n",
    "        \n",
    "        Args:\n",
    "            files: List of code files to score\n",
    "            sample_size: Size of initial sample (default: all files)\n",
    "        \n",
    "        Returns:\n",
    "            List of files with updated scores\n",
    "        \"\"\"\n",
    "        if sample_size is None:\n",
    "            sample_size = len(files)\n",
    "        \n",
    "        # Sample files for scoring\n",
    "        sample_files = random.sample(files, min(sample_size, len(files)))\n",
    "        \n",
    "        print(f\"Starting three-pass scoring on {len(sample_files)} files...\")\n",
    "        \n",
    "        # Pass 1: Random reference\n",
    "        print(\"\\nPass 1: Random reference\")\n",
    "        reference1 = random.choice(sample_files)\n",
    "        scores1 = self.score_against_reference(sample_files, reference1)\n",
    "        \n",
    "        # Find highest scored file from Pass 1\n",
    "        best_id1 = max(scores1, key=scores1.get)\n",
    "        reference2 = next(f for f in sample_files if f.id == best_id1)\n",
    "        \n",
    "        # Pass 2: Best from Pass 1 as reference\n",
    "        print(f\"\\nPass 2: Using {reference2.id} as reference\")\n",
    "        scores2 = self.score_against_reference(sample_files, reference2)\n",
    "        \n",
    "        # Find highest scored file from Pass 2\n",
    "        best_id2 = max(scores2, key=scores2.get)\n",
    "        reference3 = next(f for f in sample_files if f.id == best_id2)\n",
    "        \n",
    "        # Pass 3: Best from Pass 2 as reference\n",
    "        print(f\"\\nPass 3: Using {reference3.id} as reference\")\n",
    "        scores3 = self.score_against_reference(sample_files, reference3)\n",
    "        \n",
    "        # Calculate final scores (average of Pass 2 and Pass 3)\n",
    "        print(\"\\nCalculating final scores...\")\n",
    "        for file in sample_files:\n",
    "            # Paper: \"averaged the scores from second and third passes\"\n",
    "            file.score = (scores2[file.id] + scores3[file.id]) / 2\n",
    "        \n",
    "        # Store results for analysis\n",
    "        self.pass_results = [\n",
    "            {\"pass\": 1, \"scores\": scores1, \"reference\": reference1.id},\n",
    "            {\"pass\": 2, \"scores\": scores2, \"reference\": reference2.id},\n",
    "            {\"pass\": 3, \"scores\": scores3, \"reference\": reference3.id}\n",
    "        ]\n",
    "        \n",
    "        return sorted(sample_files, key=lambda x: x.score, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstration with Mock Kotlin Code\n",
    "\n",
    "Let's create a realistic demonstration using mock Kotlin code files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock Kotlin code files with varying quality\n",
    "mock_kotlin_files = [\n",
    "    CodeFile(\n",
    "        id=\"high_quality_1\",\n",
    "        content=\"\"\"// Binary Search implementation with detailed explanation\n",
    "class BinarySearch {\n",
    "    /**\n",
    "     * Performs binary search on a sorted array.\n",
    "     * Time complexity: O(log n)\n",
    "     * Space complexity: O(1)\n",
    "     */\n",
    "    fun search(arr: IntArray, target: Int): Int {\n",
    "        var left = 0\n",
    "        var right = arr.size - 1\n",
    "        \n",
    "        while (left <= right) {\n",
    "            val mid = left + (right - left) / 2 // Prevents overflow\n",
    "            \n",
    "            when {\n",
    "                arr[mid] == target -> return mid\n",
    "                arr[mid] < target -> left = mid + 1\n",
    "                else -> right = mid - 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return -1 // Element not found\n",
    "    }\n",
    "    \n",
    "    // Recursive implementation for educational comparison\n",
    "    fun searchRecursive(arr: IntArray, target: Int, left: Int = 0, right: Int = arr.size - 1): Int {\n",
    "        if (left > right) return -1\n",
    "        \n",
    "        val mid = left + (right - left) / 2\n",
    "        \n",
    "        return when {\n",
    "            arr[mid] == target -> mid\n",
    "            arr[mid] < target -> searchRecursive(arr, target, mid + 1, right)\n",
    "            else -> searchRecursive(arr, target, left, mid - 1)\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "    ),\n",
    "    \n",
    "    CodeFile(\n",
    "        id=\"high_quality_2\",\n",
    "        content=\"\"\"// Graph traversal algorithms with Kotlin idioms\n",
    "class Graph(private val vertices: Int) {\n",
    "    private val adjacencyList = Array(vertices) { mutableListOf<Int>() }\n",
    "    \n",
    "    fun addEdge(source: Int, destination: Int) {\n",
    "        adjacencyList[source].add(destination)\n",
    "        adjacencyList[destination].add(source) // For undirected graph\n",
    "    }\n",
    "    \n",
    "    /**\n",
    "     * Breadth-First Search using Kotlin collections\n",
    "     */\n",
    "    fun bfs(start: Int): List<Int> {\n",
    "        val visited = BooleanArray(vertices)\n",
    "        val queue = ArrayDeque<Int>()\n",
    "        val result = mutableListOf<Int>()\n",
    "        \n",
    "        queue.add(start)\n",
    "        visited[start] = true\n",
    "        \n",
    "        while (queue.isNotEmpty()) {\n",
    "            val current = queue.removeFirst()\n",
    "            result.add(current)\n",
    "            \n",
    "            adjacencyList[current].forEach { neighbor ->\n",
    "                if (!visited[neighbor]) {\n",
    "                    visited[neighbor] = true\n",
    "                    queue.add(neighbor)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    }\n",
    "    \n",
    "    /**\n",
    "     * Depth-First Search with tail recursion optimization\n",
    "     */\n",
    "    tailrec fun dfs(current: Int, visited: BooleanArray = BooleanArray(vertices), \n",
    "                    result: MutableList<Int> = mutableListOf()): List<Int> {\n",
    "        visited[current] = true\n",
    "        result.add(current)\n",
    "        \n",
    "        adjacencyList[current]\n",
    "            .filterNot { visited[it] }\n",
    "            .forEach { dfs(it, visited, result) }\n",
    "        \n",
    "        return result\n",
    "    }\n",
    "}\"\"\"\n",
    "    ),\n",
    "    \n",
    "    CodeFile(\n",
    "        id=\"medium_quality_1\",\n",
    "        content=\"\"\"// Simple sorting algorithm\n",
    "fun bubbleSort(arr: IntArray) {\n",
    "    val n = arr.size\n",
    "    for (i in 0 until n) {\n",
    "        for (j in 0 until n - i - 1) {\n",
    "            if (arr[j] > arr[j + 1]) {\n",
    "                // Swap elements\n",
    "                val temp = arr[j]\n",
    "                arr[j] = arr[j + 1]\n",
    "                arr[j + 1] = temp\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "    ),\n",
    "    \n",
    "    CodeFile(\n",
    "        id=\"low_quality_1\",\n",
    "        content=\"\"\"fun doSomething(x: Int): Int {\n",
    "    return x * 2\n",
    "}\"\"\"\n",
    "    ),\n",
    "    \n",
    "    CodeFile(\n",
    "        id=\"low_quality_2\",\n",
    "        content=\"\"\"val list = listOf(1, 2, 3, 4, 5)\n",
    "println(list)\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create classifier and scorer\n",
    "classifier = PairwiseQualityClassifier(use_mock=True)\n",
    "scorer = ThreePassScorer(classifier)\n",
    "\n",
    "# Run the three-pass algorithm\n",
    "scored_files = scorer.three_pass_scoring(mock_kotlin_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Results\n",
    "\n",
    "Let's visualize how the three-pass algorithm converges on quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final rankings\n",
    "print(\"\\nFinal Quality Rankings:\")\n",
    "print(\"=\" * 50)\n",
    "for i, file in enumerate(scored_files, 1):\n",
    "    print(f\"{i}. {file.id}: Score = {file.score:.3f} (Comparisons: {file.comparisons})\")\n",
    "    print(f\"   Preview: {file.content.split(chr(10))[0][:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score evolution across passes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, pass_result in enumerate(scorer.pass_results):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Extract scores for each file\n",
    "    file_ids = list(pass_result[\"scores\"].keys())\n",
    "    scores = list(pass_result[\"scores\"].values())\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(range(len(file_ids)), scores)\n",
    "    \n",
    "    # Color bars based on score\n",
    "    colors = ['green' if s > 0.6 else 'orange' if s > 0.4 else 'red' for s in scores]\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    ax.set_xlabel('Files')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(f'Pass {i+1} (Reference: {pass_result[\"reference\"][:15]}...)')\n",
    "    ax.set_xticks(range(len(file_ids)))\n",
    "    ax.set_xticklabels([fid[:10] for fid in file_ids], rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(y=0.5, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Insights and Learnings\n",
    "\n",
    "### 6.1 Why This Algorithm Works\n",
    "\n",
    "1. **Bidirectional Comparison**: Eliminates ordering bias by testing both A vs B and B vs A\n",
    "2. **Progressive Refinement**: Each pass uses a better reference, converging on true quality\n",
    "3. **Efficiency**: O(3n) instead of O(n²) comparisons\n",
    "\n",
    "### 6.2 Critical Implementation Details from the Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the importance of bidirectional scoring\n",
    "print(\"Demonstrating Bidirectional Scoring:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with two files\n",
    "file1 = mock_kotlin_files[0]  # high_quality_1\n",
    "file2 = mock_kotlin_files[2]  # medium_quality_1\n",
    "\n",
    "# One-directional scores\n",
    "result_12 = classifier.compare_files(file1.content, file2.content)\n",
    "result_21 = classifier.compare_files(file2.content, file1.content)\n",
    "\n",
    "print(f\"File1 vs File2 (File1 as A): {result_12}\")\n",
    "print(f\"File2 vs File1 (File2 as A): {result_21}\")\n",
    "\n",
    "# Bidirectional score\n",
    "bidirectional = classifier.calculate_bidirectional_score(file1.content, file2.content)\n",
    "print(f\"\\nBidirectional score for File1: {bidirectional:.3f}\")\n",
    "print(\"\\nThis accounts for potential ordering bias in the LLM's responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Practical Considerations\n",
    "\n",
    "From Section III.B and our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate computational savings\n",
    "n_files = 128000  # From the paper\n",
    "n_sample = 128000  # They scored all 128K files\n",
    "\n",
    "# Full pairwise comparisons\n",
    "full_comparisons = n_files * (n_files - 1) // 2\n",
    "\n",
    "# Three-pass comparisons\n",
    "three_pass_comparisons = 3 * n_sample\n",
    "\n",
    "# Savings\n",
    "savings_ratio = three_pass_comparisons / full_comparisons\n",
    "\n",
    "print(f\"Dataset size: {n_files:,} files\")\n",
    "print(f\"\\nFull pairwise comparisons: {full_comparisons:,}\")\n",
    "print(f\"Three-pass comparisons: {three_pass_comparisons:,}\")\n",
    "print(f\"\\nComputational savings: {(1 - savings_ratio) * 100:.2f}%\")\n",
    "print(f\"\\nThis makes the approach feasible even for large datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extension: Binary Classifier Training\n",
    "\n",
    "The paper mentions training a binary classifier on the scored data. Let's simulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare data for binary classification\n",
    "# Top 5% as positive (high quality), rest as negative\n",
    "threshold_percentile = 95\n",
    "threshold_score = np.percentile([f.score for f in scored_files], threshold_percentile)\n",
    "\n",
    "# Create labels\n",
    "X_texts = [f.content for f in scored_files]\n",
    "y_labels = [1 if f.score >= threshold_score else 0 for f in scored_files]\n",
    "\n",
    "print(f\"Threshold score (top 5%): {threshold_score:.3f}\")\n",
    "print(f\"High quality samples: {sum(y_labels)}\")\n",
    "print(f\"Low quality samples: {len(y_labels) - sum(y_labels)}\")\n",
    "\n",
    "# Feature extraction (simplified - paper uses CodeT5+)\n",
    "vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "X_features = vectorizer.fit_transform(X_texts)\n",
    "\n",
    "# Train binary classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"\\nClassifier Performance:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Quality', 'High Quality']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Replicating Figure 3: Mistral vs GPT-3.5 Classifier\n",
    "\n",
    "The paper shows that Mistral-based classification outperforms GPT-3.5. Let's understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training curves from Figure 3\n",
    "optimization_steps = np.linspace(0, 1400, 50)\n",
    "\n",
    "# Paper insight: \"noise in the log-probabilities of the completion distribution in the OpenAI API\"\n",
    "# Mistral has more stable log-probabilities\n",
    "\n",
    "# Simulate the curves\n",
    "base_pass_rate = 26  # Starting pass rate\n",
    "\n",
    "# GPT-3.5: Higher noise, lower final improvement\n",
    "gpt35_noise = np.random.normal(0, 1.5, 50)  # Higher noise\n",
    "gpt35_curve = base_pass_rate + 10 * (1 - np.exp(-optimization_steps / 400)) + gpt35_noise\n",
    "\n",
    "# Mistral: Lower noise, better final improvement\n",
    "mistral_noise = np.random.normal(0, 0.5, 50)  # Lower noise\n",
    "mistral_curve = base_pass_rate + 14 * (1 - np.exp(-optimization_steps / 300)) + mistral_noise\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(optimization_steps, gpt35_curve, 'b-', label='OpenAI GPT-3.5-based classifier', alpha=0.7)\n",
    "plt.plot(optimization_steps, mistral_curve, 'orange', label='Mistral-based classifier', alpha=0.7)\n",
    "\n",
    "# Add smoothed trends\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "plt.plot(optimization_steps, gaussian_filter1d(gpt35_curve, sigma=5), 'b--', linewidth=2)\n",
    "plt.plot(optimization_steps, gaussian_filter1d(mistral_curve, sigma=5), 'orange', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.xlabel('Optimization step')\n",
    "plt.ylabel('Pass rate')\n",
    "plt.title('Figure 3: Pass rate on HumanEval for Different Filtration Strategies')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(25, 42)\n",
    "\n",
    "# Add annotation\n",
    "plt.annotate('Mistral: More stable\\nlog-probabilities', \n",
    "             xy=(1000, 38), xytext=(700, 35),\n",
    "             arrowprops=dict(arrowstyle='->', color='orange', alpha=0.5))\n",
    "plt.annotate('GPT-3.5: Noisy\\nlog-probabilities', \n",
    "             xy=(1000, 34), xytext=(700, 30),\n",
    "             arrowprops=dict(arrowstyle='->', color='blue', alpha=0.5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight from the paper:\")\n",
    "print(\"GPT-3.5 API adds artificial noise to log-probabilities as defense against distillation.\")\n",
    "print(\"This noise reduces the effectiveness of the pairwise comparison method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Practical Applications\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Quality > Quantity**: 25K carefully selected files outperform 4M random files\n",
    "2. **Clever Algorithms**: Three-pass approximation makes O(n²) problem tractable\n",
    "3. **Bidirectional Scoring**: Essential for eliminating LLM ordering bias\n",
    "4. **Model Choice Matters**: Stable log-probabilities (Mistral) beat noisy ones (GPT-3.5)\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "This technique can be applied to:\n",
    "- Curating training datasets for any programming language\n",
    "- Quality assessment of code repositories\n",
    "- Educational content filtering\n",
    "- Code review prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"Dataset Quality Filtering Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original dataset: 4,000,000 files\")\n",
    "print(f\"After filtering: 25,000 files (0.625% retained)\")\n",
    "print(f\"Pass rate improvement: +11.8% (26.09% → 37.89%)\")\n",
    "print(f\"\\nThis 160x reduction in data size led to significant quality improvements!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}