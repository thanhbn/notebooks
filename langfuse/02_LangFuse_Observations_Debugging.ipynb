{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 02. LangFuse Observations & Debugging\n",
    "\n",
    "## üìã M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "\n",
    "Notebook n√†y s·∫Ω h∆∞·ªõng d·∫´n b·∫°n:\n",
    "- Hi·ªÉu c√°ch s·ª≠ d·ª•ng LangFuse ƒë·ªÉ quan s√°t v√† debug ·ª©ng d·ª•ng LLM\n",
    "- Ph√¢n t√≠ch c√°c Span, Event, v√† Observation trong LangFuse UI\n",
    "- Theo d√µi lu·ªìng ho·∫°t ƒë·ªông c·ªßa LLM chains v√† RAG pipelines\n",
    "- X√°c ƒë·ªãnh v√† kh·∫Øc ph·ª•c c√°c l·ªói ti·ªÅm ·∫©n trong ·ª©ng d·ª•ng LLM\n",
    "- ƒêo l∆∞·ªùng hi·ªáu su·∫•t (latency, token usage) v√† t·ªëi ∆∞u h√≥a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## üîç Gi·ªõi thi·ªáu v·ªÅ LangFuse Debugging\n",
    "\n",
    "LangFuse cung c·∫•p kh·∫£ nƒÉng quan s√°t chi ti·∫øt cho ·ª©ng d·ª•ng LLM th√¥ng qua:\n",
    "\n",
    "### üìä C√°c th√†nh ph·∫ßn quan s√°t ch√≠nh:\n",
    "- **Traces**: To√†n b·ªô lu·ªìng th·ª±c thi c·ªßa m·ªôt request\n",
    "- **Spans**: C√°c b∆∞·ªõc con trong m·ªôt trace (LLM calls, retrieval, processing)\n",
    "- **Events**: C√°c s·ª± ki·ªán ƒë∆°n l·∫ª (logging, errors, milestones)\n",
    "- **Observations**: T·ªïng h·ª£p t·∫•t c·∫£ ho·∫°t ƒë·ªông c√≥ th·ªÉ quan s√°t ƒë∆∞·ª£c\n",
    "\n",
    "### üéØ L·ª£i √≠ch c·ªßa debugging v·ªõi LangFuse:\n",
    "- **Visibility**: Nh√¨n th·∫•y to√†n b·ªô lu·ªìng ho·∫°t ƒë·ªông c·ªßa LLM\n",
    "- **Performance**: ƒêo l∆∞·ªùng latency v√† token usage chi ti·∫øt\n",
    "- **Error Detection**: Ph√°t hi·ªán l·ªói v√† bottlenecks\n",
    "- **Quality Assurance**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng output\n",
    "- **Cost Optimization**: Theo d√µi chi ph√≠ s·ª≠ d·ª•ng LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è C√†i ƒë·∫∑t & C·∫•u h√¨nh\n",
    "\n",
    "Nh·∫Øc l·∫°i c·∫•u h√¨nh LangFuse t·ª´ notebook tr∆∞·ªõc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "# C·∫•u h√¨nh LangFuse\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-your-public-key\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\", \"sk-lf-your-secret-key\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    ")\n",
    "\n",
    "# T·∫°o callback handler\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-your-public-key\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\", \"sk-lf-your-secret-key\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LangFuse configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## üì¶ Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ü§ñ C·∫•u h√¨nh LLM v·ªõi LangFuse tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o ChatAnthropic v·ªõi LangFuse callback\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    callbacks=[langfuse_handler]\n",
    ")\n",
    "\n",
    "print(\"ü§ñ LLM initialized with LangFuse tracing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## üö® V√ç D·ª§ 1: Chain c√≥ l·ªói ti·ªÅm ·∫©n\n",
    "\n",
    "Ch√∫ng ta s·∫Ω t·∫°o m·ªôt chain c√≥ th·ªÉ g·∫∑p l·ªói ƒë·ªÉ th·∫•y c√°ch LangFuse gi√∫p debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"problematic_chain\")\n",
    "def problematic_math_chain(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Chain gi·∫£i to√°n c√≥ th·ªÉ g·∫∑p l·ªói v·ªõi m·ªôt s·ªë input nh·∫•t ƒë·ªãnh\n",
    "    \"\"\"\n",
    "    # T·∫°o span cho preprocessing\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"math_preprocessing\",\n",
    "        input={\"raw_question\": question}\n",
    "    )\n",
    "    \n",
    "    # Preprocessing - c√≥ th·ªÉ g√¢y l·ªói n·∫øu input kh√¥ng ph√π h·ª£p\n",
    "    if len(question.strip()) < 5:\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\",\n",
    "            status_message=\"Input qu√° ng·∫Øn ƒë·ªÉ x·ª≠ l√Ω\"\n",
    "        )\n",
    "        raise ValueError(\"Question too short for processing\")\n",
    "    \n",
    "    # T·∫°o prompt cho to√°n h·ªçc\n",
    "    math_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"B·∫°n l√† m·ªôt tr·ª£ l√Ω to√°n h·ªçc th√¥ng minh. H√£y gi·∫£i b√†i to√°n m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c.\"),\n",
    "        (\"human\", \"C√¢u h·ªèi: {question}\\n\\nH√£y gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc m·ªôt c√°ch r√µ r√†ng.\")\n",
    "    ])\n",
    "    \n",
    "    # T·∫°o chain\n",
    "    chain = math_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Th·ª±c thi v·ªõi error handling\n",
    "    try:\n",
    "        result = chain.invoke(\n",
    "            {\"question\": question},\n",
    "            config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "        \n",
    "        # Log th√†nh c√¥ng\n",
    "        langfuse_context.update_current_observation(\n",
    "            output={\"answer\": result, \"status\": \"success\"}\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log l·ªói\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\",\n",
    "            status_message=f\"Chain execution failed: {str(e)}\",\n",
    "            output={\"error\": str(e)}\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "print(\"üîß Problematic chain function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### üß™ Test chain v·ªõi c√°c input kh√°c nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_cases = [\n",
    "    \"T√≠nh 15 + 27 = ?\",  # C√¢u h·ªèi b√¨nh th∆∞·ªùng\n",
    "    \"2+2\",  # C√¢u h·ªèi ng·∫Øn - s·∫Ω g√¢y l·ªói\n",
    "    \"Gi·∫£i ph∆∞∆°ng tr√¨nh x¬≤ - 5x + 6 = 0\",  # C√¢u h·ªèi ph·ª©c t·∫°p\n",
    "    \"Hi\"  # Input qu√° ng·∫Øn - s·∫Ω g√¢y l·ªói\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing problematic chain with different inputs...\\n\")\n",
    "\n",
    "for i, question in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        result = problematic_math_chain(question)\n",
    "        print(f\"‚úÖ Success: {result[:100]}...\" if len(result) > 100 else f\"‚úÖ Success: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    time.sleep(1)  # Delay ƒë·ªÉ d·ªÖ theo d√µi tr√™n LangFuse\n",
    "\n",
    "print(\"üîç Check LangFuse UI ƒë·ªÉ xem traces v√† errors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## üîç V√ç D·ª§ 2: RAG Pipeline v·ªõi detailed tracing\n",
    "\n",
    "T·∫°o m·ªôt RAG pipeline ƒë∆°n gi·∫£n v·ªõi tracing chi ti·∫øt cho vi·ªác retrieval v√† generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o sample documents\n",
    "sample_documents = [\n",
    "    \"Python l√† m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh c·∫•p cao, d·ªÖ h·ªçc v√† m·∫°nh m·∫Ω. Python ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong khoa h·ªçc d·ªØ li·ªáu, ph√°t tri·ªÉn web, v√† tr√≠ tu·ªá nh√¢n t·∫°o.\",\n",
    "    \"Machine Learning l√† m·ªôt nh√°nh c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o cho ph√©p m√°y t√≠nh h·ªçc t·ª´ d·ªØ li·ªáu m√† kh√¥ng c·∫ßn ƒë∆∞·ª£c l·∫≠p tr√¨nh c·ª• th·ªÉ cho t·ª´ng t√°c v·ª•.\",\n",
    "    \"LangChain l√† m·ªôt framework m·∫°nh m·∫Ω ƒë·ªÉ x√¢y d·ª±ng ·ª©ng d·ª•ng v·ªõi c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM). N√≥ cung c·∫•p c√°c c√¥ng c·ª• ƒë·ªÉ t·∫°o chains, agents v√† c√°c th√†nh ph·∫ßn kh√°c.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) l√† m·ªôt k·ªπ thu·∫≠t k·∫øt h·ª£p vi·ªác truy xu·∫•t th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu v·ªõi kh·∫£ nƒÉng sinh vƒÉn b·∫£n c·ªßa LLM.\",\n",
    "    \"Vector databases nh∆∞ FAISS, Pinecone ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ l∆∞u tr·ªØ v√† t√¨m ki·∫øm embeddings m·ªôt c√°ch hi·ªáu qu·∫£ trong c√°c h·ªá th·ªëng RAG.\"\n",
    "]\n",
    "\n",
    "print(\"üìÑ Sample documents created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"setup_rag_pipeline\")\n",
    "def setup_rag_pipeline(documents: List[str]):\n",
    "    \"\"\"\n",
    "    Thi·∫øt l·∫≠p RAG pipeline v·ªõi detailed tracing\n",
    "    \"\"\"\n",
    "    # T·∫°o span cho document processing\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"document_processing\",\n",
    "        input={\"num_documents\": len(documents)}\n",
    "    )\n",
    "    \n",
    "    # Text splitting\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    \n",
    "    splits = text_splitter.create_documents(documents)\n",
    "    \n",
    "    # T·∫°o embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    )\n",
    "    \n",
    "    # T·∫°o vector store\n",
    "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "    \n",
    "    # Log th√¥ng tin v·ªÅ vector store\n",
    "    langfuse_context.update_current_observation(\n",
    "        output={\n",
    "            \"num_chunks\": len(splits),\n",
    "            \"embedding_model\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"vectorstore_type\": \"FAISS\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Setup RAG pipeline\n",
    "vectorstore = setup_rag_pipeline(sample_documents)\n",
    "print(\"üîß RAG pipeline setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"rag_query\")\n",
    "def rag_query(question: str, vectorstore, k: int = 3):\n",
    "    \"\"\"\n",
    "    Th·ª±c hi·ªán RAG query v·ªõi detailed tracing\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # T·∫°o span cho retrieval\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"retrieval_phase\",\n",
    "        input={\"question\": question, \"k\": k}\n",
    "    )\n",
    "    \n",
    "    # Retrieval\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Log retrieval results\n",
    "    retrieval_time = time.time() - start_time\n",
    "    retrieval_results = [\n",
    "        {\"content\": doc.page_content[:100] + \"...\", \"metadata\": doc.metadata}\n",
    "        for doc in retrieved_docs\n",
    "    ]\n",
    "    \n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"retrieval_results\",\n",
    "        output={\n",
    "            \"num_retrieved\": len(retrieved_docs),\n",
    "            \"retrieval_time_ms\": round(retrieval_time * 1000, 2),\n",
    "            \"documents\": retrieval_results\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # T·∫°o context t·ª´ retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # T·∫°o span cho generation\n",
    "    generation_start = time.time()\n",
    "    \n",
    "    # RAG prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.\"),\n",
    "        (\"human\", \"\"\"Th√¥ng tin tham kh·∫£o:\n",
    "{context}\n",
    "\n",
    "C√¢u h·ªèi: {question}\n",
    "\n",
    "H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin tham kh·∫£o tr√™n. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # T·∫°o chain\n",
    "    rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Generation\n",
    "    response = rag_chain.invoke(\n",
    "        {\"context\": context, \"question\": question},\n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "    \n",
    "    # Log generation results\n",
    "    generation_time = time.time() - generation_start\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"generation_results\",\n",
    "        output={\n",
    "            \"response\": response,\n",
    "            \"generation_time_ms\": round(generation_time * 1000, 2),\n",
    "            \"total_time_ms\": round(total_time * 1000, 2),\n",
    "            \"context_length\": len(context)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"context\": context,\n",
    "        \"metrics\": {\n",
    "            \"retrieval_time_ms\": round(retrieval_time * 1000, 2),\n",
    "            \"generation_time_ms\": round(generation_time * 1000, 2),\n",
    "            \"total_time_ms\": round(total_time * 1000, 2)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"üîç RAG query function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### üß™ Test RAG pipeline v·ªõi c√°c c√¢u h·ªèi kh√°c nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Python ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ l√†m g√¨?\",\n",
    "    \"RAG l√† g√¨ v√† ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?\",\n",
    "    \"Vector database c√≥ vai tr√≤ g√¨ trong RAG?\",\n",
    "    \"Blockchain l√† g√¨?\"  # C√¢u h·ªèi ngo√†i ph·∫°m vi t√†i li·ªáu\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing RAG pipeline with different queries...\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        result = rag_query(query, vectorstore)\n",
    "        \n",
    "        print(f\"üìù Answer: {result['answer'][:200]}...\" if len(result['answer']) > 200 else f\"üìù Answer: {result['answer']}\")\n",
    "        print(f\"üìä Metrics: {result['metrics']}\")\n",
    "        print(f\"üìö Retrieved {len(result['retrieved_docs'])} documents\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    time.sleep(2)  # Delay ƒë·ªÉ d·ªÖ theo d√µi tr√™n LangFuse\n",
    "\n",
    "print(\"üîç Check LangFuse UI ƒë·ªÉ xem RAG traces chi ti·∫øt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## üìä T·∫°o trace v·ªõi custom metadata v√† tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"advanced_llm_call\")\n",
    "def advanced_llm_call_with_metadata(prompt: str, user_id: str = \"demo_user\"):\n",
    "    \"\"\"\n",
    "    LLM call v·ªõi metadata v√† tags chi ti·∫øt\n",
    "    \"\"\"\n",
    "    # C·∫≠p nh·∫≠t observation v·ªõi metadata\n",
    "    langfuse_context.update_current_observation(\n",
    "        user_id=user_id,\n",
    "        session_id=f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        metadata={\n",
    "            \"prompt_length\": len(prompt),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_version\": \"claude-3-sonnet-20240229\",\n",
    "            \"temperature\": 0.7\n",
    "        },\n",
    "        tags=[\"advanced_call\", \"with_metadata\", \"demo\"]\n",
    "    )\n",
    "    \n",
    "    # T·∫°o messages\n",
    "    messages = [\n",
    "        SystemMessage(content=\"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch v√† th√¥ng minh.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    # Call LLM\n",
    "    response = llm.invoke(\n",
    "        messages,\n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "    \n",
    "    # Log response metadata\n",
    "    langfuse_context.update_current_observation(\n",
    "        output={\n",
    "            \"response\": response.content,\n",
    "            \"response_length\": len(response.content),\n",
    "            \"token_usage\": getattr(response, 'usage_metadata', {})\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Test v·ªõi metadata\n",
    "response = advanced_llm_call_with_metadata(\n",
    "    \"Gi·∫£i th√≠ch kh√°i ni·ªám Machine Learning m·ªôt c√°ch ƒë∆°n gi·∫£n\",\n",
    "    user_id=\"user_123\"\n",
    ")\n",
    "\n",
    "print(\"üìù Response with metadata:\")\n",
    "print(response[:300] + \"...\" if len(response) > 300 else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## üéØ T·∫°o scores v√† evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"evaluated_llm_call\")\n",
    "def evaluated_llm_call(question: str, expected_topics: List[str]):\n",
    "    \"\"\"\n",
    "    LLM call v·ªõi evaluation scores\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM\n",
    "    response = llm.invoke(\n",
    "        [HumanMessage(content=question)],\n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    response_text = response.content\n",
    "    \n",
    "    # Simple evaluation scores\n",
    "    relevance_score = sum(1 for topic in expected_topics if topic.lower() in response_text.lower()) / len(expected_topics)\n",
    "    length_score = min(len(response_text) / 500, 1.0)  # ƒêi·ªÉm d·ª±a tr√™n ƒë·ªô d√†i\n",
    "    speed_score = max(0, 1 - (response_time / 10))  # ƒêi·ªÉm d·ª±a tr√™n t·ªëc ƒë·ªô\n",
    "    \n",
    "    # T·∫°o scores trong LangFuse\n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"relevance\",\n",
    "        value=relevance_score,\n",
    "        comment=f\"Matches {int(relevance_score * len(expected_topics))}/{len(expected_topics)} expected topics\"\n",
    "    )\n",
    "    \n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"response_length\",\n",
    "        value=length_score,\n",
    "        comment=f\"Response length: {len(response_text)} characters\"\n",
    "    )\n",
    "    \n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"response_speed\",\n",
    "        value=speed_score,\n",
    "        comment=f\"Response time: {response_time:.2f} seconds\"\n",
    "    )\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = (relevance_score + length_score + speed_score) / 3\n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"overall_quality\",\n",
    "        value=overall_score,\n",
    "        comment=\"Average of relevance, length, and speed scores\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"scores\": {\n",
    "            \"relevance\": relevance_score,\n",
    "            \"length\": length_score,\n",
    "            \"speed\": speed_score,\n",
    "            \"overall\": overall_score\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"response_time\": response_time,\n",
    "            \"response_length\": len(response_text)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test v·ªõi evaluation\n",
    "result = evaluated_llm_call(\n",
    "    \"Gi·∫£i th√≠ch v·ªÅ Deep Learning v√† ·ª©ng d·ª•ng c·ªßa n√≥\",\n",
    "    expected_topics=[\"neural networks\", \"training\", \"applications\", \"data\"]\n",
    ")\n",
    "\n",
    "print(\"üìä Evaluation Results:\")\n",
    "print(f\"Scores: {result['scores']}\")\n",
    "print(f\"Metrics: {result['metrics']}\")\n",
    "print(f\"Response: {result['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## üîç Ph√¢n t√≠ch v√† Debug tr√™n LangFuse UI\n",
    "\n",
    "### üì± C√°ch s·ª≠ d·ª•ng LangFuse UI ƒë·ªÉ ph√¢n t√≠ch:\n",
    "\n",
    "1. **M·ªü LangFuse Dashboard**: Truy c·∫≠p `http://localhost:3000`\n",
    "\n",
    "2. **Xem Traces**:\n",
    "   - V√†o tab \"Traces\" ƒë·ªÉ xem t·∫•t c·∫£ execution traces\n",
    "   - Click v√†o trace ƒë·ªÉ xem chi ti·∫øt\n",
    "   - Quan s√°t timeline v√† nested spans\n",
    "\n",
    "3. **Ph√¢n t√≠ch Observations**:\n",
    "   - **Input/Output**: Xem d·ªØ li·ªáu ƒë·∫ßu v√†o v√† ƒë·∫ßu ra\n",
    "   - **Latency**: ƒêo th·ªùi gian th·ª±c thi t·ª´ng b∆∞·ªõc\n",
    "   - **Token Usage**: Theo d√µi s·ªë token ƒë∆∞·ª£c s·ª≠ d·ª•ng\n",
    "   - **Errors**: X√°c ƒë·ªãnh l·ªói v√† nguy√™n nh√¢n\n",
    "\n",
    "4. **Metrics Dashboard**:\n",
    "   - Xem t·ªïng quan v·ªÅ performance\n",
    "   - Theo d√µi cost v√† usage\n",
    "   - Ph√¢n t√≠ch trends theo th·ªùi gian\n",
    "\n",
    "5. **Scores v√† Evaluations**:\n",
    "   - Xem scores cho t·ª´ng trace\n",
    "   - So s√°nh quality metrics\n",
    "   - T·∫°o evaluations t·ª± ƒë·ªông\n",
    "\n",
    "### üéØ C√°c ƒëi·ªÉm c·∫ßn ch√∫ √Ω khi debug:\n",
    "\n",
    "- **Latency Bottlenecks**: Spans n√†o m·∫•t th·ªùi gian nh·∫•t?\n",
    "- **Error Patterns**: L·ªói xu·∫•t hi·ªán ·ªü ƒë√¢u v√† t·∫ßn su·∫•t?\n",
    "- **Token Usage**: Chi ph√≠ c√≥ t·ªëi ∆∞u kh√¥ng?\n",
    "- **Quality Scores**: Output c√≥ ƒë·∫°t ch·∫•t l∆∞·ª£ng mong mu·ªën?\n",
    "- **User Experience**: Response time c√≥ ch·∫•p nh·∫≠n ƒë∆∞·ª£c kh√¥ng?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## üìà T·∫°o comprehensive trace cho production debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"production_rag_pipeline\")\n",
    "def production_rag_pipeline(query: str, user_id: str, session_id: str):\n",
    "    \"\"\"\n",
    "    Production-ready RAG pipeline v·ªõi comprehensive tracing\n",
    "    \"\"\"\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    # Set session v√† user context\n",
    "    langfuse_context.update_current_observation(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        metadata={\n",
    "            \"pipeline_version\": \"v1.0.0\",\n",
    "            \"environment\": \"development\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        tags=[\"production\", \"rag\", \"comprehensive\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Query preprocessing\n",
    "        with langfuse_context.observe(name=\"query_preprocessing\") as preprocessing_span:\n",
    "            preprocessed_query = query.strip().lower()\n",
    "            preprocessing_span.update(\n",
    "                input={\"raw_query\": query},\n",
    "                output={\"preprocessed_query\": preprocessed_query},\n",
    "                metadata={\"preprocessing_time\": time.time() - pipeline_start}\n",
    "            )\n",
    "        \n",
    "        # Step 2: Retrieval\n",
    "        retrieval_start = time.time()\n",
    "        with langfuse_context.observe(name=\"document_retrieval\") as retrieval_span:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "            retrieved_docs = retriever.get_relevant_documents(preprocessed_query)\n",
    "            \n",
    "            retrieval_time = time.time() - retrieval_start\n",
    "            retrieval_span.update(\n",
    "                input={\"query\": preprocessed_query, \"k\": 3},\n",
    "                output={\n",
    "                    \"num_documents\": len(retrieved_docs),\n",
    "                    \"documents\": [doc.page_content[:100] for doc in retrieved_docs]\n",
    "                },\n",
    "                metadata={\"retrieval_time_ms\": round(retrieval_time * 1000, 2)}\n",
    "            )\n",
    "            \n",
    "            # Score retrieval quality\n",
    "            retrieval_span.score(\n",
    "                name=\"retrieval_quality\",\n",
    "                value=min(len(retrieved_docs) / 3, 1.0),\n",
    "                comment=f\"Retrieved {len(retrieved_docs)} out of 3 requested documents\"\n",
    "            )\n",
    "        \n",
    "        # Step 3: Context preparation\n",
    "        with langfuse_context.observe(name=\"context_preparation\") as context_span:\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "            context_span.update(\n",
    "                input={\"documents\": [doc.page_content for doc in retrieved_docs]},\n",
    "                output={\"context_length\": len(context)},\n",
    "                metadata={\"num_documents_used\": len(retrieved_docs)}\n",
    "            )\n",
    "        \n",
    "        # Step 4: Generation\n",
    "        generation_start = time.time()\n",
    "        with langfuse_context.observe(name=\"answer_generation\") as generation_span:\n",
    "            rag_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.\"),\n",
    "                (\"human\", \"Th√¥ng tin: {context}\\n\\nC√¢u h·ªèi: {question}\\n\\nTr·∫£ l·ªùi:\")\n",
    "            ])\n",
    "            \n",
    "            chain = rag_prompt | llm | StrOutputParser()\n",
    "            answer = chain.invoke(\n",
    "                {\"context\": context, \"question\": query},\n",
    "                config={\"callbacks\": [langfuse_handler]}\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - generation_start\n",
    "            generation_span.update(\n",
    "                input={\"context_length\": len(context), \"question\": query},\n",
    "                output={\"answer\": answer, \"answer_length\": len(answer)},\n",
    "                metadata={\"generation_time_ms\": round(generation_time * 1000, 2)}\n",
    "            )\n",
    "            \n",
    "            # Score generation quality\n",
    "            generation_span.score(\n",
    "                name=\"answer_completeness\",\n",
    "                value=min(len(answer) / 200, 1.0),\n",
    "                comment=f\"Answer length: {len(answer)} characters\"\n",
    "            )\n",
    "        \n",
    "        # Step 5: Post-processing & final results\n",
    "        total_time = time.time() - pipeline_start\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_docs\": len(retrieved_docs),\n",
    "            \"context_length\": len(context),\n",
    "            \"total_time_ms\": round(total_time * 1000, 2),\n",
    "            \"retrieval_time_ms\": round(retrieval_time * 1000, 2),\n",
    "            \"generation_time_ms\": round(generation_time * 1000, 2)\n",
    "        }\n",
    "        \n",
    "        # Overall pipeline score\n",
    "        langfuse_context.score_current_observation(\n",
    "            name=\"pipeline_performance\",\n",
    "            value=max(0, 1 - (total_time / 30)),  # Penalty for slow responses\n",
    "            comment=f\"Total pipeline time: {total_time:.2f}s\"\n",
    "        )\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            output=result,\n",
    "            status_message=\"Pipeline completed successfully\"\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - pipeline_start\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\",\n",
    "            status_message=f\"Pipeline failed after {error_time:.2f}s: {str(e)}\",\n",
    "            output={\"error\": str(e), \"error_type\": type(e).__name__}\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "# Test production pipeline\n",
    "production_result = production_rag_pipeline(\n",
    "    query=\"Gi·∫£i th√≠ch v·ªÅ LangChain v√† c√°c ·ª©ng d·ª•ng c·ªßa n√≥\",\n",
    "    user_id=\"prod_user_001\",\n",
    "    session_id=\"session_prod_20241224\"\n",
    ")\n",
    "\n",
    "print(\"üöÄ Production Pipeline Results:\")\n",
    "print(json.dumps(production_result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## üîß Utility functions cho debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_url(trace_id: str) -> str:\n",
    "    \"\"\"\n",
    "    T·∫°o URL ƒë·ªÉ xem trace tr√™n LangFuse UI\n",
    "    \"\"\"\n",
    "    base_url = os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    "    return f\"{base_url}/trace/{trace_id}\"\n",
    "\n",
    "def flush_langfuse():\n",
    "    \"\"\"\n",
    "    Flush t·∫•t c·∫£ pending traces to LangFuse\n",
    "    \"\"\"\n",
    "    langfuse.flush()\n",
    "    print(\"‚úÖ All traces flushed to LangFuse!\")\n",
    "\n",
    "def create_debug_session(session_name: str):\n",
    "    \"\"\"\n",
    "    T·∫°o debug session v·ªõi metadata\n",
    "    \"\"\"\n",
    "    return langfuse.trace(\n",
    "        name=f\"debug_session_{session_name}\",\n",
    "        metadata={\n",
    "            \"session_type\": \"debug\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"notebook\": \"02_LangFuse_Observations_Debugging\"\n",
    "        },\n",
    "        tags=[\"debug\", \"notebook\", session_name]\n",
    "    )\n",
    "\n",
    "# Flush t·∫•t c·∫£ traces\n",
    "flush_langfuse()\n",
    "\n",
    "print(\"üîß Debug utilities ready!\")\n",
    "print(\"üåê LangFuse UI: http://localhost:3000\")\n",
    "print(\"üìä Check your traces and observations in the UI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## üéØ K·∫øt lu·∫≠n & B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "### üìö N·ªôi dung ƒë√£ h·ªçc:\n",
    "\n",
    "1. **Quan s√°t chi ti·∫øt**: S·ª≠ d·ª•ng LangFuse ƒë·ªÉ theo d√µi t·ª´ng b∆∞·ªõc trong LLM pipeline\n",
    "2. **Error Detection**: Ph√°t hi·ªán v√† debug l·ªói v·ªõi detailed tracing\n",
    "3. **Performance Monitoring**: ƒêo l∆∞·ªùng latency, token usage, v√† quality metrics\n",
    "4. **RAG Debugging**: Trace chi ti·∫øt cho retrieval v√† generation phases\n",
    "5. **Production Ready**: T·∫°o comprehensive traces cho production systems\n",
    "6. **Evaluation Scores**: T√≠ch h·ª£p scoring v√† evaluation v√†o traces\n",
    "\n",
    "### üîç Key Benefits c·ªßa LangFuse Debugging:\n",
    "\n",
    "- **üéØ Visibility**: Nh√¨n th·∫•y to√†n b·ªô lu·ªìng ho·∫°t ƒë·ªông\n",
    "- **‚ö° Performance**: T·ªëi ∆∞u h√≥a speed v√† cost\n",
    "- **üêõ Debug**: Nhanh ch√≥ng x√°c ƒë·ªãnh v√† s·ª≠a l·ªói\n",
    "- **üìä Analytics**: Ph√¢n t√≠ch patterns v√† trends\n",
    "- **üîÑ Iteration**: C·∫£i thi·ªán li√™n t·ª•c d·ª±a tr√™n data\n",
    "\n",
    "### üöÄ B∆∞·ªõc ti·∫øp theo:\n",
    "\n",
    "1. **Notebook 03**: LangFuse Evaluations & Prompts - ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng v√† qu·∫£n l√Ω prompts\n",
    "2. **Advanced Tracing**: T√≠ch h·ª£p v·ªõi production systems\n",
    "3. **Custom Evaluations**: T·∫°o evaluation metrics ph√π h·ª£p v·ªõi use case\n",
    "4. **Monitoring Dashboards**: Setup alerting v√† monitoring\n",
    "5. **Team Collaboration**: Chia s·∫ª insights v√† debugging v·ªõi team\n",
    "\n",
    "### üí° Best Practices:\n",
    "\n",
    "- Lu√¥n flush traces khi k·∫øt th√∫c session\n",
    "- S·ª≠ d·ª•ng meaningful names cho traces v√† spans\n",
    "- Th√™m metadata v√† tags ƒë·ªÉ d·ªÖ filter v√† search\n",
    "- T·∫°o scores cho c√°c metrics quan tr·ªçng\n",
    "- Regular review traces ƒë·ªÉ identify improvement opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## üìù Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final flush ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ traces ƒë∆∞·ª£c g·ª≠i\n",
    "langfuse.flush()\n",
    "\n",
    "print(\"‚úÖ Notebook completed successfully!\")\n",
    "print(\"üîç All traces have been sent to LangFuse\")\n",
    "print(\"üåê Check your LangFuse dashboard: http://localhost:3000\")\n",
    "print(\"üìä Analyze your traces and observations for insights!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}