{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning\n",
    "\n",
    "## Paper Information\n",
    "- **Title**: LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning\n",
    "- **Authors**: Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo\n",
    "- **Paper Link**: [arXiv:2308.11148v2](https://arxiv.org/abs/2308.11148v2)\n",
    "- **GitHub**: [https://github.com/LLaMA-Reviewer](https://doi.org/10.5281/zenodo.7991113)\n",
    "\n",
    "## Abstract Summary\n",
    "This paper presents LLaMA-Reviewer, a framework that leverages LLaMA (Large Language Model) for automating code review tasks using Parameter-Efficient Fine-Tuning (PEFT) methods. The framework achieves competitive performance with state-of-the-art code review models while using less than 1% of trainable parameters. The system addresses three core tasks: review necessity prediction, review comment generation, and code refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "Install required packages for implementing LLaMA-Reviewer with LangChain and related tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q transformers==4.36.0\n",
    "!pip install -q peft==0.7.1  # Parameter-Efficient Fine-Tuning library\n",
    "!pip install -q bitsandbytes==0.41.3  # For 8-bit quantization\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q datasets==2.16.0\n",
    "!pip install -q langchain==0.1.0\n",
    "!pip install -q langchain-community==0.1.0\n",
    "!pip install -q torch==2.1.2\n",
    "!pip install -q einops==0.7.0\n",
    "!pip install -q deepeval==0.20.90  # For evaluation metrics\n",
    "!pip install -q rouge-score==0.1.2\n",
    "!pip install -q nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Prompt Templates\n",
    "\n",
    "Implementation of the prompt templates as described in the paper (Figure 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CodeReviewTask:\n",
    "    \"\"\"Dataclass for code review tasks\"\"\"\n",
    "    name: str\n",
    "    instruction: str\n",
    "    input_format: str\n",
    "    output_format: str\n",
    "\n",
    "# Define the three core tasks from the paper\n",
    "REVIEW_TASKS = {\n",
    "    \"review_necessity_prediction\": CodeReviewTask(\n",
    "        name=\"Review Necessity Prediction\",\n",
    "        instruction=\"Determine whether the provided diff hunk requires a code review. Respond with either 'yes' or 'no'.\",\n",
    "        input_format=\"The diff hunk is: '{diff_hunk}'\",\n",
    "        output_format=\"yes/no\"\n",
    "    ),\n",
    "    \"review_comment_generation\": CodeReviewTask(\n",
    "        name=\"Review Comment Generation\",\n",
    "        instruction=\"Review the given code and provide a constructive code review comment.\",\n",
    "        input_format=\"The code is: '{code}'\",\n",
    "        output_format=\"{comment}\"\n",
    "    ),\n",
    "    \"code_refinement\": CodeReviewTask(\n",
    "        name=\"Code Refinement\",\n",
    "        instruction=\"Refine the given code based on the provided code review comment.\",\n",
    "        input_format=\"The comment is: '{comment}'\\nThe code is: '{source_code}'\",\n",
    "        output_format=\"{target_code}\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def create_prompt_template(task: CodeReviewTask) -> str:\n",
    "    \"\"\"Create the prompt template as per Figure 3 in the paper\"\"\"\n",
    "    template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    return template\n",
    "\n",
    "# Example usage\n",
    "print(\"Prompt template structure:\")\n",
    "print(create_prompt_template(REVIEW_TASKS[\"review_necessity_prediction\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mock Dataset Creation\n",
    "\n",
    "Creating synthetic datasets that simulate the CRer and Tufano datasets mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_code_review_dataset(task_type: str, num_samples: int = 100) -> Dataset:\n",
    "    \"\"\"Create mock dataset for code review tasks\"\"\"\n",
    "    \n",
    "    if task_type == \"review_necessity_prediction\":\n",
    "        # Mock diff hunks\n",
    "        diff_hunks = [\n",
    "            \"+    if (user != null && user.isActive()) {\",\n",
    "            \"-    // TODO: implement this function\",\n",
    "            \"+    logger.debug('Processing user: ' + user.id);\",\n",
    "            \"-    return x + y;\",\n",
    "            \"+    return x + y; // Fixed addition\"\n",
    "        ]\n",
    "        \n",
    "        data = []\n",
    "        for i in range(num_samples):\n",
    "            diff = diff_hunks[i % len(diff_hunks)]\n",
    "            needs_review = \"yes\" if i % 2 == 0 else \"no\"\n",
    "            data.append({\n",
    "                \"diff_hunk\": diff,\n",
    "                \"label\": needs_review\n",
    "            })\n",
    "    \n",
    "    elif task_type == \"review_comment_generation\":\n",
    "        # Mock code snippets and comments\n",
    "        code_snippets = [\n",
    "            \"public void processUser(User user) {\\n    user.save();\\n}\",\n",
    "            \"def calculate_sum(a, b):\\n    return a + b\",\n",
    "            \"const fetchData = async () => {\\n    const res = await fetch(url);\\n    return res;\\n}\"\n",
    "        ]\n",
    "        \n",
    "        comments = [\n",
    "            \"Consider adding null check before saving user\",\n",
    "            \"Add type hints for better code clarity\",\n",
    "            \"Missing error handling for fetch operation\"\n",
    "        ]\n",
    "        \n",
    "        data = []\n",
    "        for i in range(num_samples):\n",
    "            data.append({\n",
    "                \"code\": code_snippets[i % len(code_snippets)],\n",
    "                \"comment\": comments[i % len(comments)]\n",
    "            })\n",
    "    \n",
    "    elif task_type == \"code_refinement\":\n",
    "        # Mock code refinement examples\n",
    "        refinements = [\n",
    "            {\n",
    "                \"source_code\": \"public void save(User u) {\\n    u.save();\\n}\",\n",
    "                \"comment\": \"Add null check before saving\",\n",
    "                \"target_code\": \"public void save(User u) {\\n    if (u != null) {\\n        u.save();\\n    }\\n}\"\n",
    "            },\n",
    "            {\n",
    "                \"source_code\": \"def add(a, b):\\n    return a + b\",\n",
    "                \"comment\": \"Add type hints\",\n",
    "                \"target_code\": \"def add(a: int, b: int) -> int:\\n    return a + b\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        data = []\n",
    "        for i in range(num_samples):\n",
    "            ref = refinements[i % len(refinements)]\n",
    "            data.append(ref)\n",
    "    \n",
    "    return Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "# Create sample datasets\n",
    "rnp_dataset = create_mock_code_review_dataset(\"review_necessity_prediction\", 50)\n",
    "rcg_dataset = create_mock_code_review_dataset(\"review_comment_generation\", 50)\n",
    "cr_dataset = create_mock_code_review_dataset(\"code_refinement\", 50)\n",
    "\n",
    "print(f\"Review Necessity Prediction dataset size: {len(rnp_dataset)}\")\n",
    "print(f\"Review Comment Generation dataset size: {len(rcg_dataset)}\")\n",
    "print(f\"Code Refinement dataset size: {len(cr_dataset)}\")\n",
    "print(\"\\nSample from RNP dataset:\")\n",
    "print(rnp_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PEFT Configuration: LoRA and Prefix-Tuning\n",
    "\n",
    "Implementation of the two PEFT methods described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEFTConfigFactory:\n",
    "    \"\"\"Factory for creating PEFT configurations as per paper specifications\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_lora_config(r: int = 16, lora_alpha: int = 16) -> LoraConfig:\n",
    "        \"\"\"\n",
    "        Create LoRA configuration based on paper parameters:\n",
    "        - r (rank): 8 or 16 as tested in the paper\n",
    "        - lora_alpha: scaling factor set to 16\n",
    "        \"\"\"\n",
    "        return LoraConfig(\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_prefix_config(num_virtual_tokens: int = 10, prefix_projection: bool = True) -> PrefixTuningConfig:\n",
    "        \"\"\"\n",
    "        Create Prefix-tuning configuration:\n",
    "        - num_virtual_tokens: 10 as per paper\n",
    "        - prefix_projection: True for zero-init attention\n",
    "        \"\"\"\n",
    "        return PrefixTuningConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            num_virtual_tokens=num_virtual_tokens,\n",
    "            encoder_hidden_size=None,  # Will be set automatically\n",
    "            prefix_projection=prefix_projection,\n",
    "        )\n",
    "\n",
    "# Create configurations\n",
    "lora_config_r8 = PEFTConfigFactory.create_lora_config(r=8)\n",
    "lora_config_r16 = PEFTConfigFactory.create_lora_config(r=16)\n",
    "prefix_config = PEFTConfigFactory.create_prefix_config()\n",
    "\n",
    "print(\"LoRA Configuration (r=16):\")\n",
    "print(lora_config_r16)\n",
    "print(\"\\nPrefix-tuning Configuration:\")\n",
    "print(prefix_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading and PEFT Application\n",
    "\n",
    "Due to computational constraints, we'll use a smaller model to demonstrate the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_with_peft(model_name: str = \"microsoft/phi-2\", peft_config=None, load_in_8bit: bool = True):\n",
    "    \"\"\"\n",
    "    Load model with PEFT configuration.\n",
    "    Note: In the paper, they use LLaMA-7B. Here we use a smaller model for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Quantization config for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=load_in_8bit,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "        bnb_8bit_use_double_quant=True,\n",
    "        bnb_8bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config if load_in_8bit else None,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Apply PEFT if config provided\n",
    "    if peft_config:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Trainable params: {trainable_params:,} || All params: {all_params:,} || Trainable%: {100 * trainable_params / all_params:.2f}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Example: Load model with LoRA\n",
    "print(\"Loading model with LoRA (r=16)...\")\n",
    "# Uncomment below to actually load the model (requires GPU)\n",
    "# model_lora, tokenizer = load_model_with_peft(peft_config=lora_config_r16)\n",
    "print(\"Model loading demonstration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Pipeline\n",
    "\n",
    "Implementation of the training pipeline for the three code review tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewTrainer:\n",
    "    \"\"\"Trainer for code review tasks using PEFT\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, task_type: str):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task = REVIEW_TASKS[task_type]\n",
    "    \n",
    "    def prepare_dataset(self, dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Prepare dataset with proper formatting\"\"\"\n",
    "        \n",
    "        def format_example(example):\n",
    "            # Format according to the prompt template\n",
    "            if hasattr(self.task, 'input_format'):\n",
    "                input_text = self.task.input_format.format(**example)\n",
    "            else:\n",
    "                input_text = str(example)\n",
    "            \n",
    "            prompt = create_prompt_template(self.task).format(\n",
    "                instruction=self.task.instruction,\n",
    "                input=input_text,\n",
    "                output=example.get('label', example.get('comment', example.get('target_code', '')))\n",
    "            )\n",
    "            \n",
    "            return {'text': prompt}\n",
    "        \n",
    "        return dataset.map(format_example)\n",
    "    \n",
    "    def get_training_args(self, output_dir: str, num_epochs: int = 5) -> TrainingArguments:\n",
    "        \"\"\"Get training arguments based on paper specifications\"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=16,  # Effective batch size of 64\n",
    "            warmup_steps=100,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=3e-4,  # LoRA learning rate from paper\n",
    "            fp16=True,\n",
    "            report_to=[],\n",
    "        )\n",
    "\n",
    "# Example trainer setup\n",
    "print(\"Code Review Trainer configured successfully.\")\n",
    "print(\"Training arguments based on paper specifications:\")\n",
    "trainer = CodeReviewTrainer(None, None, \"review_necessity_prediction\")\n",
    "print(trainer.get_training_args(\"./output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics Implementation\n",
    "\n",
    "Implementation of evaluation metrics using deepeval as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class CodeReviewEvaluator:\n",
    "    \"\"\"Evaluator for code review tasks\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_review_necessity(predictions: List[str], labels: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate review necessity prediction (binary classification)\"\"\"\n",
    "        # Convert to binary\n",
    "        pred_binary = [1 if p.lower() == 'yes' else 0 for p in predictions]\n",
    "        label_binary = [1 if l.lower() == 'yes' else 0 for l in labels]\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            label_binary, pred_binary, average='binary'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_bleu4(predictions: List[str], references: List[str]) -> float:\n",
    "        \"\"\"Calculate BLEU-4 score for generation tasks\"\"\"\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        bleu_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Tokenize\n",
    "            pred_tokens = pred.split()\n",
    "            ref_tokens = ref.split()\n",
    "            \n",
    "            # Calculate BLEU-4\n",
    "            score = sentence_bleu(\n",
    "                [ref_tokens], \n",
    "                pred_tokens,\n",
    "                weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                smoothing_function=smoothing\n",
    "            )\n",
    "            bleu_scores.append(score)\n",
    "        \n",
    "        return np.mean(bleu_scores) * 100  # Return as percentage\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_generation_task(predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate comment generation and code refinement tasks\"\"\"\n",
    "        bleu4 = CodeReviewEvaluator.calculate_bleu4(predictions, references)\n",
    "        \n",
    "        return {\n",
    "            'bleu4': bleu4\n",
    "        }\n",
    "\n",
    "# Test evaluation metrics\n",
    "evaluator = CodeReviewEvaluator()\n",
    "\n",
    "# Test review necessity evaluation\n",
    "test_preds = ['yes', 'no', 'yes', 'yes', 'no']\n",
    "test_labels = ['yes', 'no', 'no', 'yes', 'no']\n",
    "rnp_metrics = evaluator.evaluate_review_necessity(test_preds, test_labels)\n",
    "print(\"Review Necessity Prediction Metrics:\")\n",
    "print(f\"Precision: {rnp_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {rnp_metrics['recall']:.3f}\")\n",
    "print(f\"F1: {rnp_metrics['f1']:.3f}\")\n",
    "\n",
    "# Test BLEU-4 calculation\n",
    "test_pred_text = [\"Add null check before saving\"]\n",
    "test_ref_text = [\"Consider adding null check before saving user\"]\n",
    "bleu_score = evaluator.calculate_bleu4(test_pred_text, test_ref_text)\n",
    "print(f\"\\nBLEU-4 Score: {bleu_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LangChain Integration for Inference\n",
    "\n",
    "Using LangChain to create inference pipelines for the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMAReviewerChain:\n",
    "    \"\"\"LangChain-based inference for LLaMA-Reviewer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = None):\n",
    "        self.chains = {}\n",
    "        self._setup_chains()\n",
    "    \n",
    "    def _setup_chains(self):\n",
    "        \"\"\"Setup LangChain chains for each task\"\"\"\n",
    "        \n",
    "        # Review Necessity Prediction Chain\n",
    "        rnp_template = PromptTemplate(\n",
    "            input_variables=[\"diff_hunk\"],\n",
    "            template=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Determine whether the provided diff hunk requires a code review. Respond with either 'yes' or 'no'.\n",
    "\n",
    "### Input:\n",
    "The diff hunk is: '{diff_hunk}'\n",
    "\n",
    "### Response:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Review Comment Generation Chain\n",
    "        rcg_template = PromptTemplate(\n",
    "            input_variables=[\"code\"],\n",
    "            template=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Review the given code and provide a constructive code review comment.\n",
    "\n",
    "### Input:\n",
    "The code is: '{code}'\n",
    "\n",
    "### Response:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Code Refinement Chain\n",
    "        cr_template = PromptTemplate(\n",
    "            input_variables=[\"comment\", \"source_code\"],\n",
    "            template=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Refine the given code based on the provided code review comment.\n",
    "\n",
    "### Input:\n",
    "The comment is: '{comment}'\n",
    "The code is: '{source_code}'\n",
    "\n",
    "### Response:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Store templates (chains would be created with actual LLM)\n",
    "        self.chains = {\n",
    "            'review_necessity': rnp_template,\n",
    "            'comment_generation': rcg_template,\n",
    "            'code_refinement': cr_template\n",
    "        }\n",
    "    \n",
    "    def predict_review_necessity(self, diff_hunk: str) -> str:\n",
    "        \"\"\"Predict if code review is needed\"\"\"\n",
    "        # In practice, this would use the actual model\n",
    "        return \"yes\" if \"TODO\" in diff_hunk or \"debug\" in diff_hunk else \"no\"\n",
    "    \n",
    "    def generate_comment(self, code: str) -> str:\n",
    "        \"\"\"Generate review comment\"\"\"\n",
    "        # Mock implementation\n",
    "        if \"null\" not in code and \"User\" in code:\n",
    "            return \"Consider adding null check before processing user\"\n",
    "        return \"Code looks good\"\n",
    "    \n",
    "    def refine_code(self, source_code: str, comment: str) -> str:\n",
    "        \"\"\"Refine code based on comment\"\"\"\n",
    "        # Mock implementation\n",
    "        if \"null check\" in comment:\n",
    "            return source_code.replace(\"{\", \"{\\n    if (user != null) {\")\n",
    "        return source_code\n",
    "\n",
    "# Test the chain\n",
    "reviewer = LLaMAReviewerChain()\n",
    "\n",
    "# Test review necessity\n",
    "test_diff = \"+    // TODO: implement validation\"\n",
    "necessity = reviewer.predict_review_necessity(test_diff)\n",
    "print(f\"Review needed for diff '{test_diff}': {necessity}\")\n",
    "\n",
    "# Test comment generation\n",
    "test_code = \"public void processUser(User user) {\\n    user.save();\\n}\"\n",
    "comment = reviewer.generate_comment(test_code)\n",
    "print(f\"\\nGenerated comment: {comment}\")\n",
    "\n",
    "# Test code refinement\n",
    "refined = reviewer.refine_code(test_code, comment)\n",
    "print(f\"\\nRefined code:\\n{refined}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis and Visualization\n",
    "\n",
    "Recreating key results from the paper's evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Results from Table V and VI in the paper\n",
    "results_data = {\n",
    "    'Review Comment Generation': {\n",
    "        'Models': ['Transformer-b', 'CodeT5', 'CodeReviewer', 'LLaMA-Reviewer (Prefix)', 'LLaMA-Reviewer (LoRA)'],\n",
    "        'BLEU-4': [4.76, 4.83, 5.32, 5.16, 5.70],\n",
    "        'Parameters (M)': [220, 220, 220, 1.2, 8.4]\n",
    "    },\n",
    "    'Code Refinement': {\n",
    "        'Models': ['CodeT5', 'CodeReviewer', 'LLaMA-Reviewer (Prefix)', 'LLaMA-Reviewer (LoRA r=8)', 'LLaMA-Reviewer (LoRA r=16)'],\n",
    "        'BLEU-4': [80.82, 82.61, 76.71, 81.59, 82.27],\n",
    "        'Parameters (M)': [220, 220, 1.2, 4.2, 8.4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for idx, (task, data) in enumerate(results_data.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create bar plot\n",
    "    x = np.arange(len(data['Models']))\n",
    "    width = 0.35\n",
    "    \n",
    "    # BLEU-4 scores\n",
    "    bars1 = ax.bar(x - width/2, data['BLEU-4'], width, label='BLEU-4', color='skyblue')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('BLEU-4 Score')\n",
    "    ax.set_title(f'{task} Performance')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(data['Models'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Parameter efficiency comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['Full Fine-tuning', 'LLaMA-Reviewer\\n(LoRA r=16)', 'LLaMA-Reviewer\\n(LoRA r=8)', 'LLaMA-Reviewer\\n(Prefix)']\n",
    "params = [6700, 8.4, 4.2, 1.2]  # in millions\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "\n",
    "bars = ax.bar(models, params, color=colors, alpha=0.7)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, param) in enumerate(zip(bars, params)):\n",
    "    height = bar.get_height()\n",
    "    percentage = (param / 6700) * 100 if i > 0 else 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{param}M\\n({percentage:.2f}%)', ha='center', va='bottom')\n",
    "\n",
    "ax.set_ylabel('Trainable Parameters (Millions)')\n",
    "ax.set_title('Parameter Efficiency: PEFT vs Full Fine-tuning')\n",
    "ax.set_ylim(0, 7000)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Research Extension Template\n",
    "\n",
    "Template for extending this research with your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchExtension:\n",
    "    \"\"\"Template for extending LLaMA-Reviewer research\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiment_log = []\n",
    "    \n",
    "    def experiment_1_different_llm(self):\n",
    "        \"\"\"\n",
    "        Experiment: Test with different LLMs (e.g., CodeLLaMA, StarCoder)\n",
    "        \"\"\"\n",
    "        print(\"Experiment 1: Testing different LLMs\")\n",
    "        print(\"- Try CodeLLaMA for better code understanding\")\n",
    "        print(\"- Compare with StarCoder for code-specific tasks\")\n",
    "        print(\"- Evaluate trade-offs between unified and code-specific LLMs\")\n",
    "    \n",
    "    def experiment_2_advanced_peft(self):\n",
    "        \"\"\"\n",
    "        Experiment: Test newer PEFT methods\n",
    "        \"\"\"\n",
    "        print(\"\\nExperiment 2: Advanced PEFT methods\")\n",
    "        print(\"- QLoRA: 4-bit quantization + LoRA\")\n",
    "        print(\"- AdaLoRA: Adaptive rank allocation\")\n",
    "        print(\"- IA3: Infused Adapter by Inhibiting and Amplifying\")\n",
    "    \n",
    "    def experiment_3_multilingual_review(self):\n",
    "        \"\"\"\n",
    "        Experiment: Extend to multilingual code review\n",
    "        \"\"\"\n",
    "        print(\"\\nExperiment 3: Multilingual code review\")\n",
    "        print(\"- Test on non-English comments\")\n",
    "        print(\"- Mixed language code bases\")\n",
    "        print(\"- Cross-language code refinement\")\n",
    "    \n",
    "    def experiment_4_realtime_integration(self):\n",
    "        \"\"\"\n",
    "        Experiment: Integration with development tools\n",
    "        \"\"\"\n",
    "        print(\"\\nExperiment 4: Real-time integration\")\n",
    "        print(\"- VS Code extension using the model\")\n",
    "        print(\"- GitHub Actions integration\")\n",
    "        print(\"- Performance optimization for real-time use\")\n",
    "\n",
    "# Show research extension ideas\n",
    "research = ResearchExtension()\n",
    "research.experiment_1_different_llm()\n",
    "research.experiment_2_advanced_peft()\n",
    "research.experiment_3_multilingual_review()\n",
    "research.experiment_4_realtime_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Paper Contributions:\n",
    "1. **First application of LLMs to code review automation** - Demonstrated that general-purpose LLMs can match specialized models\n",
    "2. **PEFT paradigm for code review** - Achieved <1% trainable parameters while maintaining performance\n",
    "3. **Comprehensive evaluation** - Tested on multiple datasets and tasks with thorough ablation studies\n",
    "4. **Open-source implementation** - Made code and models publicly available\n",
    "\n",
    "### Key Technical Insights:\n",
    "- **LoRA outperforms Prefix-tuning** for code review tasks\n",
    "- **Input representation matters** - Keeping code format close to pre-training data improves performance\n",
    "- **Instruction tuning helps** but the effect is task-dependent\n",
    "- **Larger LoRA rank (r=16)** provides better performance than r=8\n",
    "\n",
    "### Practical Applications:\n",
    "- Automated PR review systems\n",
    "- IDE integration for real-time code suggestions\n",
    "- Training data generation for code quality models\n",
    "- Multi-language code review support\n",
    "\n",
    "### Future Research Directions:\n",
    "1. Testing with larger LLaMA models (13B, 70B)\n",
    "2. Exploring code-specific LLMs (CodeLLaMA)\n",
    "3. Real-time optimization for production use\n",
    "4. Multi-modal approaches (code + documentation)\n",
    "5. Integration with existing development workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}