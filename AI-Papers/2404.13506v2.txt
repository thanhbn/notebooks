# 2404.13506v2.pdf
# Converted from PDF to TXT
# Source path: D:\llm\notebooks\AI-Papers\2404.13506v2.pdf
# File size: 376226 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications
Charith Chandra Sai Balne1,Sreyoshi Bhaduri2∗,Tamoghna Roy3†and Vinija Jain4and Aman
Chadha4,5*
1University of Southern California
2Amazon
3Deepsig Inc.
4Stanford University
5Amazon GenAI
charithchandra23@gmail.com, sreyoshibhaduri@gmail.com,
tamoghna.roy@gmail.com, hi@vinija.ai, hi@aman.ai
Abstract
The rise of deep learning has marked significant
progress in fields such as computer vision, natu-
ral language processing, and medical imaging, pri-
marily through the adaptation of pre-trained mod-
els for specific tasks. Traditional fine-tuning meth-
ods, involving adjustments to all parameters, face
challenges due to high computational and mem-
ory demands. This has led to the development
of Parameter Efficient Fine-Tuning (PEFT) tech-
niques, which selectively update parameters to bal-
ance computational efficiency with performance.
This review examines PEFT approaches, offering
a detailed comparison of various strategies high-
lighting applications across different domains, in-
cluding text generation, medical imaging, protein
modeling, and speech synthesis. By assessing the
effectiveness of PEFT methods in reducing com-
putational load, speeding up training, and lower-
ing memory usage, this paper contributes to making
deep learning more accessible and adaptable, facil-
itating its wider application and encouraging inno-
vation in model optimization. Ultimately, the pa-
per aims to contribute towards insights into PEFT’s
evolving landscape, guiding researchers and prac-
titioners in overcoming the limitations of conven-
tional fine-tuning approaches.
1 Introduction
Deep learning has revolutionized the field of artificial intel-
ligence, enabling remarkable advancements in various ap-
plications such as Large-scale vision-language (VL) models
[Radford et al. , 2021 ],[Jiaet al. , 2021 ],[Yaoet al. , 2021 ],
[Alayrac et al. , 2022 ],[Yuan et al. , 2021 ]natural language
processing [Luet al. , 2022 ],[Yanet al. , 2022 ], and speech
recognition [Nassif et al. , 2019 ],[Prabhavalkar et al. , 2023 ].
However, the fine-tuning process, which involves adjusting
∗Work does not relate to position at Amazon.
†Work does not relate to position at DeepSig Inc.model weights to fit new tasks or datasets, can be computa-
tionally expensive and memory-intensive. This has led to a
growing interest in PEFT methods that can reduce the com-
putational cost and memory usage while maintaining perfor-
mance.
PEFT methods aim to strike a balance between accuracy
and efficiency by selectively updating a subset of model
parameters, leveraging knowledge distillation, or exploiting
structural redundancy. These methods have the potential to
significantly reduce the computational cost and memory us-
age, making deep learning more accessible and scalable for
a wider range of applications and devices. This review pa-
per aims to provide a comprehensive overview of the recent
advances in PEFT methods, discussing their underlying prin-
ciples, applications, and trade-offs. We explore state-of-the-
art techniques, compare their performance, and highlight the
challenges and future research directions in this emerging
field. By shedding light on the efficiency aspects of fine-
tuning, our paper aspires to contribute to democratizing deep
learning and enabling its widespread adoption across applica-
tions.
2 Fine-tuning Methods
Modern pre-trained models (such as BERT [Devlin et al. ,
2018 ], GPT [Radford et al. , 2019 ], T5 [Raffel et al. , 2020 ],
etc.) consist of billions, if not trillions (especially in case of
mixture-of-experts architectures), of parameters. Traditional
fine-tuning methods involve adjusting allmodel parameters
to fit the new task or dataset, which can be computationally
expensive and memory-intensive. This approach is often re-
ferred to as ”full fine-tuning” [Lvet al. , 2023 ]. Full fine-
tuning requires a large amount of data and computational re-
sources to converge [Mohammadi and Chapon, 2020 ], which
can be a limitation for tasks with limited data availability or
computational budgets. Additionally, fine-tuning all param-
eters often lead to over-fitting, especially when the new task
has limited data.
Another limitation of traditional fine-tuning methods is
that they do not leverage the knowledge gained during pre-
training [Han et al. , 2024 ]. Pre-trained models are typically
trained on large datasets and have learned general featuresarXiv:2404.13506v2  [cs.LG]  23 Apr 2024

--- PAGE 2 ---
Parameter Ef ficient
Fine TuningVideo Text
Generation
Medical Imaging
Protein Models
Code Review
Generation
Speech SynthesisCLIP , LLAMA-7B
Bio Mistral, ResNet-
50, Supervised V iT
Base, V iT Base MAE,
scBER T
ESM2
LLAMA-6.7B
WavLm, W av2V ec
2.0, Whisper TinyAGAdapter + KaAdapter
+PgAdapter
Task-Specific Adapters
(TSA), Scale-Shift Features
(SSF), freezing layers tuning
(FL) + BitFit (BF) + LoRA
(LR)
BitFit, LoRA
Zero-init attention
prefix tuning +LoRA
adapter tuning,
embedding prompt
tuning, LoRACLIP: 0.09%
AP: 1.18%, FL: 16.66%,
BF: 0.22%, LR: 0.81%
BF: 0.22, LR: 0.81
LR: < 1%
LR: < 0.8%ApplicationsBackbone model
usedPEFT methods usedMaximum % of
parameters fine-
tuned
Commonsense 
and 
Arithmetic ReasoningLLaMA-7B,
LLaMA-13B10x-50x <: LR LoReFTFigure 1: Comparative study of PEFT across different applications.
that are useful across multiple tasks. Full fine-tuning discards
this knowledge and starts from scratch (e.g., [Korbak et al. ,
2022 ]), which can lead to sub-optimal performance.
Finally, traditional fine-tuning methods can result in catas-
trophic forgetting, where the model forgets the knowledge
learned during pre-training [Chen et al. , 2020 ]. This can lead
to poor performance on both the new task and the original
task, making it difficult to achieve good performance across
multiple tasks. These limitations have led researchers to ex-
plore PEFT methods that can address these issues. PEFT
allows to only fine-tune a small number of model parame-
ters while freezing most of the parameters of the pre-trained
LLM. PEFT has the following advantages: (i) reduced com-
putational costs (requires fewer GPUs and GPU time); (ii)
faster training times (finishes training faster); (iii) lower
hardware requirements (works with cheaper GPUs with less
VRAM); (iv) better modeling performance (reduces over-
fitting); and (v) less storage (majority of weights can be
shared across different tasks).
3 Applications
In this section, we explore parameter-efficient fine-tuning
across various applications including commonsense and
arithmetic reasoning, generating descriptive texts for videos,enhancing medical imaging accuracy, refining protein mod-
els for better scientific insights, automating code review and
generation, and advancing speech synthesis technologies. A
comparative analysis of PEFT methods is given in Table 3.
3.1 Commonsense and Arithmetic Reasoning
Representation Fine-Tuning (ReFT) is a technique that mod-
ifies only a minimal subset of model weights to fine-tune
large-scale language models through [Wuet al. , 2024 ]. The
paper presents a specific variant of ReFT, dubbed Low-
rank Linear Subspace ReFT (LoReFT), which modifies the
model’s internal representations and exhibits far greater pa-
rameter efficiency, with improvements by factors of 10 to
50 compared to contemporary PEFT methods. The foun-
dational mechanism of the LoReFT framework, is defined
by the Distributed Interchange Intervention (DII) formula
DII (b, s, R ) =b+R⊤(Rs−Rb).[Wuet al. , 2024 ]employ
the projection matrix R to refine the hidden states b, steer-
ing them toward a target state s. This method is crafted to
subtly yet efficiently influence the model’s output, guiding
it towards desired behaviors or responses. Extensive evalua-
tions conducted by the authors on various reasoning tasks and
benchmarks such as Alpaca-Eval v1.0 and GLUE indicated
that LoReFT not only achieves better efficiency but also su-

--- PAGE 3 ---
perior performance relative to leading PEFT approaches over
different datasets in their respective categories.
LoReFT achieved state-of-the-art performance for com-
monsense reasoning, surpassing other methods such as Pre-
fix Tuning [Bisk et al. , 2019 ], Adapter-based methods, and
LoRA, particularly on LLaMA-7B and LLaMA-13B mod-
els. LoReFT showed an accuracy improvement, averaging
an 80.2% and 83.3% across different datasets BoolQ, PIQA,
SIQA, HellaS., WinoG., ARC-e, ARC-c, OBQA, for the
Llama 7B and 13B models respectively. See specific results
from the paper in Table 1.
Table 1: Average performance of commonsense reasoning over
BoolQ, PIQA, SIQA, HellaS., WinoG., ARC-e, ARC-c, OBQA,
datasets for the LLaMA-7B and LLaMA-13B models. Comparisons
from research conducted by [Wuet al. , 2024 ]
Model PEFT Params (%) Avg. Accuracy
ChatGPT — — 77.0%
LLaMA-7B
PrefT 0.110% 64.6%
AdapterS 0.990% 70.8%
AdapterP 3.540% 72.3%
LoRA 0.830% 74.7%
DoRA (half) 0.430% 77.5%
DoRA 0.840% 78.1%
LoReFT 0.031% 80.2%
LLaMA-13B
PrefT 0.030% 68.4%
AdapterS 0.800% 79.5%
AdapterP 2.890% 81.5%
LoRA 0.670% 80.5%
DoRA (half) 0.350% 80.8%
DoRA 0.680% 81.5%
LoReFT 0.025% 83.3%
The performance of the LoReFT in arithmetic reasoning
[Huet al. , 2023 ]tasks is found to be inferior to that of LoRA
and adapters, though it surpasses prefix-tuning. The anal-
ysis indicates that LoReFT may encounter more challenges
in chain-of-thought reasoning as opposed to single-step com-
monsense reasoning tasks. This difficulty is attributed to the
extended length of generations, which diminishes the efficacy
of the intervention, and the inherent complexity of the task.
Additionally, the paper revealed that LoReFT demonstrates
improved performance with the 13B model compared to the
7B model, suggesting scalability of LoReFT with increased
model size.See specific results from the paper in Table 2.
3.2 Video Text Generation
Video-text understanding pertains to how videos and words
relate to each other. This area looks into finding videos based
on text descriptions and creating captions for videos, which
is key for making sense of what’s happening in a video just
by looking at the words linked to it. Fang et al. introduce
the Alignment and Generation Adapter (AGAdapter) for
enhancing video-text understanding [Fang et al. , 2023 ].Table 2: Arithmetic reasoning performance of LLaMA-7B and
LLaMA-13B models over AQuA, GSM8K, MAWPS, SV AMP
datasets. Comparisons from research conducted by [Wuet al. , 2024 ]
Model PEFT Params (%) AQuA GSM8K MA WPS SV AMP Avg.
LLaMA-7B
PrefT 0.110% 14.2 24.4 63.4 38.1 35.0
AdapterS 0.990% 15.0 33.3 77.7 52.3 44.6
AdapterP 3.540% 18.1 35.3 82.4 49.6 46.4
LoRA 0.830% 18.9 37.5 79.0 52.1 46.9
LoReFT 0.031% 21.4 26.0 76.2 46.8 42.6
LLaMA-13B
PrefT 0.300% 15.7 31.1 66.8 41.4 38.8
AdapterS 0.800% 22.0 44.0 78.6 50.8 48.9
AdapterP 2.890% 20.5 43.3 81.1 55.7 50.2
LoRA 0.670% 18.5 47.5 83.6 54.6 51.1
LoReFT 0.025% 23.6 38.1 82.4 54.2 49.6
This integrates a knowledge-sharing alignment adapter with
a large language model for video-text retrieval and video
captioning tasks, achieving state-of-the-art performance on
MSR-VTT and ActivityNet benchmarks. Their research
introduces a novel approach to video-text understanding
by integrating the pre-trained CLIP model (CLIP-bigG/14)
for encoding and the LLaMA-7B model for language pro-
cessing, alongside KaAdapter and Pg Adapter for efficient
adaptation. These components work together within a robust
tech stack that optimizes video and text alignment across
various datasets, including MSR-VTT and ActivityNet,
tailored with video and caption lengths set to dataset-specific
requirements. Numerical results from an ablation study
on the MSR-VTT dataset reveal the AGAdapter’s efficacy,
particularly when augmented with LIcap, showcasing re-
markable enhancements in video-text retrieval and video
captioning metrics compared to the CLIP-finetuned baseline.
These outcomes underscore the method’s success in deliver-
ing significant performance uplifts within minimal training
times (0.12 to 0.5 hours), affirming its potential in advancing
video-text comprehension tasks with high efficiency and
effectiveness.
Similarly, the KAdaptation method, achieves a trade-off
between accuracy and parameter efficiency in the vision
transformer (ViT-B-224/32) through CLIP pretraining [Heet
al., 2023 ]. Evaluated across 20 datasets from the ELEV ATER
benchmark, this approach notably excels by updating merely
0.09 percent of the model’s parameters, underscoring its ef-
ficiency. This result emphasizes the method’s capability to
maintain high accuracy while significantly reducing the num-
ber of trainable parameters, showcasing its potential for ef-
fective and efficient model adaptation .
3.3 Medical Imaging
Advancements in medical imaging technologies are spear-
heading transformative changes across various sectors of
modern medicine [Azizi et al. , 2021 ], encompassing both
clinical diagnostics and biomedical research. [Dutt et al. ,
2023 ]evaluates PEFT techniques for medical image analysis
[Chambon et al. , 2022 ],[Kirillov et al. , 2023 ], focusing on
convolutional and transformer-based networks across six
datasets. It assesses 16 PEFT methods through over 600
experiments, showing performance gains of up to 22 percent

--- PAGE 4 ---
Table 3: Comparative analysis of prevalent PEFT methods.
Method Parameter reduction (%) Advantages Disadvantages
Full Fine-Tuning (ViT-B/16, BARD)
[Liuet al. , 2022 ]0 Performant baselineHigh memory footprint (33B
parameters)
Adapter Modules (Tiny)
[van der Marel et al. , 2022 ]85 Flexible, modular design Requires hyperparameter tuning
Adapter Modules (Small) 75 Flexible, modular design Requires hyperparameter tuning
LoRA
[Zhou et al. , 2021 ]90 Memory efficient (3.3B parameters) Limited control over updates
LoReFT
[Wuet al. , 2024 ]70-90Memory efficient, potentially
interpretableEfficiency depends on task and
hyperparameters
Prefix Tuning (Learned)
[Luoet al. , 2021 ]65 Simple implementationMay not capture complex video
features
Sparse Fine-Tuning (40% pruning)
[Saied, 2016 ]60 Memory efficient (13.2B parameters)Requires careful selection of
parameters
Sparse Fine-Tuning (80% pruning)80Extremely memory efficient (6.6B
parameters)Significant accuracy drop at high
pruning ratio
BitFit (8-bit)
[Zaken et al. , 2022 ]95Extremely memory efficient (1.65B
parameters)Limited performance gains in
high-data regime
in some scenarios, especially in medical text-to-image
generation tasks. The study demonstrates PEFT’s superiority
over traditional fine-tuning in certain conditions, particularly
when data is scarce or model size is large. It underscores
the effectiveness of PEFT in reducing computational costs
while maintaining or improving performance, making it a
valuable approach for the medical domain. [Liuet al. , 2023 ]
explore parameter-efficient fine-tuning methods for cell type
annotation in scRNA-seq data using scBERT [Choromanski
et al. , 2022 ]. It demonstrates that such methods can achieve
high performance with significantly fewer parameters. Key
results show that methods like Adapter [Houlsby et al. ,
2019 ], BitFit, and LoRA, despite reducing tunable parame-
ters BitFit uses only 0.22 percent of the model’s parameters,
maintain performance close to full fine-tuning, with LoRA
and a combination of BitFit and LoRA among the most
effective strategies. As per the Experiment conducted FT
[vanilla fine-tuning] uses 100 percent of the model’s param-
eters, whereas parameter-efficient methods use significantly
less: AP[adapter] uses 1.18 percent, FL[freezing layers
tuning] uses 16.66 percent, BF[BitFit] uses 0.22 percent, and
LR[LoRA] uses 0.81 percent.
Biomedical question answering was shown to significantly
improve accuracy with only 0.152 percent of baseline param-
eters fine-tuned [Wang et al. , 2023 ]. The strategy adopted
includes contrastive learning and self-consistency voting,
tested on PubMedQA and BioASQ datasets. Remarkably, it
achieves comparable performance to GPT-4, outperforming
domain-specific models without external knowledge. The T5
models highlights efficient tuning in resource-constrained en-
vironments, balancing performance and computational costs.
3.4 Protein Models
Large-scale protein models have significantly transformed
the field of proteomics through their capacity to learn from
extensive volumes of sequence data autonomously. Later,
these models get a bit of training on specific tasks to make
them even better at what they do [Sledzieski et al. , 2023 ]in-
troduced parameter-efficient fine-tuning methods for proteinlanguage models, focusing on tasks like protein-protein in-
teraction (PPI) prediction and homooligomer symmetry pre-
diction. It shows that PEFT can achieve comparable or supe-
rior performance to traditional fine-tuning with significantly
fewer parameters. For PPI prediction, PEFT models even out-
perform traditional methods. Despite the dramatic reduction
in tunable parameters (BitFit at 0.22 percent , Adapter at 1.18
percent, Low-Rank Adaptation at 0.81 percent, and Freez-
ing Layers at 16.66 percent compared to the full model’s 100
percent), these methods maintain or nearly match the per-
formance of traditional fine-tuning across various datasets.
For instance, on the Zheng68k dataset, accuracy and F1
scores were closely aligned across methods, with Adapter and
Low-Rank Adaptation showing particularly strong perfor-
mance. Similar trends were observed in the Baron-human and
Baron-mus datasets, where these parameter-efficient meth-
ods achieved high accuracy and F1 scores, showcasing their
capability to deliver efficient and scalable solutions for cell
type annotation while significantly reducing computational
resources.
3.5 Code Review / Generation
Since Fagan [Fang et al. , 2023 ]introduced it in 1976, code
review has been key in finding bugs, improving quality, and
sharing knowledge in software development. But, this mostly
manual task can really pile on the work for developers. Even
with today’s modern code review methods, which are a bit
smoother than the old ways, it still asks a lot from them.
[Luet al. , 2023 ]The study introduces LLaMA-Reviewer, a
framework that automates code review tasks by leveraging
PEFT techniques on the LLaMA model. It achieved notable
numerical insights across various metrics: For Review Ne-
cessity Prediction on the CRer dataset, it reached a preci-
sion of 60.99 percent, a recall of 83.50 percent, and an F1
score of 70.49 percent using Low-Rank Adaptation (LoRA).
In Code Review Comment Generation, LLaMA-Reviewer
scored BLEU-4 scores of 5.70 on the CRer dataset and 5.04
on the Tufano dataset, showcasing its superior performance
over existing models like CodeReviewer and AUGER. Ad-
ditionally, for Code Refinement tasks, it attained BLEU-4

--- PAGE 5 ---
θ θ'
'θ'  =
'
θ'  =
'
θ'  =
'Initial Model
(φ)Pre-trained model
(θ)
Parameter Ef ficient Fine TuningAddition
Specification
ReparameterizationFrozen Parameters Tunable ParametersFigure 2: Illustration of workflow for the PEFT paradigm starting with a pre-trained model ( θ), to which modifications such as additions,
specifications, and reparameterizations are applied, effectively differentiating between frozen and tunable parameters to enhance model
performance.
scores of 82.27 on the CRer dataset and 78.23 on the Tu-
fano dataset, demonstrating its competitive or superior capa-
bility compared to traditional models. These results highlight
LLaMA-Reviewer’s efficiency in code review automation, of-
fering promising directions for future software engineering
research with a focus on minimizing the need for extensive
parameter tuning while maintaining high performance.
3.6 3D Pretrained Models
In exploring efficient approaches for fine-tuning pre-trained
3D models, a novel framework named Point-PEFT [Tang et
al., 2023 ]has been proposed, demonstrating enhanced perfor-
mance over traditional full fine-tuning methods with a signifi-
cantly reduced computational footprint. Notably, Point-PEFT
managed to outperform the full fine-tuning benchmarks on
ModelNet40 and ScanObjectNN [Uyet al. , 2019 ], achieving
accuracy levels of 94.2% and 89.1% respectively, while re-
quiring merely 5% of the trainable parameters compared to
22.1M parameters in the full fine-tuning setup. Such results
underscore the efficiency and general applicability of Point-
PEFT across various pre-trained 3D models, including Point-
BERT [Yuet al. , 2022 ]and Point-M2AE [Zhang et al. , 2022 ],
highlighting its potential for broader adoption in the field of
3D point cloud processing [Tang et al. , 2024 ]
3.7 Speech Synthesis
In[Feng and Narayanan, 2023 ], the authors meticulously
evaluated the effectiveness of PEFT methods, namely adapter
tuning, embedding prompt tuning, and Low-rank approxima-
tion (LoRA), across four prominent SER [Chen and Rud-
nicky, 2023 ],[Feng et al. , 2023 ]datasets [Houlsby et al. ,
2019 ]. Fine-tuning methods comparatively provided better
results than previous methods, which were solely dependenton MLP (Multilayer Perceptron), CNN (Convolutional Neu-
ral Networks), RNN (Recurrent Neural Networks), Mixed
data Neural Networks [Sanjeev et al. , 2021 ]by extract-
ing higher-order melfrequency cepstral coefficients [Wanli
and Guoxin, 2013 ]. The results reveal a notable superi-
ority of LoRA in enhancing the fine-tuning performance
of pre-trained speech models for emotion recognition tasks
by using generative [?], dis- criminative [Baevski et al. ,
2020 ],[Schneider et al. , 2019 ]and multi-task learning ob-
jectives. Specifically, LoRA outperformed other PEFT meth-
ods, achieving the highest average Unweighted Average Re-
call (UAR) of 67.3% on the WavLM Base+ model, demon-
strating its effectiveness in adapting pre-trained models to
SER tasks efficiently. In contrast, traditional adapter tun-
ing and embedding prompt methods yielded lower perfor-
mance, with adapter tuning achieving an average UAR of
63.07‘%‘ on the Wav2Vec 2.0 Base model [Radford et al. ,
2022 ]and embedding prompt tuning showing less impact on
performance across various models. Furthermore, the study
highlighted the minimal additional parameter requirement in-
troduced by LoRA, underlining its practicality for real-world
applications. Additionally, the research underscored the im-
portance of fairness in SER systems, with LoRA showing
promising results in improving fairness scores across multiple
datasets. These findings not only demonstrate the potential
of LoRA in achieving high performance and fairness in SER
tasks but also pave the way for future research directions fo-
cusing on the optimization of PEFT methods for speech emo-
tion recognition. A similar and innovative study in [Liuet
al., 2024 ]states child whisper recognition, whereas [Anjali
et al. , 2022 ]uses some similar techniques of transfer learn-
ing to understand child behaviours using their speech and cry
sounds.

--- PAGE 6 ---
4 Considerations for Evaluation Across PEFT
Methods
PEFT has emerged as a compelling approach for tailoring
large pre-trained models to specific tasks while minimizing
computational demands. Our review found that leveraging
PEFT across diverse applications presents several key chal-
lenges that require careful consideration, as practitioners con-
sider applying PEFT for their applications:
A) Balancing Efficiency and Performance : A core chal-
lenge lies in striking a delicate balance between reducing
trainable parameters and maintaining robust performance
[Naveed et al. , 2024 ]. Fine-tuning too few parameters might
hinder the model’s ability to adapt effectively to the target
task, while excessively fine-tuning can negate the computa-
tional benefits of PEFT [Dutt et al. , 2023 ].
B) Data Scarcity and Generalizability : The success of
PEFT can be contingent on the quality and quantity of data
available for fine-tuning. In domains with limited or noisy
data, PEFT may struggle to achieve the same level of accu-
racy attainable with full fine-tuning on a larger dataset [Dutt
et al. , 2024 ]. Careful selection of data augmentation tech-
niques and transfer learning strategies [Anjali et al. , 2022 ]can
be crucial to mitigate this challenge.
C) Over-fitting and Generalization Trade-off : There is
an inherent risk of over-fitting the model to the training data
[Chavan et al. , 2024 ], particularly when using a restricted set
of parameters for fine-tuning. This can lead to a scenario
where the model performs well on the training data but ex-
hibits poor performance on unseen examples. To address this,
employing appropriate regularization techniques and metic-
ulous hyperparameter tuning becomes essential to promote
better generalization to new data [Kirk et al. , 2024 ].
D) Capacity Constraints of Incremental Modules : Cer-
tain PEFT methods introduce additional modules with a re-
duced number of parameters on top of the pre-trained model.
The challenge here lies in ensuring that these smaller mod-
ules possess sufficient capacity to learn the intricacies of the
specific task effectively, especially when there are strict con-
straints on the allowable number of parameters. Ongoing re-
search is focused on developing methods to enhance the ca-
pacity of these modules without compromising parameter ef-
ficiency.
5 Discussions
This study provides an exhaustive review of the literature con-
cerning the effectiveness of various PEFT techniques across
multiple applications.
These include Video Text Generation utilizing distinct
adaptors for downstream tasks, Biomedical Imaging charac-
terized by stringent data confidentiality and significant an-
notation costs, Protein models necessitating extensive pa-
rameters for comprehensive fine-tuning, and Code Review
Generation. Our analysis reveals that Low-Rank Adaptation
(LoRA) fine-tunes a minimal number of parameters, thus en-
abling the recalibration of training weights on a single GPU.
Conversely, Differentiable Rank Adaptation (DoRA) demon-
strates superior performance, outperforming LoRA.We also propose several potential directions for future re-
search to further advance the PEFT field, particularly focus-
ing on the evaluation of specific applications:
A) Task-Agnostic PEFT Techniques:
Future research should focus on developing PEFT methods
that are universally applicable across different downstream
tasks. This would reduce the necessity for specialized adap-
tors in each application domain, enhancing the flexibility and
ease of PEFT deployment. Exploring meta-learning or trans-
ferable parameter approaches may achieve task-agnostic effi-
cacy.
B) Privacy-Preserving PEFT for Sensitive Data:
In fields such as biomedical imaging where data privacy
is crucial, it is essential to adapt PEFT to operate on sensi-
tive datasets without breaching patient confidentiality. Ex-
ploring federated learning or homomorphic encryption tech-
niques could allow for privacy-preserving PEFT.
C) Limited Labeled Data and PEFT:
Given the frequent scarcity of labeled data in domains like
biomedical imaging, enhancing the robustness of PEFT in
these contexts is critical. Future investigations could consider
active learning or curriculum learning techniques to improve
fine-tuning under limited data conditions.
D) Interpretability of Fine-Tuned Protein Models:
While PEFT reduces the parameter count in protein mod-
els, its impact on model interpretability remains uncertain.
Future research should examine methods to elucidate the
decision-making processes and mechanisms within these
fine-tuned models.
By addressing these future research directions, we can fully
harness the capabilities of PEFT, ensuring its progressive de-
velopment for efficient and effective fine-tuning of large mod-
els across diverse applications.
References
[Alayrac et al. , 2022 ]Jean-Baptiste Alayrac, Jeff Donahue,
Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katie Millican, Malcolm
Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,
Tengda Han, Zhitao Gong, Sina Samangooei, Marianne
Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew
Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj
Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zis-
serman, and Karen Simonyan. Flamingo: a visual lan-
guage model for few-shot learning, 2022.
[Anjali et al. , 2022 ]Golla Anjali, Santosh Sanjeev, Akuraju
Mounika, Gangireddy Suhas, G. Pradeep Reddy, and
Yarlagadda Kshiraja. Infant cry classification using trans-
fer learning. In TENCON 2022 - 2022 IEEE Region 10
Conference (TENCON) , pages 1–7, 2022.
[Azizi et al. , 2021 ]Shekoofeh Azizi, Basil Mustafa, Fiona
Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton,
Aaron Loh, Alan Karthikesalingam, Simon Kornblith,
Ting Chen, Vivek Natarajan, and Mohammad Norouzi.
Big self-supervised models advance medical image clas-
sification, 2021.
[Baevski et al. , 2020 ]Alexei Baevski, Henry Zhou, Abdel-
rahman Mohamed, and Michael Auli. wav2vec 2.0: A

--- PAGE 7 ---
framework for self-supervised learning of speech repre-
sentations, 2020.
[Bisk et al. , 2019 ]Yonatan Bisk, Rowan Zellers, Ronan Le
Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning
about physical commonsense in natural language, 2019.
[Chambon et al. , 2022 ]Pierre Chambon, Christian Blueth-
gen, Jean-Benoit Delbrouck, Rogier Van der Sluijs,
Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tan-
ishq Mathew Abraham, Shivanshu Purohit, Curtis P. Lan-
glotz, and Akshay Chaudhari. Roentgen: Vision-language
foundation model for chest x-ray generation, 2022.
[Chavan et al. , 2024 ]Arnav Chavan, Raghav Magazine,
Shubham Kushwaha, M ´erouane Debbah, and Deepak
Gupta. Faster and lighter llms: A survey on current chal-
lenges and way forward, 2024.
[Chen and Rudnicky, 2023 ]Li-Wei Chen and Alexander
Rudnicky. Exploring wav2vec 2.0 fine-tuning for im-
proved speech emotion recognition, 2023.
[Chen et al. , 2020 ]Sanyuan Chen, Yutai Hou, Yiming Cui,
Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and
learn: Fine-tuning deep pretrained language models with
less forgetting. arXiv preprint arXiv:2004.12651 , 2020.
[Choromanski et al. , 2022 ]Krzysztof Choromanski, Valerii
Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Col-
well, and Adrian Weller. Rethinking attention with per-
formers, 2022.
[Devlin et al. , 2018 ]Jacob Devlin, Ming-Wei Chang, Ken-
ton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understand-
ing.arXiv preprint arXiv:1810.04805 , 2018.
[Dutt et al. , 2023 ]Raman Dutt, Linus Ericsson, Pedro
Sanchez, Sotirios A. Tsaftaris, and Timothy Hospedales.
Parameter-efficient fine-tuning for medical image analy-
sis: The missed opportunity, 2023.
[Dutt et al. , 2024 ]Raman Dutt, Ondrej Bohdal, Sotirios A.
Tsaftaris, and Timothy Hospedales. Fairtune: Optimizing
parameter efficient fine tuning for fairness in medical im-
age analysis, 2024.
[Fang et al. , 2023 ]Han Fang, Zhifei Yang, Yuhan Wei, Xi-
anghao Zang, Chao Ban, Zerun Feng, Zhongjiang He,
Yongxiang Li, and Hao Sun. Alignment and generation
adapter for efficient video-text understanding. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 2791–2797, 2023.
[Feng and Narayanan, 2023 ]Tiantian Feng and Shrikanth
Narayanan. Peft-ser: On the use of parameter efficient
transfer learning approaches for speech emotion recogni-
tion using pre-trained speech models. In 2023 11th Inter-
national Conference on Affective Computing and Intelli-
gent Interaction (ACII) . IEEE, September 2023.
[Feng et al. , 2023 ]Tiantian Feng, Rajat Hebbar, and
Shrikanth Narayanan. Trustser: On the trustworthinessof fine-tuning pre-trained speech embeddings for speech
emotion recognition, 2023.
[Hanet al. , 2024 ]Zeyu Han, Chao Gao, Jinyang Liu,
Sai Qian Zhang, et al. Parameter-efficient fine-tuning for
large models: A comprehensive survey. arXiv preprint
arXiv:2403.14608 , 2024.
[Heet al. , 2023 ]Xuehai He, Chunyuan Li, Pengchuan
Zhang, Jianwei Yang, and Xin Eric Wang. Parameter-
efficient model adaptation for vision transformers, 2023.
[Houlsby et al. , 2019 ]Neil Houlsby, Andrei Giurgiu, Stanis-
law Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
Parameter-efficient transfer learning for nlp, 2019.
[Huet al. , 2023 ]Zhiqiang Hu, Lei Wang, Yihuai Lan,
Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Sou-
janya Poria, and Roy Lee. LLM-adapters: An adapter fam-
ily for parameter-efficient fine-tuning of large language
models. In Houda Bouamor, Juan Pino, and Kalika Bali,
editors, Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing , pages 5254–
5276, Singapore, December 2023. Association for Com-
putational Linguistics.
[Jiaet al. , 2021 ]Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting
Chen, Zarana Parekh, Hieu Pham, Quoc V . Le, Yunhsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and
vision-language representation learning with noisy text su-
pervision, 2021.
[Kirillov et al. , 2023 ]Alexander Kirillov, Eric Mintun,
Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander C.
Berg, Wan-Yen Lo, Piotr Doll ´ar, and Ross Girshick.
Segment anything, 2023.
[Kirk et al. , 2024 ]Robert Kirk, Ishita Mediratta, Christo-
foros Nalmpantis, Jelena Luketina, Eric Hambro, Edward
Grefenstette, and Roberta Raileanu. Understanding the ef-
fects of RLHF on LLM generalisation and diversity. In
The Twelfth International Conference on Learning Repre-
sentations , 2024.
[Korbak et al. , 2022 ]Tomasz Korbak, Hady Elsahar, Ger-
man Kruszewski, and Marc Dymetman. Controlling con-
ditional language models without catastrophic forgetting.
InInternational Conference on Machine Learning , pages
11499–11528. PMLR, 2022.
[Liuet al. , 2022 ]Siqi Liu, Marc Lanctot, Luke Marris, and
Nicolas Heess. Simplex neural population learning: Any-
mixture bayes-optimality in symmetric zero-sum games,
2022.
[Liuet al. , 2023 ]Yuhang Liu, Tianhao Li, Zixuan Wang,
Guiquan Zhu, Yongqing Zhang, and Quan Zou. Explor-
ing parameter-efficient fine-tuning of a large-scale pre-
trained model for scrna-seq cell type annotation. In
2023 IEEE International Conference on Bioinformatics
and Biomedicine (BIBM) , pages 580–585, 2023.
[Liuet al. , 2024 ]Wei Liu, Ying Qin, Zhiyuan Peng, and Tan
Lee. Sparsely shared lora on whisper for child speech
recognition, 2024.

--- PAGE 8 ---
[Luet al. , 2022 ]Yujie Lu, Wanrong Zhu, Xin Eric Wang,
Miguel Eckstein, and William Yang Wang. Imagination-
augmented natural language understanding, 2022.
[Luet al. , 2023 ]Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, and
Chun Zuo. Llama-reviewer: Advancing code review au-
tomation with large language models through parameter-
efficient fine-tuning, 2023.
[Luoet al. , 2021 ]Huixiang Luo, Hao Cheng, Fanxu Meng,
Yuting Gao, Ke Li, Mengdan Zhang, and Xing Sun. An
empirical study and analysis on open-set semi-supervised
learning, 2021.
[Lvet al. , 2023 ]Kai Lv, Yuqing Yang, Tengxiao Liu,
Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full param-
eter fine-tuning for large language models with limited re-
sources, 2023.
[Mohammadi and Chapon, 2020 ]Samin Mohammadi and
Mathieu Chapon. Investigating the performance of fine-
tuned text classification models based-on bert. In 2020
IEEE 22nd International Conference on High Perfor-
mance Computing and Communications; IEEE 18th In-
ternational Conference on Smart City; IEEE 6th In-
ternational Conference on Data Science and Systems
(HPCC/SmartCity/DSS) , pages 1252–1257, 2020.
[Nassif et al. , 2019 ]Ali Bou Nassif, Ismail Shahin, Imtinan
Attili, Mohammad Azzeh, and Khaled Shaalan. Speech
recognition using deep neural networks: A systematic re-
view. IEEE access , 7:19143–19165, 2019.
[Naveed et al. , 2024 ]Humza Naveed, Asad Ullah Khan, Shi
Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Us-
man, Naveed Akhtar, Nick Barnes, and Ajmal Mian. A
comprehensive overview of large language models, 2024.
[Prabhavalkar et al. , 2023 ]Rohit Prabhavalkar, Takaaki
Hori, Tara N Sainath, Ralf Schl ¨uter, and Shinji Watanabe.
End-to-end speech recognition: A survey. IEEE/ACM
Transactions on Audio, Speech, and Language Processing ,
2023.
[Radford et al. , 2019 ]Alec Radford, Jeffrey Wu, Rewon
Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners.
OpenAI blog , 1(8):9, 2019.
[Radford et al. , 2021 ]Alec Radford, Jong Wook Kim, Chris
Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervi-
sion, 2021.
[Radford et al. , 2022 ]Alec Radford, Jong Wook Kim, Tao
Xu, Greg Brockman, Christine McLeavey, and Ilya
Sutskever. Robust speech recognition via large-scale weak
supervision, 2022.
[Raffel et al. , 2020 ]Colin Raffel, Noam Shazeer, Adam
Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the lim-
its of transfer learning with a unified text-to-text trans-
former. Journal of machine learning research , 21(140):1–
67, 2020.[Saied, 2016 ]Amin Saied. On the fi-module structure of
hi(γn,s), 2016.
[Sanjeev et al. , 2021 ]Santosh Sanjeev, Charith Chandra Sai
Balne, Tudi Jayadeep Reddy, and G.Pradeep Reddy. Deep
learning-based mixed data approach for covid-19 detec-
tion. In 2021 IEEE 18th India Council International Con-
ference (INDICON) , pages 1–6, 2021.
[Schneider et al. , 2019 ]Steffen Schneider, Alexei Baevski,
Ronan Collobert, and Michael Auli. wav2vec: Unsuper-
vised pre-training for speech recognition, 2019.
[Sledzieski et al. , 2023 ]Samuel Sledzieski, Meghana Kshir-
sagar, Minkyung Baek, Bonnie Berger, Rahul Dodhia, and
Juan Lavista Ferres. Democratizing protein language mod-
els with parameter-efficient fine-tuning. bioRxiv , 2023.
[Tang et al. , 2023 ]Yiwen Tang, Ray Zhang, Zoey Guo, Xi-
anzheng Ma, Dong Wang, Zhigang Wang, Bin Zhao, and
Xuelong Li. Point-peft: Parameter-efficient fine-tuning for
3d pre-trained models, 2023.
[Tang et al. , 2024 ]Yiwen Tang, Ray Zhang, Zoey Guo,
Dong Wang, Zhigang Wang, Bin Zhao, and Xuelong
Li. Point-peft: Parameter-efficient fine-tuning for 3d pre-
trained models, 2024.
[Uyet al. , 2019 ]Mikaela Angelina Uy, Quang-Hieu Pham,
Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung.
Revisiting point cloud classification: A new benchmark
dataset and classification model on real-world data, 2019.
[van der Marel et al. , 2022 ]Nienke van der Marel,
Jonathan P. Williams, Giovanni Picogna, Sierk van
Terwisga, Stefano Facchini, Carlo F. Manara, Apostolos
Zormpas, Megan Ansdell, and . High-resolution alma
observations of transition disk candidates in lupus, 2022.
[Wang et al. , 2023 ]Binrui Wang, Yongping Du, Xingnan
Jin, Rui Yan, and Qi Zhang. Low-resource efficient multi-
stage tuning strategy for biomedical question answering
task. In 2023 IEEE International Conference on Bioinfor-
matics and Biomedicine (BIBM) , pages 2281–2284, 2023.
[Wanli and Guoxin, 2013 ]Zhang Wanli and Li Guoxin. The
research of feature extraction based on mfcc for speaker
recognition. In Proceedings of 2013 3rd International
Conference on Computer Science and Network Technol-
ogy, pages 1074–1077, 2013.
[Wuet al. , 2024 ]Zhengxuan Wu, Aryaman Arora, Zheng
Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Man-
ning, and Christopher Potts. Reft: Representation finetun-
ing for language models, 2024.
[Yanet al. , 2022 ]An Yan, Jiacheng Li, Wanrong Zhu, Yujie
Lu, William Yang Wang, and Julian McAuley. Clip also
understands text: Prompting clip for phrase understanding,
2022.
[Yaoet al. , 2021 ]Lewei Yao, Runhui Huang, Lu Hou, Guan-
song Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo
Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained inter-
active language-image pre-training, 2021.

--- PAGE 9 ---
[Yuet al. , 2022 ]Xumin Yu, Lulu Tang, Yongming Rao,
Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-
training 3d point cloud transformers with masked point
modeling, 2022.
[Yuan et al. , 2021 ]Lu Yuan, Dongdong Chen, Yi-Ling
Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong
Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu,
Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Li-
juan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jian-
wei Yang, Michael Zeng, Luowei Zhou, and Pengchuan
Zhang. Florence: A new foundation model for computer
vision, 2021.
[Zaken et al. , 2022 ]Elad Ben Zaken, Shauli Ravfogel, and
Yoav Goldberg. Bitfit: Simple parameter-efficient fine-
tuning for transformer-based masked language-models,
2022.
[Zhang et al. , 2022 ]Renrui Zhang, Ziyu Guo, Rongyao
Fang, Bin Zhao, Dong Wang, Yu Qiao, Hongsheng Li, and
Peng Gao. Point-m2ae: Multi-scale masked autoencoders
for hierarchical point cloud pre-training, 2022.
[Zhou et al. , 2021 ]Youjia Zhou, Archit Rathore, Emilie
Purvine, and Bei Wang. Topological simplifications of hy-
pergraphs, 2021.
