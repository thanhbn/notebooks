{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIPO: Free-form Instruction-oriented Prompt Optimization - Main Implementation\n",
    "\n",
    "## Paper Information\n",
    "- **Title**: FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema\n",
    "- **Authors**: Junru Lu, Siyu An, Min Zhang, Yulan He, Di Yin, Xing Sun\n",
    "- **Link**: [arXiv:2402.11811v4](https://arxiv.org/abs/2402.11811)\n",
    "- **Abstract**: Paper giá»›i thiá»‡u FIPO - phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a prompt tá»± Ä‘á»™ng dá»±a trÃªn preference learning vÃ  fine-tuning local LLM, giáº£i quyáº¿t cÃ¡c háº¡n cháº¿ vá» privacy vÃ  generalization cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p APO truyá»n thá»‘ng.\n",
    "\n",
    "## Key Contributions\n",
    "1. **Local End-to-End Optimization**: KhÃ´ng phá»¥ thuá»™c vÃ o API LLMs bÃªn ngoÃ i\n",
    "2. **Prompt Optimization Preference (POP) Dataset**: 30,000 máº«u preference data\n",
    "3. **Multiple Fine-tuning Strategies**: SFT, DPO, IPO, vÃ  IPL (Iterative Preference Learning)\n",
    "4. **Model-agnostic Approach**: Hoáº¡t Ä‘á»™ng vá»›i báº¥t ká»³ downstream generator nÃ o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Installation\n",
    "\n",
    "CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho FIPO implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "!pip install -q torch transformers datasets\n",
    "!pip install -q langchain langchain-openai langchain-community\n",
    "!pip install -q deepeval ragas  # For evaluation\n",
    "!pip install -q trl peft  # For preference optimization\n",
    "!pip install -q accelerate bitsandbytes\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "!pip install -q wandb  # Optional: for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import DPOTrainer, SFTTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FIPO Core Components\n",
    "\n",
    "### 2.1 Meta-Template for Universal APO\n",
    "\n",
    "FIPO sá»­ dá»¥ng má»™t meta-template linh hoáº¡t cho phÃ©p tá»‘i Æ°u hÃ³a prompt mÃ  khÃ´ng cáº§n phá»¥ thuá»™c vÃ o testing generator cá»¥ thá»ƒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FIPOPromptOptimization:\n",
    "    \"\"\"Class Ä‘á»ƒ quáº£n lÃ½ FIPO prompt optimization\"\"\"\n",
    "    naive_prompt: str\n",
    "    naive_response: Optional[str] = None\n",
    "    ground_truth: Optional[str] = None\n",
    "    optimized_prompt: Optional[str] = None\n",
    "    \n",
    "class FIPOMetaTemplate:\n",
    "    \"\"\"Meta-template for FIPO optimization (Section 2.2)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_optimization_prompt(\n",
    "        naive_prompt: str,\n",
    "        naive_response: Optional[str] = None,\n",
    "        ground_truth: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Create FIPO optimization prompt using meta-template\"\"\"\n",
    "        \n",
    "        template = \"\"\"You are an expert of prompt optimization.\n",
    "\n",
    "```\n",
    "Silver Prompt:\n",
    "{naive_prompt}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Add optional naive response\n",
    "        if naive_response:\n",
    "            template += \"\"\"\n",
    "```\n",
    "Silver Response:\n",
    "{naive_response}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Add optional ground truth\n",
    "        if ground_truth:\n",
    "            template += \"\"\"\n",
    "```\n",
    "Golden Response:\n",
    "{ground_truth}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        template += \"\"\"\n",
    "```\n",
    "Task Introduction:\n",
    "Based on the Silver Prompt, optional Silver Response and optional Golden Response, perform the following actions:\n",
    "\n",
    "1 - The optional Silver Response was generated by an AI based on the Silver Prompt. Please help modify the Silver Prompt to Golden Prompt that can obtain a more correct response, in reference to the optional Golden Response.\n",
    "\n",
    "2 - When building the Golden Prompt, you can consider several aspects:\n",
    "   (1) A roleplay leading sentence to adapt the AI to the task-specific scenario\n",
    "   (2) Details of task characteristics\n",
    "   (3) Further clarification of ambiguous terms\n",
    "   (4) More detailed solution guidance (step-by-step plans, exception handling, etc.)\n",
    "   (5) Specific requirements for the response (length, format, style, tone, language, etc.)\n",
    "\n",
    "3 - Show me only the Golden Prompt, do not contain any other content.\n",
    "```\n",
    "\n",
    "Golden Prompt:\"\"\"\n",
    "        \n",
    "        return template.format(\n",
    "            naive_prompt=naive_prompt,\n",
    "            naive_response=naive_response if naive_response else \"\",\n",
    "            ground_truth=ground_truth if ground_truth else \"\"\n",
    "        )\n",
    "\n",
    "# Test meta-template\n",
    "meta_template = FIPOMetaTemplate()\n",
    "example_prompt = meta_template.create_optimization_prompt(\n",
    "    naive_prompt=\"Calculate the average value of the list\",\n",
    "    naive_response=\"The answer value is 44.1\",\n",
    "    ground_truth=\"The answer value is 44.25\"\n",
    ")\n",
    "print(\"Example FIPO Meta-template:\")\n",
    "print(example_prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Diversification Strategy\n",
    "\n",
    "FIPO sá»­ dá»¥ng 8 loáº¡i format khÃ¡c nhau Ä‘á»ƒ giáº£m exposure gap giá»¯a training vÃ  testing (Section 2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDiversifier:\n",
    "    \"\"\"Diversify dataset into 8 different formats (Figure 3)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.format_types = [\n",
    "            \"generation_with_both\",      # Type 1: Generation vá»›i cáº£ naive response vÃ  ground truth\n",
    "            \"generation_with_naive\",     # Type 2: Generation chá»‰ vá»›i naive response\n",
    "            \"generation_no_response\",    # Type 3: Generation khÃ´ng cÃ³ response\n",
    "            \"generation_with_truth\",     # Type 4: Generation chá»‰ vá»›i ground truth\n",
    "            \"multichoice_with_both\",     # Type 5: Multi-choice vá»›i cáº£ hai\n",
    "            \"multichoice_with_naive\",    # Type 6: Multi-choice vá»›i naive response\n",
    "            \"multichoice_no_response\",   # Type 7: Multi-choice khÃ´ng cÃ³ response\n",
    "            \"multichoice_with_truth\"     # Type 8: Multi-choice vá»›i ground truth\n",
    "        ]\n",
    "    \n",
    "    def diversify_sample(self, sample: FIPOPromptOptimization, format_type: str) -> Dict:\n",
    "        \"\"\"Diversify a single sample based on format type\"\"\"\n",
    "        \n",
    "        if \"multichoice\" in format_type:\n",
    "            # Convert to multi-choice format\n",
    "            sample = self._convert_to_multichoice(sample)\n",
    "        \n",
    "        # Apply response filtering based on type\n",
    "        if \"no_response\" in format_type:\n",
    "            sample.naive_response = None\n",
    "            sample.ground_truth = None\n",
    "        elif \"with_naive\" in format_type:\n",
    "            sample.ground_truth = None\n",
    "        elif \"with_truth\" in format_type:\n",
    "            sample.naive_response = None\n",
    "        \n",
    "        return {\n",
    "            \"naive_prompt\": sample.naive_prompt,\n",
    "            \"naive_response\": sample.naive_response,\n",
    "            \"ground_truth\": sample.ground_truth,\n",
    "            \"format_type\": format_type\n",
    "        }\n",
    "    \n",
    "    def _convert_to_multichoice(self, sample: FIPOPromptOptimization) -> FIPOPromptOptimization:\n",
    "        \"\"\"Convert generation format to multi-choice\"\"\"\n",
    "        # Simulate multi-choice conversion\n",
    "        if sample.naive_response and sample.ground_truth:\n",
    "            sample.naive_prompt += \"\\nA. \" + sample.naive_response + \"\\nB. \" + sample.ground_truth\n",
    "            sample.naive_response = \"A\"\n",
    "            sample.ground_truth = \"B\"\n",
    "        return sample\n",
    "\n",
    "# Example diversification\n",
    "diversifier = DatasetDiversifier()\n",
    "example_sample = FIPOPromptOptimization(\n",
    "    naive_prompt=\"What is 2+2?\",\n",
    "    naive_response=\"The answer is 5\",\n",
    "    ground_truth=\"The answer is 4\"\n",
    ")\n",
    "\n",
    "diversified = diversifier.diversify_sample(example_sample, \"multichoice_with_both\")\n",
    "print(\"Diversified sample:\")\n",
    "print(json.dumps(diversified, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preference Dataset Construction\n",
    "\n",
    "### 3.1 Simulating POP Dataset Creation\n",
    "\n",
    "Trong thá»±c táº¿, FIPO sá»­ dá»¥ng GPT-3.5-turbo vÃ  GPT-4 Ä‘á»ƒ táº¡o 30k máº«u preference data. á»ž Ä‘Ã¢y, chÃºng ta sáº½ simulate quÃ¡ trÃ¬nh nÃ y vá»›i LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POPDatasetBuilder:\n",
    "    \"\"\"Build Prompt Optimization Preference (POP) dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, use_langchain: bool = True):\n",
    "        self.use_langchain = use_langchain\n",
    "        if use_langchain:\n",
    "            # Initialize LangChain models for demonstration\n",
    "            self.suboptimal_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "            self.optimal_model = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "        self.meta_template = FIPOMetaTemplate()\n",
    "    \n",
    "    def create_preference_pair(\n",
    "        self,\n",
    "        naive_prompt: str,\n",
    "        naive_response: str,\n",
    "        ground_truth: str\n",
    "    ) -> Tuple[str, str]:\n",
    "        \"\"\"Create preference pair: (rejected, chosen) prompts\"\"\"\n",
    "        \n",
    "        optimization_prompt = self.meta_template.create_optimization_prompt(\n",
    "            naive_prompt, naive_response, ground_truth\n",
    "        )\n",
    "        \n",
    "        if self.use_langchain:\n",
    "            # Get suboptimal optimization (rejected)\n",
    "            rejected_prompt = self.suboptimal_model.invoke(optimization_prompt).content\n",
    "            \n",
    "            # Get optimal optimization (chosen)\n",
    "            chosen_prompt = self.optimal_model.invoke(optimization_prompt).content\n",
    "        else:\n",
    "            # Simulate for demonstration\n",
    "            rejected_prompt = f\"Please {naive_prompt} carefully.\"\n",
    "            chosen_prompt = f\"Step 1: Understand the task - {naive_prompt}\\n\" + \\\n",
    "                           f\"Step 2: Apply systematic approach\\n\" + \\\n",
    "                           f\"Step 3: Verify your answer matches expected format\"\n",
    "        \n",
    "        return rejected_prompt, chosen_prompt\n",
    "    \n",
    "    def build_dataset(self, samples: List[Dict], use_langchain: bool = False) -> Dataset:\n",
    "        \"\"\"Build preference dataset from samples\"\"\"\n",
    "        preference_data = []\n",
    "        \n",
    "        for sample in tqdm(samples, desc=\"Building preference dataset\"):\n",
    "            if use_langchain and self.use_langchain:\n",
    "                rejected, chosen = self.create_preference_pair(\n",
    "                    sample[\"naive_prompt\"],\n",
    "                    sample[\"naive_response\"],\n",
    "                    sample[\"ground_truth\"]\n",
    "                )\n",
    "            else:\n",
    "                # Use pre-generated for demonstration\n",
    "                rejected = sample.get(\"rejected_prompt\", f\"Please {sample['naive_prompt']}\")\n",
    "                chosen = sample.get(\"chosen_prompt\", f\"Carefully {sample['naive_prompt']} step by step\")\n",
    "            \n",
    "            preference_data.append({\n",
    "                \"naive_prompt\": sample[\"naive_prompt\"],\n",
    "                \"naive_response\": sample.get(\"naive_response\"),\n",
    "                \"ground_truth\": sample.get(\"ground_truth\"),\n",
    "                \"rejected\": rejected,\n",
    "                \"chosen\": chosen\n",
    "            })\n",
    "        \n",
    "        return Dataset.from_list(preference_data)\n",
    "\n",
    "# Create sample dataset\n",
    "sample_data = [\n",
    "    {\n",
    "        \"naive_prompt\": \"Calculate the average of [12, 34, 56, 75]\",\n",
    "        \"naive_response\": \"The average is 44.1\",\n",
    "        \"ground_truth\": \"The average is 44.25\",\n",
    "        \"rejected_prompt\": \"Find the average of the given numbers\",\n",
    "        \"chosen_prompt\": \"To find the average: 1) Add all numbers: 12+34+56+75=177, 2) Divide by count: 177/4=44.25\"\n",
    "    },\n",
    "    {\n",
    "        \"naive_prompt\": \"What is the capital of France?\",\n",
    "        \"naive_response\": \"Paris is a city in France\",\n",
    "        \"ground_truth\": \"The capital of France is Paris\",\n",
    "        \"rejected_prompt\": \"Tell me about France's capital\",\n",
    "        \"chosen_prompt\": \"Identify the capital city of France. Provide a direct answer in the format: 'The capital of France is [city name]'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "pop_builder = POPDatasetBuilder(use_langchain=False)\n",
    "preference_dataset = pop_builder.build_dataset(sample_data)\n",
    "print(f\"Created preference dataset with {len(preference_dataset)} samples\")\n",
    "print(\"\\nSample entry:\")\n",
    "print(preference_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Quality Validation\n",
    "\n",
    "FIPO validate cháº¥t lÆ°á»£ng dataset báº±ng 3 phÆ°Æ¡ng phÃ¡p: UltraRM, GPT-4 self-check, vÃ  Human expert (Table 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetValidator:\n",
    "    \"\"\"Validate preference dataset quality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_methods = [\n",
    "            \"ultraRM\",      # External alignment model\n",
    "            \"gpt4_self\",    # GPT-4 self-judgement  \n",
    "            \"human_expert\"  # Manual checking\n",
    "        ]\n",
    "    \n",
    "    def validate_preference_pair(self, rejected: str, chosen: str, method: str = \"simulated\") -> float:\n",
    "        \"\"\"Validate if chosen > rejected\"\"\"\n",
    "        \n",
    "        if method == \"simulated\":\n",
    "            # Simulate validation based on length and structure\n",
    "            chosen_score = len(chosen.split()) + chosen.count(\"Step\") * 10\n",
    "            rejected_score = len(rejected.split()) + rejected.count(\"Step\") * 10\n",
    "            return 1.0 if chosen_score > rejected_score else 0.0\n",
    "        \n",
    "        # Real validation would use actual models\n",
    "        return 0.85  # Placeholder\n",
    "    \n",
    "    def validate_dataset(self, dataset: Dataset, sample_size: int = 100) -> Dict[str, float]:\n",
    "        \"\"\"Validate entire dataset and return win rates\"\"\"\n",
    "        \n",
    "        results = {\"response_win_rate\": [], \"prompt_win_rate\": []}\n",
    "        \n",
    "        # Sample validation\n",
    "        indices = np.random.choice(len(dataset), min(sample_size, len(dataset)), replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            sample = dataset[int(idx)]\n",
    "            \n",
    "            # Validate prompt optimization\n",
    "            prompt_win = self.validate_preference_pair(\n",
    "                sample[\"rejected\"], \n",
    "                sample[\"chosen\"],\n",
    "                method=\"simulated\"\n",
    "            )\n",
    "            results[\"prompt_win_rate\"].append(prompt_win)\n",
    "            \n",
    "            # Simulate response validation\n",
    "            results[\"response_win_rate\"].append(0.87)  # Placeholder\n",
    "        \n",
    "        return {\n",
    "            \"prompt_win_rate\": np.mean(results[\"prompt_win_rate\"]),\n",
    "            \"response_win_rate\": np.mean(results[\"response_win_rate\"]),\n",
    "            \"average_win_rate\": np.mean([np.mean(results[\"prompt_win_rate\"]), \n",
    "                                         np.mean(results[\"response_win_rate\"])])\n",
    "        }\n",
    "\n",
    "# Validate dataset\n",
    "validator = DatasetValidator()\n",
    "validation_results = validator.validate_dataset(preference_dataset)\n",
    "\n",
    "print(\"Dataset Validation Results:\")\n",
    "print(f\"Prompt Win Rate: {validation_results['prompt_win_rate']:.2%}\")\n",
    "print(f\"Response Win Rate: {validation_results['response_win_rate']:.2%}\")\n",
    "print(f\"Average Win Rate: {validation_results['average_win_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning Strategies Implementation\n",
    "\n",
    "### 4.1 Supervised Fine-tuning (SFT)\n",
    "\n",
    "SFT chá»‰ sá»­ dá»¥ng optimal prompt lÃ m supervision signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIPOSFTTrainer:\n",
    "    \"\"\"Supervised Fine-tuning for FIPO\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/phi-2\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def prepare_sft_dataset(self, preference_dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Prepare dataset for SFT - only use chosen prompts\"\"\"\n",
    "        \n",
    "        def format_sft_example(example):\n",
    "            # Create input-output pairs for SFT\n",
    "            input_text = FIPOMetaTemplate.create_optimization_prompt(\n",
    "                example[\"naive_prompt\"],\n",
    "                example.get(\"naive_response\"),\n",
    "                example.get(\"ground_truth\")\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"input\": input_text,\n",
    "                \"output\": example[\"chosen\"],\n",
    "                \"text\": f\"{input_text}\\n\\n{example['chosen']}\"\n",
    "            }\n",
    "        \n",
    "        return preference_dataset.map(format_sft_example)\n",
    "    \n",
    "    def compute_sft_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute SFT loss (Equation 8)\"\"\"\n",
    "        # L_SFT = -E[(xÌ‚o - xo+)Â²]\n",
    "        return torch.nn.functional.mse_loss(predictions, targets)\n",
    "    \n",
    "    def train(self, dataset: Dataset, output_dir: str = \"./fipo_sft_model\"):\n",
    "        \"\"\"Train SFT model (demonstration only)\"\"\"\n",
    "        print(f\"SFT Training would proceed with {len(dataset)} samples\")\n",
    "        print(f\"Model: {self.model_name}\")\n",
    "        print(f\"Output: {output_dir}\")\n",
    "        \n",
    "        # In real implementation, would use TRL's SFTTrainer\n",
    "        return f\"SFT model trained at {output_dir}\"\n",
    "\n",
    "# Prepare SFT training\n",
    "sft_trainer = FIPOSFTTrainer()\n",
    "sft_dataset = sft_trainer.prepare_sft_dataset(preference_dataset)\n",
    "print(\"SFT Dataset sample:\")\n",
    "print(sft_dataset[0][\"text\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Direct Preference Optimization (DPO)\n",
    "\n",
    "DPO sá»­ dá»¥ng cáº£ rejected vÃ  chosen prompts Ä‘á»ƒ há»c preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIPODPOTrainer:\n",
    "    \"\"\"Direct Preference Optimization for FIPO\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/phi-2\", beta: float = 0.01):\n",
    "        self.model_name = model_name\n",
    "        self.beta = beta  # Hyperparameter factor\n",
    "    \n",
    "    def prepare_dpo_dataset(self, preference_dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Prepare dataset for DPO training\"\"\"\n",
    "        \n",
    "        def format_dpo_example(example):\n",
    "            prompt = FIPOMetaTemplate.create_optimization_prompt(\n",
    "                example[\"naive_prompt\"],\n",
    "                example.get(\"naive_response\"),\n",
    "                example.get(\"ground_truth\")\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": example[\"chosen\"],\n",
    "                \"rejected\": example[\"rejected\"]\n",
    "            }\n",
    "        \n",
    "        return preference_dataset.map(format_dpo_example)\n",
    "    \n",
    "    def compute_dpo_loss(self, chosen_logps: torch.Tensor, rejected_logps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute DPO loss (Equation 9)\"\"\"\n",
    "        # L_DPO = -E[log Ïƒ(Î²Â·Î”)]\n",
    "        # where Î” = log p(chosen) - log p(rejected)\n",
    "        \n",
    "        delta = chosen_logps - rejected_logps\n",
    "        loss = -torch.nn.functional.logsigmoid(self.beta * delta).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, dataset: Dataset, output_dir: str = \"./fipo_dpo_model\"):\n",
    "        \"\"\"Train DPO model\"\"\"\n",
    "        print(f\"DPO Training Configuration:\")\n",
    "        print(f\"- Model: {self.model_name}\")\n",
    "        print(f\"- Beta: {self.beta}\")\n",
    "        print(f\"- Dataset size: {len(dataset)}\")\n",
    "        print(f\"- Output: {output_dir}\")\n",
    "        \n",
    "        # Simulate loss computation\n",
    "        chosen_logps = torch.randn(4)  # Batch of 4\n",
    "        rejected_logps = torch.randn(4)\n",
    "        loss = self.compute_dpo_loss(chosen_logps, rejected_logps)\n",
    "        print(f\"\\nSample DPO loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return f\"DPO model trained at {output_dir}\"\n",
    "\n",
    "# Prepare DPO training  \n",
    "dpo_trainer = FIPODPOTrainer(beta=0.01)\n",
    "dpo_dataset = dpo_trainer.prepare_dpo_dataset(preference_dataset)\n",
    "print(\"DPO Dataset sample:\")\n",
    "print(f\"Prompt: {dpo_dataset[0]['prompt'][:100]}...\")\n",
    "print(f\"Chosen: {dpo_dataset[0]['chosen']}\")\n",
    "print(f\"Rejected: {dpo_dataset[0]['rejected']}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "dpo_trainer.train(dpo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Identity Preference Optimization (IPO)\n",
    "\n",
    "IPO lÃ  phiÃªn báº£n regularized cá»§a DPO vá»›i squared loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIPOIPOTrainer:\n",
    "    \"\"\"Identity Preference Optimization for FIPO\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/phi-2\", beta: float = 0.01):\n",
    "        self.model_name = model_name\n",
    "        self.beta = beta\n",
    "    \n",
    "    def compute_ipo_loss(self, chosen_logps: torch.Tensor, rejected_logps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute IPO loss (Equation 10)\"\"\"\n",
    "        # L_IPO = -E[(Î” - 1/2Î²)Â²]\n",
    "        # where Î” = log p(chosen) - log p(rejected)\n",
    "        \n",
    "        delta = chosen_logps - rejected_logps\n",
    "        loss = ((delta - 1/(2*self.beta)) ** 2).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compare_with_dpo(self, chosen_logps: torch.Tensor, rejected_logps: torch.Tensor):\n",
    "        \"\"\"Compare IPO vs DPO loss\"\"\"\n",
    "        \n",
    "        # Compute both losses\n",
    "        ipo_loss = self.compute_ipo_loss(chosen_logps, rejected_logps)\n",
    "        \n",
    "        # DPO loss for comparison\n",
    "        delta = chosen_logps - rejected_logps\n",
    "        dpo_loss = -torch.nn.functional.logsigmoid(self.beta * delta).mean()\n",
    "        \n",
    "        print(f\"Loss Comparison (beta={self.beta}):\")\n",
    "        print(f\"IPO Loss: {ipo_loss.item():.4f}\")\n",
    "        print(f\"DPO Loss: {dpo_loss.item():.4f}\")\n",
    "        print(f\"Difference: {abs(ipo_loss.item() - dpo_loss.item()):.4f}\")\n",
    "        \n",
    "        return ipo_loss, dpo_loss\n",
    "\n",
    "# Test IPO trainer\n",
    "ipo_trainer = FIPOIPOTrainer(beta=0.01)\n",
    "\n",
    "# Simulate logprobs\n",
    "chosen_logps = torch.tensor([-1.2, -0.8, -1.5, -0.9])\n",
    "rejected_logps = torch.tensor([-2.1, -1.9, -2.3, -1.7])\n",
    "\n",
    "ipo_trainer.compare_with_dpo(chosen_logps, rejected_logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Iterative Preference Learning (IPL)\n",
    "\n",
    "IPL lÃ  phÆ°Æ¡ng phÃ¡p self-rewarding má»›i cá»§a FIPO, cho phÃ©p model tá»± cáº£i thiá»‡n qua nhiá»u iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIPOIPLTrainer:\n",
    "    \"\"\"Iterative Preference Learning for FIPO (Algorithm 1)\"\"\"\n",
    "    \n",
    "    def __init__(self, base_trainer=\"IPO\", iterations: int = 3):\n",
    "        self.base_trainer = base_trainer\n",
    "        self.iterations = iterations\n",
    "        self.meta_template = FIPOMetaTemplate()\n",
    "    \n",
    "    def self_rewarding_update(\n",
    "        self, \n",
    "        naive_prompt: str,\n",
    "        naive_response: str,\n",
    "        ground_truth: str,\n",
    "        current_optimizer\n",
    "    ) -> Tuple[str, str]:\n",
    "        \"\"\"Self-rewarding update (Equations 13-14)\"\"\"\n",
    "        \n",
    "        # Generate new prompt x_n+ using optimizer\n",
    "        optimization_request = self.meta_template.create_optimization_prompt(\n",
    "            naive_prompt, naive_response, ground_truth\n",
    "        )\n",
    "        \n",
    "        # Simulate optimizer output\n",
    "        new_prompt = f\"[Iteration improved] {naive_prompt} with step-by-step guidance\"\n",
    "        \n",
    "        # Judge if new prompt is better\n",
    "        is_better = self._judge_prompts(\n",
    "            naive_prompt, new_prompt, ground_truth\n",
    "        )\n",
    "        \n",
    "        if is_better:\n",
    "            # Generate new response with new prompt\n",
    "            new_response = f\"[Better response from improved prompt]\"\n",
    "            return new_prompt, new_response\n",
    "        else:\n",
    "            return naive_prompt, naive_response\n",
    "    \n",
    "    def _judge_prompts(self, prompt1: str, prompt2: str, ground_truth: str) -> bool:\n",
    "        \"\"\"Judge which prompt is better (discrimination task)\"\"\"\n",
    "        # Simulate judgement - in reality would use trained discriminator\n",
    "        return len(prompt2) > len(prompt1)  # Simple heuristic\n",
    "    \n",
    "    def iterative_training_loop(self, dataset: Dataset):\n",
    "        \"\"\"IPL training loop (Algorithm 1)\"\"\"\n",
    "        \n",
    "        print(f\"Starting IPL with {self.iterations} iterations\\n\")\n",
    "        \n",
    "        for iteration in range(self.iterations):\n",
    "            print(f\"=== Iteration {iteration + 1}/{self.iterations} ===\")\n",
    "            \n",
    "            if iteration > 0:  # After warm-up\n",
    "                # Self-rewarding updates\n",
    "                updated_samples = 0\n",
    "                \n",
    "                for idx in range(min(2, len(dataset))):  # Demo with 2 samples\n",
    "                    sample = dataset[idx]\n",
    "                    \n",
    "                    new_prompt, new_response = self.self_rewarding_update(\n",
    "                        sample[\"naive_prompt\"],\n",
    "                        sample.get(\"naive_response\", \"\"),\n",
    "                        sample.get(\"ground_truth\", \"\"),\n",
    "                        current_optimizer=None  # Placeholder\n",
    "                    )\n",
    "                    \n",
    "                    if new_prompt != sample[\"naive_prompt\"]:\n",
    "                        updated_samples += 1\n",
    "                        print(f\"  Sample {idx}: Updated prompt\")\n",
    "                \n",
    "                print(f\"Updated {updated_samples} samples via self-rewarding\")\n",
    "            \n",
    "            # Train with base method (IPO or DPO)\n",
    "            print(f\"Training with {self.base_trainer}...\")\n",
    "            print(f\"Validation accuracy: {85 + iteration * 2}%\\n\")  # Simulate improvement\n",
    "        \n",
    "        print(\"IPL Training completed!\")\n",
    "\n",
    "# Run IPL training simulation\n",
    "ipl_trainer = FIPOIPLTrainer(base_trainer=\"IPO\", iterations=3)\n",
    "ipl_trainer.iterative_training_loop(preference_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Framework\n",
    "\n",
    "### 5.1 Benchmark Evaluation\n",
    "\n",
    "FIPO Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn 5 benchmarks: GSM8K, BBH, PiQA, CosmosQA, vÃ  MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIPOEvaluator:\n",
    "    \"\"\"Evaluate FIPO on downstream tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.benchmarks = {\n",
    "            \"GSM8K\": {\"type\": \"generation\", \"samples\": 1300},\n",
    "            \"BBH\": {\"type\": \"generation\", \"samples\": 6400},\n",
    "            \"PiQA\": {\"type\": \"multichoice\", \"samples\": 1800},\n",
    "            \"CosmosQA\": {\"type\": \"multichoice\", \"samples\": 3000},\n",
    "            \"MMLU\": {\"type\": \"multichoice\", \"samples\": 14000}\n",
    "        }\n",
    "    \n",
    "    def evaluate_prompt_optimization(\n",
    "        self,\n",
    "        naive_prompt: str,\n",
    "        optimized_prompt: str,\n",
    "        benchmark: str,\n",
    "        generator_model: str = \"Tulu2-7B\"\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate prompt optimization effectiveness\"\"\"\n",
    "        \n",
    "        # Simulate evaluation\n",
    "        naive_score = np.random.uniform(0.3, 0.6)\n",
    "        \n",
    "        # Optimized prompts generally perform better\n",
    "        improvement = np.random.uniform(0.05, 0.15)\n",
    "        optimized_score = min(naive_score + improvement, 0.95)\n",
    "        \n",
    "        return {\n",
    "            \"benchmark\": benchmark,\n",
    "            \"generator\": generator_model,\n",
    "            \"naive_score\": naive_score,\n",
    "            \"optimized_score\": optimized_score,\n",
    "            \"improvement\": optimized_score - naive_score,\n",
    "            \"relative_improvement\": (optimized_score - naive_score) / naive_score * 100\n",
    "        }\n",
    "    \n",
    "    def run_full_evaluation(self, test_prompts: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Run evaluation across all benchmarks\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        generators = [\"Llama2-7B\", \"Tulu2-13B\", \"Baichuan2-13B\"]\n",
    "        \n",
    "        for benchmark in self.benchmarks:\n",
    "            for generator in generators:\n",
    "                # Simulate evaluation on sample prompts\n",
    "                benchmark_results = []\n",
    "                \n",
    "                for prompt_pair in test_prompts[:2]:  # Use 2 samples for demo\n",
    "                    result = self.evaluate_prompt_optimization(\n",
    "                        prompt_pair[\"naive\"],\n",
    "                        prompt_pair[\"optimized\"],\n",
    "                        benchmark,\n",
    "                        generator\n",
    "                    )\n",
    "                    benchmark_results.append(result)\n",
    "                \n",
    "                # Aggregate results\n",
    "                avg_result = {\n",
    "                    \"benchmark\": benchmark,\n",
    "                    \"generator\": generator,\n",
    "                    \"naive_score\": np.mean([r[\"naive_score\"] for r in benchmark_results]),\n",
    "                    \"optimized_score\": np.mean([r[\"optimized_score\"] for r in benchmark_results]),\n",
    "                    \"avg_improvement\": np.mean([r[\"improvement\"] for r in benchmark_results])\n",
    "                }\n",
    "                results.append(avg_result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = FIPOEvaluator()\n",
    "\n",
    "# Sample test prompts\n",
    "test_prompts = [\n",
    "    {\n",
    "        \"naive\": \"Calculate the average\",\n",
    "        \"optimized\": \"To calculate the average: 1) Sum all values, 2) Divide by count\"\n",
    "    },\n",
    "    {\n",
    "        \"naive\": \"What is the capital?\",\n",
    "        \"optimized\": \"Identify the capital city. Format: 'The capital is [city]'\"\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluation_df = evaluator.run_full_evaluation(test_prompts)\n",
    "print(\"Evaluation Results Summary:\")\n",
    "print(evaluation_df.groupby('generator')[['naive_score', 'optimized_score', 'avg_improvement']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Comparison with Baselines\n",
    "\n",
    "So sÃ¡nh FIPO vá»›i APE vÃ  PromptAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_optimization_methods():\n",
    "    \"\"\"Compare FIPO with baseline methods\"\"\"\n",
    "    \n",
    "    # Results from Table 2 and Figure 4\n",
    "    comparison_data = {\n",
    "        \"Method\": [\"Naive\", \"APE\", \"PromptAgent\", \"GPT-4\", \"FIPO\"],\n",
    "        \"Llama2-7B\": [46.8, 49.2, 54.1, 53.1, 56.7],\n",
    "        \"GPT-3.5\": [51.3, 68.1, 79.0, 68.0, 73.2],\n",
    "        \"GPT-4\": [76.2, 79.7, 82.0, 81.3, 84.4],\n",
    "        \"Avg_Improvement\": [0, 8.7, 19.8, 15.2, 22.1]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Performance comparison\n",
    "    methods = comparison_df[\"Method\"]\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax1.bar(x - width, comparison_df[\"Llama2-7B\"], width, label=\"Llama2-7B\")\n",
    "    ax1.bar(x, comparison_df[\"GPT-3.5\"], width, label=\"GPT-3.5\")\n",
    "    ax1.bar(x + width, comparison_df[\"GPT-4\"], width, label=\"GPT-4\")\n",
    "    \n",
    "    ax1.set_xlabel(\"Methods\")\n",
    "    ax1.set_ylabel(\"Performance (%)\")\n",
    "    ax1.set_title(\"Performance Comparison Across Models\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(methods, rotation=45)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Average improvement\n",
    "    colors = ['gray', 'blue', 'green', 'orange', 'red']\n",
    "    ax2.bar(methods[1:], comparison_df[\"Avg_Improvement\"][1:], color=colors[1:])\n",
    "    ax2.set_xlabel(\"Optimization Methods\")\n",
    "    ax2.set_ylabel(\"Average Improvement (%)\")\n",
    "    ax2.set_title(\"Average Improvement over Naive Prompts\")\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Cost comparison\n",
    "    print(\"\\nCost Comparison (Table 9):\")\n",
    "    cost_data = {\n",
    "        \"Method\": [\"APE\", \"PromptAgent\", \"GPT-4\", \"FIPO\"],\n",
    "        \"Dataset_Cost\": [\"$0\", \"$0\", \"$0\", \"$300\"],\n",
    "        \"Training_Cost\": [\"$0\", \"$0\", \"$0\", \"$60\"],\n",
    "        \"Inference_Cost\": [\"$5\", \"$5\", \"$4\", \"$0\"],\n",
    "        \"Inference_Time\": [\"2h\", \"2h\", \"1h\", \"30s\"]\n",
    "    }\n",
    "    print(pd.DataFrame(cost_data).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nâœ… FIPO achieves best performance with lowest inference cost!\")\n",
    "\n",
    "compare_optimization_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Case Study Analysis\n",
    "\n",
    "PhÃ¢n tÃ­ch cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ tá»« Table 4 Ä‘á»ƒ hiá»ƒu cÃ¡ch FIPO optimize prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fipo_optimizations():\n",
    "    \"\"\"Analyze FIPO optimization examples from Table 4\"\"\"\n",
    "    \n",
    "    examples = [\n",
    "        {\n",
    "            \"benchmark\": \"BBH\",\n",
    "            \"naive\": \"Jane was born on the last day of February in 2000. Today is her 16-year-old birthday. What is the date a month ago in MM/DD/YYYY?\",\n",
    "            \"optimized\": \"As today marks Jane's 16th birthday, determine the date from exactly one month prior. Jane was born on the last day of February in the year 2000, a leap year. Ensure your response is in the format MM/DD/YYYY.\",\n",
    "            \"key_improvements\": [\n",
    "                \"âœ“ Explicitly mentions '2000, a leap year'\",\n",
    "                \"âœ“ Clear instruction structure\",\n",
    "                \"âœ“ Format specification emphasized\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"benchmark\": \"MMLU\",\n",
    "            \"naive\": \"Three of the following are classic security properties; which one is not? A. Confidentiality, B. Availability, C. Correctness, D. Integrity.\",\n",
    "            \"optimized\": \"Which of the following is NOT one of the core security principles? A. Confidentiality, B. Availability, C. Correctness, D. Integrity. Provide the letter corresponding to the incorrect option.\",\n",
    "            \"key_improvements\": [\n",
    "                \"âœ“ Capitalizes 'NOT' for emphasis\",\n",
    "                \"âœ“ Clarifies 'core security principles'\",\n",
    "                \"âœ“ Explicit instruction for answer format\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"benchmark\": \"GSM8K\",\n",
    "            \"naive\": \"Lily has a list of numbers: [12, 34, 56, 75]. Calculate the average value of the list.\",\n",
    "            \"optimized\": \"To find the average of the given list of numbers, first, you need to add all the numbers in the list, which are: 12, 34, 56, and 75. Add them up, and you will get a sum of 177. Then, divide the sum by the total number of items in the list, which is 4. So, 177 divided by 4 equals 44.25.\",\n",
    "            \"key_improvements\": [\n",
    "                \"âœ“ Step-by-step breakdown\",\n",
    "                \"âœ“ Explicit calculation process\",\n",
    "                \"âš ï¸ Contains answer (cheating note issue)\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Example {i} - {example['benchmark']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nðŸ“ Naive Prompt:\\n{example['naive']}\")\n",
    "        print(f\"\\nðŸš€ Optimized Prompt:\\n{example['optimized']}\")\n",
    "        print(f\"\\nðŸ” Key Improvements:\")\n",
    "        for improvement in example['key_improvements']:\n",
    "            print(f\"   {improvement}\")\n",
    "    \n",
    "    # Analyze optimization patterns\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Common FIPO Optimization Patterns:\")\n",
    "    print(\"=\"*60)\n",
    "    patterns = [\n",
    "        \"1. **Clarification**: Ambiguous terms are explicitly defined\",\n",
    "        \"2. **Structure**: Step-by-step guidance is provided\",\n",
    "        \"3. **Format**: Output format is clearly specified\",\n",
    "        \"4. **Context**: Relevant details are highlighted (e.g., leap year)\",\n",
    "        \"5. **Emphasis**: Key instructions use formatting (e.g., NOT)\"\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        print(pattern)\n",
    "    \n",
    "    print(\"\\nâš ï¸ Limitation: ~10% of math problems contain 'cheating notes' with answers\")\n",
    "\n",
    "analyze_fipo_optimizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Implementation Guide\n",
    "\n",
    "### 7.1 End-to-End FIPO Pipeline\n",
    "\n",
    "TÃ­ch há»£p táº¥t cáº£ components vÃ o má»™t pipeline hoÃ n chá»‰nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIPOPipeline:\n",
    "    \"\"\"Complete FIPO implementation pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer_model: str = \"Tulu2-13B\", training_strategy: str = \"IPL-IPO\"):\n",
    "        self.optimizer_model = optimizer_model\n",
    "        self.training_strategy = training_strategy\n",
    "        self.meta_template = FIPOMetaTemplate()\n",
    "        self.diversifier = DatasetDiversifier()\n",
    "        \n",
    "    def optimize_prompt(\n",
    "        self,\n",
    "        naive_prompt: str,\n",
    "        naive_response: Optional[str] = None,\n",
    "        ground_truth: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Optimize a single prompt using trained FIPO model\"\"\"\n",
    "        \n",
    "        # Create optimization request\n",
    "        optimization_prompt = self.meta_template.create_optimization_prompt(\n",
    "            naive_prompt, naive_response, ground_truth\n",
    "        )\n",
    "        \n",
    "        # In production, would use actual trained model\n",
    "        # Here we simulate the optimization\n",
    "        optimized = self._simulate_optimization(naive_prompt)\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def _simulate_optimization(self, naive_prompt: str) -> str:\n",
    "        \"\"\"Simulate FIPO optimization\"\"\"\n",
    "        \n",
    "        # Common optimization patterns\n",
    "        optimizations = [\n",
    "            \"Carefully analyze the task: \",\n",
    "            \"Follow these steps to \",\n",
    "            \"Ensure your answer is clear and \"\n",
    "        ]\n",
    "        \n",
    "        # Apply optimization\n",
    "        prefix = np.random.choice(optimizations)\n",
    "        \n",
    "        # Add structure\n",
    "        structured = f\"{prefix}{naive_prompt.lower()}. \"\n",
    "        structured += \"Break down the problem systematically and verify your answer.\"\n",
    "        \n",
    "        return structured\n",
    "    \n",
    "    def batch_optimize(self, prompts: List[str]) -> List[str]:\n",
    "        \"\"\"Optimize multiple prompts\"\"\"\n",
    "        \n",
    "        optimized_prompts = []\n",
    "        \n",
    "        for prompt in tqdm(prompts, desc=\"Optimizing prompts\"):\n",
    "            optimized = self.optimize_prompt(prompt)\n",
    "            optimized_prompts.append(optimized)\n",
    "        \n",
    "        return optimized_prompts\n",
    "    \n",
    "    def evaluate_optimization(self, naive: str, optimized: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate optimization quality\"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            \"length_increase\": len(optimized.split()) / len(naive.split()),\n",
    "            \"structure_score\": optimized.count(\"step\") + optimized.count(\"Step\"),\n",
    "            \"clarity_score\": 1 if any(word in optimized.lower() for word in [\"ensure\", \"verify\", \"check\"]) else 0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = FIPOPipeline(optimizer_model=\"Tulu2-13B\", training_strategy=\"IPL-IPO\")\n",
    "\n",
    "# Test with sample prompts\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Calculate 15% of 200\",\n",
    "    \"Explain photosynthesis\"\n",
    "]\n",
    "\n",
    "print(\"FIPO Prompt Optimization Examples:\\n\")\n",
    "optimized = pipeline.batch_optimize(test_prompts)\n",
    "\n",
    "for i, (naive, opt) in enumerate(zip(test_prompts, optimized)):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Naive: {naive}\")\n",
    "    print(f\"Optimized: {opt}\")\n",
    "    \n",
    "    metrics = pipeline.evaluate_optimization(naive, opt)\n",
    "    print(f\"Metrics: Length increase: {metrics['length_increase']:.1f}x, \"\n",
    "          f\"Structure: {metrics['structure_score']}, \"\n",
    "          f\"Clarity: {metrics['clarity_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Integration with LangChain\n",
    "\n",
    "TÃ­ch há»£p FIPO vá»›i LangChain Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.schema import BasePromptTemplate\n",
    "\n",
    "class FIPOPromptTemplate(BasePromptTemplate):\n",
    "    \"\"\"LangChain-compatible FIPO prompt template\"\"\"\n",
    "    \n",
    "    def __init__(self, fipo_pipeline: FIPOPipeline):\n",
    "        self.fipo_pipeline = fipo_pipeline\n",
    "        self.input_variables = [\"input\"]\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        \"\"\"Format and optimize prompt\"\"\"\n",
    "        naive_prompt = kwargs.get(\"input\", \"\")\n",
    "        \n",
    "        # Optimize using FIPO\n",
    "        optimized = self.fipo_pipeline.optimize_prompt(naive_prompt)\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def format_prompt(self, **kwargs) -> str:\n",
    "        \"\"\"Format prompt for LangChain\"\"\"\n",
    "        return self.format(**kwargs)\n",
    "\n",
    "class FIPOChain:\n",
    "    \"\"\"LangChain-style chain with FIPO optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: BaseLanguageModel, fipo_pipeline: FIPOPipeline):\n",
    "        self.llm = llm\n",
    "        self.fipo_template = FIPOPromptTemplate(fipo_pipeline)\n",
    "    \n",
    "    def run(self, query: str) -> Dict[str, str]:\n",
    "        \"\"\"Run chain with FIPO optimization\"\"\"\n",
    "        \n",
    "        # Get optimized prompt\n",
    "        optimized_prompt = self.fipo_template.format(input=query)\n",
    "        \n",
    "        # Run with LLM (simulated)\n",
    "        response = f\"[LLM Response to optimized prompt: {optimized_prompt[:50]}...]\"\n",
    "        \n",
    "        return {\n",
    "            \"naive_prompt\": query,\n",
    "            \"optimized_prompt\": optimized_prompt,\n",
    "            \"response\": response\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"FIPO + LangChain Integration Example:\\n\")\n",
    "\n",
    "# Initialize components\n",
    "fipo_pipeline = FIPOPipeline()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # Would use actual LLM\n",
    "fipo_chain = FIPOChain(llm, fipo_pipeline)\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"Summarize the key points about climate change\",\n",
    "    \"Write a Python function to calculate factorial\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = fipo_chain.run(query)\n",
    "    print(f\"Query: {result['naive_prompt']}\")\n",
    "    print(f\"Optimized: {result['optimized_prompt']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Research Extensions & Future Work\n",
    "\n",
    "### Ideas for Personal Research\n",
    "\n",
    "Template nÃ y cung cáº¥p foundation Ä‘á»ƒ explore cÃ¡c hÆ°á»›ng nghiÃªn cá»©u má»›i vá»›i FIPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_extensions():\n",
    "    \"\"\"Suggest research directions based on FIPO\"\"\"\n",
    "    \n",
    "    extensions = [\n",
    "        {\n",
    "            \"direction\": \"Multi-lingual FIPO\",\n",
    "            \"description\": \"Extend FIPO to optimize prompts across languages\",\n",
    "            \"approach\": \"Train on multilingual preference data, test cross-lingual transfer\"\n",
    "        },\n",
    "        {\n",
    "            \"direction\": \"Domain-specific FIPO\",\n",
    "            \"description\": \"Specialize FIPO for specific domains (medical, legal, etc.)\",\n",
    "            \"approach\": \"Fine-tune on domain-specific preference data with expert validation\"\n",
    "        },\n",
    "        {\n",
    "            \"direction\": \"FIPO + Chain-of-Thought\",\n",
    "            \"description\": \"Combine FIPO with CoT prompting strategies\",\n",
    "            \"approach\": \"Optimize not just prompts but reasoning chains\"\n",
    "        },\n",
    "        {\n",
    "            \"direction\": \"Adversarial FIPO\",\n",
    "            \"description\": \"Make FIPO robust to adversarial prompt attacks\",\n",
    "            \"approach\": \"Train with adversarial examples in preference data\"\n",
    "        },\n",
    "        {\n",
    "            \"direction\": \"FIPO for Few-shot Learning\",\n",
    "            \"description\": \"Optimize few-shot examples selection and ordering\",\n",
    "            \"approach\": \"Extend meta-template to handle example selection\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ”¬ Research Extension Ideas for FIPO:\\n\")\n",
    "    \n",
    "    for i, ext in enumerate(extensions, 1):\n",
    "        print(f\"{i}. {ext['direction']}\")\n",
    "        print(f\"   ðŸ“ {ext['description']}\")\n",
    "        print(f\"   ðŸ’¡ {ext['approach']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Evaluation Metrics to Consider:\")\n",
    "    metrics = [\n",
    "        \"- Task performance improvement\",\n",
    "        \"- Generalization across models\",\n",
    "        \"- Computational efficiency\",\n",
    "        \"- Human preference alignment\",\n",
    "        \"- Robustness to distribution shift\"\n",
    "    ]\n",
    "    for metric in metrics:\n",
    "        print(metric)\n",
    "\n",
    "research_extensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "FIPO giá»›i thiá»‡u má»™t paradigm má»›i cho automatic prompt optimization:\n",
    "\n",
    "1. **Local & Private**: KhÃ´ng phá»¥ thuá»™c API services, báº£o vá»‡ privacy\n",
    "2. **Model-agnostic**: Hoáº¡t Ä‘á»™ng vá»›i báº¥t ká»³ downstream generator\n",
    "3. **Preference-based**: Sá»­ dá»¥ng contrastive learning tá»« preference data\n",
    "4. **Self-improving**: IPL cho phÃ©p model tá»± cáº£i thiá»‡n qua iterations\n",
    "5. **Cost-effective**: Chi phÃ­ tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i ad-hoc methods\n",
    "\n",
    "Paper nÃ y má»Ÿ ra nhiá»u hÆ°á»›ng nghiÃªn cá»©u má»›i trong prompt engineering vÃ  preference learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}