{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Mixture-of-Logits: L√Ω thuy·∫øt v√† Universal Approximation\n",
    "\n",
    "## üéØ M·ª•c ti√™u H·ªçc t·∫≠p\n",
    "\n",
    "Hi·ªÉu s√¢u v·ªÅ:\n",
    "1. **L√Ω thuy·∫øt to√°n h·ªçc** ƒë·∫±ng sau Mixture-of-Logits (MoL)\n",
    "2. **Universal Approximation Property** - t·∫°i sao MoL c√≥ th·ªÉ bi·ªÉu di·ªÖn b·∫•t k·ª≥ similarity function n√†o\n",
    "3. **Matrix Rank Theory** v√† m·ªëi li√™n h·ªá v·ªõi expressiveness\n",
    "4. **So s√°nh v·ªõi Dot Product** v√† h·∫°n ch·∫ø c·ªßa low-rank bottleneck\n",
    "\n",
    "## üìñ Tr√≠ch xu·∫•t t·ª´ Paper\n",
    "\n",
    "### Section 2.1 - Key Insights:\n",
    "\n",
    "> *\"Our key insight is that learned similarity approaches are but different ways to increase the expressiveness of the retrieval stage. Formally, for a query q and an item x, the expressiveness of the similarity function boils down to deriving alternative parameterizations of p(x|q) matrices, with full rank matrices being the most expressive among them.\"*\n",
    "\n",
    "> *\"Dot products, on the other hand, induces a low-rank bottleneck due to the dimensionality of the embedding, i.e., ln p(x|q) ‚àù ‚ü®f(q), g(x)‚ü© (f(q), g(x) ‚àà R^d).\"*\n",
    "\n",
    "### Mathematical Foundation:\n",
    "\n",
    "**MoL Definition (Equation 1)**:\n",
    "$$\\phi(q,x) = \\sum_{p=1}^{P} \\pi_p(q,x) \\langle f_p(q), g_p(x) \\rangle$$\n",
    "\n",
    "**Outer Product Form**:\n",
    "$$\\phi(q,x) = \\sum_{p_q=1}^{P_q} \\sum_{p_x=1}^{P_x} \\pi_{p_q,p_x}(q,x) \\left\\langle \\frac{f_{p_q}(q)}{||f_{p_q}(q)||_2}, \\frac{g_{p_x}(x)}{||g_{p_x}(x)||_2} \\right\\rangle$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List\n",
    "import math\n",
    "from scipy.linalg import svd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Ph·∫ßn 1: Matrix Rank v√† Expressiveness\n",
    "\n",
    "### üìä L√Ω thuy·∫øt:\n",
    "\n",
    "**Similarity Matrix**: Cho Q queries v√† X items, similarity matrix S ‚àà R^(Q√óX)\n",
    "- **Full Rank**: rank(S) = min(Q, X) ‚Üí Most Expressive\n",
    "- **Low Rank**: rank(S) ‚â§ d (embedding dim) ‚Üí Limited Expressiveness\n",
    "\n",
    "**Dot Product Limitation**: \n",
    "- S[i,j] = f(q_i)^T g(x_j)\n",
    "- S = F G^T where F ‚àà R^(Q√ód), G ‚àà R^(X√ód) \n",
    "- rank(S) ‚â§ min(Q, X, d) ‚Üí **Bottleneck at dimension d**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_matrix_rank_expressiveness():\n",
    "    \"\"\"\n",
    "    Demonstrate the relationship between matrix rank and expressiveness\n",
    "    \"\"\"\n",
    "    print(\"üîç Matrix Rank Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Parameters\n",
    "    num_queries = 100\n",
    "    num_items = 500\n",
    "    embedding_dims = [16, 32, 64, 128]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for d in embedding_dims:\n",
    "        # Generate random embeddings\n",
    "        Q = np.random.randn(num_queries, d)\n",
    "        X = np.random.randn(num_items, d)\n",
    "        \n",
    "        # Compute dot product similarity matrix\n",
    "        S_dot = Q @ X.T  # [num_queries, num_items]\n",
    "        \n",
    "        # Compute rank\n",
    "        rank_dot = np.linalg.matrix_rank(S_dot)\n",
    "        max_possible_rank = min(num_queries, num_items)\n",
    "        \n",
    "        # Expressiveness ratio\n",
    "        expressiveness = rank_dot / max_possible_rank\n",
    "        \n",
    "        results.append({\n",
    "            'dim': d,\n",
    "            'rank': rank_dot,\n",
    "            'max_rank': max_possible_rank,\n",
    "            'expressiveness': expressiveness\n",
    "        })\n",
    "        \n",
    "        print(f\"Embedding Dim {d:3d}: Rank = {rank_dot:3d}/{max_possible_rank} ({expressiveness:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demonstrate_full_rank_approximation():\n",
    "    \"\"\"\n",
    "    Show how MoL can approximate full-rank matrices\n",
    "    \"\"\"\n",
    "    print(\"\\nüß† MoL Full-Rank Approximation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a target full-rank similarity matrix\n",
    "    Q, X = 20, 30\n",
    "    target_matrix = np.random.randn(Q, X)\n",
    "    target_rank = np.linalg.matrix_rank(target_matrix)\n",
    "    \n",
    "    print(f\"Target Matrix: {Q}√ó{X}, Rank = {target_rank}\")\n",
    "    \n",
    "    # Approximate with different numbers of components\n",
    "    component_counts = [1, 2, 4, 8, 16]\n",
    "    embedding_dim = 8\n",
    "    \n",
    "    approximation_errors = []\n",
    "    \n",
    "    for P in component_counts:\n",
    "        # Generate MoL components\n",
    "        mol_approx = np.zeros((Q, X))\n",
    "        \n",
    "        for p in range(P):\n",
    "            # Random component embeddings\n",
    "            f_p = np.random.randn(Q, embedding_dim)\n",
    "            g_p = np.random.randn(X, embedding_dim)\n",
    "            \n",
    "            # Random gating weights (simplified)\n",
    "            weight = 1.0 / P\n",
    "            \n",
    "            # Component similarity\n",
    "            component_sim = f_p @ g_p.T\n",
    "            mol_approx += weight * component_sim\n",
    "        \n",
    "        # Compute approximation error\n",
    "        error = np.linalg.norm(target_matrix - mol_approx, 'fro')\n",
    "        approximation_errors.append(error)\n",
    "        \n",
    "        mol_rank = np.linalg.matrix_rank(mol_approx)\n",
    "        print(f\"P={P:2d}: Rank = {mol_rank:2d}, Error = {error:.3f}\")\n",
    "    \n",
    "    return component_counts, approximation_errors\n",
    "\n",
    "# Run analysis\n",
    "rank_results = analyze_matrix_rank_expressiveness()\n",
    "components, errors = demonstrate_full_rank_approximation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ph·∫ßn 2: Universal Approximation Theorem cho MoL\n",
    "\n",
    "### üìñ Theorem (Informal):\n",
    "\n",
    "**Mixture-of-Logits Universal Approximation**:\n",
    "- V·ªõi ƒë·ªß s·ªë components P, MoL c√≥ th·ªÉ approximate b·∫•t k·ª≥ similarity function n√†o\n",
    "- ƒê·∫∑c bi·ªát, MoL c√≥ th·ªÉ t·∫°o ra matrices v·ªõi rank cao h∆°n embedding dimension\n",
    "- ƒêi·ªÅu n√†y gi·∫£i th√≠ch t·∫°i sao MoL outperform dot products\n",
    "\n",
    "### üî¨ Proof Sketch:\n",
    "1. M·ªói component t·∫°o ra m·ªôt rank-r matrix (r ‚â§ embedding_dim)\n",
    "2. Weighted combination c√≥ th·ªÉ tƒÉng t·ªïng rank l√™n P √ó r\n",
    "3. V·ªõi P ƒë·ªß l·ªõn ‚Üí c√≥ th·ªÉ ƒë·∫°t full rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalApproximationDemo:\n",
    "    \"\"\"\n",
    "    Demonstrate Universal Approximation property of MoL\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, query_dim: int = 32, item_dim: int = 32, embedding_dim: int = 16):\n",
    "        self.query_dim = query_dim\n",
    "        self.item_dim = item_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def create_target_similarity_function(self, complexity: str = 'nonlinear'):\n",
    "        \"\"\"\n",
    "        Create various target similarity functions to approximate\n",
    "        \"\"\"\n",
    "        Q = torch.randn(self.query_dim, 64)  # Query features\n",
    "        X = torch.randn(self.item_dim, 64)   # Item features\n",
    "        \n",
    "        if complexity == 'linear':\n",
    "            # Linear similarity (dot product baseline)\n",
    "            W = torch.randn(64, 64)\n",
    "            S = Q @ W @ X.T\n",
    "        \n",
    "        elif complexity == 'quadratic':\n",
    "            # Quadratic similarity\n",
    "            S = torch.zeros(self.query_dim, self.item_dim)\n",
    "            for i in range(self.query_dim):\n",
    "                for j in range(self.item_dim):\n",
    "                    S[i, j] = torch.sum(Q[i] * X[j])**2\n",
    "        \n",
    "        elif complexity == 'nonlinear':\n",
    "            # Complex nonlinear similarity\n",
    "            S = torch.zeros(self.query_dim, self.item_dim)\n",
    "            for i in range(self.query_dim):\n",
    "                for j in range(self.item_dim):\n",
    "                    dot_prod = torch.dot(Q[i], X[j])\n",
    "                    S[i, j] = torch.tanh(dot_prod) + 0.5 * torch.sin(dot_prod)\n",
    "        \n",
    "        return S, Q, X\n",
    "    \n",
    "    def approximate_with_mol(self, target_S: torch.Tensor, Q: torch.Tensor, X: torch.Tensor, \n",
    "                           num_components: int = 8, num_iterations: int = 1000):\n",
    "        \"\"\"\n",
    "        Approximate target similarity using MoL\n",
    "        \"\"\"\n",
    "        # Initialize MoL components\n",
    "        query_embeddings = []\n",
    "        item_embeddings = []\n",
    "        gating_weights = []\n",
    "        \n",
    "        for p in range(num_components):\n",
    "            # Component embeddings (learnable)\n",
    "            f_p = nn.Parameter(torch.randn(Q.size(0), self.embedding_dim))\n",
    "            g_p = nn.Parameter(torch.randn(X.size(0), self.embedding_dim))\n",
    "            \n",
    "            query_embeddings.append(f_p)\n",
    "            item_embeddings.append(g_p)\n",
    "            \n",
    "            # Gating weight (learnable)\n",
    "            w_p = nn.Parameter(torch.tensor(1.0 / num_components))\n",
    "            gating_weights.append(w_p)\n",
    "        \n",
    "        # Optimizer\n",
    "        all_params = query_embeddings + item_embeddings + gating_weights\n",
    "        optimizer = torch.optim.Adam(all_params, lr=0.01)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute MoL similarity\n",
    "            mol_similarity = torch.zeros_like(target_S)\n",
    "            \n",
    "            # Ensure gating weights sum to 1\n",
    "            weights = F.softmax(torch.stack(gating_weights), dim=0)\n",
    "            \n",
    "            for p in range(num_components):\n",
    "                # Normalize embeddings\n",
    "                f_p_norm = F.normalize(query_embeddings[p], dim=1)\n",
    "                g_p_norm = F.normalize(item_embeddings[p], dim=1)\n",
    "                \n",
    "                # Component similarity\n",
    "                component_sim = torch.mm(f_p_norm, g_p_norm.t())\n",
    "                \n",
    "                # Weighted sum\n",
    "                mol_similarity += weights[p] * component_sim\n",
    "            \n",
    "            # Loss (MSE)\n",
    "            loss = F.mse_loss(mol_similarity, target_S)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if iteration % 200 == 0:\n",
    "                print(f\"Iteration {iteration:4d}: Loss = {loss.item():.6f}\")\n",
    "        \n",
    "        return mol_similarity.detach(), losses, weights.detach()\n",
    "\n",
    "# Demonstration\n",
    "print(\"üéØ Universal Approximation Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "demo = UniversalApproximationDemo(query_dim=20, item_dim=25, embedding_dim=8)\n",
    "\n",
    "# Test different complexity levels\n",
    "complexities = ['linear', 'quadratic', 'nonlinear']\n",
    "approximation_results = {}\n",
    "\n",
    "for complexity in complexities:\n",
    "    print(f\"\\nüìä Approximating {complexity} similarity...\")\n",
    "    \n",
    "    target_S, Q, X = demo.create_target_similarity_function(complexity)\n",
    "    target_rank = torch.linalg.matrix_rank(target_S).item()\n",
    "    \n",
    "    print(f\"Target matrix rank: {target_rank}\")\n",
    "    \n",
    "    # Approximate with MoL\n",
    "    mol_S, losses, weights = demo.approximate_with_mol(target_S, Q, X, num_components=12, num_iterations=800)\n",
    "    \n",
    "    mol_rank = torch.linalg.matrix_rank(mol_S).item()\n",
    "    final_error = F.mse_loss(mol_S, target_S).item()\n",
    "    \n",
    "    print(f\"MoL approximation rank: {mol_rank}\")\n",
    "    print(f\"Final approximation error: {final_error:.6f}\")\n",
    "    \n",
    "    approximation_results[complexity] = {\n",
    "        'target_rank': target_rank,\n",
    "        'mol_rank': mol_rank,\n",
    "        'error': final_error,\n",
    "        'losses': losses,\n",
    "        'weights': weights\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ph·∫ßn 3: So s√°nh Dot Product vs MoL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_expressiveness_dot_vs_mol():\n",
    "    \"\"\"\n",
    "    Direct comparison between dot product and MoL expressiveness\n",
    "    \"\"\"\n",
    "    print(\"\\n‚öîÔ∏è Dot Product vs MoL Expressiveness\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Parameters\n",
    "    num_queries, num_items = 50, 80\n",
    "    feature_dim = 128\n",
    "    embedding_dims = [8, 16, 32, 64]\n",
    "    \n",
    "    # Generate random query and item features\n",
    "    queries = torch.randn(num_queries, feature_dim)\n",
    "    items = torch.randn(num_items, feature_dim)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for embed_dim in embedding_dims:\n",
    "        print(f\"\\nüìè Embedding Dimension: {embed_dim}\")\n",
    "        \n",
    "        # Dot Product Model\n",
    "        dot_query_proj = nn.Linear(feature_dim, embed_dim)\n",
    "        dot_item_proj = nn.Linear(feature_dim, embed_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_emb = F.normalize(dot_query_proj(queries), dim=1)\n",
    "            i_emb = F.normalize(dot_item_proj(items), dim=1)\n",
    "            dot_similarity = torch.mm(q_emb, i_emb.t())\n",
    "            dot_rank = torch.linalg.matrix_rank(dot_similarity).item()\n",
    "        \n",
    "        # MoL Model with different component counts\n",
    "        mol_ranks = []\n",
    "        component_counts = [2, 4, 8, 16]\n",
    "        \n",
    "        for num_comp in component_counts:\n",
    "            mol_similarity = torch.zeros(num_queries, num_items)\n",
    "            \n",
    "            for p in range(num_comp):\n",
    "                # Random component projections\n",
    "                f_proj = nn.Linear(feature_dim, embed_dim)\n",
    "                g_proj = nn.Linear(feature_dim, embed_dim)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    f_p = F.normalize(f_proj(queries), dim=1)\n",
    "                    g_p = F.normalize(g_proj(items), dim=1)\n",
    "                    component_sim = torch.mm(f_p, g_p.t())\n",
    "                    mol_similarity += (1.0 / num_comp) * component_sim\n",
    "            \n",
    "            mol_rank = torch.linalg.matrix_rank(mol_similarity).item()\n",
    "            mol_ranks.append(mol_rank)\n",
    "        \n",
    "        max_possible_rank = min(num_queries, num_items)\n",
    "        \n",
    "        print(f\"   Dot Product Rank: {dot_rank:2d} / {max_possible_rank} ({dot_rank/max_possible_rank:.3f})\")\n",
    "        print(f\"   MoL Ranks:\")\n",
    "        for i, (nc, mr) in enumerate(zip(component_counts, mol_ranks)):\n",
    "            print(f\"     {nc:2d} components: {mr:2d} / {max_possible_rank} ({mr/max_possible_rank:.3f})\")\n",
    "        \n",
    "        results.append({\n",
    "            'embed_dim': embed_dim,\n",
    "            'dot_rank': dot_rank,\n",
    "            'mol_ranks': mol_ranks,\n",
    "            'max_rank': max_possible_rank\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_expressiveness_dot_vs_mol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualization v√† Ph√¢n t√≠ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Matrix Rank vs Embedding Dimension\n",
    "dims = [r['dim'] for r in rank_results]\n",
    "ranks = [r['rank'] for r in rank_results]\n",
    "max_ranks = [r['max_rank'] for r in rank_results]\n",
    "\n",
    "axes[0, 0].plot(dims, ranks, 'o-', label='Achieved Rank', linewidth=2, markersize=8)\n",
    "axes[0, 0].axhline(y=max_ranks[0], color='red', linestyle='--', label='Max Possible Rank', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Embedding Dimension')\n",
    "axes[0, 0].set_ylabel('Matrix Rank')\n",
    "axes[0, 0].set_title('Dot Product: Rank vs Embedding Dimension')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MoL Approximation Error vs Components\n",
    "axes[0, 1].plot(components, errors, 'go-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Components')\n",
    "axes[0, 1].set_ylabel('Approximation Error')\n",
    "axes[0, 1].set_title('MoL: Approximation Error vs Components')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Universal Approximation Results\n",
    "complexities = list(approximation_results.keys())\n",
    "target_ranks = [approximation_results[c]['target_rank'] for c in complexities]\n",
    "mol_ranks = [approximation_results[c]['mol_rank'] for c in complexities]\n",
    "errors = [approximation_results[c]['error'] for c in complexities]\n",
    "\n",
    "x = np.arange(len(complexities))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 2].bar(x - width/2, target_ranks, width, label='Target Rank', alpha=0.8)\n",
    "axes[0, 2].bar(x + width/2, mol_ranks, width, label='MoL Approximation Rank', alpha=0.8)\n",
    "axes[0, 2].set_xlabel('Similarity Complexity')\n",
    "axes[0, 2].set_ylabel('Matrix Rank')\n",
    "axes[0, 2].set_title('Universal Approximation: Rank Achievement')\n",
    "axes[0, 2].set_xticks(x)\n",
    "axes[0, 2].set_xticklabels(complexities)\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Loss for Universal Approximation\n",
    "for i, complexity in enumerate(complexities):\n",
    "    losses = approximation_results[complexity]['losses']\n",
    "    axes[1, 0].plot(losses, label=f'{complexity.capitalize()}', linewidth=2)\n",
    "\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('MSE Loss')\n",
    "axes[1, 0].set_title('MoL Training: Convergence Analysis')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Expressiveness Comparison\n",
    "embed_dims = [r['embed_dim'] for r in comparison_results]\n",
    "dot_expressiveness = [r['dot_rank'] / r['max_rank'] for r in comparison_results]\n",
    "mol_expressiveness = [[mr / r['max_rank'] for mr in r['mol_ranks']] for r in comparison_results]\n",
    "\n",
    "axes[1, 1].plot(embed_dims, dot_expressiveness, 'ro-', label='Dot Product', linewidth=2, markersize=8)\n",
    "\n",
    "# Plot MoL with different component counts\n",
    "component_counts = [2, 4, 8, 16]\n",
    "colors = ['green', 'blue', 'purple', 'orange']\n",
    "for i, (nc, color) in enumerate(zip(component_counts, colors)):\n",
    "    mol_expr = [me[i] for me in mol_expressiveness]\n",
    "    axes[1, 1].plot(embed_dims, mol_expr, f'{color[0]}o-', label=f'MoL-{nc}', linewidth=2, markersize=6)\n",
    "\n",
    "axes[1, 1].set_xlabel('Embedding Dimension')\n",
    "axes[1, 1].set_ylabel('Expressiveness (Rank Ratio)')\n",
    "axes[1, 1].set_title('Expressiveness: Dot Product vs MoL')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Component Weight Distribution\n",
    "if 'nonlinear' in approximation_results:\n",
    "    weights = approximation_results['nonlinear']['weights'].numpy()\n",
    "    axes[1, 2].bar(range(len(weights)), weights, alpha=0.7, color='skyblue')\n",
    "    axes[1, 2].set_xlabel('Component Index')\n",
    "    axes[1, 2].set_ylabel('Weight')\n",
    "    axes[1, 2].set_title('MoL Component Weight Distribution')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'Component weights\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "    axes[1, 2].set_title('Component Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Insights v√† K·∫øt lu·∫≠n\n",
    "\n",
    "### üîç Quan s√°t t·ª´ Th√≠ nghi·ªám:\n",
    "\n",
    "1. **Dot Product Limitation**: \n",
    "   - Matrix rank b·ªã gi·ªõi h·∫°n b·ªüi embedding dimension\n",
    "   - Khi d << min(Q, X), expressiveness b·ªã nghi√™m tr·ªçng h·∫°n ch·∫ø\n",
    "   - Kh√¥ng th·ªÉ tƒÉng expressiveness b·∫±ng c√°ch tƒÉng d (memory & overfitting)\n",
    "\n",
    "2. **MoL Advantages**:\n",
    "   - C√≥ th·ªÉ ƒë·∫°t rank cao h∆°n embedding dimension\n",
    "   - Expressiveness tƒÉng theo s·ªë components P\n",
    "   - Universal approximation property ƒë∆∞·ª£c verify empirically\n",
    "\n",
    "3. **Practical Implications**:\n",
    "   - MoL ph√π h·ª£p cho complex similarity patterns\n",
    "   - Trade-off gi·ªØa accuracy v√† computational cost\n",
    "   - Load balancing quan tr·ªçng ƒë·ªÉ tr√°nh component collapse\n",
    "\n",
    "### üìñ Theoretical Foundation:\n",
    "\n",
    "**Theorem**: MoL with P components v√† embedding dimension d c√≥ th·ªÉ approximate matrices v·ªõi rank l√™n ƒë·∫øn P √ó d (trong ƒëi·ªÅu ki·ªán l√Ω t∆∞·ªüng).\n",
    "\n",
    "**Proof Idea**: \n",
    "- M·ªói component ‚àë·µ¢ w·µ¢ f·µ¢(q) g·µ¢(x)·µÄ c√≥ rank ‚â§ d\n",
    "- Linear combination c·ªßa P components c√≥ rank ‚â§ P √ó d  \n",
    "- V·ªõi gating weights ph√π h·ª£p, c√≥ th·ªÉ achieve rank cao\n",
    "\n",
    "### üöÄ Practical Applications:\n",
    "\n",
    "1. **Recommendation Systems**: Model complex user-item interactions\n",
    "2. **Information Retrieval**: Capture nuanced query-document relationships  \n",
    "3. **Question Answering**: Rich semantic matching beyond dot products\n",
    "4. **Multi-modal Retrieval**: Cross-modal similarity learning\n",
    "\n",
    "### üéØ Next Steps:\n",
    "\n",
    "- **Load Balancing**: Ensure component utilization\n",
    "- **Efficient Algorithms**: Fast top-K retrieval\n",
    "- **GPU Optimization**: Hardware-friendly implementations\n",
    "- **Real-world Evaluation**: Large-scale datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}