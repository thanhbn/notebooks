{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 02. LangFuse Observations & Debugging\n",
    "\n",
    "## ğŸ“‹ Má»¥c tiÃªu há»c táº­p\n",
    "\n",
    "Notebook nÃ y sáº½ hÆ°á»›ng dáº«n báº¡n:\n",
    "- Hiá»ƒu cÃ¡ch sá»­ dá»¥ng LangFuse Ä‘á»ƒ quan sÃ¡t vÃ  debug á»©ng dá»¥ng LLM\n",
    "- PhÃ¢n tÃ­ch cÃ¡c Span, Event, vÃ  Observation trong LangFuse UI\n",
    "- Theo dÃµi luá»“ng hoáº¡t Ä‘á»™ng cá»§a LLM chains vÃ  RAG pipelines\n",
    "- XÃ¡c Ä‘á»‹nh vÃ  kháº¯c phá»¥c cÃ¡c lá»—i tiá»m áº©n trong á»©ng dá»¥ng LLM\n",
    "- Äo lÆ°á»ng hiá»‡u suáº¥t (latency, token usage) vÃ  tá»‘i Æ°u hÃ³a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## ğŸ” Giá»›i thiá»‡u vá» LangFuse Debugging\n",
    "\n",
    "LangFuse cung cáº¥p kháº£ nÄƒng quan sÃ¡t chi tiáº¿t cho á»©ng dá»¥ng LLM thÃ´ng qua:\n",
    "\n",
    "### ğŸ“Š CÃ¡c thÃ nh pháº§n quan sÃ¡t chÃ­nh:\n",
    "- **Traces**: ToÃ n bá»™ luá»“ng thá»±c thi cá»§a má»™t request\n",
    "- **Spans**: CÃ¡c bÆ°á»›c con trong má»™t trace (LLM calls, retrieval, processing)\n",
    "- **Events**: CÃ¡c sá»± kiá»‡n Ä‘Æ¡n láº» (logging, errors, milestones)\n",
    "- **Observations**: Tá»•ng há»£p táº¥t cáº£ hoáº¡t Ä‘á»™ng cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c\n",
    "\n",
    "### ğŸ¯ Lá»£i Ã­ch cá»§a debugging vá»›i LangFuse:\n",
    "- **Visibility**: NhÃ¬n tháº¥y toÃ n bá»™ luá»“ng hoáº¡t Ä‘á»™ng cá»§a LLM\n",
    "- **Performance**: Äo lÆ°á»ng latency vÃ  token usage chi tiáº¿t\n",
    "- **Error Detection**: PhÃ¡t hiá»‡n lá»—i vÃ  bottlenecks\n",
    "- **Quality Assurance**: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng output\n",
    "- **Cost Optimization**: Theo dÃµi chi phÃ­ sá»­ dá»¥ng LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## âš™ï¸ CÃ i Ä‘áº·t & Cáº¥u hÃ¬nh\n",
    "\n",
    "Nháº¯c láº¡i cáº¥u hÃ¬nh LangFuse tá»« notebook trÆ°á»›c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "# Cáº¥u hÃ¬nh LangFuse\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-your-public-key\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\", \"sk-lf-your-secret-key\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    ")\n",
    "\n",
    "# Táº¡o callback handler\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-your-public-key\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\", \"sk-lf-your-secret-key\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    ")\n",
    "\n",
    "print(\"âœ… LangFuse configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"ğŸ“š All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ğŸ¤– Cáº¥u hÃ¬nh LLM vá»›i LangFuse tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khá»Ÿi táº¡o ChatAnthropic vá»›i LangFuse callback\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    callbacks=[langfuse_handler]\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– LLM initialized with LangFuse tracing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## ğŸš¨ VÃ Dá»¤ 1: Chain cÃ³ lá»—i tiá»m áº©n\n",
    "\n",
    "ChÃºng ta sáº½ táº¡o má»™t chain cÃ³ thá»ƒ gáº·p lá»—i Ä‘á»ƒ tháº¥y cÃ¡ch LangFuse giÃºp debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"problematic_chain\")\n",
    "def problematic_math_chain(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Chain giáº£i toÃ¡n cÃ³ thá»ƒ gáº·p lá»—i vá»›i má»™t sá»‘ input nháº¥t Ä‘á»‹nh\n",
    "    \"\"\"\n",
    "    # Táº¡o span cho preprocessing\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"math_preprocessing\",\n",
    "        input={\"raw_question\": question}\n",
    "    )\n",
    "    \n",
    "    # Preprocessing - cÃ³ thá»ƒ gÃ¢y lá»—i náº¿u input khÃ´ng phÃ¹ há»£p\n",
    "    if len(question.strip()) < 5:\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\",\n",
    "            status_message=\"Input quÃ¡ ngáº¯n Ä‘á»ƒ xá»­ lÃ½\"\n",
    "        )\n",
    "        raise ValueError(\"Question too short for processing\")\n",
    "    \n",
    "    # Táº¡o prompt cho toÃ¡n há»c\n",
    "    math_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Báº¡n lÃ  má»™t trá»£ lÃ½ toÃ¡n há»c thÃ´ng minh. HÃ£y giáº£i bÃ i toÃ¡n má»™t cÃ¡ch chi tiáº¿t vÃ  chÃ­nh xÃ¡c.\"),\n",
    "        (\"human\", \"CÃ¢u há»i: {question}\\n\\nHÃ£y giáº£i thÃ­ch tá»«ng bÆ°á»›c má»™t cÃ¡ch rÃµ rÃ ng.\")\n",
    "    ])\n",
    "    \n",
    "    # Táº¡o chain\n",
    "    chain = math_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Thá»±c thi vá»›i error handling\n",
    "    try:\n",
    "        result = chain.invoke(\n",
    "            {\"question\": question},\n",
    "            config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "        \n",
    "        # Log thÃ nh cÃ´ng\n",
    "        langfuse_context.update_current_observation(\n",
    "            output={\"answer\": result, \"status\": \"success\"}\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log lá»—i\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\",\n",
    "            status_message=f\"Chain execution failed: {str(e)}\",\n",
    "            output={\"error\": str(e)}\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "print(\"ğŸ”§ Problematic chain function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test chain vá»›i cÃ¡c input khÃ¡c nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_cases = [\n",
    "    \"TÃ­nh 15 + 27 = ?\",  # CÃ¢u há»i bÃ¬nh thÆ°á»ng\n",
    "    \"2+2\",  # CÃ¢u há»i ngáº¯n - sáº½ gÃ¢y lá»—i\n",
    "    \"Giáº£i phÆ°Æ¡ng trÃ¬nh xÂ² - 5x + 6 = 0\",  # CÃ¢u há»i phá»©c táº¡p\n",
    "    \"Hi\"  # Input quÃ¡ ngáº¯n - sáº½ gÃ¢y lá»—i\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing problematic chain with different inputs...\\n\")\n",
    "\n",
    "for i, question in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        result = problematic_math_chain(question)\n",
    "        print(f\"âœ… Success: {result[:100]}...\" if len(result) > 100 else f\"âœ… Success: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    time.sleep(1)  # Delay Ä‘á»ƒ dá»… theo dÃµi trÃªn LangFuse\n",
    "\n",
    "print(\"ğŸ” Check LangFuse UI Ä‘á»ƒ xem traces vÃ  errors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## ğŸ” VÃ Dá»¤ 2: RAG Pipeline vá»›i detailed tracing\n",
    "\n",
    "Táº¡o má»™t RAG pipeline Ä‘Æ¡n giáº£n vá»›i tracing chi tiáº¿t cho viá»‡c retrieval vÃ  generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o sample documents\n",
    "sample_documents = [\n",
    "    \"Python lÃ  má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh cáº¥p cao, dá»… há»c vÃ  máº¡nh máº½. Python Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong khoa há»c dá»¯ liá»‡u, phÃ¡t triá»ƒn web, vÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o.\",\n",
    "    \"Machine Learning lÃ  má»™t nhÃ¡nh cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cho phÃ©p mÃ¡y tÃ­nh há»c tá»« dá»¯ liá»‡u mÃ  khÃ´ng cáº§n Ä‘Æ°á»£c láº­p trÃ¬nh cá»¥ thá»ƒ cho tá»«ng tÃ¡c vá»¥.\",\n",
    "    \"LangChain lÃ  má»™t framework máº¡nh máº½ Ä‘á»ƒ xÃ¢y dá»±ng á»©ng dá»¥ng vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM). NÃ³ cung cáº¥p cÃ¡c cÃ´ng cá»¥ Ä‘á»ƒ táº¡o chains, agents vÃ  cÃ¡c thÃ nh pháº§n khÃ¡c.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) lÃ  má»™t ká»¹ thuáº­t káº¿t há»£p viá»‡c truy xuáº¥t thÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u vá»›i kháº£ nÄƒng sinh vÄƒn báº£n cá»§a LLM.\",\n",
    "    \"Vector databases nhÆ° FAISS, Pinecone Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÆ°u trá»¯ vÃ  tÃ¬m kiáº¿m embeddings má»™t cÃ¡ch hiá»‡u quáº£ trong cÃ¡c há»‡ thá»‘ng RAG.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“„ Sample documents created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"setup_rag_pipeline\")\n",
    "def setup_rag_pipeline(documents: List[str]):\n",
    "    \"\"\"\n",
    "    Thiáº¿t láº­p RAG pipeline vá»›i detailed tracing\n",
    "    \"\"\"\n",
    "    # Táº¡o span cho document processing\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"document_processing\",\n",
    "        input={\"num_documents\": len(documents)}\n",
    "    )\n",
    "    \n",
    "    # Text splitting\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    \n",
    "    splits = text_splitter.create_documents(documents)\n",
    "    \n",
    "    # Táº¡o embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    )\n",
    "    \n",
    "    # Táº¡o vector store\n",
    "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "    \n",
    "    # Log thÃ´ng tin vá» vector store\n",
    "    langfuse_context.update_current_observation(\n",
    "        output={\n",
    "            \"num_chunks\": len(splits),\n",
    "            \"embedding_model\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"vectorstore_type\": \"FAISS\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Setup RAG pipeline\n",
    "vectorstore = setup_rag_pipeline(sample_documents)\n",
    "print(\"ğŸ”§ RAG pipeline setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"rag_query\")\n",
    "def rag_query(question: str, vectorstore, k: int = 3):\n",
    "    \"\"\"\n",
    "    Thá»±c hiá»‡n RAG query vá»›i detailed tracing\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Táº¡o span cho retrieval\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"retrieval_phase\",\n",
    "        input={\"question\": question, \"k\": k}\n",
    "    )\n",
    "    \n",
    "    # Retrieval\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Log retrieval results\n",
    "    retrieval_time = time.time() - start_time\n",
    "    retrieval_results = [\n",
    "        {\"content\": doc.page_content[:100] + \"...\", \"metadata\": doc.metadata}\n",
    "        for doc in retrieved_docs\n",
    "    ]\n",
    "    \n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"retrieval_results\",\n",
    "        output={\n",
    "            \"num_retrieved\": len(retrieved_docs),\n",
    "            \"retrieval_time_ms\": round(retrieval_time * 1000, 2),\n",
    "            \"documents\": retrieval_results\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Táº¡o context tá»« retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Táº¡o span cho generation\n",
    "    generation_start = time.time()\n",
    "    \n",
    "    # RAG prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p.\"),\n",
    "        (\"human\", \"\"\"ThÃ´ng tin tham kháº£o:\n",
    "{context}\n",
    "\n",
    "CÃ¢u há»i: {question}\n",
    "\n",
    "HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin tham kháº£o trÃªn. Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§, hÃ£y nÃ³i rÃµ Ä‘iá»u Ä‘Ã³.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Táº¡o chain\n",
    "    rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Generation\n",
    "    response = rag_chain.invoke(\n",
    "        {\"context\": context, \"question\": question},\n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "    \n",
    "    # Log generation results\n",
    "    generation_time = time.time() - generation_start\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"generation_results\",\n",
    "        output={\n",
    "            \"response\": response,\n",
    "            \"generation_time_ms\": round(generation_time * 1000, 2),\n",
    "            \"total_time_ms\": round(total_time * 1000, 2),\n",
    "            \"context_length\": len(context)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"context\": context,\n",
    "        \"metrics\": {\n",
    "            \"retrieval_time_ms\": round(retrieval_time * 1000, 2),\n",
    "            \"generation_time_ms\": round(generation_time * 1000, 2),\n",
    "            \"total_time_ms\": round(total_time * 1000, 2)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"ğŸ” RAG query function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test RAG pipeline vá»›i cÃ¡c cÃ¢u há»i khÃ¡c nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Python Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÃ m gÃ¬?\",\n",
    "    \"RAG lÃ  gÃ¬ vÃ  hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?\",\n",
    "    \"Vector database cÃ³ vai trÃ² gÃ¬ trong RAG?\",\n",
    "    \"Blockchain lÃ  gÃ¬?\"  # CÃ¢u há»i ngoÃ i pháº¡m vi tÃ i liá»‡u\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing RAG pipeline with different queries...\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        result = rag_query(query, vectorstore)\n",
    "        \n",
    "        print(f\"ğŸ“ Answer: {result['answer'][:200]}...\" if len(result['answer']) > 200 else f\"ğŸ“ Answer: {result['answer']}\")\n",
    "        print(f\"ğŸ“Š Metrics: {result['metrics']}\")\n",
    "        print(f\"ğŸ“š Retrieved {len(result['retrieved_docs'])} documents\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    time.sleep(2)  # Delay Ä‘á»ƒ dá»… theo dÃµi trÃªn LangFuse\n",
    "\n",
    "print(\"ğŸ” Check LangFuse UI Ä‘á»ƒ xem RAG traces chi tiáº¿t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## ğŸ“Š Táº¡o trace vá»›i custom metadata vÃ  tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"advanced_llm_call\")\n",
    "def advanced_llm_call_with_metadata(prompt: str, user_id: str = \"demo_user\"):\n",
    "    \"\"\"\n",
    "    LLM call vá»›i metadata vÃ  tags chi tiáº¿t\n",
    "    \"\"\"\n",
    "    # Cáº­p nháº­t observation vá»›i metadata\n",
    "    langfuse_context.update_current_observation(\n",
    "        user_id=user_id,\n",
    "        session_id=f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        metadata={\n",
    "            \"prompt_length\": len(prompt),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_version\": \"claude-3-sonnet-20240229\",\n",
    "            \"temperature\": 0.7\n",
    "        },\n",
    "        tags=[\"advanced_call\", \"with_metadata\", \"demo\"]\n",
    "    )\n",
    "    \n",
    "    # Táº¡o messages\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»¯u Ã­ch vÃ  thÃ´ng minh.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    # Call LLM\n",
    "    response = llm.invoke(\n",
    "        messages,\n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "    \n",
    "    # Log response metadata\n",
    "    langfuse_context.update_current_observation(\n",
    "        output={\n",
    "            \"response\": response.content,\n",
    "            \"response_length\": len(response.content),\n",
    "            \"token_usage\": getattr(response, 'usage_metadata', {})\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Test vá»›i metadata\n",
    "response = advanced_llm_call_with_metadata(\n",
    "    \"Giáº£i thÃ­ch khÃ¡i niá»‡m Machine Learning má»™t cÃ¡ch Ä‘Æ¡n giáº£n\",\n",
    "    user_id=\"user_123\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Response with metadata:\")\n",
    "print(response[:300] + \"...\" if len(response) > 300 else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## ğŸ¯ Táº¡o scores vÃ  evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"evaluated_llm_call\")\n",
    "def evaluated_llm_call(question: str, expected_topics: List[str]):\n",
    "    \"\"\"\n",
    "    LLM call vá»›i evaluation scores\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM\n",
    "    response = llm.invoke(\n",
    "        [HumanMessage(content=question)],\n",
    "        config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    response_text = response.content\n",
    "    \n",
    "    # Simple evaluation scores\n",
    "    relevance_score = sum(1 for topic in expected_topics if topic.lower() in response_text.lower()) / len(expected_topics)\n",
    "    length_score = min(len(response_text) / 500, 1.0)  # Äiá»ƒm dá»±a trÃªn Ä‘á»™ dÃ i\n",
    "    speed_score = max(0, 1 - (response_time / 10))  # Äiá»ƒm dá»±a trÃªn tá»‘c Ä‘á»™\n",
    "    \n",
    "    # Táº¡o scores trong LangFuse\n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"relevance\",\n",
    "        value=relevance_score,\n",
    "        comment=f\"Matches {int(relevance_score * len(expected_topics))}/{len(expected_topics)} expected topics\"\n",
    "    )\n",
    "    \n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"response_length\",\n",
    "        value=length_score,\n",
    "        comment=f\"Response length: {len(response_text)} characters\"\n",
    "    )\n",
    "    \n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"response_speed\",\n",
    "        value=speed_score,\n",
    "        comment=f\"Response time: {response_time:.2f} seconds\"\n",
    "    )\n",
    "    \n",
    "    # Overall score\n",
    "    overall_score = (relevance_score + length_score + speed_score) / 3\n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"overall_quality\",\n",
    "        value=overall_score,\n",
    "        comment=\"Average of relevance, length, and speed scores\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"scores\": {\n",
    "            \"relevance\": relevance_score,\n",
    "            \"length\": length_score,\n",
    "            \"speed\": speed_score,\n",
    "            \"overall\": overall_score\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"response_time\": response_time,\n",
    "            \"response_length\": len(response_text)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test vá»›i evaluation\n",
    "result = evaluated_llm_call(\n",
    "    \"Giáº£i thÃ­ch vá» Deep Learning vÃ  á»©ng dá»¥ng cá»§a nÃ³\",\n",
    "    expected_topics=[\"neural networks\", \"training\", \"applications\", \"data\"]\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Evaluation Results:\")\n",
    "print(f\"Scores: {result['scores']}\")\n",
    "print(f\"Metrics: {result['metrics']}\")\n",
    "print(f\"Response: {result['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## ğŸ” PhÃ¢n tÃ­ch vÃ  Debug trÃªn LangFuse UI\n",
    "\n",
    "### ğŸ“± CÃ¡ch sá»­ dá»¥ng LangFuse UI Ä‘á»ƒ phÃ¢n tÃ­ch:\n",
    "\n",
    "1. **Má»Ÿ LangFuse Dashboard**: Truy cáº­p `http://localhost:3000`\n",
    "\n",
    "2. **Xem Traces**:\n",
    "   - VÃ o tab \"Traces\" Ä‘á»ƒ xem táº¥t cáº£ execution traces\n",
    "   - Click vÃ o trace Ä‘á»ƒ xem chi tiáº¿t\n",
    "   - Quan sÃ¡t timeline vÃ  nested spans\n",
    "\n",
    "3. **PhÃ¢n tÃ­ch Observations**:\n",
    "   - **Input/Output**: Xem dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra\n",
    "   - **Latency**: Äo thá»i gian thá»±c thi tá»«ng bÆ°á»›c\n",
    "   - **Token Usage**: Theo dÃµi sá»‘ token Ä‘Æ°á»£c sá»­ dá»¥ng\n",
    "   - **Errors**: XÃ¡c Ä‘á»‹nh lá»—i vÃ  nguyÃªn nhÃ¢n\n",
    "\n",
    "4. **Metrics Dashboard**:\n",
    "   - Xem tá»•ng quan vá» performance\n",
    "   - Theo dÃµi cost vÃ  usage\n",
    "   - PhÃ¢n tÃ­ch trends theo thá»i gian\n",
    "\n",
    "5. **Scores vÃ  Evaluations**:\n",
    "   - Xem scores cho tá»«ng trace\n",
    "   - So sÃ¡nh quality metrics\n",
    "   - Táº¡o evaluations tá»± Ä‘á»™ng\n",
    "\n",
    "### ğŸ¯ CÃ¡c Ä‘iá»ƒm cáº§n chÃº Ã½ khi debug:\n",
    "\n",
    "- **Latency Bottlenecks**: Spans nÃ o máº¥t thá»i gian nháº¥t?\n",
    "- **Error Patterns**: Lá»—i xuáº¥t hiá»‡n á»Ÿ Ä‘Ã¢u vÃ  táº§n suáº¥t?\n",
    "- **Token Usage**: Chi phÃ­ cÃ³ tá»‘i Æ°u khÃ´ng?\n",
    "- **Quality Scores**: Output cÃ³ Ä‘áº¡t cháº¥t lÆ°á»£ng mong muá»‘n?\n",
    "- **User Experience**: Response time cÃ³ cháº¥p nháº­n Ä‘Æ°á»£c khÃ´ng?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Táº¡o comprehensive trace cho production debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(name=\"production_rag_pipeline\")\n",
    "def production_rag_pipeline(query: str, user_id: str, session_id: str):\n",
    "    \"\"\"\n",
    "    Production-ready RAG pipeline vá»›i comprehensive tracing\n",
    "    \"\"\"\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    # Set session vÃ  user context\n",
    "    langfuse_context.update_current_observation(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        metadata={\n",
    "            \"pipeline_version\": \"v1.0.0\",\n",
    "            \"environment\": \"development\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        tags=[\"production\", \"rag\", \"comprehensive\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Query preprocessing\n",
    "        with langfuse_context.observe(name=\"query_preprocessing\") as preprocessing_span:\n",
    "            preprocessed_query = query.strip().lower()\n",
    "            preprocessing_span.update(\n",
    "                input={\"raw_query\": query},\n",
    "                output={\"preprocessed_query\": preprocessed_query},\n",
    "                metadata={\"preprocessing_time\": time.time() - pipeline_start}\n",
    "            )\n",
    "        \n",
    "        # Step 2: Retrieval\n",
    "        retrieval_start = time.time()\n",
    "        with langfuse_context.observe(name=\"document_retrieval\") as retrieval_span:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "            retrieved_docs = retriever.get_relevant_documents(preprocessed_query)\n",
    "            \n",
    "            retrieval_time = time.time() - retrieval_start\n",
    "            retrieval_span.update(\n",
    "                input={\"query\": preprocessed_query, \"k\": 3},\n",
    "                output={\n",
    "                    \"num_documents\": len(retrieved_docs),\n",
    "                    \"documents\": [doc.page_content[:100] for doc in retrieved_docs]\n",
    "                },\n",
    "                metadata={\"retrieval_time_ms\": round(retrieval_time * 1000, 2)}\n",
    "            )\n",
    "            \n",
    "            # Score retrieval quality\n",
    "            retrieval_span.score(\n",
    "                name=\"retrieval_quality\",\n",
    "                value=min(len(retrieved_docs) / 3, 1.0),\n",
    "                comment=f\"Retrieved {len(retrieved_docs)} out of 3 requested documents\"\n",
    "            )\n",
    "        \n",
    "        # Step 3: Context preparation\n",
    "        with langfuse_context.observe(name=\"context_preparation\") as context_span:\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "            context_span.update(\n",
    "                input={\"documents\": [doc.page_content for doc in retrieved_docs]},\n",
    "                output={\"context_length\": len(context)},\n",
    "                metadata={\"num_documents_used\": len(retrieved_docs)}\n",
    "            )\n",
    "        \n",
    "        # Step 4: Generation\n",
    "        generation_start = time.time()\n",
    "        with langfuse_context.observe(name=\"answer_generation\") as generation_span:\n",
    "            rag_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ´ng minh. Tráº£ lá»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p.\"),\n",
    "                (\"human\", \"ThÃ´ng tin: {context}\\n\\nCÃ¢u há»i: {question}\\n\\nTráº£ lá»i:\")\n",
    "            ])\n",
    "            \n",
    "            chain = rag_prompt | llm | StrOutputParser()\n",
    "            answer = chain.invoke(\n",
    "                {\"context\": context, \"question\": query},\n",
    "                config={\"callbacks\": [langfuse_handler]}\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - generation_start\n",
    "            generation_span.update(\n",
    "                input={\"context_length\": len(context), \"question\": query},\n",
    "                output={\"answer\": answer, \"answer_length\": len(answer)},\n",
    "                metadata={\"generation_time_ms\": round(generation_time * 1000, 2)}\n",
    "            )\n",
    "            \n",
    "            # Score generation quality\n",
    "            generation_span.score(\n",
    "                name=\"answer_completeness\",\n",
    "                value=min(len(answer) / 200, 1.0),\n",
    "                comment=f\"Answer length: {len(answer)} characters\"\n",
    "            )\n",
    "        \n",
    "        # Step 5: Post-processing & final results\n",
    "        total_time = time.time() - pipeline_start\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_docs\": len(retrieved_docs),\n",
    "            \"context_length\": len(context),\n",
    "            \"total_time_ms\": round(total_time * 1000, 2),\n",
    "            \"retrieval_time_ms\": round(retrieval_time * 1000, 2),\n",
    "            \"generation_time_ms\": round(generation_time * 1000, 2)\n",
    "        }\n",
    "        \n",
    "        # Overall pipeline score\n",
    "        langfuse_context.score_current_observation(\n",
    "            name=\"pipeline_performance\",\n",
    "            value=max(0, 1 - (total_time / 30)),  # Penalty for slow responses\n",
    "            comment=f\"Total pipeline time: {total_time:.2f}s\"\n",
    "        )\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            output=result,\n",
    "            status_message=\"Pipeline completed successfully\"\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_time = time.time() - pipeline_start\n",
    "        langfuse_context.update_current_observation(\n",
    "            level=\"ERROR\",\n",
    "            status_message=f\"Pipeline failed after {error_time:.2f}s: {str(e)}\",\n",
    "            output={\"error\": str(e), \"error_type\": type(e).__name__}\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "# Test production pipeline\n",
    "production_result = production_rag_pipeline(\n",
    "    query=\"Giáº£i thÃ­ch vá» LangChain vÃ  cÃ¡c á»©ng dá»¥ng cá»§a nÃ³\",\n",
    "    user_id=\"prod_user_001\",\n",
    "    session_id=\"session_prod_20241224\"\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ Production Pipeline Results:\")\n",
    "print(json.dumps(production_result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## ğŸ”§ Utility functions cho debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_url(trace_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Táº¡o URL Ä‘á»ƒ xem trace trÃªn LangFuse UI\n",
    "    \"\"\"\n",
    "    base_url = os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    "    return f\"{base_url}/trace/{trace_id}\"\n",
    "\n",
    "def flush_langfuse():\n",
    "    \"\"\"\n",
    "    Flush táº¥t cáº£ pending traces to LangFuse\n",
    "    \"\"\"\n",
    "    langfuse.flush()\n",
    "    print(\"âœ… All traces flushed to LangFuse!\")\n",
    "\n",
    "def create_debug_session(session_name: str):\n",
    "    \"\"\"\n",
    "    Táº¡o debug session vá»›i metadata\n",
    "    \"\"\"\n",
    "    return langfuse.trace(\n",
    "        name=f\"debug_session_{session_name}\",\n",
    "        metadata={\n",
    "            \"session_type\": \"debug\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"notebook\": \"02_LangFuse_Observations_Debugging\"\n",
    "        },\n",
    "        tags=[\"debug\", \"notebook\", session_name]\n",
    "    )\n",
    "\n",
    "# Flush táº¥t cáº£ traces\n",
    "flush_langfuse()\n",
    "\n",
    "print(\"ğŸ”§ Debug utilities ready!\")\n",
    "print(\"ğŸŒ LangFuse UI: http://localhost:3000\")\n",
    "print(\"ğŸ“Š Check your traces and observations in the UI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## ğŸ¯ Káº¿t luáº­n & BÆ°á»›c tiáº¿p theo\n",
    "\n",
    "### ğŸ“š Ná»™i dung Ä‘Ã£ há»c:\n",
    "\n",
    "1. **Quan sÃ¡t chi tiáº¿t**: Sá»­ dá»¥ng LangFuse Ä‘á»ƒ theo dÃµi tá»«ng bÆ°á»›c trong LLM pipeline\n",
    "2. **Error Detection**: PhÃ¡t hiá»‡n vÃ  debug lá»—i vá»›i detailed tracing\n",
    "3. **Performance Monitoring**: Äo lÆ°á»ng latency, token usage, vÃ  quality metrics\n",
    "4. **RAG Debugging**: Trace chi tiáº¿t cho retrieval vÃ  generation phases\n",
    "5. **Production Ready**: Táº¡o comprehensive traces cho production systems\n",
    "6. **Evaluation Scores**: TÃ­ch há»£p scoring vÃ  evaluation vÃ o traces\n",
    "\n",
    "### ğŸ” Key Benefits cá»§a LangFuse Debugging:\n",
    "\n",
    "- **ğŸ¯ Visibility**: NhÃ¬n tháº¥y toÃ n bá»™ luá»“ng hoáº¡t Ä‘á»™ng\n",
    "- **âš¡ Performance**: Tá»‘i Æ°u hÃ³a speed vÃ  cost\n",
    "- **ğŸ› Debug**: Nhanh chÃ³ng xÃ¡c Ä‘á»‹nh vÃ  sá»­a lá»—i\n",
    "- **ğŸ“Š Analytics**: PhÃ¢n tÃ­ch patterns vÃ  trends\n",
    "- **ğŸ”„ Iteration**: Cáº£i thiá»‡n liÃªn tá»¥c dá»±a trÃªn data\n",
    "\n",
    "### ğŸš€ BÆ°á»›c tiáº¿p theo:\n",
    "\n",
    "1. **Notebook 03**: LangFuse Evaluations & Prompts - ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng vÃ  quáº£n lÃ½ prompts\n",
    "2. **Advanced Tracing**: TÃ­ch há»£p vá»›i production systems\n",
    "3. **Custom Evaluations**: Táº¡o evaluation metrics phÃ¹ há»£p vá»›i use case\n",
    "4. **Monitoring Dashboards**: Setup alerting vÃ  monitoring\n",
    "5. **Team Collaboration**: Chia sáº» insights vÃ  debugging vá»›i team\n",
    "\n",
    "### ğŸ’¡ Best Practices:\n",
    "\n",
    "- LuÃ´n flush traces khi káº¿t thÃºc session\n",
    "- Sá»­ dá»¥ng meaningful names cho traces vÃ  spans\n",
    "- ThÃªm metadata vÃ  tags Ä‘á»ƒ dá»… filter vÃ  search\n",
    "- Táº¡o scores cho cÃ¡c metrics quan trá»ng\n",
    "- Regular review traces Ä‘á»ƒ identify improvement opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## ğŸ“ Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final flush Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ traces Ä‘Æ°á»£c gá»­i\n",
    "langfuse.flush()\n",
    "\n",
    "print(\"âœ… Notebook completed successfully!\")\n",
    "print(\"ğŸ” All traces have been sent to LangFuse\")\n",
    "print(\"ğŸŒ Check your LangFuse dashboard: http://localhost:3000\")\n",
    "print(\"ğŸ“Š Analyze your traces and observations for insights!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}