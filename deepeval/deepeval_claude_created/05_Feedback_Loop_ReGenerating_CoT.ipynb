{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: V√≤ng L·∫∑p Ph·∫£n H·ªìi & T√°i T·∫°o CoT\n",
    "\n",
    "**M·ª•c ti√™u**: X√¢y d·ª±ng v√≤ng l·∫∑p t·ª± ƒë·ªông c·∫£i thi·ªán d·ª±a tr√™n feedback t·ª´ DeepEval\n",
    "\n",
    "Trong notebook n√†y, ch√∫ng ta s·∫Ω h·ªçc c√°ch:\n",
    "1. X√¢y d·ª±ng automated feedback pipeline v·ªõi 5 giai ƒëo·∫°n\n",
    "2. Tri·ªÉn khai advanced feedback strategies\n",
    "3. T·∫°o pattern recognition cho failures\n",
    "4. ·ª®ng d·ª•ng th·ª±c ti·ªÖn v·ªõi real-world case studies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è C√†i ƒê·∫∑t v√† Import\n",
    "\n",
    "Tr∆∞·ªõc ti√™n, h√£y import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt v√† thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate, assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    HallucinationMetric,\n",
    "    GEval,\n",
    "    BaseMetric\n",
    ")\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ C√°c th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Thi·∫øt L·∫≠p API Keys\n",
    "\n",
    "ƒê·∫£m b·∫£o b·∫°n ƒë√£ thi·∫øt l·∫≠p c√°c API keys c·∫ßn thi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra API keys\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è Ch∆∞a thi·∫øt l·∫≠p OPENAI_API_KEY\")\n",
    "    print(\"H√£y thi·∫øt l·∫≠p: export OPENAI_API_KEY='your-api-key'\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Custom Metrics cho CoT Evaluation\n",
    "\n",
    "ƒê·∫ßu ti√™n, ch√∫ng ta s·∫Ω t·∫°o c√°c custom metrics ƒë·ªÉ ƒë√°nh gi√° Chain-of-Thought reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalFlowMetric(BaseMetric):\n",
    "    \"\"\"ƒê√°nh gi√° chu·ªói suy lu·∫≠n logic trong CoT\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.7):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        ƒê√°nh gi√° t√≠nh logic v√† m·∫°ch l·∫°c c·ªßa chu·ªói suy lu·∫≠n sau ƒë√¢y:\n",
    "        \n",
    "        C√¢u h·ªèi: {test_case.input}\n",
    "        Chu·ªói suy lu·∫≠n: {test_case.actual_output}\n",
    "        \n",
    "        Ti√™u ch√≠ ƒë√°nh gi√°:\n",
    "        1. T√≠nh logic: C√°c b∆∞·ªõc c√≥ theo th·ª© t·ª± h·ª£p l√Ω kh√¥ng?\n",
    "        2. T√≠nh m·∫°ch l·∫°c: C√≥ m·ªëi li√™n k·∫øt r√µ r√†ng gi·ªØa c√°c b∆∞·ªõc?\n",
    "        3. T√≠nh ƒë·∫ßy ƒë·ªß: C√≥ b·ªè qua b∆∞·ªõc quan tr·ªçng n√†o kh√¥ng?\n",
    "        4. T√≠nh ch√≠nh x√°c: K·∫øt lu·∫≠n c√≥ ph√π h·ª£p v·ªõi qu√° tr√¨nh suy lu·∫≠n?\n",
    "        \n",
    "        Cho ƒëi·ªÉm t·ª´ 0-10 (10 = ho√†n h·∫£o, 0 = ho√†n to√†n thi·∫øu logic).\n",
    "        Ch·ªâ tr·∫£ v·ªÅ s·ªë ƒëi·ªÉm, kh√¥ng gi·∫£i th√≠ch.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke(evaluation_prompt)\n",
    "        try:\n",
    "            score = float(response.content.strip()) / 10.0\n",
    "            self.score = score\n",
    "            self.reason = f\"Logic flow score: {score:.2f}\"\n",
    "            self.success = score >= self.threshold\n",
    "            return score\n",
    "        except ValueError:\n",
    "            self.score = 0.0\n",
    "            self.reason = \"Kh√¥ng th·ªÉ ƒë√°nh gi√° logic flow\"\n",
    "            self.success = False\n",
    "            return 0.0\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "class StepCompletenessMetric(BaseMetric):\n",
    "    \"\"\"ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√°c b∆∞·ªõc trong CoT\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.8):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        ƒê√°nh gi√° t√≠nh ƒë·∫ßy ƒë·ªß c·ªßa c√°c b∆∞·ªõc gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ:\n",
    "        \n",
    "        V·∫•n ƒë·ªÅ: {test_case.input}\n",
    "        C√°c b∆∞·ªõc ƒë∆∞·ª£c th·ª±c hi·ªán: {test_case.actual_output}\n",
    "        \n",
    "        Ti√™u ch√≠:\n",
    "        1. C√≥ x√°c ƒë·ªãnh r√µ v·∫•n ƒë·ªÅ?\n",
    "        2. C√≥ ph√¢n t√≠ch c√°c y·∫øu t·ªë li√™n quan?\n",
    "        3. C√≥ ƒë·ªÅ xu·∫•t gi·∫£i ph√°p c·ª• th·ªÉ?\n",
    "        4. C√≥ ki·ªÉm tra t√≠nh kh·∫£ thi?\n",
    "        5. C√≥ k·∫øt lu·∫≠n r√µ r√†ng?\n",
    "        \n",
    "        Cho ƒëi·ªÉm t·ª´ 0-10 d·ª±a tr√™n s·ªë b∆∞·ªõc thi·∫øt y·∫øu ƒë∆∞·ª£c th·ª±c hi·ªán.\n",
    "        Ch·ªâ tr·∫£ v·ªÅ s·ªë ƒëi·ªÉm.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke(evaluation_prompt)\n",
    "        try:\n",
    "            score = float(response.content.strip()) / 10.0\n",
    "            self.score = score\n",
    "            self.reason = f\"Step completeness: {score:.2f}\"\n",
    "            self.success = score >= self.threshold\n",
    "            return score\n",
    "        except ValueError:\n",
    "            self.score = 0.0\n",
    "            self.reason = \"Kh√¥ng th·ªÉ ƒë√°nh gi√° step completeness\"\n",
    "            self.success = False\n",
    "            return 0.0\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "print(\"‚úÖ Custom metrics ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Core Feedback Pipeline Classes\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω t·∫°o c√°c class c·ªët l√µi cho feedback pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeedbackResult:\n",
    "    \"\"\"L∆∞u tr·ªØ k·∫øt qu·∫£ c·ªßa m·ªôt l·∫ßn feedback\"\"\"\n",
    "    iteration: int\n",
    "    original_prompt: str\n",
    "    enhanced_prompt: str\n",
    "    original_output: str\n",
    "    enhanced_output: str\n",
    "    original_scores: Dict[str, float]\n",
    "    enhanced_scores: Dict[str, float]\n",
    "    improvement: Dict[str, float]\n",
    "    feedback_used: List[str]\n",
    "    timestamp: str\n",
    "    success: bool\n",
    "\n",
    "@dataclass\n",
    "class FailurePattern:\n",
    "    \"\"\"M√¥ t·∫£ pattern c·ªßa failure\"\"\"\n",
    "    pattern_type: str\n",
    "    description: str\n",
    "    frequency: int\n",
    "    suggested_fix: str\n",
    "    examples: List[str]\n",
    "\n",
    "class FeedbackExtractor:\n",
    "    \"\"\"Tr√≠ch xu·∫•t feedback t·ª´ failed evaluations\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def extract_feedback(self, test_case: LLMTestCase, failed_metrics: List[BaseMetric]) -> List[str]:\n",
    "        \"\"\"Tr√≠ch xu·∫•t feedback t·ª´ c√°c metrics b·ªã fail\"\"\"\n",
    "        feedback_list = []\n",
    "        \n",
    "        for metric in failed_metrics:\n",
    "            if hasattr(metric, 'reason') and metric.reason:\n",
    "                # T·∫°o feedback chi ti·∫øt h∆°n\n",
    "                detailed_feedback = self._generate_detailed_feedback(\n",
    "                    test_case, metric\n",
    "                )\n",
    "                feedback_list.append(detailed_feedback)\n",
    "        \n",
    "        return feedback_list\n",
    "    \n",
    "    def _generate_detailed_feedback(self, test_case: LLMTestCase, metric: BaseMetric) -> str:\n",
    "        \"\"\"T·∫°o feedback chi ti·∫øt cho metric c·ª• th·ªÉ\"\"\"\n",
    "        feedback_prompt = f\"\"\"\n",
    "        M·ªôt AI agent ƒë√£ th·∫•t b·∫°i trong ƒë√°nh gi√° sau:\n",
    "        \n",
    "        Input: {test_case.input}\n",
    "        Output: {test_case.actual_output}\n",
    "        Metric: {metric.__class__.__name__}\n",
    "        Reason: {getattr(metric, 'reason', 'Unknown')}\n",
    "        Score: {getattr(metric, 'score', 'Unknown')}\n",
    "        \n",
    "        H√£y ph√¢n t√≠ch v·∫•n ƒë·ªÅ v√† ƒë∆∞a ra feedback c·ª• th·ªÉ ƒë·ªÉ c·∫£i thi·ªán:\n",
    "        1. V·∫•n ƒë·ªÅ ch√≠nh l√† g√¨?\n",
    "        2. L√†m th·∫ø n√†o ƒë·ªÉ s·ª≠a?\n",
    "        3. V√≠ d·ª• c·ª• th·ªÉ v·ªÅ c√°ch c·∫£i thi·ªán?\n",
    "        \n",
    "        Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·∫≠p trung v√†o h√†nh ƒë·ªông c·ª• th·ªÉ.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(feedback_prompt)\n",
    "        return response.content.strip()\n",
    "\n",
    "class PromptEnhancer:\n",
    "    \"\"\"C·∫£i thi·ªán prompt d·ª±a tr√™n feedback\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def enhance_prompt(self, original_prompt: str, feedback_list: List[str]) -> str:\n",
    "        \"\"\"C·∫£i thi·ªán prompt d·ª±a tr√™n feedback\"\"\"\n",
    "        enhancement_prompt = f\"\"\"\n",
    "        B·∫°n l√† m·ªôt chuy√™n gia prompt engineering. H√£y c·∫£i thi·ªán prompt sau d·ª±a tr√™n feedback:\n",
    "        \n",
    "        PROMPT G·ªêC:\n",
    "        {original_prompt}\n",
    "        \n",
    "        FEEDBACK:\n",
    "        {chr(10).join(f\"- {fb}\" for fb in feedback_list)}\n",
    "        \n",
    "        H√£y t·∫°o prompt m·ªõi:\n",
    "        1. Gi·ªØ nguy√™n √Ω nghƒ©a ch√≠nh\n",
    "        2. T√≠ch h·ª£p feedback ƒë·ªÉ kh·∫Øc ph·ª•c v·∫•n ƒë·ªÅ\n",
    "        3. Th√™m h∆∞·ªõng d·∫´n c·ª• th·ªÉ cho Chain-of-Thought\n",
    "        4. ƒê·∫£m b·∫£o prompt r√µ r√†ng v√† c√≥ c·∫•u tr√∫c\n",
    "        \n",
    "        CH·ªà TR·∫¢ V·ªÄ PROMPT M·ªöI, KH√îNG GI·∫¢I TH√çCH.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(enhancement_prompt)\n",
    "        return response.content.strip()\n",
    "\n",
    "print(\"‚úÖ Feedback pipeline classes ƒë√£ ƒë∆∞·ª£c t·∫°o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Automated Feedback Pipeline\n",
    "\n",
    "ƒê√¢y l√† ph·∫ßn c·ªët l√µi - pipeline t·ª± ƒë·ªông v·ªõi 5 giai ƒëo·∫°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatedFeedbackPipeline:\n",
    "    \"\"\"Pipeline t·ª± ƒë·ªông cho feedback loop\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_iterations: int = 3):\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.feedback_extractor = FeedbackExtractor(llm)\n",
    "        self.prompt_enhancer = PromptEnhancer(llm)\n",
    "        self.results: List[FeedbackResult] = []\n",
    "        self.failure_patterns: List[FailurePattern] = []\n",
    "    \n",
    "    def run_pipeline(self, \n",
    "                    initial_prompt: str, \n",
    "                    test_input: str,\n",
    "                    metrics: List[BaseMetric],\n",
    "                    context: Optional[str] = None) -> FeedbackResult:\n",
    "        \"\"\"Ch·∫°y to√†n b·ªô pipeline feedback\"\"\"\n",
    "        \n",
    "        current_prompt = initial_prompt\n",
    "        iteration = 0\n",
    "        \n",
    "        print(f\"üöÄ B·∫Øt ƒë·∫ßu Automated Feedback Pipeline\")\n",
    "        print(f\"üìù Test input: {test_input[:100]}...\")\n",
    "        print(f\"üìä S·ªë metrics: {len(metrics)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        while iteration < self.max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"\\nüîÑ ITERATION {iteration}\")\n",
    "            \n",
    "            # Phase 1: Execution\n",
    "            print(\"1Ô∏è‚É£ Execution Phase...\")\n",
    "            output = self._execute_prompt(current_prompt, test_input)\n",
    "            \n",
    "            # Phase 2: Evaluation\n",
    "            print(\"2Ô∏è‚É£ Evaluation Phase...\")\n",
    "            test_case = LLMTestCase(\n",
    "                input=test_input,\n",
    "                actual_output=output,\n",
    "                context=[context] if context else None\n",
    "            )\n",
    "            \n",
    "            scores, failed_metrics = self._evaluate_test_case(test_case, metrics)\n",
    "            \n",
    "            # Ki·ªÉm tra n·∫øu ƒë√£ pass t·∫•t c·∫£ metrics\n",
    "            if not failed_metrics:\n",
    "                print(\"‚úÖ T·∫•t c·∫£ metrics ƒë√£ pass!\")\n",
    "                success_result = FeedbackResult(\n",
    "                    iteration=iteration,\n",
    "                    original_prompt=initial_prompt,\n",
    "                    enhanced_prompt=current_prompt,\n",
    "                    original_output=\"N/A\",\n",
    "                    enhanced_output=output,\n",
    "                    original_scores={},\n",
    "                    enhanced_scores=scores,\n",
    "                    improvement=scores,\n",
    "                    feedback_used=[],\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    success=True\n",
    "                )\n",
    "                self.results.append(success_result)\n",
    "                return success_result\n",
    "            \n",
    "            # Phase 3: Feedback Extraction\n",
    "            print(\"3Ô∏è‚É£ Feedback Extraction Phase...\")\n",
    "            feedback_list = self.feedback_extractor.extract_feedback(test_case, failed_metrics)\n",
    "            print(f\"   üìã Extracted {len(feedback_list)} feedback items\")\n",
    "            \n",
    "            # Phase 4: Prompt Enhancement\n",
    "            print(\"4Ô∏è‚É£ Prompt Enhancement Phase...\")\n",
    "            enhanced_prompt = self.prompt_enhancer.enhance_prompt(current_prompt, feedback_list)\n",
    "            \n",
    "            # Phase 5: Re-execution\n",
    "            print(\"5Ô∏è‚É£ Re-execution Phase...\")\n",
    "            enhanced_output = self._execute_prompt(enhanced_prompt, test_input)\n",
    "            \n",
    "            # ƒê√°nh gi√° output m·ªõi\n",
    "            enhanced_test_case = LLMTestCase(\n",
    "                input=test_input,\n",
    "                actual_output=enhanced_output,\n",
    "                context=[context] if context else None\n",
    "            )\n",
    "            \n",
    "            enhanced_scores, enhanced_failed = self._evaluate_test_case(enhanced_test_case, metrics)\n",
    "            \n",
    "            # T√≠nh improvement\n",
    "            improvement = self._calculate_improvement(scores, enhanced_scores)\n",
    "            \n",
    "            # L∆∞u k·∫øt qu·∫£\n",
    "            result = FeedbackResult(\n",
    "                iteration=iteration,\n",
    "                original_prompt=current_prompt if iteration == 1 else initial_prompt,\n",
    "                enhanced_prompt=enhanced_prompt,\n",
    "                original_output=output,\n",
    "                enhanced_output=enhanced_output,\n",
    "                original_scores=scores,\n",
    "                enhanced_scores=enhanced_scores,\n",
    "                improvement=improvement,\n",
    "                feedback_used=feedback_list,\n",
    "                timestamp=datetime.now().isoformat(),\n",
    "                success=len(enhanced_failed) == 0\n",
    "            )\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            # In k·∫øt qu·∫£ iteration\n",
    "            self._print_iteration_summary(result)\n",
    "            \n",
    "            # Update prompt cho iteration ti·∫øp theo\n",
    "            current_prompt = enhanced_prompt\n",
    "            \n",
    "            if result.success:\n",
    "                print(\"‚úÖ Pipeline ho√†n th√†nh th√†nh c√¥ng!\")\n",
    "                return result\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è ƒê√£ ƒë·∫°t max iterations ({self.max_iterations})\")\n",
    "        return self.results[-1] if self.results else None\n",
    "    \n",
    "    def _execute_prompt(self, prompt: str, test_input: str) -> str:\n",
    "        \"\"\"Th·ª±c hi·ªán prompt v·ªõi input\"\"\"\n",
    "        full_prompt = f\"{prompt}\\n\\nInput: {test_input}\\n\\nOutput:\"\n",
    "        response = self.llm.invoke(full_prompt)\n",
    "        return response.content.strip()\n",
    "    \n",
    "    def _evaluate_test_case(self, test_case: LLMTestCase, metrics: List[BaseMetric]) -> Tuple[Dict[str, float], List[BaseMetric]]:\n",
    "        \"\"\"ƒê√°nh gi√° test case v·ªõi c√°c metrics\"\"\"\n",
    "        scores = {}\n",
    "        failed_metrics = []\n",
    "        \n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                score = metric.measure(test_case)\n",
    "                scores[metric.__class__.__name__] = score\n",
    "                \n",
    "                if not metric.is_successful():\n",
    "                    failed_metrics.append(metric)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating {metric.__class__.__name__}: {e}\")\n",
    "                scores[metric.__class__.__name__] = 0.0\n",
    "                failed_metrics.append(metric)\n",
    "        \n",
    "        return scores, failed_metrics\n",
    "    \n",
    "    def _calculate_improvement(self, original_scores: Dict[str, float], enhanced_scores: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"T√≠nh to√°n m·ª©c c·∫£i thi·ªán\"\"\"\n",
    "        improvement = {}\n",
    "        for metric_name in original_scores:\n",
    "            if metric_name in enhanced_scores:\n",
    "                improvement[metric_name] = enhanced_scores[metric_name] - original_scores[metric_name]\n",
    "        return improvement\n",
    "    \n",
    "    def _print_iteration_summary(self, result: FeedbackResult):\n",
    "        \"\"\"In t√≥m t·∫Øt iteration\"\"\"\n",
    "        print(f\"\\nüìä ITERATION {result.iteration} SUMMARY:\")\n",
    "        print(f\"   ‚úÖ Success: {result.success}\")\n",
    "        \n",
    "        print(\"   üìà Improvements:\")\n",
    "        for metric, improvement in result.improvement.items():\n",
    "            direction = \"üìà\" if improvement > 0 else \"üìâ\" if improvement < 0 else \"‚û°Ô∏è\"\n",
    "            print(f\"      {direction} {metric}: {improvement:+.3f}\")\n",
    "        \n",
    "        print(f\"   üí¨ Feedback items: {len(result.feedback_used)}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"‚úÖ Automated Feedback Pipeline ƒë√£ ƒë∆∞·ª£c t·∫°o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Th·ª±c H√†nh: Complex Problem Solving v·ªõi Feedback Loop\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω test pipeline v·ªõi m·ªôt b√†i to√°n ph·ª©c t·∫°p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o test case ph·ª©c t·∫°p\n",
    "complex_problem = \"\"\"\n",
    "M·ªôt c√¥ng ty c√¥ng ngh·ªá c√≥ 200 nh√¢n vi√™n ƒëang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ hi·ªáu su·∫•t l√†m vi·ªác t·ª´ xa. \n",
    "Kh·∫£o s√°t cho th·∫•y: 60% nh√¢n vi√™n c·∫£m th·∫•y thi·∫øu k·∫øt n·ªëi v·ªõi ƒë·ªìng nghi·ªáp, \n",
    "45% b√°o c√°o kh√≥ khƒÉn trong vi·ªác qu·∫£n l√Ω th·ªùi gian, v√† 30% n√≥i r·∫±ng h·ªç thi·∫øu \n",
    "ƒë·ªông l·ª±c. Chi ph√≠ v·∫≠n h√†nh vƒÉn ph√≤ng ƒë√£ gi·∫£m 40% nh∆∞ng doanh thu gi·∫£m 15%.\n",
    "\n",
    "H√£y ph√¢n t√≠ch v·∫•n ƒë·ªÅ v√† ƒë·ªÅ xu·∫•t gi·∫£i ph√°p to√†n di·ªán.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt ban ƒë·∫ßu (c·ªë t√¨nh l√†m ƒë∆°n gi·∫£n ƒë·ªÉ test feedback)\n",
    "initial_prompt = \"\"\"\n",
    "B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n doanh nghi·ªáp. H√£y ph√¢n t√≠ch v·∫•n ƒë·ªÅ v√† ƒë∆∞a ra gi·∫£i ph√°p.\n",
    "Suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt c√°ch logic.\n",
    "\"\"\"\n",
    "\n",
    "# T·∫°o metrics ƒë·ªÉ ƒë√°nh gi√°\n",
    "metrics = [\n",
    "    LogicalFlowMetric(threshold=0.7),\n",
    "    StepCompletenessMetric(threshold=0.8),\n",
    "    AnswerRelevancyMetric(threshold=0.8)\n",
    "]\n",
    "\n",
    "print(\"üéØ Test case ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
    "print(f\"Problem: {complex_problem[:100]}...\")\n",
    "print(f\"Initial prompt: {initial_prompt[:100]}...\")\n",
    "print(f\"Metrics: {[m.__class__.__name__ for m in metrics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y Feedback Pipeline\n",
    "pipeline = AutomatedFeedbackPipeline(llm, max_iterations=3)\n",
    "\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu ch·∫°y Automated Feedback Pipeline...\\n\")\n",
    "\n",
    "final_result = pipeline.run_pipeline(\n",
    "    initial_prompt=initial_prompt,\n",
    "    test_input=complex_problem,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÅ PIPELINE HO√ÄN TH√ÄNH!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ph√¢n T√≠ch K·∫øt Qu·∫£ Pipeline\n",
    "\n",
    "H√£y ph√¢n t√≠ch chi ti·∫øt k·∫øt qu·∫£ c·ªßa pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch k·∫øt qu·∫£ chi ti·∫øt\n",
    "def analyze_pipeline_results(pipeline: AutomatedFeedbackPipeline):\n",
    "    \"\"\"Ph√¢n t√≠ch chi ti·∫øt k·∫øt qu·∫£ pipeline\"\"\"\n",
    "    \n",
    "    print(\"üìä PH√ÇN T√çCH K·∫æT QU·∫¢ PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not pipeline.results:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch\")\n",
    "        return\n",
    "    \n",
    "    # T·ªïng quan\n",
    "    total_iterations = len(pipeline.results)\n",
    "    final_success = pipeline.results[-1].success\n",
    "    \n",
    "    print(f\"üî¢ T·ªïng s·ªë iterations: {total_iterations}\")\n",
    "    print(f\"‚úÖ K·∫øt qu·∫£ cu·ªëi: {'Th√†nh c√¥ng' if final_success else 'Th·∫•t b·∫°i'}\")\n",
    "    print()\n",
    "    \n",
    "    # Ph√¢n t√≠ch t·ª´ng iteration\n",
    "    for i, result in enumerate(pipeline.results, 1):\n",
    "        print(f\"üîÑ ITERATION {i}:\")\n",
    "        print(f\"   üìÖ Timestamp: {result.timestamp}\")\n",
    "        print(f\"   ‚úÖ Success: {result.success}\")\n",
    "        \n",
    "        # Scores\n",
    "        print(\"   üìä Scores:\")\n",
    "        if result.original_scores:\n",
    "            for metric, score in result.original_scores.items():\n",
    "                enhanced_score = result.enhanced_scores.get(metric, score)\n",
    "                improvement = result.improvement.get(metric, 0)\n",
    "                print(f\"      {metric}: {score:.3f} ‚Üí {enhanced_score:.3f} ({improvement:+.3f})\")\n",
    "        else:\n",
    "            for metric, score in result.enhanced_scores.items():\n",
    "                print(f\"      {metric}: {score:.3f}\")\n",
    "        \n",
    "        # Feedback s·ª≠ d·ª•ng\n",
    "        print(f\"   üí¨ Feedback items: {len(result.feedback_used)}\")\n",
    "        for j, feedback in enumerate(result.feedback_used[:2], 1):  # Ch·ªâ hi·ªÉn th·ªã 2 feedback ƒë·∫ßu\n",
    "            print(f\"      {j}. {feedback[:100]}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Ph√¢n t√≠ch improvement t·ªïng th·ªÉ\n",
    "    if len(pipeline.results) > 1:\n",
    "        print(\"üìà T·ªîNG K·∫æT IMPROVEMENT:\")\n",
    "        first_result = pipeline.results[0]\n",
    "        last_result = pipeline.results[-1]\n",
    "        \n",
    "        for metric in first_result.enhanced_scores:\n",
    "            if metric in last_result.enhanced_scores:\n",
    "                initial_score = first_result.original_scores.get(metric, first_result.enhanced_scores[metric])\n",
    "                final_score = last_result.enhanced_scores[metric]\n",
    "                total_improvement = final_score - initial_score\n",
    "                print(f\"   {metric}: {initial_score:.3f} ‚Üí {final_score:.3f} ({total_improvement:+.3f})\")\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch\n",
    "analyze_pipeline_results(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã So S√°nh Prompt v√† Output\n",
    "\n",
    "H√£y so s√°nh prompt v√† output ban ƒë·∫ßu v·ªõi k·∫øt qu·∫£ cu·ªëi c√πng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh prompt v√† output\n",
    "def compare_prompts_and_outputs(pipeline: AutomatedFeedbackPipeline):\n",
    "    \"\"\"So s√°nh prompt v√† output qua c√°c iterations\"\"\"\n",
    "    \n",
    "    if not pipeline.results:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ so s√°nh\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç SO S√ÅNH PROMPT V√Ä OUTPUT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prompt evolution\n",
    "    print(\"üìù EVOLUTION C·ª¶A PROMPT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, result in enumerate(pipeline.results, 1):\n",
    "        print(f\"\\nüîÑ ITERATION {i} PROMPT:\")\n",
    "        prompt_to_show = result.enhanced_prompt if i == 1 else result.enhanced_prompt\n",
    "        print(f\"   {prompt_to_show}\")\n",
    "        print()\n",
    "    \n",
    "    # Output comparison\n",
    "    print(\"\\nüìÑ SO S√ÅNH OUTPUT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, result in enumerate(pipeline.results, 1):\n",
    "        print(f\"\\nüîÑ ITERATION {i}:\")\n",
    "        \n",
    "        if result.original_output != \"N/A\":\n",
    "            print(f\"   üì§ Original Output ({len(result.original_output)} chars):\")\n",
    "            print(f\"   {result.original_output[:200]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"   üì• Enhanced Output ({len(result.enhanced_output)} chars):\")\n",
    "        print(f\"   {result.enhanced_output[:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Final comparison\n",
    "    if len(pipeline.results) > 1:\n",
    "        first_result = pipeline.results[0]\n",
    "        last_result = pipeline.results[-1]\n",
    "        \n",
    "        print(\"\\nüèÅ FINAL COMPARISON:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        initial_output = first_result.original_output if first_result.original_output != \"N/A\" else first_result.enhanced_output\n",
    "        final_output = last_result.enhanced_output\n",
    "        \n",
    "        print(f\"üìä Length comparison: {len(initial_output)} ‚Üí {len(final_output)} chars\")\n",
    "        print(f\"üìà Length change: {len(final_output) - len(initial_output):+d} chars\")\n",
    "\n",
    "# Ch·∫°y so s√°nh\n",
    "compare_prompts_and_outputs(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Advanced Feedback Strategies\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω tri·ªÉn khai c√°c strategies n√¢ng cao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedFeedbackAnalyzer:\n",
    "    \"\"\"Ph√¢n t√≠ch feedback n√¢ng cao v·ªõi pattern recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.failure_patterns = defaultdict(list)\n",
    "        self.improvement_history = []\n",
    "    \n",
    "    def analyze_failure_patterns(self, pipeline_results: List[FeedbackResult]) -> List[FailurePattern]:\n",
    "        \"\"\"Ph√¢n t√≠ch patterns c·ªßa failures\"\"\"\n",
    "        \n",
    "        print(\"üîç ANALYZING FAILURE PATTERNS...\")\n",
    "        \n",
    "        # Collect all feedback\n",
    "        all_feedback = []\n",
    "        for result in pipeline_results:\n",
    "            all_feedback.extend(result.feedback_used)\n",
    "        \n",
    "        if not all_feedback:\n",
    "            print(\"   ‚ÑπÔ∏è Kh√¥ng c√≥ feedback ƒë·ªÉ ph√¢n t√≠ch\")\n",
    "            return []\n",
    "        \n",
    "        # Categorize feedback\n",
    "        categories = self._categorize_feedback(all_feedback)\n",
    "        \n",
    "        # Create failure patterns\n",
    "        patterns = []\n",
    "        for category, feedback_items in categories.items():\n",
    "            if len(feedback_items) > 1:  # Pattern n·∫øu xu·∫•t hi·ªán > 1 l·∫ßn\n",
    "                pattern = FailurePattern(\n",
    "                    pattern_type=category,\n",
    "                    description=f\"Recurring issue with {category.lower()}\",\n",
    "                    frequency=len(feedback_items),\n",
    "                    suggested_fix=self._generate_pattern_fix(category, feedback_items),\n",
    "                    examples=feedback_items[:3]  # Top 3 examples\n",
    "                )\n",
    "                patterns.append(pattern)\n",
    "        \n",
    "        # Print patterns\n",
    "        self._print_failure_patterns(patterns)\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _categorize_feedback(self, feedback_list: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Ph√¢n lo·∫°i feedback theo categories\"\"\"\n",
    "        \n",
    "        categories = defaultdict(list)\n",
    "        \n",
    "        # Define keywords for categories\n",
    "        category_keywords = {\n",
    "            \"Logic Flow\": [\"logic\", \"flow\", \"sequence\", \"order\", \"reasoning\", \"step\"],\n",
    "            \"Completeness\": [\"missing\", \"incomplete\", \"lack\", \"absent\", \"need more\"],\n",
    "            \"Relevance\": [\"relevant\", \"related\", \"connection\", \"context\", \"topic\"],\n",
    "            \"Structure\": [\"structure\", \"organization\", \"format\", \"layout\", \"arrange\"],\n",
    "            \"Clarity\": [\"clear\", \"unclear\", \"confusing\", \"ambiguous\", \"vague\"]\n",
    "        }\n",
    "        \n",
    "        for feedback in feedback_list:\n",
    "            feedback_lower = feedback.lower()\n",
    "            categorized = False\n",
    "            \n",
    "            for category, keywords in category_keywords.items():\n",
    "                if any(keyword in feedback_lower for keyword in keywords):\n",
    "                    categories[category].append(feedback)\n",
    "                    categorized = True\n",
    "                    break\n",
    "            \n",
    "            if not categorized:\n",
    "                categories[\"Other\"].append(feedback)\n",
    "        \n",
    "        return dict(categories)\n",
    "    \n",
    "    def _generate_pattern_fix(self, category: str, feedback_items: List[str]) -> str:\n",
    "        \"\"\"T·∫°o suggested fix cho pattern\"\"\"\n",
    "        \n",
    "        fix_templates = {\n",
    "            \"Logic Flow\": \"Add explicit step-by-step reasoning with clear transitions\",\n",
    "            \"Completeness\": \"Include comprehensive analysis covering all aspects\",\n",
    "            \"Relevance\": \"Ensure all points directly address the main question\",\n",
    "            \"Structure\": \"Use clear headings and organized sections\",\n",
    "            \"Clarity\": \"Simplify language and provide clear explanations\"\n",
    "        }\n",
    "        \n",
    "        return fix_templates.get(category, \"Review and improve based on feedback\")\n",
    "    \n",
    "    def _print_failure_patterns(self, patterns: List[FailurePattern]):\n",
    "        \"\"\"In failure patterns\"\"\"\n",
    "        \n",
    "        if not patterns:\n",
    "            print(\"   ‚úÖ Kh√¥ng ph√°t hi·ªán failure patterns l·∫∑p l·∫°i\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   üîç Ph√°t hi·ªán {len(patterns)} failure patterns:\")\n",
    "        print()\n",
    "        \n",
    "        for i, pattern in enumerate(patterns, 1):\n",
    "            print(f\"   {i}. üìä {pattern.pattern_type}\")\n",
    "            print(f\"      üìà Frequency: {pattern.frequency}\")\n",
    "            print(f\"      üìã Description: {pattern.description}\")\n",
    "            print(f\"      üí° Suggested fix: {pattern.suggested_fix}\")\n",
    "            print(f\"      üìù Example: {pattern.examples[0][:100]}...\")\n",
    "            print()\n",
    "    \n",
    "    def generate_meta_feedback(self, pipeline_results: List[FeedbackResult]) -> str:\n",
    "        \"\"\"T·∫°o meta-feedback v·ªÅ to√†n b·ªô pipeline\"\"\"\n",
    "        \n",
    "        if not pipeline_results:\n",
    "            return \"No results to analyze\"\n",
    "        \n",
    "        meta_prompt = f\"\"\"\n",
    "        Ph√¢n t√≠ch to√†n b·ªô qu√° tr√¨nh feedback loop v·ªõi {len(pipeline_results)} iterations:\n",
    "        \n",
    "        RESULTS SUMMARY:\n",
    "        {self._format_results_summary(pipeline_results)}\n",
    "        \n",
    "        H√£y ƒë∆∞a ra meta-feedback v·ªÅ:\n",
    "        1. Hi·ªáu qu·∫£ c·ªßa feedback loop\n",
    "        2. Patterns trong qu√° tr√¨nh c·∫£i thi·ªán\n",
    "        3. ƒêi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu\n",
    "        4. ƒê·ªÅ xu·∫•t ƒë·ªÉ t·ªëi ∆∞u pipeline\n",
    "        \n",
    "        Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·∫≠p trung v√†o insights h·ªØu √≠ch.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(meta_prompt)\n",
    "        return response.content.strip()\n",
    "    \n",
    "    def _format_results_summary(self, results: List[FeedbackResult]) -> str:\n",
    "        \"\"\"Format t√≥m t·∫Øt k·∫øt qu·∫£\"\"\"\n",
    "        summary_lines = []\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            success_status = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "            avg_improvement = np.mean(list(result.improvement.values())) if result.improvement else 0\n",
    "            summary_lines.append(\n",
    "                f\"Iteration {i}: {success_status} Success={result.success}, \"\n",
    "                f\"Avg_Improvement={avg_improvement:.3f}, \"\n",
    "                f\"Feedback_Count={len(result.feedback_used)}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\".join(summary_lines)\n",
    "\n",
    "print(\"‚úÖ Advanced Feedback Analyzer ƒë√£ ƒë∆∞·ª£c t·∫°o!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y advanced analysis\n",
    "advanced_analyzer = AdvancedFeedbackAnalyzer(llm)\n",
    "\n",
    "print(\"üîç ADVANCED FEEDBACK ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Ph√¢n t√≠ch failure patterns\n",
    "failure_patterns = advanced_analyzer.analyze_failure_patterns(pipeline.results)\n",
    "\n",
    "print(\"\\nüß† META-FEEDBACK:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# T·∫°o meta-feedback\n",
    "meta_feedback = advanced_analyzer.generate_meta_feedback(pipeline.results)\n",
    "print(meta_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Tracking v√† Visualization\n",
    "\n",
    "T·∫°o dashboard ƒë·ªÉ theo d√µi performance qua c√°c iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "\n",
    "def create_performance_dashboard(pipeline_results: List[FeedbackResult]):\n",
    "    \"\"\"T·∫°o dashboard hi·ªÉn th·ªã performance\"\"\"\n",
    "    \n",
    "    if not pipeline_results:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ visualize\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    iterations = list(range(1, len(pipeline_results) + 1))\n",
    "    \n",
    "    # Collect metrics data\n",
    "    metrics_data = defaultdict(list)\n",
    "    success_data = []\n",
    "    \n",
    "    for result in pipeline_results:\n",
    "        # Use enhanced scores (final scores for each iteration)\n",
    "        for metric_name, score in result.enhanced_scores.items():\n",
    "            metrics_data[metric_name].append(score)\n",
    "        \n",
    "        success_data.append(1 if result.success else 0)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('üöÄ Feedback Pipeline Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Metrics scores over iterations\n",
    "    ax1 = axes[0, 0]\n",
    "    for metric_name, scores in metrics_data.items():\n",
    "        if len(scores) == len(iterations):\n",
    "            ax1.plot(iterations, scores, marker='o', linewidth=2, label=metric_name)\n",
    "    \n",
    "    ax1.set_title('üìà Metrics Scores Over Iterations')\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Success rate\n",
    "    ax2 = axes[0, 1]\n",
    "    colors = ['red' if s == 0 else 'green' for s in success_data]\n",
    "    ax2.bar(iterations, success_data, color=colors, alpha=0.7)\n",
    "    ax2.set_title('‚úÖ Success Rate by Iteration')\n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Success (1) / Failure (0)')\n",
    "    ax2.set_ylim(0, 1.2)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Improvement trends\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    improvement_data = defaultdict(list)\n",
    "    for result in pipeline_results:\n",
    "        for metric_name, improvement in result.improvement.items():\n",
    "            improvement_data[metric_name].append(improvement)\n",
    "    \n",
    "    for metric_name, improvements in improvement_data.items():\n",
    "        if len(improvements) == len(iterations):\n",
    "            ax3.bar([i + 0.1 * list(improvement_data.keys()).index(metric_name) for i in iterations], \n",
    "                   improvements, width=0.15, label=metric_name, alpha=0.8)\n",
    "    \n",
    "    ax3.set_title('üìä Improvement per Iteration')\n",
    "    ax3.set_xlabel('Iteration')\n",
    "    ax3.set_ylabel('Improvement Score')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 4: Feedback count per iteration\n",
    "    ax4 = axes[1, 1]\n",
    "    feedback_counts = [len(result.feedback_used) for result in pipeline_results]\n",
    "    ax4.bar(iterations, feedback_counts, color='purple', alpha=0.7)\n",
    "    ax4.set_title('üí¨ Feedback Items per Iteration')\n",
    "    ax4.set_xlabel('Iteration')\n",
    "    ax4.set_ylabel('Number of Feedback Items')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    final_success_rate = sum(success_data) / len(success_data)\n",
    "    print(f\"‚úÖ Overall success rate: {final_success_rate:.1%}\")\n",
    "    \n",
    "    if metrics_data:\n",
    "        print(\"\\nüìà Final Metrics Scores:\")\n",
    "        for metric_name, scores in metrics_data.items():\n",
    "            if scores:\n",
    "                initial_score = scores[0]\n",
    "                final_score = scores[-1]\n",
    "                improvement = final_score - initial_score\n",
    "                print(f\"   {metric_name}: {initial_score:.3f} ‚Üí {final_score:.3f} ({improvement:+.3f})\")\n",
    "    \n",
    "    total_feedback = sum(feedback_counts)\n",
    "    avg_feedback = total_feedback / len(feedback_counts) if feedback_counts else 0\n",
    "    print(f\"\\nüí¨ Total feedback items: {total_feedback}\")\n",
    "    print(f\"üí¨ Average feedback per iteration: {avg_feedback:.1f}\")\n",
    "\n",
    "# T·∫°o dashboard\n",
    "create_performance_dashboard(pipeline.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Real-World Case Study: Code Review Agent\n",
    "\n",
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω √°p d·ª•ng feedback loop cho m·ªôt use case th·ª±c t·∫ø - Code Review Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world case study: Code Review Agent\n",
    "code_review_problem = \"\"\"\n",
    "H√£y review ƒëo·∫°n code Python sau v√† ƒë∆∞a ra feedback chi ti·∫øt:\n",
    "\n",
    "```python\n",
    "def process_user_data(data):\n",
    "    users = []\n",
    "    for item in data:\n",
    "        if item['age'] > 18:\n",
    "            user = {\n",
    "                'name': item['name'],\n",
    "                'email': item['email'],\n",
    "                'age': item['age']\n",
    "            }\n",
    "            users.append(user)\n",
    "    return users\n",
    "\n",
    "def send_emails(users):\n",
    "    import smtplib\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.login('admin@company.com', 'password123')\n",
    "    \n",
    "    for user in users:\n",
    "        message = f\"Hello {user['name']}, welcome!\"\n",
    "        server.sendmail('admin@company.com', user['email'], message)\n",
    "    \n",
    "    server.quit()\n",
    "```\n",
    "\n",
    "T·∫≠p trung v√†o: security issues, error handling, code quality, performance.\n",
    "\"\"\"\n",
    "\n",
    "# Code review prompt (c·ªë t√¨nh ƒë∆°n gi·∫£n)\n",
    "code_review_initial_prompt = \"\"\"\n",
    "B·∫°n l√† m·ªôt senior developer. H√£y review code v√† ƒë∆∞a ra feedback.\n",
    "H√£y suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt c√°ch c√≥ h·ªá th·ªëng.\n",
    "\"\"\"\n",
    "\n",
    "# Custom metrics cho code review\n",
    "class SecurityAnalysisMetric(BaseMetric):\n",
    "    \"\"\"ƒê√°nh gi√° kh·∫£ nƒÉng ph√¢n t√≠ch security issues\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.8):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        security_keywords = [\n",
    "            'password', 'hardcoded', 'credential', 'authentication',\n",
    "            'security', 'vulnerability', 'sensitive', 'encryption',\n",
    "            'sql injection', 'xss', 'csrf', 'validation'\n",
    "        ]\n",
    "        \n",
    "        output_lower = test_case.actual_output.lower()\n",
    "        security_mentions = sum(1 for keyword in security_keywords if keyword in output_lower)\n",
    "        \n",
    "        # Check if mentions hardcoded password specifically\n",
    "        has_password_issue = 'password123' in test_case.actual_output or 'hardcoded' in output_lower\n",
    "        \n",
    "        # Score based on security awareness\n",
    "        score = min(1.0, (security_mentions * 0.2) + (0.5 if has_password_issue else 0))\n",
    "        \n",
    "        self.score = score\n",
    "        self.reason = f\"Security analysis score: {score:.2f} (mentions: {security_mentions})\"  \n",
    "        self.success = score >= self.threshold\n",
    "        return score\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "class ErrorHandlingMetric(BaseMetric):\n",
    "    \"\"\"ƒê√°nh gi√° kh·∫£ nƒÉng ph√°t hi·ªán thi·∫øu error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.7):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        error_keywords = [\n",
    "            'try', 'except', 'error', 'exception', 'handle',\n",
    "            'validation', 'check', 'null', 'none', 'empty'\n",
    "        ]\n",
    "        \n",
    "        output_lower = test_case.actual_output.lower()\n",
    "        error_mentions = sum(1 for keyword in error_keywords if keyword in output_lower)\n",
    "        \n",
    "        # Specific issues that should be caught\n",
    "        catches_key_error = 'keyerror' in output_lower or 'key error' in output_lower\n",
    "        mentions_validation = 'validation' in output_lower or 'validate' in output_lower\n",
    "        \n",
    "        score = min(1.0, (error_mentions * 0.15) + \n",
    "                   (0.3 if catches_key_error else 0) + \n",
    "                   (0.2 if mentions_validation else 0))\n",
    "        \n",
    "        self.score = score\n",
    "        self.reason = f\"Error handling analysis: {score:.2f} (mentions: {error_mentions})\"\n",
    "        self.success = score >= self.threshold\n",
    "        return score\n",
    "    \n",
    "    def is_successful(self) -> bool:\n",
    "        return self.success\n",
    "\n",
    "# Code review metrics\n",
    "code_review_metrics = [\n",
    "    LogicalFlowMetric(threshold=0.7),\n",
    "    StepCompletenessMetric(threshold=0.8),\n",
    "    SecurityAnalysisMetric(threshold=0.8),\n",
    "    ErrorHandlingMetric(threshold=0.7)\n",
    "]\n",
    "\n",
    "print(\"üéØ Code Review Case Study ƒë√£ ƒë∆∞·ª£c setup!\")\n",
    "print(f\"Problem: Code review for security and quality issues\")\n",
    "print(f\"Metrics: {[m.__class__.__name__ for m in code_review_metrics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y Code Review Pipeline\n",
    "code_review_pipeline = AutomatedFeedbackPipeline(llm, max_iterations=4)\n",
    "\n",
    "print(\"üîç CH·∫†Y CODE REVIEW FEEDBACK PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "code_review_result = code_review_pipeline.run_pipeline(\n",
    "    initial_prompt=code_review_initial_prompt,\n",
    "    test_input=code_review_problem,\n",
    "    metrics=code_review_metrics\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÅ CODE REVIEW PIPELINE HO√ÄN TH√ÄNH!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch k·∫øt qu·∫£ Code Review\n",
    "print(\"üìä PH√ÇN T√çCH K·∫æT QU·∫¢ CODE REVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "analyze_pipeline_results(code_review_pipeline)\n",
    "\n",
    "print(\"\\nüìã SO S√ÅNH CODE REVIEW OUTPUT:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if code_review_pipeline.results:\n",
    "    final_result = code_review_pipeline.results[-1]\n",
    "    \n",
    "    print(\"üéØ FINAL CODE REVIEW OUTPUT:\")\n",
    "    print(final_result.enhanced_output)\n",
    "    \n",
    "    print(\"\\nüìà FINAL SCORES:\")\n",
    "    for metric, score in final_result.enhanced_scores.items():\n",
    "        status = \"‚úÖ\" if score >= 0.7 else \"‚ùå\"\n",
    "        print(f\"   {status} {metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o dashboard cho Code Review Pipeline\n",
    "print(\"üìä CODE REVIEW PERFORMANCE DASHBOARD\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "create_performance_dashboard(code_review_pipeline.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Adaptive Threshold Adjustment\n",
    "\n",
    "Tri·ªÉn khai t√≠nh nƒÉng t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh threshold d·ª±a tr√™n performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveThresholdManager:\n",
    "    \"\"\"Qu·∫£n l√Ω adaptive threshold cho metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_thresholds: Dict[str, float]):\n",
    "        self.initial_thresholds = initial_thresholds.copy()\n",
    "        self.current_thresholds = initial_thresholds.copy()\n",
    "        self.performance_history = defaultdict(list)\n",
    "        self.adjustment_history = defaultdict(list)\n",
    "    \n",
    "    def update_thresholds(self, pipeline_results: List[FeedbackResult]) -> Dict[str, float]:\n",
    "        \"\"\"C·∫≠p nh·∫≠t thresholds d·ª±a tr√™n performance history\"\"\"\n",
    "        \n",
    "        if not pipeline_results:\n",
    "            return self.current_thresholds\n",
    "        \n",
    "        # Collect performance data\n",
    "        for result in pipeline_results:\n",
    "            for metric_name, score in result.enhanced_scores.items():\n",
    "                self.performance_history[metric_name].append(score)\n",
    "        \n",
    "        # Adjust thresholds\n",
    "        new_thresholds = {}\n",
    "        \n",
    "        for metric_name, scores in self.performance_history.items():\n",
    "            if len(scores) < 2:\n",
    "                new_thresholds[metric_name] = self.current_thresholds.get(metric_name, 0.7)\n",
    "                continue\n",
    "            \n",
    "            # Calculate statistics\n",
    "            avg_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            recent_scores = scores[-3:]  # Last 3 scores\n",
    "            recent_avg = np.mean(recent_scores)\n",
    "            \n",
    "            current_threshold = self.current_thresholds.get(metric_name, 0.7)\n",
    "            \n",
    "            # Adjustment logic\n",
    "            if recent_avg > current_threshold + 0.1:  # Consistently performing well\n",
    "                new_threshold = min(0.9, current_threshold + 0.05)  # Increase threshold\n",
    "                adjustment_reason = \"Performance consistently high\"\n",
    "            elif recent_avg < current_threshold - 0.1:  # Consistently underperforming\n",
    "                new_threshold = max(0.3, current_threshold - 0.05)  # Decrease threshold\n",
    "                adjustment_reason = \"Performance consistently low\"\n",
    "            else:\n",
    "                new_threshold = current_threshold  # Keep current\n",
    "                adjustment_reason = \"Performance stable\"\n",
    "            \n",
    "            new_thresholds[metric_name] = new_threshold\n",
    "            \n",
    "            # Record adjustment\n",
    "            if new_threshold != current_threshold:\n",
    "                self.adjustment_history[metric_name].append({\n",
    "                    'old_threshold': current_threshold,\n",
    "                    'new_threshold': new_threshold,\n",
    "                    'reason': adjustment_reason,\n",
    "                    'avg_score': avg_score,\n",
    "                    'recent_avg': recent_avg\n",
    "                })\n",
    "        \n",
    "        self.current_thresholds = new_thresholds\n",
    "        return new_thresholds\n",
    "    \n",
    "    def print_threshold_adjustments(self):\n",
    "        \"\"\"In l·ªãch s·ª≠ ƒëi·ªÅu ch·ªânh threshold\"\"\"\n",
    "        \n",
    "        print(\"üéØ ADAPTIVE THRESHOLD ADJUSTMENTS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if not any(self.adjustment_history.values()):\n",
    "            print(\"   ‚ÑπÔ∏è Ch∆∞a c√≥ ƒëi·ªÅu ch·ªânh threshold n√†o\")\n",
    "            return\n",
    "        \n",
    "        for metric_name, adjustments in self.adjustment_history.items():\n",
    "            if adjustments:\n",
    "                print(f\"\\nüìä {metric_name}:\")\n",
    "                for i, adj in enumerate(adjustments, 1):\n",
    "                    direction = \"üìà\" if adj['new_threshold'] > adj['old_threshold'] else \"üìâ\"\n",
    "                    print(f\"   {i}. {direction} {adj['old_threshold']:.3f} ‚Üí {adj['new_threshold']:.3f}\")\n",
    "                    print(f\"      Reason: {adj['reason']}\")\n",
    "                    print(f\"      Recent avg: {adj['recent_avg']:.3f}\")\n",
    "        \n",
    "        print(\"\\nüéØ CURRENT THRESHOLDS:\")\n",
    "        for metric_name, threshold in self.current_thresholds.items():\n",
    "            initial = self.initial_thresholds.get(metric_name, threshold)\n",
    "            change = threshold - initial\n",
    "            direction = \"üìà\" if change > 0 else \"üìâ\" if change < 0 else \"‚û°Ô∏è\"\n",
    "            print(f\"   {direction} {metric_name}: {initial:.3f} ‚Üí {threshold:.3f} ({change:+.3f})\")\n",
    "\n",
    "# Test Adaptive Threshold Manager\n",
    "initial_thresholds = {\n",
    "    'LogicalFlowMetric': 0.7,\n",
    "    'StepCompletenessMetric': 0.8,\n",
    "    'SecurityAnalysisMetric': 0.8,\n",
    "    'ErrorHandlingMetric': 0.7\n",
    "}\n",
    "\n",
    "threshold_manager = AdaptiveThresholdManager(initial_thresholds)\n",
    "\n",
    "# Update v·ªõi k·∫øt qu·∫£ t·ª´ code review pipeline\n",
    "updated_thresholds = threshold_manager.update_thresholds(code_review_pipeline.results)\n",
    "\n",
    "# Print adjustments\n",
    "threshold_manager.print_threshold_adjustments()\n",
    "\n",
    "print(\"\\n‚úÖ Adaptive Threshold Manager ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Export v√† L∆∞u Tr·ªØ K·∫øt Qu·∫£\n",
    "\n",
    "Cu·ªëi c√πng, ch√∫ng ta s·∫Ω export k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch sau n√†y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackResultsExporter:\n",
    "    \"\"\"Export feedback results cho analysis v√† reporting\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.export_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    def export_to_json(self, pipeline_results: List[FeedbackResult], filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export k·∫øt qu·∫£ ra JSON file\"\"\"\n",
    "        \n",
    "        if not filename:\n",
    "            filename = f\"feedback_results_{self.export_timestamp}.json\"\n",
    "        \n",
    "        # Convert to serializable format\n",
    "        export_data = {\n",
    "            'export_timestamp': self.export_timestamp,\n",
    "            'total_iterations': len(pipeline_results),\n",
    "            'overall_success': pipeline_results[-1].success if pipeline_results else False,\n",
    "            'results': [asdict(result) for result in pipeline_results]\n",
    "        }\n",
    "        \n",
    "        # Write to file\n",
    "        filepath = f\"data/{filename}\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Exported results to {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def export_to_csv(self, pipeline_results: List[FeedbackResult], filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Export metrics data ra CSV\"\"\"\n",
    "        \n",
    "        if not filename:\n",
    "            filename = f\"feedback_metrics_{self.export_timestamp}.csv\"\n",
    "        \n",
    "        # Prepare data for DataFrame\n",
    "        rows = []\n",
    "        \n",
    "        for result in pipeline_results:\n",
    "            base_row = {\n",
    "                'iteration': result.iteration,\n",
    "                'timestamp': result.timestamp,\n",
    "                'success': result.success,\n",
    "                'feedback_count': len(result.feedback_used)\n",
    "            }\n",
    "            \n",
    "            # Add metric scores\n",
    "            for metric_name, score in result.enhanced_scores.items():\n",
    "                base_row[f'{metric_name}_score'] = score\n",
    "            \n",
    "            # Add improvements\n",
    "            for metric_name, improvement in result.improvement.items():\n",
    "                base_row[f'{metric_name}_improvement'] = improvement\n",
    "            \n",
    "            rows.append(base_row)\n",
    "        \n",
    "        # Create DataFrame and save\n",
    "        df = pd.DataFrame(rows)\n",
    "        filepath = f\"data/{filename}\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        df.to_csv(filepath, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Exported metrics to {filepath}\")\n",
    "        print(f\"üìä Data shape: {df.shape}\")\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    def generate_summary_report(self, pipeline_results: List[FeedbackResult]) -> str:\n",
    "        \"\"\"T·∫°o summary report\"\"\"\n",
    "        \n",
    "        if not pipeline_results:\n",
    "            return \"No results to report\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"üöÄ FEEDBACK PIPELINE SUMMARY REPORT\",\n",
    "            \"=\" * 50,\n",
    "            f\"üìÖ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"üîÑ Total Iterations: {len(pipeline_results)}\",\n",
    "            f\"‚úÖ Final Success: {pipeline_results[-1].success}\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        # Performance summary\n",
    "        report_lines.extend([\n",
    "            \"üìä PERFORMANCE SUMMARY:\",\n",
    "            \"-\" * 30\n",
    "        ])\n",
    "        \n",
    "        # Collect all metrics\n",
    "        all_metrics = set()\n",
    "        for result in pipeline_results:\n",
    "            all_metrics.update(result.enhanced_scores.keys())\n",
    "        \n",
    "        for metric_name in sorted(all_metrics):\n",
    "            scores = []\n",
    "            for result in pipeline_results:\n",
    "                if metric_name in result.enhanced_scores:\n",
    "                    scores.append(result.enhanced_scores[metric_name])\n",
    "            \n",
    "            if scores:\n",
    "                initial_score = scores[0]\n",
    "                final_score = scores[-1]\n",
    "                avg_score = np.mean(scores)\n",
    "                improvement = final_score - initial_score\n",
    "                \n",
    "                direction = \"üìà\" if improvement > 0 else \"üìâ\" if improvement < 0 else \"‚û°Ô∏è\"\n",
    "                report_lines.append(\n",
    "                    f\"   {direction} {metric_name}:\"\n",
    "                )\n",
    "                report_lines.append(\n",
    "                    f\"      Initial: {initial_score:.3f} | Final: {final_score:.3f} | \"\n",
    "                    f\"Avg: {avg_score:.3f} | Change: {improvement:+.3f}\"\n",
    "                )\n",
    "        \n",
    "        # Feedback analysis\n",
    "        total_feedback = sum(len(result.feedback_used) for result in pipeline_results)\n",
    "        avg_feedback = total_feedback / len(pipeline_results)\n",
    "        \n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"üí¨ FEEDBACK ANALYSIS:\",\n",
    "            \"-\" * 30,\n",
    "            f\"   Total feedback items: {total_feedback}\",\n",
    "            f\"   Average per iteration: {avg_feedback:.1f}\",\n",
    "        ])\n",
    "        \n",
    "        # Success progression\n",
    "        success_progression = [result.success for result in pipeline_results]\n",
    "        success_rate = sum(success_progression) / len(success_progression)\n",
    "        \n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"‚úÖ SUCCESS ANALYSIS:\",\n",
    "            \"-\" * 30,\n",
    "            f\"   Success rate: {success_rate:.1%}\",\n",
    "            f\"   Success progression: {' ‚Üí '.join('‚úÖ' if s else '‚ùå' for s in success_progression)}\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# Export results\n",
    "exporter = FeedbackResultsExporter()\n",
    "\n",
    "print(\"üì§ EXPORTING FEEDBACK RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Export JSON\n",
    "json_file = exporter.export_to_json(code_review_pipeline.results)\n",
    "\n",
    "# Export CSV\n",
    "csv_file = exporter.export_to_csv(code_review_pipeline.results)\n",
    "\n",
    "# Generate and print report\n",
    "print(\"\\nüìã SUMMARY REPORT:\")\n",
    "print(\"-\" * 40)\n",
    "summary_report = exporter.generate_summary_report(code_review_pipeline.results)\n",
    "print(summary_report)\n",
    "\n",
    "# Save report to file\n",
    "report_filename = f\"data/feedback_report_{exporter.export_timestamp}.txt\"\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\n‚úÖ Report saved to {report_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì T·ªïng K·∫øt v√† B√†i T·∫≠p\n",
    "\n",
    "### üìö Ki·∫øn Th·ª©c ƒê√£ H·ªçc\n",
    "\n",
    "Trong notebook n√†y, ch√∫ng ta ƒë√£ h·ªçc:\n",
    "\n",
    "1. **Automated Feedback Pipeline**: 5-phase execution cycle\n",
    "2. **Advanced Feedback Strategies**: Pattern recognition, meta-feedback\n",
    "3. **Adaptive Thresholds**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh d·ª±a tr√™n performance\n",
    "4. **Real-world Applications**: Code Review Agent use case\n",
    "5. **Performance Tracking**: Visualization v√† analysis\n",
    "6. **Export v√† Reporting**: L∆∞u tr·ªØ k·∫øt qu·∫£ cho ph√¢n t√≠ch sau\n",
    "\n",
    "### üéØ B√†i T·∫≠p Th·ª±c H√†nh\n",
    "\n",
    "1. **T√πy ch·ªânh Metrics**: T·∫°o custom metrics cho domain c·ª• th·ªÉ\n",
    "2. **Multi-agent Evaluation**: √Åp d·ª•ng cho complex agent workflows\n",
    "3. **Production Integration**: T√≠ch h·ª£p v√†o CI/CD pipeline\n",
    "4. **A/B Testing**: So s√°nh different feedback strategies\n",
    "\n",
    "### üöÄ ·ª®ng D·ª•ng Th·ª±c Ti·ªÖn\n",
    "\n",
    "Framework n√†y c√≥ th·ªÉ √°p d·ª•ng cho:\n",
    "- **Code Review Automation**\n",
    "- **Content Generation Quality Control**\n",
    "- **Customer Support Bot Improvement**\n",
    "- **Educational Content Assessment**\n",
    "- **Legal Document Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c m·ª´ng!** B·∫°n ƒë√£ ho√†n th√†nh series DeepEval comprehensive framework!\n",
    "\n",
    "Ti·∫øp theo, h√£y √°p d·ª•ng nh·ªØng ki·∫øn th·ª©c n√†y v√†o d·ª± √°n th·ª±c t·∫ø c·ªßa b·∫°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration - Mini exercise\n",
    "print(\"üéØ MINI EXERCISE: T·∫°o Feedback Loop cho Domain c·ªßa B·∫°n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "üìù CHALLENGE:\n",
    "H√£y t·∫°o m·ªôt feedback loop cho m·ªôt use case c·ª• th·ªÉ trong domain c·ªßa b·∫°n.\n",
    "\n",
    "STEPS:\n",
    "1. X√°c ƒë·ªãnh problem domain (VD: Email writing, SQL query generation, etc.)\n",
    "2. T·∫°o 2-3 custom metrics ph√π h·ª£p\n",
    "3. Thi·∫øt k·∫ø initial prompt (c·ªë t√¨nh ƒë∆°n gi·∫£n)\n",
    "4. Ch·∫°y feedback pipeline\n",
    "5. Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    "\n",
    "T·ª™ KH√ìA QUAN TR·ªåNG:\n",
    "- Domain-specific metrics\n",
    "- Iterative improvement\n",
    "- Performance tracking\n",
    "- Real-world applicability\n",
    "\n",
    "üí° TIP: B·∫Øt ƒë·∫ßu v·ªõi use case ƒë∆°n gi·∫£n, sau ƒë√≥ m·ªü r·ªông complexity!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüöÄ B·∫°n ƒë√£ s·∫µn s√†ng √°p d·ª•ng DeepEval v√†o d·ª± √°n th·ª±c t·∫ø!\")\n",
    "print(\"‚ú® Good luck v√† happy coding! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}