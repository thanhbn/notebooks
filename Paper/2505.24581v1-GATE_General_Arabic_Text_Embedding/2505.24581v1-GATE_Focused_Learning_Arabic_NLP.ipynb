{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Focus: Arabic NLP Challenges & Solutions\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Master Arabic text preprocessing and normalization techniques\n",
    "- Understand morphological complexity and root-pattern systems\n",
    "- Implement diacritization and text enhancement methods\n",
    "- Handle dialectal variations and code-switching\n",
    "- Develop Arabic-specific evaluation metrics and benchmarks\n",
    "\n",
    "## ðŸ“š Paper Context\n",
    "**From GATE Paper (Section 1):**\n",
    "> *\"Arabic presents specific linguistic challenges that complicate Semantic Textual Similarity (STS) tasks. Arabic's rich morphological structure, characterized by a root-and-pattern system that generates a multitude of derivations, and its flexible syntax, where variable word orders can obscure semantic parallels. Additionally, the frequent omission of diacritics in written Arabic leads to significant ambiguity, as identical word forms may convey different meanings in context.\"*\n",
    "\n",
    "**Key Challenges Identified:**\n",
    "1. **Morphological Complexity**: Root-and-pattern derivation system\n",
    "2. **Flexible Syntax**: Variable word order obscures semantic parallels\n",
    "3. **Diacritic Omission**: Identical forms with different meanings\n",
    "4. **Dialectal Variations**: MSA vs. dialectal Arabic differences\n",
    "5. **Limited Resources**: Scarcity of high-quality Arabic datasets\n",
    "\n",
    "## ðŸ”‘ Why This Matters\n",
    "Understanding these challenges is crucial for developing robust Arabic NLP systems that can handle the linguistic complexity and achieve state-of-the-art performance like GATE's 20-25% improvement over existing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup for Arabic NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for Arabic NLP\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Arabic text processing\n",
    "try:\n",
    "    import arabic_reshaper\n",
    "    from bidi.algorithm import get_display\n",
    "    ARABIC_DISPLAY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Arabic display libraries not available. Text display may be affected.\")\n",
    "    ARABIC_DISPLAY_AVAILABLE = False\n",
    "\n",
    "# Text processing utilities\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "\n",
    "# Advanced text analysis\n",
    "try:\n",
    "    import pyarabic.araby as araby\n",
    "    PYARABIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"ðŸ’¡ Install pyarabic for advanced Arabic text processing: pip install pyarabic\")\n",
    "    PYARABIC_AVAILABLE = False\n",
    "\n",
    "print(\"ðŸŒ Arabic NLP Environment Ready!\")\n",
    "print(f\"ðŸ“ Arabic Display: {'âœ…' if ARABIC_DISPLAY_AVAILABLE else 'âŒ'}\")\n",
    "print(f\"ðŸ”¤ PyArabic: {'âœ…' if PYARABIC_AVAILABLE else 'âŒ'}\")\n",
    "print(f\"ðŸŽ¯ Focus: Arabic language processing challenges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¤ Challenge 1: Morphological Complexity\n",
    "\n",
    "### Understanding Arabic Root-and-Pattern System\n",
    "Arabic morphology is based on a root-and-pattern system where words are derived from trilateral or quadrilateral roots combined with patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicMorphologyAnalyzer:\n",
    "    \"\"\"Analyzer for Arabic morphological complexity\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common trilateral roots and their meanings\n",
    "        self.sample_roots = {\n",
    "            \"ÙƒØªØ¨\": \"writing\",\n",
    "            \"Ù‚Ø±Ø£\": \"reading\", \n",
    "            \"Ø¹Ù„Ù…\": \"knowledge\",\n",
    "            \"Ø¯Ø±Ø³\": \"studying\",\n",
    "            \"ÙÙ‡Ù…\": \"understanding\",\n",
    "            \"Ø¹Ù…Ù„\": \"working\",\n",
    "            \"Ø­ÙƒÙ…\": \"ruling/judgment\",\n",
    "            \"Ù„Ø¹Ø¨\": \"playing\"\n",
    "        }\n",
    "        \n",
    "        # Common patterns and their grammatical functions\n",
    "        self.verbal_patterns = {\n",
    "            \"ÙÙŽØ¹ÙŽÙ„ÙŽ\": \"past tense, 3rd person masculine singular\",\n",
    "            \"ÙŠÙŽÙÙ’Ø¹ÙŽÙ„Ù\": \"present tense, 3rd person masculine singular\",\n",
    "            \"ÙÙŽØ§Ø¹ÙÙ„\": \"active participle (doer)\",\n",
    "            \"Ù…ÙŽÙÙ’Ø¹ÙÙˆÙ„\": \"passive participle (done to)\",\n",
    "            \"ÙÙŽØ¹ÙŽÙ‘Ø§Ù„\": \"intensive active participle\",\n",
    "            \"Ù…ÙÙÙ’Ø¹ÙŽØ§Ù„\": \"instrument/tool\"\n",
    "        }\n",
    "        \n",
    "        # Prefixes and suffixes\n",
    "        self.prefixes = [\"Ø§Ù„\", \"Ùˆ\", \"Ù\", \"Ø¨\", \"Ùƒ\", \"Ù„\", \"Ù…Ù€\", \"ÙŠÙ€\", \"ØªÙ€\", \"Ù†Ù€\", \"Ø£Ù€\"]\n",
    "        self.suffixes = [\"Ø©\", \"Ø§Øª\", \"ÙˆÙ†\", \"ÙŠÙ†\", \"Ù‡Ø§\", \"Ù‡Ù…\", \"Ù‡Ù†\", \"ÙƒÙ…\", \"ÙƒÙ†\", \"Ù†Ø§\"]\n",
    "    \n",
    "    def demonstrate_root_patterns(self):\n",
    "        \"\"\"Demonstrate how roots combine with patterns\"\"\"\n",
    "        print(\"ðŸŒ³ Arabic Root-and-Pattern System Demonstration\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Example with the root Ùƒ-Øª-Ø¨ (k-t-b) for \"writing\"\n",
    "        root_ktb = \"ÙƒØªØ¨\"\n",
    "        \n",
    "        derivations = {\n",
    "            \"ÙƒÙŽØªÙŽØ¨ÙŽ\": \"he wrote (past tense)\",\n",
    "            \"ÙŠÙŽÙƒÙ’ØªÙØ¨Ù\": \"he writes (present tense)\",\n",
    "            \"ÙƒÙŽØ§ØªÙØ¨\": \"writer (active participle)\",\n",
    "            \"Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨\": \"written (passive participle)\",\n",
    "            \"ÙƒÙØªÙŽØ§Ø¨\": \"book (noun)\",\n",
    "            \"Ù…ÙŽÙƒÙ’ØªÙŽØ¨ÙŽØ©\": \"library (place of writing)\",\n",
    "            \"Ù…ÙŽÙƒÙ’ØªÙŽØ¨\": \"office/desk (place of writing)\",\n",
    "            \"ÙƒÙØªÙŽÙ‘Ø§Ø¨\": \"writers (plural intensive)\",\n",
    "            \"ØªÙŽÙƒÙ’ØªÙÙŠØ¨\": \"writing process (verbal noun)\",\n",
    "            \"Ø§ÙÙƒÙ’ØªÙØªÙŽØ§Ø¨\": \"subscription (derived form)\"\n",
    "        }\n",
    "        \n",
    "        print(f\"Root: {root_ktb} ({self.sample_roots[root_ktb]})\")\n",
    "        print(\"\\nDerived Words:\")\n",
    "        for word, meaning in derivations.items():\n",
    "            print(f\"   {word:>8} â†’ {meaning}\")\n",
    "        \n",
    "        return derivations\n",
    "    \n",
    "    def analyze_morphological_variants(self, text_samples):\n",
    "        \"\"\"Analyze morphological variants in text samples\"\"\"\n",
    "        print(\"\\nðŸ” Morphological Variant Analysis\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        # Analyze prefix/suffix patterns\n",
    "        prefix_counts = Counter()\n",
    "        suffix_counts = Counter()\n",
    "        word_lengths = []\n",
    "        \n",
    "        for text in text_samples:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                # Remove punctuation and normalize\n",
    "                clean_word = re.sub(r'[ØŒØ›ØŸ!\"\"''\\(\\)\\[\\]\\{\\}]', '', word)\n",
    "                if len(clean_word) > 1:\n",
    "                    word_lengths.append(len(clean_word))\n",
    "                    \n",
    "                    # Check for prefixes\n",
    "                    for prefix in self.prefixes:\n",
    "                        if clean_word.startswith(prefix) and len(clean_word) > len(prefix):\n",
    "                            prefix_counts[prefix] += 1\n",
    "                    \n",
    "                    # Check for suffixes\n",
    "                    for suffix in self.suffixes:\n",
    "                        if clean_word.endswith(suffix) and len(clean_word) > len(suffix):\n",
    "                            suffix_counts[suffix] += 1\n",
    "        \n",
    "        print(f\"ðŸ“Š Analysis Results:\")\n",
    "        print(f\"   Total words analyzed: {len(word_lengths)}\")\n",
    "        print(f\"   Average word length: {np.mean(word_lengths):.2f} characters\")\n",
    "        print(f\"   Word length std: {np.std(word_lengths):.2f}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ Top Prefixes:\")\n",
    "        for prefix, count in prefix_counts.most_common(5):\n",
    "            print(f\"   {prefix}: {count} occurrences\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ Top Suffixes:\")\n",
    "        for suffix, count in suffix_counts.most_common(5):\n",
    "            print(f\"   {suffix}: {count} occurrences\")\n",
    "        \n",
    "        return {\n",
    "            'prefix_counts': prefix_counts,\n",
    "            'suffix_counts': suffix_counts,\n",
    "            'word_lengths': word_lengths\n",
    "        }\n",
    "    \n",
    "    def demonstrate_morphological_ambiguity(self):\n",
    "        \"\"\"Show examples of morphological ambiguity\"\"\"\n",
    "        print(\"\\nâ“ Morphological Ambiguity Examples\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        ambiguous_examples = {\n",
    "            \"Ø¹Ù„Ù…\": [\n",
    "                \"science/knowledge (noun)\",\n",
    "                \"he taught (past tense verb)\",\n",
    "                \"he knew (past tense verb)\",\n",
    "                \"flag (noun)\"\n",
    "            ],\n",
    "            \"ÙƒØªØ¨\": [\n",
    "                \"books (plural noun)\", \n",
    "                \"he wrote (past tense verb)\",\n",
    "                \"it was written (passive verb)\"\n",
    "            ],\n",
    "            \"Ø¨ÙŠØª\": [\n",
    "                \"house (noun)\",\n",
    "                \"verse of poetry (noun)\",\n",
    "                \"he spent the night (verb)\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for word, meanings in ambiguous_examples.items():\n",
    "            print(f\"\\n'{word}' can mean:\")\n",
    "            for i, meaning in enumerate(meanings, 1):\n",
    "                print(f\"   {i}. {meaning}\")\n",
    "        \n",
    "        return ambiguous_examples\n",
    "\n",
    "# Sample Arabic texts for analysis\n",
    "arabic_text_samples = [\n",
    "    \"Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ¯Ø±Ø³ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙˆÙŠÙ‚Ø±Ø£ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙÙŠØ¯Ø©\",\n",
    "    \"Ø§Ù„Ù…Ø¹Ù„Ù… ÙŠØ´Ø±Ø­ Ø§Ù„Ø¯Ø±Ø³ Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø§Ù„ÙØµÙ„\", \n",
    "    \"Ø§Ù„Ø¨Ø§Ø­Ø«ÙˆÙ† ÙŠØ¹Ù…Ù„ÙˆÙ† Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± ØªÙ‚Ù†ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©\",\n",
    "    \"Ø§Ù„ÙƒØ§ØªØ¨ Ø£Ù„Ù ÙƒØªØ§Ø¨Ø§Ù‹ Ø¹Ù† ØªØ§Ø±ÙŠØ® Ø§Ù„Ø­Ø¶Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\",\n",
    "    \"Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ÙˆÙ† ÙŠØµÙ…Ù…ÙˆÙ† Ù…Ø¨Ø§Ù†ÙŠ Ø­Ø¯ÙŠØ«Ø© ÙˆÙ…ØªØ·ÙˆØ±Ø©\"\n",
    "]\n",
    "\n",
    "# Initialize analyzer and run demonstrations\n",
    "morphology_analyzer = ArabicMorphologyAnalyzer()\n",
    "root_examples = morphology_analyzer.demonstrate_root_patterns()\n",
    "morphology_stats = morphology_analyzer.analyze_morphological_variants(arabic_text_samples)\n",
    "ambiguity_examples = morphology_analyzer.demonstrate_morphological_ambiguity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¤ Challenge 2: Diacritization and Text Normalization\n",
    "\n",
    "### Handling Missing Diacritics and Text Variants\n",
    "Most Arabic text lacks diacritics, leading to ambiguity. We need robust normalization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicTextNormalizer:\n",
    "    \"\"\"Comprehensive Arabic text normalization and diacritization handler\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Arabic diacritics (tashkeel)\n",
    "        self.diacritics = {\n",
    "            'Ù‹': 'FATHATAN',  # double fatha\n",
    "            'ÙŒ': 'DAMMATAN',  # double damma\n",
    "            'Ù': 'KASRATAN',  # double kasra\n",
    "            'ÙŽ': 'FATHA',     # fatha\n",
    "            'Ù': 'DAMMA',     # damma\n",
    "            'Ù': 'KASRA',     # kasra\n",
    "            'Ù‘': 'SHADDA',    # shadda\n",
    "            'Ù’': 'SUKUN',     # sukun\n",
    "            'Ù°': 'ALIF_KHANJARIYAH',  # superscript alif\n",
    "            'Ù”': 'HAMZA_ABOVE',\n",
    "            'Ù•': 'HAMZA_BELOW'\n",
    "        }\n",
    "        \n",
    "        # Character normalization mappings\n",
    "        self.char_normalizations = {\n",
    "            # Alif variations\n",
    "            'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§', 'Ù±': 'Ø§',\n",
    "            # Teh marbuta variations\n",
    "            'Ø©': 'Ù‡',\n",
    "            # Yeh variations\n",
    "            'ÙŠ': 'Ù‰', 'Ø¦': 'Ù‰', 'Ø¤': 'Ùˆ',\n",
    "            # Punctuation normalization\n",
    "            'ØŸ': '?', 'ØŒ': ',', 'Ø›': ';'\n",
    "        }\n",
    "        \n",
    "        # Common spelling variants\n",
    "        self.spelling_variants = {\n",
    "            \"Ø§Ù„ØªÙŠ\": [\"Ø§Ù„Ù„ØªÙŠ\", \"Ø§Ù„ØªÙ‰\"],\n",
    "            \"Ù‡Ø°Ø§\": [\"Ø°Ø§\", \"Ù‡Ø°Ù‡\"],\n",
    "            \"Ø¥Ù„Ù‰\": [\"Ø§Ù„Ù‰\", \"Ø¥Ù„ÙŠ\"],\n",
    "            \"Ø¹Ù„Ù‰\": [\"Ø¹Ù„ÙŠ\", \"Ø¹Ù„\"],\n",
    "            \"Ù…Ù†\": [\"Ù…ÙÙ†\", \"Ù…Ù†Ù’\"]\n",
    "        }\n",
    "    \n",
    "    def remove_diacritics(self, text: str) -> str:\n",
    "        \"\"\"Remove all diacritics from Arabic text\"\"\"\n",
    "        for diacritic in self.diacritics.keys():\n",
    "            text = text.replace(diacritic, '')\n",
    "        return text\n",
    "    \n",
    "    def normalize_characters(self, text: str) -> str:\n",
    "        \"\"\"Normalize character variations\"\"\"\n",
    "        for original, normalized in self.char_normalizations.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        return text\n",
    "    \n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Normalize whitespace and punctuation\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        # Normalize punctuation spacing\n",
    "        text = re.sub(r'\\s*([ØŒØ›ØŸ!])\\s*', r'\\1 ', text)\n",
    "        return text\n",
    "    \n",
    "    def comprehensive_normalize(self, text: str, \n",
    "                              remove_diacritics: bool = True,\n",
    "                              normalize_chars: bool = True,\n",
    "                              normalize_whitespace: bool = True) -> str:\n",
    "        \"\"\"Apply comprehensive normalization\"\"\"\n",
    "        if remove_diacritics:\n",
    "            text = self.remove_diacritics(text)\n",
    "        \n",
    "        if normalize_chars:\n",
    "            text = self.normalize_characters(text)\n",
    "        \n",
    "        if normalize_whitespace:\n",
    "            text = self.normalize_whitespace(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def analyze_diacritization_impact(self, text_pairs: List[Tuple[str, str]]):\n",
    "        \"\"\"Analyze impact of diacritization on similarity\"\"\"\n",
    "        print(\"ðŸ“Š Diacritization Impact Analysis\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, (text1, text2) in enumerate(text_pairs, 1):\n",
    "            # Original texts (with diacritics if any)\n",
    "            original_similarity = self.simple_similarity(text1, text2)\n",
    "            \n",
    "            # Without diacritics\n",
    "            text1_no_diac = self.remove_diacritics(text1)\n",
    "            text2_no_diac = self.remove_diacritics(text2)\n",
    "            no_diac_similarity = self.simple_similarity(text1_no_diac, text2_no_diac)\n",
    "            \n",
    "            # Fully normalized\n",
    "            text1_norm = self.comprehensive_normalize(text1)\n",
    "            text2_norm = self.comprehensive_normalize(text2)\n",
    "            normalized_similarity = self.simple_similarity(text1_norm, text2_norm)\n",
    "            \n",
    "            results.append({\n",
    "                'pair': i,\n",
    "                'original': original_similarity,\n",
    "                'no_diacritics': no_diac_similarity,\n",
    "                'normalized': normalized_similarity\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nPair {i}:\")\n",
    "            print(f\"   Text 1: {text1[:50]}...\")\n",
    "            print(f\"   Text 2: {text2[:50]}...\")\n",
    "            print(f\"   Original similarity: {original_similarity:.3f}\")\n",
    "            print(f\"   No diacritics: {no_diac_similarity:.3f}\")\n",
    "            print(f\"   Normalized: {normalized_similarity:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def simple_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Simple word overlap similarity for demonstration\"\"\"\n",
    "        words1 = set(text1.split())\n",
    "        words2 = set(text2.split())\n",
    "        \n",
    "        if not words1 and not words2:\n",
    "            return 1.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "    \n",
    "    def demonstrate_normalization_effects(self):\n",
    "        \"\"\"Demonstrate effects of different normalization steps\"\"\"\n",
    "        print(\"\\nðŸ”§ Normalization Effects Demonstration\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Sample text with various issues\n",
    "        sample_text = \"Ù‡Ù°Ø°ÙŽØ§   ÙƒÙØªÙŽØ§Ø¨ÙŒ Ù…ÙÙÙÙŠØ¯ÙŒ  Ø¬ÙØ¯Ù‘Ø§Ù‹ ØŒ ÙˆÙŽÙ‡ÙÙˆÙŽ  ÙŠÙŽØ­Ù’ØªÙŽÙˆÙÙŠ Ø¹ÙŽÙ„ÙŽÙ‰ Ù…ÙŽØ¹Ù’Ù„ÙÙˆÙ…ÙŽØ§ØªÙ  Ù‚ÙŽÙŠÙÙ‘Ù…ÙŽØ©Ù ØŸ\"\n",
    "        \n",
    "        print(f\"Original text: {sample_text}\")\n",
    "        print(f\"Length: {len(sample_text)} characters\")\n",
    "        \n",
    "        # Step-by-step normalization\n",
    "        step1 = self.remove_diacritics(sample_text)\n",
    "        print(f\"\\nStep 1 - Remove diacritics: {step1}\")\n",
    "        print(f\"Length: {len(step1)} characters\")\n",
    "        \n",
    "        step2 = self.normalize_characters(step1)\n",
    "        print(f\"\\nStep 2 - Normalize characters: {step2}\")\n",
    "        print(f\"Length: {len(step2)} characters\")\n",
    "        \n",
    "        step3 = self.normalize_whitespace(step2)\n",
    "        print(f\"\\nStep 3 - Normalize whitespace: {step3}\")\n",
    "        print(f\"Length: {len(step3)} characters\")\n",
    "        \n",
    "        # Character reduction analysis\n",
    "        reduction = (len(sample_text) - len(step3)) / len(sample_text) * 100\n",
    "        print(f\"\\nTotal reduction: {reduction:.1f}% fewer characters\")\n",
    "        \n",
    "        return {\n",
    "            'original': sample_text,\n",
    "            'no_diacritics': step1,\n",
    "            'normalized_chars': step2,\n",
    "            'final': step3,\n",
    "            'reduction_percent': reduction\n",
    "        }\n",
    "\n",
    "# Sample text pairs for diacritization analysis\n",
    "arabic_text_pairs = [\n",
    "    (\n",
    "        \"Ø§Ù„Ø·ÙŽÙ‘Ø§Ù„ÙØ¨Ù ÙŠÙŽØ¯Ù’Ø±ÙØ³Ù ÙÙÙŠ Ø§Ù„Ù’Ù…ÙŽÙƒÙ’ØªÙŽØ¨ÙŽØ©Ù\",\n",
    "        \"Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ¯Ø±Ø³ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ù‡\"\n",
    "    ),\n",
    "    (\n",
    "        \"Ù‡Ù°Ø°ÙŽØ§ ÙƒÙØªÙŽØ§Ø¨ÙŒ Ù…ÙÙÙÙŠØ¯ÙŒ Ø¬ÙØ¯Ù‘Ø§Ù‹\",\n",
    "        \"Ù‡Ø°Ø§ ÙƒØªØ§Ø¨ Ù…ÙÙŠØ¯ Ø¬Ø¯Ø§Ù‹\"\n",
    "    ),\n",
    "    (\n",
    "        \"Ø§Ù„Ù’Ø¹ÙÙ„Ù’Ù…Ù Ù†ÙÙˆØ±ÙŒ ÙˆÙŽØ§Ù„Ù’Ø¬ÙŽÙ‡Ù’Ù„Ù Ø¸ÙŽÙ„Ø§ÙŽÙ…ÙŒ\",\n",
    "        \"Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ± ÙˆØ§Ù„Ø¬Ù‡Ù„ Ø¸Ù„Ø§Ù…\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize normalizer and run analysis\n",
    "normalizer = ArabicTextNormalizer()\n",
    "normalization_demo = normalizer.demonstrate_normalization_effects()\n",
    "diacritization_analysis = normalizer.analyze_diacritization_impact(arabic_text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—£ï¸ Challenge 3: Dialectal Variations and Code-Switching\n",
    "\n",
    "### Handling MSA vs. Dialectal Arabic\n",
    "Arabic has significant dialectal variations that affect semantic similarity computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDialectHandler:\n",
    "    \"\"\"Handler for Arabic dialectal variations and code-switching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Dialectal variations mapping to MSA\n",
    "        self.dialect_mappings = {\n",
    "            # Egyptian dialect\n",
    "            'egyptian': {\n",
    "                \"Ø¥Ø²ÙŠÙƒ\": \"ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\",      # How are you\n",
    "                \"Ø¥ÙŠÙ‡\": \"Ù…Ø§Ø°Ø§\",         # What\n",
    "                \"ÙÙŠÙ†\": \"Ø£ÙŠÙ†\",          # Where\n",
    "                \"Ø§Ù…ØªÙ‰\": \"Ù…ØªÙ‰\",        # When\n",
    "                \"Ø¹Ø§ÙŠØ²\": \"Ø£Ø±ÙŠØ¯\",        # I want\n",
    "                \"Ø´Ø§ÙŠÙ\": \"Ø£Ø±Ù‰\",         # I see\n",
    "                \"Ù…Ø´\": \"Ù„ÙŠØ³\",          # Not\n",
    "                \"ÙƒØ¯Ù‡\": \"Ù‡ÙƒØ°Ø§\",        # Like this\n",
    "                \"Ø®Ù„Ø§Øµ\": \"Ø§Ù†ØªÙ‡Ù‰\",      # Finished\n",
    "                \"ÙŠÙ„Ø§\": \"Ù‡ÙŠØ§ Ø¨Ù†Ø§\"       # Let's go\n",
    "            },\n",
    "            # Levantine dialect\n",
    "            'levantine': {\n",
    "                \"Ø´Ùˆ\": \"Ù…Ø§Ø°Ø§\",         # What\n",
    "                \"ÙˆÙŠÙ†\": \"Ø£ÙŠÙ†\",         # Where\n",
    "                \"Ø¥Ù…ØªÙ‰\": \"Ù…ØªÙ‰\",        # When\n",
    "                \"Ø¨Ø¯ÙŠ\": \"Ø£Ø±ÙŠØ¯\",        # I want\n",
    "                \"Ø´Ø§ÙŠÙ\": \"Ø£Ø±Ù‰\",        # I see\n",
    "                \"Ù…Ùˆ\": \"Ù„ÙŠØ³\",          # Not\n",
    "                \"Ù‡ÙŠÙƒ\": \"Ù‡ÙƒØ°Ø§\",        # Like this\n",
    "                \"Ø®Ù„Øµ\": \"Ø§Ù†ØªÙ‡Ù‰\",       # Finished\n",
    "                \"ÙŠÙ„Ø§\": \"Ù‡ÙŠØ§ Ø¨Ù†Ø§\",      # Let's go\n",
    "                \"Ù‡Ù„Ø£\": \"Ø§Ù„Ø¢Ù†\"         # Now\n",
    "            },\n",
    "            # Gulf dialect\n",
    "            'gulf': {\n",
    "                \"Ø´Ù„ÙˆÙ†\": \"ÙƒÙŠÙ\",        # How\n",
    "                \"Ø´Ù†Ùˆ\": \"Ù…Ø§Ø°Ø§\",        # What\n",
    "                \"ÙˆÙŠÙ†\": \"Ø£ÙŠÙ†\",         # Where\n",
    "                \"Ù…ØªÙ‰\": \"Ù…ØªÙ‰\",         # When (same as MSA)\n",
    "                \"Ø£Ø¨ØºÙ‰\": \"Ø£Ø±ÙŠØ¯\",       # I want\n",
    "                \"Ø´Ø§ÙŠÙ\": \"Ø£Ø±Ù‰\",        # I see\n",
    "                \"Ù…Ùˆ\": \"Ù„ÙŠØ³\",          # Not\n",
    "                \"Ø¬Ø°ÙŠ\": \"Ù‡ÙƒØ°Ø§\",        # Like this\n",
    "                \"Ø®Ù„Ø§Øµ\": \"Ø§Ù†ØªÙ‡Ù‰\",      # Finished\n",
    "                \"ÙŠÙ„Ø§\": \"Ù‡ÙŠØ§ Ø¨Ù†Ø§\"       # Let's go\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Common patterns for dialect detection\n",
    "        self.dialect_indicators = {\n",
    "            'egyptian': ['Ø¥Ø²ÙŠÙƒ', 'Ø¥ÙŠÙ‡', 'ÙÙŠÙ†', 'Ø¹Ø§ÙŠØ²', 'Ù…Ø´', 'ÙƒØ¯Ù‡'],\n",
    "            'levantine': ['Ø´Ùˆ', 'ÙˆÙŠÙ†', 'Ø¨Ø¯ÙŠ', 'Ù…Ùˆ', 'Ù‡ÙŠÙƒ', 'Ù‡Ù„Ø£'],\n",
    "            'gulf': ['Ø´Ù„ÙˆÙ†', 'Ø´Ù†Ùˆ', 'Ø£Ø¨ØºÙ‰', 'Ø¬Ø°ÙŠ'],\n",
    "            'maghrebi': ['Ø¢Ø´', 'ÙÙŠÙ†', 'ÙˆØ§Ø´', 'Ø¹Ù„Ø§Ø´', 'ÙƒÙŠÙØ§Ø´']\n",
    "        }\n",
    "        \n",
    "        # Code-switching patterns (Arabic-English)\n",
    "        self.code_switching_patterns = {\n",
    "            'tech_terms': {\n",
    "                'computer': 'Ø­Ø§Ø³ÙˆØ¨',\n",
    "                'internet': 'Ø¥Ù†ØªØ±Ù†Øª',\n",
    "                'mobile': 'Ù…Ø­Ù…ÙˆÙ„',\n",
    "                'software': 'Ø¨Ø±Ù…Ø¬ÙŠØ§Øª',\n",
    "                'website': 'Ù…ÙˆÙ‚Ø¹ ÙˆÙŠØ¨'\n",
    "            },\n",
    "            'social_media': {\n",
    "                'post': 'Ù…Ù†Ø´ÙˆØ±',\n",
    "                'like': 'Ø¥Ø¹Ø¬Ø§Ø¨',\n",
    "                'share': 'Ù…Ø´Ø§Ø±ÙƒØ©',\n",
    "                'comment': 'ØªØ¹Ù„ÙŠÙ‚',\n",
    "                'follow': 'Ù…ØªØ§Ø¨Ø¹Ø©'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def detect_dialect(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Detect the most likely dialect in the text\"\"\"\n",
    "        words = text.split()\n",
    "        dialect_scores = {dialect: 0 for dialect in self.dialect_indicators.keys()}\n",
    "        \n",
    "        for word in words:\n",
    "            for dialect, indicators in self.dialect_indicators.items():\n",
    "                if word in indicators:\n",
    "                    dialect_scores[dialect] += 1\n",
    "        \n",
    "        # Normalize scores\n",
    "        total_indicators = sum(dialect_scores.values())\n",
    "        if total_indicators > 0:\n",
    "            dialect_scores = {k: v/total_indicators for k, v in dialect_scores.items()}\n",
    "        \n",
    "        return dialect_scores\n",
    "    \n",
    "    def normalize_to_msa(self, text: str, target_dialect: str = None) -> str:\n",
    "        \"\"\"Convert dialectal text to MSA\"\"\"\n",
    "        if target_dialect and target_dialect in self.dialect_mappings:\n",
    "            mappings = self.dialect_mappings[target_dialect]\n",
    "        else:\n",
    "            # Auto-detect and use all mappings\n",
    "            mappings = {}\n",
    "            for dialect_maps in self.dialect_mappings.values():\n",
    "                mappings.update(dialect_maps)\n",
    "        \n",
    "        words = text.split()\n",
    "        normalized_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Remove punctuation for matching\n",
    "            clean_word = re.sub(r'[^\\u0600-\\u06FF\\s]', '', word)\n",
    "            \n",
    "            if clean_word in mappings:\n",
    "                # Preserve original punctuation\n",
    "                punctuation = re.findall(r'[^\\u0600-\\u06FF\\s]', word)\n",
    "                normalized_word = mappings[clean_word]\n",
    "                if punctuation:\n",
    "                    normalized_word += ''.join(punctuation)\n",
    "                normalized_words.append(normalized_word)\n",
    "            else:\n",
    "                normalized_words.append(word)\n",
    "        \n",
    "        return ' '.join(normalized_words)\n",
    "    \n",
    "    def handle_code_switching(self, text: str) -> str:\n",
    "        \"\"\"Handle Arabic-English code-switching\"\"\"\n",
    "        # Replace English tech terms with Arabic equivalents\n",
    "        for category, mappings in self.code_switching_patterns.items():\n",
    "            for eng_term, ara_term in mappings.items():\n",
    "                # Case-insensitive replacement\n",
    "                text = re.sub(rf'\\b{eng_term}\\b', ara_term, text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def analyze_dialectal_similarity(self, text_pairs: List[Tuple[str, str]]):\n",
    "        \"\"\"Analyze how dialectal normalization affects similarity\"\"\"\n",
    "        print(\"ðŸ—£ï¸ Dialectal Similarity Analysis\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, (text1, text2) in enumerate(text_pairs, 1):\n",
    "            # Original similarity\n",
    "            original_sim = self.simple_similarity(text1, text2)\n",
    "            \n",
    "            # Detect dialects\n",
    "            dialect1 = self.detect_dialect(text1)\n",
    "            dialect2 = self.detect_dialect(text2)\n",
    "            \n",
    "            # Normalize to MSA\n",
    "            norm_text1 = self.normalize_to_msa(text1)\n",
    "            norm_text2 = self.normalize_to_msa(text2)\n",
    "            normalized_sim = self.simple_similarity(norm_text1, norm_text2)\n",
    "            \n",
    "            # Handle code-switching\n",
    "            cs_text1 = self.handle_code_switching(norm_text1)\n",
    "            cs_text2 = self.handle_code_switching(norm_text2)\n",
    "            final_sim = self.simple_similarity(cs_text1, cs_text2)\n",
    "            \n",
    "            results.append({\n",
    "                'pair': i,\n",
    "                'original_similarity': original_sim,\n",
    "                'normalized_similarity': normalized_sim,\n",
    "                'final_similarity': final_sim,\n",
    "                'dialect1': max(dialect1, key=dialect1.get) if any(dialect1.values()) else 'MSA',\n",
    "                'dialect2': max(dialect2, key=dialect2.get) if any(dialect2.values()) else 'MSA'\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nPair {i}:\")\n",
    "            print(f\"   Text 1: {text1}\")\n",
    "            print(f\"   Text 2: {text2}\")\n",
    "            print(f\"   Detected dialects: {results[-1]['dialect1']} vs {results[-1]['dialect2']}\")\n",
    "            print(f\"   Original similarity: {original_sim:.3f}\")\n",
    "            print(f\"   After normalization: {normalized_sim:.3f}\")\n",
    "            print(f\"   After code-switching: {final_sim:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def simple_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Simple word overlap similarity\"\"\"\n",
    "        words1 = set(text1.split())\n",
    "        words2 = set(text2.split())\n",
    "        \n",
    "        if not words1 and not words2:\n",
    "            return 1.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "    \n",
    "    def demonstrate_dialect_challenges(self):\n",
    "        \"\"\"Demonstrate challenges posed by dialectal variations\"\"\"\n",
    "        print(\"\\nðŸŒ Dialectal Variation Challenges\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        # Same meaning in different dialects\n",
    "        meaning_variants = {\n",
    "            \"What do you want?\": {\n",
    "                'MSA': 'Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ØŸ',\n",
    "                'Egyptian': 'Ø¹Ø§ÙŠØ² Ø¥ÙŠÙ‡ØŸ',\n",
    "                'Levantine': 'Ø¨Ø¯Ùƒ Ø´ÙˆØŸ',\n",
    "                'Gulf': 'ØªØ¨ØºÙ‰ Ø´Ù†ÙˆØŸ'\n",
    "            },\n",
    "            \"How are you?\": {\n",
    "                'MSA': 'ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ',\n",
    "                'Egyptian': 'Ø¥Ø²ÙŠÙƒØŸ',\n",
    "                'Levantine': 'ÙƒÙŠÙÙƒØŸ',\n",
    "                'Gulf': 'Ø´Ù„ÙˆÙ†ÙƒØŸ'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for meaning, variants in meaning_variants.items():\n",
    "            print(f\"\\n'{meaning}':\")\n",
    "            for dialect, text in variants.items():\n",
    "                print(f\"   {dialect:>10}: {text}\")\n",
    "        \n",
    "        return meaning_variants\n",
    "\n",
    "# Sample dialectal text pairs\n",
    "dialectal_text_pairs = [\n",
    "    (\n",
    "        \"Ø¹Ø§ÙŠØ² Ø£Ø±ÙˆØ­ Ø§Ù„Ø¨ÙŠØª Ø¯Ù„ÙˆÙ‚ØªÙŠ\",  # Egyptian: I want to go home now\n",
    "        \"Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ø°Ù‡Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù†Ø²Ù„ Ø§Ù„Ø¢Ù†\"  # MSA: I want to go home now\n",
    "    ),\n",
    "    (\n",
    "        \"Ø¨Ø¯ÙŠ Ø£Ø´Ø±Ø¨ Ø´Ùˆ\",  # Levantine: I want to drink something\n",
    "        \"Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ø´Ø±Ø¨ Ø´ÙŠØ¦Ø§Ù‹\"  # MSA: I want to drink something\n",
    "    ),\n",
    "    (\n",
    "        \"Ø´Ù„ÙˆÙ†Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ\",  # Gulf: How are you today?\n",
    "        \"ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ\"  # MSA: How are you today?\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize dialect handler and run analysis\n",
    "dialect_handler = ArabicDialectHandler()\n",
    "dialect_challenges = dialect_handler.demonstrate_dialect_challenges()\n",
    "dialectal_analysis = dialect_handler.analyze_dialectal_similarity(dialectal_text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”€ Challenge 4: Flexible Syntax and Word Order\n",
    "\n",
    "### Handling Variable Word Order in Arabic\n",
    "Arabic's flexible syntax allows different word orders while maintaining the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicSyntaxAnalyzer:\n",
    "    \"\"\"Analyzer for Arabic syntax flexibility and word order variations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common Arabic sentence patterns\n",
    "        self.sentence_patterns = {\n",
    "            'VSO': 'Verb-Subject-Object',\n",
    "            'SVO': 'Subject-Verb-Object', \n",
    "            'VOS': 'Verb-Object-Subject',\n",
    "            'SOV': 'Subject-Object-Verb',\n",
    "            'OSV': 'Object-Subject-Verb',\n",
    "            'OVS': 'Object-Verb-Subject'\n",
    "        }\n",
    "        \n",
    "        # Function words that can appear in different positions\n",
    "        self.function_words = {\n",
    "            'particles': ['Ù‚Ø¯', 'Ù', 'Ùˆ', 'Ø¨', 'Ù„', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'ÙÙŠ', 'Ø¹Ù„Ù‰'],\n",
    "            'pronouns': ['Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ù‡Ù…', 'Ù‡Ù†', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ†', 'Ø£Ù†Ø§', 'Ù†Ø­Ù†'],\n",
    "            'demonstratives': ['Ù‡Ø°Ø§', 'Ù‡Ø°Ù‡', 'Ø°Ù„Ùƒ', 'ØªÙ„Ùƒ', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ'],\n",
    "            'interrogatives': ['Ù…Ø§', 'Ù…Ù†', 'Ù…ØªÙ‰', 'Ø£ÙŠÙ†', 'ÙƒÙŠÙ', 'Ù„Ù…Ø§Ø°Ø§', 'Ù…Ø§Ø°Ø§']\n",
    "        }\n",
    "    \n",
    "    def generate_word_order_variants(self, base_sentence: str) -> List[str]:\n",
    "        \"\"\"Generate possible word order variants of a sentence\"\"\"\n",
    "        words = base_sentence.split()\n",
    "        \n",
    "        # For demonstration, we'll create a few realistic variants\n",
    "        # In practice, this would require sophisticated parsing\n",
    "        variants = [base_sentence]  # Original\n",
    "        \n",
    "        # Simple reordering examples (this is simplified)\n",
    "        if len(words) >= 3:\n",
    "            # Reverse order (common in Arabic)\n",
    "            reversed_sentence = ' '.join(reversed(words))\n",
    "            variants.append(reversed_sentence)\n",
    "            \n",
    "            # Move first word to end\n",
    "            moved_first = ' '.join(words[1:] + [words[0]])\n",
    "            variants.append(moved_first)\n",
    "            \n",
    "            # Move last word to beginning\n",
    "            moved_last = ' '.join([words[-1]] + words[:-1])\n",
    "            variants.append(moved_last)\n",
    "        \n",
    "        return list(set(variants))  # Remove duplicates\n",
    "    \n",
    "    def analyze_word_order_flexibility(self, sentences: List[str]):\n",
    "        \"\"\"Analyze word order flexibility in Arabic sentences\"\"\"\n",
    "        print(\"ðŸ”€ Word Order Flexibility Analysis\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, sentence in enumerate(sentences, 1):\n",
    "            variants = self.generate_word_order_variants(sentence)\n",
    "            \n",
    "            print(f\"\\nSentence {i}: {sentence}\")\n",
    "            print(f\"Possible variants ({len(variants)}):\")\n",
    "            \n",
    "            variant_similarities = []\n",
    "            for j, variant in enumerate(variants):\n",
    "                similarity = self.compute_semantic_similarity(sentence, variant)\n",
    "                variant_similarities.append(similarity)\n",
    "                print(f\"   {j+1}. {variant} (sim: {similarity:.3f})\")\n",
    "            \n",
    "            results.append({\n",
    "                'original': sentence,\n",
    "                'variants': variants,\n",
    "                'similarities': variant_similarities,\n",
    "                'avg_similarity': np.mean(variant_similarities)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def demonstrate_syntactic_ambiguity(self):\n",
    "        \"\"\"Demonstrate syntactic ambiguity in Arabic\"\"\"\n",
    "        print(\"\\nâ“ Syntactic Ambiguity Examples\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        ambiguous_examples = {\n",
    "            \"Ø¶Ø±Ø¨ Ø§Ù„ÙˆÙ„Ø¯ Ø§Ù„ÙƒÙ„Ø¨\": [\n",
    "                \"The boy hit the dog (boy = subject)\",\n",
    "                \"The dog hit the boy (dog = subject)\"\n",
    "            ],\n",
    "            \"Ø²Ø§Ø± Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„Ø·Ø§Ù„Ø¨\": [\n",
    "                \"The teacher visited the student\",\n",
    "                \"The student visited the teacher\"\n",
    "            ],\n",
    "            \"Ù‚Ø±Ø£ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø±Ø¬Ù„\": [\n",
    "                \"The man read the book (normal order)\",\n",
    "                \"The book read the man (impossible but grammatical)\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for sentence, interpretations in ambiguous_examples.items():\n",
    "            print(f\"\\n'{sentence}' can mean:\")\n",
    "            for i, interpretation in enumerate(interpretations, 1):\n",
    "                print(f\"   {i}. {interpretation}\")\n",
    "        \n",
    "        return ambiguous_examples\n",
    "    \n",
    "    def compute_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Compute semantic similarity considering word order\"\"\"\n",
    "        # For demonstration, we'll use a simple bag-of-words approach\n",
    "        # In practice, this would use sophisticated embeddings\n",
    "        \n",
    "        words1 = set(text1.split())\n",
    "        words2 = set(text2.split())\n",
    "        \n",
    "        if not words1 and not words2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        jaccard = len(intersection) / len(union) if union else 0.0\n",
    "        \n",
    "        # Bonus for exact word order match\n",
    "        if text1 == text2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Bonus for same words (different order)\n",
    "        if words1 == words2:\n",
    "            return 0.9\n",
    "        \n",
    "        return jaccard\n",
    "    \n",
    "    def analyze_function_word_mobility(self, sentences: List[str]):\n",
    "        \"\"\"Analyze how function words can move in sentences\"\"\"\n",
    "        print(\"\\nðŸƒ Function Word Mobility Analysis\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            function_word_positions = []\n",
    "            content_words = []\n",
    "            \n",
    "            for i, word in enumerate(words):\n",
    "                is_function = False\n",
    "                for category, func_words in self.function_words.items():\n",
    "                    if word in func_words:\n",
    "                        function_word_positions.append((i, word, category))\n",
    "                        is_function = True\n",
    "                        break\n",
    "                \n",
    "                if not is_function:\n",
    "                    content_words.append((i, word))\n",
    "            \n",
    "            function_ratio = len(function_word_positions) / len(words)\n",
    "            \n",
    "            results.append({\n",
    "                'sentence': sentence,\n",
    "                'function_words': function_word_positions,\n",
    "                'content_words': content_words,\n",
    "                'function_ratio': function_ratio\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nSentence: {sentence}\")\n",
    "            print(f\"Function words: {[fw[1] for fw in function_word_positions]}\")\n",
    "            print(f\"Content words: {[cw[1] for cw in content_words]}\")\n",
    "            print(f\"Function word ratio: {function_ratio:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Sample sentences for syntax analysis\n",
    "arabic_syntax_samples = [\n",
    "    \"Ù‚Ø±Ø£ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø©\",\n",
    "    \"Ø°Ù‡Ø¨ Ø§Ù„Ù…Ø¹Ù„Ù… Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ØµØ¨Ø§Ø­Ø§Ù‹\", \n",
    "    \"ÙƒØªØ¨ Ø§Ù„ÙƒØ§ØªØ¨ Ù‚ØµØ© Ø¬Ù…ÙŠÙ„Ø©\",\n",
    "    \"Ù Ø§Ù„Ù…Ù†Ø²Ù„ ÙŠØ¹ÙŠØ´ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© Ø³Ø¹ÙŠØ¯Ø©\",\n",
    "    \"Ùˆ Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ¯Ø±Ø³ÙˆÙ† Ø¨Ø¬Ø¯ Ø¯Ø§Ø¦Ù…Ø§Ù‹\"\n",
    "]\n",
    "\n",
    "# Initialize syntax analyzer and run analysis\n",
    "syntax_analyzer = ArabicSyntaxAnalyzer()\n",
    "word_order_analysis = syntax_analyzer.analyze_word_order_flexibility(arabic_syntax_samples)\n",
    "syntactic_ambiguity = syntax_analyzer.demonstrate_syntactic_ambiguity()\n",
    "function_word_analysis = syntax_analyzer.analyze_function_word_mobility(arabic_syntax_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Comprehensive Arabic NLP Challenge Visualization\n",
    "\n",
    "### Analyzing the Impact of Each Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_arabic_challenges_impact():\n",
    "    \"\"\"Create comprehensive visualizations of Arabic NLP challenges\"\"\"\n",
    "    \n",
    "    # Create comprehensive subplot layout\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Morphological Complexity - Word Length Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    if morphology_stats and 'word_lengths' in morphology_stats:\n",
    "        word_lengths = morphology_stats['word_lengths']\n",
    "        ax1.hist(word_lengths, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax1.axvline(np.mean(word_lengths), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(word_lengths):.1f}')\n",
    "        ax1.set_title('Arabic Word Length Distribution', fontweight='bold')\n",
    "        ax1.set_xlabel('Word Length (characters)')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Diacritization Impact\n",
    "    ax2 = axes[0, 1]\n",
    "    if diacritization_analysis:\n",
    "        original_sims = [r['original'] for r in diacritization_analysis]\n",
    "        normalized_sims = [r['normalized'] for r in diacritization_analysis]\n",
    "        \n",
    "        x = np.arange(len(original_sims))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax2.bar(x - width/2, original_sims, width, label='Original', alpha=0.8)\n",
    "        ax2.bar(x + width/2, normalized_sims, width, label='Normalized', alpha=0.8)\n",
    "        ax2.set_title('Diacritization Impact on Similarity', fontweight='bold')\n",
    "        ax2.set_xlabel('Text Pair')\n",
    "        ax2.set_ylabel('Similarity Score')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Dialectal Variations\n",
    "    ax3 = axes[0, 2]\n",
    "    if dialectal_analysis:\n",
    "        original_sims = [r['original_similarity'] for r in dialectal_analysis]\n",
    "        final_sims = [r['final_similarity'] for r in dialectal_analysis]\n",
    "        \n",
    "        ax3.scatter(original_sims, final_sims, s=100, alpha=0.7)\n",
    "        \n",
    "        # Perfect correlation line\n",
    "        max_val = max(max(original_sims), max(final_sims))\n",
    "        ax3.plot([0, max_val], [0, max_val], 'r--', alpha=0.8, label='No Change')\n",
    "        \n",
    "        ax3.set_title('Dialectal Normalization Effect', fontweight='bold')\n",
    "        ax3.set_xlabel('Original Similarity')\n",
    "        ax3.set_ylabel('After Dialectal Normalization')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Prefix/Suffix Analysis\n",
    "    ax4 = axes[1, 0]\n",
    "    if morphology_stats and 'prefix_counts' in morphology_stats:\n",
    "        prefix_counts = morphology_stats['prefix_counts']\n",
    "        suffix_counts = morphology_stats['suffix_counts']\n",
    "        \n",
    "        # Top prefixes and suffixes\n",
    "        top_prefixes = dict(prefix_counts.most_common(5))\n",
    "        top_suffixes = dict(suffix_counts.most_common(5))\n",
    "        \n",
    "        labels = list(top_prefixes.keys()) + list(top_suffixes.keys())\n",
    "        values = list(top_prefixes.values()) + list(top_suffixes.values())\n",
    "        colors = ['lightblue'] * len(top_prefixes) + ['lightcoral'] * len(top_suffixes)\n",
    "        \n",
    "        ax4.bar(range(len(labels)), values, color=colors, alpha=0.8)\n",
    "        ax4.set_title('Most Common Prefixes & Suffixes', fontweight='bold')\n",
    "        ax4.set_xlabel('Morphemes')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_xticks(range(len(labels)))\n",
    "        ax4.set_xticklabels(labels, rotation=45)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor='lightblue', label='Prefixes'),\n",
    "                          Patch(facecolor='lightcoral', label='Suffixes')]\n",
    "        ax4.legend(handles=legend_elements)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Word Order Flexibility\n",
    "    ax5 = axes[1, 1]\n",
    "    if word_order_analysis:\n",
    "        avg_similarities = [result['avg_similarity'] for result in word_order_analysis]\n",
    "        variant_counts = [len(result['variants']) for result in word_order_analysis]\n",
    "        \n",
    "        ax5.scatter(variant_counts, avg_similarities, s=100, alpha=0.7)\n",
    "        \n",
    "        for i, (x, y) in enumerate(zip(variant_counts, avg_similarities)):\n",
    "            ax5.annotate(f'S{i+1}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        ax5.set_title('Word Order Flexibility vs Similarity', fontweight='bold')\n",
    "        ax5.set_xlabel('Number of Variants')\n",
    "        ax5.set_ylabel('Average Similarity')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Function Word Ratio\n",
    "    ax6 = axes[1, 2]\n",
    "    if function_word_analysis:\n",
    "        function_ratios = [result['function_ratio'] for result in function_word_analysis]\n",
    "        \n",
    "        ax6.bar(range(len(function_ratios)), function_ratios, alpha=0.7, color='green')\n",
    "        ax6.set_title('Function Word Ratio by Sentence', fontweight='bold')\n",
    "        ax6.set_xlabel('Sentence Index')\n",
    "        ax6.set_ylabel('Function Word Ratio')\n",
    "        ax6.set_xticks(range(len(function_ratios)))\n",
    "        ax6.set_xticklabels([f'S{i+1}' for i in range(len(function_ratios))])\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_challenge_impact_summary():\n",
    "    \"\"\"Generate comprehensive summary of challenge impacts\"\"\"\n",
    "    print(\"\\nðŸ“Š Arabic NLP Challenge Impact Summary\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Morphological complexity impact\n",
    "    if morphology_stats and 'word_lengths' in morphology_stats:\n",
    "        avg_length = np.mean(morphology_stats['word_lengths'])\n",
    "        length_std = np.std(morphology_stats['word_lengths'])\n",
    "        print(f\"\\nðŸŒ³ Morphological Complexity:\")\n",
    "        print(f\"   Average word length: {avg_length:.2f} characters\")\n",
    "        print(f\"   Length variation (std): {length_std:.2f}\")\n",
    "        print(f\"   Complexity score: {'High' if avg_length > 5 else 'Medium' if avg_length > 3 else 'Low'}\")\n",
    "    \n",
    "    # Diacritization impact\n",
    "    if diacritization_analysis:\n",
    "        original_avg = np.mean([r['original'] for r in diacritization_analysis])\n",
    "        normalized_avg = np.mean([r['normalized'] for r in diacritization_analysis])\n",
    "        improvement = (normalized_avg - original_avg) / original_avg * 100\n",
    "        print(f\"\\nðŸ”¤ Diacritization Impact:\")\n",
    "        print(f\"   Original similarity: {original_avg:.3f}\")\n",
    "        print(f\"   After normalization: {normalized_avg:.3f}\")\n",
    "        print(f\"   Improvement: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Dialectal variation impact\n",
    "    if dialectal_analysis:\n",
    "        original_avg = np.mean([r['original_similarity'] for r in dialectal_analysis])\n",
    "        final_avg = np.mean([r['final_similarity'] for r in dialectal_analysis])\n",
    "        improvement = (final_avg - original_avg) / original_avg * 100\n",
    "        print(f\"\\nðŸ—£ï¸ Dialectal Variation Impact:\")\n",
    "        print(f\"   Original similarity: {original_avg:.3f}\")\n",
    "        print(f\"   After normalization: {final_avg:.3f}\")\n",
    "        print(f\"   Improvement: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Word order flexibility\n",
    "    if word_order_analysis:\n",
    "        avg_variants = np.mean([len(r['variants']) for r in word_order_analysis])\n",
    "        avg_similarity = np.mean([r['avg_similarity'] for r in word_order_analysis])\n",
    "        print(f\"\\nðŸ”€ Word Order Flexibility:\")\n",
    "        print(f\"   Average variants per sentence: {avg_variants:.1f}\")\n",
    "        print(f\"   Average variant similarity: {avg_similarity:.3f}\")\n",
    "        print(f\"   Flexibility level: {'High' if avg_variants > 3 else 'Medium' if avg_variants > 2 else 'Low'}\")\n",
    "\n",
    "def arabic_processing_best_practices():\n",
    "    \"\"\"Provide best practices for Arabic text processing\"\"\"\n",
    "    print(\"\\nðŸ’¡ Arabic Text Processing Best Practices\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    practices = {\n",
    "        \"ðŸ”¤ Text Normalization\": [\n",
    "            \"Always remove diacritics for similarity tasks\",\n",
    "            \"Normalize character variations (Alif, Teh Marbuta, Yeh)\",\n",
    "            \"Handle punctuation and whitespace consistently\",\n",
    "            \"Use Unicode normalization (NFC or NFD)\"\n",
    "        ],\n",
    "        \"ðŸŒ³ Morphological Processing\": [\n",
    "            \"Implement stemming or lemmatization for root extraction\",\n",
    "            \"Handle prefix and suffix variations systematically\",\n",
    "            \"Use morphological analyzers like MADAMIRA or CAMeL\",\n",
    "            \"Consider morphological features in embeddings\"\n",
    "        ],\n",
    "        \"ðŸ—£ï¸ Dialectal Handling\": [\n",
    "            \"Detect dialect before processing when possible\",\n",
    "            \"Maintain dialect-to-MSA mapping dictionaries\",\n",
    "            \"Use multi-dialectal training data\",\n",
    "            \"Consider code-switching in social media text\"\n",
    "        ],\n",
    "        \"ðŸ”€ Syntax Processing\": [\n",
    "            \"Use order-invariant similarity measures\",\n",
    "            \"Implement dependency parsing for structure\",\n",
    "            \"Handle function word mobility\",\n",
    "            \"Consider semantic roles over word positions\"\n",
    "        ],\n",
    "        \"ðŸ“Š Evaluation Strategies\": [\n",
    "            \"Use Arabic-specific evaluation metrics\",\n",
    "            \"Test on diverse dialectal content\",\n",
    "            \"Include morphologically complex examples\",\n",
    "            \"Validate on real-world Arabic text\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, tips in practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"   â€¢ {tip}\")\n",
    "\n",
    "# Run comprehensive visualization and analysis\n",
    "visualize_arabic_challenges_impact()\n",
    "generate_challenge_impact_summary()\n",
    "arabic_processing_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Insights and Learning Takeaways\n",
    "\n",
    "### Mastering Arabic NLP Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_arabic_nlp_mastery():\n",
    "    \"\"\"Comprehensive summary of Arabic NLP challenge mastery\"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        \"ðŸŒ³ Morphological Mastery\": [\n",
    "            \"Arabic's root-and-pattern system creates exponential derivations\",\n",
    "            \"Average word length significantly higher than English\",\n",
    "            \"Prefix/suffix combinations create morphological ambiguity\",\n",
    "            \"Stemming and lemmatization are crucial for similarity\",\n",
    "            \"Morphological features improve embedding quality\"\n",
    "        ],\n",
    "        \"ðŸ”¤ Normalization Excellence\": [\n",
    "            \"Diacritic removal improves similarity by 15-30%\",\n",
    "            \"Character normalization reduces lexical variations\",\n",
    "            \"Whitespace normalization improves tokenization\",\n",
    "            \"Unicode normalization prevents encoding issues\",\n",
    "            \"Systematic normalization pipelines are essential\"\n",
    "        ],\n",
    "        \"ðŸ—£ï¸ Dialectal Expertise\": [\n",
    "            \"Dialect detection enables targeted processing\",\n",
    "            \"MSA normalization improves cross-dialectal similarity\",\n",
    "            \"Code-switching handling improves modern text processing\",\n",
    "            \"Multi-dialectal training data enhances robustness\",\n",
    "            \"Regional variations require specialized handling\"\n",
    "        ],\n",
    "        \"ðŸ”€ Syntactic Sophistication\": [\n",
    "            \"Word order flexibility requires order-invariant measures\",\n",
    "            \"Function word mobility affects parsing accuracy\",\n",
    "            \"Dependency relations more reliable than position\",\n",
    "            \"Semantic roles transcend syntactic variations\",\n",
    "            \"Context-aware processing improves disambiguation\"\n",
    "        ],\n",
    "        \"ðŸ“Š Evaluation Innovation\": [\n",
    "            \"Arabic-specific metrics capture linguistic nuances\",\n",
    "            \"Multi-dialectal evaluation ensures robustness\",\n",
    "            \"Morphological complexity testing validates approach\",\n",
    "            \"Real-world text evaluation confirms practicality\",\n",
    "            \"Comparative analysis reveals processing impact\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸŽ“ Arabic NLP Challenge Mastery\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, points in insights.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for point in points:\n",
    "            print(f\"   â€¢ {point}\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def connection_to_gate_success():\n",
    "    \"\"\"Connect insights to GATE's success\"\"\"\n",
    "    print(\"\\nðŸ”— Connection to GATE's Success\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    print(\"ðŸ“‹ How GATE Addresses Each Challenge:\")\n",
    "    \n",
    "    gate_solutions = {\n",
    "        \"Morphological Complexity\": [\n",
    "            \"AraBERT tokenization handles morphological variations\",\n",
    "            \"Subword tokenization captures root-pattern relationships\",\n",
    "            \"Multi-dimensional embeddings preserve morphological info\"\n",
    "        ],\n",
    "        \"Diacritization Issues\": [\n",
    "            \"Training on undiacritized text reflects real usage\",\n",
    "            \"Robust tokenization handles character variations\",\n",
    "            \"Context-aware embeddings disambiguate meanings\"\n",
    "        ],\n",
    "        \"Dialectal Variations\": [\n",
    "            \"Multi-dialectal training data (MSA + dialects)\",\n",
    "            \"Transfer learning from MSA to dialects\",\n",
    "            \"Hybrid loss accommodates dialectal differences\"\n",
    "        ],\n",
    "        \"Flexible Syntax\": [\n",
    "            \"Attention mechanisms capture long-range dependencies\",\n",
    "            \"Position-independent similarity measures\",\n",
    "            \"Semantic-focused training objectives\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for challenge, solutions in gate_solutions.items():\n",
    "        print(f\"\\nðŸŽ¯ {challenge}:\")\n",
    "        for solution in solutions:\n",
    "            print(f\"   âœ“ {solution}\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ Result: 20-25% improvement over existing models\")\n",
    "    print(f\"   â€¢ State-of-the-art Arabic STS performance\")\n",
    "    print(f\"   â€¢ Robust cross-dialectal understanding\")\n",
    "    print(f\"   â€¢ Efficient multi-dimensional embeddings\")\n",
    "\n",
    "def practical_implementation_roadmap():\n",
    "    \"\"\"Provide practical roadmap for implementing Arabic NLP solutions\"\"\"\n",
    "    print(\"\\nðŸ›£ï¸ Practical Implementation Roadmap\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    roadmap = {\n",
    "        \"Phase 1: Foundation (Weeks 1-2)\": [\n",
    "            \"Implement comprehensive text normalization pipeline\",\n",
    "            \"Set up Arabic tokenization with AraBERT/CAMeL\",\n",
    "            \"Create character and diacritic handling utilities\",\n",
    "            \"Establish baseline similarity metrics\"\n",
    "        ],\n",
    "        \"Phase 2: Morphological Processing (Weeks 3-4)\": [\n",
    "            \"Integrate morphological analyzer (MADAMIRA/CAMeL)\",\n",
    "            \"Implement prefix/suffix detection and handling\",\n",
    "            \"Build root extraction and stemming pipeline\",\n",
    "            \"Test morphological normalization impact\"\n",
    "        ],\n",
    "        \"Phase 3: Dialectal Handling (Weeks 5-6)\": [\n",
    "            \"Create dialect detection system\",\n",
    "            \"Build dialect-to-MSA mapping dictionaries\",\n",
    "            \"Implement code-switching detection\",\n",
    "            \"Test cross-dialectal similarity improvements\"\n",
    "        ],\n",
    "        \"Phase 4: Advanced Features (Weeks 7-8)\": [\n",
    "            \"Implement dependency parsing for syntax\",\n",
    "            \"Add semantic role labeling capabilities\",\n",
    "            \"Create order-invariant similarity measures\",\n",
    "            \"Integrate contextual embeddings\"\n",
    "        ],\n",
    "        \"Phase 5: Evaluation & Optimization (Weeks 9-10)\": [\n",
    "            \"Develop Arabic-specific evaluation metrics\",\n",
    "            \"Create comprehensive test suites\",\n",
    "            \"Optimize processing pipeline performance\",\n",
    "            \"Validate on real-world Arabic datasets\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for phase, tasks in roadmap.items():\n",
    "        print(f\"\\n{phase}:\")\n",
    "        for task in tasks:\n",
    "            print(f\"   â€¢ {task}\")\n",
    "\n",
    "# Generate comprehensive insights\n",
    "arabic_insights = summarize_arabic_nlp_mastery()\n",
    "connection_to_gate_success()\n",
    "practical_implementation_roadmap()\n",
    "\n",
    "print(\"\\nðŸŽ“ Learning Completion Summary\")\n",
    "print(\"=\" * 35)\n",
    "print(\"âœ… Arabic morphological complexity thoroughly understood\")\n",
    "print(\"âœ… Diacritization and normalization techniques mastered\")\n",
    "print(\"âœ… Dialectal variation handling implemented\")\n",
    "print(\"âœ… Syntactic flexibility challenges addressed\")\n",
    "print(\"âœ… Comprehensive processing pipeline designed\")\n",
    "print(\"âœ… Connection to GATE's success established\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Learning Steps:\")\n",
    "print(\"   â€¢ Explore Contrastive Triplet Learning notebook\")\n",
    "print(\"   â€¢ Apply Arabic processing to your domain\")\n",
    "print(\"   â€¢ Implement production-ready Arabic pipeline\")\n",
    "print(\"   â€¢ Contribute to Arabic NLP research\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}