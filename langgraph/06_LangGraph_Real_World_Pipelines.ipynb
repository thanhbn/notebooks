{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_LangGraph_Real_World_Pipelines - X√¢y d·ª±ng Pipeline AI Th·ª±c t·∫ø\n",
    "\n",
    "## üéØ M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "\n",
    "Trong notebook n√†y, ch√∫ng ta s·∫Ω h·ªçc c√°ch:\n",
    "- Hi·ªÉu t·∫°i sao LangGraph ph√π h·ª£p cho c√°c pipeline AI ph·ª©c t·∫°p\n",
    "- X√¢y d·ª±ng pipeline ph√¢n t√≠ch v√† t√≥m t·∫Øt t√†i li·ªáu (RAG n√¢ng cao)\n",
    "- T·∫°o h·ªá th·ªëng h·ªó tr·ª£ quy·∫øt ƒë·ªãnh v·ªõi nhi·ªÅu b∆∞·ªõc t∆∞∆°ng t√°c\n",
    "- Qu·∫£n l√Ω tr·∫°ng th√°i v√† x·ª≠ l√Ω l·ªói trong pipeline th·ª±c t·∫ø\n",
    "- √Åp d·ª•ng conditional logic v√† branching ph·ª©c t·∫°p\n",
    "\n",
    "## üìã T·ªïng quan\n",
    "\n",
    "### T·∫°i sao LangGraph cho Pipeline AI?\n",
    "\n",
    "**LangGraph vs Chains ƒë∆°n gi·∫£n:**\n",
    "- **Qu·∫£n l√Ω tr·∫°ng th√°i**: Theo d√µi th√¥ng tin qua nhi·ªÅu b∆∞·ªõc\n",
    "- **ƒêi·ªÅu h∆∞·ªõng c√≥ ƒëi·ªÅu ki·ªán**: R·∫Ω nh√°nh d·ª±a tr√™n k·∫øt qu·∫£ trung gian\n",
    "- **Kh·∫£ nƒÉng m·ªü r·ªông**: D·ªÖ d√†ng th√™m/b·ªõt c√°c b∆∞·ªõc x·ª≠ l√Ω\n",
    "- **Debugging**: Theo d√µi v√† ki·ªÉm so√°t t·ª´ng b∆∞·ªõc th·ª±c thi\n",
    "- **Error handling**: X·ª≠ l√Ω l·ªói v√† retry logic t·ªët h∆°n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è C√†i ƒë·∫∑t & C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra v√† c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ö†Ô∏è ƒêang c√†i ƒë·∫∑t {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "packages = [\n",
    "    \"langgraph\",\n",
    "    \"langchain-anthropic\", \n",
    "    \"langchain-community\",\n",
    "    \"langchain-core\",\n",
    "    \"requests\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"python-dotenv\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Ki·ªÉm tra API key\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"‚ö†Ô∏è Vui l√≤ng ƒë·∫∑t ANTHROPIC_API_KEY trong file .env\")\n",
    "    ANTHROPIC_API_KEY = input(\"Nh·∫≠p ANTHROPIC_API_KEY: \")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "else:\n",
    "    print(\"‚úÖ ANTHROPIC_API_KEY ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "from typing import TypedDict, List, Dict, Any, Optional, Annotated\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import th√†nh c√¥ng t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup LLM v√† Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o ChatAnthropic\")\n",
    "\n",
    "# Test k·∫øt n·ªëi\n",
    "try:\n",
    "    test_response = llm.invoke([HumanMessage(content=\"Xin ch√†o, b·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát kh√¥ng?\")])\n",
    "    print(f\"üîó K·∫øt n·ªëi th√†nh c√¥ng: {test_response.content[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói k·∫øt n·ªëi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions cho document processing\n",
    "def extract_text_from_url(url: str) -> str:\n",
    "    \"\"\"Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ URL\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Lo·∫°i b·ªè script v√† style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # L·∫•y text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # L√†m s·∫°ch text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text[:5000]  # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° d√†i\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói khi tr√≠ch xu·∫•t t·ª´ {url}: {str(e)}\"\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Chia text th√†nh c√°c ƒëo·∫°n nh·ªè v·ªõi overlap\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # T√¨m ƒëi·ªÉm ng·∫Øt t·ª± nhi√™n (d·∫•u c√¢u ho·∫∑c kho·∫£ng tr·∫Øng)\n",
    "        if end < len(text):\n",
    "            for i in range(end, max(start + chunk_size - 200, start), -1):\n",
    "                if text[i] in '.!?\\n':\n",
    "                    end = i + 1\n",
    "                    break\n",
    "                elif text[i] == ' ':\n",
    "                    end = i\n",
    "                    break\n",
    "        \n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap\n",
    "        \n",
    "        if start >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a utility functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Pipeline 1: Ph√¢n t√≠ch & T√≥m t·∫Øt T√†i li·ªáu (RAG N√¢ng cao)\n",
    "\n",
    "Pipeline n√†y s·∫Ω:\n",
    "1. **T·∫£i t√†i li·ªáu** t·ª´ URL ho·∫∑c text\n",
    "2. **Chia ƒëo·∫°n** th√¥ng minh\n",
    "3. **Truy xu·∫•t th√¥ng tin** li√™n quan\n",
    "4. **ƒê√°nh gi√° ƒë·ªô li√™n quan** c·ªßa th√¥ng tin\n",
    "5. **T√≥m t·∫Øt** ho·∫∑c **tr·∫£ l·ªùi c√¢u h·ªèi** d·ª±a tr√™n th√¥ng tin ƒë√£ l·ªçc\n",
    "6. **X·ª≠ l√Ω feedback** v√† c·∫£i thi·ªán k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a State cho Document Analysis Pipeline\n",
    "class DocumentAnalysisState(TypedDict):\n",
    "    # Input\n",
    "    query: str\n",
    "    document_source: str  # URL ho·∫∑c text\n",
    "    \n",
    "    # Intermediate states\n",
    "    raw_content: str\n",
    "    chunks: List[str]\n",
    "    relevant_chunks: List[str]\n",
    "    relevance_scores: List[float]\n",
    "    \n",
    "    # Output\n",
    "    analysis_result: str\n",
    "    confidence_score: float\n",
    "    suggestions: List[str]\n",
    "    \n",
    "    # Control flow\n",
    "    needs_refinement: bool\n",
    "    error_message: Optional[str]\n",
    "    step_count: int\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a DocumentAnalysisState\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node functions cho Document Analysis Pipeline\n",
    "\n",
    "def load_document(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"T·∫£i t√†i li·ªáu t·ª´ ngu·ªìn\"\"\"\n",
    "    print(f\"üìÑ ƒêang t·∫£i t√†i li·ªáu t·ª´: {state['document_source'][:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        if state['document_source'].startswith(('http://', 'https://')):\n",
    "            content = extract_text_from_url(state['document_source'])\n",
    "        else:\n",
    "            content = state['document_source']\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'raw_content': content,\n",
    "            'error_message': None,\n",
    "            'step_count': state.get('step_count', 0) + 1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            'error_message': f\"L·ªói t·∫£i t√†i li·ªáu: {str(e)}\",\n",
    "            'step_count': state.get('step_count', 0) + 1\n",
    "        }\n",
    "\n",
    "def chunk_document(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"Chia t√†i li·ªáu th√†nh c√°c ƒëo·∫°n nh·ªè\"\"\"\n",
    "    print(\"‚úÇÔ∏è ƒêang chia t√†i li·ªáu th√†nh c√°c ƒëo·∫°n...\")\n",
    "    \n",
    "    if state.get('error_message'):\n",
    "        return state\n",
    "    \n",
    "    chunks = chunk_text(state['raw_content'])\n",
    "    print(f\"üìã ƒê√£ t·∫°o {len(chunks)} ƒëo·∫°n vƒÉn b·∫£n\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        'chunks': chunks,\n",
    "        'step_count': state['step_count'] + 1\n",
    "    }\n",
    "\n",
    "def retrieve_relevant_chunks(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"Truy xu·∫•t c√°c ƒëo·∫°n li√™n quan ƒë·∫øn query\"\"\"\n",
    "    print(f\"üîç ƒêang t√¨m ki·∫øm th√¥ng tin li√™n quan ƒë·∫øn: '{state['query']}'\")\n",
    "    \n",
    "    if state.get('error_message'):\n",
    "        return state\n",
    "    \n",
    "    # S·ª≠ d·ª•ng LLM ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan\n",
    "    relevance_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa th√¥ng tin.\n",
    "        \n",
    "Query: {query}\n",
    "ƒêo·∫°n vƒÉn b·∫£n: {chunk}\n",
    "\n",
    "H√£y ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa ƒëo·∫°n vƒÉn b·∫£n n√†y v·ªõi query tr√™n thang ƒëi·ªÉm t·ª´ 0 ƒë·∫øn 1.\n",
    "Ch·ªâ tr·∫£ v·ªÅ m·ªôt s·ªë th·∫≠p ph√¢n t·ª´ 0.0 ƒë·∫øn 1.0, kh√¥ng gi·∫£i th√≠ch th√™m.\n",
    "V√≠ d·ª•: 0.8\"\"\"\n",
    "    )\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    relevance_scores = []\n",
    "    \n",
    "    for chunk in state['chunks']:\n",
    "        try:\n",
    "            score_response = llm.invoke(\n",
    "                relevance_prompt.format_messages(query=state['query'], chunk=chunk[:500])\n",
    "            )\n",
    "            \n",
    "            # Tr√≠ch xu·∫•t ƒëi·ªÉm s·ªë\n",
    "            score_text = score_response.content.strip()\n",
    "            score = float(re.findall(r'\\d+\\.\\d+|\\d+', score_text)[0])\n",
    "            \n",
    "            if score > 0.3:  # Ng∆∞·ª°ng li√™n quan\n",
    "                relevant_chunks.append(chunk)\n",
    "                relevance_scores.append(score)\n",
    "                \n",
    "        except (ValueError, IndexError):\n",
    "            # N·∫øu kh√¥ng th·ªÉ parse ƒëi·ªÉm s·ªë, b·ªè qua chunk n√†y\n",
    "            continue\n",
    "    \n",
    "    # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë gi·∫£m d·∫ßn\n",
    "    sorted_pairs = sorted(zip(relevant_chunks, relevance_scores), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if sorted_pairs:\n",
    "        relevant_chunks, relevance_scores = zip(*sorted_pairs)\n",
    "        relevant_chunks = list(relevant_chunks[:5])  # Top 5 chunks\n",
    "        relevance_scores = list(relevance_scores[:5])\n",
    "    else:\n",
    "        relevant_chunks = []\n",
    "        relevance_scores = []\n",
    "    \n",
    "    print(f\"‚úÖ T√¨m ƒë∆∞·ª£c {len(relevant_chunks)} ƒëo·∫°n li√™n quan\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        'relevant_chunks': relevant_chunks,\n",
    "        'relevance_scores': relevance_scores,\n",
    "        'step_count': state['step_count'] + 1\n",
    "    }\n",
    "\n",
    "def analyze_and_summarize(state: DocumentAnalysisState) -> DocumentAnalysisState:\n",
    "    \"\"\"Ph√¢n t√≠ch v√† t√≥m t·∫Øt th√¥ng tin\"\"\"\n",
    "    print(\"üß† ƒêang ph√¢n t√≠ch v√† t√≥m t·∫Øt th√¥ng tin...\")\n",
    "    \n",
    "    if state.get('error_message'):\n",
    "        return state\n",
    "    \n",
    "    if not state['relevant_chunks']:\n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_result': \"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n trong t√†i li·ªáu.\",\n",
    "            'confidence_score': 0.0,\n",
    "            'suggestions': [\"Th·ª≠ v·ªõi c√¢u h·ªèi kh√°c\", \"Ki·ªÉm tra l·∫°i t√†i li·ªáu ngu·ªìn\"],\n",
    "            'needs_refinement': True,\n",
    "            'step_count': state['step_count'] + 1\n",
    "        }\n",
    "    \n",
    "    # T·∫°o context t·ª´ c√°c chunks li√™n quan\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"ƒêo·∫°n {i+1} (ƒë·ªô li√™n quan: {score:.2f}):\\n{chunk}\"\n",
    "        for i, (chunk, score) in enumerate(zip(state['relevant_chunks'], state['relevance_scores']))\n",
    "    ])\n",
    "    \n",
    "    analysis_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu. H√£y ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.\n",
    "\n",
    "C√¢u h·ªèi: {query}\n",
    "\n",
    "Th√¥ng tin t·ª´ t√†i li·ªáu:\n",
    "{context}\n",
    "\n",
    "H√£y:\n",
    "1. Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c\n",
    "2. Tr√≠ch d·∫´n th√¥ng tin t·ª´ t√†i li·ªáu\n",
    "3. ƒê√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa c√¢u tr·∫£ l·ªùi (0-1)\n",
    "4. ƒê∆∞a ra 2-3 g·ª£i √Ω ƒë·ªÉ t√¨m hi·ªÉu th√™m\n",
    "\n",
    "ƒê·ªãnh d·∫°ng tr·∫£ l·ªùi:\n",
    "**C√¢u tr·∫£ l·ªùi:**\n",
    "[C√¢u tr·∫£ l·ªùi chi ti·∫øt]\n",
    "\n",
    "**ƒê·ªô tin c·∫≠y:** [0.0-1.0]\n",
    "\n",
    "**G·ª£i √Ω th√™m:**\n",
    "- [G·ª£i √Ω 1]\n",
    "- [G·ª£i √Ω 2]\n",
    "- [G·ª£i √Ω 3]\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            analysis_prompt.format_messages(query=state['query'], context=context)\n",
    "        )\n",
    "        \n",
    "        result = response.content\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t confidence score\n",
    "        confidence_match = re.search(r'\\*\\*ƒê·ªô tin c·∫≠y:\\*\\*\\s*([0-9.]+)', result)\n",
    "        confidence_score = float(confidence_match.group(1)) if confidence_match else 0.5\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t suggestions\n",
    "        suggestions_section = re.search(r'\\*\\*G·ª£i √Ω th√™m:\\*\\*\\s*(.+)', result, re.DOTALL)\n",
    "        suggestions = []\n",
    "        if suggestions_section:\n",
    "            suggestion_lines = suggestions_section.group(1).strip().split('\\n')\n",
    "            suggestions = [line.strip('- ').strip() for line in suggestion_lines if line.strip().startswith('-')]\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_result': result,\n",
    "            'confidence_score': confidence_score,\n",
    "            'suggestions': suggestions,\n",
    "            'needs_refinement': confidence_score < 0.6,\n",
    "            'step_count': state['step_count'] + 1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            'error_message': f\"L·ªói ph√¢n t√≠ch: {str(e)}\",\n",
    "            'step_count': state['step_count'] + 1\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c node functions cho Document Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o Document Analysis Graph\n",
    "def create_document_analysis_graph():\n",
    "    \"\"\"T·∫°o workflow graph cho ph√¢n t√≠ch t√†i li·ªáu\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(DocumentAnalysisState)\n",
    "    \n",
    "    # Th√™m c√°c nodes\n",
    "    workflow.add_node(\"load_document\", load_document)\n",
    "    workflow.add_node(\"chunk_document\", chunk_document)\n",
    "    workflow.add_node(\"retrieve_relevant\", retrieve_relevant_chunks)\n",
    "    workflow.add_node(\"analyze_summarize\", analyze_and_summarize)\n",
    "    \n",
    "    # ƒê·ªãnh nghƒ©a flow\n",
    "    workflow.add_edge(START, \"load_document\")\n",
    "    \n",
    "    # Conditional edge sau load_document\n",
    "    def should_continue_after_load(state: DocumentAnalysisState) -> str:\n",
    "        if state.get('error_message'):\n",
    "            return END\n",
    "        return \"chunk_document\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"load_document\",\n",
    "        should_continue_after_load,\n",
    "        {\"chunk_document\": \"chunk_document\", END: END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"chunk_document\", \"retrieve_relevant\")\n",
    "    workflow.add_edge(\"retrieve_relevant\", \"analyze_summarize\")\n",
    "    \n",
    "    # Conditional edge sau analyze_summarize\n",
    "    def should_refine(state: DocumentAnalysisState) -> str:\n",
    "        if state.get('needs_refinement') and state.get('step_count', 0) < 5:\n",
    "            return \"retrieve_relevant\"  # Retry v·ªõi threshold th·∫•p h∆°n\n",
    "        return END\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze_summarize\",\n",
    "        should_refine,\n",
    "        {\"retrieve_relevant\": \"retrieve_relevant\", END: END}\n",
    "    )\n",
    "    \n",
    "    # Compile graph\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# T·∫°o graph\n",
    "doc_analysis_app = create_document_analysis_graph()\n",
    "print(\"‚úÖ ƒê√£ t·∫°o Document Analysis Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Document Analysis Pipeline\n",
    "def test_document_analysis():\n",
    "    \"\"\"Test pipeline ph√¢n t√≠ch t√†i li·ªáu\"\"\"\n",
    "    \n",
    "    # Sample document\n",
    "    sample_doc = \"\"\"\n",
    "    Tr√≠ tu·ªá nh√¢n t·∫°o (AI) ƒëang thay ƒë·ªïi c√°ch ch√∫ng ta l√†m vi·ªác v√† s·ªëng. \n",
    "    Machine Learning l√† m·ªôt nh√°nh quan tr·ªçng c·ªßa AI, cho ph√©p m√°y t√≠nh h·ªçc t·ª´ d·ªØ li·ªáu m√† kh√¥ng c·∫ßn l·∫≠p tr√¨nh c·ª• th·ªÉ.\n",
    "    \n",
    "    Deep Learning, m·ªôt ph·∫ßn c·ªßa Machine Learning, s·ª≠ d·ª•ng m·∫°ng neural nh√¢n t·∫°o ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu ph·ª©c t·∫°p.\n",
    "    C√°c ·ª©ng d·ª•ng c·ªßa AI bao g·ªìm:\n",
    "    - X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP)\n",
    "    - Computer Vision\n",
    "    - Robotics\n",
    "    - Autonomous vehicles\n",
    "    \n",
    "    LangChain l√† m·ªôt framework m·∫°nh m·∫Ω ƒë·ªÉ x√¢y d·ª±ng ·ª©ng d·ª•ng AI s·ª≠ d·ª•ng Large Language Models (LLMs).\n",
    "    LangGraph m·ªü r·ªông LangChain b·∫±ng c√°ch cung c·∫•p kh·∫£ nƒÉng t·∫°o workflow ph·ª©c t·∫°p v·ªõi state management.\n",
    "    \n",
    "    Trong t∆∞∆°ng lai, AI s·∫Ω ti·∫øp t·ª•c ph√°t tri·ªÉn v√† t√≠ch h·ª£p s√¢u h∆°n v√†o cu·ªôc s·ªëng h√†ng ng√†y.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"LangGraph l√† g√¨ v√† c√≥ t√°c d·ª•ng g√¨?\",\n",
    "            \"document_source\": sample_doc\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"C√°c ·ª©ng d·ª•ng c·ªßa AI c√≥ g√¨?\",\n",
    "            \"document_source\": sample_doc\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üß™ TEST CASE {i+1}: {test_case['query']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # T·∫°o initial state\n",
    "        initial_state = {\n",
    "            \"query\": test_case['query'],\n",
    "            \"document_source\": test_case['document_source'],\n",
    "            \"step_count\": 0,\n",
    "            \"needs_refinement\": False\n",
    "        }\n",
    "        \n",
    "        # Ch·∫°y pipeline\n",
    "        config = {\"configurable\": {\"thread_id\": f\"test_{i}\"}}\n",
    "        \n",
    "        try:\n",
    "            # Execute graph\n",
    "            result = doc_analysis_app.invoke(initial_state, config)\n",
    "            \n",
    "            # Display results\n",
    "            if result.get('error_message'):\n",
    "                print(f\"‚ùå L·ªói: {result['error_message']}\")\n",
    "            else:\n",
    "                print(f\"üìä K·∫øt qu·∫£ ph√¢n t√≠ch:\")\n",
    "                print(f\"üìà ƒê·ªô tin c·∫≠y: {result.get('confidence_score', 0):.2f}\")\n",
    "                print(f\"üìù K·∫øt qu·∫£:\\n{result.get('analysis_result', 'Kh√¥ng c√≥ k·∫øt qu·∫£')}\")\n",
    "                \n",
    "                if result.get('suggestions'):\n",
    "                    print(f\"\\nüí° G·ª£i √Ω:\")\n",
    "                    for suggestion in result['suggestions']:\n",
    "                        print(f\"   ‚Ä¢ {suggestion}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói th·ª±c thi: {str(e)}\")\n",
    "\n",
    "# Ch·∫°y test\n",
    "test_document_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Pipeline 2: H·ªá th·ªëng H·ªó tr·ª£ Quy·∫øt ƒë·ªãnh (Decision Support Agent)\n",
    "\n",
    "Pipeline n√†y s·∫Ω:\n",
    "1. **Ph√¢n t√≠ch y√™u c·∫ßu** c·ªßa ng∆∞·ªùi d√πng\n",
    "2. **T√¨m ki·∫øm th√¥ng tin** t·ª´ nhi·ªÅu ngu·ªìn\n",
    "3. **Ph√¢n t√≠ch d·ªØ li·ªáu** v√† so s√°nh c√°c l·ª±a ch·ªçn\n",
    "4. **ƒê·ªÅ xu·∫•t gi·∫£i ph√°p** v·ªõi l√Ω do\n",
    "5. **T∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng** ƒë·ªÉ l√†m r√µ\n",
    "6. **ƒê∆∞a ra quy·∫øt ƒë·ªãnh cu·ªëi c√πng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a State cho Decision Support Pipeline\n",
    "class DecisionSupportState(TypedDict):\n",
    "    # Input\n",
    "    user_request: str\n",
    "    context: Dict[str, Any]  # Th√¥ng tin b·ªï sung t·ª´ user\n",
    "    \n",
    "    # Analysis phase\n",
    "    request_type: str  # \"comparison\", \"recommendation\", \"analysis\", \"planning\"\n",
    "    key_factors: List[str]\n",
    "    information_needs: List[str]\n",
    "    \n",
    "    # Research phase\n",
    "    collected_info: Dict[str, Any]\n",
    "    sources: List[str]\n",
    "    \n",
    "    # Decision phase\n",
    "    options: List[Dict[str, Any]]\n",
    "    analysis_results: Dict[str, Any]\n",
    "    recommendations: List[str]\n",
    "    \n",
    "    # Interaction phase\n",
    "    clarifications_needed: List[str]\n",
    "    user_feedback: Optional[str]\n",
    "    \n",
    "    # Output\n",
    "    final_decision: str\n",
    "    reasoning: str\n",
    "    confidence_level: float\n",
    "    next_steps: List[str]\n",
    "    \n",
    "    # Control\n",
    "    needs_clarification: bool\n",
    "    is_complete: bool\n",
    "    iteration_count: int\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a DecisionSupportState\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node functions cho Decision Support Pipeline\n",
    "\n",
    "def analyze_request(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Ph√¢n t√≠ch y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng\"\"\"\n",
    "    print(f\"üîç ƒêang ph√¢n t√≠ch y√™u c·∫ßu: {state['user_request'][:100]}...\")\n",
    "    \n",
    "    analysis_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch y√™u c·∫ßu. H√£y ph√¢n t√≠ch y√™u c·∫ßu sau v√† tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng JSON.\n",
    "\n",
    "Y√™u c·∫ßu: {request}\n",
    "Context: {context}\n",
    "\n",
    "H√£y x√°c ƒë·ªãnh:\n",
    "1. Lo·∫°i y√™u c·∫ßu (comparison/recommendation/analysis/planning)\n",
    "2. C√°c y·∫øu t·ªë quan tr·ªçng c·∫ßn xem x√©t\n",
    "3. Th√¥ng tin c·∫ßn thu th·∫≠p\n",
    "\n",
    "Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng JSON:\n",
    "{{\n",
    "    \"request_type\": \"lo·∫°i y√™u c·∫ßu\",\n",
    "    \"key_factors\": [\"y·∫øu t·ªë 1\", \"y·∫øu t·ªë 2\", \"y·∫øu t·ªë 3\"],\n",
    "    \"information_needs\": [\"th√¥ng tin c·∫ßn 1\", \"th√¥ng tin c·∫ßn 2\"]\n",
    "}}\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            analysis_prompt.format_messages(\n",
    "                request=state['user_request'],\n",
    "                context=json.dumps(state.get('context', {}), ensure_ascii=False)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result_text = response.content.strip()\n",
    "        json_match = re.search(r'\\{[^}]+\\}', result_text, re.DOTALL)\n",
    "        \n",
    "        if json_match:\n",
    "            result_json = json.loads(json_match.group())\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                'request_type': result_json.get('request_type', 'analysis'),\n",
    "                'key_factors': result_json.get('key_factors', []),\n",
    "                'information_needs': result_json.get('information_needs', []),\n",
    "                'iteration_count': state.get('iteration_count', 0) + 1\n",
    "            }\n",
    "        else:\n",
    "            # Fallback n·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON\n",
    "            return {\n",
    "                **state,\n",
    "                'request_type': 'analysis',\n",
    "                'key_factors': ['Ch·∫•t l∆∞·ª£ng', 'Chi ph√≠', 'Th·ªùi gian'],\n",
    "                'information_needs': ['Th√¥ng tin c∆° b·∫£n', 'So s√°nh l·ª±a ch·ªçn'],\n",
    "                'iteration_count': state.get('iteration_count', 0) + 1\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói ph√¢n t√≠ch y√™u c·∫ßu: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            'request_type': 'analysis',\n",
    "            'key_factors': ['Y·∫øu t·ªë ch√≠nh'],\n",
    "            'information_needs': ['Th√¥ng tin c·∫ßn thi·∫øt'],\n",
    "            'iteration_count': state.get('iteration_count', 0) + 1\n",
    "        }\n",
    "\n",
    "def collect_information(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Thu th·∫≠p th√¥ng tin c·∫ßn thi·∫øt\"\"\"\n",
    "    print(f\"üìä ƒêang thu th·∫≠p th√¥ng tin cho {len(state['information_needs'])} nhu c·∫ßu\")\n",
    "    \n",
    "    # M√¥ ph·ªèng vi·ªác thu th·∫≠p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn\n",
    "    collected_info = {}\n",
    "    sources = []\n",
    "    \n",
    "    research_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"B·∫°n l√† m·ªôt nh√† nghi√™n c·ª©u chuy√™n nghi·ªáp. H√£y cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ:\n",
    "\n",
    "Ch·ªß ƒë·ªÅ nghi√™n c·ª©u: {topic}\n",
    "Y√™u c·∫ßu g·ªëc: {original_request}\n",
    "Y·∫øu t·ªë quan tr·ªçng: {key_factors}\n",
    "\n",
    "H√£y cung c·∫•p:\n",
    "1. Th√¥ng tin chi ti·∫øt v√† c·∫≠p nh·∫≠t\n",
    "2. ∆Øu ƒëi·ªÉm v√† nh∆∞·ª£c ƒëi·ªÉm\n",
    "3. C√°c l·ª±a ch·ªçn c√≥ th·ªÉ\n",
    "4. ƒê√°nh gi√° d·ª±a tr√™n c√°c y·∫øu t·ªë quan tr·ªçng\n",
    "\n",
    "ƒê·ªãnh d·∫°ng tr·∫£ l·ªùi:\n",
    "**Th√¥ng tin t·ªïng quan:**\n",
    "[Th√¥ng tin chi ti·∫øt]\n",
    "\n",
    "**C√°c l·ª±a ch·ªçn:**\n",
    "1. [L·ª±a ch·ªçn 1]: [M√¥ t·∫£ v√† ƒë√°nh gi√°]\n",
    "2. [L·ª±a ch·ªçn 2]: [M√¥ t·∫£ v√† ƒë√°nh gi√°]\n",
    "\n",
    "**Ph√¢n t√≠ch theo y·∫øu t·ªë:**\n",
    "[Ph√¢n t√≠ch d·ª±a tr√™n key_factors]\"\"\"\n",
    "    )\n",
    "    \n",
    "    for info_need in state['information_needs']:\n",
    "        try:\n",
    "            response = llm.invoke(\n",
    "                research_prompt.format_messages(\n",
    "                    topic=info_need,\n",
    "                    original_request=state['user_request'],\n",
    "                    key_factors=', '.join(state['key_factors'])\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            collected_info[info_need] = response.content\n",
    "            sources.append(f\"Nghi√™n c·ª©u n·ªôi b·ªô - {info_need}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói thu th·∫≠p th√¥ng tin cho {info_need}: {e}\")\n",
    "            collected_info[info_need] = f\"Kh√¥ng th·ªÉ thu th·∫≠p th√¥ng tin cho {info_need}\"\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ thu th·∫≠p th√¥ng tin t·ª´ {len(sources)} ngu·ªìn\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        'collected_info': collected_info,\n",
    "        'sources': sources,\n",
    "        'iteration_count': state['iteration_count'] + 1\n",
    "    }\n",
    "\n",
    "def analyze_and_recommend(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Ph√¢n t√≠ch th√¥ng tin v√† ƒë∆∞a ra khuy·∫øn ngh·ªã\"\"\"\n",
    "    print(\"üß† ƒêang ph√¢n t√≠ch th√¥ng tin v√† t·∫°o khuy·∫øn ngh·ªã...\")\n",
    "    \n",
    "    # T·ªïng h·ª£p th√¥ng tin\n",
    "    all_info = \"\\n\\n\".join([\n",
    "        f\"**{topic}:**\\n{info}\"\n",
    "        for topic, info in state['collected_info'].items()\n",
    "    ])\n",
    "    \n",
    "    recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n quy·∫øt ƒë·ªãnh. D·ª±a tr√™n th√¥ng tin ƒë√£ thu th·∫≠p, h√£y ƒë∆∞a ra ph√¢n t√≠ch v√† khuy·∫øn ngh·ªã.\n",
    "\n",
    "Y√™u c·∫ßu g·ªëc: {request}\n",
    "Lo·∫°i y√™u c·∫ßu: {request_type}\n",
    "Y·∫øu t·ªë quan tr·ªçng: {key_factors}\n",
    "\n",
    "Th√¥ng tin ƒë√£ thu th·∫≠p:\n",
    "{collected_info}\n",
    "\n",
    "H√£y cung c·∫•p:\n",
    "1. Ph√¢n t√≠ch chi ti·∫øt c√°c l·ª±a ch·ªçn\n",
    "2. So s√°nh ∆∞u nh∆∞·ª£c ƒëi·ªÉm\n",
    "3. Khuy·∫øn ngh·ªã c·ª• th·ªÉ v·ªõi l√Ω do\n",
    "4. ƒê√°nh gi√° ƒë·ªô tin c·∫≠y (0-1)\n",
    "5. C√°c c√¢u h·ªèi c·∫ßn l√†m r√µ th√™m (n·∫øu c√≥)\n",
    "\n",
    "ƒê·ªãnh d·∫°ng:\n",
    "**PH√ÇN T√çCH:**\n",
    "[Ph√¢n t√≠ch chi ti·∫øt]\n",
    "\n",
    "**KHUY·∫æN NGH·ªä:**\n",
    "[Khuy·∫øn ngh·ªã c·ª• th·ªÉ]\n",
    "\n",
    "**L√ù DO:**\n",
    "[L√Ω do cho khuy·∫øn ngh·ªã]\n",
    "\n",
    "**ƒê·ªò TIN C·∫¨Y:** [0.0-1.0]\n",
    "\n",
    "**C√ÇU H·ªéI L√ÄM R√ï:**\n",
    "- [C√¢u h·ªèi 1]\n",
    "- [C√¢u h·ªèi 2]\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            recommendation_prompt.format_messages(\n",
    "                request=state['user_request'],\n",
    "                request_type=state['request_type'],\n",
    "                key_factors=', '.join(state['key_factors']),\n",
    "                collected_info=all_info\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result = response.content\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t confidence level\n",
    "        confidence_match = re.search(r'\\*\\*ƒê·ªò TIN C·∫¨Y:\\*\\*\\s*([0-9.]+)', result)\n",
    "        confidence_level = float(confidence_match.group(1)) if confidence_match else 0.7\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t clarifications\n",
    "        clarification_section = re.search(r'\\*\\*C√ÇU H·ªéI L√ÄM R√ï:\\*\\*\\s*(.+)', result, re.DOTALL)\n",
    "        clarifications = []\n",
    "        if clarification_section:\n",
    "            clarification_lines = clarification_section.group(1).strip().split('\\n')\n",
    "            clarifications = [line.strip('- ').strip() for line in clarification_lines if line.strip().startswith('-')]\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_results': {'full_analysis': result},\n",
    "            'confidence_level': confidence_level,\n",
    "            'clarifications_needed': clarifications,\n",
    "            'needs_clarification': len(clarifications) > 0 and confidence_level < 0.8,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói ph√¢n t√≠ch: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            'analysis_results': {'error': str(e)},\n",
    "            'confidence_level': 0.3,\n",
    "            'needs_clarification': True,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "\n",
    "def finalize_decision(state: DecisionSupportState) -> DecisionSupportState:\n",
    "    \"\"\"Ho√†n thi·ªán quy·∫øt ƒë·ªãnh cu·ªëi c√πng\"\"\"\n",
    "    print(\"‚úÖ ƒêang ho√†n thi·ªán quy·∫øt ƒë·ªãnh cu·ªëi c√πng...\")\n",
    "    \n",
    "    final_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"D·ª±a tr√™n ph√¢n t√≠ch ƒë√£ th·ª±c hi·ªán, h√£y ƒë∆∞a ra quy·∫øt ƒë·ªãnh cu·ªëi c√πng v√† k·∫ø ho·∫°ch h√†nh ƒë·ªông.\n",
    "\n",
    "Y√™u c·∫ßu g·ªëc: {request}\n",
    "Ph√¢n t√≠ch ƒë√£ th·ª±c hi·ªán: {analysis}\n",
    "ƒê·ªô tin c·∫≠y: {confidence}\n",
    "\n",
    "H√£y cung c·∫•p:\n",
    "1. Quy·∫øt ƒë·ªãnh cu·ªëi c√πng r√µ r√†ng\n",
    "2. L√Ω do chi ti·∫øt\n",
    "3. C√°c b∆∞·ªõc ti·∫øp theo c·∫ßn th·ª±c hi·ªán\n",
    "\n",
    "ƒê·ªãnh d·∫°ng:\n",
    "**QUY·∫æT ƒê·ªäNH CU·ªêI C√ôNG:**\n",
    "[Quy·∫øt ƒë·ªãnh c·ª• th·ªÉ]\n",
    "\n",
    "**L√ù DO:**\n",
    "[L√Ω do chi ti·∫øt]\n",
    "\n",
    "**C√ÅC B∆Ø·ªöC TI·∫æP THEO:**\n",
    "1. [B∆∞·ªõc 1]\n",
    "2. [B∆∞·ªõc 2]\n",
    "3. [B∆∞·ªõc 3]\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(\n",
    "            final_prompt.format_messages(\n",
    "                request=state['user_request'],\n",
    "                analysis=state['analysis_results'].get('full_analysis', 'Kh√¥ng c√≥ ph√¢n t√≠ch'),\n",
    "                confidence=state['confidence_level']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result = response.content\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t next steps\n",
    "        steps_section = re.search(r'\\*\\*C√ÅC B∆Ø·ªöC TI·∫æP THEO:\\*\\*\\s*(.+)', result, re.DOTALL)\n",
    "        next_steps = []\n",
    "        if steps_section:\n",
    "            step_lines = steps_section.group(1).strip().split('\\n')\n",
    "            next_steps = [re.sub(r'^\\d+\\.\\s*', '', line.strip()) for line in step_lines if re.match(r'^\\d+\\.', line.strip())]\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'final_decision': result,\n",
    "            'reasoning': result,\n",
    "            'next_steps': next_steps,\n",
    "            'is_complete': True,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            'final_decision': f\"L·ªói ho√†n thi·ªán quy·∫øt ƒë·ªãnh: {str(e)}\",\n",
    "            'is_complete': True,\n",
    "            'iteration_count': state['iteration_count'] + 1\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c node functions cho Decision Support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o Decision Support Graph\n",
    "def create_decision_support_graph():\n",
    "    \"\"\"T·∫°o workflow graph cho h·ªó tr·ª£ quy·∫øt ƒë·ªãnh\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(DecisionSupportState)\n",
    "    \n",
    "    # Th√™m c√°c nodes\n",
    "    workflow.add_node(\"analyze_request\", analyze_request)\n",
    "    workflow.add_node(\"collect_info\", collect_information)\n",
    "    workflow.add_node(\"analyze_recommend\", analyze_and_recommend)\n",
    "    workflow.add_node(\"finalize\", finalize_decision)\n",
    "    \n",
    "    # ƒê·ªãnh nghƒ©a flow\n",
    "    workflow.add_edge(START, \"analyze_request\")\n",
    "    workflow.add_edge(\"analyze_request\", \"collect_info\")\n",
    "    workflow.add_edge(\"collect_info\", \"analyze_recommend\")\n",
    "    \n",
    "    # Conditional edge sau analyze_recommend\n",
    "    def should_clarify(state: DecisionSupportState) -> str:\n",
    "        if (state.get('needs_clarification') and \n",
    "            state.get('iteration_count', 0) < 3):\n",
    "            return \"collect_info\"  # Collect more info\n",
    "        return \"finalize\"\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze_recommend\",\n",
    "        should_clarify,\n",
    "        {\"collect_info\": \"collect_info\", \"finalize\": \"finalize\"}\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"finalize\", END)\n",
    "    \n",
    "    # Compile graph\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# T·∫°o graph\n",
    "decision_support_app = create_decision_support_graph()\n",
    "print(\"‚úÖ ƒê√£ t·∫°o Decision Support Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Decision Support Pipeline\n",
    "def test_decision_support():\n",
    "    \"\"\"Test pipeline h·ªó tr·ª£ quy·∫øt ƒë·ªãnh\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"user_request\": \"T√¥i mu·ªën h·ªçc m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh m·ªõi ƒë·ªÉ ph√°t tri·ªÉn ·ª©ng d·ª•ng AI. N√™n ch·ªçn Python hay JavaScript?\",\n",
    "            \"context\": {\n",
    "                \"current_skills\": [\"HTML\", \"CSS\", \"basic programming\"],\n",
    "                \"goal\": \"AI application development\",\n",
    "                \"time_available\": \"3-6 months\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"user_request\": \"C√¥ng ty t√¥i c·∫ßn ch·ªçn gi·ªØa vi·ªác x√¢y d·ª±ng chatbot n·ªôi b·ªô hay thu√™ d·ªãch v·ª• b√™n ngo√†i. Gi√∫p t√¥i quy·∫øt ƒë·ªãnh.\",\n",
    "            \"context\": {\n",
    "                \"company_size\": \"50-100 employees\",\n",
    "                \"budget\": \"moderate\",\n",
    "                \"technical_team\": \"limited\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ü§ñ DECISION SUPPORT TEST {i+1}\")\n",
    "        print(f\"üìã Y√™u c·∫ßu: {test_case['user_request']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # T·∫°o initial state\n",
    "        initial_state = {\n",
    "            \"user_request\": test_case['user_request'],\n",
    "            \"context\": test_case['context'],\n",
    "            \"iteration_count\": 0,\n",
    "            \"needs_clarification\": False,\n",
    "            \"is_complete\": False\n",
    "        }\n",
    "        \n",
    "        # Ch·∫°y pipeline\n",
    "        config = {\"configurable\": {\"thread_id\": f\"decision_test_{i}\"}}\n",
    "        \n",
    "        try:\n",
    "            # Execute graph\n",
    "            result = decision_support_app.invoke(initial_state, config)\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\nüìä K·∫æT QU·∫¢ PH√ÇN T√çCH:\")\n",
    "            print(f\"üéØ Lo·∫°i y√™u c·∫ßu: {result.get('request_type', 'N/A')}\")\n",
    "            print(f\"üîë Y·∫øu t·ªë quan tr·ªçng: {', '.join(result.get('key_factors', []))}\")\n",
    "            print(f\"üìà ƒê·ªô tin c·∫≠y: {result.get('confidence_level', 0):.2f}\")\n",
    "            \n",
    "            if result.get('final_decision'):\n",
    "                print(f\"\\n‚úÖ QUY·∫æT ƒê·ªäNH CU·ªêI C√ôNG:\")\n",
    "                print(result['final_decision'])\n",
    "            \n",
    "            if result.get('next_steps'):\n",
    "                print(f\"\\nüìã C√ÅC B∆Ø·ªöC TI·∫æP THEO:\")\n",
    "                for j, step in enumerate(result['next_steps'], 1):\n",
    "                    print(f\"   {j}. {step}\")\n",
    "            \n",
    "            if result.get('clarifications_needed'):\n",
    "                print(f\"\\n‚ùì C√ÇU H·ªéI C·∫¶N L√ÄM R√ï:\")\n",
    "                for clarification in result['clarifications_needed']:\n",
    "                    print(f\"   ‚Ä¢ {clarification}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói th·ª±c thi: {str(e)}\")\n",
    "\n",
    "# Ch·∫°y test\n",
    "test_decision_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Ph√¢n t√≠ch C·∫•u tr√∫c v√† Hi·ªáu su·∫•t Pipeline\n",
    "\n",
    "### üîÑ C·∫•u tr√∫c Graph\n",
    "\n",
    "H√£y ph√¢n t√≠ch c·∫•u tr√∫c c·ªßa c√°c pipeline ch√∫ng ta ƒë√£ t·∫°o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch c·∫•u tr√∫c Graph\n",
    "def analyze_graph_structure():\n",
    "    \"\"\"Ph√¢n t√≠ch v√† hi·ªÉn th·ªã c·∫•u tr√∫c c·ªßa c√°c graph\"\"\"\n",
    "    \n",
    "    print(\"üìä PH√ÇN T√çCH C·∫§U TR√öC PIPELINE\\n\")\n",
    "    \n",
    "    # Document Analysis Pipeline\n",
    "    print(\"üîç DOCUMENT ANALYSIS PIPELINE:\")\n",
    "    print(\"‚îú‚îÄ‚îÄ START\")\n",
    "    print(\"‚îú‚îÄ‚îÄ load_document (T·∫£i t√†i li·ªáu)\")\n",
    "    print(\"‚îÇ   ‚îú‚îÄ‚îÄ Success ‚Üí chunk_document\")\n",
    "    print(\"‚îÇ   ‚îî‚îÄ‚îÄ Error ‚Üí END\")\n",
    "    print(\"‚îú‚îÄ‚îÄ chunk_document (Chia ƒëo·∫°n)\")\n",
    "    print(\"‚îú‚îÄ‚îÄ retrieve_relevant (Truy xu·∫•t li√™n quan)\")\n",
    "    print(\"‚îú‚îÄ‚îÄ analyze_summarize (Ph√¢n t√≠ch & t√≥m t·∫Øt)\")\n",
    "    print(\"‚îÇ   ‚îú‚îÄ‚îÄ needs_refinement ‚Üí retrieve_relevant (retry)\")\n",
    "    print(\"‚îÇ   ‚îî‚îÄ‚îÄ complete ‚Üí END\")\n",
    "    print(\"‚îî‚îÄ‚îÄ END\\n\")\n",
    "    \n",
    "    # Decision Support Pipeline\n",
    "    print(\"ü§ñ DECISION SUPPORT PIPELINE:\")\n",
    "    print(\"‚îú‚îÄ‚îÄ START\")\n",
    "    print(\"‚îú‚îÄ‚îÄ analyze_request (Ph√¢n t√≠ch y√™u c·∫ßu)\")\n",
    "    print(\"‚îú‚îÄ‚îÄ collect_info (Thu th·∫≠p th√¥ng tin)\")\n",
    "    print(\"‚îú‚îÄ‚îÄ analyze_recommend (Ph√¢n t√≠ch & khuy·∫øn ngh·ªã)\")\n",
    "    print(\"‚îÇ   ‚îú‚îÄ‚îÄ needs_clarification ‚Üí collect_info (retry)\")\n",
    "    print(\"‚îÇ   ‚îî‚îÄ‚îÄ ready ‚Üí finalize\")\n",
    "    print(\"‚îú‚îÄ‚îÄ finalize (Ho√†n thi·ªán quy·∫øt ƒë·ªãnh)\")\n",
    "    print(\"‚îî‚îÄ‚îÄ END\\n\")\n",
    "    \n",
    "    # Key Features\n",
    "    print(\"üîë T√çNH NƒÇNG QUAN TR·ªåNG:\")\n",
    "    print(\"‚Ä¢ State Management: Theo d√µi tr·∫°ng th√°i qua c√°c b∆∞·ªõc\")\n",
    "    print(\"‚Ä¢ Conditional Routing: R·∫Ω nh√°nh d·ª±a tr√™n k·∫øt qu·∫£\")\n",
    "    print(\"‚Ä¢ Error Handling: X·ª≠ l√Ω l·ªói v√† retry logic\")\n",
    "    print(\"‚Ä¢ Memory Persistence: L∆∞u tr·∫°ng th√°i v·ªõi MemorySaver\")\n",
    "    print(\"‚Ä¢ Iterative Refinement: C·∫£i thi·ªán k·∫øt qu·∫£ qua nhi·ªÅu l·∫ßn th·ª±c hi·ªán\")\n",
    "    \n",
    "    # Benefits\n",
    "    print(\"\\n‚úÖ L·ª¢I √çCH C·ª¶A LANGGRAPH:\")\n",
    "    print(\"‚Ä¢ Modularity: D·ªÖ d√†ng th√™m/b·ªõt/thay ƒë·ªïi c√°c b∆∞·ªõc\")\n",
    "    print(\"‚Ä¢ Debugging: Theo d√µi t·ª´ng b∆∞·ªõc th·ª±c thi\")\n",
    "    print(\"‚Ä¢ Scalability: M·ªü r·ªông pipeline ph·ª©c t·∫°p\")\n",
    "    print(\"‚Ä¢ Reliability: X·ª≠ l√Ω l·ªói v√† retry t·ªët h∆°n\")\n",
    "    print(\"‚Ä¢ Flexibility: Thay ƒë·ªïi lu·ªìng d·ª±a tr√™n ƒëi·ªÅu ki·ªán\")\n",
    "\n",
    "analyze_graph_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Best Practices v√† T·ªëi ∆∞u h√≥a\n",
    "\n",
    "### C√°c nguy√™n t·∫Øc quan tr·ªçng khi x√¢y d·ª±ng pipeline th·ª±c t·∫ø:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices cho LangGraph Pipelines\n",
    "def demonstrate_best_practices():\n",
    "    \"\"\"Minh h·ªça c√°c best practices\"\"\"\n",
    "    \n",
    "    print(\"üèÜ BEST PRACTICES CHO LANGGRAPH PIPELINES\\n\")\n",
    "    \n",
    "    print(\"1. üìã STATE DESIGN:\")\n",
    "    print(\"   ‚Ä¢ S·ª≠ d·ª•ng TypedDict cho type safety\")\n",
    "    print(\"   ‚Ä¢ Bao g·ªìm control fields (error_message, step_count)\")\n",
    "    print(\"   ‚Ä¢ Thi·∫øt k·∫ø state t·ªëi thi·ªÉu nh∆∞ng ƒë·∫ßy ƒë·ªß\")\n",
    "    print(\"   ‚Ä¢ S·ª≠ d·ª•ng Optional cho c√°c field kh√¥ng b·∫Øt bu·ªôc\\n\")\n",
    "    \n",
    "    print(\"2. üîÑ ERROR HANDLING:\")\n",
    "    print(\"   ‚Ä¢ Lu√¥n c√≥ try-catch trong c√°c node functions\")\n",
    "    print(\"   ‚Ä¢ Truy·ªÅn error th√¥ng qua state\")\n",
    "    print(\"   ‚Ä¢ Conditional routing d·ª±a tr√™n error status\")\n",
    "    print(\"   ‚Ä¢ Graceful degradation khi g·∫∑p l·ªói\\n\")\n",
    "    \n",
    "    print(\"3. üéØ CONDITIONAL LOGIC:\")\n",
    "    print(\"   ‚Ä¢ S·ª≠ d·ª•ng conditional edges cho branching\")\n",
    "    print(\"   ‚Ä¢ ƒê·∫∑t ƒëi·ªÅu ki·ªán d·ª±a tr√™n business logic\")\n",
    "    print(\"   ‚Ä¢ Tr√°nh v√≤ng l·∫∑p v√¥ h·∫°n v·ªõi iteration counters\")\n",
    "    print(\"   ‚Ä¢ Default paths cho c√°c tr∆∞·ªùng h·ª£p edge cases\\n\")\n",
    "    \n",
    "    print(\"4. üöÄ PERFORMANCE:\")\n",
    "    print(\"   ‚Ä¢ S·ª≠ d·ª•ng MemorySaver cho persistent state\")\n",
    "    print(\"   ‚Ä¢ Batch processing khi c√≥ th·ªÉ\")\n",
    "    print(\"   ‚Ä¢ Caching k·∫øt qu·∫£ trung gian\")\n",
    "    print(\"   ‚Ä¢ Parallel execution cho independent tasks\\n\")\n",
    "    \n",
    "    print(\"5. üß™ TESTING:\")\n",
    "    print(\"   ‚Ä¢ Test t·ª´ng node function ri√™ng bi·ªát\")\n",
    "    print(\"   ‚Ä¢ Integration tests cho to√†n b·ªô pipeline\")\n",
    "    print(\"   ‚Ä¢ Mock external dependencies\")\n",
    "    print(\"   ‚Ä¢ Test c√°c edge cases v√† error conditions\\n\")\n",
    "    \n",
    "    print(\"6. üìä MONITORING:\")\n",
    "    print(\"   ‚Ä¢ Log state changes t·∫°i m·ªói node\")\n",
    "    print(\"   ‚Ä¢ Track execution time v√† performance\")\n",
    "    print(\"   ‚Ä¢ Monitor success/failure rates\")\n",
    "    print(\"   ‚Ä¢ Alert tr√™n c√°c l·ªói quan tr·ªçng\")\n",
    "\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Debugging v√† Monitoring\n",
    "\n",
    "### C√°ch debug v√† monitor pipeline hi·ªáu qu·∫£:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging utilities\n",
    "def create_debug_pipeline():\n",
    "    \"\"\"T·∫°o pipeline v·ªõi debugging capabilities\"\"\"\n",
    "    \n",
    "    class DebugState(TypedDict):\n",
    "        input_data: str\n",
    "        processed_data: str\n",
    "        result: str\n",
    "        debug_info: Dict[str, Any]\n",
    "        execution_log: List[str]\n",
    "    \n",
    "    def debug_node_1(state: DebugState) -> DebugState:\n",
    "        \"\"\"Node v·ªõi debugging\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Actual processing\n",
    "            processed = f\"Processed: {state['input_data']}\"\n",
    "            \n",
    "            # Debug info\n",
    "            end_time = datetime.now()\n",
    "            execution_time = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            debug_info = state.get('debug_info', {})\n",
    "            debug_info['node_1'] = {\n",
    "                'execution_time': execution_time,\n",
    "                'input_length': len(state['input_data']),\n",
    "                'output_length': len(processed),\n",
    "                'timestamp': start_time.isoformat()\n",
    "            }\n",
    "            \n",
    "            execution_log = state.get('execution_log', [])\n",
    "            execution_log.append(f\"Node 1 executed in {execution_time:.3f}s\")\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                'processed_data': processed,\n",
    "                'debug_info': debug_info,\n",
    "                'execution_log': execution_log\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution_log = state.get('execution_log', [])\n",
    "            execution_log.append(f\"Node 1 failed: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                **state,\n",
    "                'debug_info': state.get('debug_info', {}),\n",
    "                'execution_log': execution_log\n",
    "            }\n",
    "    \n",
    "    def debug_node_2(state: DebugState) -> DebugState:\n",
    "        \"\"\"Node 2 v·ªõi debugging\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result = f\"Final: {state['processed_data']}\"\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        debug_info = state.get('debug_info', {})\n",
    "        debug_info['node_2'] = {\n",
    "            'execution_time': execution_time,\n",
    "            'timestamp': start_time.isoformat()\n",
    "        }\n",
    "        \n",
    "        execution_log = state.get('execution_log', [])\n",
    "        execution_log.append(f\"Node 2 executed in {execution_time:.3f}s\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            'result': result,\n",
    "            'debug_info': debug_info,\n",
    "            'execution_log': execution_log\n",
    "        }\n",
    "    \n",
    "    # Create graph\n",
    "    workflow = StateGraph(DebugState)\n",
    "    workflow.add_node(\"node_1\", debug_node_1)\n",
    "    workflow.add_node(\"node_2\", debug_node_2)\n",
    "    \n",
    "    workflow.add_edge(START, \"node_1\")\n",
    "    workflow.add_edge(\"node_1\", \"node_2\")\n",
    "    workflow.add_edge(\"node_2\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Test debug pipeline\n",
    "def test_debug_pipeline():\n",
    "    \"\"\"Test debug pipeline\"\"\"\n",
    "    print(\"üîç TESTING DEBUG PIPELINE\\n\")\n",
    "    \n",
    "    debug_app = create_debug_pipeline()\n",
    "    \n",
    "    initial_state = {\n",
    "        \"input_data\": \"Test data for debugging\",\n",
    "        \"debug_info\": {},\n",
    "        \"execution_log\": []\n",
    "    }\n",
    "    \n",
    "    result = debug_app.invoke(initial_state)\n",
    "    \n",
    "    print(\"üìä DEBUG INFORMATION:\")\n",
    "    print(f\"Input: {result['input_data']}\")\n",
    "    print(f\"Result: {result['result']}\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è EXECUTION LOG:\")\n",
    "    for log_entry in result['execution_log']:\n",
    "        print(f\"  ‚Ä¢ {log_entry}\")\n",
    "    \n",
    "    print(\"\\nüîß DEBUG INFO:\")\n",
    "    for node, info in result['debug_info'].items():\n",
    "        print(f\"  {node}:\")\n",
    "        for key, value in info.items():\n",
    "            print(f\"    - {key}: {value}\")\n",
    "\n",
    "test_debug_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö T√†i li·ªáu Tham kh·∫£o\n",
    "\n",
    "### C√°c ngu·ªìn t√†i li·ªáu quan tr·ªçng ƒë·ªÉ t√¨m hi·ªÉu th√™m:\n",
    "\n",
    "1. **LangGraph Official Documentation**\n",
    "   - [LangGraph Introduction](https://langchain-ai.github.io/langgraph/)\n",
    "   - [How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/)\n",
    "   - [API Reference](https://langchain-ai.github.io/langgraph/reference/)\n",
    "\n",
    "2. **Advanced Patterns**\n",
    "   - [Conditional Edges](https://langchain-ai.github.io/langgraph/how-tos/branching/)\n",
    "   - [State Management](https://langchain-ai.github.io/langgraph/how-tos/state-model/)\n",
    "   - [Error Handling](https://langchain-ai.github.io/langgraph/how-tos/error-handling/)\n",
    "\n",
    "3. **Real-world Examples**\n",
    "   - [Multi-agent Systems](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/)\n",
    "   - [RAG Applications](https://langchain-ai.github.io/langgraph/tutorials/rag/)\n",
    "   - [Chatbot Implementation](https://langchain-ai.github.io/langgraph/tutorials/chatbots/)\n",
    "\n",
    "4. **Integration Guides**\n",
    "   - [LangSmith Integration](https://langchain-ai.github.io/langgraph/how-tos/langsmith/)\n",
    "   - [Anthropic Models](https://python.langchain.com/docs/integrations/chat/anthropic/)\n",
    "   - [Memory and Persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ K·∫øt lu·∫≠n v√† B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "### üìù T√≥m t·∫Øt nh·ªØng g√¨ ƒë√£ h·ªçc:\n",
    "\n",
    "1. **Pipeline Architecture**: C√°ch thi·∫øt k·∫ø v√† x√¢y d·ª±ng pipeline AI c√≥ c·∫•u tr√∫c\n",
    "2. **State Management**: Qu·∫£n l√Ω tr·∫°ng th√°i ph·ª©c t·∫°p qua nhi·ªÅu b∆∞·ªõc\n",
    "3. **Conditional Logic**: R·∫Ω nh√°nh v√† ƒëi·ªÅu h∆∞·ªõng d·ª±a tr√™n k·∫øt qu·∫£\n",
    "4. **Error Handling**: X·ª≠ l√Ω l·ªói v√† retry logic hi·ªáu qu·∫£\n",
    "5. **Real-world Applications**: ·ª®ng d·ª•ng th·ª±c t·∫ø v·ªõi RAG v√† Decision Support\n",
    "\n",
    "### üöÄ B∆∞·ªõc ti·∫øp theo:\n",
    "\n",
    "1. **Th·ª±c h√†nh**: T·ª± t·∫°o pipeline cho use case ri√™ng c·ªßa b·∫°n\n",
    "2. **T·ªëi ∆∞u h√≥a**: √Åp d·ª•ng c√°c best practices v√† monitoring\n",
    "3. **M·ªü r·ªông**: Th√™m c√°c t√≠nh nƒÉng nh∆∞ async processing, parallel execution\n",
    "4. **Production**: Deploy pipeline v·ªõi proper infrastructure\n",
    "5. **Integration**: T√≠ch h·ª£p v·ªõi c√°c h·ªá th·ªëng kh√°c trong organization\n",
    "\n",
    "### üí° G·ª£i √Ω cho d·ª± √°n ti·∫øp theo:\n",
    "\n",
    "- **Content Generation Pipeline**: Multi-step content creation v√† review\n",
    "- **Data Analysis Pipeline**: Automated data processing v√† insights\n",
    "- **Customer Service Agent**: Intelligent routing v√† response system\n",
    "- **Research Assistant**: Automated research v√† report generation\n",
    "- **Code Review Pipeline**: Automated code analysis v√† suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final message\n",
    "print(\"üéâ HO√ÄN TH√ÄNH NOTEBOOK 06 - LANGGRAPH REAL WORLD PIPELINES\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ B·∫°n ƒë√£ h·ªçc ƒë∆∞·ª£c:\")\n",
    "print(\"   ‚Ä¢ X√¢y d·ª±ng pipeline RAG n√¢ng cao\")\n",
    "print(\"   ‚Ä¢ T·∫°o h·ªá th·ªëng h·ªó tr·ª£ quy·∫øt ƒë·ªãnh\")\n",
    "print(\"   ‚Ä¢ Qu·∫£n l√Ω state v√† conditional logic\")\n",
    "print(\"   ‚Ä¢ Best practices v√† debugging\")\n",
    "print(\"\\nüöÄ H√£y ti·∫øp t·ª•c kh√°m ph√° v√† x√¢y d·ª±ng c√°c pipeline ph·ª©c t·∫°p h∆°n!\")\n",
    "print(\"üìö Tham kh·∫£o t√†i li·ªáu ch√≠nh th·ª©c ƒë·ªÉ t√¨m hi·ªÉu th√™m c√°c t√≠nh nƒÉng n√¢ng cao.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}