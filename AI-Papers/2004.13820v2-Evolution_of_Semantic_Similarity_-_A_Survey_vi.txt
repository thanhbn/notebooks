# 2004.13820v2.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: D:\llm\notebooks\AI-Papers\2004.13820v2.pdf
# Kích thước file: 2784225 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Sự Phát triển của Độ Tương đồng Ngữ nghĩa - Một Khảo sát
DHIVYA CHANDRASEKARAN và VIJAY MAGO, Đại học Lakehead

Ước lượng độ tương đồng ngữ nghĩa giữa dữ liệu văn bản là một trong những bài toán nghiên cứu thách thức và còn mở trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP). Tính đa dạng của ngôn ngữ tự nhiên khiến việc định nghĩa các phương pháp dựa trên quy tắc để xác định các độ đo tương đồng ngữ nghĩa trở nên khó khăn. Để giải quyết vấn đề này, nhiều phương pháp tương đồng ngữ nghĩa khác nhau đã được đề xuất qua các năm. Bài khảo sát này truy tìm sự phát triển của các phương pháp đó bắt đầu từ các kỹ thuật NLP truyền thống như phương pháp dựa trên kernel đến công trình nghiên cứu gần đây nhất về các mô hình dựa trên transformer, phân loại chúng dựa trên các nguyên lý cơ bản như các phương pháp dựa trên kiến thức, dựa trên corpus, dựa trên mạng neural sâu, và các phương pháp lai. Thảo luận về điểm mạnh và điểm yếu của từng phương pháp, khảo sát này cung cấp cái nhìn toàn diện về các hệ thống hiện có, cho các nhà nghiên cứu mới thử nghiệm và phát triển các ý tưởng sáng tạo để giải quyết vấn đề tương đồng ngữ nghĩa.

CCS Concepts: •Tổng quát và tham khảo →Khảo sát và tổng quan ;•Hệ thống thông tin →
Ontology ;•Lý thuyết tính toán →Học không giám sát và phân cụm ;•Phương pháp luận 
tính toán →Ngữ nghĩa từ vựng .

Từ khóa và Cụm từ bổ sung: tương đồng ngữ nghĩa, ngôn ngữ học, phương pháp có giám sát và không giám sát, phương pháp dựa trên kiến thức, word embeddings, phương pháp dựa trên corpus

Định dạng Tham khảo ACM:
Dhivya Chandrasekaran và Vijay Mago. 2020. Evolution of Semantic Similarity - A Survey. 1, 1 (February 2020), 35 pages. https://doi.org/---------

1 GIỚI THIỆU
Với sự gia tăng theo cấp số nhân của dữ liệu văn bản được tạo ra theo thời gian, Xử lý Ngôn ngữ Tự nhiên (NLP) đã nhận được sự chú ý đáng kể từ các chuyên gia Trí tuệ Nhân tạo (AI). Đo lường độ tương đồng ngữ nghĩa giữa các thành phần văn bản khác nhau như từ, câu, hoặc tài liệu đóng vai trò quan trọng trong một loạt các nhiệm vụ NLP như truy xuất thông tin [48], tóm tắt văn bản [80], phân loại văn bản [49], đánh giá bài luận [42], dịch máy [134], trả lời câu hỏi [19,66], trong số những nhiệm vụ khác. Trong những ngày đầu, hai đoạn văn bản được coi là tương tự nếu chúng chứa cùng các từ/ký tự. Các kỹ thuật như Bag of Words (BoW), Term Frequency - Inverse Document Frequency (TF-IDF) được sử dụng để biểu diễn văn bản, như các vector giá trị thực để hỗ trợ tính toán độ tương đồng ngữ nghĩa. Tuy nhiên, những kỹ thuật này không tính đến thực tế rằng các từ có nghĩa khác nhau và các từ khác nhau có thể được sử dụng để biểu diễn một khái niệm tương tự. Ví dụ, xem xét hai câu "John và David đã học Toán và Khoa học." và "John đã học Toán và David đã học Khoa học." Mặc dù hai câu này có chính xác cùng các từ nhưng chúng không truyền đạt cùng một ý nghĩa. Tương tự, các câu "Mary bị dị ứng với các sản phẩm từ sữa." và "Mary không dung nạp lactose." truyền đạt cùng một ý nghĩa; tuy nhiên, chúng không có cùng một tập hợp các từ. Những phương pháp này nắm bắt được đặc trưng từ vựng của văn bản và đơn giản để triển khai, tuy nhiên, chúng bỏ qua các thuộc tính ngữ nghĩa và cú pháp của văn bản. Để giải quyết những hạn chế này của các độ đo từ vựng, nhiều kỹ thuật tương đồng ngữ nghĩa khác nhau đã được đề xuất trong ba thập kỷ qua.

Độ Tương đồng Văn bản Ngữ nghĩa (STS) được định nghĩa là độ đo tương đương ngữ nghĩa giữa hai khối văn bản. Các phương pháp tương đồng ngữ nghĩa thường đưa ra một xếp hạng hoặc tỷ lệ phần trăm tương đồng giữa các văn bản, thay vì một quyết định nhị phân là tương tự hay không tương tự. Độ tương đồng ngữ nghĩa thường được sử dụng đồng nghĩa với mối liên quan ngữ nghĩa. Tuy nhiên, mối liên quan ngữ nghĩa không chỉ tính đến độ tương đồng ngữ nghĩa giữa các văn bản mà còn xem xét một góc nhìn rộng hơn phân tích các thuộc tính ngữ nghĩa chung của hai từ. Ví dụ, các từ 'cà phê' và 'cốc' có thể liên quan chặt chẽ với nhau, nhưng chúng không được coi là tương đồng ngữ nghĩa trong khi các từ 'cà phê' và 'trà' thì tương đồng ngữ nghĩa. Do đó, độ tương đồng ngữ nghĩa có thể được coi là một trong những khía cạnh của mối liên quan ngữ nghĩa. Mối quan hệ ngữ nghĩa bao gồm tương đồng được đo bằng khoảng cách ngữ nghĩa, tỷ lệ nghịch với mối quan hệ [37].

Hình 1. Kiến trúc Khảo sát

1.1 Động lực đằng sau khảo sát
Hầu hết các bài khảo sát được xuất bản gần đây liên quan đến độ tương đồng ngữ nghĩa, cung cấp kiến thức sâu sắc về một kỹ thuật tương đồng ngữ nghĩa cụ thể hoặc một ứng dụng duy nhất của độ tương đồng ngữ nghĩa. Lastra-Díaz et al. khảo sát các phương pháp dựa trên kiến thức khác nhau [55] và các phương pháp dựa trên IC [53], Camacho-Collados et al. [20] thảo luận các phương pháp biểu diễn vector khác nhau của từ, Taieb et al. [37], mặt khác, mô tả các phương pháp liên quan ngữ nghĩa khác nhau và Berna Altınel et al. [8] tóm tắt các phương pháp tương đồng ngữ nghĩa khác nhau được sử dụng cho phân loại văn bản. Động lực đằng sau khảo sát này là cung cấp một báo cáo toàn diện về các kỹ thuật tương đồng ngữ nghĩa khác nhau bao gồm những tiến bộ gần đây nhất sử dụng các phương pháp dựa trên mạng neural sâu.

--- TRANG 2 ---
2 D Chandrasekaran và V Mago

--- TRANG 3 ---
Sự Phát triển của Độ Tương đồng Ngữ nghĩa - Một Khảo sát 3

Khảo sát này truy tìm sự phát triển của các Kỹ thuật Tương đồng Ngữ nghĩa qua các thập kỷ qua, phân biệt chúng dựa trên các phương pháp cơ bản được sử dụng trong đó. Hình 1 hiển thị cấu trúc của khảo sát. Một báo cáo chi tiết về các dataset được sử dụng rộng rãi có sẵn cho độ tương đồng ngữ nghĩa được cung cấp trong Phần 2. Các Phần 3 đến 6 cung cấp mô tả chi tiết về các phương pháp tương đồng ngữ nghĩa được phân loại rộng rãi thành 1) Phương pháp dựa trên kiến thức, 2) Phương pháp dựa trên corpus, 3) Phương pháp dựa trên mạng neural sâu, và 4) Phương pháp lai. Phần 7 phân tích các khía cạnh và suy luận khác nhau của khảo sát được tiến hành. Khảo sát này cung cấp kiến thức sâu và rộng về các kỹ thuật hiện có cho các nhà nghiên cứu mới mạo hiểm khám phá một trong những nhiệm vụ NLP thách thức nhất, Độ Tương đồng Văn bản Ngữ nghĩa.

2 DATASETS
Trong phần này, chúng tôi thảo luận về một số dataset phổ biến được sử dụng để đánh giá hiệu suất của các thuật toán tương đồng ngữ nghĩa. Các dataset có thể bao gồm các cặp từ hoặc các cặp câu với các giá trị tương đồng tiêu chuẩn liên quan. Hiệu suất của các thuật toán tương đồng ngữ nghĩa khác nhau được đo bằng sự tương quan của các kết quả đạt được với các độ đo tiêu chuẩn có sẵn trong các dataset này.

Bảng 1 liệt kê một số dataset phổ biến được sử dụng để đánh giá hiệu suất của các thuật toán tương đồng ngữ nghĩa. Phần tiếp theo mô tả các thuộc tính của dataset và phương pháp được sử dụng để xây dựng chúng.

Tên Dataset | Cặp Từ/Câu | Khoảng điểm tương đồng | Năm | Tham khảo
R&G | 65 | 0-4 | 1965 | [107]
M&C | 30 | 0-4 | 1991 | [78]
WS353 | 353 | 0-10 | 2002 | [30]
LiSent | 65 | 0-4 | 2007 | [63]  
SRS | 30 | 0-4 | 2007 | [94]
WS353-Sim | 203 | 0-10 | 2009 | [1]
STS2012 | 5250 | 0-5 | 2012 | [5]
STS2013 | 2250 | 0-5 | 2013 | [6]
WP300 | 300 | 0-1 | 2013 | [61]
STS2014 | 3750 | 0-5 | 2014 | [3]
SL7576 | 7576 | 1-5 | 2014 | [116]
SimLex-999 | 999 | 0-10 | 2014 | [40]
SICK | 10000 | 1-5 | 2014 | [69]
STS2015 | 3000 | 0-5 | 2015 | [2]
SimVerb | 3500 | 0-10 | 2016 | [34]
STS2016 | 1186 | 0-5 | 2016 | [4]
WiC | 5428 | NA | 2019 | [97]

Bảng 1. Các dataset chuẩn phổ biến cho Tương đồng ngữ nghĩa

--- TRANG 4 ---
4 D Chandrasekaran và V Mago

2.1 Dataset tương đồng ngữ nghĩa
Sau đây là danh sách các dataset tương đồng ngữ nghĩa được sử dụng rộng rãi được sắp xếp theo thứ tự thời gian.

• Rubenstein và Goodenough (R&G) [107]: Dataset này được tạo ra như kết quả của một thí nghiệm được thực hiện với 51 sinh viên đại học (người bản ngữ tiếng Anh) trong hai phiên khác nhau. Các đối tượng được cung cấp 65 cặp danh từ tiếng Anh được chọn lọc và được yêu cầu gán điểm tương đồng cho mỗi cặp trên thang điểm từ 0 đến 4, trong đó 0 thể hiện rằng các từ hoàn toàn không tương đồng và 4 thể hiện rằng chúng rất tương đồng. Dataset này là dataset đầu tiên và được sử dụng rộng rãi nhất trong các nhiệm vụ tương đồng ngữ nghĩa [133].

• Miller và Charles (M&C) [78]: Miller và Charles đã lặp lại thí nghiệm được thực hiện bởi Rubenstein và Goodenough vào năm 1991 với một tập con gồm 30 cặp từ từ 65 cặp từ gốc. 38 đối tượng con người đã xếp hạng các cặp từ trên thang điểm từ 0 đến 4, 4 có nghĩa là "tương đồng nhất."

• WS353 [30]: WS353 chứa 353 cặp từ với điểm số liên quan từ 0 đến 10. 0 thể hiện mức độ tương đồng thấp nhất và 10 thể hiện mức độ tương đồng cao nhất. Thí nghiệm được thực hiện với một nhóm 16 đối tượng con người. Dataset này đo lường mối liên quan ngữ nghĩa hơn là tương đồng ngữ nghĩa. Sau đó, dataset tiếp theo đã được đề xuất.

• WS353-Sim [1]: Dataset này là một tập con của WS353 chứa 203 cặp từ từ 353 cặp từ ban đầu phù hợp hơn đặc biệt cho các thuật toán tương đồng ngữ nghĩa.

• LiSent [63]: 65 cặp câu được xây dựng sử dụng định nghĩa từ điển của 65 cặp từ được sử dụng trong dataset R&G. 32 người bản ngữ tiếng Anh tình nguyện cung cấp khoảng tương đồng từ 0 đến 4, 4 là cao nhất. Giá trị trung bình của các điểm số được đưa ra bởi tất cả các tình nguyện viên được lấy làm điểm số cuối cùng.

• SRS [94]: Pedersen et al. [94] đã cố gắng xây dựng một dataset tương đồng ngữ nghĩa chuyên biệt cho lĩnh vực y sinh. Ban đầu 120 cặp được một bác sĩ lựa chọn phân bố với 30 cặp trên 4 giá trị tương đồng. Các cặp thuật ngữ này sau đó được xếp hạng bởi 13 người mã hóa y tế trên thang điểm 1-10. 30 cặp từ từ 120 cặp được chọn để tăng độ tin cậy và các cặp từ này được chú thích bởi 3 bác sĩ và 9 (trong số 13) người mã hóa y tế để tạo thành dataset cuối cùng.

• SimLex-999 [40]: 999 cặp từ được chọn từ Dataset UFS [89] trong đó 900 cặp tương đồng và 99 cặp liên quan nhưng không tương đồng. 500 người bản ngữ tiếng Anh, được tuyển dụng qua Amazon Mechanical Turk được yêu cầu xếp hạng sự tương đồng giữa các cặp từ trên thang điểm từ 0 đến 6, 6 là tương đồng nhất. Dataset chứa 666 cặp danh từ, 222 cặp động từ, và 111 cặp tính từ.

• Dataset Câu Liên quan Kiến thức Tổng hợp (SICK) [69]: Dataset SICK bao gồm 10.000 cặp câu, được dẫn xuất từ hai dataset hiện có là ImageFlickr 8 và dataset mô tả MSR-Video. Mỗi cặp câu được liên kết với một điểm liên quan và một quan hệ entailment văn bản. Điểm liên quan dao động từ 1 đến 5, và ba quan hệ entailment là "NEUTRAL, ENTAILMENT và CONTRADICTION." Việc chú thích được thực hiện bằng kỹ thuật crowd-sourcing.

• Dataset STS [2–6,24]: Các dataset STS được xây dựng bằng cách kết hợp các cặp câu từ các nguồn khác nhau bởi các tổ chức của nhiệm vụ chia sẻ SemEVAL. Dataset được chú thích bằng Amazon Mechanical Turk và được xác minh thêm bởi chính các tổ chức. Bảng 2 hiển thị các nguồn khác nhau từ đó dataset STS được xây dựng.

Năm | Dataset | Cặp | Nguồn
2012 | MSRPar | 1500 | newswire
2012 | MSRvid | 1500 | videos
2012 | OnWN | 750 | glosses
2012 | SMTNews | 750 | WMT eval.
2012 | SMTeuroparl | 750 | WMT eval.
2013 | HDL | 750 | newswire
2013 | FNWN | 189 | glosses
2013 | OnWN | 561 | glosses
2013 | SMT | 750 | MT eval.

--- TRANG 5 ---
Sự Phát triển của Độ Tương đồng Ngữ nghĩa - Một Khảo sát 5

Bảng 2 tiếp tục từ trang trước
Năm | Dataset | Cặp | Nguồn
2014 | HDL | 750 | newswire headlines
2014 | OnWN | 750 | glosses
2014 | Deft-forum | 450 | forum posts
2014 | Deft-news | 300 | news summary
2014 | Images | 750 | image descriptions
2014 | Tweet-news | 750 | tweet-news pairs
2015 | HDL | 750 | newswire headlines
2015 | Images | 750 | image descriptions
2015 | Ans.-student | 750 | student answers
2015 | Ans.-forum | 375 | Q & A forum answers
2015 | Belief | 375 | committed belief
2016 | HDL | 249 | newswire headlines
2016 | Plagiarism | 230 | short-answers plag.
2016 | post-editing | 244 | MT postedits
2016 | Ans.-Ans | 254 | Q & A forum answers
2016 | Quest.-Quest. | 209 | Q & A forum questions
2017 | Trail | 23 | Mixed STS 2016

Bảng 2. Dataset huấn luyện tiếng Anh STS (2012-2017) [24]

3 PHƯƠNG PHÁP TƯƠNG ĐỒNG NGỮ NGHĨA DỰA TRÊN KIẾN THỨC

Các phương pháp tương đồng ngữ nghĩa dựa trên kiến thức tính toán độ tương đồng ngữ nghĩa giữa hai thuật ngữ dựa trên thông tin có được từ một hoặc nhiều nguồn kiến thức cơ bản như ontology/cơ sở dữ liệu từ vựng, từ điển đồng nghĩa, từ điển, v.v. Cơ sở kiến thức cơ bản cung cấp cho các phương pháp này một biểu diễn có cấu trúc của các thuật ngữ hoặc khái niệm được kết nối bởi các quan hệ ngữ nghĩa, hơn nữa cung cấp một độ đo ngữ nghĩa không có sự mơ hồ, vì ý nghĩa thực tế của các thuật ngữ được xem xét [123]. Trong phần này, chúng tôi thảo luận về bốn cơ sở dữ liệu từ vựng được sử dụng rộng rãi trong các phương pháp tương đồng ngữ nghĩa dựa trên kiến thức và thảo luận ngắn gọn các phương pháp khác nhau được một số phương pháp tương đồng ngữ nghĩa dựa trên kiến thức áp dụng.

3.1 Cơ sở dữ liệu từ vựng

• WordNet [77] là một cơ sở dữ liệu từ vựng được sử dụng rộng rãi cho các phương pháp tương đồng ngữ nghĩa dựa trên kiến thức bao gồm hơn 100.000 khái niệm tiếng Anh [123]. WordNet có thể được hình dung như một đồ thị, trong đó các nút biểu diễn ý nghĩa của các từ (khái niệm), và các cạnh định nghĩa mối quan hệ giữa các từ [133]. Cấu trúc của WordNet chủ yếu dựa trên từ đồng nghĩa, trong đó mỗi từ có các synset khác nhau được gán cho các ý nghĩa khác nhau của chúng. Sự tương đồng giữa hai từ phụ thuộc vào khoảng cách đường đi giữa chúng [93].

• Wiktionary¹ là một cơ sở dữ liệu từ vựng mã nguồn mở bao gồm khoảng 6,2 triệu từ từ 4.000 ngôn ngữ khác nhau. Mỗi mục có một trang bài viết liên quan và nó tính đến một nghĩa khác nhau của mỗi mục. Wiktionary không có một mối quan hệ từ vựng phân loại được thiết lập tốt trong các mục, không giống như WordNet, điều này khiến nó khó được sử dụng trong các thuật toán tương đồng ngữ nghĩa [99].

¹https://en.wiktionary.org

--- TRANG 6 ---
6 D Chandrasekaran và V Mago

• Với sự ra đời của Wikipedia², hầu hết các kỹ thuật tương đồng ngữ nghĩa khai thác dữ liệu văn bản phong phú có sẵn miễn phí để huấn luyện các mô hình [74]. Wikipedia có dữ liệu văn bản được tổ chức thành các Bài viết. Mỗi bài viết có một tiêu đề (khái niệm), hàng xóm, mô tả, và danh mục. Nó được sử dụng như cả dữ liệu phân loại có cấu trúc và/hoặc như một corpus để huấn luyện các phương pháp dựa trên corpus [100]. Cấu trúc danh mục phức tạp của Wikipedia được sử dụng như một đồ thị để xác định Nội dung Thông tin của các khái niệm, điều này lần lượt hỗ trợ tính toán độ tương đồng ngữ nghĩa [44].

• BabelNet [88] là một tài nguyên từ vựng kết hợp WordNet với dữ liệu có sẵn trên Wikipedia cho mỗi synset. Nó là ontology ngữ nghĩa đa ngôn ngữ lớn nhất có sẵn với gần hơn 13 triệu synset và 380 triệu quan hệ ngữ nghĩa trong 271 ngôn ngữ. Nó bao gồm hơn bốn triệu synset với ít nhất một trang Wikipedia liên quan cho tiếng Anh [22].

3.2 Các loại phương pháp tương đồng ngữ nghĩa dựa trên kiến thức

Dựa trên nguyên lý cơ bản về cách đánh giá độ tương đồng ngữ nghĩa giữa các từ, các phương pháp tương đồng ngữ nghĩa dựa trên kiến thức có thể được phân loại thêm thành các phương pháp đếm cạnh, phương pháp dựa trên đặc trưng, và phương pháp dựa trên nội dung thông tin.

3.2.1 Phương pháp đếm cạnh: Phương pháp đếm cạnh đơn giản nhất là xem xét ontology cơ bản như một đồ thị kết nối các từ theo phân loại và đếm các cạnh giữa hai thuật ngữ để đo lường sự tương đồng giữa chúng. Càng xa khoảng cách giữa các thuật ngữ thì chúng càng ít tương đồng. Độ đo này được gọi là path được đề xuất bởi Rada et al. [102] trong đó sự tương đồng tỷ lệ nghịch với độ dài đường đi ngắn nhất giữa hai thuật ngữ. Trong phương pháp đếm cạnh này, thực tế là các từ ở sâu hơn trong hệ thống phân cấp có ý nghĩa cụ thể hơn, và rằng, chúng có thể tương đồng với nhau hơn mặc dù chúng có cùng khoảng cách như hai từ đại diện cho một khái niệm chung hơn không được xem xét. Wu và Palmer [131] đề xuất độ đo wup, trong đó độ sâu của các từ trong ontology được coi là một thuộc tính quan trọng. Độ đo wup đếm số cạnh giữa mỗi thuật ngữ và Least Common Subsumer (LCS) của chúng. LCS là tổ tiên chung được chia sẻ bởi cả hai thuật ngữ trong ontology đã cho. Xem xét, hai thuật ngữ được ký hiệu là t₁,t₂, LCS của chúng được ký hiệu là t_lcs, và độ dài đường đi ngắn nhất giữa chúng được ký hiệu là min_len(t₁,t₂),

path được đo như sau:
sim_path(t₁,t₂) = 1/(1+min_len(t₁,t₂))    (1)

và wup được đo như sau:
sim_wup(t₁,t₂) = 2depth(t_lcs)/(depth(t₁)+depth(t₂))    (2)

Li et al. [62] đề xuất một độ đo tính đến cả khoảng cách đường đi tối thiểu và độ sâu. li được đo như sau:
sim_li = e^(-αmin_len(t₁,t₂)) · (e^(βdepth(t_lcs)) - e^(-βdepth(t_lcs)))/(e^(βdepth(t_lcs)) + e^(-βdepth(t_lcs)))    (3)

Tuy nhiên, các phương pháp đếm cạnh bỏ qua thực tế rằng các cạnh trong ontology không nhất thiết phải có độ dài bằng nhau. Để khắc phục khuyết điểm này của các phương pháp đếm cạnh đơn giản, các phương pháp tương đồng ngữ nghĩa dựa trên đặc trưng đã được đề xuất.

²http://www.wikipedia.org

--- TRANG 7 ---
Sự Phát triển của Độ Tương đồng Ngữ nghĩa - Một Khảo sát 7

3.2.2 Phương pháp dựa trên đặc trưng: Các phương pháp dựa trên đặc trưng tính toán sự tương đồng như một hàm của các thuộc tính của các từ, như gloss, các khái niệm lân cận, v.v. [123]. Gloss được định nghĩa là ý nghĩa của một từ trong từ điển; một tập hợp các gloss được gọi là từ điển thuật ngữ. Có nhiều phương pháp tương đồng ngữ nghĩa khác nhau được đề xuất dựa trên gloss của các từ. Các độ đo tương đồng ngữ nghĩa dựa trên gloss khai thác kiến thức rằng các từ có ý nghĩa tương tự có nhiều từ chung hơn trong gloss của chúng. Sự tương đồng ngữ nghĩa được đo bằng mức độ chồng lấp giữa gloss của các từ đang xem xét. Độ đo Lesk [11], gán một giá trị liên quan giữa hai từ dựa trên sự chồng lấp của các từ trong gloss của chúng và các gloss của các khái niệm mà chúng liên quan đến trong một ontology như WordNet [55]. Jiang et al. [45] đề xuất một phương pháp dựa trên đặc trưng trong đó sự tương đồng ngữ nghĩa được đo bằng các gloss của các khái niệm có mặt trong Wikipedia. Hầu hết các phương pháp dựa trên đặc trưng tính đến các đặc trưng chung và không chung giữa hai từ/thuật ngữ. Các đặc trưng chung góp phần tăng giá trị tương đồng và các đặc trưng không chung làm giảm giá trị tương đồng. Hạn chế chính của các phương pháp dựa trên đặc trưng là sự phụ thuộc vào các ontology có các đặc trưng ngữ nghĩa, và hầu hết các ontology ít khi tích hợp bất kỳ đặc trưng ngữ nghĩa nào khác ngoài các mối quan hệ phân loại [123].

3.2.3 Phương pháp dựa trên nội dung thông tin: Nội dung thông tin (IC) của một khái niệm được định nghĩa là thông tin có được từ khái niệm khi nó xuất hiện trong ngữ cảnh [122]. Một giá trị IC cao cho thấy rằng từ đó cụ thể hơn và mô tả rõ ràng một khái niệm với ít sự mơ hồ hơn, trong khi các giá trị IC thấp hơn cho thấy rằng các từ có ý nghĩa trừu tượng hơn [133]. Tính cụ thể của từ được xác định bằng Tần số Tài liệu Nghịch đảo (IDF), dựa trên nguyên tắc rằng từ càng cụ thể thì càng ít xuất hiện trong tài liệu. Các phương pháp dựa trên nội dung thông tin đo lường sự tương đồng giữa các thuật ngữ bằng giá trị IC liên quan với chúng. Resnik và Philip [104] đề xuất một độ đo tương đồng ngữ nghĩa gọi là res đo lường sự tương đồng dựa trên ý tưởng rằng nếu hai khái niệm chia sẻ một subsumer chung, chúng chia sẻ nhiều thông tin hơn vì giá trị IC của LCS cao hơn. Xem xét IC đại diện cho Nội dung Thông tin của thuật ngữ đã cho,

res được đo như sau:
sim_res(t₁,t₂) = IC_t_lcs    (4)

D. Lin [64] đề xuất một mở rộng của độ đo res xem xét giá trị IC của cả hai thuật ngữ gán cho thông tin cá nhân hoặc mô tả của các thuật ngữ và giá trị IC của LCS của chúng cung cấp tính chung được chia sẻ giữa các thuật ngữ. lin được đo như sau:

sim_lin(t₁,t₂) = 2IC_t_lcs/(IC_t₁ + IC_t₂)    (5)

Jiang và Conrath [43] tính toán một độ đo khoảng cách dựa trên sự khác biệt giữa tổng các giá trị IC cá nhân của các thuật ngữ và giá trị IC của LCS của chúng bằng phương trình dưới đây:

dis_jcn(t₁,t₂) = IC_t₁ + IC_t₂ - 2IC_t_lcs    (6)

Độ đo khoảng cách thay thế độ dài đường đi ngắn nhất trong phương trình (1), và sự tương đồng tỷ lệ nghịch với khoảng cách trên. Do đó jcn được đo như sau:

sim_jcn(t₁,t₂) = 1/(1 + dis_jcn(t₁,t₂))    (7)

IC có thể được đo bằng một corpus cơ bản hoặc từ cấu trúc nội tại của chính ontology [108] dựa trên giả định rằng các ontology được cấu trúc theo cách có ý nghĩa. Một số thuật ngữ có thể không được bao gồm trong một ontology, điều này cung cấp phạm vi để sử dụng nhiều ontology để tính toán mối quan hệ của chúng [105]. Dựa trên việc liệu các thuật ngữ đã cho có cả hai đều có mặt trong một ontology duy nhất hay không, các phương pháp dựa trên IC có thể được phân loại thành các phương pháp mono-ontological hoặc multi-ontological. Khi nhiều ontology tham gia, IC của Least Common Subsumer

--- TRANG 8 ---
8 D Chandrasekaran và V Mago

từ cả hai ontology được truy cập để ước lượng các giá trị tương đồng ngữ nghĩa. Jiang et al. [44] đề xuất các độ đo tương đồng ngữ nghĩa dựa trên IC dựa trên các trang, khái niệm và hàng xóm của Wikipedia. Wikipedia đã được sử dụng cả như một phân loại có cấu trúc cũng như một corpus để cung cấp các giá trị IC.

3.2.4 Phương pháp dựa trên kiến thức kết hợp: Nhiều độ đo tương đồng khác nhau đã được đề xuất kết hợp các phương pháp dựa trên kiến thức khác nhau. Goa et al. [33] đề xuất một phương pháp tương đồng ngữ nghĩa dựa trên ontology WordNet trong đó ba chiến lược khác nhau được sử dụng để thêm trọng số vào các cạnh và đường đi có trọng số ngắn nhất được sử dụng để đo lường sự tương đồng ngữ nghĩa. Theo chiến lược đầu tiên, độ sâu của tất cả các thuật ngữ trong WordNet dọc theo đường đi giữa hai thuật ngữ đang xem xét được thêm như một trọng số vào đường đi ngắn nhất. Trong chiến lược thứ hai, chỉ độ sâu của LCS của các thuật ngữ được thêm như trọng số, và trong chiến lược thứ ba, giá trị IC của các thuật ngữ được thêm như trọng số. Độ dài đường đi có trọng số ngắn nhất hiện được tính toán và sau đó được biến đổi phi tuyến để tạo ra các độ đo tương đồng ngữ nghĩa. So sánh cho thấy chiến lược thứ ba đạt được sự tương quan tốt hơn với các tiêu chuẩn vàng so với các phương pháp truyền thống và hai chiến lược khác được đề xuất. Zhu và Iglesias [133] đề xuất một độ đo đường đi có trọng số khác gọi là wpath thêm giá trị IC của Least Common Subsumer như một trọng số vào độ dài đường đi ngắn nhất.

wpath được tính như sau:
sim_wpath(t₁,t₂) = 1/(1 + min_len(t₁,t₂) * k^IC_t_lcs)    (8)

Phương pháp này được đề xuất để sử dụng trong các đồ thị kiến thức (KG) khác nhau như WordNet [77], DBPedia [17], YAGO [41], v.v. và tham số k là một siêu tham số phải được điều chỉnh cho các KG khác nhau và các lĩnh vực khác nhau vì các KG khác nhau có phân phối khác nhau của các thuật ngữ trong mỗi lĩnh vực. Cả giá trị IC dựa trên corpus và IC nội tại đều được thử nghiệm và độ đo wpath dựa trên IC corpus đạt được sự tương quan lớn hơn trong hầu hết các dataset tiêu chuẩn vàng.

Các phương pháp tương đồng ngữ nghĩa dựa trên kiến thức đơn giản về mặt tính toán, và cơ sở kiến thức cơ bản hoạt động như một xương sống mạnh mẽ cho các mô hình, và vấn đề phổ biến nhất về sự mơ hồ như từ đồng nghĩa, thành ngữ, và cụm từ được xử lý hiệu quả. Các phương pháp dựa trên kiến thức có thể dễ dàng được mở rộng để tính toán độ đo tương đồng từ câu đến câu bằng cách định nghĩa các quy tắc tổng hợp [58]. Lastra-Díaz et al. [54] đã phát triển một thư viện phần mềm Half-Edge Semantic Measures Library (HESML) để triển khai các độ đo tương đồng ngữ nghĩa dựa trên ontology khác nhau được đề xuất và đã chỉ ra sự gia tăng trong thời gian hiệu suất và khả năng mở rộng của các mô hình.

Tuy nhiên, các hệ thống dựa trên kiến thức phụ thuộc cao vào nguồn cơ bản dẫn đến nhu cầu cập nhật chúng thường xuyên đòi hỏi thời gian và tài nguyên tính toán cao. Mặc dù các ontology mạnh như WordNet tồn tại cho tiếng Anh, các tài nguyên tương tự không có sẵn cho các ngôn ngữ khác dẫn đến nhu cầu xây dựng các cơ sở kiến thức mạnh và có cấu trúc để triển khai các phương pháp dựa trên kiến thức trong các ngôn ngữ khác nhau và qua các lĩnh vực khác nhau. Nhiều công trình nghiên cứu đã được thực hiện về việc mở rộng các độ đo tương đồng ngữ nghĩa trong lĩnh vực y sinh [94,118]. McInnes et al. [71] đã xây dựng một mô hình chuyên biệt gọi là UMLS để đo lường sự tương đồng giữa các từ trong lĩnh vực y sinh. Với gần 6.500 ngôn ngữ thế giới và nhiều lĩnh vực, điều này trở thành một nhược điểm nghiêm trọng cho các hệ thống dựa trên kiến thức.

4 PHƯƠNG PHÁP TƯƠNG ĐỒNG NGỮ NGHĨA DỰA TRÊN CORPUS

Các phương pháp tương đồng ngữ nghĩa dựa trên corpus đo lường độ tương đồng ngữ nghĩa giữa các thuật ngữ bằng thông tin thu được từ các corpus lớn. Nguyên lý cơ bản được gọi là 'giả thuyết phân phối' [36] khai thác ý tưởng rằng "các từ tương tự xuất hiện cùng nhau, thường xuyên"; tuy nhiên, ý nghĩa thực tế của các từ không được xem xét. Trong khi nhiều kỹ thuật khác nhau được sử dụng để xây dựng biểu diễn vector của dữ liệu văn bản, một số độ đo khoảng cách ngữ nghĩa dựa trên giả thuyết phân phối

--- TRANG 9 ---
Sự Phát triển của Độ Tương đồng Ngữ nghĩa - Một Khảo sát 9

đã được đề xuất để ước lượng sự tương đồng giữa các vector. Một khảo sát toàn diện về các độ đo ngữ nghĩa phân phối khác nhau đã được thực hiện bởi Mohammad và Hurst [81], và các độ đo khác nhau và công thức tương ứng của chúng được cung cấp trong Bảng 4 trong Phụ lục A. Tuy nhiên, trong tất cả các độ đo này, độ tương đồng cosine có được ý nghĩa và đã được sử dụng rộng rãi trong các nhà nghiên cứu NLP đến nay [81]. Trong phần này, chúng tôi thảo luận chi tiết về một số word embedding được sử dụng rộng rãi được xây dựng bằng giả thuyết phân phối và một số phương pháp tương đồng ngữ nghĩa dựa trên corpus quan trọng.

4.1 Word Embeddings

Word embedding cung cấp biểu diễn vector của các từ trong đó các vector này giữ lại mối quan hệ ngôn ngữ cơ bản giữa các từ [111]. Các vector này được tính toán bằng các phương pháp tiếp cận khác nhau như mạng neural [75], ma trận đồng xuất hiện từ [95], hoặc biểu diễn theo ngữ cảnh mà từ xuất hiện [59]. Một số word embedding được huấn luyện trước được sử dụng rộng rãi nhất bao gồm:

• word2vec [75]: Được phát triển từ dataset Google News, chứa khoảng 3 triệu biểu diễn vector của các từ và cụm từ, word2vec là một mô hình mạng neural được sử dụng để tạo ra biểu diễn vector phân phối của các từ dựa trên một corpus cơ bản. Có hai mô hình khác nhau của word2vec được đề xuất: Continuous Bag of Words (CBOW) và mô hình Skip-gram. Kiến trúc của mạng khá đơn giản và chứa một lớp đầu vào, một lớp ẩn, và một lớp đầu ra. Mạng được cung cấp một corpus văn bản lớn làm đầu vào, và đầu ra của mô hình là các biểu diễn vector của các từ. Mô hình CBOW dự đoán từ hiện tại bằng các từ ngữ cảnh lân cận, trong khi mô hình Skip-gram dự đoán các từ ngữ cảnh lân cận được cho một từ mục tiêu. Các mô hình word2vec hiệu quả trong việc biểu diễn các từ như các vector giữ lại sự tương đồng ngữ cảnh giữa các từ. Các tính toán vector từ đã mang lại kết quả tốt trong việc dự đoán độ tương đồng ngữ nghĩa [76]. Nhiều nhà nghiên cứu đã mở rộng mô hình word2vec để đề xuất các vector ngữ cảnh [73], vector từ điển [127], vector câu [91] và vector đoạn văn [56].

• GloVe [95]: GloVe được phát triển bởi Đại học Stanford dựa trên ma trận đồng xuất hiện từ toàn cục được hình thành dựa trên corpus cơ bản. Nó ước lượng sự tương đồng dựa trên nguyên tắc rằng các từ tương tự nhau xuất hiện cùng nhau. Ma trận đồng xuất hiện được điền đầy với các giá trị xuất hiện bằng cách thực hiện một lần duyệt qua các corpus lớn cơ bản. Mô hình 𝐺𝑙𝑜𝑉𝑒 được huấn luyện bằng cách sử dụng năm corpus khác nhau, chủ yếu là các bản dump Wikipedia. Trong khi tạo thành các vector, các từ được chọn trong một cửa sổ ngữ cảnh cụ thể do thực tế là các từ ở xa có ít liên quan đến từ ngữ cảnh đang xem xét. Hàm loss của 𝐺𝑙𝑜𝑉𝑒 tối thiểu hóa khoảng cách bình phương tối thiểu giữa các giá trị đồng xuất hiện cửa sổ ngữ cảnh và các giá trị đồng xuất hiện toàn cục [55]. Các vector 𝐺𝑙𝑜𝑉𝑒 được mở rộng để tạo thành các vector từ theo ngữ cảnh nhằm phân biệt các từ dựa trên ngữ cảnh [70].

•fastText [18]: Các nhà nghiên cứu AI của Facebook đã phát triển một mô hình nhúng từ xây dựng các vector từ dựa trên các mô hình Skip-gram trong đó mỗi từ được biểu diễn như một tập hợp các n-gram ký tự. 𝑓𝑎𝑠𝑡𝑇𝑒𝑥𝑡 học các nhúng từ dưới dạng trung bình của các nhúng ký tự của nó do đó tính đến cấu trúc hình thái của từ điều này chứng minh là hiệu quả trong các ngôn ngữ khác nhau như tiếng Phần Lan và tiếng Thổ Nhĩ Kỳ. Ngay cả các từ ngoài vốn từ vựng cũng được gán các vector từ dựa trên các ký tự hoặc đơn vị con của chúng.

•Bidirectional Encoder Representations from Transformers(BERT) [29]: Devlin và cộng sự [29] đã đề xuất một nhúng từ dựa trên transformer được huấn luyện trước có thể được tinh chỉnh bằng cách thêm một lớp đầu ra cuối cùng để điều chỉnh các nhúng cho các tác vụ NLP khác nhau. BERT sử dụng kiến trúc transformer được đề xuất bởi Vaswani và cộng sự [128], tạo ra các vector từ dựa trên attention bằng cách sử dụng bộ mã hóa transformer hai chiều. Khung BERT bao gồm hai quy trình quan trọng được gọi là 'huấn luyện trước' và 'tinh chỉnh'. Mô hình được huấn luyện trước bằng cách sử dụng một corpus gần 3.300M từ từ cả Book corpus và English Wikipedia. Vì mô hình là hai chiều để tránh khả năng mô hình biết chính token đó khi huấn luyện từ cả hai hướng, quy trình huấn luyện trước được thực hiện theo hai cách khác nhau. Trong tác vụ đầu tiên, các từ ngẫu nhiên trong corpus được che và mô hình được huấn luyện để dự đoán những từ này. Trong tác vụ thứ hai, mô hình được trình bày với các cặp câu từ corpus, trong đó 50% các câu thực sự liên tiếp trong khi phần còn lại là các cặp ngẫu nhiên. Mô hình được huấn luyện để dự đoán xem cặp câu đã cho có liên tiếp hay không. Trong quy trình 'tinh chỉnh', mô hình được huấn luyện cho tác vụ NLP downstream cụ thể trong tay. Mô hình được cấu trúc để nhận đầu vào cả câu đơn lẫn nhiều câu để đáp ứng nhiều tác vụ NLP. Để huấn luyện mô hình thực hiện tác vụ hỏi đáp, mô hình được cung cấp với các cặp câu hỏi-trả lời khác nhau và tất cả các tham số được tinh chỉnh theo tác vụ. Các nhúng BERT cung cấp kết quả tiên tiến nhất trong tập dữ liệu STS-B với hệ số tương quan Spearman là 86.5% vượt trội hơn các mô hình BiLSTM khác bao gồm ELMo [96].

Word embeddings được sử dụng để đo lường độ tương tự ngữ nghĩa giữa các văn bản của các ngôn ngữ khác nhau bằng cách ánh xạ nhúng từ của một ngôn ngữ lên không gian vector của ngôn ngữ khác. Khi huấn luyện với một số lượng hạn chế nhưng đủ các cặp dịch, ma trận dịch có thể được tính toán để cho phép sự chồng chéo của các nhúng qua các ngôn ngữ [35]. Một trong những thách thức chính khi triển khai word-embeddings để đo lường độ tương tự là Meaning Conflation Deficiency. Nó chỉ ra rằng word embeddings không gán cho các nghĩa khác nhau của một từ điều này làm ô nhiễm không gian ngữ nghĩa với nhiễu bằng cách đưa các từ không liên quan gần nhau hơn. Ví dụ, các từ 'finance' và 'river' có thể xuất hiện trong cùng một không gian ngữ nghĩa vì từ 'bank' có hai nghĩa khác nhau [20]. Điều quan trọng cần hiểu là word-embeddings khai thác giả thuyết phân phối để xây dựng các vector và dựa vào các corpus lớn, do đó, chúng được phân loại dưới các phương pháp đo lường độ tương tự ngữ nghĩa dựa trên corpus. Tuy nhiên, các phương pháp dựa trên mạng neural sâu và hầu hết các phương pháp đo lường độ tương tự ngữ nghĩa lai sử dụng word-embeddings để chuyển đổi dữ liệu văn bản thành các vector chiều cao, và hiệu quả của các nhúng này đóng vai trò quan trọng trong hiệu suất của các phương pháp đo lường độ tương tự ngữ nghĩa [60, 79].

4.2 Các loại phương pháp đo lường độ tương tự ngữ nghĩa dựa trên corpus

Dựa trên các phương pháp cơ bản sử dụng để xây dựng word-vectors có nhiều loại phương pháp dựa trên corpus khác nhau, một số trong đó được thảo luận trong phần này.

4.2.1 Latent Semantic Analysis (LSA) [51]: LSA là một trong những kỹ thuật dựa trên corpus phổ biến và được sử dụng rộng rãi nhất để đo lường độ tương tự ngữ nghĩa. Một ma trận đồng xuất hiện từ được tạo thành trong đó các hàng biểu diễn các từ và các cột biểu diễn các đoạn văn, và các ô được điền với số lượng từ. Ma trận này được tạo thành với một corpus lớn cơ bản, và việc giảm chiều được đạt bằng một kỹ thuật toán học gọi là Singular Value Decomposition (SVD). SVD biểu diễn một ma trận đã cho như tích của ba ma trận, trong đó hai ma trận biểu diễn các hàng và cột như các vector được dẫn xuất từ các eigenvalue của chúng và ma trận thứ ba là một ma trận đường chéo có các giá trị sẽ tái tạo ma trận gốc khi nhân với hai ma trận khác [52]. SVD giảm số lượng cột trong khi giữ nguyên số lượng hàng do đó bảo tồn cấu trúc tương tự giữa các từ. Sau đó mỗi từ được biểu diễn như một vector bằng cách sử dụng các giá trị trong các hàng tương ứng của nó và độ tương tự ngữ nghĩa được tính toán như giá trị cosine giữa các vector này. Các mô hình LSA được tổng quát hóa bằng cách thay thế các từ bằng văn bản và các cột bằng các mẫu khác nhau và được sử dụng để tính toán độ tương tự giữa các câu, đoạn văn và tài liệu.

4.2.2 Hyperspace Analogue to Language(HAL) [68]: HAL xây dựng một ma trận đồng xuất hiện từ có cả hàng và cột biểu diễn các từ trong vốn từ vựng và các phần tử ma trận được điền với các giá trị cường độ kết hợp. Các giá trị cường độ kết hợp được tính bằng cách trượt một "cửa sổ" có kích thước có thể thay đổi qua corpus cơ bản. Cường độ kết hợp giữa các từ trong cửa sổ giảm theo sự tăng khoảng cách của chúng từ từ được tập trung xem xét. Ví dụ, trong câu "This is a survey of various semantic similarity measures", các từ 'survey' và 'variety' có giá trị kết hợp lớn hơn các từ 'survey' và 'measures'. Các vector từ được tạo thành bằng cách xem xét cả hàng và cột của từ đã cho. Việc giảm chiều được đạt bằng cách loại bỏ bất kỳ cột nào có giá trị entropy thấp. Độ tương tự ngữ nghĩa sau đó được tính bằng cách đo khoảng cách Euclidean hoặc Manhattan giữa các vector từ.

4.2.3 Explicit Semantic Analysis (ESA) [31]: ESA đo lường độ tương tự ngữ nghĩa dựa trên các khái niệm Wikipedia. Việc sử dụng Wikipedia đảm bảo rằng phương pháp được đề xuất có thể được sử dụng trong các lĩnh vực và ngôn ngữ khác nhau. Vì Wikipedia được cập nhật liên tục, phương pháp này có thể thích ứng với các thay đổi theo thời gian. Đầu tiên, mỗi khái niệm trong Wikipedia được biểu diễn như một vector thuộc tính của các từ xuất hiện trong nó, sau đó một chỉ mục ngược được tạo thành, trong đó mỗi từ được liên kết với tất cả các khái niệm mà nó được kết hợp với. Cường độ kết hợp được tính trọng số bằng kỹ thuật TF-IDF, và các khái niệm kết hợp yếu với các từ được loại bỏ. Do đó văn bản đầu vào được biểu diễn bằng các vector có trọng số của các khái niệm gọi là "interpretation vectors". Độ tương tự ngữ nghĩa được đo bằng cách tính toán độ tương tự cosine giữa các vector từ này.

4.2.4 Word-Alignment models [120]: Các mô hình Word-Alignment tính toán độ tương tự ngữ nghĩa của các câu dựa trên sự căn chỉnh của chúng trên một corpus lớn [24,47,119]. Vị trí thứ hai, thứ ba và thứ năm trong các tác vụ SemEval 2015 được đảm bảo bởi các phương pháp dựa trên word alignment. Phương pháp không giám sát ở vị trí thứ năm đã triển khai kỹ thuật word alignment dựa trên Paraphrase Database (PPDB) [32]. Hệ thống tính toán độ tương tự ngữ nghĩa giữa hai câu như một tỷ lệ của các từ ngữ cảnh được căn chỉnh trong các câu so với tổng số từ trong cả hai câu. Các phương pháp có giám sát ở vị trí thứ hai và thứ ba đã sử dụng 𝑤𝑜𝑟𝑑 2𝑣𝑒𝑐 để có được sự căn chỉnh của các từ. Trong phương pháp đầu tiên, một vector câu được tạo thành bằng cách tính "component-wise average" của các từ trong câu, và độ tương tự cosine giữa các vector câu này được sử dụng như một thước đo độ tương tự ngữ nghĩa. Phương pháp có giám sát thứ hai chỉ xem xét những từ có độ tương tự ngữ nghĩa theo ngữ cảnh [120].

4.2.5 Latent Dirichlet Allocation (LDA) [117]: LDA được sử dụng để biểu diễn một chủ đề hoặc ý tưởng chung đằng sau một tài liệu như một vector thay vì mỗi từ trong tài liệu. Kỹ thuật này được sử dụng rộng rãi cho các tác vụ mô hình hóa chủ đề và nó có lợi thế của việc giảm chiều xem xét rằng các chủ đề ít hơn đáng kể so với các từ thực tế trong một tài liệu [117]. Một trong những cách tiếp cận mới để xác định độ tương tự tài liệu-đến-tài liệu là việc sử dụng biểu diễn vector của các tài liệu và tính toán độ tương tự cosine giữa các vector để xác định độ tương tự ngữ nghĩa giữa các tài liệu [16].

4.2.6 Normalised Google Distance [25]: NGD đo lường độ tương tự giữa hai thuật ngữ dựa trên kết quả thu được khi các thuật ngữ được truy vấn bằng công cụ tìm kiếm Google. Nó dựa trên giả định rằng hai từ xuất hiện cùng nhau thường xuyên hơn trong các trang web nếu chúng có liên quan hơn. Cho hai thuật ngữ 𝑡1 và 𝑡2, công thức sau được sử dụng để tính NGD giữa hai thuật ngữ.

𝑁𝐺𝐷(𝑥,𝑦)=𝑚𝑎𝑥{𝑙𝑜𝑔 𝑓(𝑡1),𝑙𝑜𝑔 𝑓(𝑡2)}−𝑙𝑜𝑔 𝑓(𝑡1,𝑡2) / 𝑙𝑜𝑔𝐺−𝑚𝑖𝑛{𝑙𝑜𝑔 𝑓(𝑡1),𝑙𝑜𝑔 𝑓(𝑡2)}(9)

trong đó các hàm 𝑓(𝑥) và 𝑓(𝑦) trả về số lượng kết quả trong tìm kiếm Google của các thuật ngữ đã cho, 𝑓(𝑥,𝑦) trả về số lượng kết quả trong tìm kiếm Google khi các thuật ngữ được tìm kiếm cùng nhau và 𝐺 đại diện cho tổng số trang trong tìm kiếm google tổng thể. NGD được sử dụng rộng rãi để đo lường liên quan ngữ nghĩa hơn là độ tương tự ngữ nghĩa vì các thuật ngữ liên quan xuất hiện cùng nhau thường xuyên hơn trong các trang web mặc dù chúng có thể có nghĩa đối lập.

4.2.7 Dependency-based models [1]: Các cách tiếp cận dựa trên dependency xác định nghĩa của một từ hoặc cụm từ đã cho bằng cách sử dụng các láng giềng của từ trong một cửa sổ đã cho. Các mô hình dựa trên dependency ban đầu phân tích corpus dựa trên phân phối của nó bằng Inductive Dependency Parsing [90]. Đối với mỗi từ đã cho, một "syntactic context template" được xây dựng xem xét cả các nút đi trước và đi sau từ trong cây phân tích được xây dựng. Ví dụ, cụm từ "thinks <term> delicious" có thể có một template ngữ cảnh như "pizza, burger, food". Biểu diễn vector của một từ được tạo thành bằng cách thêm mỗi cửa sổ qua vị trí có từ đang xem xét làm từ gốc của nó, cùng với tần suất của cửa sổ từ xuất hiện trong toàn bộ corpus. Khi vector này được tạo thành, độ tương tự ngữ nghĩa được tính bằng độ tương tự cosine giữa các vector này. Levy và cộng sự [59] đã đề xuất DEPS embedding như một mô hình word-embedding dựa trên dependency-based bag of words. Mô hình này được thử nghiệm với tập dữ liệu WS353 trong đó tác vụ là xếp hạng các từ tương tự cao hơn các từ liên quan. Khi vẽ đường cong recall precision, đường cong DEPS cho thấy ưu tiên lớn hơn đối với xếp hạng độ tương tự so với các phương pháp BoW được so sánh.

4.2.8 Kernel-based models [115]: Các phương pháp dựa trên kernel được sử dụng để tìm các mẫu trong dữ liệu văn bản do đó cho phép phát hiện độ tương tự giữa các đoạn văn bản. Hai loại kernel chính được sử dụng trong dữ liệu văn bản cụ thể là string hoặc sequence kernel [23] và tree kernel [84]. Moschitti và cộng sự [84] đề xuất tree kernels vào năm 2007, chứa ba cấu trúc con khác nhau trong không gian tree kernel cụ thể là subtree - một cây có gốc không phải là nút lá cùng với các nút con của nó, subset tree - một cây có gốc không phải là nút lá nhưng không kết hợp tất cả các nút con của nó và không phá vỡ các quy tắc ngữ pháp, partial tree - một cấu trúc cây tương tự chặt chẽ với subset tree nhưng nó không phải lúc nào cũng tuân theo các quy tắc ngữ pháp. Tree kernels được sử dụng rộng rãi trong việc xác định cấu trúc trong các câu đầu vào dựa trên constituency hoặc dependency, xem xét các quy tắc ngữ pháp của ngôn ngữ. Kernels được sử dụng bởi các thuật toán machine learning như Support Vector Machines(SVMs) để thích ứng với dữ liệu văn bản trong các tác vụ khác nhau như Semantic Role Labelling, Paraphrase Identification [28], Answer Extraction [85], Question-Answer classification [86], Relational text categorization [83], Answer Re-ranking in QA tasks [112] và Relational text entailment [87]. Severyn và cộng sự [113] đề xuất một phương pháp đo lường độ tương tự ngữ nghĩa dựa trên kernel biểu diễn văn bản trực tiếp như "structural objects" bằng cách sử dụng Syntactic tree kernel [27] và Partial tree kernels [82]. Hàm kernel sau đó kết hợp các cấu trúc cây với các vector đặc trưng ngữ nghĩa từ hai mô hình hoạt động tốt nhất trong STS 2012 cụ thể là UKP [12] và Takelab [110] và một số đặc trưng bổ sung bao gồm điểm độ tương tự cosine dựa trên named entities, part of speech tags, v.v. Các tác giả so sánh hiệu suất của mô hình được xây dựng bằng bốn cấu trúc cây khác nhau cụ thể là shallow tree, constituency tree, dependency tree, phrase-dependency tree, và các vector đặc trưng được đề cập ở trên. Họ thiết lập rằng các mô hình tree kernel hoạt động tốt hơn tất cả các vector đặc trưng kết hợp. Mô hình sử dụng Support Vector Regression để có được điểm độ tương tự cuối cùng và nó có thể hữu ích trong các ứng dụng NLP downstream khác nhau như question-answering, text-entailment extraction, v.v. Amir và cộng sự [9] đề xuất một thuật toán đo lường độ tương tự ngữ nghĩa khác sử dụng các hàm kernel. Họ sử dụng tree kernels dựa trên constituency trong đó câu được phân tích thành subject, verb, và object dựa trên giả định rằng hầu hết các thuộc tính ngữ nghĩa của câu được gán cho các thành phần này. Các câu đầu vào được phân tích bằng Stanford Parser để trích xuất các kết hợp khác nhau của subject, verb, và object. Độ tương tự giữa các thành phần khác nhau của các câu đã cho được tính bằng một knowledge base, và các kỹ thuật tính trung bình khác nhau được sử dụng để tính trung bình các giá trị độ tương tự để ước tính độ tương tự tổng thể, và kỹ thuật tốt nhất trong số chúng được chọn dựa trên giá trị root mean squared error cho một tập dữ liệu cụ thể. Trong nghiên cứu gần đây, các phương pháp deep learning đã được sử dụng để thay thế các mô hình machine learning truyền thống và sử dụng hiệu quả tính toàn vẹn cấu trúc của kernels trong giai đoạn trích xuất đặc trưng nhúng [26,28]. Mô hình đạt kết quả tốt nhất trong SemEval-2017 Task 1, được đề xuất bởi Tian và cộng sự [125] sử dụng kernels để trích xuất đặc trưng từ dữ liệu văn bản để tính toán độ tương tự. Mô hình đề xuất một mô hình ensemble sử dụng cả phương pháp NLP truyền thống và phương pháp deep learning. Hai đặc trưng khác nhau cụ thể là sentence pair matching features và single sentence features được sử dụng để dự đoán các giá trị độ tương tự bằng các regressor thêm tính phi tuyến vào dự đoán. Trong trích xuất single sentence feature, dependency-based tree kernels được sử dụng để trích xuất các đặc trưng dependency trong một câu đã cho, và trong sentence pair matching features, constituency-based parse tree kernels được sử dụng để tìm các cấu trúc con chung giữa ba đặc tính khác nhau của không gian tree kernel. Điểm độ tương tự cuối cùng được truy cập bằng cách tính trung bình giá trị độ tương tự NLP truyền thống và giá trị độ tương tự dựa trên deep learning. Mô hình đạt hệ số tương quan Pearson 73.16% trong tập dữ liệu STS.

4.2.9 Word-attention models [57]: Trong hầu hết các phương pháp dựa trên corpus, tất cả các thành phần văn bản được xem xét có tầm quan trọng bằng nhau; tuy nhiên, việc diễn giải của con người về đo lường độ tương tự thường phụ thuộc vào các từ khóa trong một ngữ cảnh đã cho. Các mô hình word attention nắm bắt tầm quan trọng của các từ từ các corpus cơ bản [67] trước khi tính toán độ tương tự ngữ nghĩa. Các kỹ thuật khác nhau như word frequency, alignment, word association được sử dụng để nắm bắt attention-weights của văn bản đang xem xét. Attention Constituency Vector Tree (ACV-Tree) được đề xuất bởi Le và cộng sự [57] tương tự như một parse tree trong đó một từ của câu được làm gốc và phần còn lại của câu được phân tích như một Noun Phrase (NP) và một Verb Phrase (VP). Các nút trong cây lưu trữ ba thuộc tính khác nhau của từ đang xem xét: word vector được xác định bởi một corpus cơ bản, attention-weight, và "modification-relations" của từ. Modification relations có thể được định nghĩa là các tính từ hoặc trạng từ sửa đổi nghĩa của từ khác. Tất cả ba thành phần được liên kết để tạo thành biểu diễn của từ. Một hàm tree kernel được sử dụng để xác định độ tương tự giữa hai từ dựa trên phương trình dưới đây:

𝑇𝑟𝑒𝑒𝐾𝑒𝑟𝑛𝑒𝑙(𝑇1,𝑇2)=∑︁𝑛1∈𝑁𝑇1∑︁𝑛2∈𝑁𝑇2Δ(𝑛1,𝑛2) (10)

Δ(𝑛1,𝑛2)=(0,if(𝑛1and / or𝑛2are non-leaf-nodes) and 𝑛1≠𝑛2
𝐴𝑤×𝑆𝐼𝑀(𝑣𝑒𝑐 1,𝑣𝑒𝑐 2),if𝑛1,𝑛2are leaf nodes
𝜇(𝜆2+Í𝑙𝑚𝑝=1𝛿𝑝(𝑐𝑛1,𝑐𝑛2)),otherwise(11)

trong đó 𝑛1,𝑛2 biểu diễn các nút, 𝑆𝐼𝑀(𝑣𝑒𝑐 1,𝑣𝑒𝑐 2) đo độ tương tự cosine giữa các vector, 𝛿𝑝(.) tính số lượng các dãy con chung có độ dài 𝑝, 𝜆,𝜇 biểu thị các yếu tố decay cho độ dài của các dãy con và chiều cao của cây tương ứng, 𝑐𝑛1, 𝑐𝑛2 đề cập đến các nút con và 𝑙𝑚=𝑚𝑖𝑛(𝑙𝑒𝑛𝑔𝑡ℎ(𝑐𝑛1),𝑙𝑒𝑛𝑔𝑡ℎ(𝑐𝑛2)). Thuật toán được thử nghiệm bằng các tập dữ liệu benchmark STS và đã cho thấy hiệu suất tốt hơn trong 12 trên 19 tập dữ liệu STS được chọn [57, 101].

Không giống như các hệ thống dựa trên kiến thức, các hệ thống dựa trên corpus độc lập với ngôn ngữ và lĩnh vực [8]. Vì chúng phụ thuộc vào các thước đo thống kê, các phương pháp có thể dễ dàng thích ứng qua các ngôn ngữ khác nhau bằng cách sử dụng một corpus hiệu quả. Với sự phát triển của internet, việc xây dựng corpus của hầu hết các ngôn ngữ hoặc lĩnh vực đã trở nên khá dễ dàng. Các kỹ thuật web crawling đơn giản có thể được sử dụng để xây dựng các corpus lớn [13]. Tuy nhiên, các phương pháp dựa trên corpus không xem xét nghĩa thực tế của các từ. Thách thức khác mà các phương pháp dựa trên corpus gặp phải là nhu cầu xử lý các corpus lớn được xây dựng, đây là một tác vụ khá tốn thời gian và phụ thuộc vào tài nguyên. Vì hiệu suất của các thuật toán phụ thuộc lớn vào corpus cơ bản, việc xây dựng một corpus hiệu quả là điều tối quan trọng. Mặc dù các nhà nghiên cứu đã nỗ lực để xây dựng một corpus sạch và hiệu quả như corpus C4 được xây dựng bằng web crawling và năm bước để làm sạch corpus [103], một "corpus lý tưởng" vẫn chưa được các nhà nghiên cứu xác định.

5 CÁC PHƯƠNG PHÁP DỰA TRÊN MẠNG NEURAL SÂU

Các phương pháp đo lường độ tương tự ngữ nghĩa đã khai thác những phát triển gần đây trong mạng neural để nâng cao hiệu suất. Các kỹ thuật được sử dụng rộng rãi nhất bao gồm Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM), Bidirectional Long Short Term Memory (Bi-LSTM), và Recursive Tree LSTM. Các mô hình mạng neural sâu được xây dựng dựa trên hai thao tác cơ bản: convolution và pooling. Thao tác convolution trong dữ liệu văn bản có thể được định nghĩa là tổng của tích element-wise của một vector câu và một ma trận trọng số. Các thao tác convolution được sử dụng để trích xuất đặc trưng. Các thao tác pooling được sử dụng để loại bỏ các đặc trưng có tác động tiêu cực, và chỉ xem xét những giá trị đặc trưng có tác động đáng kể đến tác vụ trong tay. Có các loại thao tác pooling khác nhau và được sử dụng rộng rãi nhất là Max pooling, trong đó chỉ giá trị tối đa trong không gian filter đã cho được chọn. Phần này mô tả một số phương pháp triển khai mạng neural sâu để ước tính độ tương tự ngữ nghĩa giữa các đoạn văn bản. Mặc dù các phương pháp được mô tả dưới đây khai thác word embeddings được xây dựng bằng các corpus lớn, mạng neural sâu được sử dụng để ước tính độ tương tự giữa các word-embeddings, do đó chúng được phân loại riêng khỏi các phương pháp dựa trên corpus.

5.1 Các loại phương pháp đo lường độ tương tự ngữ nghĩa dựa trên mạng neural sâu:

•Wang và cộng sự [130] đề xuất một mô hình để ước tính độ tương tự ngữ nghĩa giữa hai câu dựa trên lexical decomposition và composition. Mô hình sử dụng các nhúng được huấn luyện trước 𝑤𝑜𝑟𝑑 2𝑣𝑒𝑐 để tạo thành biểu diễn vector của các câu 𝑠1 và 𝑠2. Một ma trận tương tự 𝑀 với chiều 𝑖x𝑗 được xây dựng trong đó i và j là số lượng từ trong câu 1 (𝑆1) và câu 2 (𝑆2) tương ứng. Các ô của ma trận được điền với độ tương tự cosine giữa các từ trong các chỉ số của ma trận. Ba hàm khác nhau được sử dụng để xây dựng các vector semantic matching ®𝑠1 và ®𝑠2, global function, local function, và max function. Global function xây dựng semantic matching vector của 𝑆1 bằng cách lấy tổng có trọng số của các vector, của tất cả các từ trong 𝑆2, local function, chỉ xem xét các word vectors trong một kích thước cửa sổ đã cho, và max function chỉ lấy các vector của các từ có độ tương tự tối đa. Giai đoạn thứ hai của thuật toán sử dụng ba decomposition functions khác nhau - rigid, linear, và orthogonal - để ước tính similarity component và dissimilarity component giữa các sentence vectors và semantic matching vectors. Cả similarity component và dissimilarity component vectors được truyền qua một lớp convolution hai kênh theo sau bởi một lớp max-pooling đơn. Độ tương tự sau đó được tính bằng một lớp sigmoid ước tính giá trị độ tương tự trong phạm vi từ 0 đến 1. Mô hình được thử nghiệm bằng tập dữ liệu QASent [129] và tập dữ liệu WikiQA [72]. Hai thước đo được sử dụng để ước tính hiệu suất là mean average precision (MAP) và mean reciprocal rank (MRR). Mô hình đạt MAP tốt nhất trong tập dữ liệu QASent và MAP và MRR tốt nhất trong tập dữ liệu WikiQA. Yang Shao [114] đề xuất một thuật toán đo lường độ tương tự ngữ nghĩa khai thác phát triển gần đây trong mạng neural bằng cách sử dụng word embeddings 𝐺𝑙𝑜𝑉𝑒. Cho hai câu, mô hình dự đoán một phân phối xác suất trên tập các giá trị độ tương tự ngữ nghĩa. Các bước tiền xử lý bao gồm loại bỏ dấu câu, tokenization, và sử dụng các vector 𝐺𝑙𝑜𝑉𝑒 để thay thế các từ bằng word embeddings. Độ dài của đầu vào được đặt thành 30 từ, điều này được đạt bằng cách loại bỏ hoặc padding khi cần thiết. Một số đặc trưng thủ công đặc biệt như flag values chỉ ra liệu các từ hoặc số có xuất hiện trong cả hai câu và POS tagging one hot encoded values, được thêm vào các vector 𝐺𝑙𝑜𝑉𝑒. Các vector sau đó được đưa vào một CNN với 300 filters và một lớp max-pooling được sử dụng để tạo thành các sentence vectors. Hàm kích hoạt ReLU được sử dụng trong lớp convolution. Sự khác biệt ngữ nghĩa giữa các vector được tính bằng element-wise absolute difference và element-wise multiplication của hai sentence-vectors được tạo. Các vector được truyền tiếp qua hai lớp fully-connected, dự đoán phân phối xác suất của các giá trị độ tương tự ngữ nghĩa. Hiệu suất của mô hình được đánh giá bằng các tập dữ liệu SemEval trong đó mô hình được xếp hạng thứ 3 trong track tập dữ liệu SemEval 2017.

•Mạng LSTM là một loại đặc biệt của Recurrent Neural Networks (RNN). Trong khi xử lý dữ liệu văn bản, điều cần thiết cho các mạng là nhớ các từ trước đó, để nắm bắt ngữ cảnh, và RNNs có khả năng làm như vậy. Tuy nhiên, không phải tất cả nội dung trước đó đều có ý nghĩa đối với từ/cụm từ tiếp theo, do đó RNNs gặp nhược điểm của long term dependency. LSTMs được thiết kế để khắc phục vấn đề này. LSTMs có các gates cho phép mạng chọn nội dung nó phải nhớ. Ví dụ, xem xét đoạn văn bản, "Mary is from Finland. She is fluent in Finnish. She loves to travel." Trong khi chúng ta đến câu thứ hai của đoạn văn bản, điều cần thiết là nhớ các từ "Mary" và "Finland." Tuy nhiên, khi đến câu thứ ba, mạng có thể quên từ "Finland." Kiến trúc của LSTMs cho phép điều này. Nhiều nhà nghiên cứu sử dụng kiến trúc LSTM để đo lường độ tương tự ngữ nghĩa giữa các khối văn bản. Tien và cộng sự [126] sử dụng một mạng kết hợp với LSTM và CNN để tạo thành sentence embedding từ các word embeddings được huấn luyện trước theo sau bởi một kiến trúc LSTM để dự đoán độ tương tự của chúng. Tai và cộng sự [124] đề xuất một kiến trúc LSTM để ước tính độ tương tự ngữ nghĩa giữa hai câu đã cho. Ban đầu, các câu được chuyển đổi thành sentence representations bằng cách sử dụng Tree-LSTM trên parse tree của các câu. Những sentence representations này sau đó được đưa vào một mạng neural tính toán khoảng cách tuyệt đối giữa các vector và góc giữa các vector. Thí nghiệm được tiến hành bằng tập dữ liệu SICK, và thước đo độ tương tự thay đổi với phạm vi từ 1 đến 5. Lớp ẩn bao gồm 50 neurons và lớp softmax cuối cùng phân loại các câu trong phạm vi đã cho. Mô hình Tree-LSTM đạt được hệ số tương quan Pearson và Spearman tốt hơn trong các tập dữ liệu gold standard, so với các mô hình mạng neural khác được so sánh.

•He và Lin [39] đề xuất một kiến trúc lai sử dụng Bi-LSTM và CNN để ước tính độ tương tự ngữ nghĩa của mô hình. Bi-LSTMs có hai LSTMs chạy song song, một từ đầu câu và một từ cuối, do đó nắm bắt toàn bộ ngữ cảnh. Trong mô hình của họ, He và Lin sử dụng Bi-LSTM cho context modelling. Một pairwise word interaction model được xây dựng tính toán một comparison unit giữa các vector được dẫn xuất từ các hidden states của hai LSTMs bằng cách sử dụng công thức dưới đây:

𝐶𝑜𝑈(®ℎ1,®ℎ2)={𝑐𝑜𝑠(®ℎ1,®ℎ2),𝑒𝑢𝑐(®ℎ1,®ℎ2),𝑚𝑎𝑛ℎ((®ℎ1,®ℎ2)} (12)

trong đó ®ℎ1 và ®ℎ2 biểu diễn các vector từ hidden state của LSTMs và các hàm 𝑐𝑜𝑠(),𝑒𝑢𝑐(),𝑚𝑎𝑛ℎ() tính toán Cosine distance, Euclidean distance, và Manhattan distance, tương ứng. Mô hình này tương tự như các mô hình word attention dựa trên mạng neural gần đây khác [7,10]. Tuy nhiên, attention weights không được thêm vào, thay vào đó các khoảng cách được thêm như weights. Word interaction model được theo sau bởi một similarity focus layer trong đó weights được thêm vào các word interactions (được tính trong các lớp trước đó) dựa trên tầm quan trọng của chúng trong việc xác định độ tương tự. Những re-weighted vectors này được đưa vào mạng convolution cuối cùng. Mạng bao gồm các lớp spatial convolution và spatial max pooling xen kẽ, hàm kích hoạt ReLU được sử dụng và ở cuối mạng kết thúc bằng hai lớp fully connected theo sau bởi một lớp LogSoftmax để có được một giải pháp phi tuyến. Mô hình này vượt trội hơn mô hình Tree-LSTM được đề cập trước đó trên tập dữ liệu SICK.

•Lopez-Gazpio và cộng sự [67] đề xuất một phần mở rộng cho Decomposable Attention Model (DAM) hiện có được đề xuất bởi Parikh và cộng sự [92] ban đầu được sử dụng cho Natural Language Inference(NLI). NLI được sử dụng để phân loại một khối văn bản đã cho thành một quan hệ cụ thể như entailment, neutral, hoặc contradiction. Mô hình DAM sử dụng feed-forward neural networks trong ba lớp liên tiếp là attention layer, comparison layer, và aggregation layer. Cho hai câu, attention layer tạo ra hai attention vectors cho mỗi câu bằng cách tìm sự chồng chéo giữa chúng. Comparison layer nối các attention vectors với sentence vectors để tạo thành một representative vector duy nhất cho mỗi câu. Aggregation layer cuối cùng làm phẳng các vector và tính toán phân phối xác suất trên các giá trị đã cho. Lopez-Gazpio và cộng sự [67] sử dụng word n-grams để nắm bắt attention trong lớp đầu tiên thay vì các từ riêng lẻ. 𝑛−𝑔𝑟𝑎𝑚𝑠 có thể được định nghĩa là một chuỗi n từ liền kề với từ đã cho, n-grams được sử dụng để nắm bắt ngữ cảnh trong các tác vụ NLP khác nhau. Để chứa n-grams, một Recurrent Neural Network (RNN) được thêm vào attention layer. Các biến thể được đề xuất bằng cách thay thế RNN bằng Long-Term Short memory (LSTM) và Convolutional Neural Network (CNN). Mô hình được sử dụng cho các tính toán độ tương tự ngữ nghĩa bằng cách thay thế các lớp cuối cùng của entailment relationships bằng các phạm vi độ tương tự ngữ nghĩa từ 0 đến 5. Các mô hình đạt hiệu suất tốt hơn trong việc nắm bắt độ tương tự ngữ nghĩa trong tập dữ liệu SICK và tập dữ liệu benchmark STS khi so sánh với DAM và các mô hình khác như Sent2vec [91] và BiLSTM trong số những mô hình khác.

•Transformer-based models: Vaswani và cộng sự [128] đề xuất một mô hình transformer dựa trên các cơ chế attention để nắm bắt các thuộc tính ngữ nghĩa của các từ trong các embeddings. Transformer có hai phần 'encoder' và 'decoder'. Encoder bao gồm các lớp multi-head attention mechanisms theo sau bởi một mạng neural feed-forward fully connected. Decoder tương tự như encoder với một lớp bổ sung của multi-head attention nắm bắt attention weights trong đầu ra của encoder. Mặc dù mô hình này được đề xuất cho tác vụ machine translation, Devlin và cộng sự [29] đã sử dụng mô hình transformer để tạo ra BERT word embeddings. Sun và cộng sự [121] đề xuất một framework multi-tasking sử dụng transformers gọi là ERNIE 2.0. Trong framework này, mô hình được huấn luyện trước liên tục tức là khi một tác vụ mới được trình bày, mô hình được tinh chỉnh để chứa tác vụ mới trong khi giữ lại kiến thức đã có trước đó. Mô hình vượt trội hơn BERT. XLNet được đề xuất bởi Yang và cộng sự [132] sử dụng một mô hình autoregression trái ngược với mô hình autoencoder và vượt trội hơn BERT và ERNIE 2.0. Một số biến thể của các mô hình BERT được đề xuất dựa trên corpus được sử dụng để huấn luyện mô hình và bằng cách tối ưu hóa các tài nguyên tính toán. Lan và cộng sự [50] đề xuất ALBERT, với hai kỹ thuật để giảm độ phức tạp tính toán của BERT cụ thể là 'factorized embedding parameterization' và 'cross-layer parameter sharing'. ALBERT vượt trội hơn tất cả ba mô hình trên. Các biến thể khác của các mô hình BERT sử dụng transformers bao gồm TinyBERT [46], RoBERTa [65,109], và một biến thể specific-domain được huấn luyện trên một corpus khoa học với tập trung vào lĩnh vực BioMedical là SciBERT [15]. Raffel và cộng sự [103] đề xuất một mô hình transformer với một corpus được định nghĩa rõ ràng gọi là 'Colossal Clean Crawled Corpus' hoặc C4 để huấn luyện mô hình có tên T5-11B. Không giống như BERT, họ áp dụng một 'text-to-text framework' trong đó input sequence được đính kèm với một token để xác định tác vụ NLP cần thực hiện do đó loại bỏ hai giai đoạn pre-training và fine-tuning. Họ đề xuất năm phiên bản khác nhau của mô hình của họ dựa trên số lượng tham số có thể huấn luyện mà mỗi mô hình có cụ thể là 1) T5-Small 2) T5-Base 3) T5-Large 4) T5-3B và 5)T511B và chúng có 60 triệu, 220 triệu, 770 triệu, 3 tỷ, và 11 tỷ tham số tương ứng. Mô hình này vượt trội hơn tất cả các mô hình dựa trên transformer khác và đạt kết quả tiên tiến nhất. Kết quả từ nghiên cứu của họ, họ xác nhận rằng hiệu suất của các mô hình tăng lên với dữ liệu và sức mạnh tính toán tăng lên và hiệu suất có thể được cải thiện thêm nếu các mô hình lớn hơn được xây dựng và điều quan trọng cần lưu ý là để tái tạo mô hình tốt nhất của họ cần năm GPUs trong số các tài nguyên khác. Một biên soạn các mô hình dựa trên transformer khác nhau và hệ số tương quan Pearson của chúng trên tập dữ liệu STS-B được cung cấp dưới đây trong Bảng 3.

Tên Mô Hình | Tiêu Đề | Năm | Hệ Số Tương Quan Pearson
T5-11B | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | 2019 | 0.925
XLNet | XLNet: Generalized Autoregressive Pretraining for Language Understanding | 2019 | 0.925
ALBERT | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | 2019 | 0.925
RoBERTa | RoBERTa: A Robustly Optimized BERT Pretraining Approach | 2019 | 0.922
ERNIE 2.0 | ERNIE 2.0: A Continual Pre-training Framework for Language Understanding | 2019 | 0.912
DistilBERT | DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter | 2019 | 0.907
TinyBERT | TinyBERT: Distilling BERT for Natural Language Understanding | 2019 | 0.799

Bảng 3. Hệ số tương quan Pearson của các mô hình dựa trên transformer khác nhau trên tập dữ liệu benchmark STS.

Các phương pháp dựa trên mạng neural sâu vượt trội hơn hầu hết các phương pháp truyền thống và sự thành công gần đây của các mô hình dựa trên transformer đã đóng vai trò như một bước đột phá trong nghiên cứu độ tương tự ngữ nghĩa. Tuy nhiên, việc triển khai các mô hình deep learning đòi hỏi tài nguyên tính toán lớn, mặc dù các biến thể của mô hình để giảm thiểu tài nguyên tính toán đang được đề xuất, chúng ta thấy rằng hiệu suất của mô hình cũng bị ảnh hưởng, ví dụ như TinyBERT [46]. Và hiệu suất của các mô hình được cải thiện đáng kể bằng việc sử dụng corpus lớn hơn, điều này lại đặt ra thách thức xây dựng một corpus lý tưởng. Hầu hết các mô hình deep learning là các mô hình "hộp đen" và khó có thể xác định các đặc trưng mà hiệu suất được đạt được dựa trên đó, do đó trở nên khó giải thích không giống như trường hợp các phương pháp dựa trên corpus có nền tảng toán học mạnh mẽ. Các lĩnh vực khác nhau như tài chính, bảo hiểm, v.v., xử lý dữ liệu nhạy cảm có thể miễn cưỡng triển khai các phương pháp dựa trên mạng neural sâu do thiếu khả năng giải thích.

6 CÁC PHƯƠNG PHÁP LAI

Dựa trên tất cả các phương pháp đã thảo luận trước đó, chúng ta thấy rằng mỗi phương pháp đều có ưu điểm và nhược điểm. Các phương pháp dựa trên kiến thức khai thác các ontology cơ bản để phân biệt từ đồng nghĩa, trong khi các phương pháp dựa trên corpus thì linh hoạt vì chúng có thể được sử dụng qua các ngôn ngữ. Các hệ thống dựa trên mạng neural sâu, mặc dù tốn kém về mặt tính toán, cung cấp kết quả tốt hơn. Tuy nhiên, nhiều nhà nghiên cứu đã tìm ra cách khai thác những điểm tốt nhất của mỗi phương pháp và xây dựng các mô hình lai để đo lường độ tương tự ngữ nghĩa. Trong phần này, chúng tôi mô tả các phương pháp được sử dụng trong một số mô hình lai được sử dụng rộng rãi.

6.1 Các loại phương pháp đo lường độ tương tự ngữ nghĩa lai:

•Novel Approach to a Semantically-Aware Representation of Items (NASARI) [21]: Camacho Collados và cộng sự [21] đề xuất một phương pháp NASARI trong đó nguồn kiến thức BabelNet được sử dụng để xây dựng một corpus mà dựa trên đó biểu diễn vector cho các khái niệm (từ hoặc nhóm từ) được hình thành. Ban đầu, các trang Wikipedia liên kết với một khái niệm đã cho, trong trường hợp này là synset của BabelNet, và tất cả các liên kết đi ra từ trang đã cho được sử dụng để tạo thành một sub-corpus cho khái niệm cụ thể. Sub-corpus được mở rộng thêm với các trang Wikipedia của hypernyms và hyponyms của khái niệm trong mạng BabelNet. Toàn bộ Wikipedia được xem như corpus tham chiếu. Hai loại biểu diễn vector khác nhau được đề xuất. Trong phương pháp đầu tiên, các vector có trọng số được hình thành bằng cách sử dụng tính đặc thù từ vựng. Tính đặc thù từ vựng là một phương pháp thống kê xác định các từ đại diện nhất cho một văn bản đã cho, dựa trên phân phối siêu hình học (lấy mẫu không hoàn lại). Cho "T và t" biểu thị tổng số từ nội dung trong corpus tham chiếu RC và sub-corpus SC tương ứng và "F và f" biểu thị tần suất của từ đã cho trong corpus tham chiếu RC và sub-corpus SC tương ứng, thì tính đặc thù từ vựng có thể được biểu diễn bằng phương trình dưới đây:

spec(T,t,F,f)=−log₁₀P(X≥f) (13)

X biểu diễn một biến ngẫu nhiên tuân theo quan hệ siêu hình học với các tham số T, t và F và P(X≥f) được định nghĩa là:

P(X≥f)=∑ᶠᵢ₌f P(X=i) (14)

P(X=i) là xác suất của một thuật ngữ đã cho xuất hiện chính xác i lần trong sub-corpus đã cho trong phân phối siêu hình học với T, t và F. Phương pháp thứ hai tạo thành một cụm từ trong sub-corpus chia sẻ một hypernym chung trong taxonomy WordNet được nhúng trong BabelNet. Tính đặc thù sau đó được đo dựa trên tần suất của hypernym và tất cả hyponyms của nó trong taxonomy, ngay cả những từ không xuất hiện trong sub-corpus đã cho. Kỹ thuật phân cụm này tạo thành một biểu diễn thống nhất của các từ bảo tồn các thuộc tính ngữ nghĩa. Các giá trị tính đặc thù được thêm làm trọng số trong cả hai phương pháp để xếp hạng các thuật ngữ trong một văn bản đã cho. Phương pháp biểu diễn vector đầu tiên được gọi là NASARIlexical và phương pháp thứ hai được gọi là NASARIunified. Độ tương tự giữa các vector này được tính bằng cách sử dụng thước đo gọi là Weighted Overlap [98] như sau:

WO(v₁,v₂)=√(∑d∈O(rank(d,v₁)+rank(d,v₂))⁻¹/∑ᵢ₌₁|O|(2i)⁻¹) (15)

trong đó O biểu thị các thuật ngữ chồng chéo trong mỗi vector và rank(d,vᵢ) biểu diễn thứ hạng của thuật ngữ d trong vector vᵢ.

Camacho Collados và cộng sự [22] đề xuất một phần mở rộng cho công việc trước đó của họ và đề xuất biểu diễn vector thứ ba bằng cách ánh xạ vector từ vựng vào không gian ngữ nghĩa của word embeddings được tạo ra bởi các kỹ thuật word embedding phức tạp như word2vec. Biểu diễn này được gọi là NASARIembedded. Độ tương tự được đo như độ tương tự cosine giữa các vector này. Cả ba phương pháp đều được thử nghiệm trên các tập dữ liệu chuẩn vàng M&C, WS-Sim và SimLex-999. NASARIlexical đạt hệ số tương quan Pearson và Spearman cao hơn trung bình trên ba tập dữ liệu so với các phương pháp khác như ESA, word2vec, và lin.

•Most Suitable Sense Annotation (MSSA) [106]: Ruas và cộng sự đề xuất ba phương pháp khác nhau để tạo thành word-sense embeddings. Cho một corpus, bước phân biệt nghĩa từ được thực hiện bằng một trong ba phương pháp được đề xuất: Most Suitable Sense Annotation (MSSA), Most Suitable Sense Annotation N Refined (MSSA-NR), và Most Suitable Sense Annotation Dijkstra (MSSA-D). Cho một corpus, mỗi từ trong corpus được liên kết với một synset trong ontology WordNet và "gloss-average-vector" được tính cho mỗi synset. Gloss-average-vector được tạo thành bằng cách sử dụng biểu diễn vector của các từ trong gloss của mỗi synset. MSSA tính gloss-average-vector bằng cách sử dụng một cửa sổ nhỏ các từ và trả về synset của từ có giá trị gloss-average-vector cao nhất. MSSA-D, tuy nhiên, xem xét toàn bộ tài liệu từ từ đầu tiên đến từ cuối cùng và sau đó xác định synset liên kết. Hai hệ thống này sử dụng Google News vectors để tạo thành synset-embeddings. MSSA-NR là một mô hình lặp, trong đó lần chạy đầu tiên tạo ra synset-embeddings, được đưa trở lại trong lần chạy thứ hai như một sự thay thế cho gloss-average-vectors để tạo ra synset-embeddings tinh chỉnh hơn. Những synset-embeddings này sau đó được đưa vào mô hình word2vec CBOW để tạo ra multi-sense word embeddings được sử dụng để tính độ tương tự ngữ nghĩa. Sự kết hợp của các biến thể MSSA và word2vec tạo ra kết quả vững chắc trong các tập dữ liệu chuẩn vàng như R&G, M&C, WS353-Sim, và SimLex-999 [106].

•Unsupervised Ensemble Semantic Textual Similarity Methods (UESTS) [38]: Hassan và cộng sự đề xuất một phương pháp độ tương tự ngữ nghĩa ensemble dựa trên một word-aligner không giám sát cơ bản. Mô hình tính độ tương tự ngữ nghĩa như tổng có trọng số của bốn thước đo độ tương tự ngữ nghĩa khác nhau giữa các câu S₁ và S₂ bằng phương trình dưới đây:

simUSETS(S₁,S₂)=α∗simWAL(S₁,S₂)+β∗simSC(S₁,S₂)+γ∗simembed(S₁,S₂)+θ∗simED(S₁,S₂) (16)

simWAL(S₁,S₂) tính độ tương tự bằng word aligner dựa trên synset. Độ tương tự giữa văn bản được đo dựa trên số lượng láng giềng chung mà mỗi thuật ngữ có trong taxonomy BabelNet. simSC(S₁,S₂) đo độ tương tự bằng thước đo soft cardinality giữa các thuật ngữ được so sánh. Hàm soft cardinality xử lý mỗi từ như một tập hợp và độ tương tự giữa chúng như giao điểm giữa các tập hợp. simembed(S₁,S₂) tạo thành biểu diễn word vector bằng word embeddings được đề xuất bởi Baroni và cộng sự [14]. Sau đó độ tương tự được đo như giá trị cosine giữa hai vector. simED(S₁,S₂) là thước đo khác biệt giữa hai câu đã cho. Khoảng cách chỉnh sửa được định nghĩa là số lượng tối thiểu các chỉnh sửa cần thiết để chuyển đổi một câu thành câu khác. Các chỉnh sửa có thể bao gồm chèn, xóa, hoặc thay thế. simED(S₁,S₂) sử dụng khoảng cách chỉnh sửa word-sense trong đó word-senses được xem xét thay vì các từ thực tế. Các siêu tham số α, β, γ, và θ được điều chỉnh đến các giá trị từ 0 đến 0.5 cho các tập dữ liệu benchmark STS khác nhau. Mô hình ensemble vượt trội hơn các mô hình không giám sát benchmark STS trong loạt SemEval 2017 trên nhiều tập dữ liệu benchmark STS.

Các phương pháp lai khai thác cả hiệu quả cấu trúc được cung cấp bởi các phương pháp dựa trên kiến thức và tính linh hoạt của các phương pháp dựa trên corpus. Nhiều nghiên cứu đã được tiến hành để xây dựng multi-sense embeddings nhằm kết hợp nghĩa thực tế của các từ vào word vectors. Iacobacci và cộng sự tạo thành word embeddings gọi là "Sensembed" bằng cách sử dụng BabelNet để tạo thành một corpus có chú thích nghĩa và sau đó sử dụng word2vec để xây dựng word vectors do đó có các vector khác nhau cho các nghĩa khác nhau của từ. Như chúng ta có thể thấy, các mô hình lai bù đắp cho những thiếu sót của một phương pháp bằng cách kết hợp các phương pháp khác. Do đó hiệu suất của các phương pháp lai tương đối cao. 5 vị trí đầu tiên của các tác vụ độ tương tự ngữ nghĩa SemEval 2017 được trao cho các mô hình ensemble, điều này cho thấy rõ ràng sự chuyển đổi trong nghiên cứu hướng tới các mô hình lai [24].

7 PHÂN TÍCH KHẢO SÁT

Phần này thảo luận về phương pháp được sử dụng để xây dựng bài khảo sát này và cung cấp tổng quan về các bài báo nghiên cứu khác nhau được xem xét.

7.1 Chiến lược tìm kiếm:

Các bài báo được xem xét cho khảo sát này được thu thập bằng công cụ tìm kiếm Google Scholar và các từ khóa được sử dụng bao gồm "semantic similarity, word embedding, knowledge-based methods, corpus-based methods, deep neural network-based semantic similarity, LSTM, text processing, và semantic similarity datasets." Kết quả tìm kiếm được tinh chỉnh bằng các tham số khác nhau như Xếp hạng Tạp chí, Chỉ số Google Scholar, số lượng trích dẫn, năm xuất bản, v.v. Chỉ các bài báo được xuất bản trong các tạp chí có xếp hạng Scimago Journal ở Quartile 1 và các hội nghị có chỉ số H-index của Google metrics trên 50 mới được xem xét. Ngoại lệ được thực hiện cho một số bài báo có tác động và liên quan cao hơn. Bảng tham khảo được sắp xếp theo năm xuất bản được bao gồm trong Phụ lục B dưới dạng Bảng 5. Bảng ghi lại 1) Tiêu đề, 2) Năm xuất bản, 3) Tên tác giả, 4) Địa điểm, 5) Quartile SJR (cho tạp chí), 6) H-Index, và 7) Số lượng trích dẫn (tính đến 02.04.2020). Một số kết quả thống kê của các bài báo được chọn được hiển thị trong các hình dưới đây. Những hình này làm nổi bật chất lượng của các bài báo được chọn, từ đó làm nổi bật chất lượng của khảo sát. Hình 2 hiển thị phân phối của các bài báo tham khảo trên các hội nghị, tạp chí, và khác. 55% bài báo từ hội nghị và 38% bài báo từ tạp chí. 7% còn lại từ arXiv và sách. Tuy nhiên, chúng có tác động khá cao liên quan đến chủ đề của khảo sát. Hình 3 làm nổi bật phân phối của các bài báo được chọn theo năm. Gần 72% bài báo được chọn là các công trình được thực hiện sau năm 2010, 28% còn lại đại diện cho các phương pháp truyền thống được áp dụng trong giai đoạn đầu của sự phát triển độ tương tự ngữ nghĩa. Hình 4 biểu diễn phạm vi trích dẫn của các bài báo. 54% bài báo có 50 đến 500 trích dẫn, 26% có 1.000-5.000 trích dẫn, và 12% bài báo có hơn 5000 trích dẫn. Chúng ta thấy rằng 33% bài báo có trích dẫn dưới 50 tuy nhiên, tất cả những bài báo này được xuất bản sau năm 2017 điều này giải thích cho số trích dẫn ít hơn.

Hình 2. Phân phối bài báo theo địa điểm.
Hình 3. Phân phối bài báo theo năm.

7.2 Tạo word-cloud:

Chúng tôi triển khai một đoạn mã python đơn giản để tạo word cloud bằng cách sử dụng các tóm tắt từ tất cả các bài báo được sử dụng trong khảo sát này. Các tóm tắt từ tất cả 118 bài báo được sử dụng để xây dựng một tập dữ liệu sau đó được sử dụng trong mã python. Các tóm tắt được trích xuất ban đầu được tiền xử lý bằng cách chuyển đổi văn bản thành chữ thường, loại bỏ dấu câu, và loại bỏ các từ dừng tiếng Anh thường được sử dụng nhất có sẵn trong thư viện nltk. Sau đó word-cloud được xây dựng bằng thư viện python wordcloud. Word cloud được xây dựng như vậy được hiển thị trong Hình 5. Từ word cloud, chúng tôi suy ra rằng mặc dù các từ khóa khác nhau được sử dụng trong tìm kiếm bài báo của chúng tôi, trọng tâm chung của các bài báo được chọn là độ tương tự ngữ nghĩa. Trong word cloud, kích thước của các từ tỷ lệ thuận với tần suất sử dụng của những từ này. Từ "word" lớn hơn đáng kể so với từ "sentence" cho thấy rằng hầu hết các công trình nghiên cứu tập trung vào độ tương tự từ-đến-từ hơn là độ tương tự câu-đến-câu. Chúng ta cũng có thể suy ra rằng các từ "vector" và "representation" được sử dụng thường xuyên hơn các từ "information", "context", và "concept" cho thấy ảnh hưởng của các phương pháp dựa trên corpus so với các phương pháp dựa trên kiến thức. Với word cloud đã cho, chúng tôi thể hiện trọng tâm của khảo sát một cách đồ họa.

Hình 4. Phân phối phạm vi trích dẫn trên các bài báo.
Hình 5. Word cloud biểu diễn tập hợp các từ từ tóm tắt của các bài báo được sử dụng trong khảo sát.

8 KẾT LUẬN

Đo lường độ tương tự ngữ nghĩa giữa hai đoạn văn bản đã là một trong những tác vụ thách thức nhất trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên. Nhiều phương pháp khác nhau đã được đề xuất qua các năm để đo lường độ tương tự ngữ nghĩa và khảo sát này thảo luận về sự phát triển, ưu điểm, và nhược điểm của những phương pháp này. Các phương pháp dựa trên kiến thức xem xét nghĩa thực tế của văn bản tuy nhiên, chúng không thích ứng được qua các lĩnh vực và ngôn ngữ khác nhau. Các phương pháp dựa trên corpus có nền tảng thống kê và có thể được triển khai qua các ngôn ngữ nhưng chúng không xem xét nghĩa thực tế của văn bản. Các phương pháp dựa trên mạng neural sâu cho thấy hiệu suất tốt hơn, nhưng chúng đòi hỏi tài nguyên tính toán cao và thiếu khả năng giải thích. Các phương pháp lai được hình thành để tận dụng lợi ích từ các phương pháp khác nhau bù đắp cho những thiếu sót của nhau. Rõ ràng từ khảo sát rằng mỗi phương pháp đều có ưu điểm và nhược điểm của nó và khó có thể chọn một mô hình tốt nhất, tuy nhiên, hầu hết các phương pháp lai gần đây đã cho thấy kết quả hứa hẹn so với các mô hình độc lập khác. Trong khi trọng tâm của nghiên cứu gần đây được chuyển hướng tới việc xây dựng word embeddings có nhận thức ngữ nghĩa hơn, và các mô hình transformer đã cho thấy kết quả hứa hẹn, nhu cầu xác định sự cân bằng giữa hiệu quả tính toán và hiệu suất vẫn đang được nghiên cứu. Khoảng trống nghiên cứu cũng có thể được thấy trong các lĩnh vực như xây dựng word embeddings cụ thể cho lĩnh vực, giải quyết nhu cầu về một corpus lý tưởng. Khảo sát này sẽ phục vụ như một nền tảng tốt cho các nhà nghiên cứu có ý định tìm các phương pháp mới để đo lường độ tương tự ngữ nghĩa.

LỜI CẢM ƠN

Các tác giả muốn gửi lời cảm ơn đến nhóm nghiên cứu tại DaTALab thuộc Đại học Lakehead vì sự hỗ trợ của họ, đặc biệt là Abhijit Rao, Mohiuddin Qudar, Punardeep Sikka, và Andrew Heppner vì phản hồi và chỉnh sửa của họ cho ấn phẩm này. Chúng tôi cũng muốn cảm ơn Đại học Lakehead, CASES, và Hội đồng Ontario về Phát âm và Chuyển giao (ONCAT), nếu không có sự hỗ trợ của họ, nghiên cứu này sẽ không thể thực hiện được.

TÀI LIỆU THAM KHẢO

PHỤ LỤC A CÁC THƯỚC ĐO KHOẢNG CÁCH NGỮ NGHĨA VÀ CÔNG THỨC CỦA CHÚNG

STT | Thước đo khoảng cách ngữ nghĩa | Công thức
1 | α-skew divergence (ASD) | ∑w∈C(w₁)∪C(w₂) P(w|w₁)log P(w|w₁)/(αP(w|w₂)+(1-α)P(w|w₁))
2 | Độ tương tự Cosine | ∑w∈C(w₁)∪C(w₂) P(w|w₁)×P(w|w₂)/√(∑w∈C(w₁) P(w|w₁)²)×√(∑w∈C(w₂) P(w|w₂)²)
3 | Co-occurrence Retrieval Models (CRM) | γ²×P×R/(P+R) + (1-γ)/(β[P]+(1-β)[R])
4 | Hệ số Dice | 2×∑w∈C(w₁)∪C(w₂) min(P(w|w₁),P(w|w₂))/(∑w∈C(w₁) P(w|w₁)+∑w∈C(w₂) P(w|w₂))
5 | Khoảng cách Manhattan hoặc chuẩn L1 | ∑w∈C(w₁)∪C(w₂) |P(w|w₁)-P(w|w₂)|
6 | Thước đo chia | ∑w∈C(w₁)∪C(w₂) log P(w|w₁)/P(w|w₂)

Bảng 4 tiếp tục từ trang trước
STT | Thước đo khoảng cách ngữ nghĩa | Công thức
7 | Hindle | ∑w∈C(w) min(I(w,w₁),I(w,w₂)), nếu cả I(w,w₁) và I(w,w₂)>0; |max(I(w,w₁),I(w,w₂))|, nếu cả I(w,w₁) và I(w,w₂)<0; 0, nếu ngược lại
8 | Jaccard | ∑w∈C(w₁)∪C(w₂) min(P(w|w₁),P(w|w₂))/∑w∈C(w₁)∪C(w₂) max(P(w|w₁),P(w|w₂))
9 | Jensen-Shannon divergence (JSD) | ∑w∈C(w₁)∪C(w₂) P(w|w₁)log P(w|w₁)/(½(P(w|w₁)+P(w|w₂))) + P(w|w₂)log P(w|w₂)/(½(P(w|w₁)+P(w|w₂)))
10 | Kullback-Leibler divergence - common occurrence | ∑w∈C(w₁)∪C(w₂) P(w|w₁)log P(w|w₁)/P(w|w₂)
11 | Kullback-Leibler divergence - absolute | ∑w∈C(w₁)∪C(w₂) P(w|w₁)log P(w|w₁)/P(w|w₂)
12 | Kullback-Leibler divergence - average | ½∑w∈C(w₁)∪C(w₂) (P(w|w₁)-P(w|w₂))log P(w|w₁)/P(w|w₂)
13 | Kullback-Leibler divergence - maximum | max(KLD(w₁,w₂),KLD(w₂,w₁))
14 | Khoảng cách Euclidean hoặc chuẩn L2 | √(∑w∈C(w₁)∪C(w₂) (P(w|w₁)-P(w|w₂))²)
15 | Lin | ∑(r,w)∈T(w₁)∩T(w₂) (I(w₁,r,w)+I(w₂,r,w))/(∑(r,w')∈T(w₁) I(w₁,r,w')+∑(r,w'')∈T(w₂) I(w₂,r,w''))
16 | Thước đo tích | ∑w∈C(w₁)∪C(w₂) P(w|w₁)×P(w|w₂)/(½(P(w|w₁)+P(w|w₂)))²

Bảng 4. Bảng các thước đo ngữ nghĩa và công thức của chúng - được điều chỉnh từ Mohammad và Hurst [81]

PHỤ LỤC B BẢNG TÀI LIỆU THAM KHẢO

Trích dẫn | Tiêu đề | Năm | Tác giả | Địa điểm | SJR Quartile | H-Index | Số trích dẫn tính đến 02.04.2020

Bảng 5. Bảng tài liệu tham khảo được sử dụng trong phân tích khảo sát.