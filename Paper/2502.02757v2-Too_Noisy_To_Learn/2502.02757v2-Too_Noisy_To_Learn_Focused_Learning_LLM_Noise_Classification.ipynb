{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: LLM-based Noise Classification in Code Reviews\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook provides an in-depth exploration of using Large Language Models (LLMs) to classify code review comments as \"valid\" or \"noisy\". This is a fundamental concept from the paper \"Too Noisy To Learn\" (Section IV) that enables semantic data cleaning.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to design effective prompts for classification tasks\n",
    "2. The impact of different prompt strategies on classification performance\n",
    "3. Why simpler prompts often outperform complex ones\n",
    "4. How to evaluate classification performance with appropriate metrics\n",
    "\n",
    "**Paper Reference**: Section IV - Semantic Data Cleaning via LLMs (RQ1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Foundation\n",
    "\n",
    "### 1.1 The Classification Problem\n",
    "\n",
    "The paper identifies a critical challenge in automated code review: training datasets contain significant noise (32-36% noisy comments). This noise includes:\n",
    "\n",
    "- **Vague questions**: \"Why this change?\"\n",
    "- **Non-actionable feedback**: \"I don't understand this\"\n",
    "- **Clarification requests**: \"What does this do?\"\n",
    "\n",
    "### 1.2 Valid vs Noisy Comments\n",
    "\n",
    "**Valid Comments** (Definition from Section III):\n",
    "- Provide clear suggestions for improvement\n",
    "- Explicitly express issues\n",
    "- Outline necessary actions\n",
    "- Have clear action types (refactoring, testing, bug fixes)\n",
    "\n",
    "**Noisy Comments**:\n",
    "- Don't request direct actions\n",
    "- Unclear or difficult to understand\n",
    "- Merely justify code changes\n",
    "- Vague or ambiguous\n",
    "\n",
    "### 1.3 Why LLMs for Classification?\n",
    "\n",
    "Traditional approaches using heuristics (keyword matching, sentence length) fail because:\n",
    "1. They lack semantic understanding\n",
    "2. Cannot handle nuanced language\n",
    "3. Miss context-dependent meanings\n",
    "\n",
    "LLMs offer:\n",
    "- Deep semantic understanding\n",
    "- Context awareness\n",
    "- Cross-language generalizability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Classification Output Schema\n",
    "\n",
    "The paper uses structured output from LLMs to ensure consistent classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentClassification(BaseModel):\n",
    "    \"\"\"Schema for LLM classification output\n",
    "    \n",
    "    This ensures the LLM returns structured data that we can reliably parse.\n",
    "    \"\"\"\n",
    "    label: str = Field(\n",
    "        description=\"Classification result: must be exactly 'valid' or 'noisy'\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"Detailed reasoning for the classification decision\"\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        description=\"Confidence score between 0.0 and 1.0\",\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "\n",
    "# Example of expected output\n",
    "example_output = CommentClassification(\n",
    "    label=\"valid\",\n",
    "    explanation=\"The comment provides a specific suggestion to rename a variable for better readability\",\n",
    "    confidence=0.85\n",
    ")\n",
    "\n",
    "print(\"Example Classification Output:\")\n",
    "print(json.dumps(example_output.dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering Deep Dive\n",
    "\n",
    "The paper tests 4 prompt configurations:\n",
    "1. P_DEFINITION with RNL (review comment only)\n",
    "2. P_DEFINITION with RNL+CDIFF (comment + code diff)\n",
    "3. P_AUXILIARY with RNL\n",
    "4. P_AUXILIARY with RNL+CDIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDesigner:\n",
    "    \"\"\"Implements different prompt strategies from the paper\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_definitions() -> Tuple[str, str]:\n",
    "        \"\"\"Get valid and noisy comment definitions from Section III\"\"\"\n",
    "        \n",
    "        valid_def = \"\"\"Valid comments are review comments that provide clear suggestions aimed at improving \n",
    "the source code. Given the submitted code change (code diff), the valid comment should:\n",
    "- Explicitly express the issues\n",
    "- Clearly outline necessary actions to improve the code\n",
    "- Have clear type of requested actions such as:\n",
    "  * Refactoring for code quality\n",
    "  * Writing tests\n",
    "  * Aligning with design principles\n",
    "  * Fixing bugs\n",
    "  * Enhancing logging\n",
    "  * Addressing specific needs\"\"\"\n",
    "        \n",
    "        noisy_def = \"\"\"Noisy comments are review comments that do not request direct and applicable actions \n",
    "to refine the code, or the message expressed is unclear and difficult to understand.\n",
    "This includes:\n",
    "- Comments that do not explicitly ask for specific changes\n",
    "- Comments merely justifying the submitted code change\n",
    "- Comments of low quality due to vagueness\n",
    "- Comments with ambiguity that hinders understanding\n",
    "- Questions without actionable suggestions\"\"\"\n",
    "        \n",
    "        return valid_def, noisy_def\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_auxiliary_rules() -> str:\n",
    "        \"\"\"Get auxiliary rules developed during annotation (Section IV-A)\"\"\"\n",
    "        \n",
    "        return \"\"\"Auxiliary Rules (based on annotation guidelines):\n",
    "1. Comments asking \"why\" without suggesting alternatives are typically NOISY\n",
    "2. Comments that only express confusion without specific feedback are NOISY\n",
    "3. Comments providing specific code suggestions or improvements are VALID\n",
    "4. Comments identifying bugs or potential issues with solutions are VALID\n",
    "5. Comments requesting documentation or test additions are VALID\n",
    "6. One-word or very short comments without context are typically NOISY\n",
    "7. Comments that merely acknowledge changes without feedback are NOISY\n",
    "\n",
    "Examples:\n",
    "- \"Why do we have this flag?\" → NOISY (asks why without suggestion)\n",
    "- \"Consider using a constant for this magic number\" → VALID (specific suggestion)\n",
    "- \"What is the purpose of this line?\" → NOISY (seeks clarification only)\n",
    "- \"Add null check before accessing user.getName()\" → VALID (specific action)\"\"\"\n",
    "    \n",
    "    def create_p_definition_prompt(self, with_context: bool = False) -> ChatPromptTemplate:\n",
    "        \"\"\"Create P_DEFINITION prompt (simpler version)\"\"\"\n",
    "        \n",
    "        valid_def, noisy_def = self.get_definitions()\n",
    "        \n",
    "        system_message = f\"\"\"You are an experienced code reviewer evaluating review comments.\n",
    "Your task is to classify each comment as either 'valid' or 'noisy'.\n",
    "\n",
    "DEFINITIONS:\n",
    "\n",
    "{valid_def}\n",
    "\n",
    "{noisy_def}\n",
    "\n",
    "Evaluation Criteria:\n",
    "1. Relevance: Does the comment address the code change?\n",
    "2. Clarity: Is the comment clear with actionable feedback?\n",
    "3. Improvement Focus: Does it aim to improve code quality?\n",
    "\n",
    "Return your analysis in JSON format.\"\"\"\n",
    "        \n",
    "        if with_context:\n",
    "            human_message = \"\"\"Analyze this code review:\n",
    "\n",
    "CODE DIFF:\n",
    "{code_diff}\n",
    "\n",
    "REVIEW COMMENT:\n",
    "{comment}\n",
    "\n",
    "Classification:\"\"\"\n",
    "        else:\n",
    "            human_message = \"\"\"Analyze this review comment:\n",
    "\n",
    "COMMENT:\n",
    "{comment}\n",
    "\n",
    "Classification:\"\"\"\n",
    "        \n",
    "        return ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            HumanMessagePromptTemplate.from_template(human_message)\n",
    "        ])\n",
    "    \n",
    "    def create_p_auxiliary_prompt(self, with_context: bool = False) -> ChatPromptTemplate:\n",
    "        \"\"\"Create P_AUXILIARY prompt (with additional rules)\"\"\"\n",
    "        \n",
    "        valid_def, noisy_def = self.get_definitions()\n",
    "        auxiliary = self.get_auxiliary_rules()\n",
    "        \n",
    "        system_message = f\"\"\"You are an experienced code reviewer evaluating review comments.\n",
    "Your task is to classify each comment as either 'valid' or 'noisy'.\n",
    "\n",
    "DEFINITIONS:\n",
    "\n",
    "{valid_def}\n",
    "\n",
    "{noisy_def}\n",
    "\n",
    "{auxiliary}\n",
    "\n",
    "Return your analysis in JSON format.\"\"\"\n",
    "        \n",
    "        if with_context:\n",
    "            human_message = \"\"\"Analyze this code review:\n",
    "\n",
    "CODE DIFF:\n",
    "{code_diff}\n",
    "\n",
    "REVIEW COMMENT:\n",
    "{comment}\n",
    "\n",
    "Classification:\"\"\"\n",
    "        else:\n",
    "            human_message = \"\"\"Analyze this review comment:\n",
    "\n",
    "COMMENT:\n",
    "{comment}\n",
    "\n",
    "Classification:\"\"\"\n",
    "        \n",
    "        return ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            HumanMessagePromptTemplate.from_template(human_message)\n",
    "        ])\n",
    "\n",
    "# Initialize prompt designer\n",
    "prompt_designer = PromptDesigner()\n",
    "\n",
    "# Display the different prompts\n",
    "print(\"=== P_DEFINITION Prompt (Simpler) ===\")\n",
    "print(prompt_designer.create_p_definition_prompt().messages[0].prompt.template[:500] + \"...\")\n",
    "print(\"\\n=== P_AUXILIARY Prompt (With Rules) ===\")\n",
    "print(prompt_designer.create_p_auxiliary_prompt().messages[0].prompt.template[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Classification Pipeline\n",
    "\n",
    "Now let's implement the complete classification system with different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseClassifier:\n",
    "    \"\"\"Complete implementation of the LLM-based noise classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\", temperature: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize classifier with specified model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: LLM to use (gpt-3.5-turbo, gpt-4, etc.)\n",
    "            temperature: Set to 0.1 for consistency (as in paper)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "        self.parser = JsonOutputParser(pydantic_object=CommentClassification)\n",
    "        self.prompt_designer = PromptDesigner()\n",
    "        \n",
    "    def classify(self, \n",
    "                comment: str, \n",
    "                code_diff: str = None,\n",
    "                prompt_type: str = \"definition\") -> Dict:\n",
    "        \"\"\"\n",
    "        Classify a single comment.\n",
    "        \n",
    "        Args:\n",
    "            comment: The review comment text\n",
    "            code_diff: Optional code diff context\n",
    "            prompt_type: \"definition\" or \"auxiliary\"\n",
    "            \n",
    "        Returns:\n",
    "            Classification result with label, explanation, and confidence\n",
    "        \"\"\"\n",
    "        # Select prompt\n",
    "        with_context = code_diff is not None\n",
    "        \n",
    "        if prompt_type == \"definition\":\n",
    "            prompt = self.prompt_designer.create_p_definition_prompt(with_context)\n",
    "        else:\n",
    "            prompt = self.prompt_designer.create_p_auxiliary_prompt(with_context)\n",
    "        \n",
    "        # Add format instructions\n",
    "        format_instructions = self.parser.get_format_instructions()\n",
    "        \n",
    "        # Build chain\n",
    "        chain = prompt | self.llm | self.parser\n",
    "        \n",
    "        # Prepare inputs\n",
    "        inputs = {\"comment\": comment}\n",
    "        if with_context:\n",
    "            inputs[\"code_diff\"] = code_diff\n",
    "        \n",
    "        try:\n",
    "            result = chain.invoke(inputs)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Classification error: {e}\")\n",
    "            return {\n",
    "                \"label\": \"noisy\",\n",
    "                \"explanation\": \"Error in processing\",\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "    \n",
    "    def batch_classify(self, \n",
    "                      comments: List[Dict],\n",
    "                      prompt_type: str = \"definition\") -> List[Dict]:\n",
    "        \"\"\"Classify multiple comments\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, item in enumerate(comments):\n",
    "            comment = item['comment']\n",
    "            code_diff = item.get('code_diff', None)\n",
    "            true_label = item.get('label', None)\n",
    "            \n",
    "            # Classify\n",
    "            classification = self.classify(comment, code_diff, prompt_type)\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'comment': comment,\n",
    "                'true_label': true_label,\n",
    "                'predicted_label': classification['label'],\n",
    "                'confidence': classification['confidence'],\n",
    "                'explanation': classification['explanation']\n",
    "            })\n",
    "            \n",
    "            print(f\"[{i+1}/{len(comments)}] Classified: {classification['label']} \"\n",
    "                  f\"(confidence: {classification['confidence']:.2f})\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Test Dataset\n",
    "\n",
    "Let's create a test dataset based on examples from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset based on paper examples\n",
    "test_comments = [\n",
    "    # Noisy examples (from paper)\n",
    "    {\n",
    "        \"comment\": \"Why do we have this flag?\",\n",
    "        \"code_diff\": \"+   this.useWriterSchema = true;\",\n",
    "        \"label\": \"noisy\",\n",
    "        \"source\": \"Figure 1 - Top\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"What is the purpose of this line?\",\n",
    "        \"code_diff\": \"+ data = normalize_values(data)\",\n",
    "        \"label\": \"noisy\",\n",
    "        \"source\": \"Section VI-D\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"Why do we need to change this?\",\n",
    "        \"code_diff\": \"- DEFAULT_TIMEOUT = 30\\n+ DEFAULT_TIMEOUT = 60\",\n",
    "        \"label\": \"noisy\",\n",
    "        \"source\": \"Inferred from patterns\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"I don't understand this change\",\n",
    "        \"code_diff\": \"- debug: false,\\n+ debug: true,\",\n",
    "        \"label\": \"noisy\",\n",
    "        \"source\": \"Inferred from patterns\"\n",
    "    },\n",
    "    \n",
    "    # Valid examples (from paper)\n",
    "    {\n",
    "        \"comment\": \"This can be simplified as new ArrayList<>(Arrays.asList(new ProtocolConfig(protocol)))\",\n",
    "        \"code_diff\": \"- this.protocols = Arrays.asList(new ProtocolConfig[]{new ProtocolConfig(protocol)});\\n\" +\n",
    "                     \"+ this.protocols = new ArrayList<>(Arrays.asList(new ProtocolConfig[]{new ProtocolConfig(protocol)}));\",\n",
    "        \"label\": \"valid\",\n",
    "        \"source\": \"Figure 1 - Bottom\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"Please rename $prev_ref to $previousRef\",\n",
    "        \"code_diff\": \"+ $prev_ref = $product->getRef();\",\n",
    "        \"label\": \"valid\",\n",
    "        \"source\": \"Figure 5\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"Shouldn't this be an assert instead of a throw?\",\n",
    "        \"code_diff\": \"if (value < 0) {\\n-   throw std::invalid_argument(\\\"Value must be positive\\\");\\n+   assert(value >= 0);\",\n",
    "        \"label\": \"valid\",\n",
    "        \"source\": \"Section VI-A\"\n",
    "    },\n",
    "    {\n",
    "        \"comment\": \"Add error handling for null values\",\n",
    "        \"code_diff\": \"+ if (user == null) {\\n+     throw new IllegalArgumentException(\\\"User cannot be null\\\");\\n+ }\\n  return user.getName().toUpperCase();\",\n",
    "        \"label\": \"valid\",\n",
    "        \"source\": \"Inferred from patterns\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display dataset statistics\n",
    "df_test = pd.DataFrame(test_comments)\n",
    "print(\"Test Dataset Statistics:\")\n",
    "print(f\"Total samples: {len(df_test)}\")\n",
    "print(f\"Valid comments: {len(df_test[df_test['label'] == 'valid'])}\")\n",
    "print(f\"Noisy comments: {len(df_test[df_test['label'] == 'noisy'])}\")\n",
    "print(\"\\nSample distribution:\")\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running Classification Experiments\n",
    "\n",
    "Let's reproduce the experiments from Table I in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(classifier: NoiseClassifier, \n",
    "                  test_data: List[Dict],\n",
    "                  experiment_name: str,\n",
    "                  prompt_type: str,\n",
    "                  use_context: bool) -> Dict:\n",
    "    \"\"\"Run a single classification experiment\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Running Experiment: {experiment_name} ===\")\n",
    "    print(f\"Prompt type: {prompt_type}, Use context: {use_context}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    if not use_context:\n",
    "        # Remove code_diff for RNL-only experiments\n",
    "        test_data_processed = [\n",
    "            {k: v for k, v in item.items() if k != 'code_diff'}\n",
    "            for item in test_data\n",
    "        ]\n",
    "    else:\n",
    "        test_data_processed = test_data\n",
    "    \n",
    "    # Run classification\n",
    "    results = classifier.batch_classify(test_data_processed, prompt_type)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    true_labels = [r['true_label'] for r in results]\n",
    "    pred_labels = [r['predicted_label'] for r in results]\n",
    "    \n",
    "    # Binary encoding for metrics\n",
    "    true_binary = [1 if l == 'valid' else 0 for l in true_labels]\n",
    "    pred_binary = [1 if l == 'valid' else 0 for l in pred_labels]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'experiment': experiment_name,\n",
    "        'overall': {\n",
    "            'precision': precision_score(true_binary, pred_binary, average='weighted'),\n",
    "            'recall': recall_score(true_binary, pred_binary, average='weighted'),\n",
    "            'f1': f1_score(true_binary, pred_binary, average='weighted')\n",
    "        },\n",
    "        'valid': {\n",
    "            'precision': precision_score(true_binary, pred_binary, pos_label=1),\n",
    "            'recall': recall_score(true_binary, pred_binary, pos_label=1),\n",
    "            'f1': f1_score(true_binary, pred_binary, pos_label=1),\n",
    "            'count': sum(pred_binary)\n",
    "        },\n",
    "        'noisy': {\n",
    "            'precision': precision_score(true_binary, pred_binary, pos_label=0),\n",
    "            'recall': recall_score(true_binary, pred_binary, pos_label=0),\n",
    "            'f1': f1_score(true_binary, pred_binary, pos_label=0),\n",
    "            'count': len(pred_binary) - sum(pred_binary)\n",
    "        },\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = NoiseClassifier(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Run experiments (reproducing Table I configurations)\n",
    "experiments = [\n",
    "    (\"P_DEFINITION with RNL\", \"definition\", False),\n",
    "    (\"P_DEFINITION with RNL+CDIFF\", \"definition\", True),\n",
    "    (\"P_AUXILIARY with RNL\", \"auxiliary\", False),\n",
    "    (\"P_AUXILIARY with RNL+CDIFF\", \"auxiliary\", True)\n",
    "]\n",
    "\n",
    "# Run one experiment for demonstration\n",
    "exp_name, prompt_type, use_context = experiments[0]\n",
    "results = run_experiment(classifier, test_comments, exp_name, prompt_type, use_context)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n=== Results for {exp_name} ===\")\n",
    "print(f\"Overall - Precision: {results['overall']['precision']:.3f}, \"\n",
    "      f\"Recall: {results['overall']['recall']:.3f}, \"\n",
    "      f\"F1: {results['overall']['f1']:.3f}\")\n",
    "print(f\"Valid - Precision: {results['valid']['precision']:.3f}, \"\n",
    "      f\"Recall: {results['valid']['recall']:.3f}, \"\n",
    "      f\"Count: {results['valid']['count']}\")\n",
    "print(f\"Noisy - Precision: {results['noisy']['precision']:.3f}, \"\n",
    "      f\"Recall: {results['noisy']['recall']:.3f}, \"\n",
    "      f\"Count: {results['noisy']['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyzing Classification Results\n",
    "\n",
    "Let's visualize and analyze the classification results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(results: Dict):\n",
    "    \"\"\"Create comprehensive visualizations of classification results\"\"\"\n",
    "    \n",
    "    # Extract data\n",
    "    true_labels = [r['true_label'] for r in results['results']]\n",
    "    pred_labels = [r['predicted_label'] for r in results['results']]\n",
    "    confidences = [r['confidence'] for r in results['results']]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=['valid', 'noisy'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['valid', 'noisy'],\n",
    "                yticklabels=['valid', 'noisy'],\n",
    "                ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    axes[0, 0].set_xlabel('Predicted Label')\n",
    "    axes[0, 0].set_ylabel('True Label')\n",
    "    \n",
    "    # 2. Confidence Distribution\n",
    "    axes[0, 1].hist(confidences, bins=10, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Confidence Score')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Confidence Score Distribution')\n",
    "    axes[0, 1].axvline(np.mean(confidences), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(confidences):.2f}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Performance Metrics\n",
    "    metrics_data = {\n",
    "        'Valid': [results['valid']['precision'], results['valid']['recall'], results['valid']['f1']],\n",
    "        'Noisy': [results['noisy']['precision'], results['noisy']['recall'], results['noisy']['f1']],\n",
    "        'Overall': [results['overall']['precision'], results['overall']['recall'], results['overall']['f1']]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (label, values) in enumerate(metrics_data.items()):\n",
    "        axes[1, 0].bar(x + i*width, values, width, label=label)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Metrics')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Classification Performance by Class')\n",
    "    axes[1, 0].set_xticks(x + width)\n",
    "    axes[1, 0].set_xticklabels(['Precision', 'Recall', 'F1'])\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].set_ylim(0, 1.1)\n",
    "    \n",
    "    # 4. Confidence by Correctness\n",
    "    correct_conf = [c for i, c in enumerate(confidences) if true_labels[i] == pred_labels[i]]\n",
    "    incorrect_conf = [c for i, c in enumerate(confidences) if true_labels[i] != pred_labels[i]]\n",
    "    \n",
    "    axes[1, 1].boxplot([correct_conf, incorrect_conf], labels=['Correct', 'Incorrect'])\n",
    "    axes[1, 1].set_ylabel('Confidence Score')\n",
    "    axes[1, 1].set_title('Confidence by Prediction Correctness')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Findings\n",
    "\n",
    "Let's analyze why certain prompts perform better, reproducing insights from Section IV-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prompt_effectiveness():\n",
    "    \"\"\"Analyze why simpler prompts outperform complex ones\"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        \"Finding 1: Simpler is Better\": {\n",
    "            \"observation\": \"P_DEFINITION with RNL only achieves highest precision (85.1% for GPT-3.5)\",\n",
    "            \"explanation\": \"Simpler prompts reduce cognitive load on the model and avoid distraction\",\n",
    "            \"evidence\": \"Table I shows consistent pattern across all models\"\n",
    "        },\n",
    "        \n",
    "        \"Finding 2: Context Can Hurt\": {\n",
    "            \"observation\": \"Adding code diff (CDIFF) reduces performance by 9.9-24%\",\n",
    "            \"explanation\": \"Additional context can distract from the core classification task\",\n",
    "            \"evidence\": \"CodeLlama precision drops from 66.0% to 63.8% with context\"\n",
    "        },\n",
    "        \n",
    "        \"Finding 3: Auxiliary Rules Have Mixed Effects\": {\n",
    "            \"observation\": \"P_AUXILIARY improves some models but hurts others\",\n",
    "            \"explanation\": \"Model-specific: helps CodeLlama but reduces GPT-3.5 performance\",\n",
    "            \"evidence\": \"CodeLlama F1 improves from 58.0% to 70.1% with auxiliary rules\"\n",
    "        },\n",
    "        \n",
    "        \"Finding 4: Model Temperature Matters\": {\n",
    "            \"observation\": \"Low temperature (0.1) ensures consistency\",\n",
    "            \"explanation\": \"Classification tasks benefit from deterministic behavior\",\n",
    "            \"evidence\": \"Paper uses 0.1 temperature for all experiments\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Display insights\n",
    "    for title, details in insights.items():\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"=\" * len(title))\n",
    "        for key, value in details.items():\n",
    "            print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "analyze_prompt_effectiveness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis\n",
    "\n",
    "Let's examine common classification errors to understand model limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_error_analysis(results: Dict):\n",
    "    \"\"\"Analyze misclassified comments to understand patterns\"\"\"\n",
    "    \n",
    "    errors = []\n",
    "    for r in results['results']:\n",
    "        if r['true_label'] != r['predicted_label']:\n",
    "            errors.append(r)\n",
    "    \n",
    "    print(f\"Total errors: {len(errors)} out of {len(results['results'])} \"\n",
    "          f\"({len(errors)/len(results['results'])*100:.1f}%)\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\n=== Error Analysis ===\")\n",
    "        for i, error in enumerate(errors, 1):\n",
    "            print(f\"\\nError {i}:\")\n",
    "            print(f\"Comment: '{error['comment']}'\")\n",
    "            print(f\"True: {error['true_label']}, Predicted: {error['predicted_label']}\")\n",
    "            print(f\"Confidence: {error['confidence']:.2f}\")\n",
    "            print(f\"Explanation: {error['explanation'][:100]}...\")\n",
    "    \n",
    "    # Common error patterns from the paper (Section VII)\n",
    "    print(\"\\n=== Common Error Patterns (from paper) ===\")\n",
    "    patterns = [\n",
    "        \"1. Domain-specific terms: LLMs may incorrectly classify comments with technical terms\",\n",
    "        \"2. Implicit suggestions: Comments that imply actions without explicit statements\",\n",
    "        \"3. Context-dependent: Some comments need project context to classify correctly\",\n",
    "        \"4. Edge cases: Very short comments or questions can be ambiguous\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        print(pattern)\n",
    "\n",
    "# Perform error analysis\n",
    "perform_error_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Practical Implementation Guide\n",
    "\n",
    "Based on the paper's findings, here's a practical guide for implementing noise classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionClassifier:\n",
    "    \"\"\"Production-ready implementation based on paper's best practices\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use best performing configuration from Table I\n",
    "        self.model = \"gpt-3.5-turbo\"  # 85.1% precision on valid\n",
    "        self.temperature = 0.1  # Low temperature for consistency\n",
    "        self.prompt_type = \"definition\"  # Simpler prompt performs better\n",
    "        self.use_context = False  # RNL only (no code diff)\n",
    "        \n",
    "        self.classifier = NoiseClassifier(\n",
    "            model_name=self.model,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "    \n",
    "    def classify_for_cleaning(self, comment: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Classify comment for dataset cleaning.\n",
    "        \n",
    "        Returns:\n",
    "            (label, confidence) tuple\n",
    "        \"\"\"\n",
    "        result = self.classifier.classify(\n",
    "            comment=comment,\n",
    "            code_diff=None,  # Best performance without context\n",
    "            prompt_type=self.prompt_type\n",
    "        )\n",
    "        \n",
    "        return result['label'], result['confidence']\n",
    "    \n",
    "    def should_keep_comment(self, comment: str, threshold: float = 0.7) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if comment should be kept in cleaned dataset.\n",
    "        \n",
    "        Args:\n",
    "            comment: Review comment text\n",
    "            threshold: Confidence threshold for keeping comments\n",
    "            \n",
    "        Returns:\n",
    "            True if comment is valid with sufficient confidence\n",
    "        \"\"\"\n",
    "        label, confidence = self.classify_for_cleaning(comment)\n",
    "        \n",
    "        # Keep only valid comments with high confidence\n",
    "        return label == \"valid\" and confidence >= threshold\n",
    "    \n",
    "    def clean_dataset(self, comments: List[str], \n",
    "                     threshold: float = 0.7) -> Dict:\n",
    "        \"\"\"\n",
    "        Clean a dataset of comments.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with cleaned comments and statistics\n",
    "        \"\"\"\n",
    "        kept = []\n",
    "        removed = []\n",
    "        \n",
    "        for comment in comments:\n",
    "            if self.should_keep_comment(comment, threshold):\n",
    "                kept.append(comment)\n",
    "            else:\n",
    "                removed.append(comment)\n",
    "        \n",
    "        return {\n",
    "            'kept': kept,\n",
    "            'removed': removed,\n",
    "            'original_size': len(comments),\n",
    "            'cleaned_size': len(kept),\n",
    "            'reduction_rate': len(removed) / len(comments),\n",
    "            'improvement': 'Expected 64% → 85% valid ratio'\n",
    "        }\n",
    "\n",
    "# Demonstration\n",
    "prod_classifier = ProductionClassifier()\n",
    "\n",
    "# Test on sample comments\n",
    "test_samples = [\n",
    "    \"Why do we have this flag?\",  # Noisy\n",
    "    \"Consider using a constant for this magic number\",  # Valid\n",
    "    \"What does this do?\",  # Noisy\n",
    "    \"Add null check before accessing the property\"  # Valid\n",
    "]\n",
    "\n",
    "print(\"=== Production Classifier Test ===\")\n",
    "for comment in test_samples:\n",
    "    label, conf = prod_classifier.classify_for_cleaning(comment)\n",
    "    keep = prod_classifier.should_keep_comment(comment)\n",
    "    print(f\"\\nComment: '{comment}'\")\n",
    "    print(f\"Classification: {label} (confidence: {conf:.2f})\")\n",
    "    print(f\"Keep in dataset: {keep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cost-Benefit Analysis\n",
    "\n",
    "Let's calculate the cost-benefit of LLM-based cleaning (from Section VII)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_benefit(dataset_size: int = 128058):  # Paper's dataset size\n",
    "    \"\"\"Calculate cost-benefit analysis for LLM vs manual cleaning\"\"\"\n",
    "    \n",
    "    # Costs from paper\n",
    "    llm_cost = 50  # USD for GPT-3.5\n",
    "    llm_time = 39  # hours\n",
    "    \n",
    "    # Manual annotation costs\n",
    "    manual_rate = 8  # USD per hour\n",
    "    time_per_comment = 1/60  # 1 minute per comment in hours\n",
    "    manual_time = dataset_size * time_per_comment\n",
    "    manual_cost = manual_time * manual_rate\n",
    "    \n",
    "    # Benefits\n",
    "    bleu_improvement = 0.13  # 13% improvement\n",
    "    quality_improvement = 0.24  # 24% information score improvement\n",
    "    training_reduction = 0.66  # 66% less data needed\n",
    "    \n",
    "    print(\"=== Cost-Benefit Analysis ===\")\n",
    "    print(f\"\\nDataset size: {dataset_size:,} comments\")\n",
    "    \n",
    "    print(\"\\n--- Costs ---\")\n",
    "    print(f\"LLM Approach:\")\n",
    "    print(f\"  Cost: ${llm_cost}\")\n",
    "    print(f\"  Time: {llm_time} hours\")\n",
    "    \n",
    "    print(f\"\\nManual Approach:\")\n",
    "    print(f\"  Cost: ${manual_cost:,.0f}\")\n",
    "    print(f\"  Time: {manual_time:,.0f} hours\")\n",
    "    \n",
    "    print(f\"\\nCost Reduction: {(1 - llm_cost/manual_cost)*100:.1f}%\")\n",
    "    print(f\"Time Reduction: {(1 - llm_time/manual_time)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n--- Benefits ---\")\n",
    "    print(f\"BLEU-4 Score Improvement: +{bleu_improvement*100:.0f}%\")\n",
    "    print(f\"Information Quality: +{quality_improvement*100:.0f}%\")\n",
    "    print(f\"Training Data Reduction: {training_reduction*100:.0f}% (with better results!)\")\n",
    "    \n",
    "    print(\"\\n--- ROI ---\")\n",
    "    roi = (manual_cost - llm_cost) / llm_cost\n",
    "    print(f\"Return on Investment: {roi:.0f}x\")\n",
    "    print(f\"Break-even dataset size: {int(50 / (manual_cost/dataset_size)):,} comments\")\n",
    "\n",
    "calculate_cost_benefit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Key Takeaways\n",
    "\n",
    "### Main Findings:\n",
    "\n",
    "1. **Simpler Prompts Win**: P_DEFINITION with only comment text achieves best results\n",
    "2. **Context Can Hurt**: Adding code diff reduces performance by up to 24%\n",
    "3. **High Precision Achievable**: 85% precision in identifying valid comments\n",
    "4. **Cost-Effective**: 512x cheaper than manual annotation\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. Use simple, focused prompts\n",
    "2. Keep temperature low (0.1) for consistency\n",
    "3. Focus on comment text only (skip code context)\n",
    "4. Use confidence thresholds for production\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "1. Ensemble methods combining multiple LLMs\n",
    "2. Fine-tuning smaller models on classified data\n",
    "3. Active learning for edge cases\n",
    "4. Domain-specific adaptations\n",
    "\n",
    "This focused learning notebook has demonstrated how LLMs can effectively classify code review comments, enabling semantic data cleaning that significantly improves model performance while reducing costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}