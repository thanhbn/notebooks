# 1.6.1. Automated Code Review In Practice- 2412.18531v2.pdf
# Converted from PDF to TXT
# Source path: D:\llm\notebooks\AI-Papers\1.6.1. Automated Code Review In Practice- 2412.18531v2.pdf
# File size: 1003700 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Automated Code Review In Practice
Umut Cihan∗, Vahid Haratian∗, Arda ˙Ic ¸¨oz∗,
Mert Kaan G ¨ul†,¨Omercan Devran†, Emircan Furkan Bayendur†,
Baykal Mehmet Uc ¸ar†, Eray T ¨uz¨un∗
∗Bilkent University, Ankara, T ¨urkiye
umut.cihan@bilkent.edu.tr, vahid.haratian@bilkent.edu.tr arda.icoz@bilkent.edu.tr, eraytuzun@cs.bilkent.edu.tr
†Beko, ˙Istanbul, T ¨urkiye
mertkaan.gul@beko.com, omercan.devran@beko.com emircanfurkan bayendur@beko.com, baykal.ucar@beko.com
Abstract — Context: Code review is a widespread practice
among practitioners to improve software quality and transfer
knowledge. It is often perceived as time-consuming due to the
need for manual effort and potential delays in the development
process. Several AI-assisted code review tools (Qodo, GitHub
Copilot, Coderabbit, etc.) provide automated code reviews using
large language models (LLMs). The overall effects of such tools
in the industry setting are yet to be examined.
Objective: This study examines the impact of LLM-based
automated code review tools in an industry setting.
Method: The study was conducted within an industrial soft-
ware development environment that adopted an AI-assisted
code review tool (based on open-source Qodo PR Agent). 238
practitioners across ten projects had access to the tool. We
focused our analysis on three projects, encompassing 4,335 pull
requests, of which 1,568 underwent automated reviews. Our data
collection comprised three primary sources: (1) a quantitative
analysis of pull request data, including comment labels indicat-
ing whether developers acted on the automated comments, (2)
surveys sent to developers regarding their experience with the
reviews on individual pull requests, and (3) a broader survey of
22 practitioners capturing their general opinions on automated
code reviews.
Results: 73.8% of automated code review comments were
labeled as resolved. However, the overall average pull request
closure duration increased from five hours 52 minutes to eight
hours 20 minutes, with varying trends observed across differ-
ent projects. According to survey responses, most practitioners
observed a minor improvement in code quality as a result of
automated code reviews.
Conclusion: The LLM-based automated code review tool
proved useful in software development, enhancing bug detection,
increasing awareness of code quality, and promoting best prac-
tices. However, it also led to longer pull request closure times
and introduced drawbacks such as faulty reviews, unnecessary
corrections, and irrelevant comments. Based on these findings,
we discussed how practitioners can more effectively utilize
automated code review technologies.
Index Terms —code review, large language models, pull re-
quests, AI-assisted code review, industry case study, code review
automation
I. I NTRODUCTION
Code review is a quality assurance process, with different
adaptations across the industry, that involves developers check-
ing each others’ code changes [1]. Emerging as formal code
inspections [2], code review has evolved and become a lighter
process often referred to as Modern Code Review (MCR)
[3]. MCR is characterized by being informal, tool-based, andregular [4]. The common benefits of the process are knowledge
sharing, learning, defect identification, and code improvement
[5], [6].
To conduct a code review, developers spend time under-
standing other developers’ work and where it stands in the
overall project. The Open Source Software (OSS developers
self-reported an average review time of 6.4 hours per week
for code review [7], whereas the Google case study reveals a
lower number with 3.2 hours [6]. The numbers suggest that
developers spend a considerable amount of time on reviews.
Due to developers’ workloads, developers often postpone
code review tasks. In doing so, developers postpone the merge
of code changes. In the same case study at Google, the median
time for merge approval is under four hours, while for large
change sets, it is five hours [6]. Higher median approval times
are reported from various companies: 15.7 hours for Google
Chrome, 20.8 hours for Android at Google, 17.5 hours for
AMD, 14.7 hours for Bing, 19.8 hours for SQL, and 18.9 hours
for Office at Microsoft [8]. This variation in approval times
suggests differences across companies and projects. However,
the actual costs of delays in critical code changes, which are
not reflected in median or average statistics, could be more
detrimental. In a study of Microsoft developers, receiving
timely feedback was cited as the top challenge regarding code
review practices [5].
Automation can potentially shorten the time for code ap-
proval and reduce the burden of manual effort on developers.
In this regard, several attempts have been made to automate
the code review process [9]–[19], focusing mainly on the
assignment of reviewers [20]. These efforts come from tools
that generate code reviews [10]–[12], predict the approval
of code changes [21], [22], and fix code according to code
reviews [23], [24]. In our study, we focus on the automation
of code review generation.
To generate code reviews, developers spend time under-
standing the code change, looking for mistakes, and detecting
performance bottlenecks or general deviations from coding
standards. There have been several attempts with pre-trained
models to provide code reviews [21], [22], [25]. With the
introduction of ChatGPT [26], there have been considerable
automation efforts to automate code review generation [27].
Several tools with code review capability were created, often
working with OpenAI’s LLMs such as GPT-4 [28], [29].arXiv:2412.18531v2  [cs.SE]  28 Dec 2024

--- PAGE 2 ---
Automated code review tools are increasingly used in the
industry [27]. However, there is a lack of empirical evidence
regarding their potential benefits. For example, the time saved
through automated reviews could be offset by new problems
they introduce. Additionally, the financial benefits of reducing
developer effort may be negligible compared to the cost
of operating such tools. To address this research gap, we
conducted a study aimed at answering the following research
questions:
•RQ1: How useful are LLM-based automated code re-
views in the context of the software industry?
•RQ2: How do LLM-based automated code reviews im-
pact the pace of the pull request closure process?
•RQ3: How does the introduction of LLM-based auto-
mated code reviews influence the volume of human code
review activities?
•RQ4: How do developers perceive the LLM-based auto-
mated code review tools?
We collaborated with Beko, a multinational home appli-
ances company, whose software division adopted an automatic
code review tool based on the open-source Qodo (Formerly-
CodiumAI) PR Agent [29] using GPT-4 Turbo model [30].
This tool provides automatic code review comments for each
pull request across 10 projects and 22 project repositories.
Our data collection process included three sources. First, we
extracted data from the version control system and develop-
ment platform Azure DevOps, which hosts the project reposi-
tories. This data encompassed pull request information, review
comments, review comment labels, and comparisons between
initial and final versions of pull requests. Developers were
asked to label each review comment to indicate whether they
had implemented the suggestions into the code. Second, the
authors received a short survey for each pull request. Lastly,
we conducted a broader survey involving 22 practitioners to
gather their overall perceptions.
II. R ELATED WORK
Code reviews demand significant developer effort and time
[5], [7]. These demands, along with the need for frequent
context switching, have driven the push towards automation
[31]. The automation in code review is a well-investigated
part of the research, where the majority of efforts tackle
the problem of reviewer assignment [32]–[40]. There is also
considerable effort being put into other code review aspects. In
2018, Gupta et al. [25] presented a model to match historical
reviews with code snippets using supervised deep learning. In
2019, a Convolutional Neural Network (CNN) based model
by Li et al. [22] and a framework utilizing CNN and LSTM
by Shi et al. [21] were presented to predict approvals of code
changes. In 2022, Thontanunam et al. [24] presented a model
to modify the source code automatically during code review
processes.
In addition to the aforementioned efforts, several techniques
were investigated for automation of code review generation, in-
cluding information retrieval [9], pre-trained models [10]–[12],
LLMs [13]–[15], LLM prompt fine-tuning [16], fine-tuningLLMs with human feedback [17], and LLM agents [18], [19].
The effectiveness of state-of-the-art code review automation
has been investigated by Tufano et al. [20]. In their study,
Tufano et al. investigated their model based on a pre-trained
Text-To-Text Transfer Transformer (T5) model [11], [41],
CommentFinder model using information retrieval techniques
to recommend code reviews [9], CodeReview Model using
pre-training techniques [10], as well as ChatGPT [26] without
specifying the version they used. They found that ChatGPT
could serve as a competitive baseline for improving code, both
in direct code-to-code transformations and in generating code
based on comments (comment-to-code). In contrast, ChatGPT
did not outperform the state-of-the-art in comment generation
(code-to-comment).
Usage of LLMs and generative artificial intelligence for
code review generation has attracted other studies. Davila et al.
[27] conducted a grey literature review regarding the usage of
generative artificial intelligence for code reviews and showed
how LLM-based tools like ChatGPT have been explored for
code review. Fan et al. [14] explored the capabilities of LLMs
on three code change-related tasks: code review generation,
commit message generation and just-in-time comment update.
They concluded that LLMs are promising for the tasks men-
tioned earlier. Watanabe et al. [42] investigated 229 review
comments from 179 GitHub projects that included ChatGPT
conversation links. Their analysis revealed that 30.7% of the
reactions to ChatGPT’s answer were negative, with developers
often citing the lack of extra benefits. In 2024, Vijayvergiya
et al. [15] from Google and the University of Washington
demonstrated their findings on the development and large-scale
industry implementation of AutoCommenter. AutoCommenter
is an LLM-backed automated code review system for four
programming languages (C++, Java, Python, and Go). Their
findings suggest the feasibility of developing an end-to-end
automated code review system while achieving high end-user
acceptance.
In this study, we aim to address the lack of longitudinal
research on the impact of automated code review tools in
software development. Unlike previous studies, our research
examines the effects of a commercial LLM-based automated
code review tool [29] in real-world industry settings regarding
its impact on development artifacts and developer perceptions.
This study aims to provide practitioners with valuable insights
into whether and how to adopt an LLM-based automated code
review tool.
III. R ESEARCH SETTINGS
In this study, we conducted an evaluative case study to
assess the impact of LLM-based automated code reviews
in software development. We evaluated their effectiveness,
influence on the speed of pull request closures, and changes
in the volume of human code reviews. This section presents
our research settings. Section III-A introduces the study object.
Section III-B presents this study’s goal and research questions.
Our study involves both quantitative and qualitative data
sources. Section III-C describes how we collected quantitative

--- PAGE 3 ---
Fig. 1. Data Collection Timeline
data from the projects’ repositories. Section III-D describes
our qualitative data collection through surveys. Section III-E
describes our approach toward research questions.
A. Study Object
We conducted our study within the software development
division of Beko, a multinational company. It operates in
the consumer durables and electronics sectors. The software
development division of Beko is responsible for developing
customer-facing and employee-facing software with 238 prac-
titioners.
Figure 1 outlines Beko’s journey to implement the CodeRe-
viewBot, starting with the investigation phase on 25thAu-
gust 2023, driven by a need to enhance code quality and
efficiency. Beko evaluated several open-source projects, in-
cluding CodeRabbit [28], Qodo (Formerly CodiumAI) [29],
and pipeline extension tools like Reviewbot1, ChatGPT-
CodeReview2, and Codereview.gpt3. After careful considera-
tion, Beko selected Qodo PR-Agent [29], for its high perfor-
mance and seamless integration. They customized its function-
ality to meet their needs, and named it ”Code Review Bot”
internally. For the rest of the study, we will use ”CodeReview-
Bot” to enhance readability. The first pilot project launched
on 7thNovember 2023. By 5thJune 2024, Beko had adopted
CodeReviewBot across 10 projects and 22 repositories. On
24thMay 2024, practitioners were informed that the commit
resolution policy was in place via an email campaign, and
20thSeptember 2024 marked our study’s final data collection
date. In Figure 2 you can see an example comment from
CodeReviewBot.
Fig. 2. Example Review of CodeReviewBot
CodeReviewBot utilizes the GPT-4-32K Model to provide
automatic code review comments for each pull request. The
tool complements the code review process; developers can still
add their reviews. In Figure 3, the flow of the pull request
process is depicted. The tool works on a diverse portfolio of 22
repositories spanning 10 distinct projects. These repositories
contained Java, JavaScript, C, HTML, SQL, C# and TypeScript
code.
1https://github.com/reviewboard/ReviewBot
2https://github.com/anc95/ChatGPT-CodeReview
3https://github.com/sturdy-dev/codereview.gpt
Fig. 3. Flow of Pull Request Process
TABLE I
PROJECTS INCLUDED INOURSTUDY
Project Explanation # of Devel-
opersLanguage
Project
#1B2B E-commerce
Portal21TypeScript, Java,
JavaScript
Project
#2Enterprise Management
Platform51HTML, JavaScript,
C#
Project
#3Customizable AI
Solutions Hub22C#, HTML,
JavaScript
SQL
The Beko was selected as the study object since they had
used the CodeReviewBot for a considerable time. The size of
their development teams (103 developers) and the diversity of
projects (10 projects) employing the tool are other qualities
that motivated us to select them. To avoid potential risks or
unintended consequences with collecting real-world data, we
did not associate any result with an individual practitioner, and
the individual survey results were not shared with the authors
from Beko. Our data analysis is conducted on three of the
ten projects that employed CodeReviewBot. This is due to the
early start of these three projects with the CodeReviewBot
(Project #1 7thNovember 2023, Project #2 13thNovember
2023 and Project #3 3rdApril 2024). The other seven projects
adopted CodeReviewBot around June 2024; hence, they did
not accumulate considerable data. Table I provides information
regarding the three projects.
B. Study Goal and Research Questions
Our study investigates the impact of automated code reviews
in software development from multiple perspectives. Academ-
ically, it provides empirical data to understand the future
direction of modern code review and reveals the potential
of promising automated code review tools. For practitioners,
it is crucial to determine whether automated code reviews
positively affect the development process. From a business
standpoint, implementing such tools involves costs that their
benefits must justify. Quality-wise, automated code reviews
should be expected to enhance rather than diminish the ex-
isting code review process. Therefore, this study is a well-
motivated and valuable investigation for industry professionals
and academics.
We addressed the following research questions by collecting
data from the project repositories and surveys:

--- PAGE 4 ---
Research Question 1: How useful are LLM-based auto-
mated code reviews in the context of the software industry?
This research question assesses LLM-based automated code
reviews’ utility in software development. Specifically, we
seek to determine whether comments generated by automated
code review tools are effectively incorporated into code. This
depends on the usefulness of the automated reviews and the
developer’s reception of them.
Research Question 2: How do LLM-based automated code
reviews impact the pace of the pull request closure process?
One of the main motivations for the automation is time
gains. With this research question, we examine the effect of
LLM-based automated code reviews on the pace of develop-
ment. Specifically, we aim to determine whether integrating
automated code reviews accelerates the pull request closure
process. By analyzing this impact, we seek to understand
whether LLM-based tools contribute to more efficient devel-
opment workflows.
Research Question 3: How does the introduction of LLM-
based automated code reviews influence the volume of human
code review activities?
Human code reviews require substantial effort. With the
introduction of LLM-based automated code reviews, assessing
how this affects the volume of human reviews is essential.
With this research question, we aim to determine whether au-
tomated tools lead to an increase or decrease in human review
activities. We aim to understand how automation impacts the
effort and workload associated with human code reviews.
Research Question 4: How do developers perceive the
LLM-based automated code review tools?
Developers are important actors in the code review process.
Their perception of the automated code review tools is critical
for realizing the expected benefits. With this research question,
we aim to analyze how developers perceive the comments they
receive and the general process by triangulating our different
data sources. This comprehensive assessment will provide
insights into developers’ attitudes toward automation and its
role in the code review process.
We utilized qualitative and quantitative data to achieve our
research objectives and address our questions. The interaction
between practitioners and automated code reviews is a key
focus of our study, which required qualitative data collection.
To achieve this, we developed a pull request survey and a gen-
eral opinion survey and implemented a mandatory comment
resolution policy requiring developers to label how comments
were addressed. For quantitative data, we opted for repository
mining, which offers valuable insights into real-world usage
by systematically recording digital activities. We triangulated
our findings from the surveys and comment resolution labels
by analyzing historical commit data, pull requests, and related
comments. Figure 4 depicts our data collection overview.
C. Data Collection From Azure DevOps
We collected data from the project’s version control system
(”Azure DevOps”). This data includes pull request informa-
tion, review comments, and commits to pull requests. Tobetter understand the data extraction process, we described
the components illustrated in Figure 4.
Fig. 4. Data Collection Overview
1) Initial Pull Request: A pull request is a mechanism for
introducing changes to the code base. Once a pull request
is created, other developers are informed about the proposed
changes. They add their review comments to notify the author,
who is expected to make necessary changes based on the
reviews. The pull request is accepted once the code reaches
acceptable quality and the changes are merged into the code
base.
Pull request authors receive automatic review comments and
other reviewers’ comments in our case. Our study considers
4335 pull requests, of which 1568 were created after adopting
automated code review.
2) Pull Request Comments: Pull request comments serve as
the artifacts of the code review process. Human reviewers add
their reviews in the form of comments. The CodeReviewBot
added its reviews as separate comments. We analyzed the pull
request comments regarding volume and time, who generated
them, and how pull request authors benefited.
The presence of a comment does not mean it was taken
into consideration. To investigate whether pull request authors
benefited from the review comments, we configured a required
comment resolution policy [43]. This system requires authors
to indicate how they handled the pull request comments.
Unfortunately, the comment resolution policy put in place after
CodeReviewBot, so we do not have the same data from prior
pull requests. Table II presents the comment status options for
each review comment and their explanation according to the
comment resolution policy.
3) Closed Pull Request: The code changes proposed in the
initial pull request may change with the review comments.
This is an expected event if the reviews point out problems.
The code changes are reflected as additional commits. The
pull request is either accepted or rejected. The practitioners at
Beko are not using rejection as a quality assurance mechanism.
Instead, they use rejection when the change is considered
unnecessary or untimely. Hence, we did not treat acceptance
as a quality indicator but as categorical data.
4) Data Extraction and Analysis: Our data analysis starts
with extracting raw data from the Azure DevOps server using

--- PAGE 5 ---
TABLE II
COMMENT RESOLUTION POLICY IN AZURE DEVOPS
Status Explanation
Active This is the default status for new comments.
PendingThe corresponding comment is being analyzed
or waiting for some other thing.
ResolvedThe corresponding comment is successful and
its suggestion was implemented.
Won’t fixThe corresponding comment is faulty or cannot
be implemented.
ClosedThe corresponding comment was not implemented
for some other reasons than ”Won’t Fix”.
the API. We created a data scheme that allowed us to store
information regarding projects, repositories, commits, pull
requests, and pull request comments in a relational database.
We used the data scheme to store the API responses.
Our next step was to preprocess the data to ensure its
integrity and accuracy. Firstly, we converted the tables into
CSV files, uploaded the data using the pandas library4, and
created scripts to extract the relevant metrics. The first problem
we noticed was the high number of active comments, which
should not have been the case for the comment resolution
policy. We manually examined pull requests and observed that
some pull requests were closed before the CodeReviewBot
could comment. The comments created by the tool arrived
after the pull request was closed, meaning they did not need
to be resolved. We reviewed the time it took to merge the
PRs and, using elbow evaluation, identified a threshold that
covers 93% of the PRs. We then applied this filter to the entire
history for a fair evaluation. Finally, we manually removed the
remaining 7% to eliminate all outliers.
The second issue we encountered was the high number of
comments from some developers. Two developers had unrea-
sonably higher comments than other developers. We examined
their comments and realized that their Azure DevOps accounts
were used for different software bots, such as SonarQube5. We
also excluded these accounts from our study.
Lastly, we observed that many comments had unknown
comment resolution labels due to being deleted or including
the pull request description generated by CodeReviewBot.
For that, we excluded such comments. On top of that, the
comment resolution policy was only active when the pull
request targeted the repository’s main branch. For that, we
excluded pull requests targeting other branches except for
the main branch. The data cleaning process involved several
collaborative sessions with both non-practitioner and Beko
participants, after which we confirmed the dataset was free
from corruption. Ultimately, we analyzed 4,335 pull requests,
of which 1,568 were subject to automated reviews.
D. Data Collection From Surveys
1) Code Review Surveys: With each pull request, the au-
thors were asked to give a zero to five rating for the automated
4https://pandas.pydata.org/pandas-docs/stable/index.html#
5https://www.sonarsource.com/products/sonarqube/review comments and received a survey of three questions.
These surveys are our second data source. The first two
questions have one-to-five scale answers, and the last one is
open-ended. We included code review survey questions in the
replication package.
2) General Opinion Surveys: We created a general opinion
survey and received responses from 22 developers who con-
tributed to the three projects within our research scope. These
practitioners had different years of experience and positions
in the organization. We provide information regarding the
participants in Table III. The survey questions focused on the
impact of automated code reviews on developers and their
perspectives. We included general opinion survey questions in
the replication package.
TABLE III
SURVEY PARTICIPANTS
Survey Participants
Experience in Software Development Number of Participants
0-2 4
2-5 9
5-10 6
10+ 3
Position at Beko Number of Participants
Individual Contributor 16
Lead/Manager 6
Total Practitioners 22
E. Approach Towards Research Questions
Since our research questions have multiple aspects, we
relied on findings from multiple data sources. We used the
review comment labels, commits to pull requests after reviews,
and general opinion survey question answers to establish a
multi-aspect evaluation for the first research question. We ex-
tracted pull request closure duration data from Azure DevOps
to answer the second research question. In the general opinion
survey, we asked the practitioners whether automated code
reviews affected the development pace.
We extracted the number of human code reviews from Azure
DevOps for the third research question. We asked developers
whether they could manually generate the same comments in
the survey. The last research question is centered around the
developer and pull request survey. Our data analysis scripts,
survey questions, and results are shared in our replication
package6.
IV. R ESULTS
A. Code Review Surveys
During our study, we collected ratings for 38 pull requests
that received automated code reviews. The pull request authors
were also asked to fill out a survey consisting of two multiple-
choice questions and one open-ended question, and ten people
went on to do so.
The average rating for the automated review comments was
3.46, with a standard deviation of 1.79. Figure 5 depicts the
6https://doi.org/10.5281/zenodo.13917481

--- PAGE 6 ---
ratings for the different projects and the overall rating. There is
a considerable difference between ratings across projects, with
averages of 4.04, 2.72, and 3.00 for Project #1, Project #2, and
Project #3, respectively. This might indicate a difference in the
quality of reviews concerning different project conditions, or it
may be due to the differences between developers in different
projects.
The survey that we sent to the authors contained three
questions. The first two questions were regarding whether
authors found the reviews agreeable and how they found the
presentation of the review comments. The third question was
open-ended and asked for additional feedback.
Fig. 5. Automated Code Review Ratings from Code Review Surveys
Unfortunately, we did not receive many answers for the third
question. Most of the answers were simple remarks such as
”Great.” Ten practitioners answered the first two questions,
and we reported on the responses in Figure 6. Eight people
answered with ”5” when asked how agreeable they found
the comments, while nine answered with ”5” when asked
how well-presented they found the comments. One person
answered both questions with ”1”. There is a difference
between the number of responses for ratings and surveys.
Since the survey required extra effort, dissatisfied developers
might have felt less motivated to complete the survey. Overall,
the ratings and the pull request survey show developers found
the comments agreeable and well-presented.
Fig. 6. Code Review SurveyB. General Opinion Surveys
We sent a survey consisting of eight questions to the
developers who contributed to the projects. We received 23
responses, with 22 respondents consenting to be involved in
published results. This survey was created to collect the overall
perceptions of developers.
The survey comprised six multiple-choice questions and two
open-ended questions. Figure 7 and 8 present the results. The
first question addressed the impact of automated code reviews
on development speed. Eight developers perceived a minor
improvement in the pace of development due to automated
code reviews, while four perceived no effect, three perceived
a minor deterioration, and two perceived a major improvement.
Fig. 7. General Opinion Survey Responses (Questions 1-3)
The second question focused on knowledge sharing. Nine
practitioners perceived no effect on knowledge sharing among
developers. At the same time, the eight developers perceived a
positive impact. The third question explored code quality, with
most respondents (14) suggesting that automated code reviews
contribute to a minor improvement in the overall quality of the
code.
Fig. 8. General Opinion Survey Responses (Questions 4-6)

--- PAGE 7 ---
The fourth question examined the relevance of the com-
ments generated by the automated code reviews to the pull
request. With nine respondents rating the relatedness as ”4”
and seven rating the relatedness as ”3”, results indicate that
most practitioners found the automated comments related to
pull requests.
The fifth question asked whether developers could manually
identify the issues the automated code reviews pointed out with
”1,” meaning they could not manually identify none, and ”5,”
meaning they could identify all of them. Ten respondents rated
this question ”3,” while six rated it ”2”.
The sixth question addressed the importance of the issues
highlighted by the automated reviews. Eight respondents rated
this question ”3,” while six rated it ”2”. The dispersion of the
ratings shows that practitioners have different views.
C. Analysis of the Azure DevOps Data
Fig. 9. Comment Labels on Merged PRs
1) Comment Labels: Within our analysis, we extracted
4408 comments by CodeReviewBot. Since the comment res-
olution policy was enforced sometime after the CodeReview-
Bot, we had to filter out comments before the policy intro-
duction and the currently active or pending comments. After
the filtering, we examined 1408 CodeReviewBot comments on
merged pull requests. We reported the status of comments for
each project in Figure 9. 73.8% of the comments are labeled
as ”Resolved,” while 21.3% are labeled as ”Won’t Fix.” When
we compare the results across projects, we see a significant
difference for Project #1 and Project #3 with 55% and 90%
of comments labeled as ”Resolved.”
2) Commits on Pull Requests: As a measure of change
caused by the code review process, pull requests receive com-
mits after they receive comments. We analyzed these commits
for the pull requests after CodeReviewBot’s implementation.
The results are shown in Figure 10. Pull requests received
88 commits after CodeReviewBot’s comments but before
human comments. After the human reviewer had commented,
the pull requests received 69 commits. It should be noted
that authors may also act upon CodeReviewBot’s comments
after the human reviewer’s comments. Conversely, some pull
Fig. 10. Commits After Code Reviews
requests may have been opened as drafts, with new commits
added afterward. A difference between the number of commits
after CodeReviewBot and the comments with the ”Resolved”
label is expected. Since pull requests receive more than one
comment and commits are made for the pull request, the
”Resolved” labeled comments should be more than the com-
mits. However, developers might also disregard the expected
labeling policy and use ”Resolved” instead of ”Closed” or
”Won’t Fix.” ”Closed” refers to comments that were not faulty
but were not implemented for some other reason.
RQ1: How useful are LLM-based automated code
reviews in the context of the software industry?
Our analysis showed that 73.8% of the comments
suggested by CodeReviewBot were accepted and im-
plemented by developers, highlighting the bot’s sig-
nificant impact on the code review process. Moreover,
88 commits were made after CodeReviewBot and be-
fore human reviewers’ comments, indicating proactive
changes based on the bot’s suggestions. Most survey
respondents also perceived an improvement in code
quality using CodeReviewBot. Therefore, we conclude
that LLM-based automated code reviews are highly
useful in this company’s context.
3) Pull Request Closure Durations: The overall average
pull request closure duration increased from five hours and
52 minutes before the introduction of CodeReviewBot to
eight hours and 20 minutes after its implementation. The
independent samples t-test indicated this overall increase was
statistically significant (p-value <0.001). We observed differ-
ent trends across projects as depicted in Figure 11.
Project #1 showed a significant increase in closure duration,
rising from two hours and 48 minutes to four hours and
38 minutes after CodeReviewBot was introduced. The t-test
indicated this increase was statistically significant (p-value <
0.001). Project #2 experienced a significant decrease in closure
duration, dropping from six hours and six minutes to three
hours and seven minutes. The analysis yielded a statistically
significant result (p-value <0.001). Project #3 observed an

--- PAGE 8 ---
Fig. 11. Pull Request Closure Durations Before and After CodeReviewBot
(H:MM)
increase in closure duration from 20 hours and 22 minutes
to 30 hours and 51 minutes after the bot’s introduction. The
statistical test showed this increase was statistically significant
(p-value <0.001).
RQ2: How do LLM-based automated code reviews
impact the pace of the pull request closure process?
Our results indicated a significant slowdown in the
pull request closure process. This slowdown could be
explained by developers needing to address additional
comments from the bot and those from human re-
viewers. The impact on closure durations varied across
different projects, suggesting that project-specific con-
ditions play a critical role in determining the effects.
Fig. 12. Average Number of Human Reviewer Comments
4) Human Reviewer Comments: Overall, human reviewers
left an average of 0.31 comments per pull request before
deploying CodeReviewBot, which decreased to 0.28 afterward,
as shown in Figure 12. The Poisson regression analysis
indicated that this overall decrease was not statistically sig-
nificant (p-value ≥0.05). For comparison, CodeReviewBot
left a higher average of 3.65 comments per pull request. The
statistical analysis revealed different trends across projects:After introducing CodeReviewBot, human comments per
pull request significantly increased in Project #1 (from 0.14
to 0.29, p-value <0.05), significantly decreased in Project #2
(from 0.37 to 0.08, p-value <0.05), and remained statistically
unchanged in Project #3 (from 0.50 to 0.49, p-value ≥0.05).
RQ3: How does the introduction of LLM-based au-
tomated code reviews influence the volume of human
code review activities?
The average number of human comments decreased
from 0.31 to 0.28 with the introduction of CodeRe-
viewBot. However, according to our Poisson regression
analysis, this decrease was not statistically significant.
Additionally, the impact of LLM-based automated
code reviews on human reviewer activity varied across
different projects, potentially influenced by project
dynamics and team practices.
D. General Opinion Survey Open-Ended Questions
The general opinion survey had two questions about the
advantages and disadvantages of automated code review. We
received 20 legitimate answers, with two non-useful responses
(blank spaces). A significant proportion of participants (13/20)
remarked on advantages regarding code quality improvement
and the maintenance of coding standards. Some respondents
highlighted the enhancements in the review process, such as
shortening the review process and helping spot overlooked
code smells and potential bugs. The auto-generated code
descriptions were cited as beneficial in expediting the review
process. Other notable advantages included providing sug-
gestions for improvement and enhancing awareness of best
practices.
RQ4: How do developers perceive the LLM-based
automated code review tools?
Some practitioners voiced concerns that out-of-scope
or irrelevant suggestions could slow down reviews and
create distractions. Some practitioners were concerned
that automated code reviews missed critical issues that
a human reviewer would catch. One respondent raised
a significant concern about automated code review
potentially altering the context of pull requests, which
could introduce severe bugs if changes are applied
without careful consideration. Despite these draw-
backs, the majority of developers perceive automated
code review tools as beneficial for improving code
quality and maintaining coding standards, notably by
detecting quality problems and providing improvement
suggestions.

--- PAGE 9 ---
V. D ISCUSSION
A. Does LLM-based automated code review considerably im-
prove software development activity?
Implementing an LLM-based automated code review tool is
a substantial organizational decision that involves both benefits
and costs. While such tools incur expenses—for instance,
in our study, the CodeReviewBot used an average of 3,937
tokens per pull request at a cost of 0.48$ — they also require
developers to invest time in addressing review comments.
The comment labels assigned by pull request authors
showed that 73.8% of the comments were addressed by the
author. Additionally, we observed that 88 commits were made
after the CodeReviewBot’s reviews before human reviews, sig-
nifying that the automated reviews affected the pull requests.
Our survey of developers revealed that 68.8% perceived a
minor improvement in the code quality after CodeReviewBot.
According to our survey results, practitioners consider issues
pointed out by CodeReviewBot important and related to the
respective pull requests. This further supports the usefulness
of LLM-based automated code reviews.
On top of usefulness for code quality, code reviews are also
helpful for knowledge sharing [5]. Within our general opin-
ion survey, we asked practitioners whether CodeReviewBot
affected knowledge-sharing. Most respondents cited no effect,
while no respondent pointed to a negative impact.
One of the cited benefits of automated code review is its
potential to save time and developer effort [31]. To explore
this, we analyzed the durations for pull request closures.
Although we observed an overall increase in closure times, this
could be attributed to authors spending extra time fixing issues
highlighted by the automated reviewer bot. The trends varied
significantly across different projects, suggesting that devel-
opers were actively engaging with the automated feedback,
which may have extended the closure times but potentially led
to higher code quality. Furthermore, our analysis showed that
the number of human review comments per pull request did
not decrease significantly after introducing the automated tool.
This suggests that while developers invested additional effort
to address automated comments, it did not replace the need
for human reviews. Consequently, we did not find conclusive
evidence supporting consistent time or effort savings as a result
of implementing the automated code review tool.
Our findings suggest that automated code review can mod-
erately improve software development activity. The decision
to implement such a tool should still be considered carefully.
The observed benefits within our study might be hindered by
existing code review habits or other quality assurance activities
in place.
B. Implications to Practitioners
Over-reliance on automated code reviews : One respon-
dent in the general opinion survey said, ”It may create bias
so reviewers may ignore by saying that if any other issue
exists, the bot would have written it.” This over-reliance on
automation can be harmful to the organization by allowingsevere bugs to go unnoticed. Practitioners should examine
LLM-based automated code review tools extensively before
widespread adoption. Organizational awareness of the limita-
tions should be established, and necessary precautions should
be taken.
Unnecessary Review Comments : The results from the
comment resolution labels show that 26.2% of the tool
comments were not acted upon as they were labeled with
”Won’t Fix” or ”Closed”. These comments could be too trivial,
unrelated or not a problem for the context of the pull request.
In any way, developers spend time on them. The survey
respondents also cited this problem. One respondent in the
general opinion survey said, ”It also makes suggestions that
fix code blocks that are not in the scope of the task,” and
another said, ”Sometimes the mistakes it thinks it finds are
not mistakes at all.”
Early Identification of Bugs: The survey showed that early
identification of bugs is an advantage of the automated code
review. One respondent in the general opinion survey said ”It
makes finding code defects more easy. Developers can see
their mistakes fast.” Respondents also highlighted the tool’s
ability to detect typos and forgotten test code. Another said,
”very effective in detecting overlooked code smells”. These
remarks show us how the tool can improve code quality.
C. Implications to Researchers
Effectiveness of LLM-Based Tools : This study provides
valuable empirical evidence on the effectiveness of LLM-
based automated code review tools in real-world industry
settings. The positive reception of developers and the percent-
age of comments accounted for in the pull requests (73.8%)
suggest that LLMs can enhance the code review process. One
respondent in the general opinion survey said, ”It improved
the awareness of the team about code quality”. This remark
suggests that the tool also has effects beyond code review
practice.
Human-AI Interaction Dynamics : Integrating automated
reviews into the code review process introduces new dynam-
ics in human-AI interaction. One of the findings was the
frustration caused by recursive reviews, with one respondent
saying, ”With each fix, a new review is generated. However,
after the initial review, subsequent comments on revised pull
requests often become redundant and unhelpful.”. Researchers
can investigate factors influencing developer trust, satisfaction,
and reliance on automated reviews, leading to more human-
centered design improvements in these tools.
VI. T HREATS TO VALIDITY
A. Construct Validity
Our data collection process involves collecting data from
respondents through survey answers and comment resolution
labels. These data entries are subject to misinterpretation.
To mitigate this threat to validity, extra effort was put into
explaining the expectations from the respondents. We also
thoroughly reviewed the questions with multiple reviewers to
rephrase potentially confusing and complicated language.

--- PAGE 10 ---
B. Internal Validity
Some authors of this paper are part of Beko’s software
development organization. Specifically, one of the authors is
in a managerial position in the organization. This introduces
the possibility of biases. For having a neutral stance, the
results and discussions in this paper were created by the non-
practitioner authors.
We acknowledge that the mandatory comment resolution
policy could be considered time-consuming for practitioners
because they do not provide reliable comment resolution
labels. To account for this threat, we triangulated our data
sources to come to conclusions. For example, we used com-
ment labels alongside pull request change information to see
whether they agreed to a certain extent.
Our data collection is centered around summer. Beko practi-
tioners mentioned that more developers are on vacation during
the summer, which decreases productivity and development
pace. For this matter, we acknowledge that our conclusions
are subject to seasonality.
The projects examined in this study are real projects that
commenced well before the research began. We did not
associate any results with individual practitioners to avoid
unintended effects and ethical concerns. As a result, the prac-
titioners involved in the projects had no incentive to behave
differently since the analysis was conducted independently
afterward.
Since the pull request authors the same pull request survey
repeatedly, we consider that they might become frustrated and
respond carelessly. To avoid this decline in engagement, we
decided to limit the survey to three questions.
Our quantitative data analysis relies on the integrity of data
in Azure DevOps. Corruptions in the database could impact
our results. To mitigate such corruption, we conducted joint
data cleaning sessions where we discussed the quality of the
data. We identified two accounts involved in the projects that
were used as bot accounts for a certain time. We removed
the comments made by those accounts. We acknowledge that
those comments might include human comments, though most
were from bots.
Although the ”Active” and ”Pending” comments would
not allow the pull request to be closed, we observed that
some automated comments arrived after the pull request was
closed. These comments were mostly left as active, and we
disregarded them in our analysis.
C. External Validity
Our study was conducted with a certain automated code
review tool (Qodo PR Agent [29]) based on a certain LLM
(GPT-4 32k [30]). Since we used this specific tool and model,
we acknowledge that other LLMs and automated code review
tools might exhibit different behavior. Therefore, further re-
search is needed to determine if other LLMs would behave in
a comparable manner.D. Conclusion Validity
This study might have led to different results in a different
company setting. We cannot account for the many changing
conditions to reach statistical conclusions; therefore, we con-
sidered a case study the most appropriate approach. A multiple
case study examination using our methodology would be
required to reach definitive conclusions. Given the challenges
of conducting such a comprehensive study, we limited our
scope and did not aim for statistical generalizations.
VII. C ONCLUSION
In this study, we conducted an evaluative case study within
the software development division of Beko, a multinational
company, focusing on automated code reviews. An LLM-based
automated code review tool, based on the open-source Qodo
PR-Agent [29], was adopted across ten projects. We narrowed
our research scope to three of these projects, selected based
on their longer duration of tool usage.
Our findings suggest that the tool was effective, with 73.8%
of the comments being acted upon. Additionally, most devel-
opers reported a minor improvement in code quality in the
survey and no deterioration in knowledge sharing. Regarding
pull request closure times, there was a statistically significant
increase between the average times of five hours and 52
minutes before and eight hours and 20 minutes after the tool’s
introduction. Across projects, there were different trends, with
one project’s pull request closure duration declining. Addition-
ally, there was no significant change in the number of human
code reviews before and after the tool’s implementation.
Since developers are the primary users interacting with
the tool, we examined their perceptions of automated code
reviews. The survey revealed that practitioners commonly per-
ceived the issues identified in automated reviews as important
and relevant to the pull requests. Participants noted several
advantages contributing to code quality, including faster bug
detection, eliminating code smells, increased awareness of
code quality, and the promotion of standardized best practices.
The disadvantages included suggestions for out-of-scope code
changes and occasional misaligned recommendations that dis-
tract developers and slow down the development. There were
also concerns about the downsides of potential over-reliance
on automated systems.
Our study revealed that automated code reviews can pos-
itively impact software development; however, some unin-
tended effects and disadvantages were also identified. Practi-
tioners can use these insights to make more informed decisions
on the implementation and use of LLM-based automated
code review tools. We aim to replicate our study with dif-
ferent software development companies to account for inter-
organizational differences in our future work.
VIII. A CKNOWLEDGEMENTS
This work has been supported the ITEA4 GENIUS
project, which has been funded by the national
funding authorities of the participating countries:
https://itea4.org/project/genius.html

--- PAGE 11 ---
REFERENCES
[1] T. Baum, O. Liskin, K. Niklas, and K. Schneider, “A faceted classifi-
cation scheme for change-based industrial code review processes,” in
2016 IEEE International Conference on Software Quality, Reliability
and Security (QRS) , 2016, pp. 74–85.
[2] M. E. Fagan, “Design and code inspections to reduce errors in program
development,” IBM Systems Journal , vol. 15, no. 3, pp. 182–211, 1976.
[3] N. Davila and I. Nunes, “A systematic literature review and taxonomy
of modern code review,” Journal of Systems and Software , vol. 177,
p. 110951, 2021. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0164121221000480
[4] A. Bacchelli and C. Bird, “Expectations, outcomes, and challenges of
modern code review,” in 2013 35th International Conference on Software
Engineering (ICSE) , 2013, pp. 712–721.
[5] L. MacLeod, M. Greiler, M.-A. Storey, C. Bird, and J. Czerwonka,
“Code reviewing in the trenches: Challenges and best practices,” IEEE
Software , vol. 35, no. 4, pp. 34–42, 2018.
[6] C. Sadowski, E. S ¨oderberg, L. Church, M. Sipko, and A. Bacchelli,
“Modern code review: A case study at google,” in International Confer-
ence on Software Engineering, Software Engineering in Practice track
(ICSE SEIP) , 2018.
[7] A. Bosu and J. C. Carver, “Impact of peer code review on peer
impression formation: A survey,” in 2013 ACM / IEEE International
Symposium on Empirical Software Engineering and Measurement , 2013,
pp. 133–142.
[8] P. C. Rigby and C. Bird, “Convergent contemporary software peer
review practices,” in Proceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering , ser. ESEC/FSE 2013. New
York, NY , USA: Association for Computing Machinery, 2013, p.
202–212. [Online]. Available: https://doi.org/10.1145/2491411.2491444
[9] Y . Hong, C. Tantithamthavorn, P. Thongtanunam, and A. Aleti,
“Commentfinder: a simpler, faster, more accurate code review
comments recommendation,” in Proceedings of the 30th ACM Joint
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering , ser. ESEC/FSE 2022. New
York, NY , USA: Association for Computing Machinery, 2022, p.
507–519. [Online]. Available: https://doi.org/10.1145/3540250.3549119
[10] Z. Li, S. Lu, D. Guo, N. Duan, S. Jannu, G. Jenks, D. Majumder,
J. Green, A. Svyatkovskiy, S. Fu, and N. Sundaresan, “Automating code
review activities by large-scale pre-training,” 2022.
[11] R. Tufano, S. Masiero, A. Mastropaolo, L. Pascarella, D. Poshyvanyk,
and G. Bavota, “Using pre-trained models to boost code review
automation,” CoRR , vol. abs/2201.06850, 2022. [Online]. Available:
https://arxiv.org/abs/2201.06850
[12] L. Li, L. Yang, H. Jiang, J. Yan, T. Luo, Z. Hua, G. Liang, and C. Zuo,
“Auger: Automatically generating review comments with pre-training
models,” 2022.
[13] J. Yu, P. Liang, Y . Fu, A. Tahir, M. Shahin, C. Wang, and Y . Cai,
“Security code review by large language models,” 2024. [Online].
Available: https://arxiv.org/abs/2401.16310
[14] L. Fan, J. Liu, Z. Liu, D. Lo, X. Xia, and S. Li, “Exploring the
capabilities of llms for code change related tasks,” 2024. [Online].
Available: https://arxiv.org/abs/2407.02824
[15] M. Vijayvergiya, M. Salawa, I. Budiseli ´c, D. Zheng, P. Lamblin,
M. Ivankovi ´c, J. Carin, M. Lewko, J. Andonov, G. Petrovi ´cet al. ,
“Ai-assisted assessment of coding practices in modern code review,”
inProceedings of the 1st ACM International Conference on AI-Powered
Software , 2024, pp. 85–93.
[16] C. Pornprasit and C. Tantithamthavorn, “Fine-tuning and prompt
engineering for large language models-based code review automation,”
Information and Software Technology , vol. 175, p. 107523, 2024.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0950584924001289
[17] M. Nashaat and J. Miller, “Towards efficient fine-tuning of language
models with organizational data for automated software review,” IEEE
Transactions on Software Engineering , pp. 1–14, 2024.
[18] D. Tang, K. Kim, Y . Song, C. Lothritz, B. Li, S. Ezzini,
H. Tian, J. Klein, and T. F. Bissyand ´e, “Codeagent: Collaborative
agents for software engineering,” 2024. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:267412469
[19] Z. Rasheed, M. A. Sami, M. Waseem, K.-K. Kemell, X. Wang,
A. Nguyen, K. Syst ¨a, and P. Abrahamsson, “Ai-powered code review
with llms: Early results,” 2024.[20] R. Tufano, O. Dabi ´c, A. Mastropaolo, M. Ciniselli, and G. Bavota,
“Code review automation: Strengths and weaknesses of the state of the
art,” IEEE Trans. Softw. Eng. , vol. 50, no. 2, p. 338–353, Jan. 2024.
[Online]. Available: https://doi.org/10.1109/TSE.2023.3348172
[21] S.-T. Shi, M. Li, D. Lo, F. Thung, and X. Huo, “Automatic code
review by learning the revision of source code,” Proceedings of
the AAAI Conference on Artificial Intelligence , vol. 33, no. 01, pp.
4910–4917, Jul. 2019. [Online]. Available: https://ojs.aaai.org/index.
php/AAAI/article/view/4420
[22] H.-Y . Li, S.-T. Shi, F. Thung, X. Huo, B. Xu, M. Li, and
D. Lo, “Deepreview: Automatic code review using deep multi-
instance learning,” in Advances in Knowledge Discovery and Data
Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China,
April 14-17, 2019, Proceedings, Part II . Berlin, Heidelberg:
Springer-Verlag, 2019, pp. 318–330. [Online]. Available: https:
//doi.org/10.1007/978-3-030-16145-3 25
[23] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk,
“On learning meaningful code changes via neural machine translation,”
CoRR , vol. abs/1901.09102, 2019. [Online]. Available: http://arxiv.org/
abs/1901.09102
[24] P. Thongtanunam, C. Pornprasit, and C. Tantithamthavorn, “Autotrans-
form: Automated code transformation to support modern code review
process,” in Proceedings of the 44th international conference on software
engineering , 2022, pp. 237–248.
[25] A. Gupta and N. Sundaresan, “Intelligent code reviews using deep
learning,” in Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD’18) Deep
Learning Day , 2018.
[26] OpenAI, “Chatgpt,” https://www.openai.com/chatgpt, 2023, accessed:
2024-04-28.
[27] N. Davila, J. Melegati, and I. Wiese, “Tales from the trenches: Expec-
tations and challenges from practice for code review in the generative
ai era,” IEEE Software , vol. PP, pp. 1–8, 01 2024.
[28] “What is coderabbit?” May 2023. [Online]. Available: https://docs.
coderabbit.ai/
[29] “Codium-ai/pr-agent,” September 2024. [Online]. Available: https:
//github.com/Codium-ai/pr-agent
[30] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. , “Gpt-4
technical report,” arXiv preprint arXiv:2303.08774 , 2023.
[31] R. Tufano, L. Pascarella, M. Tufano, D. Poshyvanyk, and G. Bavota,
“Towards automating code review activities,” in Proceedings of
the 43rd International Conference on Software Engineering , ser.
ICSE ’21. IEEE Press, 2021, p. 163–174. [Online]. Available:
https://doi.org/10.1109/ICSE43902.2021.00027
[32] W. H. A. Al-Zubaidi, P. Thongtanunam, H. K. Dam,
C. Tantithamthavorn, and A. Ghose, “Workload-aware reviewer
recommendation using a multi-objective search-based approach,”
inProceedings of the 16th ACM International Conference on
Predictive Models and Data Analytics in Software Engineering ,
ser. PROMISE 2020. New York, NY , USA: Association for
Computing Machinery, 2020, p. 21–30. [Online]. Available:
https://doi.org/10.1145/3416508.3417115
[33] S. Asthana, R. Kumar, R. Bhagwan, C. Bird, C. Bansal, C. Maddila,
S. Mehta, and B. Ashok, “Whodo: automating reviewer suggestions
at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering , ser. ESEC/FSE 2019. New
York, NY , USA: Association for Computing Machinery, 2019, p.
937–945. [Online]. Available: https://doi.org/10.1145/3338906.3340449
[34] J. Jiang, D. Lo, J. Zheng, X. Xia, Y . Yang, and L. Zhang, “Who
should make decision on this pull request? analyzing time-decaying
relationships and file similarities for integrator prediction,” Journal of
Systems and Software , vol. 154, pp. 196–210, 2019. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0164121219300962
[35] H. Ying, L. Chen, T. Liang, and J. Wu, “Earec: Leveraging expertise and
authority for pull-request reviewer recommendation in github,” in 2016
IEEE/ACM 3rd International Workshop on CrowdSourcing in Software
Engineering (CSI-SE) , 2016, pp. 29–35.
[36] E. Mirsaeedi and P. C. Rigby, “Mitigating turnover with code
review recommendation: balancing expertise, workload, and knowledge
distribution,” in Proceedings of the ACM/IEEE 42nd International
Conference on Software Engineering , ser. ICSE ’20. New York, NY ,

--- PAGE 12 ---
USA: Association for Computing Machinery, 2020, p. 1183–1195.
[Online]. Available: https://doi.org/10.1145/3377811.3380335
[37] A. Ouni, R. G. Kula, and K. Inoue, “Search-based peer reviewers
recommendation in modern code review,” in 2016 IEEE International
Conference on Software Maintenance and Evolution (ICSME) , 2016, pp.
367–377.
[38] M. M. Rahman, C. K. Roy, and J. A. Collins, “Correct: Code reviewer
recommendation in github based on cross-project and technology expe-
rience,” in 2016 IEEE/ACM 38th International Conference on Software
Engineering Companion (ICSE-C) , 2016, pp. 222–231.
[39] P. Thongtanunam, C. Tantithamthavorn, R. G. Kula, N. Yoshida, H. Iida,
and K.-i. Matsumoto, “Who should review my code? a file location-
based code-reviewer recommendation approach for modern code re-
view,” in 2015 IEEE 22nd International Conference on Software Anal-
ysis, Evolution, and Reengineering (SANER) , 2015, pp. 141–150.
[40] E. S ¨ul¨un, E. T ¨uz¨un, and U. Do ˘grus¨oz, “Rstrace+: Reviewer
suggestion using software artifact traceability graphs,” Information and
Software Technology , vol. 130, p. 106455, 2021. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0950584920300021
[41] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning
with a unified text-to-text transformer,” CoRR , vol. abs/1910.10683,
2019. [Online]. Available: http://arxiv.org/abs/1910.10683
[42] M. Watanabe, Y . Kashiwa, B. Lin, T. Hirao, K. Yamaguchi, and H. Iida,
“On the use of chatgpt for code review: Do developers like reviews
by chatgpt?” in Proceedings of the 28th International Conference on
Evaluation and Assessment in Software Engineering , ser. EASE ’24.
New York, NY , USA: Association for Computing Machinery, 2024, p.
375–380. [Online]. Available: https://doi.org/10.1145/3661167.3661183
[43] “Branch policies and settings,” may 2024. [Online]. Available: https:
//learn.microsoft.com/en-us/azure/devops/repos/git/branch-policies?
view=azure-devops&tabs=browser#check-for-comment-resolution
