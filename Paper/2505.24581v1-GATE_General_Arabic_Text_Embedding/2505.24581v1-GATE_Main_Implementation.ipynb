{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity\n",
    "\n",
    "## Paper Information\n",
    "- **Title**: GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training\n",
    "- **Authors**: Omer Nacar, Anis Koubaa, Serry Sibaee, Yasser Al-Habashi, Adel Ammar, Wadii Boulila\n",
    "- **ArXiv ID**: 2505.24581v1\n",
    "- **Publication Date**: May 30, 2025\n",
    "- **Paper Link**: https://arxiv.org/abs/2505.24581v1\n",
    "- **Domain**: Natural Language Processing, Arabic Text Processing, Semantic Textual Similarity\n",
    "\n",
    "## Abstract Summary\n",
    "> This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.\n",
    "\n",
    "## Key Contributions\n",
    "1. **Hybrid Loss Strategy**: Combines cosine similarity for semantic tasks and softmax-based classification\n",
    "2. **Enhanced Model Robustness**: Incorporates curated Arabic NLI triplet and labeled pair datasets\n",
    "3. **Scalable Arabic Embeddings**: Adapts Matryoshka Representation Learning to Arabic (768, 512, 256, 128, 64 dimensions)\n",
    "4. **State-of-the-Art Performance**: 20-25% improvement over larger models on Arabic STS benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependencies\n",
    "\n",
    "### Installing Required Libraries\n",
    "We'll use LangChain ecosystem for text processing and evaluation, along with specialized libraries for Arabic NLP and Matryoshka embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install torch transformers sentence-transformers\n",
    "!pip install langchain langchain-community langchain-openai\n",
    "!pip install deepeval mteb datasets\n",
    "!pip install chromadb faiss-cpu\n",
    "!pip install pandas numpy matplotlib seaborn plotly\n",
    "!pip install arabic-reshaper python-bidi  # Arabic text processing\n",
    "!pip install ctranslate2  # For neural machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and Sentence Transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# DeepEval for evaluation\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SemanticSimilarityMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Data processing\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "print(\"âœ… All dependencies imported successfully!\")\n",
    "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ¤— Transformers available: {torch.cuda.is_available()}\")\n",
    "print(f\"ğŸš€ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Loading Arabic NLI Datasets\n",
    "According to the paper, GATE uses Arabic-adapted subsets from Stanford NLI and MultiNLI datasets:\n",
    "- **STS Subset**: 8.63K training, 1.68K test samples\n",
    "- **Triplet Subset**: 571K training, 6.58K test samples  \n",
    "- **Pair Classification**: 981K training, 19.7K test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic Arabic NLI dataset for demonstration\n",
    "# In real implementation, you would use translated SNLI/MultiNLI datasets\n",
    "\n",
    "def create_arabic_sample_data():\n",
    "    \"\"\"Create sample Arabic text pairs for demonstration\"\"\"\n",
    "    \n",
    "    # Sample Arabic sentence pairs with similarity scores\n",
    "    arabic_sts_samples = [\n",
    "        {\n",
    "            \"sentence1\": \"Ø±Ø¬Ù„ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙŠØªØ§Ø±\",  # A man playing the guitar\n",
    "            \"sentence2\": \"Ø´Ø®Øµ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø¢Ù„Ø© Ù…ÙˆØ³ÙŠÙ‚ÙŠØ©\",  # A person playing a musical instrument\n",
    "            \"score\": 0.8\n",
    "        },\n",
    "        {\n",
    "            \"sentence1\": \"Ø§Ù„Ø±Ø¬Ø§Ù„ ÙŠÙ„Ø¹Ø¨ÙˆÙ† ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù…\",  # Men are playing football\n",
    "            \"sentence2\": \"Ø§Ù„Ø£ÙˆÙ„Ø§Ø¯ ÙŠÙ„Ø¹Ø¨ÙˆÙ† ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù…\",  # Boys are playing football\n",
    "            \"score\": 0.72\n",
    "        },\n",
    "        {\n",
    "            \"sentence1\": \"Ø±Ø¬Ù„ ÙŠÙ‚ÙˆÙ… Ø¨Ø®Ø¯Ø¹Ø© Ø¨Ø§Ù„ÙˆØ±Ù‚\",  # A man doing a card trick\n",
    "            \"sentence2\": \"Ø±Ø¬Ù„ ÙŠØ¤Ø¯ÙŠ Ø®Ø¯Ø¹Ø© Ø¨Ø§Ù„ÙˆØ±Ù‚\",  # A man performing a card trick\n",
    "            \"score\": 1.0\n",
    "        },\n",
    "        {\n",
    "            \"sentence1\": \"Ø±Ø¬Ù„ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙŠØªØ§Ø±\",  # A man playing the guitar\n",
    "            \"sentence2\": \"Ø±Ø¬Ù„ ÙŠÙ‚ÙˆØ¯ Ø³ÙŠØ§Ø±Ø©\",  # A man driving a car\n",
    "            \"score\": 0.1\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Sample Arabic NLI triplets (premise, positive, negative)\n",
    "    arabic_triplet_samples = [\n",
    "        {\n",
    "            \"anchor\": \"Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ¯Ø±Ø³ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø©\",  # The student studies in the library\n",
    "            \"positive\": \"Ø´Ø®Øµ ÙŠÙ‚Ø±Ø£ ÙƒØªØ§Ø¨Ø§Ù‹ ÙÙŠ Ù…ÙƒØ§Ù† Ù‡Ø§Ø¯Ø¦\",  # A person reads a book in a quiet place\n",
    "            \"negative\": \"Ø§Ù„Ø±Ø¬Ù„ ÙŠÙ„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ø³Ù„Ø©\",  # The man plays basketball\n",
    "        },\n",
    "        {\n",
    "            \"anchor\": \"Ø§Ù„Ù…Ø±Ø£Ø© ØªØ·Ø¨Ø® Ø§Ù„Ø·Ø¹Ø§Ù…\",  # The woman cooks food\n",
    "            \"positive\": \"Ø´Ø®Øµ ÙŠØ­Ø¶Ø± ÙˆØ¬Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø·Ø¨Ø®\",  # Someone prepares a meal in the kitchen\n",
    "            \"negative\": \"Ø§Ù„Ø·ÙÙ„ ÙŠÙ„Ø¹Ø¨ ÙÙŠ Ø§Ù„Ø­Ø¯ÙŠÙ‚Ø©\",  # The child plays in the garden\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Sample classification pairs (premise, hypothesis, label)\n",
    "    arabic_classification_samples = [\n",
    "        {\n",
    "            \"premise\": \"Ø§Ù„Ø±Ø¬Ù„ ÙŠØ¬Ù„Ø³ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ±Ø³ÙŠ\",  # The man sits on the chair\n",
    "            \"hypothesis\": \"Ø§Ù„Ø´Ø®Øµ ÙŠØ¬Ù„Ø³\",  # The person sits\n",
    "            \"label\": \"entailment\"\n",
    "        },\n",
    "        {\n",
    "            \"premise\": \"Ø§Ù„Ù‚Ø·Ø© ØªÙ†Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø±ÙŠØ±\",  # The cat sleeps on the bed\n",
    "            \"hypothesis\": \"Ø§Ù„ÙƒÙ„Ø¨ ÙŠÙ†Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø±ÙŠØ±\",  # The dog sleeps on the bed\n",
    "            \"label\": \"contradiction\"\n",
    "        },\n",
    "        {\n",
    "            \"premise\": \"Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ø¨ÙˆÙ† Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\",  # Students go to school\n",
    "            \"hypothesis\": \"Ø§Ù„Ù†Ø§Ø³ ÙŠØªØ­Ø±ÙƒÙˆÙ†\",  # People are moving\n",
    "            \"label\": \"neutral\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"sts_data\": arabic_sts_samples,\n",
    "        \"triplet_data\": arabic_triplet_samples,\n",
    "        \"classification_data\": arabic_classification_samples\n",
    "    }\n",
    "\n",
    "# Load sample data\n",
    "sample_data = create_arabic_sample_data()\n",
    "\n",
    "print(\"ğŸ“Š Sample Arabic Dataset Created:\")\n",
    "print(f\"   - STS samples: {len(sample_data['sts_data'])}\")\n",
    "print(f\"   - Triplet samples: {len(sample_data['triplet_data'])}\")\n",
    "print(f\"   - Classification samples: {len(sample_data['classification_data'])}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nğŸ” Sample STS Data:\")\n",
    "for i, sample in enumerate(sample_data['sts_data'][:2]):\n",
    "    print(f\"   {i+1}. Sentence 1: {sample['sentence1']}\")\n",
    "    print(f\"      Sentence 2: {sample['sentence2']}\")\n",
    "    print(f\"      Similarity Score: {sample['score']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matryoshka Representation Learning Implementation\n",
    "\n",
    "### Mathematical Foundation\n",
    "According to the paper, MRL optimizes multi-class classification loss for each dimension subset:\n",
    "\n",
    "$$L_{MRL} = \\sum_{m \\in M} c_m L_{CE}(W^{(m)} z_{1:m}, y)$$\n",
    "\n",
    "Where:\n",
    "- $z_{1:m} \\in \\mathbb{R}^m$ is the truncated embedding vector\n",
    "- $W^{(m)} \\in \\mathbb{R}^{L \\times m}$ are classifier weights for dimension $m$\n",
    "- $c_m$ represents relative importance of each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MatryoshkaEmbeddingModel(nn.Module):\n",
    "    \"\"\"Matryoshka Representation Learning Model for Arabic Text Embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_name: str, dimensions: List[int] = [768, 512, 256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.dimensions = sorted(dimensions, reverse=True)  # Largest to smallest\n",
    "        self.max_dim = max(dimensions)\n",
    "        \n",
    "        # Load base model (AraBERT or similar)\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        \n",
    "        # Matryoshka classifiers for each dimension\n",
    "        self.classifiers = nn.ModuleDict({\n",
    "            str(dim): nn.Linear(dim, 3)  # 3 classes: entailment, neutral, contradiction\n",
    "            for dim in self.dimensions\n",
    "        })\n",
    "        \n",
    "        # Dimension importance weights\n",
    "        self.dim_weights = nn.Parameter(torch.ones(len(dimensions)))\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Apply mean pooling to get sentence embeddings\"\"\"\n",
    "        token_embeddings = model_output[0]  # First element contains token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_all_dims=False):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        # Get base model output\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Apply mean pooling\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        \n",
    "        if return_all_dims:\n",
    "            # Return embeddings for all dimensions\n",
    "            dim_embeddings = {}\n",
    "            for dim in self.dimensions:\n",
    "                dim_embeddings[dim] = embeddings[:, :dim]\n",
    "            return dim_embeddings\n",
    "        else:\n",
    "            # Return full dimensional embedding\n",
    "            return embeddings\n",
    "    \n",
    "    def compute_matryoshka_loss(self, embeddings, labels):\n",
    "        \"\"\"Compute Matryoshka loss across all dimensions\"\"\"\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for i, dim in enumerate(self.dimensions):\n",
    "            # Get embeddings for this dimension\n",
    "            dim_embeddings = embeddings[:, :dim]\n",
    "            \n",
    "            # Get classifier predictions\n",
    "            logits = self.classifiers[str(dim)](dim_embeddings)\n",
    "            \n",
    "            # Compute cross-entropy loss\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            \n",
    "            # Weight by dimension importance\n",
    "            total_loss += self.dim_weights[i] * loss\n",
    "        \n",
    "        return total_loss / len(self.dimensions)\n",
    "\n",
    "# Initialize model with AraBERT base\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"  # Popular Arabic BERT model\n",
    "matryoshka_model = MatryoshkaEmbeddingModel(model_name)\n",
    "\n",
    "print(\"ğŸ¯ Matryoshka Model Initialized!\")\n",
    "print(f\"   - Base Model: {model_name}\")\n",
    "print(f\"   - Dimensions: {matryoshka_model.dimensions}\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in matryoshka_model.parameters())/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Loss Training Implementation\n",
    "\n",
    "### Mathematical Foundation\n",
    "The paper proposes two specialized loss functions:\n",
    "\n",
    "1. **Classification Loss** (SoftmaxLoss):\n",
    "$$L_{cls} = -\\frac{1}{n} \\sum_{i=1}^{n} \\log \\frac{e^{s(x_i,y^+)/\\tau}}{e^{s(x_i,y^+)/\\tau} + \\sum_{j=1}^{k} e^{s(x_i,y_j^-)/\\tau}}$$\n",
    "\n",
    "2. **STS Loss** (CoSENTLoss):\n",
    "$$L_{sts} = \\log \\left(1 + \\sum_{s(x_i,x_j) > s(x_m,x_n)} \\exp \\frac{\\cos(x_m,x_n) - \\cos(x_i,x_j)}{\\tau}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLossTrainer:\n",
    "    \"\"\"Hybrid Loss Training for GATE Model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, temperature=0.05):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def classification_loss(self, premise_embeddings, hypothesis_embeddings, labels):\n",
    "        \"\"\"Compute classification loss for NLI task\"\"\"\n",
    "        # Compute similarity scores\n",
    "        similarities = torch.cosine_similarity(premise_embeddings, hypothesis_embeddings, dim=1)\n",
    "        \n",
    "        # Scale by temperature\n",
    "        similarities = similarities / self.temperature\n",
    "        \n",
    "        # Convert similarities to logits for 3-class classification\n",
    "        # This is a simplified version - in practice, you'd use proper classifier heads\n",
    "        batch_size = similarities.size(0)\n",
    "        logits = torch.zeros(batch_size, 3).to(self.device)\n",
    "        \n",
    "        # Map similarities to class probabilities\n",
    "        # High similarity -> entailment, medium -> neutral, low -> contradiction\n",
    "        for i, sim in enumerate(similarities):\n",
    "            if sim > 0.7:\n",
    "                logits[i, 0] = sim  # entailment\n",
    "            elif sim > 0.3:\n",
    "                logits[i, 1] = sim  # neutral\n",
    "            else:\n",
    "                logits[i, 2] = 1 - sim  # contradiction\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "    \n",
    "    def sts_loss(self, sentence1_embeddings, sentence2_embeddings, similarity_scores):\n",
    "        \"\"\"Compute STS loss using CoSENT approach\"\"\"\n",
    "        # Compute cosine similarities\n",
    "        predicted_similarities = torch.cosine_similarity(sentence1_embeddings, sentence2_embeddings, dim=1)\n",
    "        \n",
    "        # Scale target similarities to match cosine range [-1, 1]\n",
    "        target_similarities = (similarity_scores - 2.5) / 2.5  # Assuming 0-5 scale -> -1,1\n",
    "        \n",
    "        # Compute pairwise ranking loss (simplified CoSENT)\n",
    "        batch_size = predicted_similarities.size(0)\n",
    "        loss = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(batch_size):\n",
    "                if i != j and target_similarities[i] > target_similarities[j]:\n",
    "                    # If target i > target j, then predicted i should > predicted j\n",
    "                    diff = predicted_similarities[j] - predicted_similarities[i]\n",
    "                    loss += torch.log(1 + torch.exp(diff / self.temperature))\n",
    "                    count += 1\n",
    "        \n",
    "        return loss / max(count, 1)\n",
    "    \n",
    "    def compute_hybrid_loss(self, batch, task_type):\n",
    "        \"\"\"Compute hybrid loss based on task type\"\"\"\n",
    "        if task_type == 'classification':\n",
    "            # NLI classification task\n",
    "            premise_embeddings = self.model(batch['premise_input_ids'], batch['premise_attention_mask'])\n",
    "            hypothesis_embeddings = self.model(batch['hypothesis_input_ids'], batch['hypothesis_attention_mask'])\n",
    "            return self.classification_loss(premise_embeddings, hypothesis_embeddings, batch['labels'])\n",
    "        \n",
    "        elif task_type == 'sts':\n",
    "            # Semantic textual similarity task\n",
    "            sentence1_embeddings = self.model(batch['sentence1_input_ids'], batch['sentence1_attention_mask'])\n",
    "            sentence2_embeddings = self.model(batch['sentence2_input_ids'], batch['sentence2_attention_mask'])\n",
    "            return self.sts_loss(sentence1_embeddings, sentence2_embeddings, batch['similarity_scores'])\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "\n",
    "# Initialize hybrid loss trainer\n",
    "hybrid_trainer = HybridLossTrainer(matryoshka_model)\n",
    "print(\"ğŸ”¥ Hybrid Loss Trainer initialized!\")\n",
    "print(f\"   - Device: {hybrid_trainer.device}\")\n",
    "print(f\"   - Temperature: {hybrid_trainer.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline Implementation\n",
    "\n",
    "### Multi-Task Training Loop\n",
    "Based on the paper's methodology, we implement a multi-dataset training strategy that alternates between classification and STS tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(sample_data):\n",
    "    \"\"\"Prepare training data for both tasks\"\"\"\n",
    "    \n",
    "    # Prepare STS data\n",
    "    sts_examples = []\n",
    "    for sample in sample_data['sts_data']:\n",
    "        sts_examples.append({\n",
    "            'sentence1': sample['sentence1'],\n",
    "            'sentence2': sample['sentence2'],\n",
    "            'score': sample['score']\n",
    "        })\n",
    "    \n",
    "    # Prepare classification data\n",
    "    classification_examples = []\n",
    "    label_map = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    \n",
    "    for sample in sample_data['classification_data']:\n",
    "        classification_examples.append({\n",
    "            'premise': sample['premise'],\n",
    "            'hypothesis': sample['hypothesis'],\n",
    "            'label': label_map[sample['label']]\n",
    "        })\n",
    "    \n",
    "    return sts_examples, classification_examples\n",
    "\n",
    "def tokenize_batch(sentences, tokenizer, max_length=512):\n",
    "    \"\"\"Tokenize a batch of sentences\"\"\"\n",
    "    return tokenizer(\n",
    "        sentences,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "def training_step_demo():\n",
    "    \"\"\"Demonstrate a training step with sample data\"\"\"\n",
    "    \n",
    "    # Prepare sample data\n",
    "    sts_examples, classification_examples = prepare_training_data(sample_data)\n",
    "    \n",
    "    print(\"ğŸš‚ Starting Training Demonstration...\")\n",
    "    \n",
    "    # Set model to training mode\n",
    "    matryoshka_model.train()\n",
    "    optimizer = torch.optim.AdamW(matryoshka_model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Training step for STS task\n",
    "    print(\"\\nğŸ“Š STS Training Step:\")\n",
    "    if sts_examples:\n",
    "        # Prepare STS batch\n",
    "        sentences1 = [ex['sentence1'] for ex in sts_examples]\n",
    "        sentences2 = [ex['sentence2'] for ex in sts_examples]\n",
    "        scores = torch.tensor([ex['score'] for ex in sts_examples], dtype=torch.float)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens1 = tokenize_batch(sentences1, matryoshka_model.tokenizer)\n",
    "        tokens2 = tokenize_batch(sentences2, matryoshka_model.tokenizer)\n",
    "        \n",
    "        # Prepare batch for hybrid trainer\n",
    "        sts_batch = {\n",
    "            'sentence1_input_ids': tokens1['input_ids'],\n",
    "            'sentence1_attention_mask': tokens1['attention_mask'],\n",
    "            'sentence2_input_ids': tokens2['input_ids'],\n",
    "            'sentence2_attention_mask': tokens2['attention_mask'],\n",
    "            'similarity_scores': scores\n",
    "        }\n",
    "        \n",
    "        # Compute loss\n",
    "        sts_loss = hybrid_trainer.compute_hybrid_loss(sts_batch, 'sts')\n",
    "        print(f\"   STS Loss: {sts_loss.item():.4f}\")\n",
    "    \n",
    "    # Training step for classification task\n",
    "    print(\"\\nğŸ” Classification Training Step:\")\n",
    "    if classification_examples:\n",
    "        # Prepare classification batch\n",
    "        premises = [ex['premise'] for ex in classification_examples]\n",
    "        hypotheses = [ex['hypothesis'] for ex in classification_examples]\n",
    "        labels = torch.tensor([ex['label'] for ex in classification_examples], dtype=torch.long)\n",
    "        \n",
    "        # Tokenize\n",
    "        premise_tokens = tokenize_batch(premises, matryoshka_model.tokenizer)\n",
    "        hypothesis_tokens = tokenize_batch(hypotheses, matryoshka_model.tokenizer)\n",
    "        \n",
    "        # Prepare batch for hybrid trainer\n",
    "        cls_batch = {\n",
    "            'premise_input_ids': premise_tokens['input_ids'],\n",
    "            'premise_attention_mask': premise_tokens['attention_mask'],\n",
    "            'hypothesis_input_ids': hypothesis_tokens['input_ids'],\n",
    "            'hypothesis_attention_mask': hypothesis_tokens['attention_mask'],\n",
    "            'labels': labels\n",
    "        }\n",
    "        \n",
    "        # Compute loss\n",
    "        cls_loss = hybrid_trainer.compute_hybrid_loss(cls_batch, 'classification')\n",
    "        print(f\"   Classification Loss: {cls_loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\nâœ… Training step demonstration completed!\")\n",
    "    print(\"ğŸ’¡ In full training, you would:\")\n",
    "    print(\"   1. Alternate between STS and classification batches\")\n",
    "    print(\"   2. Apply Matryoshka loss across multiple dimensions\")\n",
    "    print(\"   3. Use proper data loaders with larger datasets\")\n",
    "    print(\"   4. Implement learning rate scheduling\")\n",
    "    print(\"   5. Add validation and checkpointing\")\n",
    "\n",
    "# Run training demonstration\n",
    "training_step_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with MTEB and DeepEval\n",
    "\n",
    "### MTEB Benchmarks\n",
    "The paper evaluates on Arabic STS benchmarks: STS17, STS22, and STS22-v2.\n",
    "We'll implement evaluation metrics following the paper's methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "\n",
    "class GATEEvaluator:\n",
    "    \"\"\"Evaluation toolkit for GATE models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def encode_sentences(self, sentences, dimension=768):\n",
    "        \"\"\"Encode sentences to embeddings of specified dimension\"\"\"\n",
    "        self.model.eval()\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sentence in sentences:\n",
    "                # Tokenize\n",
    "                tokens = self.tokenizer(\n",
    "                    sentence, \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=512, \n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Get embeddings\n",
    "                embedding = self.model(tokens['input_ids'], tokens['attention_mask'])\n",
    "                \n",
    "                # Truncate to specified dimension\n",
    "                if dimension < embedding.size(1):\n",
    "                    embedding = embedding[:, :dimension]\n",
    "                \n",
    "                embeddings.append(embedding.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(embeddings)\n",
    "    \n",
    "    def evaluate_sts(self, test_data, dimensions=[768, 512, 256, 128, 64]):\n",
    "        \"\"\"Evaluate semantic textual similarity across multiple dimensions\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Extract sentences and scores\n",
    "        sentences1 = [item['sentence1'] for item in test_data]\n",
    "        sentences2 = [item['sentence2'] for item in test_data]\n",
    "        true_scores = [item['score'] for item in test_data]\n",
    "        \n",
    "        for dim in dimensions:\n",
    "            print(f\"\\nğŸ” Evaluating dimension {dim}...\")\n",
    "            \n",
    "            # Encode sentences\n",
    "            embeddings1 = self.encode_sentences(sentences1, dim)\n",
    "            embeddings2 = self.encode_sentences(sentences2, dim)\n",
    "            \n",
    "            # Compute similarity scores\n",
    "            predicted_scores = []\n",
    "            for emb1, emb2 in zip(embeddings1, embeddings2):\n",
    "                similarity = cosine_similarity([emb1], [emb2])[0, 0]\n",
    "                # Scale from [-1, 1] to [0, 1] for comparison\n",
    "                similarity = (similarity + 1) / 2\n",
    "                predicted_scores.append(similarity)\n",
    "            \n",
    "            # Compute correlation metrics\n",
    "            pearson_corr, _ = pearsonr(true_scores, predicted_scores)\n",
    "            spearman_corr, _ = spearmanr(true_scores, predicted_scores)\n",
    "            \n",
    "            results[dim] = {\n",
    "                'pearson': pearson_corr,\n",
    "                'spearman': spearman_corr,\n",
    "                'predicted_scores': predicted_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"   Pearson: {pearson_corr:.4f}\")\n",
    "            print(f\"   Spearman: {spearman_corr:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def deepeval_assessment(self, test_data):\n",
    "        \"\"\"Evaluate using DeepEval framework\"\"\"\n",
    "        print(\"\\nğŸ¯ DeepEval Assessment:\")\n",
    "        \n",
    "        # Create test cases for DeepEval\n",
    "        test_cases = []\n",
    "        for item in test_data[:2]:  # Limit for demo\n",
    "            test_case = LLMTestCase(\n",
    "                input=item['sentence1'],\n",
    "                actual_output=item['sentence2'],\n",
    "                expected_output=f\"Similarity: {item['score']}\"\n",
    "            )\n",
    "            test_cases.append(test_case)\n",
    "        \n",
    "        # Define semantic similarity metric\n",
    "        semantic_similarity_metric = SemanticSimilarityMetric(threshold=0.7)\n",
    "        \n",
    "        # Evaluate\n",
    "        try:\n",
    "            results = evaluate(test_cases, [semantic_similarity_metric])\n",
    "            print(f\"   DeepEval Score: {results}\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"   DeepEval assessment skipped: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = GATEEvaluator(matryoshka_model, matryoshka_model.tokenizer)\n",
    "\n",
    "# Run evaluation on sample data\n",
    "print(\"ğŸš€ Starting GATE Model Evaluation...\")\n",
    "evaluation_results = evaluator.evaluate_sts(sample_data['sts_data'])\n",
    "\n",
    "# DeepEval assessment\n",
    "deepeval_results = evaluator.deepeval_assessment(sample_data['sts_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "### Performance Comparison Across Dimensions\n",
    "Recreating Figure 1 from the paper showing correlation-based similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(evaluation_results):\n",
    "    \"\"\"Visualize evaluation results across dimensions\"\"\"\n",
    "    \n",
    "    dimensions = list(evaluation_results.keys())\n",
    "    pearson_scores = [evaluation_results[dim]['pearson'] for dim in dimensions]\n",
    "    spearman_scores = [evaluation_results[dim]['spearman'] for dim in dimensions]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Pearson Correlation across dimensions\n",
    "    ax1.plot(dimensions, pearson_scores, marker='o', linewidth=2, markersize=8)\n",
    "    ax1.set_title('Pearson Correlation vs Embedding Dimensions', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Embedding Dimensions')\n",
    "    ax1.set_ylabel('Pearson Correlation')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Spearman Correlation across dimensions\n",
    "    ax2.plot(dimensions, spearman_scores, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "    ax2.set_title('Spearman Correlation vs Embedding Dimensions', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Embedding Dimensions')\n",
    "    ax2.set_ylabel('Spearman Correlation')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 3: Comparison of both metrics\n",
    "    x = np.arange(len(dimensions))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, pearson_scores, width, label='Pearson', alpha=0.8)\n",
    "    ax3.bar(x + width/2, spearman_scores, width, label='Spearman', alpha=0.8)\n",
    "    ax3.set_title('Correlation Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Embedding Dimensions')\n",
    "    ax3.set_ylabel('Correlation Score')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(dimensions)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Predicted vs True scores for largest dimension\n",
    "    largest_dim = max(dimensions)\n",
    "    true_scores = [item['score'] for item in sample_data['sts_data']]\n",
    "    pred_scores = evaluation_results[largest_dim]['predicted_scores']\n",
    "    \n",
    "    ax4.scatter(true_scores, pred_scores, alpha=0.7, s=100)\n",
    "    ax4.plot([0, 1], [0, 1], 'r--', alpha=0.8)  # Perfect correlation line\n",
    "    ax4.set_title(f'Predicted vs True Similarity (Dim {largest_dim})', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('True Similarity Score')\n",
    "    ax4.set_ylabel('Predicted Similarity Score')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nğŸ“Š GATE Model Performance Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    for dim in dimensions:\n",
    "        pearson = evaluation_results[dim]['pearson']\n",
    "        spearman = evaluation_results[dim]['spearman']\n",
    "        print(f\"Dimension {dim:3d}: Pearson={pearson:.4f}, Spearman={spearman:.4f}\")\n",
    "    \n",
    "    # Performance degradation analysis\n",
    "    print(\"\\nğŸ“ˆ Performance Degradation Analysis:\")\n",
    "    base_pearson = evaluation_results[max(dimensions)]['pearson']\n",
    "    base_spearman = evaluation_results[max(dimensions)]['spearman']\n",
    "    \n",
    "    for dim in sorted(dimensions, reverse=True)[1:]:\n",
    "        pearson_drop = (base_pearson - evaluation_results[dim]['pearson']) / base_pearson * 100\n",
    "        spearman_drop = (base_spearman - evaluation_results[dim]['spearman']) / base_spearman * 100\n",
    "        print(f\"Dimension {dim}: Pearson drop {pearson_drop:.1f}%, Spearman drop {spearman_drop:.1f}%\")\n",
    "\n",
    "# Visualize results\n",
    "if evaluation_results:\n",
    "    visualize_results(evaluation_results)\n",
    "else:\n",
    "    print(\"âš ï¸ No evaluation results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Integration for RAG Applications\n",
    "\n",
    "### Using GATE Embeddings in LangChain Pipeline\n",
    "Demonstrate how to integrate the GATE model into a LangChain RAG system for Arabic text retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATELangChainEmbeddings:\n",
    "    \"\"\"LangChain-compatible wrapper for GATE embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, gate_model, tokenizer, dimension=768):\n",
    "        self.gate_model = gate_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dimension = dimension\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        self.gate_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                tokens = self.tokenizer(\n",
    "                    text,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                embedding = self.gate_model(tokens['input_ids'], tokens['attention_mask'])\n",
    "                \n",
    "                # Truncate to specified dimension\n",
    "                if self.dimension < embedding.size(1):\n",
    "                    embedding = embedding[:, :self.dimension]\n",
    "                \n",
    "                embeddings.append(embedding.cpu().numpy().flatten().tolist())\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "def create_arabic_rag_demo():\n",
    "    \"\"\"Demonstrate Arabic RAG system with GATE embeddings\"\"\"\n",
    "    \n",
    "    # Sample Arabic documents\n",
    "    arabic_documents = [\n",
    "        \"Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ Ù…Ø­Ø§ÙƒØ§Ø© Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø´Ø±ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø¢Ù„Ø§Øª ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©.\",\n",
    "        \"Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\",\n",
    "        \"Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù…Ù„ Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø¨Ø´Ø±ÙŠ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.\",\n",
    "        \"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØªÙŠØ­ Ù„Ù„Ø­Ø§Ø³ÙˆØ¨ ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©.\",\n",
    "        \"Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© ØªÙ…ÙƒÙ† Ø§Ù„Ø¢Ù„Ø§Øª Ù…Ù† ØªÙØ³ÙŠØ± ÙˆÙÙ‡Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨ØµØ±ÙŠ Ù…Ø«Ù„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸš€ Creating Arabic RAG System with GATE Embeddings...\")\n",
    "    \n",
    "    # Initialize GATE embeddings for LangChain\n",
    "    gate_embeddings = GATELangChainEmbeddings(\n",
    "        matryoshka_model, \n",
    "        matryoshka_model.tokenizer, \n",
    "        dimension=256  # Use smaller dimension for efficiency\n",
    "    )\n",
    "    \n",
    "    # Create documents\n",
    "    documents = [Document(page_content=text, metadata={\"source\": f\"doc_{i}\"}) \n",
    "                for i, text in enumerate(arabic_documents)]\n",
    "    \n",
    "    # Create vector store\n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(documents, gate_embeddings)\n",
    "        print(\"âœ… Vector store created successfully!\")\n",
    "        \n",
    "        # Test similarity search\n",
    "        query = \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\"  # What is machine learning?\n",
    "        print(f\"\\nğŸ” Query: {query}\")\n",
    "        \n",
    "        # Perform similarity search\n",
    "        similar_docs = vectorstore.similarity_search(query, k=3)\n",
    "        \n",
    "        print(\"\\nğŸ“‹ Most Similar Documents:\")\n",
    "        for i, doc in enumerate(similar_docs, 1):\n",
    "            print(f\"   {i}. {doc.page_content}\")\n",
    "            print(f\"      Source: {doc.metadata['source']}\\n\")\n",
    "        \n",
    "        # Test with similarity scores\n",
    "        similar_docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "        \n",
    "        print(\"ğŸ¯ Similarity Scores:\")\n",
    "        for doc, score in similar_docs_with_scores:\n",
    "            print(f\"   Score: {score:.4f} - {doc.page_content[:50]}...\")\n",
    "        \n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating vector store: {e}\")\n",
    "        print(\"ğŸ’¡ This might be due to dimension mismatch or missing dependencies\")\n",
    "        return None\n",
    "\n",
    "# Create Arabic RAG demonstration\n",
    "arabic_vectorstore = create_arabic_rag_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Research Template\n",
    "\n",
    "### Extending GATE for Your Research\n",
    "This section provides a template for extending the GATE framework for your own research projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal Research Template\n",
    "print(\"\"\"ğŸ”¬ GATE Research Extension Template\n",
    "=====================================\n",
    "\n",
    "This template guides you through extending GATE for your research:\n",
    "\n",
    "1. ğŸ“Š CUSTOM DATASET INTEGRATION\n",
    "   - Replace sample_data with your Arabic text corpus\n",
    "   - Implement domain-specific preprocessing\n",
    "   - Create evaluation benchmarks for your domain\n",
    "\n",
    "2. ğŸ—ï¸ ARCHITECTURE MODIFICATIONS\n",
    "   - Experiment with different base models (MARBERT, ARBERT, etc.)\n",
    "   - Modify Matryoshka dimensions for your use case\n",
    "   - Add domain-specific loss functions\n",
    "\n",
    "3. ğŸ¯ TASK-SPECIFIC ADAPTATIONS\n",
    "   - Information Retrieval: Optimize for document ranking\n",
    "   - Question Answering: Add QA-specific training objectives\n",
    "   - Summarization: Include summarization quality metrics\n",
    "   - Classification: Add multi-label classification support\n",
    "\n",
    "4. ğŸ”§ TRAINING OPTIMIZATIONS\n",
    "   - Implement curriculum learning strategies\n",
    "   - Add adversarial training for robustness\n",
    "   - Experiment with different optimizers and schedules\n",
    "   - Add gradient clipping and regularization\n",
    "\n",
    "5. ğŸ“ˆ EVALUATION ENHANCEMENTS\n",
    "   - Add domain-specific evaluation metrics\n",
    "   - Implement cross-lingual evaluation\n",
    "   - Create visualization dashboards\n",
    "   - Add statistical significance testing\n",
    "\n",
    "6. ğŸš€ DEPLOYMENT CONSIDERATIONS\n",
    "   - Model quantization for production\n",
    "   - API endpoint development\n",
    "   - Batch processing optimization\n",
    "   - Memory usage monitoring\n",
    "\n",
    "7. ğŸ“ RESEARCH EXTENSIONS\n",
    "   - Multi-modal Arabic embeddings (text + images)\n",
    "   - Cross-lingual transfer learning\n",
    "   - Few-shot learning capabilities\n",
    "   - Interpretability analysis\n",
    "\"\"\")\n",
    "\n",
    "class ResearchExtensionTemplate:\n",
    "    \"\"\"Template class for extending GATE research\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.research_config = {\n",
    "            'domain': 'your_domain_here',  # e.g., 'medical', 'legal', 'news'\n",
    "            'base_model': 'aubmindlab/bert-base-arabertv02',\n",
    "            'dimensions': [768, 512, 256, 128, 64],\n",
    "            'batch_size': 64,\n",
    "            'learning_rate': 2e-5,\n",
    "            'epochs': 5,\n",
    "            'max_length': 512\n",
    "        }\n",
    "    \n",
    "    def load_custom_dataset(self, dataset_path):\n",
    "        \"\"\"Load your custom Arabic dataset\"\"\"\n",
    "        # TODO: Implement your dataset loading logic\n",
    "        print(f\"ğŸ“ Loading dataset from: {dataset_path}\")\n",
    "        pass\n",
    "    \n",
    "    def custom_preprocessing(self, texts):\n",
    "        \"\"\"Apply domain-specific preprocessing\"\"\"\n",
    "        # TODO: Add your preprocessing steps\n",
    "        # - Diacritization\n",
    "        # - Normalization\n",
    "        # - Domain-specific cleaning\n",
    "        print(\"ğŸ”§ Applying custom preprocessing...\")\n",
    "        return texts\n",
    "    \n",
    "    def define_custom_loss(self):\n",
    "        \"\"\"Define domain-specific loss functions\"\"\"\n",
    "        # TODO: Implement custom loss functions\n",
    "        print(\"ğŸ¯ Defining custom loss function...\")\n",
    "        pass\n",
    "    \n",
    "    def custom_evaluation_metrics(self):\n",
    "        \"\"\"Define domain-specific evaluation metrics\"\"\"\n",
    "        # TODO: Implement custom metrics\n",
    "        print(\"ğŸ“Š Defining custom evaluation metrics...\")\n",
    "        pass\n",
    "    \n",
    "    def hyperparameter_tuning(self):\n",
    "        \"\"\"Perform hyperparameter optimization\"\"\"\n",
    "        # TODO: Implement hyperparameter search\n",
    "        print(\"ğŸ” Starting hyperparameter tuning...\")\n",
    "        pass\n",
    "    \n",
    "    def model_interpretation(self):\n",
    "        \"\"\"Analyze model behavior and interpretability\"\"\"\n",
    "        # TODO: Add interpretation techniques\n",
    "        # - Attention visualization\n",
    "        # - Embedding analysis\n",
    "        # - Error analysis\n",
    "        print(\"ğŸ”¬ Analyzing model interpretability...\")\n",
    "        pass\n",
    "\n",
    "# Initialize research template\n",
    "research_template = ResearchExtensionTemplate()\n",
    "print(\"\\nğŸ“ Research extension template initialized!\")\n",
    "print(\"   Customize the methods above for your specific research needs.\")\n",
    "print(\"   Check the focused learning notebooks for deep dives into complex concepts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Resources\n",
    "\n",
    "### Recommended Learning Path\n",
    "\n",
    "1. **ğŸ“š Study the Focused Learning Notebooks**:\n",
    "   - `MRL_Deep_Learning.ipynb` - Deep dive into Matryoshka Representation Learning\n",
    "   - `Hybrid_Loss_Architecture.ipynb` - Master multi-task loss functions\n",
    "   - `Arabic_NLP_Challenges.ipynb` - Handle Arabic language complexities\n",
    "   - `Contrastive_Triplet_Learning.ipynb` - Advanced contrastive learning\n",
    "\n",
    "2. **ğŸ”¬ Paper References**:\n",
    "   - Original GATE Paper: [arXiv:2505.24581v1](https://arxiv.org/abs/2505.24581v1)\n",
    "   - Matryoshka Representation Learning: [Kusupati et al., 2022](https://arxiv.org/abs/2205.13147)\n",
    "   - Sentence-BERT: [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "3. **ğŸ› ï¸ Implementation Resources**:\n",
    "   - GATE Models: [Hugging Face Hub](https://huggingface.co/collections/gate-models)\n",
    "   - Arabic NLP Datasets: [MTEB Benchmark](https://github.com/embeddings-benchmark/mteb)\n",
    "   - LangChain Documentation: [LangChain Docs](https://python.langchain.com/)\n",
    "\n",
    "4. **ğŸ’¡ Research Opportunities**:\n",
    "   - Multi-modal Arabic embeddings\n",
    "   - Cross-lingual transfer learning\n",
    "   - Domain adaptation techniques\n",
    "   - Efficiency optimization methods\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "âœ… **GATE achieves state-of-the-art Arabic STS performance**  \n",
    "âœ… **Matryoshka learning enables flexible embedding dimensions**  \n",
    "âœ… **Hybrid loss training improves multi-task performance**  \n",
    "âœ… **Integration with LangChain enables RAG applications**  \n",
    "âœ… **DeepEval provides comprehensive evaluation framework**  \n",
    "\n",
    "---\n",
    "\n",
    "*This notebook implements the GATE framework as described in \"GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training\" (arXiv:2505.24581v1)*\n",
    "\n",
    "**Authors**: Omer Nacar, Anis Koubaa, Serry Sibaee, Yasser Al-Habashi, Adel Ammar, Wadii Boulila  \n",
    "**Implementation**: Educational Research Notebook  \n",
    "**Framework**: LangChain + DeepEval + PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}