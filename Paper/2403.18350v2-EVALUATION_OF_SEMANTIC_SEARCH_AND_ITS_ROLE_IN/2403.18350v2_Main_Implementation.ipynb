{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION OF SEMANTIC SEARCH AND ITS ROLE IN RETRIEVED-AUGMENTED-GENERATION (RAG) FOR ARABIC LANGUAGE\n",
    "\n",
    "## Paper Information\n",
    "- **Title**: Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language\n",
    "- **Authors**: Ali Mahboub, Muhy Eddin Za'ter, Bashar Al-Rfooh, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz\n",
    "- **Organization**: Maqsam, Amman, Jordan\n",
    "- **ArXiv Link**: [2403.18350v2](https://arxiv.org/abs/2403.18350v2)\n",
    "\n",
    "## Abstract\n",
    "Paper thiết lập benchmark cho semantic search tiếng Ả Rập và đánh giá hiệu quả của semantic search trong framework RAG. Nghiên cứu so sánh 5 encoder đa ngôn ngữ và chứng minh tầm quan trọng của semantic search trong việc cải thiện chất lượng RAG cho tiếng Ả Rập."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-community\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Structures & Dataset Generation\n",
    "\n",
    "Paper sử dụng dataset gồm:\n",
    "- 2030 customer support call summaries (tiếng Ả Rập)\n",
    "- 406 search queries\n",
    "- Relevance scores: 0 (irrelevant), 1 (somewhat relevant), 2 (very relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchQuery:\n",
    "    \"\"\"Represents a search query with associated documents and relevance scores\"\"\"\n",
    "    query_id: str\n",
    "    query_text: str\n",
    "    document_relevance: Dict[str, int]  # doc_id -> relevance score (0, 1, 2)\n",
    "\n",
    "@dataclass \n",
    "class Document:\n",
    "    \"\"\"Represents a document in the corpus\"\"\"\n",
    "    doc_id: str\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset mimicking the paper's approach\n",
    "# In production, this would use GPT-4 as described in Section 3.1\n",
    "\n",
    "def generate_synthetic_arabic_dataset(n_docs: int = 100, n_queries: int = 20) -> Tuple[List[Document], List[SearchQuery]]:\n",
    "    \"\"\"\n",
    "    Generate synthetic dataset for testing.\n",
    "    In real implementation, this would use GPT-4 to generate Arabic customer support data.\n",
    "    \"\"\"\n",
    "    # Mock Arabic customer support summaries\n",
    "    arabic_summaries = [\n",
    "        \"العميل يواجه مشكلة في تسجيل الدخول إلى حسابه\",\n",
    "        \"طلب استرداد المبلغ المدفوع للمنتج المعيب\",\n",
    "        \"استفسار حول موعد التسليم المتأخر\",\n",
    "        \"شكوى بخصوص جودة الخدمة المقدمة\",\n",
    "        \"طلب تحديث معلومات الحساب الشخصي\"\n",
    "    ]\n",
    "    \n",
    "    documents = []\n",
    "    for i in range(n_docs):\n",
    "        doc = Document(\n",
    "            doc_id=f\"doc_{i}\",\n",
    "            content=arabic_summaries[i % len(arabic_summaries)] + f\" - حالة رقم {i}\",\n",
    "            metadata={\"category\": f\"category_{i % 5}\"}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Generate queries with relevance scores\n",
    "    queries = []\n",
    "    arabic_queries = [\n",
    "        \"مشكلة تسجيل دخول\",\n",
    "        \"استرداد مبلغ\",\n",
    "        \"تأخير التسليم\",\n",
    "        \"جودة الخدمة\",\n",
    "        \"تحديث المعلومات\"\n",
    "    ]\n",
    "    \n",
    "    for i in range(n_queries):\n",
    "        query = SearchQuery(\n",
    "            query_id=f\"query_{i}\",\n",
    "            query_text=arabic_queries[i % len(arabic_queries)],\n",
    "            document_relevance={}\n",
    "        )\n",
    "        \n",
    "        # Assign relevance scores (simulating GPT-4 labeling)\n",
    "        for j, doc in enumerate(documents[:5]):  # Each query associated with 5 docs\n",
    "            if j == i % 5:  # Very relevant\n",
    "                query.document_relevance[doc.doc_id] = 2\n",
    "            elif abs(j - (i % 5)) == 1:  # Somewhat relevant\n",
    "                query.document_relevance[doc.doc_id] = 1\n",
    "            else:  # Irrelevant\n",
    "                query.document_relevance[doc.doc_id] = 0\n",
    "        \n",
    "        queries.append(query)\n",
    "    \n",
    "    return documents, queries\n",
    "\n",
    "# Generate test dataset\n",
    "documents, queries = generate_synthetic_arabic_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Search Implementation with LangChain\n",
    "\n",
    "### 3.1 Encoder Setup\n",
    "Paper đánh giá 5 encoder multilingual. Tôi sẽ implement với LangChain HuggingFaceEmbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoders from the paper\n",
    "ENCODERS = {\n",
    "    \"encoder_1\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"embedding_size\": 384,\n",
    "        \"description\": \"Paraphrase Multilingual MiniLM\"\n",
    "    },\n",
    "    \"encoder_2\": {\n",
    "        \"model_name\": \"sentence-transformers/use-cmlm-multilingual\", \n",
    "        \"embedding_size\": 768,\n",
    "        \"description\": \"CMLM Multilingual\"\n",
    "    },\n",
    "    \"encoder_3\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        \"embedding_size\": 768,\n",
    "        \"description\": \"Paraphrase Multilingual MPNet\"\n",
    "    },\n",
    "    \"encoder_4\": {\n",
    "        \"model_name\": \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "        \"embedding_size\": 512,\n",
    "        \"description\": \"Multilingual DistilBERT\"\n",
    "    },\n",
    "    \"encoder_5\": {\n",
    "        \"model_name\": \"symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli\",\n",
    "        \"embedding_size\": 768,\n",
    "        \"description\": \"XLM-RoBERTa\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEvaluator:\n",
    "    \"\"\"Evaluates semantic search performance using different encoders\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_config: Dict[str, Any]):\n",
    "        self.encoder_name = encoder_config[\"model_name\"]\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.encoder_name,\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        \n",
    "    def index_documents(self, documents: List[Document]):\n",
    "        \"\"\"Create FAISS index from documents using LangChain\"\"\"\n",
    "        # Convert to LangChain Document format\n",
    "        langchain_docs = [\n",
    "            Document(page_content=doc.content, metadata={\"doc_id\": doc.doc_id})\n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents=langchain_docs,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "    def search(self, query: str, k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Perform semantic search and return doc_ids with scores\"\"\"\n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"Documents not indexed yet\")\n",
    "            \n",
    "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        return [(doc.metadata[\"doc_id\"], score) for doc, score in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation Metrics Implementation\n",
    "\n",
    "Implement các metrics từ Section 3.2: nDCG, MRR, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"Calculate evaluation metrics for semantic search as defined in the paper\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_ndcg_at_k(relevance_scores: List[int], k: int = 3) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Normalized Discounted Cumulative Gain at k\n",
    "        Equation (1) and (2) from the paper\n",
    "        \"\"\"\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "            \n",
    "        # Calculate DCG@k\n",
    "        dcg = 0.0\n",
    "        for i in range(min(k, len(relevance_scores))):\n",
    "            dcg += (2**relevance_scores[i] - 1) / np.log2(i + 2)\n",
    "        \n",
    "        # Calculate IDCG@k (ideal ranking)\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        idcg = 0.0\n",
    "        for i in range(min(k, len(ideal_scores))):\n",
    "            idcg += (2**ideal_scores[i] - 1) / np.log2(i + 2)\n",
    "        \n",
    "        # Return nDCG\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mrr_at_k(relevance_scores: List[int], k: int = 3) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Reciprocal Rank at k\n",
    "        Equation (3) from the paper\n",
    "        \"\"\"\n",
    "        # Find first very relevant document (score = 2)\n",
    "        for i in range(min(k, len(relevance_scores))):\n",
    "            if relevance_scores[i] == 2:\n",
    "                return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_map_at_k(relevance_scores: List[int], k: int = 3) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Average Precision at k\n",
    "        Equations (4) and (5) from the paper\n",
    "        \"\"\"\n",
    "        num_relevant = sum(1 for score in relevance_scores if score > 0)\n",
    "        if num_relevant == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        ap = 0.0\n",
    "        relevant_count = 0\n",
    "        \n",
    "        for i in range(min(k, len(relevance_scores))):\n",
    "            if relevance_scores[i] > 0:\n",
    "                relevant_count += 1\n",
    "                precision_at_i = relevant_count / (i + 1)\n",
    "                ap += precision_at_i * relevance_scores[i]\n",
    "        \n",
    "        return ap / num_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_encoder(encoder_config: Dict, documents: List[Document], \n",
    "                    queries: List[SearchQuery], k: int = 3) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a single encoder on the dataset\n",
    "    \"\"\"\n",
    "    # Initialize evaluator\n",
    "    evaluator = SemanticSearchEvaluator(encoder_config)\n",
    "    evaluator.index_documents(documents)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    ndcg_scores = []\n",
    "    mrr_scores = []\n",
    "    map_scores = []\n",
    "    \n",
    "    for query in tqdm(queries, desc=f\"Evaluating {encoder_config['description']}\"):\n",
    "        # Perform search\n",
    "        search_results = evaluator.search(query.query_text, k=k)\n",
    "        \n",
    "        # Get relevance scores for retrieved documents\n",
    "        relevance_scores = []\n",
    "        for doc_id, _ in search_results:\n",
    "            relevance_scores.append(query.document_relevance.get(doc_id, 0))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        ndcg_scores.append(metrics_calc.calculate_ndcg_at_k(relevance_scores, k))\n",
    "        mrr_scores.append(metrics_calc.calculate_mrr_at_k(relevance_scores, k))\n",
    "        map_scores.append(metrics_calc.calculate_map_at_k(relevance_scores, k))\n",
    "    \n",
    "    return {\n",
    "        \"NDCG@3\": np.mean(ndcg_scores),\n",
    "        \"MRR@3\": np.mean(mrr_scores),\n",
    "        \"mAP@3\": np.mean(map_scores),\n",
    "        \"Emb_Size\": encoder_config[\"embedding_size\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all encoders\n",
    "results = {}\n",
    "\n",
    "# Note: In practice, you would use the actual encoders.\n",
    "# For demonstration, we'll show how to evaluate one encoder\n",
    "print(\"Evaluating Semantic Search Encoders...\")\n",
    "print(\"Note: Full evaluation requires downloading all 5 encoder models.\")\n",
    "print(\"\\nDemonstrating with Encoder #1...\")\n",
    "\n",
    "# Evaluate first encoder as example\n",
    "encoder_1_results = evaluate_encoder(\n",
    "    ENCODERS[\"encoder_1\"], \n",
    "    documents, \n",
    "    queries\n",
    ")\n",
    "\n",
    "print(f\"\\nEncoder #1 Results:\")\n",
    "for metric, value in encoder_1_results.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Pipeline Implementation with LangChain\n",
    "\n",
    "### 4.1 RAG Dataset for Arabic FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FAQ:\n",
    "    \"\"\"Represents a FAQ with question and answer\"\"\"\n",
    "    faq_id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    domain: str\n",
    "\n",
    "def generate_arabic_faq_dataset() -> List[FAQ]:\n",
    "    \"\"\"\n",
    "    Generate synthetic Arabic FAQ dataset.\n",
    "    Paper mentions 816 FAQs from 4 domains.\n",
    "    \"\"\"\n",
    "    faqs = [\n",
    "        FAQ(\"faq_1\", \"كيف يمكنني إعادة تعيين كلمة المرور؟\", \n",
    "            \"يمكنك إعادة تعيين كلمة المرور من خلال النقر على 'نسيت كلمة المرور' في صفحة تسجيل الدخول\",\n",
    "            \"account\"),\n",
    "        FAQ(\"faq_2\", \"ما هي سياسة الإرجاع؟\",\n",
    "            \"يمكنك إرجاع المنتجات خلال 30 يومًا من تاريخ الشراء مع الاحتفاظ بالفاتورة\",\n",
    "            \"policy\"),\n",
    "        FAQ(\"faq_3\", \"كيف أتتبع طلبي؟\",\n",
    "            \"يمكنك تتبع طلبك من خلال رقم التتبع المرسل إلى بريدك الإلكتروني\",\n",
    "            \"shipping\"),\n",
    "        FAQ(\"faq_4\", \"ما هي طرق الدفع المتاحة؟\",\n",
    "            \"نقبل البطاقات الائتمانية، PayPal، والدفع عند الاستلام\",\n",
    "            \"payment\")\n",
    "    ]\n",
    "    return faqs\n",
    "\n",
    "faqs = generate_arabic_faq_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LangChain RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicRAGPipeline:\n",
    "    \"\"\"RAG Pipeline for Arabic as described in Section 3.4\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_name: str, faqs: List[FAQ]):\n",
    "        # Initialize embeddings\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=encoder_name,\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        # Create documents from FAQs\n",
    "        self.documents = [\n",
    "            Document(\n",
    "                page_content=f\"السؤال: {faq.question}\\nالإجابة: {faq.answer}\",\n",
    "                metadata={\"faq_id\": faq.faq_id, \"domain\": faq.domain}\n",
    "            )\n",
    "            for faq in faqs\n",
    "        ]\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # Initialize LLM (GPT-3.5-turbo as mentioned in paper)\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Create retrieval chain\n",
    "        self.qa_chain = self._create_qa_chain()\n",
    "    \n",
    "    def _create_qa_chain(self):\n",
    "        \"\"\"Create the QA chain with custom Arabic prompt\"\"\"\n",
    "        prompt_template = \"\"\"أنت مساعد ذكي يجيب على الأسئلة باللغة العربية.\n",
    "        استخدم المعلومات التالية للإجابة على السؤال. إذا لم تجد الإجابة في المعلومات المتاحة، قل \"لا أعرف\".\n",
    "        \n",
    "        المعلومات المتاحة:\n",
    "        {context}\n",
    "        \n",
    "        السؤال: {question}\n",
    "        الإجابة:\"\"\"\n",
    "        \n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            chain_type_kwargs={\"prompt\": PROMPT}\n",
    "        )\n",
    "    \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Answer a question using RAG pipeline\"\"\"\n",
    "        return self.qa_chain.run(question)\n",
    "    \n",
    "    def get_retrieved_docs(self, question: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"Get retrieved documents for analysis\"\"\"\n",
    "        return self.vector_store.similarity_search(question, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_accuracy(rag_pipeline: ArabicRAGPipeline, test_questions: List[Dict]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate RAG accuracy as described in Section 3.4.2\n",
    "    Uses GPT-4 to check if generated answer matches ground truth\n",
    "    \"\"\"\n",
    "    correct_answers = 0\n",
    "    \n",
    "    # Initialize evaluator LLM (GPT-4 as mentioned in paper)\n",
    "    evaluator_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    \n",
    "    for test_q in test_questions:\n",
    "        # Generate answer using RAG\n",
    "        generated_answer = rag_pipeline.answer_question(test_q[\"question\"])\n",
    "        \n",
    "        # Evaluate using GPT-4\n",
    "        eval_prompt = f\"\"\"قارن بين الإجابتين التاليتين:\n",
    "        \n",
    "        السؤال: {test_q['question']}\n",
    "        الإجابة الصحيحة: {test_q['ground_truth']}\n",
    "        الإجابة المولدة: {generated_answer}\n",
    "        \n",
    "        هل الإجابة المولدة تحتوي على نفس المعلومات الأساسية؟ أجب بنعم أو لا فقط.\"\"\"\n",
    "        \n",
    "        evaluation = evaluator_llm.predict(eval_prompt)\n",
    "        \n",
    "        if \"نعم\" in evaluation:\n",
    "            correct_answers += 1\n",
    "    \n",
    "    return correct_answers / len(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo RAG pipeline\n",
    "print(\"Demonstrating Arabic RAG Pipeline...\")\n",
    "print(\"Note: Full implementation requires OpenAI API key for LLM calls\\n\")\n",
    "\n",
    "# Create RAG pipeline with best encoder (Encoder #3 from paper)\n",
    "# rag_pipeline = ArabicRAGPipeline(\n",
    "#     encoder_name=ENCODERS[\"encoder_3\"][\"model_name\"],\n",
    "#     faqs=faqs\n",
    "# )\n",
    "\n",
    "# Example query\n",
    "# test_question = \"كيف أستطيع تغيير كلمة السر الخاصة بي؟\"\n",
    "# answer = rag_pipeline.answer_question(test_question)\n",
    "# print(f\"Question: {test_question}\")\n",
    "# print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from the paper (Table 1)\n",
    "paper_results = pd.DataFrame({\n",
    "    'Model': ['Encoder #1', 'Encoder #2', 'Encoder #3', 'Encoder #4', 'Encoder #5'],\n",
    "    'NDCG@3': [0.853, 0.789, 0.879, 0.868, 0.837],\n",
    "    'MRR@3': [0.888, 0.798, 0.911, 0.890, 0.848],\n",
    "    'mAP@3': [0.863, 0.793, 0.888, 0.876, 0.854],\n",
    "    'Emb_Size': [384, 768, 768, 512, 768]\n",
    "})\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Metrics comparison\n",
    "metrics = ['NDCG@3', 'MRR@3', 'mAP@3']\n",
    "x = np.arange(len(paper_results))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0, 0].bar(x + i*width, paper_results[metric], width, label=metric)\n",
    "\n",
    "axes[0, 0].set_xlabel('Encoder')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_title('Semantic Search Metrics Comparison')\n",
    "axes[0, 0].set_xticks(x + width)\n",
    "axes[0, 0].set_xticklabels(paper_results['Model'])\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Embedding size vs performance\n",
    "axes[0, 1].scatter(paper_results['Emb_Size'], paper_results['NDCG@3'], s=100, alpha=0.6)\n",
    "for i, model in enumerate(paper_results['Model']):\n",
    "    axes[0, 1].annotate(model, (paper_results['Emb_Size'][i], paper_results['NDCG@3'][i]))\n",
    "axes[0, 1].set_xlabel('Embedding Size')\n",
    "axes[0, 1].set_ylabel('NDCG@3')\n",
    "axes[0, 1].set_title('Embedding Size vs Performance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: RAG results (Table 2)\n",
    "rag_results = pd.DataFrame({\n",
    "    'Encoder': ['Encoder #1', 'Encoder #2', 'Encoder #3', 'Encoder #4', 'Encoder #5'],\n",
    "    'Top_3_Accuracy': [59.31, 62.01, 63.11, 62.5, 57.84],\n",
    "    'Top_1_Accuracy': [61.15, 63.23, 63.84, 63.24, np.nan]\n",
    "})\n",
    "\n",
    "axes[1, 0].bar(rag_results['Encoder'], rag_results['Top_3_Accuracy'], alpha=0.7, label='Top 3')\n",
    "axes[1, 0].bar(rag_results['Encoder'], rag_results['Top_1_Accuracy'], alpha=0.7, label='Top 1')\n",
    "axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "axes[1, 0].set_title('RAG Accuracy with Different Encoders')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Correlation between Semantic Search and RAG\n",
    "axes[1, 1].scatter(paper_results['NDCG@3'], rag_results['Top_3_Accuracy'], s=100)\n",
    "for i, model in enumerate(paper_results['Model']):\n",
    "    axes[1, 1].annotate(model, \n",
    "                       (paper_results['NDCG@3'][i], rag_results['Top_3_Accuracy'][i]),\n",
    "                       fontsize=8)\n",
    "axes[1, 1].set_xlabel('NDCG@3 (Semantic Search)')\n",
    "axes[1, 1].set_ylabel('RAG Top-3 Accuracy (%)')\n",
    "axes[1, 1].set_title('Correlation: Semantic Search vs RAG Performance')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings & Insights\n",
    "\n",
    "### Paper's Main Conclusions:\n",
    "\n",
    "1. **Best Encoder**: Encoder #3 (paraphrase-multilingual-mpnet-base-v2) achieved best performance:\n",
    "   - NDCG@3: 0.879\n",
    "   - MRR@3: 0.911\n",
    "   - mAP@3: 0.888\n",
    "\n",
    "2. **Asymmetric vs Symmetric Search**: \n",
    "   - Encoder #1 performed well in asymmetric search (long docs, short queries)\n",
    "   - Encoder #2 better for symmetric search (similar length texts)\n",
    "\n",
    "3. **RAG Integration Benefits**:\n",
    "   - Shorter prompts (fewer tokens)\n",
    "   - More precise outcomes\n",
    "   - Cost-effective inference\n",
    "\n",
    "4. **Arabic Challenges**: Larger embedding sizes (768) generally performed better for Arabic due to language complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Template for Personal Research\n",
    "\n",
    "Use this section to extend the research with your own experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for testing new encoders\n",
    "def test_custom_encoder(encoder_name: str, documents: List[Document], queries: List[SearchQuery]):\n",
    "    \"\"\"\n",
    "    Template function to test your own encoder\n",
    "    \"\"\"\n",
    "    custom_config = {\n",
    "        \"model_name\": encoder_name,\n",
    "        \"embedding_size\": 768,  # Update based on your model\n",
    "        \"description\": \"Custom Encoder\"\n",
    "    }\n",
    "    \n",
    "    results = evaluate_encoder(custom_config, documents, queries)\n",
    "    return results\n",
    "\n",
    "# Example: Test a new Arabic-specific encoder\n",
    "# custom_results = test_custom_encoder(\"aubmindlab/bert-base-arabertv2\", documents, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for custom RAG experiments\n",
    "class CustomRAGExperiment:\n",
    "    \"\"\"\n",
    "    Extend this class for your own RAG experiments\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Add your initialization\n",
    "        pass\n",
    "    \n",
    "    def experiment_1_hybrid_search(self):\n",
    "        \"\"\"\n",
    "        Experiment: Combine semantic search with keyword search\n",
    "        \"\"\"\n",
    "        # Your implementation\n",
    "        pass\n",
    "    \n",
    "    def experiment_2_reranking(self):\n",
    "        \"\"\"\n",
    "        Experiment: Add reranking layer after retrieval\n",
    "        \"\"\"\n",
    "        # Your implementation\n",
    "        pass\n",
    "    \n",
    "    def experiment_3_cross_lingual(self):\n",
    "        \"\"\"\n",
    "        Experiment: Test cross-lingual retrieval (Arabic query, English docs)\n",
    "        \"\"\"\n",
    "        # Your implementation\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Mahboub, A., Za'ter, M. E., Al-Rfooh, B., Estaitia, Y., Jaljuli, A., & Hakouz, A. (2024). Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language. arXiv preprint arXiv:2403.18350v2.\n",
    "\n",
    "2. LangChain Documentation: https://python.langchain.com/\n",
    "\n",
    "3. Sentence Transformers: https://www.sbert.net/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}