{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-intro",
   "metadata": {},
   "source": [
    "# Deep Learning: Energy Efficiency Paradox trong DRL Compression\n",
    "\n",
    "## Mục tiêu học tập\n",
    "- Hiểu sâu về paradox: model size giảm nhưng energy efficiency không cải thiện\n",
    "- Phân tích relationship giữa model compression và energy consumption\n",
    "- Tìm hiểu về computational overhead và library implementation effects\n",
    "- Đo lường và phân tích energy consumption trong DRL models\n",
    "\n",
    "## Trích xuất từ Paper\n",
    "\n",
    "### Key Finding - Energy Efficiency Paradox\n",
    "```\n",
    "\"Pruning and quantization do not improve the energy efficiency and memory usage of DRL models. While pruning and quantization reduce model size, they do not necessarily enhance the energy efficiency of DRL models due to the maintained or increased average return.\"\n",
    "```\n",
    "\n",
    "### Detailed Analysis\n",
    "```\n",
    "\"Energy consumption tends to decrease only when there is a significant drop in average return, prompting the agent to terminate early and requiring less computation.\"\n",
    "```\n",
    "\n",
    "### Memory Usage Paradox\n",
    "```\n",
    "\"Despite reducing model size, quantization does not improve memory usage, and pruning yields only a negligible 1% decrease in memory usage. Results in Figure 1 present no changes in memory utilization in any platforms while applying quantization.\"\n",
    "```\n",
    "\n",
    "### Implementation Overhead\n",
    "```\n",
    "\"Even PTDQ and PTSQ cause more memory utilization than the baseline method. This might be due to the overhead of the quantization library, and the way it is implemented is not optimized.\"\n",
    "```\n",
    "\n",
    "### Core Paradox\n",
    "Paper conclusion: **Model compression ≠ Energy efficiency** trong DRL context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-section",
   "metadata": {},
   "source": [
    "## 1. Lý thuyết về Energy Efficiency Paradox\n",
    "\n",
    "### 1.1 Tại sao Model Size Reduction ≠ Energy Savings?\n",
    "\n",
    "**Traditional Assumption (WRONG for DRL):**\n",
    "- Smaller model → Fewer operations → Less energy\n",
    "- Linear relationship between model size và energy consumption\n",
    "\n",
    "**DRL Reality:**\n",
    "1. **Inference Frequency**: DRL agents make many sequential decisions\n",
    "2. **Environment Interaction**: Energy dominated by environment simulation\n",
    "3. **Episode Length**: Performance drop → shorter episodes → less computation\n",
    "4. **Library Overhead**: Compression libraries add computational overhead\n",
    "\n",
    "### 1.2 Components of Energy Consumption trong DRL\n",
    "\n",
    "**Total Energy = Model Inference + Environment + Overhead**\n",
    "\n",
    "1. **Model Inference Energy**: Neural network forward passes\n",
    "2. **Environment Energy**: Simulation, rendering, physics\n",
    "3. **Compression Overhead**: Quantization/dequantization operations\n",
    "4. **Memory Access Energy**: Data movement between CPU/GPU\n",
    "5. **Episode Length Effect**: Longer episodes → more total energy\n",
    "\n",
    "### 1.3 The Paradox Mechanisms\n",
    "\n",
    "**Mechanism 1: Performance-Energy Trade-off**\n",
    "```\n",
    "Better Performance → Longer Episodes → More Environment Steps → Higher Total Energy\n",
    "```\n",
    "\n",
    "**Mechanism 2: Compression Overhead**\n",
    "```\n",
    "Quantization → Runtime Dequantization → Additional Operations → Energy Overhead\n",
    "```\n",
    "\n",
    "**Mechanism 3: Memory Hierarchy Effects**\n",
    "```\n",
    "Compressed Model → Different Memory Access Patterns → Cache Misses → Energy Increase\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import threading\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import NVIDIA monitoring if available\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    NVIDIA_AVAILABLE = True\n",
    "    print(\"NVIDIA monitoring available\")\n",
    "except (ImportError, Exception):\n",
    "    NVIDIA_AVAILABLE = False\n",
    "    print(\"NVIDIA monitoring not available, using CPU-only monitoring\")\n",
    "\n",
    "# Visualization setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "energy-monitoring",
   "metadata": {},
   "source": [
    "## 2. Energy Monitoring Framework\n",
    "\n",
    "### 2.1 Multi-Level Energy Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "energy-monitor-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive energy monitoring system\n",
    "    \n",
    "    Tracks multiple energy components:\n",
    "    - CPU energy consumption\n",
    "    - GPU energy consumption (if available)\n",
    "    - Memory energy consumption\n",
    "    - Total system energy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sampling_interval: float = 0.1):\n",
    "        self.sampling_interval = sampling_interval\n",
    "        self.monitoring = False\n",
    "        self.energy_data = defaultdict(list)\n",
    "        self.timestamps = []\n",
    "        self.baseline_power = None\n",
    "        \n",
    "        # Initialize monitoring capabilities\n",
    "        self.cpu_available = True\n",
    "        self.gpu_available = NVIDIA_AVAILABLE and torch.cuda.is_available()\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            self.gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        \n",
    "        # Calibrate baseline power consumption\n",
    "        self._calibrate_baseline()\n",
    "    \n",
    "    def _calibrate_baseline(self, duration: float = 2.0):\n",
    "        \"\"\"\n",
    "        Calibrate baseline power consumption\n",
    "        \n",
    "        Paper insight: Need to separate computation from baseline consumption\n",
    "        \"\"\"\n",
    "        print(\"Calibrating baseline power consumption...\")\n",
    "        \n",
    "        baseline_samples = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            power_sample = self._get_instant_power()\n",
    "            baseline_samples.append(power_sample)\n",
    "            time.sleep(self.sampling_interval)\n",
    "        \n",
    "        self.baseline_power = {\n",
    "            'cpu_power': np.mean([s['cpu_power'] for s in baseline_samples]),\n",
    "            'memory_power': np.mean([s['memory_power'] for s in baseline_samples]),\n",
    "            'total_system_power': np.mean([s['total_system_power'] for s in baseline_samples])\n",
    "        }\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            self.baseline_power['gpu_power'] = np.mean([s['gpu_power'] for s in baseline_samples])\n",
    "        \n",
    "        print(f\"Baseline calibration completed: {self.baseline_power}\")\n",
    "    \n",
    "    def _get_instant_power(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get instantaneous power consumption\n",
    "        \n",
    "        Returns power in Watts (estimated)\n",
    "        \"\"\"\n",
    "        power_data = {}\n",
    "        \n",
    "        # CPU power estimation (based on utilization)\n",
    "        cpu_percent = psutil.cpu_percent(interval=None)\n",
    "        # Rough estimation: Modern CPU at 100% ≈ 65W, idle ≈ 5W\n",
    "        cpu_power = 5.0 + (cpu_percent / 100.0) * 60.0\n",
    "        power_data['cpu_power'] = cpu_power\n",
    "        \n",
    "        # Memory power estimation\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        memory_percent = memory_info.percent\n",
    "        # Rough estimation: DDR4 at full utilization ≈ 3W per DIMM\n",
    "        memory_power = 1.0 + (memory_percent / 100.0) * 2.0\n",
    "        power_data['memory_power'] = memory_power\n",
    "        \n",
    "        # GPU power (if available)\n",
    "        if self.gpu_available:\n",
    "            try:\n",
    "                # Get GPU power consumption in mW, convert to W\n",
    "                gpu_power_mw = pynvml.nvmlDeviceGetPowerUsage(self.gpu_handle)\n",
    "                gpu_power = gpu_power_mw / 1000.0\n",
    "                power_data['gpu_power'] = gpu_power\n",
    "            except:\n",
    "                # Fallback estimation based on utilization\n",
    "                gpu_util = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)\n",
    "                # Rough estimation: RTX 4090 at 100% ≈ 450W, idle ≈ 20W\n",
    "                gpu_power = 20.0 + (gpu_util.gpu / 100.0) * 430.0\n",
    "                power_data['gpu_power'] = gpu_power\n",
    "        else:\n",
    "            power_data['gpu_power'] = 0.0\n",
    "        \n",
    "        # Total system power\n",
    "        power_data['total_system_power'] = (\n",
    "            power_data['cpu_power'] + \n",
    "            power_data['memory_power'] + \n",
    "            power_data['gpu_power']\n",
    "        )\n",
    "        \n",
    "        return power_data\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"\n",
    "        Start energy monitoring\n",
    "        \"\"\"\n",
    "        if self.monitoring:\n",
    "            return\n",
    "        \n",
    "        self.monitoring = True\n",
    "        self.energy_data = defaultdict(list)\n",
    "        self.timestamps = []\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # Start monitoring thread\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop)\n",
    "        self.monitor_thread.daemon = True\n",
    "        self.monitor_thread.start()\n",
    "        \n",
    "        print(\"Energy monitoring started\")\n",
    "    \n",
    "    def stop_monitoring(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Stop energy monitoring and return results\n",
    "        \"\"\"\n",
    "        if not self.monitoring:\n",
    "            return {}\n",
    "        \n",
    "        self.monitoring = False\n",
    "        \n",
    "        # Wait for monitoring thread to finish\n",
    "        if hasattr(self, 'monitor_thread'):\n",
    "            self.monitor_thread.join(timeout=1.0)\n",
    "        \n",
    "        # Calculate energy consumption\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        energy_results = {\n",
    "            'duration_seconds': duration,\n",
    "            'total_energy_joules': {},\n",
    "            'average_power_watts': {},\n",
    "            'peak_power_watts': {},\n",
    "            'energy_efficiency_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Calculate energy consumption for each component\n",
    "        for component in ['cpu_power', 'memory_power', 'gpu_power', 'total_system_power']:\n",
    "            if component in self.energy_data:\n",
    "                power_samples = np.array(self.energy_data[component])\n",
    "                \n",
    "                # Remove baseline consumption\n",
    "                if self.baseline_power and component in self.baseline_power:\n",
    "                    net_power_samples = power_samples - self.baseline_power[component]\n",
    "                    net_power_samples = np.maximum(net_power_samples, 0)  # No negative power\n",
    "                else:\n",
    "                    net_power_samples = power_samples\n",
    "                \n",
    "                # Energy = Power × Time (Joules = Watts × Seconds)\n",
    "                energy_joules = np.trapz(net_power_samples, dx=self.sampling_interval)\n",
    "                \n",
    "                energy_results['total_energy_joules'][component] = energy_joules\n",
    "                energy_results['average_power_watts'][component] = np.mean(net_power_samples)\n",
    "                energy_results['peak_power_watts'][component] = np.max(net_power_samples)\n",
    "        \n",
    "        print(f\"Energy monitoring stopped. Duration: {duration:.2f}s\")\n",
    "        return energy_results\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"\n",
    "        Monitoring loop running in separate thread\n",
    "        \"\"\"\n",
    "        while self.monitoring:\n",
    "            timestamp = time.time() - self.start_time\n",
    "            power_data = self._get_instant_power()\n",
    "            \n",
    "            self.timestamps.append(timestamp)\n",
    "            for component, power in power_data.items():\n",
    "                self.energy_data[component].append(power)\n",
    "            \n",
    "            time.sleep(self.sampling_interval)\n",
    "    \n",
    "    def get_monitoring_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get current monitoring data (for real-time visualization)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'timestamps': self.timestamps.copy(),\n",
    "            'energy_data': dict(self.energy_data),\n",
    "            'baseline_power': self.baseline_power\n",
    "        }\n",
    "    \n",
    "    def visualize_energy_consumption(self, results: Dict[str, Any], title: str = \"Energy Consumption Analysis\"):\n",
    "        \"\"\"\n",
    "        Visualize energy consumption results\n",
    "        \"\"\"\n",
    "        if not results or 'total_energy_joules' not in results:\n",
    "            print(\"No energy data to visualize\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'{title}\\nDuration: {results[\"duration_seconds\"]:.2f}s', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Energy consumption by component\n",
    "        components = list(results['total_energy_joules'].keys())\n",
    "        energy_values = list(results['total_energy_joules'].values())\n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'orange'][:len(components)]\n",
    "        bars = axes[0, 0].bar(components, energy_values, color=colors, alpha=0.7)\n",
    "        axes[0, 0].set_title('Total Energy Consumption by Component')\n",
    "        axes[0, 0].set_ylabel('Energy (Joules)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, energy_values):\n",
    "            height = bar.get_height()\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + max(energy_values)*0.01,\n",
    "                           f'{value:.2f}J', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 2: Average power consumption\n",
    "        avg_power_values = list(results['average_power_watts'].values())\n",
    "        axes[0, 1].bar(components, avg_power_values, color=colors, alpha=0.7)\n",
    "        axes[0, 1].set_title('Average Power Consumption')\n",
    "        axes[0, 1].set_ylabel('Power (Watts)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 3: Power consumption over time (if monitoring data available)\n",
    "        monitoring_data = self.get_monitoring_data()\n",
    "        if monitoring_data['timestamps']:\n",
    "            timestamps = monitoring_data['timestamps']\n",
    "            total_power = monitoring_data['energy_data'].get('total_system_power', [])\n",
    "            \n",
    "            if total_power:\n",
    "                axes[1, 0].plot(timestamps, total_power, 'b-', linewidth=2, label='Total Power')\n",
    "                if self.baseline_power:\n",
    "                    baseline = self.baseline_power['total_system_power']\n",
    "                    axes[1, 0].axhline(y=baseline, color='red', linestyle='--', \n",
    "                                     label=f'Baseline ({baseline:.1f}W)')\n",
    "                axes[1, 0].set_title('Power Consumption Over Time')\n",
    "                axes[1, 0].set_xlabel('Time (seconds)')\n",
    "                axes[1, 0].set_ylabel('Power (Watts)')\n",
    "                axes[1, 0].legend()\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No time-series data\\navailable', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('Power Over Time')\n",
    "        \n",
    "        # Plot 4: Energy efficiency metrics\n",
    "        total_energy = results['total_energy_joules'].get('total_system_power', 0)\n",
    "        duration = results['duration_seconds']\n",
    "        avg_power = results['average_power_watts'].get('total_system_power', 0)\n",
    "        \n",
    "        metrics_text = f\"\"\"Energy Efficiency Metrics:\n",
    "\n",
    "Total Energy: {total_energy:.2f} Joules\n",
    "Duration: {duration:.2f} seconds\n",
    "Average Power: {avg_power:.2f} Watts\n",
    "Energy per Second: {total_energy/duration:.2f} J/s\n",
    "\n",
    "Component Breakdown:\n",
    "CPU: {results['total_energy_joules'].get('cpu_power', 0):.2f}J\n",
    "Memory: {results['total_energy_joules'].get('memory_power', 0):.2f}J\n",
    "GPU: {results['total_energy_joules'].get('gpu_power', 0):.2f}J\n",
    "\n",
    "Baseline Power: {self.baseline_power['total_system_power']:.1f}W\"\"\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.9, metrics_text, transform=axes[1, 1].transAxes, \n",
    "                        fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "        axes[1, 1].set_title('Energy Efficiency Summary')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"Energy Monitor implementation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drl-simulation",
   "metadata": {},
   "source": [
    "## 3. DRL Simulation Framework\n",
    "\n",
    "### 3.1 Mock Environment để test Energy Paradox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drl-env-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockDRLEnvironment:\n",
    "    \"\"\"\n",
    "    Mock DRL environment để test energy efficiency paradox\n",
    "    \n",
    "    Simulates:\n",
    "    - Variable episode lengths based on performance\n",
    "    - Environment computational load\n",
    "    - Reward structure affecting episode duration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int = 64, action_dim: int = 8, \n",
    "                 max_episode_length: int = 1000, \n",
    "                 environment_complexity: float = 1.0):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.environment_complexity = environment_complexity\n",
    "        \n",
    "        # Episode state\n",
    "        self.current_step = 0\n",
    "        self.current_state = None\n",
    "        self.episode_reward = 0.0\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.episode_history = []\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reset environment to initial state\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.episode_reward = 0.0\n",
    "        self.current_state = torch.randn(self.state_dim)\n",
    "        return self.current_state.clone()\n",
    "    \n",
    "    def step(self, action: torch.Tensor) -> Tuple[torch.Tensor, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Execute action in environment\n",
    "        \n",
    "        Paper insight: Episode length affects total energy consumption\n",
    "        Better performing agents → longer episodes → more energy\n",
    "        \"\"\"\n",
    "        # Simulate environment computation (computational load)\n",
    "        self._simulate_environment_computation()\n",
    "        \n",
    "        # Calculate reward based on action quality\n",
    "        action_quality = self._evaluate_action_quality(action)\n",
    "        reward = action_quality\n",
    "        \n",
    "        # Update state\n",
    "        self.current_state = self._next_state(self.current_state, action)\n",
    "        self.current_step += 1\n",
    "        self.episode_reward += reward\n",
    "        \n",
    "        # Determine if episode is done\n",
    "        # Paper insight: Poor performance → early termination → less energy\n",
    "        done = self._is_episode_done(action_quality)\n",
    "        \n",
    "        info = {\n",
    "            'episode_step': self.current_step,\n",
    "            'episode_reward': self.episode_reward,\n",
    "            'action_quality': action_quality\n",
    "        }\n",
    "        \n",
    "        if done:\n",
    "            self.episode_history.append({\n",
    "                'length': self.current_step,\n",
    "                'total_reward': self.episode_reward,\n",
    "                'average_reward': self.episode_reward / self.current_step\n",
    "            })\n",
    "        \n",
    "        return self.current_state.clone(), reward, done, info\n",
    "    \n",
    "    def _simulate_environment_computation(self):\n",
    "        \"\"\"\n",
    "        Simulate computational load of environment\n",
    "        \n",
    "        Paper insight: Environment computation often dominates model inference\n",
    "        \"\"\"\n",
    "        # Simulate physics calculation, rendering, etc.\n",
    "        computation_load = int(self.environment_complexity * 1000)\n",
    "        \n",
    "        # Dummy computation to consume CPU cycles\n",
    "        dummy_matrix = torch.randn(computation_load, 10)\n",
    "        _ = torch.sum(dummy_matrix ** 2)\n",
    "    \n",
    "    def _evaluate_action_quality(self, action: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate quality of action (higher is better)\n",
    "        \n",
    "        Simulates environment response to agent actions\n",
    "        \"\"\"\n",
    "        # Simple quality metric: prefer actions close to optimal\n",
    "        optimal_action = torch.tanh(self.current_state[:self.action_dim])\n",
    "        action_distance = torch.norm(action - optimal_action)\n",
    "        \n",
    "        # Convert distance to reward (lower distance = higher reward)\n",
    "        quality = 1.0 / (1.0 + action_distance.item())\n",
    "        \n",
    "        # Add some noise\n",
    "        quality += 0.1 * torch.randn(1).item()\n",
    "        \n",
    "        return max(0.0, min(1.0, quality))\n",
    "    \n",
    "    def _next_state(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute next state\n",
    "        \"\"\"\n",
    "        # Simple state transition\n",
    "        next_state = 0.9 * state + 0.1 * torch.randn_like(state)\n",
    "        \n",
    "        # Action influence on state\n",
    "        action_effect = torch.cat([action, torch.zeros(self.state_dim - self.action_dim)])\n",
    "        next_state += 0.05 * action_effect\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def _is_episode_done(self, action_quality: float) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if episode should terminate\n",
    "        \n",
    "        Paper insight: Poor performance leads to early termination\n",
    "        \"\"\"\n",
    "        # Maximum episode length\n",
    "        if self.current_step >= self.max_episode_length:\n",
    "            return True\n",
    "        \n",
    "        # Early termination for very poor performance\n",
    "        if action_quality < 0.1 and self.current_step > 50:\n",
    "            return True\n",
    "        \n",
    "        # Random termination (small probability)\n",
    "        if torch.rand(1).item() < 0.001:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_episode_statistics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get statistics about episode performance\n",
    "        \"\"\"\n",
    "        if not self.episode_history:\n",
    "            return {}\n",
    "        \n",
    "        episode_lengths = [ep['length'] for ep in self.episode_history]\n",
    "        episode_rewards = [ep['total_reward'] for ep in self.episode_history]\n",
    "        \n",
    "        return {\n",
    "            'num_episodes': len(self.episode_history),\n",
    "            'average_episode_length': np.mean(episode_lengths),\n",
    "            'std_episode_length': np.std(episode_lengths),\n",
    "            'average_episode_reward': np.mean(episode_rewards),\n",
    "            'std_episode_reward': np.std(episode_rewards),\n",
    "            'total_steps': sum(episode_lengths),\n",
    "            'total_reward': sum(episode_rewards)\n",
    "        }\n",
    "\n",
    "class DRLAgent:\n",
    "    \"\"\"\n",
    "    Simple DRL agent để test energy consumption\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 64]):\n",
    "        # Policy network\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        self.policy = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for module in self.policy.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, gain=0.1)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "    \n",
    "    def act(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Select action based on current state\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if state.dim() == 1:\n",
    "                state = state.unsqueeze(0)\n",
    "            action = self.policy(state)\n",
    "            return action.squeeze(0)\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get model information\n",
    "        \"\"\"\n",
    "        total_params = sum(p.numel() for p in self.policy.parameters())\n",
    "        model_size_mb = total_params * 4 / 1024 / 1024  # float32\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'model_size_mb': model_size_mb,\n",
    "            'architecture': str(self.policy)\n",
    "        }\n",
    "\n",
    "print(\"DRL simulation framework implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compression-framework",
   "metadata": {},
   "source": [
    "## 4. Model Compression Framework\n",
    "\n",
    "### 4.1 Simplified Compression Methods để test Paradox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compression-framework-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressionFramework:\n",
    "    \"\"\"\n",
    "    Framework để test energy efficiency paradox với different compression methods\n",
    "    \n",
    "    Paper insight: Compression reduces model size but may not improve energy efficiency\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.compression_methods = {\n",
    "            'quantization': self._apply_quantization,\n",
    "            'pruning': self._apply_pruning,\n",
    "            'mixed': self._apply_mixed_compression\n",
    "        }\n",
    "    \n",
    "    def _apply_quantization(self, model: nn.Module, intensity: float = 0.5) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Apply quantization compression\n",
    "        \n",
    "        Paper: \"quantization does not improve memory usage\"\n",
    "        Simulates overhead of quantization operations\n",
    "        \"\"\"\n",
    "        compressed_model = copy.deepcopy(model)\n",
    "        \n",
    "        # Simulate quantization by adding computational overhead\n",
    "        class QuantizedLinear(nn.Module):\n",
    "            def __init__(self, original_linear: nn.Linear, quantization_overhead: float = 0.1):\n",
    "                super().__init__()\n",
    "                self.weight = original_linear.weight\n",
    "                self.bias = original_linear.bias\n",
    "                self.quantization_overhead = quantization_overhead\n",
    "                \n",
    "                # Simulate weight quantization (reduce precision)\n",
    "                with torch.no_grad():\n",
    "                    # Quantize to 8-bit range, then dequantize\n",
    "                    w_min, w_max = self.weight.min(), self.weight.max()\n",
    "                    scale = (w_max - w_min) / 255.0\n",
    "                    quantized = torch.round((self.weight - w_min) / scale)\n",
    "                    dequantized = quantized * scale + w_min\n",
    "                    self.weight.copy_(dequantized)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Original computation\n",
    "                output = F.linear(x, self.weight, self.bias)\n",
    "                \n",
    "                # Simulate quantization overhead (additional operations)\n",
    "                overhead_ops = int(self.quantization_overhead * x.numel())\n",
    "                if overhead_ops > 0:\n",
    "                    dummy_tensor = torch.randn(overhead_ops, device=x.device)\n",
    "                    _ = torch.sum(dummy_tensor)  # Dummy computation\n",
    "                \n",
    "                return output\n",
    "        \n",
    "        # Replace Linear layers with QuantizedLinear\n",
    "        for name, module in compressed_model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Get parent module\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                \n",
    "                if parent_name:\n",
    "                    parent = compressed_model\n",
    "                    for part in parent_name.split('.'):\n",
    "                        parent = getattr(parent, part)\n",
    "                    setattr(parent, child_name, QuantizedLinear(module, intensity))\n",
    "                else:\n",
    "                    setattr(compressed_model, child_name, QuantizedLinear(module, intensity))\n",
    "        \n",
    "        return compressed_model\n",
    "    \n",
    "    def _apply_pruning(self, model: nn.Module, intensity: float = 0.5) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Apply pruning compression\n",
    "        \n",
    "        Paper: \"pruning yields only a negligible 1% decrease in memory usage\"\n",
    "        Simulates sparse operations overhead\n",
    "        \"\"\"\n",
    "        compressed_model = copy.deepcopy(model)\n",
    "        \n",
    "        class PrunedLinear(nn.Module):\n",
    "            def __init__(self, original_linear: nn.Linear, sparsity: float = 0.5):\n",
    "                super().__init__()\n",
    "                self.original_weight = original_linear.weight.clone()\n",
    "                self.bias = original_linear.bias\n",
    "                self.sparsity = sparsity\n",
    "                \n",
    "                # Create pruning mask\n",
    "                weight_abs = torch.abs(self.original_weight)\n",
    "                threshold = torch.quantile(weight_abs.flatten(), sparsity)\n",
    "                self.mask = (weight_abs > threshold).float()\n",
    "                \n",
    "                # Apply pruning\n",
    "                self.weight = nn.Parameter(self.original_weight * self.mask)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Sparse computation (simplified)\n",
    "                output = F.linear(x, self.weight, self.bias)\n",
    "                \n",
    "                # Simulate sparse operation overhead\n",
    "                # Real sparse ops often have overhead due to irregular memory access\n",
    "                overhead_factor = 1.0 + (self.sparsity * 0.1)  # 10% overhead at max sparsity\n",
    "                if overhead_factor > 1.0:\n",
    "                    dummy_ops = int((overhead_factor - 1.0) * x.numel())\n",
    "                    if dummy_ops > 0:\n",
    "                        dummy_tensor = torch.randn(dummy_ops, device=x.device)\n",
    "                        _ = torch.sum(dummy_tensor)\n",
    "                \n",
    "                return output\n",
    "            \n",
    "            def get_sparsity(self) -> float:\n",
    "                return (self.mask == 0).float().mean().item()\n",
    "        \n",
    "        # Replace Linear layers with PrunedLinear\n",
    "        for name, module in compressed_model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                child_name = name.split('.')[-1]\n",
    "                \n",
    "                if parent_name:\n",
    "                    parent = compressed_model\n",
    "                    for part in parent_name.split('.'):\n",
    "                        parent = getattr(parent, part)\n",
    "                    setattr(parent, child_name, PrunedLinear(module, intensity))\n",
    "                else:\n",
    "                    setattr(compressed_model, child_name, PrunedLinear(module, intensity))\n",
    "        \n",
    "        return compressed_model\n",
    "    \n",
    "    def _apply_mixed_compression(self, model: nn.Module, intensity: float = 0.5) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Apply both quantization and pruning\n",
    "        \n",
    "        Tests combined effect of multiple compression methods\n",
    "        \"\"\"\n",
    "        # Apply pruning first\n",
    "        pruned_model = self._apply_pruning(model, intensity * 0.7)\n",
    "        \n",
    "        # Then apply quantization\n",
    "        compressed_model = self._apply_quantization(pruned_model, intensity * 0.3)\n",
    "        \n",
    "        return compressed_model\n",
    "    \n",
    "    def compress_model(self, model: nn.Module, method: str, intensity: float = 0.5) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Apply compression method to model\n",
    "        \n",
    "        Args:\n",
    "            model: Original model\n",
    "            method: Compression method ('quantization', 'pruning', 'mixed')\n",
    "            intensity: Compression intensity (0.0 to 1.0)\n",
    "        \n",
    "        Returns:\n",
    "            Compressed model\n",
    "        \"\"\"\n",
    "        if method not in self.compression_methods:\n",
    "            raise ValueError(f\"Unknown compression method: {method}\")\n",
    "        \n",
    "        return self.compression_methods[method](model, intensity)\n",
    "    \n",
    "    def get_compression_stats(self, original_model: nn.Module, compressed_model: nn.Module) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate compression statistics\n",
    "        \"\"\"\n",
    "        original_params = sum(p.numel() for p in original_model.parameters())\n",
    "        compressed_params = sum(p.numel() for p in compressed_model.parameters())\n",
    "        \n",
    "        # Calculate actual sparsity for pruned models\n",
    "        total_zeros = 0\n",
    "        total_elements = 0\n",
    "        \n",
    "        for param in compressed_model.parameters():\n",
    "            total_zeros += (param.data == 0).sum().item()\n",
    "            total_elements += param.numel()\n",
    "        \n",
    "        sparsity = total_zeros / total_elements if total_elements > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'original_parameters': original_params,\n",
    "            'compressed_parameters': compressed_params,\n",
    "            'parameter_reduction': 1 - (compressed_params / original_params),\n",
    "            'sparsity': sparsity,\n",
    "            'compression_ratio': original_params / compressed_params if compressed_params > 0 else 1.0\n",
    "        }\n",
    "\n",
    "print(\"Compression framework implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "energy-experiment",
   "metadata": {},
   "source": [
    "## 5. Energy Efficiency Paradox Experiment\n",
    "\n",
    "### 5.1 Comprehensive Energy vs Compression Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "energy-paradox-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyParadoxExperiment:\n",
    "    \"\"\"\n",
    "    Comprehensive experiment để validate energy efficiency paradox\n",
    "    \n",
    "    Tests:\n",
    "    1. Model compression vs energy consumption\n",
    "    2. Performance vs episode length vs total energy\n",
    "    3. Compression overhead effects\n",
    "    4. Memory usage paradox\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.energy_monitor = EnergyMonitor(sampling_interval=0.05)\n",
    "        self.compression_framework = CompressionFramework()\n",
    "        self.results = []\n",
    "    \n",
    "    def run_single_test(self, model: nn.Module, environment: MockDRLEnvironment, \n",
    "                       num_episodes: int = 20, test_name: str = \"Test\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run single test with energy monitoring\n",
    "        \n",
    "        Paper metrics: average return, inference time, energy usage\n",
    "        \"\"\"\n",
    "        print(f\"Running {test_name} with {num_episodes} episodes...\")\n",
    "        \n",
    "        # Create agent\n",
    "        agent = DRLAgent(environment.state_dim, environment.action_dim)\n",
    "        agent.policy = model\n",
    "        \n",
    "        # Start energy monitoring\n",
    "        self.energy_monitor.start_monitoring()\n",
    "        \n",
    "        # Run episodes\n",
    "        start_time = time.time()\n",
    "        total_inference_time = 0.0\n",
    "        total_environment_time = 0.0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = environment.reset()\n",
    "            episode_done = False\n",
    "            \n",
    "            while not episode_done:\n",
    "                # Model inference (timed)\n",
    "                inference_start = time.time()\n",
    "                action = agent.act(state)\n",
    "                inference_time = time.time() - inference_start\n",
    "                total_inference_time += inference_time\n",
    "                \n",
    "                # Environment step (timed)\n",
    "                env_start = time.time()\n",
    "                next_state, reward, episode_done, info = environment.step(action)\n",
    "                env_time = time.time() - env_start\n",
    "                total_environment_time += env_time\n",
    "                \n",
    "                state = next_state\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Stop energy monitoring\n",
    "        energy_results = self.energy_monitor.stop_monitoring()\n",
    "        \n",
    "        # Get episode statistics\n",
    "        episode_stats = environment.get_episode_statistics()\n",
    "        \n",
    "        # Model information\n",
    "        model_info = agent.get_model_info()\n",
    "        \n",
    "        # Compile results\n",
    "        test_results = {\n",
    "            'test_name': test_name,\n",
    "            'model_info': model_info,\n",
    "            'episode_stats': episode_stats,\n",
    "            'energy_results': energy_results,\n",
    "            'timing': {\n",
    "                'total_time': total_time,\n",
    "                'total_inference_time': total_inference_time,\n",
    "                'total_environment_time': total_environment_time,\n",
    "                'inference_percentage': (total_inference_time / total_time) * 100,\n",
    "                'environment_percentage': (total_environment_time / total_time) * 100\n",
    "            },\n",
    "            'efficiency_metrics': self._calculate_efficiency_metrics(\n",
    "                energy_results, episode_stats, model_info\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        print(f\"{test_name} completed:\")\n",
    "        print(f\"  Episodes: {episode_stats.get('num_episodes', 0)}\")\n",
    "        print(f\"  Avg episode length: {episode_stats.get('average_episode_length', 0):.1f}\")\n",
    "        print(f\"  Total energy: {energy_results.get('total_energy_joules', {}).get('total_system_power', 0):.2f}J\")\n",
    "        print(f\"  Model size: {model_info['model_size_mb']:.2f}MB\")\n",
    "        \n",
    "        return test_results\n",
    "    \n",
    "    def _calculate_efficiency_metrics(self, energy_results: Dict, episode_stats: Dict, \n",
    "                                    model_info: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate energy efficiency metrics\n",
    "        \n",
    "        Paper insight: \"Energy efficiency\" should account for performance\n",
    "        \"\"\"\n",
    "        total_energy = energy_results.get('total_energy_joules', {}).get('total_system_power', 0)\n",
    "        total_steps = episode_stats.get('total_steps', 1)\n",
    "        total_reward = episode_stats.get('total_reward', 0)\n",
    "        model_size = model_info['model_size_mb']\n",
    "        \n",
    "        return {\n",
    "            'energy_per_step': total_energy / total_steps,\n",
    "            'energy_per_reward': total_energy / (total_reward + 1e-8),\n",
    "            'energy_per_mb': total_energy / (model_size + 1e-8),\n",
    "            'steps_per_joule': total_steps / (total_energy + 1e-8),\n",
    "            'reward_per_joule': total_reward / (total_energy + 1e-8),\n",
    "            'energy_efficiency_score': (total_reward * total_steps) / (total_energy + 1e-8)\n",
    "        }\n",
    "    \n",
    "    def run_compression_comparison(self, base_model: nn.Module, \n",
    "                                 environment: MockDRLEnvironment,\n",
    "                                 num_episodes: int = 15) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Run comprehensive comparison of compression methods\n",
    "        \n",
    "        Tests paper hypothesis: compression ≠ energy efficiency\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENERGY EFFICIENCY PARADOX EXPERIMENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        experiment_results = []\n",
    "        \n",
    "        # Test 1: Baseline (no compression)\n",
    "        baseline_results = self.run_single_test(\n",
    "            base_model, environment, num_episodes, \"Baseline (No Compression)\"\n",
    "        )\n",
    "        experiment_results.append(baseline_results)\n",
    "        \n",
    "        # Test 2-4: Different compression methods\n",
    "        compression_methods = [\n",
    "            ('quantization', 'Quantization (Medium)'),\n",
    "            ('pruning', 'Pruning (Medium)'), \n",
    "            ('mixed', 'Mixed Compression')\n",
    "        ]\n",
    "        \n",
    "        for method, test_name in compression_methods:\n",
    "            print(f\"\\nApplying {method} compression...\")\n",
    "            \n",
    "            # Apply compression\n",
    "            compressed_model = self.compression_framework.compress_model(\n",
    "                base_model, method, intensity=0.5\n",
    "            )\n",
    "            \n",
    "            # Get compression stats\n",
    "            compression_stats = self.compression_framework.get_compression_stats(\n",
    "                base_model, compressed_model\n",
    "            )\n",
    "            \n",
    "            # Run test\n",
    "            compressed_results = self.run_single_test(\n",
    "                compressed_model, environment, num_episodes, test_name\n",
    "            )\n",
    "            \n",
    "            # Add compression stats\n",
    "            compressed_results['compression_stats'] = compression_stats\n",
    "            \n",
    "            experiment_results.append(compressed_results)\n",
    "        \n",
    "        # Test 5: High compression (to test performance drop effect)\n",
    "        print(\"\\nApplying high intensity mixed compression...\")\n",
    "        high_compression_model = self.compression_framework.compress_model(\n",
    "            base_model, 'mixed', intensity=0.8\n",
    "        )\n",
    "        \n",
    "        high_compression_stats = self.compression_framework.get_compression_stats(\n",
    "            base_model, high_compression_model\n",
    "        )\n",
    "        \n",
    "        high_compression_results = self.run_single_test(\n",
    "            high_compression_model, environment, num_episodes, \"High Compression\"\n",
    "        )\n",
    "        high_compression_results['compression_stats'] = high_compression_stats\n",
    "        experiment_results.append(high_compression_results)\n",
    "        \n",
    "        # Store results\n",
    "        self.results = experiment_results\n",
    "        \n",
    "        return experiment_results\n",
    "    \n",
    "    def analyze_paradox(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze results to validate energy efficiency paradox\n",
    "        \n",
    "        Paper findings to validate:\n",
    "        1. Compression reduces model size but doesn't improve energy efficiency\n",
    "        2. Energy decreases only with significant performance drop\n",
    "        3. Library overhead affects energy consumption\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PARADOX ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        analysis = {\n",
    "            'model_size_vs_energy': [],\n",
    "            'performance_vs_energy': [], \n",
    "            'compression_overhead': [],\n",
    "            'paradox_validation': {}\n",
    "        }\n",
    "        \n",
    "        baseline = results[0]  # First result is baseline\n",
    "        baseline_energy = baseline['energy_results']['total_energy_joules']['total_system_power']\n",
    "        baseline_size = baseline['model_info']['model_size_mb']\n",
    "        baseline_performance = baseline['episode_stats']['average_episode_reward']\n",
    "        \n",
    "        print(f\"\\nBaseline metrics:\")\n",
    "        print(f\"  Energy: {baseline_energy:.2f}J\")\n",
    "        print(f\"  Model size: {baseline_size:.2f}MB\")\n",
    "        print(f\"  Performance: {baseline_performance:.3f}\")\n",
    "        \n",
    "        for result in results[1:]:  # Skip baseline\n",
    "            energy = result['energy_results']['total_energy_joules']['total_system_power']\n",
    "            size = result['model_info']['model_size_mb']\n",
    "            performance = result['episode_stats']['average_episode_reward']\n",
    "            \n",
    "            # Calculate relative changes\n",
    "            size_reduction = (baseline_size - size) / baseline_size\n",
    "            energy_change = (energy - baseline_energy) / baseline_energy\n",
    "            performance_change = (performance - baseline_performance) / baseline_performance\n",
    "            \n",
    "            if 'compression_stats' in result:\n",
    "                compression_ratio = result['compression_stats']['compression_ratio']\n",
    "                sparsity = result['compression_stats']['sparsity']\n",
    "            else:\n",
    "                compression_ratio = 1.0\n",
    "                sparsity = 0.0\n",
    "            \n",
    "            analysis['model_size_vs_energy'].append({\n",
    "                'test_name': result['test_name'],\n",
    "                'size_reduction': size_reduction,\n",
    "                'energy_change': energy_change,\n",
    "                'compression_ratio': compression_ratio\n",
    "            })\n",
    "            \n",
    "            analysis['performance_vs_energy'].append({\n",
    "                'test_name': result['test_name'],\n",
    "                'performance_change': performance_change,\n",
    "                'energy_change': energy_change,\n",
    "                'avg_episode_length': result['episode_stats']['average_episode_length']\n",
    "            })\n",
    "            \n",
    "            # Analyze compression overhead\n",
    "            inference_time = result['timing']['total_inference_time']\n",
    "            baseline_inference = baseline['timing']['total_inference_time']\n",
    "            inference_change = (inference_time - baseline_inference) / baseline_inference\n",
    "            \n",
    "            analysis['compression_overhead'].append({\n",
    "                'test_name': result['test_name'],\n",
    "                'inference_time_change': inference_change,\n",
    "                'energy_change': energy_change,\n",
    "                'sparsity': sparsity\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{result['test_name']}:\")\n",
    "            print(f\"  Size reduction: {size_reduction*100:.1f}%\")\n",
    "            print(f\"  Energy change: {energy_change*100:+.1f}%\")\n",
    "            print(f\"  Performance change: {performance_change*100:+.1f}%\")\n",
    "            print(f\"  Inference time change: {inference_change*100:+.1f}%\")\n",
    "        \n",
    "        # Validate paradox findings\n",
    "        paradox_validation = self._validate_paradox_findings(analysis)\n",
    "        analysis['paradox_validation'] = paradox_validation\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _validate_paradox_findings(self, analysis: Dict) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Validate specific paper findings about the paradox\n",
    "        \"\"\"\n",
    "        validation = {}\n",
    "        \n",
    "        # Finding 1: Model size reduction doesn't improve energy efficiency\n",
    "        size_energy_data = analysis['model_size_vs_energy']\n",
    "        models_with_size_reduction = [d for d in size_energy_data if d['size_reduction'] > 0]\n",
    "        models_with_energy_increase = [d for d in models_with_size_reduction if d['energy_change'] > 0]\n",
    "        \n",
    "        validation['size_reduction_no_energy_improvement'] = (\n",
    "            len(models_with_energy_increase) >= len(models_with_size_reduction) * 0.5\n",
    "        )\n",
    "        \n",
    "        # Finding 2: Performance drop leads to energy reduction\n",
    "        perf_energy_data = analysis['performance_vs_energy']\n",
    "        models_with_perf_drop = [d for d in perf_energy_data if d['performance_change'] < -0.1]\n",
    "        models_with_energy_drop = [d for d in models_with_perf_drop if d['energy_change'] < 0]\n",
    "        \n",
    "        validation['performance_drop_energy_reduction'] = (\n",
    "            len(models_with_energy_drop) > 0 and \n",
    "            len(models_with_energy_drop) >= len(models_with_perf_drop) * 0.5\n",
    "        )\n",
    "        \n",
    "        # Finding 3: Compression overhead affects inference time\n",
    "        overhead_data = analysis['compression_overhead']\n",
    "        models_with_inference_overhead = [d for d in overhead_data if d['inference_time_change'] > 0]\n",
    "        \n",
    "        validation['compression_overhead_exists'] = len(models_with_inference_overhead) > 0\n",
    "        \n",
    "        return validation\n",
    "\n",
    "print(\"Energy Paradox Experiment framework implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-experiment",
   "metadata": {},
   "source": [
    "## 6. Run Energy Efficiency Paradox Experiment\n",
    "\n",
    "### 6.1 Comprehensive Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-energy-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive energy efficiency paradox experiment\n",
    "print(\"Starting Energy Efficiency Paradox Experiment...\")\n",
    "print(\"This experiment will validate the paper's key finding:\")\n",
    "print(\"'Compression reduces model size but does NOT improve energy efficiency'\")\n",
    "\n",
    "# Create base model and environment\n",
    "base_model = nn.Sequential(\n",
    "    nn.Linear(64, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 8),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "for module in base_model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.orthogonal_(module.weight, gain=0.1)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.fill_(0.0)\n",
    "\n",
    "# Create environment with higher complexity to emphasize the paradox\n",
    "environment = MockDRLEnvironment(\n",
    "    state_dim=64, \n",
    "    action_dim=8, \n",
    "    max_episode_length=500,\n",
    "    environment_complexity=2.0  # Higher complexity = more environment computation\n",
    ")\n",
    "\n",
    "print(f\"\\nExperimental setup:\")\n",
    "print(f\"Base model: {sum(p.numel() for p in base_model.parameters()):,} parameters\")\n",
    "print(f\"Environment: {environment.state_dim}D state, {environment.action_dim}D action\")\n",
    "print(f\"Max episode length: {environment.max_episode_length}\")\n",
    "print(f\"Environment complexity: {environment.environment_complexity}x\")\n",
    "\n",
    "# Create experiment instance\n",
    "experiment = EnergyParadoxExperiment()\n",
    "\n",
    "# Run the experiment\n",
    "results = experiment.run_compression_comparison(\n",
    "    base_model, environment, num_episodes=12  # Reduced for faster execution\n",
    ")\n",
    "\n",
    "print(f\"\\nExperiment completed! Collected {len(results)} test results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze-results",
   "metadata": {},
   "source": [
    "### 6.2 Analyze Energy Efficiency Paradox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-paradox-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the paradox\n",
    "analysis = experiment.analyze_paradox(results)\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED PARADOX ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Model Size vs Energy Consumption:\")\n",
    "for item in analysis['model_size_vs_energy']:\n",
    "    print(f\"  {item['test_name']}:\")\n",
    "    print(f\"    Size reduction: {item['size_reduction']*100:.1f}%\")\n",
    "    print(f\"    Energy change: {item['energy_change']*100:+.1f}%\")\n",
    "    print(f\"    Compression ratio: {item['compression_ratio']:.2f}x\")\n",
    "\n",
    "print(\"\\n2. Performance vs Energy Consumption:\")\n",
    "for item in analysis['performance_vs_energy']:\n",
    "    print(f\"  {item['test_name']}:\")\n",
    "    print(f\"    Performance change: {item['performance_change']*100:+.1f}%\")\n",
    "    print(f\"    Energy change: {item['energy_change']*100:+.1f}%\")\n",
    "    print(f\"    Avg episode length: {item['avg_episode_length']:.1f}\")\n",
    "\n",
    "print(\"\\n3. Compression Overhead Analysis:\")\n",
    "for item in analysis['compression_overhead']:\n",
    "    print(f\"  {item['test_name']}:\")\n",
    "    print(f\"    Inference time change: {item['inference_time_change']*100:+.1f}%\")\n",
    "    print(f\"    Energy change: {item['energy_change']*100:+.1f}%\")\n",
    "    print(f\"    Sparsity: {item['sparsity']*100:.1f}%\")\n",
    "\n",
    "# Validate paper findings\n",
    "validation = analysis['paradox_validation']\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PAPER FINDINGS VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "findings = [\n",
    "    (\"Model size reduction ≠ Energy improvement\", validation['size_reduction_no_energy_improvement']),\n",
    "    (\"Performance drop → Energy reduction\", validation['performance_drop_energy_reduction']),\n",
    "    (\"Compression overhead exists\", validation['compression_overhead_exists'])\n",
    "]\n",
    "\n",
    "for finding, validated in findings:\n",
    "    status = \"✓ VALIDATED\" if validated else \"✗ NOT VALIDATED\"\n",
    "    print(f\"{finding}: {status}\")\n",
    "\n",
    "# Overall validation\n",
    "overall_validation = sum(validation.values()) >= 2\n",
    "print(f\"\\nOVERALL PARADOX VALIDATION: {'✓ CONFIRMED' if overall_validation else '✗ NOT CONFIRMED'}\")\n",
    "\n",
    "if overall_validation:\n",
    "    print(\"\\n🎯 The Energy Efficiency Paradox has been validated!\")\n",
    "    print(\"   Model compression does NOT improve energy efficiency in DRL.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Results do not fully confirm the paradox.\")\n",
    "    print(\"   This may be due to experimental limitations or specific conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "### 6.3 Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of the energy efficiency paradox\n",
    "def visualize_energy_paradox(results, analysis):\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('Energy Efficiency Paradox in DRL Model Compression\\n(Paper: \"The Impact of Quantization and Pruning on Deep Reinforcement Learning\")', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    test_names = [r['test_name'] for r in results]\n",
    "    short_names = [name.split('(')[0].strip() for name in test_names]\n",
    "    \n",
    "    energies = [r['energy_results']['total_energy_joules']['total_system_power'] for r in results]\n",
    "    model_sizes = [r['model_info']['model_size_mb'] for r in results]\n",
    "    performances = [r['episode_stats']['average_episode_reward'] for r in results]\n",
    "    episode_lengths = [r['episode_stats']['average_episode_length'] for r in results]\n",
    "    \n",
    "    # Plot 1: Model Size vs Energy Consumption\n",
    "    axes[0, 0].scatter(model_sizes, energies, s=100, alpha=0.7, c=range(len(energies)), cmap='viridis')\n",
    "    for i, name in enumerate(short_names):\n",
    "        axes[0, 0].annotate(name, (model_sizes[i], energies[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Model Size (MB)')\n",
    "    axes[0, 0].set_ylabel('Total Energy (Joules)')\n",
    "    axes[0, 0].set_title('The Paradox: Smaller Models ≠ Less Energy\\n(Paper Finding 1)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(model_sizes) > 2:\n",
    "        z = np.polyfit(model_sizes, energies, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(min(model_sizes), max(model_sizes), 100)\n",
    "        axes[0, 0].plot(x_trend, p(x_trend), \"r--\", alpha=0.5, label=f'Trend: {z[0]:.2f}x + {z[1]:.2f}')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # Plot 2: Performance vs Energy Consumption\n",
    "    axes[0, 1].scatter(performances, energies, s=100, alpha=0.7, c=range(len(energies)), cmap='plasma')\n",
    "    for i, name in enumerate(short_names):\n",
    "        axes[0, 1].annotate(name, (performances[i], energies[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Average Episode Reward')\n",
    "    axes[0, 1].set_ylabel('Total Energy (Joules)')\n",
    "    axes[0, 1].set_title('Performance vs Energy Relationship\\n(Paper Finding 2)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Episode Length vs Energy (shows the mechanism)\n",
    "    axes[1, 0].scatter(episode_lengths, energies, s=100, alpha=0.7, c=range(len(energies)), cmap='coolwarm')\n",
    "    for i, name in enumerate(short_names):\n",
    "        axes[1, 0].annotate(name, (episode_lengths[i], energies[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Average Episode Length')\n",
    "    axes[1, 0].set_ylabel('Total Energy (Joules)')\n",
    "    axes[1, 0].set_title('Episode Length → Energy Consumption\\n(Paradox Mechanism)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Energy Components Breakdown\n",
    "    if len(results) > 0 and 'energy_results' in results[0]:\n",
    "        energy_components = ['cpu_power', 'memory_power', 'gpu_power']\n",
    "        component_data = {comp: [] for comp in energy_components}\n",
    "        \n",
    "        for result in results:\n",
    "            energy_data = result['energy_results']['total_energy_joules']\n",
    "            for comp in energy_components:\n",
    "                component_data[comp].append(energy_data.get(comp, 0))\n",
    "        \n",
    "        x_pos = np.arange(len(short_names))\n",
    "        bottom = np.zeros(len(short_names))\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        for i, comp in enumerate(energy_components):\n",
    "            axes[1, 1].bar(x_pos, component_data[comp], bottom=bottom, \n",
    "                          alpha=0.7, label=comp.replace('_', ' ').title(), color=colors[i])\n",
    "            bottom += component_data[comp]\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Model Type')\n",
    "        axes[1, 1].set_ylabel('Energy (Joules)')\n",
    "        axes[1, 1].set_title('Energy Components Breakdown')\n",
    "        axes[1, 1].set_xticks(x_pos)\n",
    "        axes[1, 1].set_xticklabels(short_names, rotation=45)\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    # Plot 5: Compression Efficiency Analysis\n",
    "    baseline_energy = energies[0]  # Assume first is baseline\n",
    "    baseline_size = model_sizes[0]\n",
    "    \n",
    "    energy_ratios = [e / baseline_energy for e in energies[1:]]  # Skip baseline\n",
    "    size_ratios = [s / baseline_size for s in model_sizes[1:]]   # Skip baseline\n",
    "    compression_names = short_names[1:]  # Skip baseline\n",
    "    \n",
    "    x_pos = np.arange(len(compression_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[2, 0].bar(x_pos - width/2, size_ratios, width, \n",
    "                          alpha=0.7, label='Model Size Ratio', color='skyblue')\n",
    "    bars2 = axes[2, 0].bar(x_pos + width/2, energy_ratios, width, \n",
    "                          alpha=0.7, label='Energy Ratio', color='lightcoral')\n",
    "    \n",
    "    axes[2, 0].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Baseline')\n",
    "    axes[2, 0].set_xlabel('Compression Method')\n",
    "    axes[2, 0].set_ylabel('Ratio to Baseline')\n",
    "    axes[2, 0].set_title('Compression Efficiency Paradox\\n(Lower Size ≠ Lower Energy)')\n",
    "    axes[2, 0].set_xticks(x_pos)\n",
    "    axes[2, 0].set_xticklabels(compression_names, rotation=45)\n",
    "    axes[2, 0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, size_ratios):\n",
    "        height = bar.get_height()\n",
    "        axes[2, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    for bar, value in zip(bars2, energy_ratios):\n",
    "        height = bar.get_height()\n",
    "        axes[2, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Plot 6: Summary and Paper Validation\n",
    "    validation = analysis['paradox_validation']\n",
    "    \n",
    "    summary_text = f\"\"\"Energy Efficiency Paradox Validation:\n",
    "\n",
    "Paper Findings:\n",
    "✓ \"Pruning and quantization do not improve \n",
    "   the energy efficiency of DRL models\"\n",
    "\n",
    "✓ \"Energy consumption tends to decrease only \n",
    "   when there is a significant drop in average \n",
    "   return, prompting the agent to terminate \n",
    "   early and requiring less computation\"\n",
    "\n",
    "✓ \"Despite reducing model size, quantization \n",
    "   does not improve memory usage\"\n",
    "\n",
    "Our Validation Results:\n",
    "{'✓' if validation['size_reduction_no_energy_improvement'] else '✗'} Size reduction ≠ Energy improvement\n",
    "{'✓' if validation['performance_drop_energy_reduction'] else '✗'} Performance drop → Energy reduction  \n",
    "{'✓' if validation['compression_overhead_exists'] else '✗'} Compression overhead exists\n",
    "\n",
    "Key Mechanisms:\n",
    "1. Better performance → Longer episodes → More energy\n",
    "2. Compression overhead in libraries\n",
    "3. Environment dominates computation\n",
    "4. Memory access patterns change\n",
    "\n",
    "Conclusion:\n",
    "The Energy Efficiency Paradox is {'VALIDATED' if sum(validation.values()) >= 2 else 'NOT FULLY VALIDATED'}\n",
    "Model compression ≠ Energy efficiency in DRL\"\"\"\n",
    "    \n",
    "    axes[2, 1].text(0.05, 0.95, summary_text, transform=axes[2, 1].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "    axes[2, 1].set_title('Paper Validation Summary')\n",
    "    axes[2, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "visualize_energy_paradox(results, analysis)\n",
    "\n",
    "# Show individual energy consumption plots for detailed analysis\n",
    "print(\"\\nDetailed energy consumption analysis for each test:\")\n",
    "for i, result in enumerate(results):\n",
    "    if i < 3:  # Show first 3 for brevity\n",
    "        experiment.energy_monitor.visualize_energy_consumption(\n",
    "            result['energy_results'], \n",
    "            title=f\"{result['test_name']} - Energy Analysis\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 7. Tổng kết và Hướng phát triển\n",
    "\n",
    "### 7.1 Những gì đã học được\n",
    "\n",
    "**Energy Efficiency Paradox:**\n",
    "- Model compression (pruning, quantization) reduces model size\n",
    "- But does NOT improve energy efficiency in DRL context\n",
    "- Counter-intuitive finding with important practical implications\n",
    "\n",
    "**Paradox Mechanisms:**\n",
    "1. **Performance-Episode Length Relationship**: Better models → longer episodes → more total energy\n",
    "2. **Compression Overhead**: Quantization/pruning libraries add computational cost\n",
    "3. **Environment Domination**: Environment simulation often consumes more energy than model inference\n",
    "4. **Memory Access Patterns**: Compressed models may have worse cache locality\n",
    "\n",
    "**Paper Validation:**\n",
    "- ✓ Size reduction ≠ Energy improvement\n",
    "- ✓ Performance drop → Energy reduction (early termination)\n",
    "- ✓ Compression overhead affects inference time\n",
    "- ✓ Memory usage paradox (library overhead)\n",
    "\n",
    "### 7.2 Practical Implications\n",
    "\n",
    "**For DRL Practitioners:**\n",
    "1. **Don't assume** compression improves energy efficiency\n",
    "2. **Consider total system energy**, not just model energy\n",
    "3. **Factor in episode length** when evaluating efficiency\n",
    "4. **Test on target deployment** environment\n",
    "\n",
    "**For Mobile/Edge Deployment:**\n",
    "1. **Battery life** may not improve with compression\n",
    "2. **Thermal management** considerations remain important\n",
    "3. **Network communication** may dominate energy consumption\n",
    "4. **Real-time constraints** vs energy trade-offs\n",
    "\n",
    "### 7.3 Hướng phát triển\n",
    "\n",
    "**Nghiên cứu tiếp theo:**\n",
    "1. **Environment-Aware Compression**: Optimize for specific environment characteristics\n",
    "2. **Episode-Length-Aware Compression**: Account for performance-episode length relationship\n",
    "3. **Hardware-Specific Energy Models**: Accurate energy modeling for different hardware\n",
    "4. **Dynamic Compression**: Adapt compression during deployment based on energy feedback\n",
    "\n",
    "**Cải tiến kỹ thuật:**\n",
    "1. **Low-Overhead Compression**: Develop compression methods with minimal runtime overhead\n",
    "2. **Energy-Aware Training**: Include energy consumption in training objectives\n",
    "3. **Efficient Library Implementation**: Optimize compression library implementations\n",
    "4. **Holistic System Optimization**: Consider entire DRL system, not just model\n",
    "\n",
    "### 7.4 Thách thức và Giải pháp\n",
    "\n",
    "**Thách thức:**\n",
    "- Energy measurement accuracy and reproducibility\n",
    "- Hardware dependency of energy characteristics\n",
    "- Complex interaction between performance and energy\n",
    "- Library and framework overhead\n",
    "\n",
    "**Giải pháp đề xuất:**\n",
    "- Standardized energy measurement protocols\n",
    "- Hardware-agnostic energy models\n",
    "- Multi-objective optimization frameworks\n",
    "- Compression-aware library design\n",
    "\n",
    "### 7.5 Key Takeaways\n",
    "\n",
    "**Fundamental Insight:**\n",
    "```\n",
    "Energy Efficiency ≠ Model Size Reduction\n",
    "```\n",
    "\n",
    "**DRL-Specific Factors:**\n",
    "1. **Sequential Decision Making**: Energy accumulates over episodes\n",
    "2. **Environment Interaction**: Often dominates energy consumption\n",
    "3. **Performance-Length Coupling**: Better performance → longer episodes\n",
    "4. **Stochastic Nature**: Variable episode lengths affect total energy\n",
    "\n",
    "**Practical Guidelines:**\n",
    "1. **Measure end-to-end energy**, not just model inference\n",
    "2. **Consider episode-level metrics**, not just step-level\n",
    "3. **Account for compression overhead** in energy calculations\n",
    "4. **Test on representative workloads** and hardware\n",
    "5. **Balance performance and energy** explicitly\n",
    "\n",
    "---\n",
    "\n",
    "**Kết luận:** Energy Efficiency Paradox trong DRL compression là một finding quan trọng thách thức các assumption truyền thống về model compression. Understanding này giúp practitioners có realistic expectations và design better energy-efficient DRL systems bằng cách xem xét toàn bộ system context chứ không chỉ model size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}