{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstores và Retrievers trong LangChain\n",
    "\n",
    "## Giới thiệu\n",
    "\n",
    "### Vectorstore là gì?\n",
    "**Vectorstore** là database được tối ưu để lưu trữ và tìm kiếm vector embeddings. Chúng cho phép:\n",
    "- **Lưu trữ** text documents dưới dạng vector representations\n",
    "- **Tìm kiếm similarity** dựa trên semantic meaning\n",
    "- **Scale** với millions of documents\n",
    "- **Fast retrieval** với approximate nearest neighbor algorithms\n",
    "\n",
    "### Retriever là gì?\n",
    "**Retriever** là interface chuẩn trong LangChain để:\n",
    "- **Tìm kiếm** relevant documents dựa trên query\n",
    "- **Trả về** danh sách documents có liên quan\n",
    "- **Tích hợp** dễ dàng với chains và agents\n",
    "- **Customize** search strategies và parameters\n",
    "\n",
    "### Workflow cơ bản:\n",
    "1. **Documents** → **Text Splitter** → **Chunks**\n",
    "2. **Chunks** → **Embedding Model** → **Vectors**\n",
    "3. **Vectors** → **Vectorstore** → **Indexed Database**\n",
    "4. **Query** → **Embedding** → **Similarity Search** → **Retrieved Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup và Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain components\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Document processing\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Retrievers\n",
    "from langchain.retrievers import (\n",
    "    ContextualCompressionRetriever,\n",
    "    EnsembleRetriever\n",
    ")\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Đã import các thư viện cần thiết\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chuẩn bị dữ liệu mẫu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo dataset mẫu về LangChain và AI\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"LangChain là một framework mạnh mẽ để xây dựng ứng dụng với Large Language Models (LLMs). \n",
    "        Framework này cung cấp các tools và abstractions để dễ dàng tích hợp LLMs vào applications. \n",
    "        LangChain hỗ trợ nhiều LLM providers như OpenAI, Anthropic, Cohere, và Hugging Face.\"\"\",\n",
    "        \"metadata\": {\"source\": \"langchain_intro\", \"category\": \"framework\", \"difficulty\": \"beginner\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Document Loaders trong LangChain giúp tải dữ liệu từ nhiều nguồn khác nhau như PDF, \n",
    "        websites, databases, và text files. Các loaders phổ biến bao gồm PyPDFLoader, WebBaseLoader, \n",
    "        CSVLoader, và TextLoader. Mỗi loader được tối ưu cho một loại data source cụ thể.\"\"\",\n",
    "        \"metadata\": {\"source\": \"document_loaders\", \"category\": \"data_ingestion\", \"difficulty\": \"beginner\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Text Splitters chia documents thành chunks nhỏ hơn để phù hợp với context window của LLMs. \n",
    "        RecursiveCharacterTextSplitter là lựa chọn phổ biến vì nó cố gắng preserve semantic boundaries. \n",
    "        Các tham số quan trọng bao gồm chunk_size và chunk_overlap.\"\"\",\n",
    "        \"metadata\": {\"source\": \"text_splitters\", \"category\": \"preprocessing\", \"difficulty\": \"intermediate\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Embeddings chuyển đổi text thành vector representations mà máy tính có thể hiểu được. \n",
    "        OpenAI Embeddings và Sentence Transformers là hai lựa chọn phổ biến. \n",
    "        Vectors này capture semantic meaning và cho phép tính toán similarity between texts.\"\"\",\n",
    "        \"metadata\": {\"source\": \"embeddings\", \"category\": \"vectorization\", \"difficulty\": \"intermediate\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Vector databases như FAISS, Pinecone, và Chroma được tối ưu để lưu trữ và tìm kiếm embeddings. \n",
    "        Chúng sử dụng algorithms như cosine similarity và euclidean distance để tìm nearest neighbors. \n",
    "        FAISS là lựa chọn tốt cho local development và prototyping.\"\"\",\n",
    "        \"metadata\": {\"source\": \"vector_databases\", \"category\": \"storage\", \"difficulty\": \"intermediate\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"RAG (Retrieval-Augmented Generation) kết hợp retrieval với generation để tạo ra câu trả lời \n",
    "        chính xác hơn. Pipeline bao gồm: retrieve relevant documents, construct context, và generate response. \n",
    "        RAG giúp giảm hallucination và cung cấp thông tin up-to-date.\"\"\",\n",
    "        \"metadata\": {\"source\": \"rag_system\", \"category\": \"architecture\", \"difficulty\": \"advanced\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Chains trong LangChain cho phép kết nối multiple components lại với nhau để tạo complex workflows. \n",
    "        LLMChain là chain đơn giản nhất, kết hợp prompt với LLM. \n",
    "        RetrievalQA chain kết hợp retriever với LLM để answer questions dựa trên retrieved context.\"\"\",\n",
    "        \"metadata\": {\"source\": \"chains\", \"category\": \"orchestration\", \"difficulty\": \"intermediate\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Agents trong LangChain có khả năng reasoning và quyết định actions cần thực hiện. \n",
    "        Chúng sử dụng LLMs làm reasoning engine và có thể gọi các tools khác nhau. \n",
    "        ReAct pattern là approach phổ biến cho agent implementation.\"\"\",\n",
    "        \"metadata\": {\"source\": \"agents\", \"category\": \"autonomous\", \"difficulty\": \"advanced\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Memory components trong LangChain giúp lưu trữ conversation history. \n",
    "        ConversationBufferMemory lưu toàn bộ conversation, trong khi ConversationSummaryMemory \n",
    "        summarize old messages để tiết kiệm tokens. Memory quan trọng cho chatbot applications.\"\"\",\n",
    "        \"metadata\": {\"source\": \"memory\", \"category\": \"conversation\", \"difficulty\": \"intermediate\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Production deployment của LangChain applications cần consider factors như scalability, \n",
    "        monitoring, và cost optimization. LangSmith cung cấp observability và debugging tools. \n",
    "        Caching strategies và async processing giúp improve performance.\"\"\",\n",
    "        \"metadata\": {\"source\": \"production\", \"category\": \"deployment\", \"difficulty\": \"advanced\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert thành Document objects\n",
    "documents = []\n",
    "for doc_data in sample_documents:\n",
    "    doc = Document(\n",
    "        page_content=doc_data[\"content\"],\n",
    "        metadata=doc_data[\"metadata\"]\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"✓ Đã tạo {len(documents)} documents mẫu\")\n",
    "print(f\"\\nDocument đầu tiên:\")\n",
    "print(f\"Content: {documents[0].page_content[:150]}...\")\n",
    "print(f\"Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Splitting cho Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents thành chunks nhỏ hơn\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split all documents\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Original documents: {len(documents)}\")\n",
    "print(f\"After splitting: {len(all_splits)} chunks\")\n",
    "print(f\"\\nChunk size distribution:\")\n",
    "\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in all_splits]\n",
    "print(f\"- Min: {min(chunk_sizes)} chars\")\n",
    "print(f\"- Max: {max(chunk_sizes)} chars\")\n",
    "print(f\"- Average: {np.mean(chunk_sizes):.1f} chars\")\n",
    "\n",
    "# Show some example chunks\n",
    "print(f\"\\n=== Example Chunks ===\")\n",
    "for i, chunk in enumerate(all_splits[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Content: {chunk.page_content}\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} chars\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Models - Chuyển đổi text thành vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: HuggingFace Embeddings (free, chạy local)\n",
    "print(\"Khởi tạo HuggingFace Embeddings...\")\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Sử dụng CPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize vectors\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_text = \"LangChain là framework cho LLM applications\"\n",
    "test_embedding = hf_embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"✓ HuggingFace embeddings ready\")\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")\n",
    "\n",
    "# Sử dụng HuggingFace embeddings làm default\n",
    "embeddings = hf_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: OpenAI Embeddings (nếu có API key)\n",
    "try:\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"\\nKhởi tạo OpenAI Embeddings...\")\n",
    "        openai_embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",  # Model mới nhất\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Test OpenAI embedding\n",
    "        openai_test_embedding = openai_embeddings.embed_query(test_text)\n",
    "        print(f\"✓ OpenAI embeddings ready\")\n",
    "        print(f\"OpenAI embedding dimension: {len(openai_test_embedding)}\")\n",
    "        \n",
    "        # Có thể switch sang OpenAI nếu muốn\n",
    "        # embeddings = openai_embeddings\n",
    "    else:\n",
    "        print(\"\\nOpenAI API key không có - sẽ sử dụng HuggingFace\")\nexcept Exception as e:\n",
    "    print(f\"\\nLỗi khi khởi tạo OpenAI embeddings: {e}\")\n",
    "    print(\"Tiếp tục với HuggingFace embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tạo FAISS Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo FAISS vectorstore từ documents\n",
    "print(\"Tạo FAISS vectorstore...\")\n",
    "print(f\"Processing {len(all_splits)} chunks...\")\n",
    "\n",
    "# FAISS.from_documents sẽ tự động:\n",
    "# 1. Embed tất cả documents\n",
    "# 2. Tạo FAISS index\n",
    "# 3. Store documents và metadata\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"✓ FAISS vectorstore đã được tạo\")\n",
    "print(f\"Total vectors: {vectorstore.index.ntotal}\")\n",
    "print(f\"Vector dimension: {vectorstore.index.d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm documents mới vào existing vectorstore\n",
    "new_doc = Document(\n",
    "    page_content=\"\"\"Prompt engineering là kỹ thuật thiết kế prompts hiệu quả để tối ưu output của LLMs. \n",
    "    Các techniques bao gồm few-shot learning, chain-of-thought prompting, và instruction tuning.\"\"\",\n",
    "    metadata={\"source\": \"prompt_engineering\", \"category\": \"optimization\", \"difficulty\": \"intermediate\"}\n",
    ")\n",
    "\n",
    "# Split new document\n",
    "new_splits = text_splitter.split_documents([new_doc])\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore.add_documents(new_splits)\n",
    "\n",
    "print(f\"✓ Đã thêm {len(new_splits)} chunks mới\")\n",
    "print(f\"Total vectors hiện tại: {vectorstore.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search\n",
    "query = \"Cách sử dụng embeddings trong LangChain\"\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\n=== Similarity Search Results ===\")\n",
    "\n",
    "# Similarity search (default k=4)\n",
    "similar_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Length: {len(doc.page_content)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search với scores\n",
    "query2 = \"RAG system architecture\"\n",
    "\n",
    "print(f\"\\nQuery: '{query2}'\")\n",
    "print(\"\\n=== Similarity Search with Scores ===\")\n",
    "\n",
    "# Search với similarity scores\n",
    "similar_docs_with_scores = vectorstore.similarity_search_with_score(query2, k=4)\n",
    "\n",
    "for i, (doc, score) in enumerate(similar_docs_with_scores):\n",
    "    print(f\"\\n--- Result {i+1} (Score: {score:.4f}) ---\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Category: {doc.metadata['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrievers - Interface chuẩn cho search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo retriever từ vectorstore\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 results\n",
    ")\n",
    "\n",
    "print(\"✓ Base retriever created\")\n",
    "\n",
    "# Test retriever\n",
    "query3 = \"text splitting strategies\"\n",
    "retrieved_docs = base_retriever.get_relevant_documents(query3)\n",
    "\n",
    "print(f\"\\nQuery: '{query3}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Retrieved Doc {i+1} ---\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different search types\n",
    "search_types = [\"similarity\", \"mmr\"]  # Maximum Marginal Relevance\n",
    "\n",
    "query4 = \"machine learning và AI\"\n",
    "\n",
    "for search_type in search_types:\n",
    "    print(f\"\\n=== Search Type: {search_type.upper()} ===\")\n",
    "    \n",
    "    if search_type == \"mmr\":\n",
    "        # MMR để giảm redundancy\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": 4,\n",
    "                \"fetch_k\": 8,  # Fetch more candidates\n",
    "                \"lambda_mult\": 0.7  # Diversity vs relevance tradeoff\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "    \n",
    "    docs = retriever.get_relevant_documents(query4)\n",
    "    \n",
    "    print(f\"Query: '{query4}'\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"  {i+1}. {doc.metadata['source']} - {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering bằng metadata\n",
    "def test_metadata_filtering():\n",
    "    print(\"=== Metadata Filtering ===\")\n",
    "    \n",
    "    query = \"LangChain framework\"\n",
    "    \n",
    "    # Search trong beginner category only\n",
    "    beginner_docs = vectorstore.similarity_search(\n",
    "        query,\n",
    "        k=3,\n",
    "        filter={\"difficulty\": \"beginner\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"Beginner level documents for '{query}':\")\n",
    "    for doc in beginner_docs:\n",
    "        print(f\"- {doc.metadata['source']}: {doc.page_content[:80]}...\")\n",
    "    \n",
    "    # Search trong advanced category\n",
    "    advanced_docs = vectorstore.similarity_search(\n",
    "        query,\n",
    "        k=3,\n",
    "        filter={\"difficulty\": \"advanced\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAdvanced level documents for '{query}':\")\n",
    "    for doc in advanced_docs:\n",
    "        print(f\"- {doc.metadata['source']}: {doc.page_content[:80]}...\")\n",
    "\n",
    "test_metadata_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score threshold filtering\n",
    "def test_score_threshold():\n",
    "    print(\"\\n=== Score Threshold Filtering ===\")\n",
    "    \n",
    "    query = \"vector database performance\"\n",
    "    \n",
    "    # Get all results with scores\n",
    "    all_results = vectorstore.similarity_search_with_score(query, k=10)\n",
    "    \n",
    "    print(f\"All results for '{query}':\")\n",
    "    for i, (doc, score) in enumerate(all_results):\n",
    "        print(f\"  {i+1}. Score: {score:.4f} - {doc.metadata['source']}\")\n",
    "    \n",
    "    # Filter by score threshold\n",
    "    threshold = 0.8\n",
    "    good_results = [(doc, score) for doc, score in all_results if score <= threshold]\n",
    "    \n",
    "    print(f\"\\nResults with score <= {threshold}:\")\n",
    "    for doc, score in good_results:\n",
    "        print(f\"  Score: {score:.4f} - {doc.metadata['source']}\")\n",
    "        print(f\"    Content: {doc.page_content[:100]}...\")\n",
    "\n",
    "test_score_threshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Contextual Compression Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual Compression để extract chỉ relevant parts\n",
    "try:\n",
    "    # Khởi tạo LLM cho compression\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        temperature=0,\n",
    "        anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Tạo compressor\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    \n",
    "    # Tạo compression retriever\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Contextual Compression Retriever created\")\n",
    "    \n",
    "    # Test compression\n",
    "    query_compression = \"Cách tạo embeddings trong LangChain\"\n",
    "    \n",
    "    print(f\"\\nQuery: '{query_compression}'\")\n",
    "    print(\"\\n=== Standard Retrieval ===\")\n",
    "    standard_docs = base_retriever.get_relevant_documents(query_compression)\n",
    "    for i, doc in enumerate(standard_docs[:2]):\n",
    "        print(f\"Doc {i+1}: {doc.page_content}\")\n",
    "        print(f\"Length: {len(doc.page_content)} chars\\n\")\n",
    "    \n",
    "    print(\"=== Compressed Retrieval ===\")\n",
    "    compressed_docs = compression_retriever.get_relevant_documents(query_compression)\n",
    "    for i, doc in enumerate(compressed_docs[:2]):\n",
    "        print(f\"Compressed Doc {i+1}: {doc.page_content}\")\n",
    "        print(f\"Length: {len(doc.page_content)} chars\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tạo Contextual Compression Retriever: {e}\")\n",
    "    print(\"Cần ANTHROPIC_API_KEY để sử dụng compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ensemble Retriever - Kết hợp multiple retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo multiple retrievers với settings khác nhau\n",
    "retriever1 = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "retriever2 = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3, \"fetch_k\": 6, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "# Ensemble retriever kết hợp results\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[retriever1, retriever2],\n",
    "    weights=[0.6, 0.4]  # Weight cho mỗi retriever\n",
    ")\n",
    "\n",
    "print(\"✓ Ensemble Retriever created\")\n",
    "\n",
    "# Test ensemble\n",
    "query_ensemble = \"document processing pipeline\"\n",
    "\n",
    "print(f\"\\nQuery: '{query_ensemble}'\")\n",
    "print(\"\\n=== Individual Retrievers ===\")\n",
    "\n",
    "# Results từ retriever 1\n",
    "docs1 = retriever1.get_relevant_documents(query_ensemble)\n",
    "print(f\"Retriever 1 (Similarity):\")\n",
    "for i, doc in enumerate(docs1):\n",
    "    print(f\"  {i+1}. {doc.metadata['source']}\")\n",
    "\n",
    "# Results từ retriever 2\n",
    "docs2 = retriever2.get_relevant_documents(query_ensemble)\n",
    "print(f\"\\nRetriever 2 (MMR):\")\n",
    "for i, doc in enumerate(docs2):\n",
    "    print(f\"  {i+1}. {doc.metadata['source']}\")\n",
    "\n",
    "# Ensemble results\n",
    "ensemble_docs = ensemble_retriever.get_relevant_documents(query_ensemble)\n",
    "print(f\"\\n=== Ensemble Results ===\")\n",
    "for i, doc in enumerate(ensemble_docs):\n",
    "    print(f\"  {i+1}. {doc.metadata['source']} - {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save và Load Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore to disk\n",
    "save_path = \"./langchain_vectorstore\"\n",
    "\n",
    "# Save FAISS index\n",
    "vectorstore.save_local(save_path)\n",
    "print(f\"✓ Vectorstore saved to {save_path}\")\n",
    "\n",
    "# Load vectorstore from disk\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    save_path,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required cho FAISS\n",
    ")\n",
    "\n",
    "print(f\"✓ Vectorstore loaded from {save_path}\")\n",
    "print(f\"Loaded vectors: {loaded_vectorstore.index.ntotal}\")\n",
    "\n",
    "# Test loaded vectorstore\n",
    "test_query = \"LangChain applications\"\n",
    "loaded_results = loaded_vectorstore.similarity_search(test_query, k=2)\n",
    "\n",
    "print(f\"\\nTest query on loaded vectorstore: '{test_query}'\")\n",
    "for i, doc in enumerate(loaded_results):\n",
    "    print(f\"  {i+1}. {doc.metadata['source']}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Analysis và Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_search_methods():\n",
    "    \"\"\"Benchmark different search methods\"\"\"\n",
    "    queries = [\n",
    "        \"LangChain framework features\",\n",
    "        \"vector database comparison\",\n",
    "        \"text splitting best practices\",\n",
    "        \"RAG implementation details\",\n",
    "        \"embedding model selection\"\n",
    "    ]\n",
    "    \n",
    "    methods = {\n",
    "        \"similarity\": lambda q: vectorstore.similarity_search(q, k=3),\n",
    "        \"similarity_with_score\": lambda q: vectorstore.similarity_search_with_score(q, k=3),\n",
    "        \"mmr\": lambda q: vectorstore.as_retriever(\n",
    "            search_type=\"mmr\", \n",
    "            search_kwargs={\"k\": 3, \"fetch_k\": 6}\n",
    "        ).get_relevant_documents(q)\n",
    "    }\n",
    "    \n",
    "    print(\"=== Performance Benchmark ===\")\n",
    "    \n",
    "    for method_name, method_func in methods.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for query in queries:\n",
    "            results = method_func(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        avg_time = (end_time - start_time) / len(queries)\n",
    "        \n",
    "        print(f\"{method_name:25s}: {avg_time:.4f}s per query\")\n",
    "\n",
    "benchmark_search_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze retrieval quality\n",
    "def analyze_retrieval_quality():\n",
    "    \"\"\"Analyze quality of retrieval results\"\"\"\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"query\": \"document loading strategies\",\n",
    "            \"expected_sources\": [\"document_loaders\", \"text_splitters\"]\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"vector search algorithms\",\n",
    "            \"expected_sources\": [\"vector_databases\", \"embeddings\"]\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"conversation memory management\",\n",
    "            \"expected_sources\": [\"memory\", \"chains\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Retrieval Quality Analysis ===\")\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        query = test_case[\"query\"]\n",
    "        expected = set(test_case[\"expected_sources\"])\n",
    "        \n",
    "        # Get retrieval results\n",
    "        results = vectorstore.similarity_search(query, k=5)\n",
    "        retrieved_sources = {doc.metadata[\"source\"] for doc in results}\n",
    "        \n",
    "        # Calculate metrics\n",
    "        intersection = expected.intersection(retrieved_sources)\n",
    "        precision = len(intersection) / len(retrieved_sources) if retrieved_sources else 0\n",
    "        recall = len(intersection) / len(expected) if expected else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Expected sources: {list(expected)}\")\n",
    "        print(f\"Retrieved sources: {list(retrieved_sources)}\")\n",
    "        print(f\"Precision: {precision:.2f}\")\n",
    "        print(f\"Recall: {recall:.2f}\")\n",
    "        print(f\"F1-Score: {f1_score:.2f}\")\n",
    "\n",
    "analyze_retrieval_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Best Practices và Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice Examples\n",
    "def demonstrate_best_practices():\n",
    "    print(\"=== VECTORSTORE BEST PRACTICES ===\")\n",
    "    \n",
    "    print(\"\\n1. CHUNK SIZE OPTIMIZATION\")\n",
    "    print(\"   - Too small: Lack context for meaningful retrieval\")\n",
    "    print(\"   - Too large: Multiple topics, reduced precision\")\n",
    "    print(\"   - Sweet spot: 200-800 characters for general text\")\n",
    "    \n",
    "    print(\"\\n2. EMBEDDING MODEL SELECTION\")\n",
    "    print(\"   - HuggingFace: Free, good for prototyping\")\n",
    "    print(\"   - OpenAI: High quality, costs tokens\")\n",
    "    print(\"   - Domain-specific: Consider specialized models\")\n",
    "    \n",
    "    print(\"\\n3. SEARCH STRATEGY\")\n",
    "    print(\"   - Similarity: Best for exact matches\")\n",
    "    print(\"   - MMR: Better for diverse results\")\n",
    "    print(\"   - Hybrid: Combine multiple approaches\")\n",
    "    \n",
    "    print(\"\\n4. METADATA UTILIZATION\")\n",
    "    print(\"   - Rich metadata enables filtering\")\n",
    "    print(\"   - Source tracking for citations\")\n",
    "    print(\"   - Category/topic for domain filtering\")\n",
    "    \n",
    "    print(\"\\n5. PERFORMANCE OPTIMIZATION\")\n",
    "    print(\"   - Index optimization for large datasets\")\n",
    "    print(\"   - Caching for repeated queries\")\n",
    "    print(\"   - Batch processing for bulk operations\")\n",
    "\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pitfalls và solutions\n",
    "def common_pitfalls_and_solutions():\n",
    "    print(\"\\n=== COMMON PITFALLS & SOLUTIONS ===\")\n",
    "    \n",
    "    pitfalls = [\n",
    "        {\n",
    "            \"problem\": \"Low retrieval quality\",\n",
    "            \"causes\": [\"Poor chunking strategy\", \"Wrong embedding model\", \"Insufficient metadata\"],\n",
    "            \"solutions\": [\"Optimize chunk size/overlap\", \"Try different embeddings\", \"Enrich metadata\"]\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Slow search performance\",\n",
    "            \"causes\": [\"Large vector space\", \"Inefficient index\", \"Too many candidates\"],\n",
    "            \"solutions\": [\"Index optimization\", \"Reduce embedding dimensions\", \"Use filtering\"]\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Irrelevant results\",\n",
    "            \"causes\": [\"Generic embeddings\", \"No domain context\", \"Poor query formulation\"],\n",
    "            \"solutions\": [\"Domain-specific embeddings\", \"Query expansion\", \"Contextual compression\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for pitfall in pitfalls:\n",
    "        print(f\"\\n❌ PROBLEM: {pitfall['problem']}\")\n",
    "        print(f\"   Causes: {', '.join(pitfall['causes'])}\")\n",
    "        print(f\"   ✅ Solutions: {', '.join(pitfall['solutions'])}\")\n",
    "\n",
    "common_pitfalls_and_solutions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup saved files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    if Path(save_path).exists():\n",
    "        shutil.rmtree(save_path)\n",
    "        print(f\"✓ Cleaned up {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not cleanup {save_path}: {e}\")\n",
    "\n",
    "print(\"\\n✓ Notebook execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng kết\n",
    "\n",
    "### **Vectorstores và Retrievers: Key Concepts**\n",
    "\n",
    "#### **Vectorstore**\n",
    "- **Purpose**: Lưu trữ và search vector embeddings\n",
    "- **FAISS**: Local, fast, good cho development\n",
    "- **Operations**: Add, search, save/load, filter\n",
    "- **Algorithms**: Cosine similarity, Euclidean distance\n",
    "\n",
    "#### **Embeddings**\n",
    "- **HuggingFace**: Free, local, good baseline\n",
    "- **OpenAI**: High quality, requires API key\n",
    "- **Considerations**: Dimension, domain specificity, cost\n",
    "\n",
    "#### **Retrievers**\n",
    "- **Interface**: Chuẩn hóa search operations\n",
    "- **Search Types**: Similarity, MMR, threshold\n",
    "- **Advanced**: Compression, ensemble, filtering\n",
    "\n",
    "### **Workflow Summary**\n",
    "```\n",
    "Documents → Split → Embed → Vectorstore → Retriever → Relevant Docs\n",
    "```\n",
    "\n",
    "### **Best Practices**\n",
    "1. **Chunking**: 200-800 chars, 10-20% overlap\n",
    "2. **Embeddings**: Choose based on domain và budget\n",
    "3. **Search**: Start với similarity, experiment với MMR\n",
    "4. **Metadata**: Rich metadata enables powerful filtering\n",
    "5. **Evaluation**: Monitor precision, recall, và user satisfaction\n",
    "\n",
    "### **Next Steps**\n",
    "- **Chains**: Combine retrievers với LLMs\n",
    "- **RAG Systems**: End-to-end question answering\n",
    "- **Production**: Scaling, monitoring, optimization\n",
    "- **Advanced**: Re-ranking, query expansion, hybrid search\n",
    "\n",
    "Vectorstores và Retrievers là backbone của modern RAG systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}