# 2002.08155v4.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: D:\llm\notebooks\AI-Papers\2002.08155v4.pdf
# KÃ­ch thÆ°á»›c file: 1320360 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================


--- TRANG 1 ---
CodeBERT:
Má»™t MÃ´ hÃ¬nh Pre-trained cho NgÃ´n ngá»¯ Láº­p trÃ¬nh vÃ  NgÃ´n ngá»¯ Tá»± nhiÃªn
Zhangyin Feng1, Daya Guo2, Duyu Tang3, Nan Duan3, Xiaocheng Feng1
Ming Gong4, Linjun Shou4, Bing Qin1, Ting Liu1, Daxin Jiang4, Ming Zhou3
1Trung tÃ¢m NghiÃªn cá»©u TÃ­nh toÃ¡n XÃ£ há»™i vÃ  Truy xuáº¥t ThÃ´ng tin, Há»c viá»‡n CÃ´ng nghá»‡ Harbin, Trung Quá»‘c
2TrÆ°á»ng Khoa há»c Dá»¯ liá»‡u vÃ  MÃ¡y tÃ­nh, Äáº¡i há»c Sun Yat-sen, Trung Quá»‘c
3Microsoft Research Asia, Báº¯c Kinh, Trung Quá»‘c
4Microsoft Search Technology Center Asia, Báº¯c Kinh, Trung Quá»‘c
fzyfeng,xcfeng,qinb,tliu g@ir.hit.edu.cn
guody5@mail2.sysu.edu.cn
fdutang,nanduan,migon,lisho,djiang,mingzhou g@microsoft.com

TÃ³m táº¯t
ChÃºng tÃ´i giá»›i thiá»‡u CodeBERT, má»™t mÃ´ hÃ¬nh pre-trained 
hai chiá»u (bimodal) cho ngÃ´n ngá»¯ láº­p trÃ¬nh (PL) vÃ  
ngÃ´n ngá»¯ tá»± nhiÃªn (NL). CodeBERT há»c 
cÃ¡c biá»ƒu diá»…n má»¥c Ä‘Ã­ch chung há»— trá»£ 
cÃ¡c á»©ng dá»¥ng NL-PL downstream nhÆ° tÃ¬m 
kiáº¿m mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn, táº¡o 
tÃ i liá»‡u mÃ£ nguá»“n, v.v. ChÃºng tÃ´i phÃ¡t triá»ƒn Code-
BERT vá»›i kiáº¿n trÃºc máº¡ng neural dá»±a trÃªn Transformer, 
vÃ  huáº¥n luyá»‡n nÃ³ vá»›i hÃ m má»¥c tiÃªu lai 
káº¿t há»£p nhiá»‡m vá»¥ pre-training cá»§a 
phÃ¡t hiá»‡n token thay tháº¿, cÃ³ nhiá»‡m vá»¥ phÃ¡t hiá»‡n 
cÃ¡c lá»±a chá»n thay tháº¿ há»£p lÃ½ Ä‘Æ°á»£c láº¥y máº«u tá»« generators. 
Äiá»u nÃ y cho phÃ©p chÃºng tÃ´i sá»­ dá»¥ng cáº£ dá»¯ liá»‡u "hai chiá»u" 
cá»§a cÃ¡c cáº·p NL-PL vÃ  dá»¯ liá»‡u "má»™t chiá»u", trong Ä‘Ã³ 
dá»¯ liá»‡u trÆ°á»›c cung cáº¥p input tokens cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh 
trong khi dá»¯ liá»‡u sau giÃºp há»c 
generators tá»‘t hÆ¡n. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ CodeBERT trÃªn 
hai á»©ng dá»¥ng NL-PL báº±ng cÃ¡ch fine-tuning cÃ¡c tham sá»‘ 
mÃ´ hÃ¬nh. Káº¿t quáº£ cho tháº¥y CodeBERT 
Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tiÃªn tiáº¿n nháº¥t trÃªn cáº£ 
tÃ¬m kiáº¿m mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  táº¡o tÃ i liá»‡u 
mÃ£ nguá»“n. HÆ¡n ná»¯a, Ä‘á»ƒ Ä‘iá»u tra 
loáº¡i kiáº¿n thá»©c nÃ o Ä‘Æ°á»£c há»c trong 
CodeBERT, chÃºng tÃ´i xÃ¢y dá»±ng má»™t dataset cho viá»‡c thÄƒm dÃ² 
NL-PL, vÃ  Ä‘Ã¡nh giÃ¡ trong setting zero-shot 
nÆ¡i cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh pre-trained Ä‘Æ°á»£c 
cá»‘ Ä‘á»‹nh. Káº¿t quáº£ cho tháº¥y CodeBERT hoáº¡t Ä‘á»™ng 
tá»‘t hÆ¡n cÃ¡c mÃ´ hÃ¬nh pre-trained trÆ°á»›c Ä‘Ã³ trÃªn viá»‡c thÄƒm dÃ² NL-
PL.1

1 Giá»›i thiá»‡u
CÃ¡c mÃ´ hÃ¬nh pre-trained lá»›n nhÆ° ELMo (Peters
et al., 2018), GPT (Radford et al., 2018), BERT
(Devlin et al., 2018), XLNet (Yang et al., 2019)
CÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n khi tÃ¡c giáº£ nÃ y lÃ  thá»±c táº­p sinh táº¡i Microsoft
Research Asia.
1Táº¥t cáº£ mÃ£ nguá»“n vÃ  dá»¯ liá»‡u cÃ³ sáºµn táº¡i https://
github.com/microsoft/CodeBERTvÃ  RoBERTa (Liu et al., 2019) Ä‘Ã£ cáº£i thiá»‡n 
Ä‘Ã¡ng ká»ƒ state-of-the-art trÃªn nhiá»u 
nhiá»‡m vá»¥ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP). Nhá»¯ng 
mÃ´ hÃ¬nh pre-trained nÃ y há»c cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh hiá»‡u quáº£ 
tá»« vÄƒn báº£n khÃ´ng nhÃ£n khá»•ng lá»“ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a 
bá»Ÿi cÃ¡c má»¥c tiÃªu tá»± giÃ¡m sÃ¡t, cháº³ng háº¡n nhÆ° masked 
language modeling, dá»± Ä‘oÃ¡n tá»« 
bá»‹ che (masked) gá»‘c tá»« má»™t chuá»—i Ä‘áº§u vÃ o 
bá»‹ che giáº£ táº¡o. Sá»± thÃ nh cÃ´ng cá»§a cÃ¡c mÃ´ hÃ¬nh pre-trained trong 
NLP cÅ©ng thÃºc Ä‘áº©y sá»± bÃ¹ng ná»• cá»§a cÃ¡c mÃ´ hÃ¬nh pre-trained 
Ä‘a phÆ°Æ¡ng thá»©c, cháº³ng háº¡n nhÆ° ViLBERT (Lu et al., 2019) cho 
ngÃ´n ngá»¯-hÃ¬nh áº£nh vÃ  VideoBERT (Sun et al., 2019) 
cho ngÃ´n ngá»¯-video, Ä‘Æ°á»£c há»c tá»« dá»¯ liá»‡u hai 
chiá»u nhÆ° cÃ¡c cáº·p ngÃ´n ngá»¯-hÃ¬nh áº£nh vá»›i cÃ¡c má»¥c tiÃªu 
tá»± giÃ¡m sÃ¡t hai chiá»u.

Trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i giá»›i thiá»‡u CodeBERT, má»™t mÃ´ hÃ¬nh 
pre-trained hai chiá»u cho ngÃ´n ngá»¯ tá»± nhiÃªn (NL) vÃ  
ngÃ´n ngá»¯ láº­p trÃ¬nh (PL) nhÆ° Python, Java, 
JavaScript, v.v. CodeBERT náº¯m báº¯t káº¿t ná»‘i ngá»¯ nghÄ©a 
giá»¯a ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh, 
vÃ  táº¡o ra cÃ¡c biá»ƒu diá»…n má»¥c Ä‘Ã­ch chung 
cÃ³ thá»ƒ há»— trá»£ rá»™ng rÃ£i cÃ¡c nhiá»‡m vá»¥ hiá»ƒu 
NL-PL (vÃ­ dá»¥: tÃ¬m kiáº¿m mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn) 
vÃ  cÃ¡c nhiá»‡m vá»¥ sinh (vÃ­ dá»¥: táº¡o tÃ i liá»‡u 
mÃ£ nguá»“n). NÃ³ Ä‘Æ°á»£c phÃ¡t triá»ƒn vá»›i 
Transformer Ä‘a lá»›p (Vaswani et al., 2017), Ä‘Æ°á»£c 
Ã¡p dá»¥ng trong Ä‘a sá»‘ cÃ¡c mÃ´ hÃ¬nh pre-trained lá»›n.

Äá»ƒ sá»­ dá»¥ng cáº£ cÃ¡c instance hai chiá»u 
cá»§a cÃ¡c cáº·p NL-PL vÃ  lÆ°á»£ng lá»›n mÃ£ nguá»“n má»™t chiá»u 
cÃ³ sáºµn, chÃºng tÃ´i huáº¥n luyá»‡n CodeBERT vá»›i hÃ m 
má»¥c tiÃªu lai, bao gá»“m masked language 
modeling tiÃªu chuáº©n (Devlin et al., 2018) vÃ  phÃ¡t hiá»‡n 
token thay tháº¿ (Clark et al., 2020), trong Ä‘Ã³ mÃ£ nguá»“n 
má»™t chiá»u giÃºp há»c generators tá»‘t hÆ¡n Ä‘á»ƒ 
táº¡o ra cÃ¡c token thay tháº¿ tá»‘t hÆ¡n cho má»¥c tiÃªu 
sau.

ChÃºng tÃ´i huáº¥n luyá»‡n CodeBERT tá»« cÃ¡c repository mÃ£ nguá»“n Github
arXiv:2002.08155v4  [cs.CL]  18 Sep 2020

--- TRANG 2 ---
vá»›i 6 ngÃ´n ngá»¯ láº­p trÃ¬nh, trong Ä‘Ã³ cÃ¡c datapoint hai chiá»u 
lÃ  cÃ¡c mÃ£ nguá»“n Ä‘Æ°á»£c ghÃ©p ná»‘i vá»›i tÃ i liá»‡u 
ngÃ´n ngá»¯ tá»± nhiÃªn cáº¥p Ä‘á»™ hÃ m (Husain et al., 
2019). Huáº¥n luyá»‡n Ä‘Æ°á»£c tiáº¿n hÃ nh trong setting tÆ°Æ¡ng tá»± 
nhÆ° multilingual BERT (Pires et al., 2019), 
trong trÆ°á»ng há»£p nÃ y má»™t mÃ´ hÃ¬nh pre-trained Ä‘Æ°á»£c há»c cho 
6 ngÃ´n ngá»¯ láº­p trÃ¬nh khÃ´ng cÃ³ marker rÃµ rÃ ng nÃ o 
Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biá»ƒu thá»‹ ngÃ´n ngá»¯ láº­p trÃ¬nh 
Ä‘áº§u vÃ o. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ CodeBERT trÃªn hai 
nhiá»‡m vá»¥ NL-PL downstream, bao gá»“m tÃ¬m kiáº¿m mÃ£ nguá»“n 
báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  táº¡o tÃ i liá»‡u mÃ£ nguá»“n. 
Káº¿t quáº£ cho tháº¥y fine-tuning cÃ¡c tham sá»‘ cá»§a 
CodeBERT Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tiÃªn tiáº¿n nháº¥t 
trÃªn cáº£ hai nhiá»‡m vá»¥. Äá»ƒ tiáº¿p tá»¥c Ä‘iá»u tra loáº¡i 
kiáº¿n thá»©c nÃ o Ä‘Æ°á»£c há»c trong CodeBERT, chÃºng tÃ´i xÃ¢y dá»±ng 
má»™t dataset cho viá»‡c thÄƒm dÃ² NL-PL, vÃ  kiá»ƒm tra CodeBERT 
trong ká»‹ch báº£n zero-shot, tá»©c lÃ  khÃ´ng fine-tuning 
cÃ¡c tham sá»‘ cá»§a CodeBERT. ChÃºng tÃ´i tháº¥y ráº±ng CodeBERT 
liÃªn tá»¥c vÆ°á»£t trá»™i hÆ¡n RoBERTa, má»™t mÃ´ hÃ¬nh pre-trained 
dá»±a hoÃ n toÃ n trÃªn ngÃ´n ngá»¯ tá»± nhiÃªn. Nhá»¯ng Ä‘Ã³ng gÃ³p 
cá»§a cÃ´ng viá»‡c nÃ y nhÆ° sau:

CodeBERT lÃ  mÃ´ hÃ¬nh pre-trained NL-PL 
lá»›n Ä‘áº§u tiÃªn cho nhiá»u ngÃ´n ngá»¯ láº­p trÃ¬nh.

Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y CodeBERT hiá»‡u quáº£ 
trong cáº£ nhiá»‡m vá»¥ tÃ¬m kiáº¿m mÃ£ nguá»“n vÃ  táº¡o 
code-to-text.

ChÃºng tÃ´i tiáº¿p tá»¥c táº¡o ra má»™t dataset lÃ  dataset 
Ä‘áº§u tiÃªn Ä‘á»ƒ Ä‘iá»u tra kháº£ nÄƒng thÄƒm dÃ² cá»§a cÃ¡c 
mÃ´ hÃ¬nh pre-trained dá»±a trÃªn mÃ£ nguá»“n.

2 Kiáº¿n thá»©c ná»n táº£ng
2.1 MÃ´ hÃ¬nh Pre-trained trong NLP
CÃ¡c mÃ´ hÃ¬nh pre-trained lá»›n (Peters et al., 2018; Rad-
ford et al., 2018; Devlin et al., 2018; Yang et al., 
2019; Liu et al., 2019; Raffel et al., 2019) Ä‘Ã£ 
mang láº¡i nhá»¯ng cáº£i thiá»‡n thá»±c nghiá»‡m Ä‘Ã¡ng ká»ƒ trÃªn háº§u nhÆ° 
má»i nhiá»‡m vá»¥ NLP trong vÃ i nÄƒm qua. CÃ¡c 
phÆ°Æ¡ng phÃ¡p thÃ nh cÃ´ng huáº¥n luyá»‡n máº¡ng neural sÃ¢u trÃªn 
vÄƒn báº£n thuáº§n tÃºy quy mÃ´ lá»›n vá»›i cÃ¡c má»¥c tiÃªu há»c 
tá»± giÃ¡m sÃ¡t. Má»™t trong nhá»¯ng kiáº¿n trÃºc neural 
Ä‘áº¡i diá»‡n nháº¥t lÃ  Transformer (Vaswani et al., 
2017), cÅ©ng lÃ  kiáº¿n trÃºc Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ´ng viá»‡c nÃ y. NÃ³ 
chá»©a nhiá»u lá»›p self-attention, vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c 
há»c theo cÃ¡ch thÃ´ng thÆ°á»ng vá»›i gradient descent theo 
cÃ¡ch end-to-end vÃ¬ má»i thÃ nh pháº§n Ä‘á»u cÃ³ thá»ƒ 
vi phÃ¢n. Thuáº­t ngá»¯ "tá»± giÃ¡m sÃ¡t" cÃ³ nghÄ©a 
ráº±ng cÃ¡c supervision Ä‘Æ°á»£c sá»­ dá»¥ng cho pre-training Ä‘Æ°á»£c tá»± Ä‘á»™ng 
thu tháº­p tá»« dá»¯ liá»‡u thÃ´ mÃ  khÃ´ng cáº§n chÃº thÃ­ch thá»§ cÃ´ng. CÃ¡c má»¥c tiÃªu há»c chá»§ Ä‘áº¡o lÃ  language 
modeling vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³. VÃ­ dá»¥, 
trong GPT (Radford et al., 2018), má»¥c tiÃªu há»c 
lÃ  language modeling, cá»¥ thá»ƒ lÃ  dá»± Ä‘oÃ¡n 
tá»« tiáº¿p theo wk cho trÆ°á»›c cÃ¡c tá»« ngá»¯ cáº£nh trÆ°á»›c Ä‘Ã³ 
fw1;w2;:::;wk 1g. VÃ¬ má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a pre-
training khÃ´ng pháº£i lÃ  huáº¥n luyá»‡n má»™t language model tá»‘t, nÃªn 
mong muá»‘n xem xÃ©t cáº£ ngá»¯ cáº£nh trÆ°á»›c vÃ  sau 
Ä‘á»ƒ há»c cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh má»¥c Ä‘Ã­ch chung tá»‘t hÆ¡n. 
Äiá»u nÃ y dáº«n chÃºng ta Ä‘áº¿n má»¥c tiÃªu masked language 
modeling Ä‘Æ°á»£c sá»­ dá»¥ng trong BERT (Devlin 
et al., 2018), há»c dá»± Ä‘oÃ¡n cÃ¡c tá»« 
bá»‹ che cá»§a má»™t chuá»—i tá»« bá»‹ che ngáº«u nhiÃªn cho trÆ°á»›c 
cÃ¡c ngá»¯ cáº£nh xung quanh. Masked language modeling 
cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t trong hai má»¥c tiÃªu há»c 
Ä‘á»ƒ huáº¥n luyá»‡n CodeBERT.

2.2 MÃ´ hÃ¬nh Pre-trained Äa phÆ°Æ¡ng thá»©c
Sá»± thÃ nh cÃ´ng Ä‘Ã¡ng ká»ƒ cá»§a mÃ´ hÃ¬nh pre-trained 
trong NLP Ä‘Ã£ thÃºc Ä‘áº©y viá»‡c phÃ¡t triá»ƒn mÃ´ hÃ¬nh pre-trained 
Ä‘a phÆ°Æ¡ng thá»©c há»c sá»± liÃªn káº¿t áº©n 
giá»¯a cÃ¡c Ä‘áº§u vÃ o cá»§a cÃ¡c phÆ°Æ¡ng thá»©c khÃ¡c nhau. Nhá»¯ng mÃ´ hÃ¬nh 
nÃ y thÆ°á»ng Ä‘Æ°á»£c há»c tá»« dá»¯ liá»‡u hai chiá»u, cháº³ng háº¡n 
nhÆ° cÃ¡c cáº·p ngÃ´n ngá»¯-hÃ¬nh áº£nh hoáº·c cÃ¡c cáº·p ngÃ´n ngá»¯-
video. VÃ­ dá»¥, ViLBERT (Lu et al., 2019) 
há»c tá»« dá»¯ liá»‡u chÃº thÃ­ch hÃ¬nh áº£nh, trong Ä‘Ã³ mÃ´ hÃ¬nh 
há»c báº±ng cÃ¡ch tÃ¡i táº¡o cÃ¡c category cá»§a vÃ¹ng hÃ¬nh áº£nh 
bá»‹ che hoáº·c cÃ¡c tá»« bá»‹ che cho trÆ°á»›c cÃ¡c 
Ä‘áº§u vÃ o quan sÃ¡t Ä‘Æ°á»£c, vÃ  Ä‘á»“ng thá»i dá»± Ä‘oÃ¡n liá»‡u chÃº thÃ­ch 
cÃ³ mÃ´ táº£ ná»™i dung hÃ¬nh áº£nh hay khÃ´ng. TÆ°Æ¡ng tá»±, 
VideoBERT (Sun et al., 2019) há»c tá»« 
dá»¯ liá»‡u ngÃ´n ngá»¯-video vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng dá»± Ä‘oÃ¡n 
token bá»‹ che cá»§a video vÃ  vÄƒn báº£n. CÃ´ng viá»‡c cá»§a chÃºng tÃ´i thuá»™c 
vá» dÃ²ng nghiÃªn cá»©u nÃ y vÃ¬ chÃºng tÃ´i coi NL vÃ  PL 
nhÆ° cÃ¡c phÆ°Æ¡ng thá»©c khÃ¡c nhau. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i khÃ¡c vá»›i 
cÃ¡c cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã³ á»Ÿ chá»— nhiÃªn liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh 
khÃ´ng chá»‰ bao gá»“m dá»¯ liá»‡u hai chiá»u cá»§a cÃ¡c cáº·p NL-PL, 
mÃ  cÃ²n lÆ°á»£ng lá»›n dá»¯ liá»‡u má»™t chiá»u nhÆ° mÃ£ nguá»“n 
khÃ´ng cÃ³ tÃ i liá»‡u Ä‘Æ°á»£c ghÃ©p ná»‘i.

Má»™t cÃ´ng viá»‡c Ä‘á»“ng thá»i (Kanade et al., 2019) sá»­ dá»¥ng 
masked language modeling vÃ  next sentence prediction 
lÃ m má»¥c tiÃªu Ä‘á»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh BERT trÃªn 
mÃ£ nguá»“n Python, trong Ä‘Ã³ má»™t cÃ¢u lÃ  má»™t dÃ²ng 
mÃ£ nguá»“n logic nhÆ° Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi tiÃªu chuáº©n Python. 
Vá» máº·t quÃ¡ trÃ¬nh pre-training, CodeBERT 
khÃ¡c vá»›i cÃ´ng viá»‡c cá»§a há» á»Ÿ chá»— (1) CodeBERT Ä‘Æ°á»£c 
huáº¥n luyá»‡n theo phong cÃ¡ch cross-modal vÃ  táº­n dá»¥ng cáº£ 
dá»¯ liá»‡u NL-PL hai chiá»u vÃ  dá»¯ liá»‡u PL/NL má»™t chiá»u, (2) 
CodeBERT Ä‘Æ°á»£c pre-trained trÃªn sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh, 
vÃ  (3) CodeBERT Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i má»¥c tiÃªu há»c 
má»›i dá»±a trÃªn phÃ¡t hiá»‡n token thay tháº¿.

--- TRANG 3 ---

3 CodeBERT
ChÃºng tÃ´i mÃ´ táº£ chi tiáº¿t vá» CodeBERT trong 
pháº§n nÃ y, bao gá»“m kiáº¿n trÃºc mÃ´ hÃ¬nh, cÃ¡c biá»ƒu diá»…n 
Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, cÃ¡c má»¥c tiÃªu vÃ  dá»¯ liá»‡u 
Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n CodeBERT, vÃ  cÃ¡ch fine-tune 
CodeBERT khi nÃ³ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c nhiá»‡m vá»¥ downstream.

3.1 Kiáº¿n trÃºc MÃ´ hÃ¬nh
ChÃºng tÃ´i theo BERT (Devlin et al., 2018) vÃ  
RoBERTa (Liu et al., 2019), vÃ  sá»­ dá»¥ng 
Transformer hai chiá»u Ä‘a lá»›p (Vaswani et al., 2017) lÃ m 
kiáº¿n trÃºc mÃ´ hÃ¬nh cá»§a CodeBERT. ChÃºng tÃ´i sáº½ khÃ´ng 
xem xÃ©t láº¡i kiáº¿n trÃºc Transformer phá»• biáº¿n má»™t cÃ¡ch 
chi tiáº¿t. ChÃºng tÃ´i phÃ¡t triá»ƒn CodeBERT báº±ng cÃ¡ch sá»­ dá»¥ng chÃ­nh xÃ¡c 
kiáº¿n trÃºc mÃ´ hÃ¬nh giá»‘ng nhÆ° RoBERTa-base. 
Tá»•ng sá»‘ tham sá»‘ mÃ´ hÃ¬nh lÃ  125M.

3.2 Biá»ƒu diá»…n Äáº§u vÃ o/Äáº§u ra
Trong giai Ä‘oáº¡n pre-training, chÃºng tÃ´i Ä‘áº·t Ä‘áº§u vÃ o lÃ  
phÃ©p ná»‘i cá»§a hai Ä‘oáº¡n vá»›i token phÃ¢n tÃ¡ch 
Ä‘áº·c biá»‡t, cá»¥ thá»ƒ lÃ  [CLS ];w1;w2;::wn;[SEP ]; 
c1;c2;:::;cm;[EOS ]. Má»™t Ä‘oáº¡n lÃ  vÄƒn báº£n ngÃ´n ngá»¯ tá»± nhiÃªn, 
vÃ  Ä‘oáº¡n khÃ¡c lÃ  mÃ£ nguá»“n tá»« má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh 
nháº¥t Ä‘á»‹nh. [CLS ] lÃ  token Ä‘áº·c biá»‡t á»Ÿ 
phÃ­a trÆ°á»›c hai Ä‘oáº¡n, biá»ƒu diá»…n áº©n cuá»‘i cÃ¹ng cá»§a nÃ³ 
Ä‘Æ°á»£c coi lÃ  biá»ƒu diá»…n chuá»—i tá»•ng há»£p 
cho phÃ¢n loáº¡i hoáº·c xáº¿p háº¡ng. Theo 
cÃ¡ch xá»­ lÃ½ vÄƒn báº£n tiÃªu chuáº©n trong Trans-
former, chÃºng tÃ´i coi má»™t vÄƒn báº£n ngÃ´n ngá»¯ tá»± nhiÃªn nhÆ° má»™t chuá»—i 
cÃ¡c tá»«, vÃ  chia nÃ³ thÃ nh WordPiece (Wu 
et al., 2016). ChÃºng tÃ´i coi má»™t Ä‘oáº¡n mÃ£ nguá»“n nhÆ° má»™t chuá»—i 
cÃ¡c token.

Äáº§u ra cá»§a CodeBERT bao gá»“m (1) biá»ƒu diá»…n 
vector ngá»¯ cáº£nh cá»§a má»—i token, cho cáº£ ngÃ´n ngá»¯ tá»± nhiÃªn 
vÃ  mÃ£ nguá»“n, vÃ  (2) biá»ƒu diá»…n cá»§a 
[CLS ], hoáº¡t Ä‘á»™ng nhÆ° biá»ƒu diá»…n chuá»—i 
tá»•ng há»£p.

3.3 Dá»¯ liá»‡u Pre-training
ChÃºng tÃ´i huáº¥n luyá»‡n CodeBERT vá»›i cáº£ dá»¯ liá»‡u hai chiá»u, 
tham chiáº¿u Ä‘áº¿n dá»¯ liá»‡u song song cá»§a cÃ¡c cáº·p 
ngÃ´n ngá»¯ tá»± nhiÃªn-mÃ£ nguá»“n, vÃ  dá»¯ liá»‡u má»™t chiá»u, 
Ä‘áº¡i diá»‡n cho mÃ£ nguá»“n khÃ´ng cÃ³ vÄƒn báº£n ngÃ´n ngá»¯ tá»± nhiÃªn 
Ä‘Æ°á»£c ghÃ©p ná»‘i vÃ  ngÃ´n ngá»¯ tá»± nhiÃªn khÃ´ng cÃ³ mÃ£ nguá»“n Ä‘Æ°á»£c ghÃ©p ná»‘i.

ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c datapoint tá»« cÃ¡c repository Github, 
trong Ä‘Ã³ má»—i datapoint hai chiá»u lÃ  má»™t hÃ m 
riÃªng láº» vá»›i tÃ i liá»‡u Ä‘Æ°á»£c ghÃ©p ná»‘i, vÃ  má»—i mÃ£ nguá»“n 
má»™t chiá»u lÃ  má»™t hÃ m khÃ´ng cÃ³ tÃ i liá»‡u Ä‘Æ°á»£c ghÃ©p ná»‘i. 
Cá»¥ thá»ƒ, chÃºng tÃ´i sá»­ dá»¥ng má»™t dataset lá»›n gáº§n Ä‘Ã¢y Dá»® LIá»†U HUáº¤N LUYá»†N Dá»® LIá»†U HAI CHIá»€U MÃƒ NGUá»’N Má»˜T CHIá»€U
GO 319,256 726,768
JAVA 500,754 1,569,889
JAVASCRIPT 143,252 1,857,835
PHP 662,907 977,821
PYTHON 458,219 1,156,085
RUBY 52,905 164,048
Táº¤T Cáº¢ 2,137,293 6,452,446

Báº£ng 1: Thá»‘ng kÃª cá»§a dataset Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n Code-
BERT.

Ä‘Æ°á»£c cung cáº¥p bá»Ÿi Husain et al. (2019), bao gá»“m 
2.1M datapoint hai chiá»u vÃ  6.4M mÃ£ nguá»“n má»™t chiá»u 
trÃªn sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh (Python, Java, 
JavaScript, PHP, Ruby, vÃ  Go). Thá»‘ng kÃª dá»¯ liá»‡u Ä‘Æ°á»£c 
hiá»ƒn thá»‹ trong Báº£ng 1.2

Dá»¯ liá»‡u Ä‘áº¿n tá»« cÃ¡c repository GitHub 
mÃ£ nguá»“n má»Ÿ cÃ´ng khai khÃ´ng pháº£i fork vÃ  Ä‘Æ°á»£c lá»c 
vá»›i má»™t táº­p há»£p cÃ¡c rÃ ng buá»™c vÃ  quy táº¯c. VÃ­ dá»¥, 
(1) má»—i project nÃªn Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi Ã­t nháº¥t 
má»™t project khÃ¡c, (2) má»—i tÃ i liá»‡u Ä‘Æ°á»£c cáº¯t ngáº¯n 
Ä‘áº¿n Ä‘oáº¡n Ä‘áº§u tiÃªn, (3) cÃ¡c tÃ i liá»‡u 
ngáº¯n hÆ¡n ba token bá»‹ loáº¡i bá», (4) cÃ¡c hÃ m 
ngáº¯n hÆ¡n ba dÃ²ng bá»‹ loáº¡i bá», vÃ  (5) 
tÃªn hÃ m cÃ³ substring "test" bá»‹ loáº¡i bá». 
Má»™t vÃ­ dá»¥ vá» dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘Æ°a ra trong HÃ¬nh 13.

HÃ¬nh 1: Má»™t vÃ­ dá»¥ vá» cáº·p NL-PL, trong Ä‘Ã³ NL lÃ  
Ä‘oáº¡n Ä‘áº§u tiÃªn (Ä‘Æ°á»£c tÃ´ mÃ u Ä‘á») tá»« tÃ i liá»‡u 
(Ä‘Æ°á»ng nÃ©t Ä‘á»©t mÃ u Ä‘en) cá»§a má»™t hÃ m.

3.4 Pre-training CodeBERT
ChÃºng tÃ´i mÃ´ táº£ hai má»¥c tiÃªu Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n 
CodeBERT á»Ÿ Ä‘Ã¢y. Má»¥c tiÃªu Ä‘áº§u tiÃªn lÃ  masked 
language modeling (MLM), Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh hiá»‡u quáº£ 
trong tÃ i liá»‡u (Devlin et al., 2018; Liu et al.,

2VÃ¬ chÃºng tÃ´i sáº½ Ä‘Ã¡nh giÃ¡ trÃªn nhiá»‡m vá»¥ tÃ¬m kiáº¿m mÃ£ nguá»“n 
báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn, chÃºng tÃ´i chá»‰ sá»­ dá»¥ng dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a Husain et al. 
(2019) Ä‘á»ƒ huáº¥n luyá»‡n CodeBERT mÃ  khÃ´ng truy cáº­p vÃ o dá»¯ liá»‡u dev vÃ  testing.

3Nguá»“n cá»§a vÃ­ dá»¥ minh há»a Ä‘áº¿n tá»« 
https://github.com/apache/spark/blob/
618d6bff71073c8c93501ab7392c3cc579730f0b/
python/pyspark/rdd.py#L125-L138

--- TRANG 4 ---
6NL Generator
NL-Code 
Discriminator[ð‘€ð´ð‘†ð¾]ð‘¤CodeBERT V2:
Má»™t MÃ´ hÃ¬nh Pre-trained cho Hiá»ƒu vÃ  Sinh NL-Code 
ð‘¤2
ð‘¤3
ð‘¤4
[ð‘€ð´ð‘†ð¾]ð‘¤ð‘¤1
ð‘¤2
ð‘¤3
ð‘¤4
ð‘¤5
[ð‘€ð´ð‘†ð¾]ð‘
ð‘5ð‘3
ð‘4
[ð‘€ð´ð‘†ð¾]ð‘ð‘1
ð‘2
ð‘3
ð‘4
ð‘5
ð‘6ð‘1ð‘¤51
ð‘¤2
ð‘¤3
ð‘¤4
ð‘¤5
ð‘29
ð‘5ð‘3
ð‘4
ð‘162ð‘1gá»‘c
thay tháº¿

Code Generatorgá»‘c
gá»‘c
gá»‘c
thay tháº¿gá»‘c
gá»‘c
gá»‘c
gá»‘c
thay tháº¿láº¥y máº«u
láº¥y máº«u
láº¥y máº«u
láº¥y máº«u

HÃ¬nh 2: Má»™t minh há»a vá» má»¥c tiÃªu phÃ¡t hiá»‡n token thay tháº¿. Cáº£ NL vÃ  code generators Ä‘á»u lÃ  language 
models, táº¡o ra cÃ¡c token há»£p lÃ½ cho cÃ¡c vá»‹ trÃ­ bá»‹ che dá»±a trÃªn ngá»¯ cáº£nh xung quanh. NL-Code discriminator 
lÃ  mÃ´ hÃ¬nh pre-trained má»¥c tiÃªu, Ä‘Æ°á»£c huáº¥n luyá»‡n thÃ´ng qua viá»‡c phÃ¡t hiá»‡n cÃ¡c token thay tháº¿ há»£p lÃ½ Ä‘Æ°á»£c láº¥y máº«u tá»« 
NL vÃ  PL generators. NL-Code discriminator Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra cÃ¡c biá»ƒu diá»…n má»¥c Ä‘Ã­ch chung trong bÆ°á»›c fine-
tuning. Cáº£ NL vÃ  code generators Ä‘á»u bá»‹ loáº¡i bá» trong bÆ°á»›c fine-tuning.

2019; Sun et al., 2019). ChÃºng tÃ´i Ã¡p dá»¥ng masked language 
modeling trÃªn dá»¯ liá»‡u hai chiá»u cá»§a cÃ¡c cáº·p NL-PL. Má»¥c tiÃªu 
thá»© hai lÃ  phÃ¡t hiá»‡n token thay tháº¿ (RTD), 
sá»­ dá»¥ng thÃªm má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u má»™t chiá»u, 
cháº³ng háº¡n nhÆ° mÃ£ nguá»“n khÃ´ng cÃ³ vÄƒn báº£n ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘Æ°á»£c ghÃ©p ná»‘i. 
CÃ¡c siÃªu tham sá»‘ chi tiáº¿t cho pre-training mÃ´ hÃ¬nh 
Ä‘Æ°á»£c Ä‘Æ°a ra trong Phá»¥ lá»¥c B.1.

Má»¥c tiÃªu #1: Masked Language Modeling 
(MLM) Cho má»™t datapoint cá»§a cáº·p NL-PL ( x= 
fw,cg) lÃ m Ä‘áº§u vÃ o, trong Ä‘Ã³ w lÃ  má»™t chuá»—i tá»« NL 
vÃ  c lÃ  má»™t chuá»—i token PL, trÆ°á»›c tiÃªn chÃºng tÃ´i 
chá»n má»™t táº­p há»£p vá»‹ trÃ­ ngáº«u nhiÃªn cho cáº£ NL vÃ  PL 
Ä‘á»ƒ che (tá»©c lÃ  mw vÃ  mc, tÆ°Æ¡ng á»©ng), vÃ  
sau Ä‘Ã³ thay tháº¿ cÃ¡c vá»‹ trÃ­ Ä‘Æ°á»£c chá»n báº±ng token 
[MASK ] Ä‘áº·c biá»‡t. Theo Devlin et al. (2018), 
15% cÃ¡c token tá»« x bá»‹ che.

mw
iuniff1;jwjgfori= 1tojwj (1)
mc
iuniff1;jcjgfori= 1tojcj (2)
wmasked=REPLACE (w;mw;[MASK ])(3)
cmasked=REPLACE (c;mc;[MASK ]) (4)
x=w+c (5)

Má»¥c tiÃªu MLM lÃ  dá»± Ä‘oÃ¡n cÃ¡c token 
gá»‘c bá»‹ che, Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° sau, 
trong Ä‘Ã³ pD1 lÃ  discriminator dá»± Ä‘oÃ¡n má»™t 
token tá»« tá»« vá»±ng lá»›n.

LMLM()=X
i2mw[mc logpD1(xijwmasked;cmasked)
(6)

Má»¥c tiÃªu #2: PhÃ¡t hiá»‡n Token Thay tháº¿ (RTD)
Trong má»¥c tiÃªu MLM, chá»‰ dá»¯ liá»‡u hai chiá»u (tá»©c lÃ  data-
points cá»§a cÃ¡c cáº·p NL-PL) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n. á»ž Ä‘Ã¢y chÃºng tÃ´i 
trÃ¬nh bÃ y má»¥c tiÃªu cá»§a phÃ¡t hiá»‡n token thay tháº¿. 
Má»¥c tiÃªu RTD (Clark et al., 2020) ban Ä‘áº§u 
Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ há»c hiá»‡u quáº£ mÃ´ hÃ¬nh pre-trained 
cho ngÃ´n ngá»¯ tá»± nhiÃªn. ChÃºng tÃ´i Ä‘iá»u chá»‰nh nÃ³ trong ká»‹ch báº£n 
cá»§a chÃºng tÃ´i, vá»›i lá»£i tháº¿ cá»§a viá»‡c sá»­ dá»¥ng cáº£ dá»¯ liá»‡u hai chiá»u 
vÃ  má»™t chiá»u Ä‘á»ƒ huáº¥n luyá»‡n. Cá»¥ thá»ƒ, cÃ³ 
hai data generators á»Ÿ Ä‘Ã¢y, má»™t NL generator pGw 
vÃ  má»™t PL generator pGc, cáº£ hai Ä‘á»ƒ táº¡o ra cÃ¡c lá»±a chá»n 
thay tháº¿ há»£p lÃ½ cho táº­p há»£p cÃ¡c vá»‹ trÃ­ 
bá»‹ che ngáº«u nhiÃªn.

^wipGw(wijwmasked)fori2mw(7)
^cipGc(cijcmasked)fori2mc(8)
wcorrupt=REPLACE (w;mw;^w) (9)
ccorrupt=REPLACE (c;mc;^c) (10)
xcorrupt=wcorrupt+ccorrupt(11)

Discriminator Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh liá»‡u 
má»™t tá»« cÃ³ pháº£i lÃ  tá»« gá»‘c hay khÃ´ng, Ä‘Ã¢y lÃ  bÃ i toÃ¡n 
phÃ¢n loáº¡i nhá»‹ phÃ¢n. ÄÃ¡ng chÃº Ã½ ráº±ng má»¥c tiÃªu 
RTD Ä‘Æ°á»£c Ã¡p dá»¥ng cho má»i vá»‹ trÃ­ trong 
Ä‘áº§u vÃ o, vÃ  nÃ³ khÃ¡c vá»›i GAN (generative adversarial 
network) á»Ÿ chá»— náº¿u má»™t generator tÃ¬nh cá» 
táº¡o ra token Ä‘Ãºng, nhÃ£n cá»§a token Ä‘Ã³ 
lÃ  "real" thay vÃ¬ "fake" (Clark et al., 2020). HÃ m 
loss cá»§a RTD liÃªn quan Ä‘áº¿n discrimina-
tor Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi Ä‘Æ°á»£c Ä‘Æ°a ra dÆ°á»›i Ä‘Ã¢y, trong Ä‘Ã³ (i) lÃ 

--- TRANG 5 ---
má»™t hÃ m chá»‰ thá»‹ vÃ  pD2 lÃ  discriminator 
dá»± Ä‘oÃ¡n xÃ¡c suáº¥t tá»« thá»© i-th lÃ  
gá»‘c.

LRTD() =jwj+jcjX
i=1
(i)logpD2(xcorrupt;i)+

1 (i)
1 logpD2(xcorrupt;i)
(12)

(i) =(
1;ifxcorrupt
i =xi:
0;otherwise:(13)

CÃ³ nhiá»u cÃ¡ch khÃ¡c nhau Ä‘á»ƒ triá»ƒn khai cÃ¡c 
generators. Trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i triá»ƒn khai hai 
mÃ´ hÃ¬nh ngÃ´n ngá»¯ n-gram hiá»‡u quáº£ (Jurafsky, 2000) 
vá»›i ngá»¯ cáº£nh hai chiá»u, má»™t cho NL vÃ  má»™t 
cho PL, vÃ  há»c chÃºng tá»« cÃ¡c datapoint má»™t chiá»u 
tÆ°Æ¡ng á»©ng, tÆ°Æ¡ng á»©ng. PhÆ°Æ¡ng phÃ¡p nÃ y 
dá»… dÃ ng tá»•ng quÃ¡t hÃ³a Ä‘á»ƒ há»c cÃ¡c generators hai chiá»u hoáº·c 
sá»­ dá»¥ng cÃ¡c generators phá»©c táº¡p hÆ¡n nhÆ° kiáº¿n trÃºc 
neural dá»±a trÃªn Transformer Ä‘Æ°á»£c há»c theo cÃ¡ch joint. 
ChÃºng tÃ´i Ä‘á»ƒ láº¡i Ä‘iá»u nÃ y cho cÃ´ng viá»‡c tÆ°Æ¡ng lai. Dá»¯ liá»‡u huáº¥n luyá»‡n PL 
lÃ  cÃ¡c mÃ£ nguá»“n má»™t chiá»u nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1, vÃ  dá»¯ liá»‡u 
huáº¥n luyá»‡n NL Ä‘áº¿n tá»« cÃ¡c tÃ i liá»‡u 
tá»« dá»¯ liá»‡u hai chiá»u. CÃ³ thá»ƒ dá»… dÃ ng má»Ÿ rá»™ng hai 
dataset huáº¥n luyá»‡n nÃ y thÃ nh lÆ°á»£ng lá»›n hÆ¡n. HÃ m 
loss cuá»‘i cÃ¹ng Ä‘Æ°á»£c Ä‘Æ°a ra dÆ°á»›i Ä‘Ã¢y.

min
LMLM() +LRTD() (14)

3.5 Fine-tuning CodeBERT
ChÃºng tÃ´i cÃ³ cÃ¡c setting khÃ¡c nhau Ä‘á»ƒ sá»­ dá»¥ng CodeBERT trong 
cÃ¡c nhiá»‡m vá»¥ NL-PL downstream. VÃ­ dá»¥, trong tÃ¬m kiáº¿m 
mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn, chÃºng tÃ´i Ä‘Æ°a Ä‘áº§u vÃ o theo 
cÃ¡ch tÆ°Æ¡ng tá»± nhÆ° giai Ä‘oáº¡n pre-training vÃ  sá»­ dá»¥ng 
biá»ƒu diá»…n cá»§a [CLS ] Ä‘á»ƒ Ä‘o sá»± liÃªn quan ngá»¯ nghÄ©a 
giá»¯a mÃ£ nguá»“n vÃ  truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn, 
trong khi trong sinh code-to-text, chÃºng tÃ´i sá»­ dá»¥ng framework encoder-
decoder vÃ  khá»Ÿi táº¡o encoder cá»§a 
má»™t mÃ´ hÃ¬nh generative vá»›i CodeBERT. Chi tiáº¿t Ä‘Æ°á»£c 
Ä‘Æ°a ra trong pháº§n thá»±c nghiá»‡m.

4 Thá»±c nghiá»‡m
ChÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ thá»±c nghiá»‡m trong pháº§n nÃ y Ä‘á»ƒ xÃ¡c minh 
tÃ­nh hiá»‡u quáº£ cá»§a CodeBERT. TrÆ°á»›c tiÃªn chÃºng tÃ´i mÃ´ táº£ 
viá»‡c sá»­ dá»¥ng CodeBERT trong tÃ¬m kiáº¿m mÃ£ nguá»“n 
báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn (x4.1), theo cÃ¡ch mÃ  cÃ¡c tham sá»‘ mÃ´ hÃ¬nh cá»§a 
CodeBERT Ä‘Æ°á»£c fine-tuned. Sau Ä‘Ã³, chÃºng tÃ´i trÃ¬nh bÃ y 
nhiá»‡m vá»¥ thÄƒm dÃ² NL-PL ( x4.2), vÃ  Ä‘Ã¡nh giÃ¡ Code-
BERT trong setting zero-shot nÆ¡i cÃ¡c tham sá»‘ cá»§a CodeBERT Ä‘Æ°á»£c cá»‘ Ä‘á»‹nh. Cuá»‘i cÃ¹ng, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Code-
BERT trÃªn má»™t bÃ i toÃ¡n sinh, tá»©c lÃ  táº¡o tÃ i liá»‡u 
mÃ£ nguá»“n (x4.3), vÃ  tiáº¿p tá»¥c Ä‘Ã¡nh giÃ¡ trÃªn má»™t 
ngÃ´n ngá»¯ láº­p trÃ¬nh chÆ°a bao giá» Ä‘Æ°á»£c tháº¥y trong 
giai Ä‘oáº¡n huáº¥n luyá»‡n (x4.4).

4.1 TÃ¬m kiáº¿m MÃ£ nguá»“n báº±ng NgÃ´n ngá»¯ Tá»± nhiÃªn
Cho má»™t ngÃ´n ngá»¯ tá»± nhiÃªn lÃ m Ä‘áº§u vÃ o, má»¥c tiÃªu 
cá»§a tÃ¬m kiáº¿m mÃ£ nguá»“n lÃ  tÃ¬m mÃ£ nguá»“n cÃ³ liÃªn quan ngá»¯ nghÄ©a 
nháº¥t tá»« má»™t táº­p há»£p cÃ¡c mÃ£ nguá»“n. ChÃºng tÃ´i tiáº¿n hÃ nh 
thá»±c nghiá»‡m trÃªn corpus CodeSearchNet 
(Husain et al., 2019)4. ChÃºng tÃ´i theo metric Ä‘Ã¡nh giÃ¡ 
chÃ­nh thá»©c Ä‘á»ƒ tÃ­nh Mean Reciprocal Rank 
(MRR) cho má»—i cáº·p dá»¯ liá»‡u kiá»ƒm tra ( c,w) trÃªn má»™t táº­p há»£p 
cá»‘ Ä‘á»‹nh 999 mÃ£ nguá»“n phÃ¢n tÃ¢m. ChÃºng tÃ´i tiáº¿p tá»¥c tÃ­nh 
macro-average MRR cho táº¥t cáº£ ngÃ´n ngá»¯ nhÆ° má»™t metric 
Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ. Há»¯u Ã­ch khi lÆ°u Ã½ ráº±ng metric nÃ y 
khÃ¡c vá»›i metric AVG trong paper 
gá»‘c, nÆ¡i cÃ¢u tráº£ lá»i Ä‘Æ°á»£c truy xuáº¥t tá»« cÃ¡c á»©ng cá»­ viÃªn 
tá»« táº¥t cáº£ sÃ¡u ngÃ´n ngá»¯. ChÃºng tÃ´i fine-tune má»™t mÃ´ hÃ¬nh 
cá»¥ thá»ƒ cho ngÃ´n ngá»¯ cho má»—i ngÃ´n ngá»¯ láº­p trÃ¬nh5.

ChÃºng tÃ´i huáº¥n luyá»‡n má»—i mÃ´ hÃ¬nh vá»›i hÃ m loss 
phÃ¢n loáº¡i nhá»‹ phÃ¢n, trong Ä‘Ã³ má»™t lá»›p softmax Ä‘Æ°á»£c káº¿t ná»‘i 
vá»›i biá»ƒu diá»…n cá»§a [CLS ]. Cáº£ dataset huáº¥n luyá»‡n vÃ  
validation Ä‘á»u Ä‘Æ°á»£c táº¡o theo cÃ¡ch mÃ  cÃ¡c máº«u 
tÃ­ch cá»±c vÃ  tiÃªu cá»±c Ä‘Æ°á»£c cÃ¢n báº±ng. CÃ¡c máº«u tiÃªu cá»±c 
gá»“m sá»‘ lÆ°á»£ng cÃ¢n báº±ng cÃ¡c instance 
vá»›i NL Ä‘Æ°á»£c thay tháº¿ ngáº«u nhiÃªn (tá»©c lÃ  ( c,^w)) vÃ  PL 
(tá»©c lÃ  ( ^c,w)). CÃ¡c siÃªu tham sá»‘ chi tiáº¿t cho fine-tuning 
mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Æ°a ra trong Phá»¥ lá»¥c B.2.

So sÃ¡nh MÃ´ hÃ¬nh Báº£ng 2 hiá»ƒn thá»‹ káº¿t quáº£ 
cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau trÃªn corpus CodeSearchNet. 
Bá»‘n hÃ ng Ä‘áº§u tiÃªn Ä‘Æ°á»£c bÃ¡o cÃ¡o bá»Ÿi Husain 
et al. (2019), lÃ  cÃ¡c embedding joint cá»§a NL vÃ  
PL (Gu et al., 2018; Mitra et al., 2018). NBOW 
Ä‘áº¡i diá»‡n cho neural bag-of-words. CNN ,BIRNN 
vÃ  SELFATT Ä‘áº¡i diá»‡n cho máº¡ng neural tÃ­ch cháº­p 
1D (Kim, 2014), máº¡ng neural tÃ¡i phÃ¡t dá»±a trÃªn GRU 
hai chiá»u (Cho et al., 2014), vÃ  
multi-head attention (Vaswani et al., 2017), tÆ°Æ¡ng á»©ng.

ChÃºng tÃ´i bÃ¡o cÃ¡o cÃ¡c con sá»‘ cÃ²n láº¡i trong Báº£ng 2. 
ChÃºng tÃ´i huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh pre-trained nÃ y báº±ng cÃ¡ch coi 
mÃ£ nguá»“n nhÆ° má»™t chuá»—i cÃ¡c token. ChÃºng tÃ´i cÅ©ng tiáº¿p tá»¥c 
huáº¥n luyá»‡n RoBERTa chá»‰ trÃªn mÃ£ nguá»“n tá»« Code-
SearchNet vá»›i masked language modeling. Káº¿t quáº£ 
cho tháº¥y CodeBERT liÃªn tá»¥c hoáº¡t Ä‘á»™ng

4Chi tiáº¿t thÃªm vá» dataset Ä‘Æ°á»£c Ä‘Æ°a ra trong Phá»¥ lá»¥c A.
5ChÃºng tÃ´i Ä‘Ã£ fine-tuned má»™t mÃ´ hÃ¬nh Ä‘a ngÃ´n ngá»¯ cho sÃ¡u ngÃ´n ngá»¯ láº­p 
trÃ¬nh, nhÆ°ng tháº¥y ráº±ng nÃ³ hoáº¡t Ä‘á»™ng kÃ©m hÆ¡n viá»‡c fine-tuning 
má»™t mÃ´ hÃ¬nh cá»¥ thá»ƒ cho ngÃ´n ngá»¯ cho má»—i ngÃ´n ngá»¯ láº­p trÃ¬nh.

--- TRANG 6 ---
MÃ” HÃŒNH RUBY JAVASCRIPT GO PYTHON JAVA PHP MA-AVG
NBOW 0.4285 0.4607 0.6409 0.5809 0.5140 0.4835 0.5181
CNN 0.2450 0.3523 0.6274 0.5708 0.5270 0.5294 0.4753
BIRNN 0.0835 0.1530 0.4524 0.3213 0.2865 0.2512 0.2580
SELF ATT 0.3651 0.4506 0.6809 0.6922 0.5866 0.6011 0.5628
ROBERTA 0.6245 0.6060 0.8204 0.8087 0.6659 0.6576 0.6972
PT W/ CODE ONLY (INIT=S) 0.5712 0.5557 0.7929 0.7855 0.6567 0.6172 0.6632
PT W/ CODE ONLY (INIT=R) 0.6612 0.6402 0.8191 0.8438 0.7213 0.6706 0.7260
CODEBERT (MLM, INIT=S) 0.5695 0.6029 0.8304 0.8261 0.7142 0.6556 0.6998
CODEBERT (MLM, INIT=R) 0.6898 0.6997 0.8383 0.8647 0.7476 0.6893 0.7549
CODEBERT (RTD, INIT=R) 0.6414 0.6512 0.8285 0.8263 0.7150 0.6774 0.7233
CODEBERT (MLM+RTD, INIT=R) 0.6926 0.7059 0.8400 0.8685 0.7484 0.7062 0.7603

Báº£ng 2: Káº¿t quáº£ trÃªn truy xuáº¥t mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn. Baselines bao gá»“m bá»‘n embedding joint (nhÃ³m Ä‘áº§u tiÃªn) cá»§a NL 
vÃ  PL, RoBERTa, vÃ  RoBERTa Ä‘Æ°á»£c huáº¥n luyá»‡n tiáº¿p tá»¥c vá»›i masked language modeling chá»‰ trÃªn mÃ£ nguá»“n 
(nhÃ³m thá»© hai). PT Ä‘áº¡i diá»‡n cho pre-training. ChÃºng tÃ´i huáº¥n luyá»‡n CodeBERT (nhÃ³m thá»© ba) vá»›i cÃ¡c setting khÃ¡c nhau, bao gá»“m 
sá»­ dá»¥ng khá»Ÿi táº¡o khÃ¡c nhau (tá»« Ä‘áº§u ( INIT=S) hoáº·c khá»Ÿi táº¡o vá»›i cÃ¡c tham sá»‘ cá»§a RoBERTa ( INIT=R)) vÃ  
sá»­ dá»¥ng cÃ¡c má»¥c tiÃªu há»c khÃ¡c nhau (MLM, RTD, hoáº·c káº¿t há»£p cá»§a cáº£ hai).

tá»‘t hÆ¡n RoBERTa vÃ  mÃ´ hÃ¬nh pre-trained 
chá»‰ vá»›i mÃ£ nguá»“n. CodeBERT (MLM) Ä‘Æ°á»£c há»c tá»« 
Ä‘áº§u hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n RoBERTa. KhÃ´ng 
ngáº¡c nhiÃªn, khá»Ÿi táº¡o CodeBERT vá»›i RoBERTa 
cáº£i thiá»‡n hiá»‡u suáº¥t6.

4.2 ThÄƒm dÃ² NL-PL
Trong pháº§n phá»¥ trÆ°á»›c, chÃºng tÃ´i cho tháº¥y tÃ­nh hiá»‡u quáº£ thá»±c nghiá»‡m 
cá»§a CodeBERT trong setting mÃ  cÃ¡c 
tham sá»‘ cá»§a CodeBERT Ä‘Æ°á»£c fine-tuned trong cÃ¡c 
nhiá»‡m vá»¥ downstream. Trong pháº§n phá»¥ nÃ y, chÃºng tÃ´i tiáº¿p tá»¥c Ä‘iá»u tra 
loáº¡i kiáº¿n thá»©c nÃ o Ä‘Æ°á»£c há»c trong Code-
BERT mÃ  khÃ´ng sá»­a Ä‘á»•i cÃ¡c tham sá»‘.

CÃ´ng thá»©c Nhiá»‡m vá»¥ vÃ  XÃ¢y dá»±ng Dá»¯ liá»‡u Theo 
cÃ¡c thá»±c nghiá»‡m thÄƒm dÃ³ trong NLP (Petroni 
et al., 2019; Talmor et al., 2019), chÃºng tÃ´i nghiÃªn cá»©u thÄƒm dÃ² 
NL-PL á»Ÿ Ä‘Ã¢y. VÃ¬ khÃ´ng cÃ³ cÃ´ng viá»‡c hiá»‡n táº¡i nÃ o 
hÆ°á»›ng tá»›i má»¥c tiÃªu nÃ y, chÃºng tÃ´i cÃ´ng thá»©c hÃ³a bÃ i toÃ¡n 
thÄƒm dÃ² NL-PL vÃ  táº¡o dataset báº±ng chÃ­nh chÃºng tÃ´i. 
Cho má»™t cáº·p NL-PL ( c,w), má»¥c tiÃªu cá»§a thÄƒm dÃ² 
NL-PL lÃ  kiá»ƒm tra kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n/khÃ´i phá»¥c 
chÃ­nh xÃ¡c token bá»‹ che quan tÃ¢m (hoáº·c lÃ  
token mÃ£ nguá»“n ci hoáº·c token tá»« wj) giá»¯a cÃ¡c distractor. 
CÃ³ hai loáº¡i distractor chÃ­nh: má»™t lÃ  
toÃ n bá»™ tá»« vá»±ng má»¥c tiÃªu Ä‘Æ°á»£c sá»­ dá»¥ng cho má»¥c tiÃªu masked language 
modeling (Petroni et al., 2019), vÃ  
loáº¡i khÃ¡c cÃ³ Ã­t á»©ng cá»­ viÃªn hÆ¡n Ä‘Æ°á»£c lá»c hoáº·c 
Ä‘Æ°á»£c tuyá»ƒn chá»n dá»±a trÃªn hiá»ƒu biáº¿t cá»§a chuyÃªn gia vá» 
kháº£ nÄƒng Ä‘Æ°á»£c kiá»ƒm tra (Talmor et al., 2019). ChÃºng tÃ´i theo 
hÆ°á»›ng thá»© hai vÃ  cÃ´ng thá»©c hÃ³a thÄƒm dÃ² NL-PL 
nhÆ° má»™t nhiá»‡m vá»¥ tráº£ lá»i cÃ¢u há»i Ä‘a lá»±a chá»n, trong Ä‘Ã³ 
cÃ¢u há»i cÃ³ dáº¡ng cloze-style mÃ  má»™t token nháº¥t Ä‘á»‹nh Ä‘Æ°á»£c thay tháº¿ bá»Ÿi [MASK ] vÃ  cÃ¡c cÃ¢u tráº£ lá»i á»©ng cá»­ viÃªn 
distractor Ä‘Æ°á»£c tuyá»ƒn chá»n dá»±a trÃªn chuyÃªn mÃ´n cá»§a chÃºng tÃ´i.

Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ trÃªn phÃ­a NL vÃ  phÃ­a PL, 
tÆ°Æ¡ng á»©ng. Äá»ƒ giáº£m nháº¹ ná»— lá»±c thu tháº­p dá»¯ liá»‡u, chÃºng tÃ´i thu tháº­p 
dá»¯ liá»‡u tá»± Ä‘á»™ng tá»« cÃ¡c cáº·p NL-PL 
trong cáº£ táº­p validation vÃ  testing cá»§a Code-
SearchNet, cáº£ hai Ä‘á»u khÃ´ng Ä‘Æ°á»£c tháº¥y trong giai Ä‘oáº¡n 
pre-training. Äá»ƒ Ä‘Ã¡nh giÃ¡ trÃªn phÃ­a NL, chÃºng tÃ´i 
chá»n cÃ¡c cáº·p NL-PL cÃ³ tÃ i liá»‡u NL bao gá»“m 
má»™t trong sÃ¡u tá»« khÃ³a ( max,maximize ,min, 
minimize ,less,greater ), vÃ  nhÃ³m chÃºng thÃ nh bá»‘n 
á»©ng cá»­ viÃªn báº±ng cÃ¡ch gá»™p hai tá»« khÃ³a Ä‘áº§u tiÃªn vÃ  hai 
tá»« khÃ³a giá»¯a. Nhiá»‡m vá»¥ lÃ  yÃªu cáº§u cÃ¡c mÃ´ hÃ¬nh pre-trained 
chá»n Ä‘Ãºng thay vÃ¬ ba 
distractor khÃ¡c. CÃ³ nghÄ©a lÃ , Ä‘áº§u vÃ o trong setting 
nÃ y bao gá»“m mÃ£ nguá»“n hoÃ n chá»‰nh vÃ  tÃ i liá»‡u 
NL bá»‹ che. Má»¥c tiÃªu lÃ  chá»n cÃ¢u tráº£ lá»i 
Ä‘Ãºng tá»« bá»‘n á»©ng cá»­ viÃªn. Cho phÃ­a PL, chÃºng tÃ´i 
chá»n mÃ£ nguá»“n chá»©a tá»« khÃ³a max vÃ  min, vÃ  
cÃ´ng thá»©c hÃ³a nhiá»‡m vá»¥ nhÆ° má»™t bÃ i toÃ¡n lá»±a chá»n cÃ¢u tráº£ lá»i 
hai lá»±a chá»n. á»ž Ä‘Ã¢y, Ä‘áº§u vÃ o bao gá»“m tÃ i liá»‡u NL 
hoÃ n chá»‰nh vÃ  mÃ£ nguá»“n PL bá»‹ che, vÃ  má»¥c tiÃªu 
lÃ  chá»n cÃ¢u tráº£ lá»i Ä‘Ãºng tá»« hai á»©ng cá»­ viÃªn.

VÃ¬ hoÃ n thÃ nh mÃ£ nguá»“n lÃ  má»™t ká»‹ch báº£n quan trá»ng, 
chÃºng tÃ´i muá»‘n kiá»ƒm tra kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh trong viá»‡c dá»± Ä‘oÃ¡n 
token Ä‘Ãºng chá»‰ dá»±a trÃªn ngá»¯ cáº£nh PL 
trÆ°á»›c Ä‘Ã³. Do Ä‘Ã³, chÃºng tÃ´i thÃªm má»™t setting bá»• sung 
cho phÃ­a PL, nÆ¡i Ä‘áº§u vÃ o bao gá»“m tÃ i liá»‡u 
NL hoÃ n chá»‰nh vÃ  mÃ£ nguá»“n PL trÆ°á»›c Ä‘Ã³. Thá»‘ng kÃª dá»¯ liá»‡u 
Ä‘Æ°á»£c Ä‘Æ°a ra trong hai hÃ ng Ä‘áº§u trong Báº£ng 3.

So sÃ¡nh MÃ´ hÃ¬nh Káº¿t quáº£ Ä‘Æ°á»£c Ä‘Æ°a ra trong Báº£ng 
3. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c, cá»¥ thá»ƒ lÃ  sá»‘ lÆ°á»£ng instance 
Ä‘Æ°á»£c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c trÃªn sá»‘ lÆ°á»£ng táº¥t cáº£ 
instance, cho má»—i ngÃ´n ngá»¯ láº­p trÃ¬nh. VÃ¬

--- TRANG 7 ---
RUBY JAVASCRIPT GO PYTHON JAVA PHP Táº¤T Cáº¢
Sá» LÆ¯á»¢NG DATAPOINT CHO THÄ‚M DÃ’
PL (2 Lá»°A CHá»ŒN) 38 272 152 1,264 482 407 2,615
NL (4 Lá»°A CHá»ŒN) 20 65 159 216 323 73 856
THÄ‚M DÃ’ PL
ROBERTA 73.68 63.97 72.37 59.18 59.96 69.78 62.45
PRE-TRAIN W / CODE ONLY 71.05 77.94 89.47 70.41 70.12 82.31 74.11
CODEBERT (MLM) 86.84 86.40 90.79 82.20 90.46 88.21 85.66
THÄ‚M DÃ’ PL Vá»šI CHá»ˆ NGá»® Cáº¢NH TRÆ¯á»šC ÄÃ“
ROBERTA 73.68 53.31 51.32 55.14 42.32 52.58 52.24
PRE-TRAIN W / CODE ONLY 63.16 48.53 61.84 56.25 58.51 58.97 56.71
CODEBERT (MLM) 65.79 50.74 59.21 62.03 54.98 59.95 59.12
THÄ‚M DÃ’ NL
ROBERTA 50.00 72.31 54.72 61.57 61.61 65.75 61.21
PRE-TRAIN W / CODE ONLY 55.00 67.69 60.38 68.06 65.02 68.49 65.19
CODEBERT (MLM) 65.00 89.23 66.67 76.85 73.37 79.45 74.53

Báº£ng 3: Thá»‘ng kÃª dá»¯ liá»‡u cho thÄƒm dÃ² NL-PL vÃ  hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh pre-trained khÃ¡c nhau. Äá»™ chÃ­nh xÃ¡c 
(%) Ä‘Æ°á»£c bÃ¡o cÃ¡o. Káº¿t quáº£ tá»‘t nháº¥t trong má»—i nhÃ³m Ä‘Æ°á»£c in Ä‘áº­m.

dataset trong cÃ¡c ngÃ´n ngá»¯ láº­p trÃ¬nh khÃ¡c nhau cá»±c ká»³ 
khÃ´ng cÃ¢n báº±ng, chÃºng tÃ´i bÃ¡o cÃ¡o metric tÃ­ch lÅ©y 
theo cÃ¡ch tÆ°Æ¡ng tá»±. ChÃºng tÃ´i sá»­ dá»¥ng CodeBERT 
(MLM) á»Ÿ Ä‘Ã¢y vÃ¬ lá»›p Ä‘áº§u ra cá»§a nÃ³ phÃ¹ há»£p tá»± nhiÃªn 
cho thÄƒm dÃ². Káº¿t quáº£ cho tháº¥y CodeBERT hoáº¡t Ä‘á»™ng 
tá»‘t hÆ¡n baselines trÃªn háº§u nhÆ° táº¥t cáº£ ngÃ´n ngá»¯ 
trÃªn cáº£ thÄƒm dÃ² NL vÃ  PL. CÃ¡c con sá»‘ vá»›i 
chá»‰ ngá»¯ cáº£nh trÆ°á»›c Ä‘Ã³ tháº¥p hÆ¡n so vá»›i cÃ³ 
ngá»¯ cáº£nh hai chiá»u, Ä‘iá»u nÃ y cho tháº¥y hoÃ n thÃ nh mÃ£ nguá»“n 
lÃ  thÃ¡ch thá»©c. ChÃºng tÃ´i Ä‘á»ƒ láº¡i nÃ³ nhÆ° má»™t cÃ´ng viá»‡c 
tÆ°Æ¡ng lai.

ChÃºng tÃ´i tiáº¿p tá»¥c Ä‘Æ°a ra má»™t nghiÃªn cá»©u trÆ°á»ng há»£p vá» thÄƒm dÃ² PL-NL. 
ChÃºng tÃ´i che token NL vÃ  token PL riÃªng biá»‡t, vÃ  
bÃ¡o cÃ¡o cÃ¡c xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a RoBERTa vÃ  
CodeBERT. HÃ¬nh 3 minh há»a vÃ­ dá»¥ vá» má»™t 
mÃ£ nguá»“n python7. ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng RoBERTa tháº¥t báº¡i trong 
cáº£ hai trÆ°á»ng há»£p, trong khi CodeBERT Ä‘Æ°a ra dá»± Ä‘oÃ¡n 
Ä‘Ãºng trong cáº£ setting NL vÃ  PL.

4.3 Táº¡o TÃ i liá»‡u MÃ£ nguá»“n
Máº·c dÃ¹ má»¥c tiÃªu pre-training cá»§a Code-
BERT khÃ´ng bao gá»“m cÃ¡c má»¥c tiÃªu dá»±a trÃªn sinh 
(Lewis et al., 2019), chÃºng tÃ´i muá»‘n Ä‘iá»u tra 
má»©c Ä‘á»™ CodeBERT hoáº¡t Ä‘á»™ng trÃªn cÃ¡c nhiá»‡m vá»¥ 
sinh. Cá»¥ thá»ƒ, chÃºng tÃ´i nghiÃªn cá»©u sinh code-to-NL, 
vÃ  bÃ¡o cÃ¡o káº¿t quáº£ cho nhiá»‡m vá»¥ táº¡o tÃ i liá»‡u 
trÃªn Corpus CodeSearchNet trong 
sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh. VÃ¬ cÃ¡c tÃ i liá»‡u 
Ä‘Æ°á»£c táº¡o ra ngáº¯n vÃ  cÃ¡c n-gram báº­c cao hÆ¡n 
cÃ³ thá»ƒ khÃ´ng trÃ¹ng khá»›p, chÃºng tÃ´i kháº¯c phá»¥c váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng 
Ä‘iá»ƒm BLEU Ä‘Æ°á»£c lÃ m má»‹n (Lin and Och, 2004).

7VÃ­ dá»¥ Ä‘áº¿n tá»« https://
github.com/peri-source/peri/blob/
61beed5deaaf978ab31ed716e8470d86ba639867/
peri/comp/psfcalc.py#L994-L1002

defvec_to_halfvec (vec):
d =vec[1:] -vec[:-1]
if((d/d.mean ()).std() >1e-14) or(d.mean () <0):
raise ValueError ('vec pháº£i lÃ  np.arange () theo thá»© tá»± tÄƒng dáº§n' )
dx =d.mean ()
lowest =np.abs(vec). min ()
highest =np.abs(vec).max ()
return np.arange (lowest, highest +0.1* dx, dx). astype (vec.dtype )"Biáº¿n Ä‘á»•i má»™t vector np.arange( -N, M, dx) thÃ nh np.arange( min (|vec|), 
max(N,M),dx)]"

token NL bá»‹ che
token PL bá»‹ che

max min less greater
NLRoberta 96.24% 3.73% 0.02% 0.01%
CodeBERT (MLM) 39.38% 60.60% 0.02% 0.0003%
PLRoberta 95.85% 4.15% - -
CodeBERT (MLM) 0.001% 99.999% - -

HÃ¬nh 3: NghiÃªn cá»©u trÆ°á»ng há»£p vá» ngÃ´n ngá»¯ python. CÃ¡c token 
bá»‹ che trong NL (mÃ u xanh) vÃ  PL (mÃ u vÃ ng) Ä‘Æ°á»£c Ã¡p dá»¥ng 
riÃªng biá»‡t. XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a RoBERTa vÃ  Code-
BERT Ä‘Æ°á»£c Ä‘Æ°a ra.

So sÃ¡nh MÃ´ hÃ¬nh ChÃºng tÃ´i so sÃ¡nh mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i 
vá»›i má»™t sá»‘ baseline, bao gá»“m má»™t mÃ´ hÃ¬nh 
dá»±a trÃªn RNN vá»›i cÆ¡ cháº¿ attention (Sutskever et al., 
2014), Transformer (Vaswani et al., 2017), 
RoBERTa vÃ  mÃ´ hÃ¬nh pre-trained chá»‰ trÃªn mÃ£ nguá»“n. 
Äá»ƒ chá»©ng minh tÃ­nh hiá»‡u quáº£ cá»§a CodeBERT 
trÃªn cÃ¡c nhiá»‡m vá»¥ sinh code-to-NL, chÃºng tÃ´i Ã¡p dá»¥ng cÃ¡c 
mÃ´ hÃ¬nh pre-trained khÃ¡c nhau lÃ m encoder vÃ  giá»¯ cÃ¡c siÃªu-
tham sá»‘ nháº¥t quÃ¡n. CÃ¡c siÃªu tham sá»‘ chi tiáº¿t 
Ä‘Æ°á»£c Ä‘Æ°a ra trong Phá»¥ lá»¥c B.3.

Báº£ng 4 hiá»ƒn thá»‹ káº¿t quáº£ vá»›i cÃ¡c mÃ´ hÃ¬nh 
khÃ¡c nhau cho nhiá»‡m vá»¥ táº¡o tÃ i liá»‡u code-to-documentation. 
NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, cÃ¡c mÃ´ hÃ¬nh pre-trained trÃªn ngÃ´n ngá»¯ 
láº­p trÃ¬nh vÆ°á»£t trá»™i hÆ¡n RoBERTa, Ä‘iá»u nÃ y minh há»a 
ráº±ng pre-training mÃ´ hÃ¬nh trÃªn ngÃ´n ngá»¯ láº­p trÃ¬nh

--- TRANG 8 ---
MÃ” HÃŒNH RUBY JAVASCRIPT GO PYTHON JAVA PHP Tá»”NG THá»‚
SEQ2SEQ 9.64 10.21 13.98 15.93 15.09 21.08 14.32
TRANSFORMER 11.18 11.59 16.38 15.81 16.26 22.12 15.56
ROBERT A 11.17 11.90 17.72 18.14 16.47 24.02 16.57
PRE-TRAIN W /CODE ONLY 11.91 13.99 17.78 18.58 17.50 24.34 17.35
CODEBERT ( RTD) 11.42 13.27 17.53 18.29 17.35 24.10 17.00
CODEBERT ( MLM ) 11.57 14.41 17.78 18.77 17.38 24.85 17.46
CODEBERT ( RTD+MLM ) 12.16 14.90 18.07 19.06 17.65 25.16 17.83

Báº£ng 4: Káº¿t quáº£ trÃªn sinh Code-to-Documentation, Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng Ä‘iá»ƒm BLEU-4 Ä‘Æ°á»£c lÃ m má»‹n.

cÃ³ thá»ƒ cáº£i thiá»‡n sinh code-to-NL. 
BÃªn cáº¡nh Ä‘Ã³, káº¿t quáº£ trong Báº£ng 4 cho tháº¥y CodeBERT 
pre-trained vá»›i cÃ¡c má»¥c tiÃªu RTD vÃ  MLM mang láº¡i 
má»™t cáº£i thiá»‡n 1.3 Ä‘iá»ƒm BLEU so vá»›i RoBERTa tá»•ng thá»ƒ 
vÃ  Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tiÃªn tiáº¿n nháº¥t8.

4.4 Tá»•ng quÃ¡t hÃ³a Ä‘áº¿n NgÃ´n ngá»¯ Láº­p trÃ¬nh 
KHÃ”NG cÃ³ trong Pre-training
ChÃºng tÃ´i muá»‘n Ä‘Ã¡nh giÃ¡ CodeBERT trÃªn ngÃ´n ngá»¯ láº­p trÃ¬nh 
chÆ°a bao giá» Ä‘Æ°á»£c tháº¥y trong bÆ°á»›c 
pre-training. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu nÃ y, chÃºng tÃ´i nghiÃªn cá»©u nhiá»‡m vá»¥ táº¡o 
má»™t tÃ³m táº¯t ngÃ´n ngá»¯ tá»± nhiÃªn cá»§a má»™t Ä‘oáº¡n mÃ£ nguá»“n 
C#. ChÃºng tÃ´i tiáº¿n hÃ nh thá»±c nghiá»‡m trÃªn dataset 
cá»§a CodeNN (Iyer et al., 2016)9, bao gá»“m 
66,015 cáº·p cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i Ä‘Æ°á»£c thu tháº­p 
tá»± Ä‘á»™ng tá»« StackOverflow. Dataset nÃ y 
khÃ³ khÄƒn vÃ¬ quy mÃ´ cá»§a dataset nhá» hÆ¡n 
nhiá»u báº­c so vá»›i Corpus CodeSearchNet. 
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng Ä‘iá»ƒm BLEU-4 Ä‘Æ°á»£c lÃ m má»‹n 
vÃ  sá»­ dá»¥ng cÃ¡c script Ä‘Ã¡nh giÃ¡ tÆ°Æ¡ng tá»± nhÆ° Iyer et al. 
(2016).

MÃ” HÃŒNH BLEU
MOSES (K OEHN ET AL ., 2007) 11.57
IR 13.66
SUM-NN (R USH ET AL ., 2015) 19.31
2-LAYER BILSTM 19.78
TRANSFORMER (VASWANI ET AL ., 2017) 19.68
TREELSTM (T AI ET AL ., 2015) 20.11
CODENN (I YER ET AL ., 2016) 20.53
CODE 2SEQ (ALON ET AL ., 2019) 23.04
ROBERT A 19.81
PRE-TRAIN W /CODE ONLY 20.65
CODEBERT (RTD) 22.14
CODEBERT (MLM) 22.32
CODEBERT (MLM+RTD) 22.36

Báº£ng 5: Sinh Code-to-NL trÃªn ngÃ´n ngá»¯ C#.

So sÃ¡nh MÃ´ hÃ¬nh Báº£ng 5 cho tháº¥y mÃ´ hÃ¬nh 
cá»§a chÃºng tÃ´i vá»›i cÃ¡c má»¥c tiÃªu pre-training MLM vÃ  RTD 
Ä‘áº¡t Ä‘Æ°á»£c 22.36 Ä‘iá»ƒm BLEU vÃ  cáº£i thiá»‡n 2.55 
Ä‘iá»ƒm so vá»›i RoBERTa, Ä‘iá»u nÃ y minh há»a CodeBERT

8ChÃºng tÃ´i tiáº¿p tá»¥c Ä‘Æ°a ra má»™t sá»‘ vÃ­ dá»¥ Ä‘áº§u ra trong Phá»¥ lá»¥c E.
9https://github.com/sriniiyer/codenn

cÃ³ thá»ƒ tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n Ä‘áº¿n ngÃ´n ngá»¯ láº­p trÃ¬nh khÃ¡c 
chÆ°a bao giá» Ä‘Æ°á»£c tháº¥y trong bÆ°á»›c pre-training. 
Tuy nhiÃªn, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tháº¥p hÆ¡n má»™t chÃºt 
so vá»›i code2seq (Alon et al., 2019). LÃ½ do chÃ­nh 
cÃ³ thá»ƒ lÃ  code2seq sá»­ dá»¥ng cÃ¡c Ä‘Æ°á»ng dáº«n compositional 
trong cÃ¢y cÃº phÃ¡p trá»«u tÆ°á»£ng (AST) cá»§a nÃ³ trong khi Code-
BERT chá»‰ láº¥y mÃ£ nguá»“n gá»‘c lÃ m Ä‘áº§u vÃ o. ChÃºng tÃ´i 
Ä‘Ã£ huáº¥n luyá»‡n má»™t phiÃªn báº£n CodeBERT báº±ng cÃ¡ch duyá»‡t 
cáº¥u trÃºc cÃ¢y cá»§a AST theo má»™t thá»© tá»± nháº¥t Ä‘á»‹nh, 
nhÆ°ng viá»‡c Ã¡p dá»¥ng mÃ´ hÃ¬nh Ä‘Ã³ khÃ´ng mang láº¡i cáº£i thiá»‡n 
trÃªn cÃ¡c nhiá»‡m vá»¥ sinh. Äiá»u nÃ y cho tháº¥y má»™t hÆ°á»›ng 
tiá»m nÄƒng Ä‘á»ƒ cáº£i thiá»‡n CodeBERT báº±ng cÃ¡ch káº¿t há»£p 
AST.

5 Káº¿t luáº­n
Trong paper nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y CodeBERT, theo hiá»ƒu biáº¿t 
tá»‘t nháº¥t cá»§a chÃºng tÃ´i lÃ  mÃ´ hÃ¬nh pre-trained hai chiá»u 
lá»›n Ä‘áº§u tiÃªn cho ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  ngÃ´n ngá»¯ 
láº­p trÃ¬nh. ChÃºng tÃ´i huáº¥n luyá»‡n CodeBERT trÃªn cáº£ 
dá»¯ liá»‡u hai chiá»u vÃ  má»™t chiá»u, vÃ  cho tháº¥y fine-
tuning CodeBERT Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tiÃªn tiáº¿n nháº¥t 
trÃªn cÃ¡c nhiá»‡m vá»¥ downstream bao gá»“m tÃ¬m kiáº¿m mÃ£ nguá»“n 
báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  táº¡o code-to-documentation. 
Äá»ƒ tiáº¿p tá»¥c Ä‘iá»u tra kiáº¿n thá»©c Ä‘Æ°á»£c thá»ƒ hiá»‡n 
trong cÃ¡c mÃ´ hÃ¬nh pre-trained, chÃºng tÃ´i cÃ´ng thá»©c hÃ³a nhiá»‡m vá»¥ 
thÄƒm dÃ² NL-PL vÃ  táº¡o má»™t dataset cho thÄƒm dÃ². 
ChÃºng tÃ´i coi nhiá»‡m vá»¥ thÄƒm dÃ² nhÆ° má»™t bÃ i toÃ¡n lá»±a chá»n cÃ¢u tráº£ lá»i 
dáº¡ng cloze-style, vÃ  tuyá»ƒn chá»n cÃ¡c distractor cho cáº£ 
pháº§n NL vÃ  PL. Káº¿t quáº£ cho tháº¥y, vá»›i cÃ¡c tham sá»‘ mÃ´ hÃ¬nh 
Ä‘Æ°á»£c cá»‘ Ä‘á»‹nh, CodeBERT hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n 
RoBERTa vÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n tiáº¿p tá»¥c chá»‰ sá»­ dá»¥ng 
mÃ£ nguá»“n.

CÃ³ nhiá»u hÆ°á»›ng tiá»m nÄƒng cho nghiÃªn cá»©u 
tiáº¿p theo trong lÄ©nh vá»±c nÃ y. Thá»© nháº¥t, cÃ³ thá»ƒ há»c cÃ¡c 
generators tá»‘t hÆ¡n vá»›i báº±ng chá»©ng hai chiá»u hoáº·c kiáº¿n trÃºc 
neural phá»©c táº¡p hÆ¡n Ä‘á»ƒ cáº£i thiá»‡n má»¥c tiÃªu phÃ¡t hiá»‡n token 
thay tháº¿. Thá»© hai, cÃ¡c hÃ m loss 
cá»§a CodeBERT chá»§ yáº¿u nháº¯m vÃ o cÃ¡c nhiá»‡m vá»¥ hiá»ƒu 
NL-PL. Máº·c dÃ¹ CodeBERT Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm 
BLEU máº¡nh trÃªn táº¡o code-to-documentation, 
báº£n thÃ¢n CodeBERT cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n thÃªm 
báº±ng cÃ¡c má»¥c tiÃªu há»c liÃªn quan Ä‘áº¿n sinh.

--- TRANG 9 ---
CÃ¡ch káº¿t há»£p thÃ nh cÃ´ng AST vÃ o bÆ°á»›c 
pre-training cÅ©ng lÃ  má»™t hÆ°á»›ng háº¥p dáº«n. Thá»© ba, 
chÃºng tÃ´i cÃ³ káº¿ hoáº¡ch Ã¡p dá»¥ng CodeBERT vÃ o nhiá»u nhiá»‡m vá»¥ 
liÃªn quan Ä‘áº¿n NL-PL hÆ¡n, vÃ  má»Ÿ rá»™ng nÃ³ sang nhiá»u ngÃ´n ngá»¯ 
láº­p trÃ¬nh hÆ¡n. CÃ¡c phÆ°Æ¡ng phÃ¡p thÃ­ch á»©ng domain/ngÃ´n ngá»¯ 
linh hoáº¡t vÃ  máº¡nh máº½ lÃ  cáº§n thiáº¿t Ä‘á»ƒ tá»•ng quÃ¡t hÃ³a 
tá»‘t.

Lá»i cáº£m Æ¡n
Xiaocheng Feng lÃ  tÃ¡c giáº£ liÃªn há»‡ cá»§a cÃ´ng viá»‡c 
nÃ y. ChÃºng tÃ´i cáº£m Æ¡n cÃ¡c nhÃ  pháº£n biá»‡n áº©n danh vÃ¬ nhá»¯ng 
bÃ¬nh luáº­n sÃ¢u sáº¯c cá»§a há». Zhangyin Feng, Xiaocheng 
Feng, Bing Qin vÃ  Ting Liu Ä‘Æ°á»£c há»— trá»£ bá»Ÿi 
ChÆ°Æ¡ng trÃ¬nh R&D Trá»ng Ä‘iá»ƒm Quá»‘c gia Trung Quá»‘c thÃ´ng qua grant 
2018YFB1005103 vÃ  Quá»¹ Khoa há»c Tá»± nhiÃªn Quá»‘c gia 
Trung Quá»‘c (NSFC) thÃ´ng qua grant 61632011 
vÃ  61772156.

TÃ i liá»‡u tham kháº£o
Uri Alon, Shaked Brody, Omer Levy, vÃ  Eran Yahav. 
2019. code2seq: Táº¡o chuá»—i tá»« biá»ƒu diá»…n 
cÃ³ cáº¥u trÃºc cá»§a mÃ£ nguá»“n. International Confer-
ence on Learning Representations .

Kyunghyun Cho, Bart Van Merri Â¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger 
Schwenk, vÃ  Yoshua Bengio. 2014. Há»c 
biá»ƒu diá»…n cá»¥m tá»« sá»­ dá»¥ng rnn encoder-decoder 
cho dá»‹ch mÃ¡y thá»‘ng kÃª. arXiv preprint 
arXiv:1406.1078 .

Kevin Clark, Minh-Thang Luong, Quoc V . Le, vÃ  
Christopher D. Manning. 2020. fELECTRAg: Pre-
training text encoders nhÆ° discriminator thay vÃ¬ 
generators. In International Conference on Learn-
ing Representations .

Jacob Devlin, Ming-Wei Chang, Kenton Lee, vÃ  
Kristina Toutanova. 2018. Bert: Pre-training cá»§a deep 
bidirectional transformers cho hiá»ƒu ngÃ´n ngá»¯. 
arXiv preprint arXiv:1810.04805 .

Xiaodong Gu, Hongyu Zhang, vÃ  Sunghun Kim. 2018. 
Deep code search. In 2018 IEEE/ACM 40th Interna-
tional Conference on Software Engineering (ICSE) , 
pages 933â€“944. IEEE.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis 
Allamanis, vÃ  Marc Brockschmidt. 2019. Code-
searchnet challenge: ÄÃ¡nh giÃ¡ state of seman-
tic code search. arXiv preprint arXiv:1909.09436 .

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, vÃ  
Luke Zettlemoyer. 2016. TÃ³m táº¯t mÃ£ nguá»“n 
sá»­ dá»¥ng mÃ´ hÃ¬nh attention neural. In Proceedings 
of the 54th Annual Meeting of the Association for 
Computational Linguistics (Volume 1: Long Papers) , 
pages 2073â€“2083.

Dan Jurafsky. 2000. Speech & language processing . 
Pearson Education India.

Aditya Kanade, Petros Maniatis, Gogul Balakrish-
nan, vÃ  Kensen Shi. 2019. Pre-trained contex-
tual embedding cá»§a mÃ£ nguá»“n. arXiv preprint 
arXiv:2001.00059 .

Yoon Kim. 2014. Convolutional neural net-
works cho phÃ¢n loáº¡i cÃ¢u. arXiv preprint 
arXiv:1408.5882 .

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, et al. 2007. Moses: Open source 
toolkit cho dá»‹ch mÃ¡y thá»‘ng kÃª. In Pro-
ceedings of the 45th annual meeting of the associ-
ation for computational linguistics companion vol-
ume proceedings of the demo and poster sessions , 
pages 177â€“180.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer 
Levy, Ves Stoyanov, vÃ  Luke Zettlemoyer. 2019. 
Bart: Denoising sequence-to-sequence pre-training 
cho sinh ngÃ´n ngá»¯ tá»± nhiÃªn, dá»‹ch thuáº­t, vÃ  
hiá»ƒu. arXiv preprint arXiv:1910.13461 .

Chin-Yew Lin vÃ  Franz Josef Och. 2004. Orange: má»™t 
phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c metric Ä‘Ã¡nh giÃ¡ tá»± Ä‘á»™ng 
cho dá»‹ch mÃ¡y. In Proceedings of the 20th 
international conference on Computational Linguis-
tics, page 501. Association for Computational Lin-
guistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, 
Luke Zettlemoyer, vÃ  Veselin Stoyanov. 2019. 
Roberta: Má»™t phÆ°Æ¡ng phÃ¡p pre-training bert Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a 
máº¡nh máº½. arXiv preprint arXiv:1907.11692 .

Jiasen Lu, Dhruv Batra, Devi Parikh, vÃ  Stefan 
Lee. 2019. Vilbert: Pretraining task-agnostic visi-
olinguistic representations cho vision-and-language 
tasks. In Advances in Neural Information Process-
ing Systems , pages 13â€“23.

Bhaskar Mitra, Nick Craswell, et al. 2018. Má»™t giá»›i thiá»‡u 
vá» neural information retrieval. Foundations 
and Trends Rin Information Retrieval , 13(1):1â€“126.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt 
Gardner, Christopher Clark, Kenton Lee, vÃ  Luke 
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. arXiv preprint arXiv:1802.05365 .

Fabio Petroni, Tim Rockt Â¨aschel, Patrick Lewis, Anton 
Bakhtin, Yuxiang Wu, Alexander H Miller, vÃ  Se-
bastian Riedel. 2019. Language models nhÆ° knowl-
edge bases? arXiv preprint arXiv:1909.01066 .

Telmo Pires, Eva Schlinger, vÃ  Dan Garrette. 2019. 
How multilingual is multilingual bert? arXiv 
preprint arXiv:1906.01502 .

--- TRANG 10 ---
Alec Radford, Karthik Narasimhan, Tim Salimans, 
vÃ  Ilya Sutskever. 2018. Cáº£i thiá»‡n hiá»ƒu ngÃ´n ngá»¯ 
báº±ng generative pre-training. URL 
https://s3-us-west-2. amazonaws. com/openai-
assets/researchcovers/languageunsupervised/language
understanding paper. pdf .

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine 
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, 
Wei Li, vÃ  Peter J Liu. 2019. KhÃ¡m phÃ¡ giá»›i háº¡n 
cá»§a transfer learning vá»›i má»™t uniï¬ed text-to-text trans-
former. arXiv preprint arXiv:1910.10683 .

Alexander M Rush, Sumit Chopra, vÃ  Jason We-
ston. 2015. Má»™t mÃ´ hÃ¬nh attention neural cho tÃ³m táº¯t 
cÃ¢u trá»«u tÆ°á»£ng. arXiv preprint 
arXiv:1509.00685 .

Chen Sun, Austin Myers, Carl V ondrick, Kevin Mur-
phy, vÃ  Cordelia Schmid. 2019. Videobert: Má»™t mÃ´ hÃ¬nh joint 
cho há»c biá»ƒu diá»…n video vÃ  ngÃ´n ngá»¯. 
arXiv preprint arXiv:1904.01766 .

Ilya Sutskever, Oriol Vinyals, vÃ  Quoc V Le. 2014. 
Sequence to sequence learning vá»›i neural networks. 
In Advances in neural information processing sys-
tems, pages 3104â€“3112.

Kai Sheng Tai, Richard Socher, vÃ  Christopher D 
Manning. 2015. Cáº£i thiá»‡n biá»ƒu diá»…n ngá»¯ nghÄ©a 
tá»« tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

Alon Talmor, Yanai Elazar, Yoav Goldberg, vÃ  
Jonathan Berant. 2019. olmpicsâ€“on what lan-
guage model pre-training captures. arXiv preprint 
arXiv:1912.13283 .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob 
Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz 
Kaiser, vÃ  Illia Polosukhin. 2017. Attention is all 
you need. In Advances in neural information pro-
cessing systems , pages 5998â€“6008.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V 
Le, Mohammad Norouzi, Wolfgang Macherey, 
Maxim Krikun, Yuan Cao, Qin Gao, Klaus 
Macherey, et al. 2016. Há»‡ thá»‘ng dá»‹ch mÃ¡y neural cá»§a Google: 
Káº¿t ná»‘i khoáº£ng cÃ¡ch giá»¯a dá»‹ch thuáº­t 
con ngÆ°á»i vÃ  mÃ¡y. arXiv preprint 
arXiv:1609.08144 .

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, vÃ  Quoc V Le. 
2019. Xlnet: Generalized autoregressive pretrain-
ing cho hiá»ƒu ngÃ´n ngá»¯. arXiv preprint 
arXiv:1906.08237 .

A Thá»‘ng kÃª Dá»¯ liá»‡u
Thá»‘ng kÃª dá»¯ liá»‡u cá»§a cÃ¡c pháº§n chia training/validation/testing 
cho sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh Ä‘Æ°á»£c Ä‘Æ°a ra trong 
Báº£ng 6.

TÃŒM KIáº¾M MÃƒ NGUá»’N TRAINING DEV TESTING
GO 635,635 28,483 14,291
JAVA 908,886 30,655 26,909
JAVASCRIPT 247,773 16,505 6,483
PHP 1,047,406 52,029 28,391
PYTHON 824,342 46,213 22,176
RUBY 97,580 4,417 2,279

Báº£ng 6: Thá»‘ng kÃª dá»¯ liá»‡u vá» Corpus CodeSearchNet 
cho tÃ¬m kiáº¿m mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn.

B Chi tiáº¿t Huáº¥n luyá»‡n
B.1 Pre-training
ChÃºng tÃ´i huáº¥n luyá»‡n CodeBERT trÃªn má»™t mÃ¡y NVIDIA DGX-2 
sá»­ dá»¥ng FP16. NÃ³ káº¿t há»£p 16 
NVIDIA Tesla V100 káº¿t ná»‘i vá»›i nhau vá»›i bá»™ nhá»› 32GB. ChÃºng tÃ´i sá»­ dá»¥ng 
táº­p há»£p siÃªu tham sá»‘ sau Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh: 
batchsize lÃ  2,048 vÃ  learning rate lÃ  5e-4. ChÃºng tÃ´i 
sá»­ dá»¥ng Adam Ä‘á»ƒ cáº­p nháº­t cÃ¡c tham sá»‘ vÃ  Ä‘áº·t sá»‘ lÆ°á»£ng 
warmup steps lÃ  10K. ChÃºng tÃ´i Ä‘áº·t max length 
lÃ  512 vÃ  max training step lÃ  100K. Huáº¥n luyá»‡n 
1,000 batch dá»¯ liá»‡u tá»‘n 600 phÃºt vá»›i má»¥c tiÃªu MLM, 
120 phÃºt vá»›i má»¥c tiÃªu RTD.

B.2 CodeSearch
Trong bÆ°á»›c fine-tuning, chÃºng tÃ´i Ä‘áº·t learning rate lÃ  
1e-5, batch size lÃ  64, max sequence length 
lÃ  200 vÃ  max fine-tuning epoch lÃ  8. TÆ°Æ¡ng tá»± 
nhÆ° pre-training, chÃºng tÃ´i sá»­ dá»¥ng Adam Ä‘á»ƒ cáº­p nháº­t 
cÃ¡c tham sá»‘. ChÃºng tÃ´i chá»n mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t 
trÃªn táº­p development, vÃ  sá»­ dá»¥ng nÃ³ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ trÃªn 
táº­p test.

B.3 TÃ³m táº¯t MÃ£ nguá»“n trÃªn SÃ¡u 
NgÃ´n ngá»¯ Láº­p trÃ¬nh
ChÃºng tÃ´i sá»­ dá»¥ng Transformer vá»›i 6 lá»›p, 768 chiá»u 
hidden states vÃ  12 attention heads lÃ m decoder 
trong táº¥t cáº£ cÃ¡c setting. ChÃºng tÃ´i Ä‘áº·t max length cá»§a Ä‘áº§u vÃ o 
vÃ  suy luáº­n lÃ  256 vÃ  64, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i sá»­ dá»¥ng 
optimizer Adam Ä‘á»ƒ cáº­p nháº­t cÃ¡c tham sá»‘ mÃ´ hÃ¬nh. 
Learning rate vÃ  batch size lÃ  5e-5 vÃ  
64, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i tinh chá»‰nh siÃªu tham sá»‘ vÃ  
thá»±c hiá»‡n early stopping trÃªn táº­p development.

B.4 TÃ³m táº¯t MÃ£ nguá»“n trÃªn C#
VÃ¬ cÃ¡c phÆ°Æ¡ng phÃ¡p tiÃªn tiáº¿n nháº¥t sá»­ dá»¥ng RNN lÃ m de-
coder, chÃºng tÃ´i chá»n GRU 2 lá»›p vá»›i cÆ¡ cháº¿ 
attention lÃ m decoder Ä‘á»ƒ so sÃ¡nh. ChÃºng tÃ´i 
fine-tune cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng grid search vá»›i táº­p há»£p 
siÃªu tham sá»‘ sau: batchsize trong f32, 
64g vÃ  learning rate trong f2e-5, 5e-5g. ChÃºng tÃ´i bÃ¡o cÃ¡o

--- TRANG 11 ---
con sá»‘ khi cÃ¡c mÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t nháº¥t 
trÃªn táº­p development.

C ÄÆ°á»ng cong Há»c cá»§a CodeSearch
Tá»« HÃ¬nh 4, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng CodeBERT hoáº¡t Ä‘á»™ng 
tá»‘t hÆ¡n á»Ÿ giai Ä‘oáº¡n Ä‘áº§u, Ä‘iá»u nÃ y pháº£n Ã¡nh ráº±ng 
CodeBERT cung cáº¥p khá»Ÿi táº¡o tá»‘t cho viá»‡c há»c 
cÃ¡c nhiá»‡m vá»¥ downstream.

1 2 3 4 5 6 7 8
Sá»‘ Epoch84.585.085.586.086.587.087.588.088.5Dev Accuracy cá»§a Python
Roberta
CodeBERT
Pre-train w/ code only

1 2 3 4 5 6 7 8
Sá»‘ Epoch79.580.080.581.081.582.082.583.0Dev Accuracy cá»§a Java
Roberta
CodeBERT
Pre-train w/ code only

HÃ¬nh 4: ÄÆ°á»ng cong há»c cá»§a cÃ¡c mÃ´ hÃ¬nh pre-trained 
khÃ¡c nhau trong bÆ°á»›c fine-tuning. ChÃºng tÃ´i hiá»ƒn thá»‹ káº¿t quáº£ trÃªn Python 
vÃ  Java.

D Late Fusion
Trong pháº§n x4.1 , chÃºng tÃ´i cho tháº¥y ráº±ng CodeBERT hoáº¡t Ä‘á»™ng 
tá»‘t trong setting nÆ¡i ngÃ´n ngá»¯ tá»± nhiÃªn 
vÃ  mÃ£ nguá»“n cÃ³ tÆ°Æ¡ng tÃ¡c sá»›m. á»ž Ä‘Ã¢y, chÃºng tÃ´i Ä‘iá»u tra 
liá»‡u CodeBERT cÃ³ tá»‘t trong viá»‡c hoáº¡t Ä‘á»™ng 
nhÆ° má»™t encoder thá»‘ng nháº¥t hay khÃ´ng. ChÃºng tÃ´i Ã¡p dá»¥ng CodeBERT cho 
tÃ¬m kiáº¿m mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn trong setting fusion muá»™n hÆ¡n, 
nÆ¡i CodeBERT trÆ°á»›c tiÃªn encode NL vÃ  PL 
riÃªng biá»‡t, vÃ  sau Ä‘Ã³ tÃ­nh similarity báº±ng dot-
product. Theo cÃ¡ch nÃ y, tÃ¬m kiáº¿m mÃ£ nguá»“n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i 
tÃ¬m cÃ¡c mÃ£ nguá»“n gáº§n nháº¥t trong khÃ´ng gian vector chung. 
Ká»‹ch báº£n nÃ y cÅ©ng táº¡o thuáº­n lá»£i cho viá»‡c sá»­ dá»¥ng CodeBERT 
trong há»‡ thá»‘ng online, nÆ¡i cÃ¡c biá»ƒu diá»…n cá»§a 
mÃ£ nguá»“n Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c. Trong runtime, má»™t 
há»‡ thá»‘ng chá»‰ cáº§n tÃ­nh biá»ƒu diá»…n 
cá»§a NL vÃ  dot-product dá»±a trÃªn vector.

ChÃºng tÃ´i fine-tune CodeBERT vá»›i má»¥c tiÃªu 
sau, tá»‘i Ä‘a hÃ³a dot-product cá»§a 
ground truth trong khi tá»‘i thiá»ƒu hÃ³a dot-product cá»§a 
cÃ¡c distractor.

 1
NX
ilogexp 
Enc(ci)|Enc(wi)
P
jexp 
Enc(cj)|Enc(wi)
(15)

Káº¿t quáº£ Ä‘Æ°á»£c Ä‘Æ°a ra trong Báº£ng 7. ChÃºng tÃ´i chá»‰ thá»±c hiá»‡n setting 
nÃ y trÃªn hai ngÃ´n ngá»¯ vá»›i lÆ°á»£ng dá»¯ liá»‡u 
tÆ°Æ¡ng Ä‘á»‘i nhá».

MÃ” HÃŒNH RUBY GO
ROBERT A 0.0043 0.0030
PRE-TRAIN W /CODE ONLY 0.1648 0.4179
CODEBERT 0.6870 0.8372

Báº£ng 7: Káº¿t quáº£ trÃªn tÃ¬m kiáº¿m mÃ£ nguá»“n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn báº±ng 
late fusion.

ChÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng CodeBERT hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n 
RoBERTa vÃ  mÃ´ hÃ¬nh pre-trained chá»‰ vá»›i mÃ£ nguá»“n. 
VÃ  late fusion hoáº¡t Ä‘á»™ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i 
cÃ¡ch tiÃªu chuáº©n. HÆ¡n ná»¯a, late fusion hiá»‡u quáº£ hÆ¡n 
vÃ  setting nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trong há»‡ thá»‘ng 
online.

E NghiÃªn cá»©u TrÆ°á»ng há»£p
Äá»ƒ phÃ¢n tÃ­ch Ä‘á»‹nh tÃ­nh tÃ­nh hiá»‡u quáº£ cá»§a Code-
BERT, chÃºng tÃ´i Ä‘Æ°a ra má»™t sá»‘ trÆ°á»ng há»£p cho tÃ¬m kiáº¿m mÃ£ nguá»“n vÃ  
cÃ¡c nhiá»‡m vá»¥ táº¡o tÃ i liá»‡u mÃ£ nguá»“n.

Xem xÃ©t khÃ´ng gian háº¡n cháº¿, chÃºng tÃ´i chá»‰ Ä‘Æ°a ra 
top2 káº¿t quáº£ cá»§a truy váº¥n cho ngÃ´n ngá»¯ láº­p trÃ¬nh python. 
NhÆ° hiá»ƒn thá»‹ trong HÃ¬nh 5, káº¿t quáº£ tÃ¬m kiáº¿m ráº¥t 
liÃªn quan vá»›i truy váº¥n.

HÃ¬nh 6 vÃ  HÃ¬nh 7 hiá»ƒn thá»‹ cÃ¡c Ä‘áº§u ra vá»›i 
cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau cho nhiá»‡m vá»¥ táº¡o tÃ i liá»‡u 
mÃ£ nguá»“n. NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y, CodeBERT hoáº¡t Ä‘á»™ng 
tá»‘t hÆ¡n táº¥t cáº£ baselines.

--- TRANG 12 ---
HÃ¬nh 5: VÃ­ dá»¥ CodeSearch Python. Káº¿t quáº£ Ä‘Æ°á»£c tÃ¬m kiáº¿m tá»« 1,156,085 dá»¯ liá»‡u mÃ£ nguá»“n python. ChÃºng tÃ´i chá»‰ Ä‘Æ°a ra 
top2 káº¿t quáº£ vÃ¬ khÃ´ng gian bá»‹ giá»›i háº¡n.

HÃ¬nh 6: VÃ­ dá»¥ Ä‘áº§u ra táº¡o tÃ i liá»‡u mÃ£ nguá»“n Java.

HÃ¬nh 7: VÃ­ dá»¥ Ä‘áº§u ra táº¡o tÃ i liá»‡u mÃ£ nguá»“n Python.