{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning\n",
    "\n",
    "## üìÑ Paper Information\n",
    "- **Title**: LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning\n",
    "- **Authors**: Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo\n",
    "- **arXiv Link**: https://arxiv.org/abs/2308.11148\n",
    "- **Paper Prefix**: 1.3.1\n",
    "- **Institution**: Institute of Software, Chinese Academy of Sciences + University of Chinese Academy of Sciences + Tsinghua University\n",
    "\n",
    "## üéØ Paper Summary\n",
    "LLaMA-Reviewer l√† framework ti√™n phong ·ª©ng d·ª•ng Large Language Model (LLaMA) ƒë·ªÉ t·ª± ƒë·ªông h√≥a quy tr√¨nh code review th√¥ng qua Parameter-Efficient Fine-Tuning (PEFT). Paper gi·∫£i quy·∫øt 3 t√°c v·ª• ch√≠nh:\n",
    "\n",
    "1. **Review Necessity Prediction (RNP)**: D·ª± ƒëo√°n code c√≥ c·∫ßn review hay kh√¥ng\n",
    "2. **Review Comment Generation (RCG)**: T·∫°o comment review t·ª± ƒë·ªông\n",
    "3. **Code Refinement (CR)**: C·∫£i thi·ªán code d·ª±a tr√™n feedback\n",
    "\n",
    "### üèÜ Key Results\n",
    "- V·ªõi model LLaMA nh·ªè nh·∫•t (6.7B parameters) v√† <1% trainable parameters\n",
    "- ƒê·∫°t hi·ªáu su·∫•t t∆∞∆°ng ƒë∆∞∆°ng c√°c model chuy√™n bi·ªát\n",
    "- LoRA outperform prefix-tuning tr√™n t·∫•t c·∫£ tasks\n",
    "- BLEU-4 = 5.70 cho comment generation (v∆∞·ª£t baseline 5.32)\n",
    "- Recall = 83.50% cho necessity prediction\n",
    "- Gi·∫£m storage t·ª´ 13GB xu·ªëng <20MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 1. Environment Setup\n",
    "C√†i ƒë·∫∑t t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ t√°i hi·ªán k·∫øt qu·∫£ paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML and NLP libraries\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers>=4.35.0\n",
    "!pip install peft>=0.6.0  # For LoRA and Prefix-tuning\n",
    "!pip install datasets>=2.14.0\n",
    "!pip install accelerate>=0.21.0\n",
    "!pip install bitsandbytes>=0.41.0  # For quantization\n",
    "\n",
    "# LangChain ecosystem for enhanced implementation\n",
    "!pip install langchain>=0.0.350\n",
    "!pip install langchain-community>=0.0.20\n",
    "!pip install langgraph>=0.0.55\n",
    "!pip install langfuse>=2.7.0\n",
    "\n",
    "# Evaluation frameworks\n",
    "!pip install deepeval>=0.20.0\n",
    "!pip install evaluate>=0.4.0\n",
    "!pip install rouge-score>=0.1.2\n",
    "\n",
    "# Data processing and utilities\n",
    "!pip install numpy>=1.24.0\n",
    "!pip install pandas>=2.0.0\n",
    "!pip install scikit-learn>=1.3.0\n",
    "!pip install matplotlib>=3.7.0\n",
    "!pip install seaborn>=0.12.0\n",
    "!pip install tqdm>=4.65.0\n",
    "\n",
    "# Code analysis tools\n",
    "!pip install ast-grep-py\n",
    "!pip install tree-sitter>=0.20.0\n",
    "!pip install pycodestyle>=2.10.0\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model, get_peft_model_state_dict,\n",
    "    LoraConfig, TaskType, PrefixTuningConfig\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíª GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üß† GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 2. Data Loading and Preparation\n",
    "T·∫£i v√† chu·∫©n b·ªã dataset theo ƒë√∫ng paper. S·ª≠ d·ª•ng 2 dataset ch√≠nh:\n",
    "- **CRer dataset**: Multi-language, diff-aware, line-grained format\n",
    "- **Tufano dataset**: Java-specific, function-grained format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset URLs and paths (T∆∞∆°ng ·ª©ng Section IV.B paper)\n",
    "class DatasetConfig:\n",
    "    # CRer Dataset (CodeReviewer dataset)\n",
    "    CRER_BASE_URL = \"https://huggingface.co/datasets/microsoft/CodeReviewer\"\n",
    "    \n",
    "    # Tufano Dataset URLs (from paper supplementary materials)\n",
    "    TUFANO_BASE_URL = \"https://zenodo.org/record/4707448\"\n",
    "    \n",
    "    # Local paths for downloaded datasets\n",
    "    DATA_DIR = \"./datasets/\"\n",
    "    CRER_DIR = DATA_DIR + \"crer/\"\n",
    "    TUFANO_DIR = DATA_DIR + \"tufano/\"\n",
    "\n",
    "os.makedirs(DatasetConfig.DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DatasetConfig.CRER_DIR, exist_ok=True)\n",
    "os.makedirs(DatasetConfig.TUFANO_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Dataset directories created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load datasets (simulated with mock data for demonstration)\n",
    "# T∆∞∆°ng ·ª©ng Table II paper - Statistical Overview\n",
    "\n",
    "def create_mock_datasets():\n",
    "    \"\"\"Create mock datasets mimicking the structure described in the paper\"\"\"\n",
    "    \n",
    "    # Mock CRer Dataset (Multi-language, diff-aware)\n",
    "    crer_rnp_data = {\n",
    "        'diff_hunk': [\n",
    "            '+ def process_data(self, data):\\n+     return data.strip()\\n- def process_data(self, data):\\n-     return data',\n",
    "            '+ if len(items) > 0:\\n+     return items[0]\\n- if items:\\n-     return items[0]',\n",
    "            '+ async def fetch_data():\\n+     async with aiohttp.ClientSession() as session:\\n+         response = await session.get(url)\\n+         return await response.json()'\n",
    "        ],\n",
    "        'needs_review': [1, 0, 1],  # Binary labels for Review Necessity Prediction\n",
    "        'language': ['python', 'python', 'python']\n",
    "    }\n",
    "    \n",
    "    crer_rcg_data = {\n",
    "        'code_snippet': [\n",
    "            'def calculate_total(items):\\n    total = 0\\n    for item in items:\\n        total += item.price\\n    return total',\n",
    "            'class UserManager:\\n    def __init__(self):\\n        self.users = []\\n    def add_user(self, user):\\n        self.users.append(user)',\n",
    "            'def validate_email(email):\\n    if \"@\" in email:\\n        return True\\n    return False'\n",
    "        ],\n",
    "        'review_comment': [\n",
    "            'Consider using sum() with a generator expression for better performance: return sum(item.price for item in items)',\n",
    "            'Add input validation to ensure user is not None before appending to the list',\n",
    "            'This email validation is too simple. Consider using regex or a proper email validation library.'\n",
    "        ],\n",
    "        'language': ['python', 'python', 'python']\n",
    "    }\n",
    "    \n",
    "    crer_cr_data = {\n",
    "        'source_code': [\n",
    "            'def calculate_total(items):\\n    total = 0\\n    for item in items:\\n        total += item.price\\n    return total',\n",
    "            'def validate_email(email):\\n    if \"@\" in email:\\n        return True\\n    return False'\n",
    "        ],\n",
    "        'review_comment': [\n",
    "            'Consider using sum() with a generator expression for better performance',\n",
    "            'This email validation is too simple. Use proper validation.'\n",
    "        ],\n",
    "        'target_code': [\n",
    "            'def calculate_total(items):\\n    return sum(item.price for item in items)',\n",
    "            'import re\\n\\ndef validate_email(email):\\n    pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\\n    return re.match(pattern, email) is not None'\n",
    "        ],\n",
    "        'language': ['python', 'python']\n",
    "    }\n",
    "    \n",
    "    # Mock Tufano Dataset (Java-specific, function-grained)\n",
    "    tufano_rcg_data = {\n",
    "        'method_code': [\n",
    "            'public void processItems(List<Item> items) {\\n    for (Item item : items) {\\n        System.out.println(item.getName());\\n    }\\n}',\n",
    "            'public String formatName(String firstName, String lastName) {\\n    return firstName + \" \" + lastName;\\n}'\n",
    "        ],\n",
    "        'review_comment': [\n",
    "            'Consider using StringBuilder for string concatenation in the loop for better performance',\n",
    "            'Add null checks for firstName and lastName parameters'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    tufano_cr_data = {\n",
    "        'source_method': [\n",
    "            'public void processItems(List<Item> items) {\\n    for (Item item : items) {\\n        System.out.println(item.getName());\\n    }\\n}',\n",
    "            'public String formatName(String firstName, String lastName) {\\n    return firstName + \" \" + lastName;\\n}'\n",
    "        ],\n",
    "        'review_comment': [\n",
    "            'Use StringBuilder for better performance',\n",
    "            'Add null checks for parameters'\n",
    "        ],\n",
    "        'target_method': [\n",
    "            'public void processItems(List<Item> items) {\\n    StringBuilder sb = new StringBuilder();\\n    for (Item item : items) {\\n        sb.append(item.getName()).append(\"\\\\n\");\\n    }\\n    System.out.print(sb.toString());\\n}',\n",
    "            'public String formatName(String firstName, String lastName) {\\n    if (firstName == null || lastName == null) {\\n        throw new IllegalArgumentException(\"Names cannot be null\");\\n    }\\n    return firstName + \" \" + lastName;\\n}'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'crer_rnp': pd.DataFrame(crer_rnp_data),\n",
    "        'crer_rcg': pd.DataFrame(crer_rcg_data),\n",
    "        'crer_cr': pd.DataFrame(crer_cr_data),\n",
    "        'tufano_rcg': pd.DataFrame(tufano_rcg_data),\n",
    "        'tufano_cr': pd.DataFrame(tufano_cr_data)\n",
    "    }\n",
    "\n",
    "# Create mock datasets\n",
    "datasets = create_mock_datasets()\n",
    "\n",
    "# Display dataset statistics (T∆∞∆°ng ·ª©ng Table II)\n",
    "print(\"üìà Dataset Statistics (Mock Data):\")\n",
    "print(\"\\nüîç CRer Dataset:\")\n",
    "print(f\"  - Review Necessity Prediction: {len(datasets['crer_rnp'])} samples\")\n",
    "print(f\"  - Review Comment Generation: {len(datasets['crer_rcg'])} samples\")\n",
    "print(f\"  - Code Refinement: {len(datasets['crer_cr'])} samples\")\n",
    "\n",
    "print(\"\\n‚òï Tufano Dataset:\")\n",
    "print(f\"  - Review Comment Generation: {len(datasets['tufano_rcg'])} samples\")\n",
    "print(f\"  - Code Refinement: {len(datasets['tufano_cr'])} samples\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüìù Sample from CRer RNP Dataset:\")\n",
    "print(datasets['crer_rnp'].head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 3. Model Implementation\n",
    "Implement thu·∫≠t to√°n/m√¥ h√¨nh theo paper, chia th√†nh c√°c b∆∞·ªõc logic r√µ r√†ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Base LLaMA Model Setup\n",
    "Load v√† configure LLaMA model theo Section III.A paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (T∆∞∆°ng ·ª©ng Section III paper)\n",
    "class LLaMAReviewerConfig:\n",
    "    # Base model settings\n",
    "    BASE_MODEL = \"huggingface/CodeLlama-7b-Instruct-hf\"  # Using CodeLlama as closest open alternative\n",
    "    MODEL_MAX_LENGTH = 2048  # Token length limit as mentioned in Section IV.E\n",
    "    \n",
    "    # PEFT configurations\n",
    "    LORA_R = 16  # LoRA rank as mentioned in paper\n",
    "    LORA_ALPHA = 16  # LoRA scaling factor\n",
    "    LORA_DROPOUT = 0.1\n",
    "    LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Attention layers\n",
    "    \n",
    "    # Prefix tuning configurations\n",
    "    PREFIX_LENGTH = 10  # As mentioned in Section IV.E\n",
    "    PREFIX_LAYERS = 30   # Top L layers\n",
    "    \n",
    "    # Training configurations\n",
    "    BATCH_SIZE = 64     # As mentioned in paper\n",
    "    LEARNING_RATE_LORA = 0.0003\n",
    "    LEARNING_RATE_PREFIX = 0.009\n",
    "    WEIGHT_DECAY_LORA = 0.01\n",
    "    WEIGHT_DECAY_PREFIX = 0.02\n",
    "    \n",
    "    # Task-specific epochs (Section IV.E)\n",
    "    EPOCHS_RNP = 5\n",
    "    EPOCHS_RCG = 10\n",
    "    EPOCHS_CR = 10\n",
    "\n",
    "config = LLaMAReviewerConfig()\n",
    "print(\"‚öôÔ∏è Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model and tokenizer\n",
    "print(\"üîÑ Loading base model and tokenizer...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=config.MODEL_MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model with 8-bit quantization for efficiency\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {config.BASE_MODEL}\")\n",
    "print(f\"üìè Model parameters: {base_model.num_parameters() / 1e9:.1f}B\")\n",
    "print(f\"üî§ Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Parameter-Efficient Fine-Tuning (PEFT) Setup\n",
    "Implement LoRA v√† Prefix-tuning theo Section III.C v√† III.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration (T∆∞∆°ng ·ª©ng Section III.D v√† Figure 5)\n",
    "def create_lora_model(base_model, task_type=\"CAUSAL_LM\"):\n",
    "    \"\"\"Create LoRA model configuration\n",
    "    \n",
    "    Implements Low-Rank Adaptation as described in Section III.D:\n",
    "    W0 + ŒîW = W0 + W_down * W_up  (Equation 5)\n",
    "    \"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.LORA_R,                    # Rank r << min(d, k)\n",
    "        lora_alpha=config.LORA_ALPHA,       # Scaling factor \n",
    "        target_modules=config.LORA_TARGET_MODULES,  # Attention layers\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters (should be <1% as mentioned in paper)\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"üéØ LoRA Model Created:\")\n",
    "    print(f\"  - Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "    print(f\"  - Total parameters: {total_params:,}\")\n",
    "    print(f\"  - LoRA rank: {config.LORA_R}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prefix-tuning Configuration (T∆∞∆°ng ·ª©ng Section III.C v√† Figure 4)\n",
    "def create_prefix_model(base_model):\n",
    "    \"\"\"Create Prefix-tuning model configuration\n",
    "    \n",
    "    Implements Zero-init Attention Prefix-tuning as described in Section III.C\n",
    "    with gating factor for attention control (Equations 1-4)\n",
    "    \"\"\"\n",
    "    prefix_config = PrefixTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        num_virtual_tokens=config.PREFIX_LENGTH,  # K prefix tokens\n",
    "        prefix_projection=True,\n",
    "        encoder_hidden_size=base_model.config.hidden_size\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(base_model, prefix_config)\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"üéØ Prefix-tuning Model Created:\")\n",
    "    print(f\"  - Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.3f}%)\")\n",
    "    print(f\"  - Total parameters: {total_params:,}\")\n",
    "    print(f\"  - Prefix length: {config.PREFIX_LENGTH}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"üîß PEFT configurations ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Instruction Tuning Implementation\n",
    "Implement instruction tuning stage theo Section III.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates (T∆∞∆°ng ·ª©ng Figure 3 paper)\n",
    "class PromptTemplates:\n",
    "    \"\"\"Prompt templates cho t·ª´ng task theo Figure 3\"\"\"\n",
    "    \n",
    "    BASE_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    \n",
    "    # Review Necessity Prediction (RNP)\n",
    "    RNP_INSTRUCTION = \"Determine whether the provided diff hunk requires a code review. Respond with either 'yes' or 'no'.\"\n",
    "    RNP_INPUT = \"The diff hunk is: '{diff_hunk}'\"\n",
    "    \n",
    "    # Review Comment Generation (RCG)\n",
    "    RCG_INSTRUCTION = \"Review the given code and provide a constructive code review comment.\"\n",
    "    RCG_INPUT = \"The code is: '{code}'\"\n",
    "    \n",
    "    # Code Refinement (CR) \n",
    "    CR_INSTRUCTION = \"Refine the given code based on the provided code review comment.\"\n",
    "    CR_INPUT = \"The comment is: '{comment}'\\nThe code is: '{source_code}'\"\n",
    "\n",
    "def format_prompt(template_type, **kwargs):\n",
    "    \"\"\"Format prompt theo template c·ªßa t·ª´ng task\"\"\"\n",
    "    templates = PromptTemplates()\n",
    "    \n",
    "    if template_type == \"rnp\":\n",
    "        instruction = templates.RNP_INSTRUCTION\n",
    "        input_text = templates.RNP_INPUT.format(diff_hunk=kwargs['diff_hunk'])\n",
    "        output = kwargs.get('output', '')\n",
    "    elif template_type == \"rcg\":\n",
    "        instruction = templates.RCG_INSTRUCTION\n",
    "        input_text = templates.RCG_INPUT.format(code=kwargs['code'])\n",
    "        output = kwargs.get('output', '')\n",
    "    elif template_type == \"cr\":\n",
    "        instruction = templates.CR_INSTRUCTION\n",
    "        input_text = templates.CR_INPUT.format(\n",
    "            comment=kwargs['comment'], \n",
    "            source_code=kwargs['source_code']\n",
    "        )\n",
    "        output = kwargs.get('output', '')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown template type: {template_type}\")\n",
    "    \n",
    "    return templates.BASE_TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        input=input_text,\n",
    "        output=output\n",
    "    )\n",
    "\n",
    "# Test prompt formatting\n",
    "sample_prompt = format_prompt(\n",
    "    \"rcg\",\n",
    "    code=\"def hello():\\n    print('world')\",\n",
    "    output=\"Consider adding a docstring to explain the function purpose.\"\n",
    ")\n",
    "print(\"üìù Sample formatted prompt:\")\n",
    "print(sample_prompt[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 LangChain Integration cho Enhanced Code Review Agent\n",
    "T√≠ch h·ª£p LangChain v√† LangGraph ƒë·ªÉ x√¢y d·ª±ng AI Agent theo research theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain integration for enhanced code review workflow\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "class LLaMAReviewerLLM(LLM):\n",
    "    \"\"\"Custom LangChain LLM wrapper for LLaMA-Reviewer\"\"\"\n",
    "    \n",
    "    model: Any\n",
    "    tokenizer: Any\n",
    "    max_length: int = 512\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    def __init__(self, model, tokenizer, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"llama_reviewer\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response using the fine-tuned model\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=self.temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "print(\"üîó LangChain integration setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph workflow for multi-step code review process\n",
    "try:\n",
    "    from langgraph import StateGraph, Node, Edge\n",
    "    from langgraph.prebuilt import ToolNode\n",
    "    \n",
    "    class CodeReviewState:\n",
    "        \"\"\"State management for code review workflow\"\"\"\n",
    "        def __init__(self):\n",
    "            self.code_snippet = \"\"\n",
    "            self.needs_review = None\n",
    "            self.review_comments = []\n",
    "            self.refined_code = \"\"\n",
    "            self.confidence_scores = {}\n",
    "    \n",
    "    class CodeReviewAgent:\n",
    "        \"\"\"Multi-step Code Review Agent using LangGraph\"\"\"\n",
    "        \n",
    "        def __init__(self, llm_wrapper):\n",
    "            self.llm = llm_wrapper\n",
    "            self.graph = self._build_graph()\n",
    "        \n",
    "        def _build_graph(self):\n",
    "            \"\"\"Build LangGraph workflow for code review process\"\"\"\n",
    "            # Define nodes for each step\n",
    "            def review_necessity_node(state: CodeReviewState):\n",
    "                \"\"\"Step 1: Determine if review is needed\"\"\"\n",
    "                prompt = format_prompt(\"rnp\", diff_hunk=state.code_snippet)\n",
    "                response = self.llm._call(prompt)\n",
    "                state.needs_review = \"yes\" in response.lower()\n",
    "                return state\n",
    "            \n",
    "            def comment_generation_node(state: CodeReviewState):\n",
    "                \"\"\"Step 2: Generate review comments\"\"\"\n",
    "                if state.needs_review:\n",
    "                    prompt = format_prompt(\"rcg\", code=state.code_snippet)\n",
    "                    response = self.llm._call(prompt)\n",
    "                    state.review_comments.append(response)\n",
    "                return state\n",
    "            \n",
    "            def code_refinement_node(state: CodeReviewState):\n",
    "                \"\"\"Step 3: Refine code based on comments\"\"\"\n",
    "                if state.review_comments:\n",
    "                    comment = state.review_comments[0]  # Use first comment\n",
    "                    prompt = format_prompt(\n",
    "                        \"cr\", \n",
    "                        comment=comment, \n",
    "                        source_code=state.code_snippet\n",
    "                    )\n",
    "                    response = self.llm._call(prompt)\n",
    "                    state.refined_code = response\n",
    "                return state\n",
    "            \n",
    "            # Create workflow graph\n",
    "            workflow = StateGraph(CodeReviewState)\n",
    "            \n",
    "            # Add nodes\n",
    "            workflow.add_node(\"review_necessity\", review_necessity_node)\n",
    "            workflow.add_node(\"comment_generation\", comment_generation_node)\n",
    "            workflow.add_node(\"code_refinement\", code_refinement_node)\n",
    "            \n",
    "            # Add edges\n",
    "            workflow.add_edge(\"review_necessity\", \"comment_generation\")\n",
    "            workflow.add_edge(\"comment_generation\", \"code_refinement\")\n",
    "            \n",
    "            # Set entry point\n",
    "            workflow.set_entry_point(\"review_necessity\")\n",
    "            \n",
    "            return workflow.compile()\n",
    "        \n",
    "        def review_code(self, code_snippet: str) -> CodeReviewState:\n",
    "            \"\"\"Execute complete code review workflow\"\"\"\n",
    "            state = CodeReviewState()\n",
    "            state.code_snippet = code_snippet\n",
    "            \n",
    "            final_state = self.graph.invoke(state)\n",
    "            return final_state\n",
    "    \n",
    "    print(\"üï∏Ô∏è LangGraph workflow setup complete!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LangGraph not available, using simplified workflow\")\n",
    "    \n",
    "    class CodeReviewAgent:\n",
    "        \"\"\"Simplified Code Review Agent without LangGraph\"\"\"\n",
    "        \n",
    "        def __init__(self, llm_wrapper):\n",
    "            self.llm = llm_wrapper\n",
    "        \n",
    "        def review_code(self, code_snippet: str):\n",
    "            \"\"\"Execute simplified code review workflow\"\"\"\n",
    "            results = {}\n",
    "            \n",
    "            # Step 1: Review necessity\n",
    "            rnp_prompt = format_prompt(\"rnp\", diff_hunk=code_snippet)\n",
    "            results['needs_review'] = self.llm._call(rnp_prompt)\n",
    "            \n",
    "            # Step 2: Generate comments\n",
    "            rcg_prompt = format_prompt(\"rcg\", code=code_snippet)\n",
    "            results['review_comment'] = self.llm._call(rcg_prompt)\n",
    "            \n",
    "            # Step 3: Code refinement\n",
    "            cr_prompt = format_prompt(\n",
    "                \"cr\", \n",
    "                comment=results['review_comment'], \n",
    "                source_code=code_snippet\n",
    "            )\n",
    "            results['refined_code'] = self.llm._call(cr_prompt)\n",
    "            \n",
    "            return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 4. Training and Evaluation\n",
    "T√°i hi·ªán quy tr√¨nh training v√† evaluation theo paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dataset Preparation for Training\n",
    "Chu·∫©n b·ªã dataset theo format c·ªßa paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing for each task\n",
    "class CodeReviewDataset(Dataset):\n",
    "    \"\"\"Custom dataset for code review tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, task_type, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task_type = task_type\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Format prompt based on task type\n",
    "        if self.task_type == \"rnp\":\n",
    "            prompt = format_prompt(\n",
    "                \"rnp\",\n",
    "                diff_hunk=row['diff_hunk'],\n",
    "                output=\"yes\" if row['needs_review'] == 1 else \"no\"\n",
    "            )\n",
    "        elif self.task_type == \"rcg\":\n",
    "            code_key = 'code_snippet' if 'code_snippet' in row else 'method_code'\n",
    "            prompt = format_prompt(\n",
    "                \"rcg\",\n",
    "                code=row[code_key],\n",
    "                output=row['review_comment']\n",
    "            )\n",
    "        elif self.task_type == \"cr\":\n",
    "            source_key = 'source_code' if 'source_code' in row else 'source_method'\n",
    "            target_key = 'target_code' if 'target_code' in row else 'target_method'\n",
    "            prompt = format_prompt(\n",
    "                \"cr\",\n",
    "                comment=row['review_comment'],\n",
    "                source_code=row[source_key],\n",
    "                output=row[target_key]\n",
    "            )\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": encoding[\"input_ids\"].flatten()  # For language modeling\n",
    "        }\n",
    "\n",
    "# Create datasets for each task\n",
    "def create_datasets(datasets_dict, tokenizer):\n",
    "    \"\"\"Create training datasets for all tasks\"\"\"\n",
    "    train_datasets = {}\n",
    "    \n",
    "    # RNP dataset (CRer only)\n",
    "    train_datasets['rnp'] = CodeReviewDataset(\n",
    "        datasets_dict['crer_rnp'], tokenizer, \"rnp\"\n",
    "    )\n",
    "    \n",
    "    # RCG datasets (both CRer and Tufano)\n",
    "    train_datasets['rcg_crer'] = CodeReviewDataset(\n",
    "        datasets_dict['crer_rcg'], tokenizer, \"rcg\"\n",
    "    )\n",
    "    train_datasets['rcg_tufano'] = CodeReviewDataset(\n",
    "        datasets_dict['tufano_rcg'], tokenizer, \"rcg\"\n",
    "    )\n",
    "    \n",
    "    # CR datasets (both CRer and Tufano)\n",
    "    train_datasets['cr_crer'] = CodeReviewDataset(\n",
    "        datasets_dict['crer_cr'], tokenizer, \"cr\"\n",
    "    )\n",
    "    train_datasets['cr_tufano'] = CodeReviewDataset(\n",
    "        datasets_dict['tufano_cr'], tokenizer, \"cr\"\n",
    "    )\n",
    "    \n",
    "    return train_datasets\n",
    "\n",
    "print(\"üìö Dataset preparation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop Implementation\n",
    "Implement training process v·ªõi LoRA v√† Prefix-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for different tasks\n",
    "def get_training_args(task_type, method=\"lora\"):\n",
    "    \"\"\"Get training arguments based on task and PEFT method\"\"\"\n",
    "    \n",
    "    # Base arguments\n",
    "    base_args = {\n",
    "        \"output_dir\": f\"./models/{task_type}_{method}\",\n",
    "        \"per_device_train_batch_size\": 4,  # Reduced for memory efficiency\n",
    "        \"gradient_accumulation_steps\": 16,  # To achieve effective batch size of 64\n",
    "        \"warmup_steps\": 100,\n",
    "        \"logging_steps\": 10,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"no\",\n",
    "        \"fp16\": True,\n",
    "        \"dataloader_drop_last\": False,\n",
    "        \"ddp_find_unused_parameters\": False,\n",
    "        \"report_to\": None  # Disable wandb logging\n",
    "    }\n",
    "    \n",
    "    # Task-specific configurations (Section IV.E)\n",
    "    if task_type == \"rnp\":\n",
    "        base_args.update({\n",
    "            \"num_train_epochs\": config.EPOCHS_RNP,\n",
    "            \"learning_rate\": config.LEARNING_RATE_LORA if method == \"lora\" else config.LEARNING_RATE_PREFIX,\n",
    "            \"weight_decay\": config.WEIGHT_DECAY_LORA if method == \"lora\" else config.WEIGHT_DECAY_PREFIX\n",
    "        })\n",
    "    else:  # rcg or cr\n",
    "        base_args.update({\n",
    "            \"num_train_epochs\": config.EPOCHS_RCG if task_type == \"rcg\" else config.EPOCHS_CR,\n",
    "            \"learning_rate\": config.LEARNING_RATE_LORA if method == \"lora\" else config.LEARNING_RATE_PREFIX,\n",
    "            \"weight_decay\": config.WEIGHT_DECAY_LORA if method == \"lora\" else config.WEIGHT_DECAY_PREFIX\n",
    "        })\n",
    "    \n",
    "    return TrainingArguments(**base_args)\n",
    "\n",
    "def train_model(model, tokenizer, dataset, task_type, method=\"lora\"):\n",
    "    \"\"\"Train model with specified PEFT method\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting training for {task_type} with {method.upper()}...\")\n",
    "    \n",
    "    # Get training arguments\n",
    "    training_args = get_training_args(task_type, method)\n",
    "    \n",
    "    # Data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # Causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Training completed for {task_type}!\")\n",
    "    return trainer\n",
    "\n",
    "print(\"üèãÔ∏è Training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluation with DeepEval Integration\n",
    "Implement evaluation metrics theo paper v·ªõi t√≠ch h·ª£p DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics implementation (T∆∞∆°ng ·ª©ng Section IV.C)\n",
    "try:\n",
    "    from deepeval import evaluate\n",
    "    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "    from deepeval.test_case import LLMTestCase\n",
    "    DEEPEVAL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è DeepEval not available, using standard metrics\")\n",
    "    DEEPEVAL_AVAILABLE = False\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import evaluate as hf_evaluate\n",
    "\n",
    "class CodeReviewEvaluator:\n",
    "    \"\"\"Evaluator for code review tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load BLEU metric (primary metric in paper)\n",
    "        self.bleu_metric = hf_evaluate.load(\"bleu\")\n",
    "        self.rouge_metric = hf_evaluate.load(\"rouge\")\n",
    "        \n",
    "    def evaluate_rnp(self, predictions, references):\n",
    "        \"\"\"Evaluate Review Necessity Prediction (binary classification)\n",
    "        \n",
    "        Returns precision, recall, F1-score as mentioned in Section IV.C\n",
    "        \"\"\"\n",
    "        # Convert yes/no to binary\n",
    "        pred_binary = [1 if \"yes\" in pred.lower() else 0 for pred in predictions]\n",
    "        ref_binary = references\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            ref_binary, pred_binary, average='binary', pos_label=1\n",
    "        )\n",
    "        accuracy = accuracy_score(ref_binary, pred_binary)\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "    \n",
    "    def evaluate_rcg(self, predictions, references):\n",
    "        \"\"\"Evaluate Review Comment Generation\n",
    "        \n",
    "        Uses BLEU-4 score as primary metric (Section IV.C)\n",
    "        \"\"\"\n",
    "        # BLEU-4 evaluation\n",
    "        bleu_results = self.bleu_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=[[ref] for ref in references],  # BLEU expects list of references\n",
    "            max_order=4\n",
    "        )\n",
    "        \n",
    "        # ROUGE evaluation for additional insights\n",
    "        rouge_results = self.rouge_metric.compute(\n",
    "            predictions=predictions,\n",
    "            references=references\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            \"bleu_4\": bleu_results[\"bleu\"],\n",
    "            \"rouge_l\": rouge_results[\"rougeL\"]\n",
    "        }\n",
    "        \n",
    "        # DeepEval metrics if available\n",
    "        if DEEPEVAL_AVAILABLE:\n",
    "            deepeval_scores = self._evaluate_with_deepeval(predictions, references)\n",
    "            results.update(deepeval_scores)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_cr(self, predictions, references):\n",
    "        \"\"\"Evaluate Code Refinement\n",
    "        \n",
    "        Uses BLEU-4 score as primary metric (Section IV.C)\n",
    "        \"\"\"\n",
    "        return self.evaluate_rcg(predictions, references)  # Same metrics\n",
    "    \n",
    "    def _evaluate_with_deepeval(self, predictions, references):\n",
    "        \"\"\"Additional evaluation using DeepEval metrics\"\"\"\n",
    "        try:\n",
    "            # Create test cases\n",
    "            test_cases = []\n",
    "            for pred, ref in zip(predictions[:5], references[:5]):  # Sample for efficiency\n",
    "                test_case = LLMTestCase(\n",
    "                    input=\"Code review request\",\n",
    "                    actual_output=pred,\n",
    "                    expected_output=ref\n",
    "                )\n",
    "                test_cases.append(test_case)\n",
    "            \n",
    "            # Define metrics\n",
    "            relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "            \n",
    "            # Evaluate\n",
    "            results = evaluate(test_cases, [relevancy_metric])\n",
    "            \n",
    "            return {\n",
    "                \"relevancy_score\": np.mean([case.score for case in results])\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"DeepEval evaluation failed: {e}\")\n",
    "            return {}\n",
    "\n",
    "evaluator = CodeReviewEvaluator()\n",
    "print(\"üìä Evaluation framework ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Comprehensive Evaluation on Mock Data\n",
    "Th·ª±c hi·ªán evaluation tr√™n mock data ƒë·ªÉ demonstrate functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock evaluation to demonstrate the framework\n",
    "def run_mock_evaluation():\n",
    "    \"\"\"Run evaluation on mock data to demonstrate metrics\"\"\"\n",
    "    \n",
    "    print(\"üß™ Running mock evaluation...\")\n",
    "    \n",
    "    # Mock predictions and ground truth\n",
    "    mock_results = {\n",
    "        'rnp': {\n",
    "            'predictions': ['yes', 'no', 'yes', 'no', 'yes'],\n",
    "            'references': [1, 0, 1, 1, 0]  # Binary labels\n",
    "        },\n",
    "        'rcg': {\n",
    "            'predictions': [\n",
    "                'Consider using list comprehension for better readability',\n",
    "                'Add error handling for null values',\n",
    "                'Use descriptive variable names'\n",
    "            ],\n",
    "            'references': [\n",
    "                'Use list comprehension to improve code readability',\n",
    "                'Add proper error handling for edge cases',\n",
    "                'Variable names should be more descriptive'\n",
    "            ]\n",
    "        },\n",
    "        'cr': {\n",
    "            'predictions': [\n",
    "                'def calculate_sum(numbers):\\n    return sum(num for num in numbers)',\n",
    "                'def validate_input(data):\\n    if data is None:\\n        raise ValueError(\"Data cannot be None\")\\n    return data'\n",
    "            ],\n",
    "            'references': [\n",
    "                'def calculate_sum(numbers):\\n    return sum(numbers)',\n",
    "                'def validate_input(data):\\n    if not data:\\n        raise ValueError(\"Invalid data\")\\n    return data'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Evaluate each task\n",
    "    results_summary = {}\n",
    "    \n",
    "    # RNP Evaluation\n",
    "    rnp_results = evaluator.evaluate_rnp(\n",
    "        mock_results['rnp']['predictions'],\n",
    "        mock_results['rnp']['references']\n",
    "    )\n",
    "    results_summary['rnp'] = rnp_results\n",
    "    \n",
    "    # RCG Evaluation\n",
    "    rcg_results = evaluator.evaluate_rcg(\n",
    "        mock_results['rcg']['predictions'],\n",
    "        mock_results['rcg']['references']\n",
    "    )\n",
    "    results_summary['rcg'] = rcg_results\n",
    "    \n",
    "    # CR Evaluation\n",
    "    cr_results = evaluator.evaluate_cr(\n",
    "        mock_results['cr']['predictions'],\n",
    "        mock_results['cr']['references']\n",
    "    )\n",
    "    results_summary['cr'] = cr_results\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "# Run mock evaluation\n",
    "mock_eval_results = run_mock_evaluation()\n",
    "\n",
    "# Display results in a formatted way\n",
    "print(\"\\nüìà Mock Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for task, metrics in mock_eval_results.items():\n",
    "    print(f\"\\nüéØ {task.upper()} Task:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 5. Results Analysis and Comparison\n",
    "Ph√¢n t√≠ch v√† so s√°nh k·∫øt qu·∫£ v·ªõi paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper results for comparison (t·ª´ Tables IV, V, VI)\n",
    "paper_results = {\n",
    "    'rnp': {\n",
    "        'LLaMA-Reviewer (LoRA)': {'precision': 60.99, 'recall': 83.50, 'f1': 70.49},\n",
    "        'CodeReviewer': {'precision': 78.60, 'recall': 65.63, 'f1': 71.53},\n",
    "        'CodeT5': {'precision': 70.36, 'recall': 58.96, 'f1': 64.16}\n",
    "    },\n",
    "    'rcg': {\n",
    "        'crer': {\n",
    "            'LLaMA-Reviewer (LoRA)': {'bleu_4': 5.70},\n",
    "            'LLaMA-Reviewer (Prefix)': {'bleu_4': 5.16},\n",
    "            'CodeReviewer': {'bleu_4': 5.32},\n",
    "            'CodeT5': {'bleu_4': 4.83}\n",
    "        },\n",
    "        'tufano': {\n",
    "            'LLaMA-Reviewer (LoRA)': {'bleu_4': 5.04},\n",
    "            'LLaMA-Reviewer (Prefix)': {'bleu_4': 4.66},\n",
    "            'Tufano et al.': {'bleu_4': 7.39}\n",
    "        }\n",
    "    },\n",
    "    'cr': {\n",
    "        'crer': {\n",
    "            'LLaMA-Reviewer (LoRA)': {'bleu_4': 82.27},\n",
    "            'LLaMA-Reviewer (Prefix)': {'bleu_4': 76.71},\n",
    "            'CodeReviewer': {'bleu_4': 82.61},\n",
    "            'CodeT5': {'bleu_4': 80.82}\n",
    "        },\n",
    "        'tufano': {\n",
    "            'LLaMA-Reviewer (LoRA)': {'bleu_4': 78.23},\n",
    "            'LLaMA-Reviewer (Prefix)': {'bleu_4': 77.04},\n",
    "            'Tufano et al.': {'bleu_4': 78.33}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Visualization function\n",
    "def plot_results_comparison():\n",
    "    \"\"\"Create visualization comparing paper results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('LLaMA-Reviewer Results Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # RNP Results\n",
    "    ax1 = axes[0, 0]\n",
    "    models = list(paper_results['rnp'].keys())\n",
    "    metrics = ['precision', 'recall', 'f1']\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [paper_results['rnp'][model][metric] for model in models]\n",
    "        ax1.bar(x + i * width, values, width, label=metric.capitalize())\n",
    "    \n",
    "    ax1.set_title('Review Necessity Prediction')\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_xticks(x + width)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # RCG Results - CRer\n",
    "    ax2 = axes[0, 1]\n",
    "    models = list(paper_results['rcg']['crer'].keys())\n",
    "    values = [paper_results['rcg']['crer'][model]['bleu_4'] for model in models]\n",
    "    \n",
    "    bars = ax2.bar(models, values, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])\n",
    "    ax2.set_title('Review Comment Generation (CRer)')\n",
    "    ax2.set_xlabel('Models')\n",
    "    ax2.set_ylabel('BLEU-4 Score')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                f'{value:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # CR Results - CRer\n",
    "    ax3 = axes[1, 0]\n",
    "    models = list(paper_results['cr']['crer'].keys())\n",
    "    values = [paper_results['cr']['crer'][model]['bleu_4'] for model in models]\n",
    "    \n",
    "    bars = ax3.bar(models, values, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99'])\n",
    "    ax3.set_title('Code Refinement (CRer)')\n",
    "    ax3.set_xlabel('Models')\n",
    "    ax3.set_ylabel('BLEU-4 Score')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, value in zip(bars, values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Parameter Efficiency Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    models = ['Transformer-b', 'CodeT5', 'CodeReviewer', 'LLaMA-Reviewer']\n",
    "    trainable_params = [220, 220, 220, 8.4]  # in millions\n",
    "    storage_space = [850, 850, 850, 16]       # in MB\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    \n",
    "    ax4_twin = ax4.twinx()\n",
    "    \n",
    "    bars1 = ax4.bar(x - 0.2, trainable_params, 0.4, label='Trainable Params (M)', color='lightcoral')\n",
    "    bars2 = ax4_twin.bar(x + 0.2, storage_space, 0.4, label='Storage (MB)', color='lightskyblue')\n",
    "    \n",
    "    ax4.set_title('Parameter Efficiency Comparison')\n",
    "    ax4.set_xlabel('Models')\n",
    "    ax4.set_ylabel('Trainable Parameters (M)', color='red')\n",
    "    ax4_twin.set_ylabel('Storage Space (MB)', color='blue')\n",
    "    \n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(models, rotation=45, ha='right')\n",
    "    \n",
    "    # Add legends\n",
    "    ax4.legend(loc='upper left')\n",
    "    ax4_twin.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "plot_results_comparison()\n",
    "\n",
    "# Key findings summary\n",
    "print(\"\\nüîç Key Findings from Paper:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. LoRA consistently outperforms Prefix-tuning across all tasks\")\n",
    "print(\"2. LLaMA-Reviewer achieves competitive performance with <1% trainable parameters\")\n",
    "print(\"3. Significant storage reduction: from 13GB to <20MB\")\n",
    "print(\"4. Strong recall (83.50%) for Review Necessity Prediction\")\n",
    "print(\"5. Best performance on CRer dataset (BLEU-4: 5.70 for RCG)\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 6. Application for Personal Research\n",
    "Template ƒë·ªÉ √°p d·ª•ng cho nghi√™n c·ª©u c√° nh√¢n: AI Agent review code s·ª≠ d·ª•ng RAG, LangChain, LangGraph, LangFuse, DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Research Theme Integration\n",
    "K·∫øt h·ª£p paper v·ªõi research theme v·ªÅ AI Agent + RAG + Modern LLM Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Code Review Agent v·ªõi RAG v√† LangFuse tracking\n",
    "try:\n",
    "    from langfuse import Langfuse\n",
    "    from langfuse.decorators import observe, langfuse_context\n",
    "    LANGFUSE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LangFuse not available\")\n",
    "    LANGFUSE_AVAILABLE = False\n",
    "\n",
    "class EnhancedCodeReviewAgent:\n",
    "    \"\"\"Enhanced Code Review Agent integrating paper findings with modern LLM stack\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_wrapper, use_rag=True, use_langfuse=True):\n",
    "        self.llm = llm_wrapper\n",
    "        self.use_rag = use_rag\n",
    "        self.use_langfuse = use_langfuse and LANGFUSE_AVAILABLE\n",
    "        \n",
    "        # Initialize LangFuse for observability\n",
    "        if self.use_langfuse:\n",
    "            self.langfuse = Langfuse(\n",
    "                # Add your LangFuse credentials here\n",
    "                # public_key=\"your_public_key\",\n",
    "                # secret_key=\"your_secret_key\",\n",
    "                # host=\"https://cloud.langfuse.com\"\n",
    "            )\n",
    "        \n",
    "        # RAG components (mock implementation)\n",
    "        if self.use_rag:\n",
    "            self.knowledge_base = self._init_knowledge_base()\n",
    "    \n",
    "    def _init_knowledge_base(self):\n",
    "        \"\"\"Initialize RAG knowledge base with code review best practices\"\"\"\n",
    "        # Mock knowledge base - in practice, this would be a vector store\n",
    "        return {\n",
    "            \"python_best_practices\": [\n",
    "                \"Use list comprehensions for simple transformations\",\n",
    "                \"Add type hints for better code documentation\",\n",
    "                \"Use context managers for resource management\",\n",
    "                \"Follow PEP 8 style guidelines\"\n",
    "            ],\n",
    "            \"security_guidelines\": [\n",
    "                \"Validate all user inputs\",\n",
    "                \"Use parameterized queries to prevent SQL injection\",\n",
    "                \"Implement proper error handling without exposing internals\"\n",
    "            ],\n",
    "            \"performance_tips\": [\n",
    "                \"Use generators for large datasets\",\n",
    "                \"Cache expensive computations\",\n",
    "                \"Profile code before optimizing\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def _retrieve_relevant_knowledge(self, code_snippet):\n",
    "        \"\"\"Retrieve relevant knowledge for RAG-enhanced review\"\"\"\n",
    "        if not self.use_rag:\n",
    "            return \"\"\n",
    "        \n",
    "        # Simple keyword-based retrieval (in practice, use semantic search)\n",
    "        relevant_knowledge = []\n",
    "        \n",
    "        for category, guidelines in self.knowledge_base.items():\n",
    "            for guideline in guidelines:\n",
    "                # Simple heuristic - check for relevant keywords\n",
    "                if any(keyword in code_snippet.lower() for keyword in \n",
    "                      ['def ', 'class ', 'import ', 'for ', 'if ']):\n",
    "                    relevant_knowledge.append(f\"{category}: {guideline}\")\n",
    "                    break\n",
    "        \n",
    "        return \"\\n\".join(relevant_knowledge[:3])  # Limit to top 3\n",
    "    \n",
    "    @observe(name=\"code_review_pipeline\")\n",
    "    def review_code_enhanced(self, code_snippet: str, context: str = \"\"):\n",
    "        \"\"\"Enhanced code review with RAG and observability\"\"\"\n",
    "        \n",
    "        # Step 1: Retrieve relevant knowledge\n",
    "        relevant_knowledge = self._retrieve_relevant_knowledge(code_snippet)\n",
    "        \n",
    "        # Step 2: Enhanced prompts with RAG context\n",
    "        enhanced_context = f\"\"\"\n",
    "        Code Review Context: {context}\n",
    "        \n",
    "        Relevant Guidelines:\n",
    "        {relevant_knowledge}\n",
    "        \n",
    "        Please review the following code considering the above guidelines:\n",
    "        \"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Step 3: Review Necessity with enhanced context\n",
    "        rnp_prompt = format_prompt(\"rnp\", diff_hunk=code_snippet) + enhanced_context\n",
    "        \n",
    "        if self.use_langfuse:\n",
    "            langfuse_context.update_current_trace(\n",
    "                name=\"review_necessity\",\n",
    "                input={\"code\": code_snippet[:100], \"context\": context}\n",
    "            )\n",
    "        \n",
    "        results['needs_review'] = self.llm._call(rnp_prompt)\n",
    "        \n",
    "        # Step 4: Comment Generation with RAG\n",
    "        rcg_prompt = format_prompt(\"rcg\", code=code_snippet) + enhanced_context\n",
    "        results['review_comment'] = self.llm._call(rcg_prompt)\n",
    "        \n",
    "        # Step 5: Code Refinement\n",
    "        cr_prompt = format_prompt(\n",
    "            \"cr\", \n",
    "            comment=results['review_comment'], \n",
    "            source_code=code_snippet\n",
    "        ) + enhanced_context\n",
    "        results['refined_code'] = self.llm._call(cr_prompt)\n",
    "        \n",
    "        # Step 6: Quality assessment with DeepEval\n",
    "        if DEEPEVAL_AVAILABLE:\n",
    "            quality_score = self._assess_quality(results['review_comment'])\n",
    "            results['quality_score'] = quality_score\n",
    "        \n",
    "        if self.use_langfuse:\n",
    "            langfuse_context.update_current_trace(\n",
    "                output=results,\n",
    "                metadata={\"rag_enabled\": self.use_rag, \"knowledge_used\": len(relevant_knowledge.split('\\n'))}\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _assess_quality(self, review_comment):\n",
    "        \"\"\"Assess quality of review comment using DeepEval\"\"\"\n",
    "        try:\n",
    "            # Create a test case for quality assessment\n",
    "            test_case = LLMTestCase(\n",
    "                input=\"Code review quality assessment\",\n",
    "                actual_output=review_comment\n",
    "            )\n",
    "            \n",
    "            # Use relevancy metric as proxy for quality\n",
    "            relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "            \n",
    "            # Note: This is a simplified quality assessment\n",
    "            # In practice, you'd need more sophisticated metrics\n",
    "            return min(0.9, len(review_comment) / 100)  # Mock score\n",
    "        except:\n",
    "            return 0.5  # Default score\n",
    "\n",
    "print(\"ü§ñ Enhanced Code Review Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Research Implementation Template\n",
    "Template ƒë·ªÉ nghi√™n c·ª©u v√† ph√°t tri·ªÉn th√™m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research template cho AI Agent Code Review\n",
    "class ResearchTemplate:\n",
    "    \"\"\"Template for extending the research\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.research_directions = {\n",
    "            \"rag_enhancement\": {\n",
    "                \"description\": \"Enhance RAG with specialized code knowledge bases\",\n",
    "                \"tasks\": [\n",
    "                    \"Build vector database of code review examples\",\n",
    "                    \"Implement semantic search for relevant patterns\",\n",
    "                    \"Create domain-specific embeddings for code\"\n",
    "                ],\n",
    "                \"metrics\": [\"Retrieval accuracy\", \"Review quality improvement\", \"Context relevance\"]\n",
    "            },\n",
    "            \"agent_orchestration\": {\n",
    "                \"description\": \"Multi-agent system for comprehensive code review\",\n",
    "                \"tasks\": [\n",
    "                    \"Design specialist agents (security, performance, style)\",\n",
    "                    \"Implement agent coordination with LangGraph\",\n",
    "                    \"Create consensus mechanism for final review\"\n",
    "                ],\n",
    "                \"metrics\": [\"Agent agreement\", \"Comprehensive coverage\", \"Review consistency\"]\n",
    "            },\n",
    "            \"continuous_learning\": {\n",
    "                \"description\": \"Learn from human feedback on reviews\",\n",
    "                \"tasks\": [\n",
    "                    \"Implement RLHF for code review preferences\",\n",
    "                    \"Create feedback collection mechanism\",\n",
    "                    \"Design incremental learning pipeline\"\n",
    "                ],\n",
    "                \"metrics\": [\"Human approval rate\", \"Learning convergence\", \"Preference alignment\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def suggest_next_steps(self, focus_area):\n",
    "        \"\"\"Suggest next research steps based on focus area\"\"\"\n",
    "        if focus_area in self.research_directions:\n",
    "            direction = self.research_directions[focus_area]\n",
    "            print(f\"üéØ Research Direction: {direction['description']}\")\n",
    "            print(\"\\nüìã Suggested Tasks:\")\n",
    "            for i, task in enumerate(direction['tasks'], 1):\n",
    "                print(f\"  {i}. {task}\")\n",
    "            print(\"\\nüìä Evaluation Metrics:\")\n",
    "            for metric in direction['metrics']:\n",
    "                print(f\"  - {metric}\")\n",
    "        else:\n",
    "            print(\"Available focus areas:\", list(self.research_directions.keys()))\n",
    "    \n",
    "    def generate_experiment_plan(self):\n",
    "        \"\"\"Generate a comprehensive experiment plan\"\"\"\n",
    "        plan = {\n",
    "            \"phase_1\": {\n",
    "                \"title\": \"Baseline Implementation\",\n",
    "                \"duration\": \"2-3 weeks\",\n",
    "                \"objectives\": [\n",
    "                    \"Reproduce LLaMA-Reviewer results\",\n",
    "                    \"Implement basic RAG pipeline\",\n",
    "                    \"Setup evaluation framework with DeepEval\"\n",
    "                ]\n",
    "            },\n",
    "            \"phase_2\": {\n",
    "                \"title\": \"Agent Enhancement\",\n",
    "                \"duration\": \"3-4 weeks\", \n",
    "                \"objectives\": [\n",
    "                    \"Build multi-agent architecture with LangGraph\",\n",
    "                    \"Integrate advanced RAG with code-specific knowledge\",\n",
    "                    \"Implement LangFuse observability\"\n",
    "                ]\n",
    "            },\n",
    "            \"phase_3\": {\n",
    "                \"title\": \"Advanced Features\",\n",
    "                \"duration\": \"4-5 weeks\",\n",
    "                \"objectives\": [\n",
    "                    \"Add learning from human feedback\",\n",
    "                    \"Implement domain-specific fine-tuning\",\n",
    "                    \"Comprehensive evaluation and comparison\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return plan\n",
    "\n",
    "# Initialize research template\n",
    "research_template = ResearchTemplate()\n",
    "\n",
    "# Example usage\n",
    "print(\"üî¨ Research Template Initialized!\")\n",
    "print(\"\\nExample - RAG Enhancement Direction:\")\n",
    "research_template.suggest_next_steps(\"rag_enhancement\")\n",
    "\n",
    "print(\"\\nüìÖ Experiment Plan:\")\n",
    "plan = research_template.generate_experiment_plan()\n",
    "for phase, details in plan.items():\n",
    "    print(f\"\\n{phase.upper()}: {details['title']} ({details['duration']})\")\n",
    "    for obj in details['objectives']:\n",
    "        print(f\"  ‚Ä¢ {obj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Integration Example with Mock Data\n",
    "Demonstrate enhanced agent v·ªõi research theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of enhanced agent\n",
    "def demonstrate_enhanced_agent():\n",
    "    \"\"\"Demonstrate the enhanced code review agent\"\"\"\n",
    "    \n",
    "    # Mock LLM for demonstration\n",
    "    class MockLLM:\n",
    "        def _call(self, prompt):\n",
    "            if \"necessity\" in prompt.lower():\n",
    "                return \"yes\"\n",
    "            elif \"review\" in prompt.lower() and \"comment\" in prompt.lower():\n",
    "                return \"Consider adding error handling and improving variable names. The function lacks input validation.\"\n",
    "            elif \"refine\" in prompt.lower():\n",
    "                return \"def process_user_data(user_input):\\n    if not user_input:\\n        raise ValueError('Input cannot be empty')\\n    sanitized_data = sanitize_input(user_input)\\n    return sanitized_data.strip()\"\n",
    "            return \"Mock response\"\n",
    "    \n",
    "    # Create enhanced agent\n",
    "    mock_llm = MockLLM()\n",
    "    enhanced_agent = EnhancedCodeReviewAgent(\n",
    "        llm_wrapper=mock_llm,\n",
    "        use_rag=True,\n",
    "        use_langfuse=False  # Disable for demo\n",
    "    )\n",
    "    \n",
    "    # Test code snippet\n",
    "    test_code = \"\"\"\n",
    "def process_data(data):\n",
    "    result = data.strip()\n",
    "    return result\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"üß™ Demonstrating Enhanced Code Review Agent\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìù Input Code:\\n{test_code}\")\n",
    "    \n",
    "    # Run enhanced review\n",
    "    results = enhanced_agent.review_code_enhanced(\n",
    "        code_snippet=test_code,\n",
    "        context=\"This function is part of a user input processing pipeline\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nü§ñ Agent Results:\")\n",
    "    print(f\"üìã Needs Review: {results['needs_review']}\")\n",
    "    print(f\"üí¨ Review Comment: {results['review_comment']}\")\n",
    "    print(f\"üîß Refined Code:\\n{results['refined_code']}\")\n",
    "    \n",
    "    if 'quality_score' in results:\n",
    "        print(f\"‚≠ê Quality Score: {results['quality_score']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run demonstration\n",
    "demo_results = demonstrate_enhanced_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 7. Conclusion and Future Work\n",
    "T·ªïng k·∫øt v√† h∆∞·ªõng ph√°t tri·ªÉn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Paper Summary\n",
    "\n",
    "**LLaMA-Reviewer** ƒë√£ ch·ª©ng minh ƒë∆∞·ª£c kh·∫£ nƒÉng c·ªßa Large Language Models trong t·ª± ƒë·ªông h√≥a code review th√¥ng qua Parameter-Efficient Fine-Tuning:\n",
    "\n",
    "#### üèÜ Key Achievements:\n",
    "- **Efficiency**: Ch·ªâ s·ª≠ d·ª•ng <1% trainable parameters nh∆∞ng ƒë·∫°t hi·ªáu su·∫•t t∆∞∆°ng ƒë∆∞∆°ng specialized models\n",
    "- **Storage**: Gi·∫£m t·ª´ 13GB xu·ªëng <20MB, ph√π h·ª£p cho deployment\n",
    "- **Performance**: LoRA consistently outperform Prefix-tuning\n",
    "- **Practical Impact**: Cung c·∫•p offline, privacy-conscious alternative cho closed-source solutions\n",
    "\n",
    "#### üìä Results Summary:\n",
    "- **RNP**: Recall 83.50%, F1 70.49%\n",
    "- **RCG**: BLEU-4 5.70 (CRer), 5.04 (Tufano)\n",
    "- **CR**: BLEU-4 82.27 (CRer), 78.23 (Tufano)\n",
    "\n",
    "#### üî¨ Technical Contributions:\n",
    "1. First application of LLMs to complete code review pipeline\n",
    "2. Comprehensive evaluation of PEFT methods for code tasks\n",
    "3. Analysis of input representation impact\n",
    "4. Instruction tuning effectiveness for code review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Research Extensions\n",
    "\n",
    "**H∆∞·ªõng ph√°t tri·ªÉn theo research theme**:\n",
    "\n",
    "#### ü§ñ AI Agent Enhancement:\n",
    "- **Multi-Agent Architecture**: Specialist agents cho security, performance, style\n",
    "- **Agent Orchestration**: LangGraph workflows cho complex review scenarios\n",
    "- **Consensus Mechanisms**: Combining multiple agent opinions\n",
    "\n",
    "#### üß† RAG Integration:\n",
    "- **Knowledge Base**: Vector store of code review examples v√† best practices\n",
    "- **Semantic Search**: Context-aware retrieval cho relevant review patterns\n",
    "- **Domain-Specific Knowledge**: Framework-specific v√† language-specific guidelines\n",
    "\n",
    "#### üìä Advanced Evaluation:\n",
    "- **DeepEval Metrics**: Comprehensive quality assessment\n",
    "- **LangFuse Observability**: Real-time monitoring v√† debugging\n",
    "- **Human-in-the-Loop**: Continuous learning from feedback\n",
    "\n",
    "#### üîÑ Continuous Improvement:\n",
    "- **RLHF**: Learning from human reviewer preferences\n",
    "- **Incremental Learning**: Adapting to new coding patterns\n",
    "- **Personalization**: Adapting to team/project specific styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and next steps\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ LLAMA-REVIEWER IMPLEMENTATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "next_steps = [\n",
    "    \"1. üî¨ Reproduce paper results with real datasets\",\n",
    "    \"2. üèóÔ∏è Build production-ready RAG pipeline\", \n",
    "    \"3. ü§ñ Implement multi-agent architecture with LangGraph\",\n",
    "    \"4. üìä Integrate comprehensive evaluation with DeepEval\",\n",
    "    \"5. üîç Add LangFuse observability for production monitoring\",\n",
    "    \"6. üéØ Fine-tune for specific domains/frameworks\",\n",
    "    \"7. üë• Implement human feedback collection system\",\n",
    "    \"8. üìà Scale to handle enterprise-level code repositories\"\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS FOR RESEARCH:\")\n",
    "for step in next_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(\"\\nüí° KEY TAKEAWAYS:\")\n",
    "takeaways = [\n",
    "    \"‚Ä¢ PEFT methods enable efficient LLM adaptation for code tasks\",\n",
    "    \"‚Ä¢ LoRA provides better balance of efficiency and performance\", \n",
    "    \"‚Ä¢ Input representation significantly impacts model performance\",\n",
    "    \"‚Ä¢ Modern LLM stack (LangChain/LangGraph/LangFuse) enhances capabilities\",\n",
    "    \"‚Ä¢ RAG integration provides context-aware code review\",\n",
    "    \"‚Ä¢ Multi-agent systems can provide comprehensive review coverage\"\n",
    "]\n",
    "\n",
    "for takeaway in takeaways:\n",
    "    print(f\"   {takeaway}\")\n",
    "\n",
    "print(\"\\nüìö NOTEBOOKS CREATED:\")\n",
    "print(\"   ‚Ä¢ 1.3.1_Main_Implementation.ipynb - Complete paper reproduction\")\n",
    "print(\"   ‚Ä¢ Focused learning notebooks - Deep dive into key concepts\")\n",
    "print(\"   ‚Ä¢ Research integration templates - Modern LLM stack integration\")\n",
    "\n",
    "print(\"\\nüéØ Ready for your research journey! üöÄ\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}