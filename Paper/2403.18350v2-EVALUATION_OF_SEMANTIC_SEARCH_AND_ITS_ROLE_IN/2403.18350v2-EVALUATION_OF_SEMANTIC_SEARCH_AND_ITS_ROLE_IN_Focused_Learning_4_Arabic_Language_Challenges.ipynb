{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning 4: Arabic Language Challenges in Semantic Search and RAG\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "- Deep dive into Arabic language complexities affecting NLP systems\n",
    "- Understand morphological richness and its impact on semantic search\n",
    "- Analyze dialectal variations and cross-dialectal understanding\n",
    "- Implement solutions for Arabic-specific challenges in RAG systems\n",
    "- Master text preprocessing and normalization techniques for Arabic\n",
    "\n",
    "## 📚 Paper Context\n",
    "**Paper Quote**: \"Similar to the majority of research endeavors and NLP tasks, the Arabic language semantic search and RAG lags behind other languages due to the challenges posed by the Arabic language, including its complex morphology, the diversity of its dialects and the shortage of datasets.\"\n",
    "\n",
    "**Key Challenges Identified**:\n",
    "1. **Complex Morphology**: Rich derivational and inflectional system\n",
    "2. **Dialectal Diversity**: MSA vs. regional variations\n",
    "3. **Dataset Shortage**: Limited labeled Arabic datasets\n",
    "4. **Script Characteristics**: Right-to-left, optional diacritics\n",
    "\n",
    "## 🌍 Why Arabic NLP is Uniquely Challenging\n",
    "\n",
    "### Linguistic Complexity:\n",
    "- **Root-Pattern System**: Words derived from 3-4 consonant roots\n",
    "- **Rich Inflection**: Gender, number, case, mood, tense variations\n",
    "- **Agglutination**: Multiple morphemes attached to word stems\n",
    "- **Free Word Order**: Flexible sentence structure\n",
    "\n",
    "### Technical Challenges:\n",
    "- **Tokenization**: Complex word boundaries\n",
    "- **Normalization**: Multiple character forms\n",
    "- **Diacritization**: Meaning-changing diacritics often omitted\n",
    "- **Code-switching**: Mixed Arabic-English in modern texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Arabic NLP libraries\n",
    "try:\n",
    "    import pyarabic.araby as araby\n",
    "    import pyarabic.number as number\n",
    "    PYARABIC_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYARABIC_AVAILABLE = False\n",
    "    print(\"📦 PyArabic not installed - using fallback methods\")\n",
    "\n",
    "# ML libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 Arabic Language Challenges Learning Environment Ready!\")\n",
    "print(f\"📚 PyArabic available: {PYARABIC_AVAILABLE}\")\n",
    "print(\"🔧 Ready to analyze Arabic linguistic phenomena\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Arabic Morphological Complexity Analysis\n",
    "\n",
    "### Understanding the Root-Pattern System and Its Impact on Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicMorphologyAnalyzer:\n",
    "    \"\"\"Comprehensive analysis of Arabic morphological complexity\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common Arabic roots and their derivations\n",
    "        self.root_families = {\n",
    "            'كتب': {  # Root: K-T-B (write)\n",
    "                'root_meaning': 'writing/scribing',\n",
    "                'derivations': [\n",
    "                    {'word': 'كتب', 'form': 'فعل', 'meaning': 'he wrote', 'pos': 'verb'},\n",
    "                    {'word': 'كتابة', 'form': 'فعالة', 'meaning': 'writing', 'pos': 'noun'},\n",
    "                    {'word': 'كاتب', 'form': 'فاعل', 'meaning': 'writer', 'pos': 'noun'},\n",
    "                    {'word': 'مكتوب', 'form': 'مفعول', 'meaning': 'written', 'pos': 'participle'},\n",
    "                    {'word': 'مكتبة', 'form': 'مفعلة', 'meaning': 'library', 'pos': 'noun'},\n",
    "                    {'word': 'مكتب', 'form': 'مفعل', 'meaning': 'office/desk', 'pos': 'noun'},\n",
    "                    {'word': 'كتيب', 'form': 'فعيل', 'meaning': 'booklet', 'pos': 'noun'},\n",
    "                    {'word': 'استكتب', 'form': 'استفعل', 'meaning': 'to ask to write', 'pos': 'verb'}\n",
    "                ]\n",
    "            },\n",
    "            'درس': {  # Root: D-R-S (study)\n",
    "                'root_meaning': 'studying/learning',\n",
    "                'derivations': [\n",
    "                    {'word': 'درس', 'form': 'فعل', 'meaning': 'he studied', 'pos': 'verb'},\n",
    "                    {'word': 'دراسة', 'form': 'فعالة', 'meaning': 'study', 'pos': 'noun'},\n",
    "                    {'word': 'طالب', 'form': 'فاعل', 'meaning': 'student', 'pos': 'noun'},\n",
    "                    {'word': 'مدرسة', 'form': 'مفعلة', 'meaning': 'school', 'pos': 'noun'},\n",
    "                    {'word': 'مدرس', 'form': 'مفعل', 'meaning': 'teacher', 'pos': 'noun'},\n",
    "                    {'word': 'درس', 'form': 'فعل', 'meaning': 'lesson', 'pos': 'noun'},\n",
    "                    {'word': 'تدريس', 'form': 'تفعيل', 'meaning': 'teaching', 'pos': 'noun'}\n",
    "                ]\n",
    "            },\n",
    "            'عمل': {  # Root: ع-م-ل (work)\n",
    "                'root_meaning': 'working/doing',\n",
    "                'derivations': [\n",
    "                    {'word': 'عمل', 'form': 'فعل', 'meaning': 'he worked', 'pos': 'verb'},\n",
    "                    {'word': 'عمل', 'form': 'فعل', 'meaning': 'work', 'pos': 'noun'},\n",
    "                    {'word': 'عامل', 'form': 'فاعل', 'meaning': 'worker', 'pos': 'noun'},\n",
    "                    {'word': 'معمل', 'form': 'مفعل', 'meaning': 'factory/lab', 'pos': 'noun'},\n",
    "                    {'word': 'عملية', 'form': 'فعلية', 'meaning': 'operation', 'pos': 'noun'},\n",
    "                    {'word': 'استعمال', 'form': 'استفعال', 'meaning': 'usage', 'pos': 'noun'},\n",
    "                    {'word': 'تعامل', 'form': 'تفاعل', 'meaning': 'dealing', 'pos': 'noun'}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Arabic verb forms (Verb patterns)\n",
    "        self.verb_forms = {\n",
    "            'I': {'pattern': 'فعل', 'example': 'كتب', 'meaning': 'basic form'},\n",
    "            'II': {'pattern': 'فعّل', 'example': 'كسّر', 'meaning': 'intensive/causative'},\n",
    "            'III': {'pattern': 'فاعل', 'example': 'شارك', 'meaning': 'associative'},\n",
    "            'IV': {'pattern': 'أفعل', 'example': 'أرسل', 'meaning': 'causative'},\n",
    "            'V': {'pattern': 'تفعّل', 'example': 'تعلّم', 'meaning': 'reflexive'},\n",
    "            'VI': {'pattern': 'تفاعل', 'example': 'تشارك', 'meaning': 'reciprocal'},\n",
    "            'VII': {'pattern': 'انفعل', 'example': 'انكسر', 'meaning': 'passive/reflexive'},\n",
    "            'VIII': {'pattern': 'افتعل', 'example': 'اجتمع', 'meaning': 'reflexive'},\n",
    "            'IX': {'pattern': 'افعلّ', 'example': 'احمرّ', 'meaning': 'color/defect'},\n",
    "            'X': {'pattern': 'استفعل', 'example': 'استخدم', 'meaning': 'seeking/requesting'}\n",
    "        }\n",
    "        \n",
    "        # Common Arabic morphological features\n",
    "        self.morphological_features = {\n",
    "            'prefixes': ['ال', 'و', 'ف', 'ب', 'ك', 'ل', 'س', 'ي', 'ت', 'ن', 'أ'],\n",
    "            'suffixes': ['ة', 'ت', 'ك', 'ه', 'ها', 'ان', 'ين', 'ون', 'ات', 'ية'],\n",
    "            'clitics': ['ني', 'ك', 'ه', 'ها', 'كم', 'هم', 'هن']\n",
    "        }\n",
    "    \n",
    "    def analyze_root_family_embeddings(self, model: SentenceTransformer, root: str) -> Dict:\n",
    "        \"\"\"Analyze how embedding models handle morphologically related words\"\"\"\n",
    "        \n",
    "        if root not in self.root_families:\n",
    "            return {}\n",
    "        \n",
    "        family = self.root_families[root]\n",
    "        words = [d['word'] for d in family['derivations']]\n",
    "        meanings = [d['meaning'] for d in family['derivations']]\n",
    "        \n",
    "        print(f\"\\n🔍 Analyzing root family: {root} ({family['root_meaning']})\")\n",
    "        print(f\"📝 Words in family: {len(words)}\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = model.encode(words)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Analyze similarities\n",
    "        family_coherence = self._calculate_family_coherence(similarity_matrix)\n",
    "        \n",
    "        print(f\"📊 Family coherence score: {family_coherence:.4f}\")\n",
    "        \n",
    "        # Show detailed similarities\n",
    "        print(\"\\n🔗 Pairwise similarities:\")\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i+1, len(words)):\n",
    "                sim = similarity_matrix[i, j]\n",
    "                print(f\"  {words[i]} ↔ {words[j]}: {sim:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'root': root,\n",
    "            'words': words,\n",
    "            'meanings': meanings,\n",
    "            'embeddings': embeddings,\n",
    "            'similarity_matrix': similarity_matrix,\n",
    "            'family_coherence': family_coherence,\n",
    "            'derivations': family['derivations']\n",
    "        }\n",
    "    \n",
    "    def _calculate_family_coherence(self, similarity_matrix: np.ndarray) -> float:\n",
    "        \"\"\"Calculate how coherent a morphological family is in embedding space\"\"\"\n",
    "        \n",
    "        # Get upper triangle (avoiding diagonal)\n",
    "        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "        \n",
    "        # Return mean similarity within family\n",
    "        return np.mean(upper_triangle)\n",
    "    \n",
    "    def compare_morphological_variants(self, model: SentenceTransformer) -> Dict:\n",
    "        \"\"\"Compare how models handle different morphological variants\"\"\"\n",
    "        \n",
    "        print(\"\\n🧪 Morphological Variants Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Test cases: same meaning, different morphological forms\n",
    "        variant_tests = [\n",
    "            {\n",
    "                'concept': 'learning',\n",
    "                'variants': ['تعلم', 'تعليم', 'دراسة', 'تحصيل'],\n",
    "                'forms': ['تفعل', 'تفعيل', 'فعالة', 'تفعيل']\n",
    "            },\n",
    "            {\n",
    "                'concept': 'working',\n",
    "                'variants': ['عمل', 'اشتغال', 'وظيفة', 'مهنة'],\n",
    "                'forms': ['فعل', 'افتعال', 'فعيلة', 'فعلة']\n",
    "            },\n",
    "            {\n",
    "                'concept': 'writing',\n",
    "                'variants': ['كتابة', 'تسجيل', 'تدوين', 'رقن'],\n",
    "                'forms': ['فعالة', 'تفعيل', 'تفعيل', 'فعل']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for test in variant_tests:\n",
    "            concept = test['concept']\n",
    "            variants = test['variants']\n",
    "            \n",
    "            print(f\"\\n📝 Testing concept: {concept}\")\n",
    "            print(f\"   Variants: {', '.join(variants)}\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(variants)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Calculate concept coherence\n",
    "            coherence = self._calculate_family_coherence(similarity_matrix)\n",
    "            \n",
    "            print(f\"   📊 Concept coherence: {coherence:.4f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'concept': concept,\n",
    "                'variants': variants,\n",
    "                'forms': test['forms'],\n",
    "                'coherence': coherence,\n",
    "                'similarity_matrix': similarity_matrix\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_inflectional_variations(self, model: SentenceTransformer) -> Dict:\n",
    "        \"\"\"Analyze how models handle Arabic inflectional morphology\"\"\"\n",
    "        \n",
    "        print(\"\\n📊 Inflectional Variations Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Test inflectional variations\n",
    "        inflection_tests = [\n",
    "            {\n",
    "                'base': 'كتاب',\n",
    "                'category': 'number_gender',\n",
    "                'variants': {\n",
    "                    'كتاب': 'book (masculine singular)',\n",
    "                    'كتب': 'books (masculine plural)',\n",
    "                    'كتابان': 'books (masculine dual)',\n",
    "                    'كتابين': 'books (masculine dual, oblique)'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'base': 'يكتب',\n",
    "                'category': 'person_number',\n",
    "                'variants': {\n",
    "                    'يكتب': 'he writes (3rd person singular masculine)',\n",
    "                    'تكتب': 'she writes (3rd person singular feminine)',\n",
    "                    'يكتبون': 'they write (3rd person plural masculine)',\n",
    "                    'يكتبن': 'they write (3rd person plural feminine)',\n",
    "                    'أكتب': 'I write (1st person singular)'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'base': 'جميل',\n",
    "                'category': 'adjective_agreement',\n",
    "                'variants': {\n",
    "                    'جميل': 'beautiful (masculine singular)',\n",
    "                    'جميلة': 'beautiful (feminine singular)',\n",
    "                    'جميلان': 'beautiful (masculine dual)',\n",
    "                    'جميلتان': 'beautiful (feminine dual)',\n",
    "                    'جميلون': 'beautiful (masculine plural)'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        inflection_results = []\n",
    "        \n",
    "        for test in inflection_tests:\n",
    "            base = test['base']\n",
    "            category = test['category']\n",
    "            variants = list(test['variants'].keys())\n",
    "            descriptions = list(test['variants'].values())\n",
    "            \n",
    "            print(f\"\\n🔄 Testing {category} for: {base}\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(variants)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Calculate inflectional coherence\n",
    "            coherence = self._calculate_family_coherence(similarity_matrix)\n",
    "            \n",
    "            print(f\"   📊 Inflectional coherence: {coherence:.4f}\")\n",
    "            \n",
    "            # Show similarities to base form\n",
    "            base_idx = variants.index(base)\n",
    "            print(f\"   🎯 Similarities to base '{base}':\")\n",
    "            for i, variant in enumerate(variants):\n",
    "                if i != base_idx:\n",
    "                    sim = similarity_matrix[base_idx, i]\n",
    "                    print(f\"     {variant}: {sim:.3f}\")\n",
    "            \n",
    "            inflection_results.append({\n",
    "                'base': base,\n",
    "                'category': category,\n",
    "                'variants': variants,\n",
    "                'descriptions': descriptions,\n",
    "                'coherence': coherence,\n",
    "                'similarity_matrix': similarity_matrix\n",
    "            })\n",
    "        \n",
    "        return inflection_results\n",
    "\n",
    "# Initialize morphology analyzer\n",
    "morphology_analyzer = ArabicMorphologyAnalyzer()\n",
    "\n",
    "# Load a multilingual model for testing\n",
    "print(\"🔄 Loading multilingual embedding model...\")\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "print(\"✅ Model loaded successfully\")\n",
    "\n",
    "print(\"\\n🧪 Starting Arabic Morphological Analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze root families\n",
    "print(\"🔍 ROOT FAMILY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "root_analyses = {}\n",
    "for root in ['كتب', 'درس', 'عمل']:\n",
    "    analysis = morphology_analyzer.analyze_root_family_embeddings(model, root)\n",
    "    root_analyses[root] = analysis\n",
    "\n",
    "# Compare morphological variants\n",
    "variant_analyses = morphology_analyzer.compare_morphological_variants(model)\n",
    "\n",
    "# Analyze inflectional variations\n",
    "inflection_analyses = morphology_analyzer.analyze_inflectional_variations(model)\n",
    "\n",
    "print(\"\\n📊 MORPHOLOGICAL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Root families analyzed: {len(root_analyses)}\")\n",
    "print(f\"Morphological variants tested: {len(variant_analyses)}\")\n",
    "print(f\"Inflectional categories tested: {len(inflection_analyses)}\")\n",
    "\n",
    "# Calculate overall morphological coherence\n",
    "all_coherences = []\n",
    "for analysis in root_analyses.values():\n",
    "    if 'family_coherence' in analysis:\n",
    "        all_coherences.append(analysis['family_coherence'])\n",
    "\n",
    "for analysis in variant_analyses:\n",
    "    all_coherences.append(analysis['coherence'])\n",
    "\n",
    "for analysis in inflection_analyses:\n",
    "    all_coherences.append(analysis['coherence'])\n",
    "\n",
    "overall_morphological_coherence = np.mean(all_coherences) if all_coherences else 0\n",
    "\n",
    "print(f\"\\n🎯 Overall Morphological Coherence: {overall_morphological_coherence:.4f}\")\n",
    "if overall_morphological_coherence > 0.7:\n",
    "    print(\"✅ GOOD: Model handles Arabic morphology well\")\n",
    "elif overall_morphological_coherence > 0.5:\n",
    "    print(\"⚠️ MODERATE: Some morphological relationships captured\")\n",
    "else:\n",
    "    print(\"❌ POOR: Limited morphological understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dialectal Variations and Cross-Dialectal Understanding\n",
    "\n",
    "### Analyzing Modern Standard Arabic (MSA) vs Regional Dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDialectAnalyzer:\n",
    "    \"\"\"Comprehensive analysis of Arabic dialectal variations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Dialectal variations for common expressions\n",
    "        self.dialectal_expressions = {\n",
    "            'greetings': {\n",
    "                'MSA': ['السلام عليكم', 'مرحباً', 'أهلاً وسهلاً'],\n",
    "                'Egyptian': ['إزيك', 'أهلاً', 'إيه أخبارك'],\n",
    "                'Levantine': ['كيفك', 'مرحبا', 'أهلين'],\n",
    "                'Gulf': ['شلونك', 'هلا', 'أهلين وسهلين'],\n",
    "                'Maghrebi': ['كيراك', 'مرحبا', 'أهلا']\n",
    "            },\n",
    "            'questions': {\n",
    "                'MSA': ['ما هذا؟', 'أين هذا؟', 'كيف الحال؟'],\n",
    "                'Egyptian': ['إيه ده؟', 'فين ده؟', 'إزيك؟'],\n",
    "                'Levantine': ['شو هاد؟', 'وين هاد؟', 'كيفك؟'],\n",
    "                'Gulf': ['شنو هذا؟', 'وين هذا؟', 'شلونك؟'],\n",
    "                'Maghrebi': ['شنو هذا؟', 'فين هذا؟', 'كيراك؟']\n",
    "            },\n",
    "            'common_words': {\n",
    "                'MSA': ['الآن', 'جيد', 'كثير', 'قليل'],\n",
    "                'Egyptian': ['دلوقتي', 'كويس', 'كتير', 'شوية'],\n",
    "                'Levantine': ['هلأ', 'منيح', 'كتير', 'شوي'],\n",
    "                'Gulf': ['الحين', 'زين', 'وايد', 'شوي'],\n",
    "                'Maghrebi': ['دابا', 'مزيان', 'بزاف', 'شوية']\n",
    "            },\n",
    "            'customer_service': {\n",
    "                'MSA': ['أحتاج مساعدة', 'لدي مشكلة', 'كيف يمكنني'],\n",
    "                'Egyptian': ['محتاج مساعدة', 'عندي مشكلة', 'إزاي ممكن'],\n",
    "                'Levantine': ['بدي مساعدة', 'عندي مشكلة', 'كيف بقدر'],\n",
    "                'Gulf': ['أبي مساعدة', 'عندي مشكلة', 'كيف أقدر'],\n",
    "                'Maghrebi': ['بغيت مساعدة', 'عندي مشكلة', 'كيفاش نقدر']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Lexical variations for same concepts\n",
    "        self.lexical_variations = {\n",
    "            'car': {\n",
    "                'MSA': 'سيارة',\n",
    "                'Egyptian': 'عربية',\n",
    "                'Levantine': 'سيارة',\n",
    "                'Gulf': 'سيارة',\n",
    "                'Maghrebi': 'طوموبيل'\n",
    "            },\n",
    "            'house': {\n",
    "                'MSA': 'منزل',\n",
    "                'Egyptian': 'بيت',\n",
    "                'Levantine': 'بيت',\n",
    "                'Gulf': 'بيت',\n",
    "                'Maghrebi': 'دار'\n",
    "            },\n",
    "            'money': {\n",
    "                'MSA': 'مال',\n",
    "                'Egyptian': 'فلوس',\n",
    "                'Levantine': 'مصاري',\n",
    "                'Gulf': 'فلوس',\n",
    "                'Maghrebi': 'دراهم'\n",
    "            },\n",
    "            'food': {\n",
    "                'MSA': 'طعام',\n",
    "                'Egyptian': 'أكل',\n",
    "                'Levantine': 'أكل',\n",
    "                'Gulf': 'أكل',\n",
    "                'Maghrebi': 'ماكلة'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_cross_dialectal_similarity(self, model: SentenceTransformer, category: str) -> Dict:\n",
    "        \"\"\"Analyze similarity across dialects for a specific category\"\"\"\n",
    "        \n",
    "        if category not in self.dialectal_expressions:\n",
    "            return {}\n",
    "        \n",
    "        dialect_data = self.dialectal_expressions[category]\n",
    "        dialects = list(dialect_data.keys())\n",
    "        \n",
    "        print(f\"\\n🗣️ Analyzing cross-dialectal similarity for: {category}\")\n",
    "        print(f\"📍 Dialects: {', '.join(dialects)}\")\n",
    "        \n",
    "        # Collect all expressions\n",
    "        all_expressions = []\n",
    "        expression_metadata = []\n",
    "        \n",
    "        for dialect in dialects:\n",
    "            for expr in dialect_data[dialect]:\n",
    "                all_expressions.append(expr)\n",
    "                expression_metadata.append({\n",
    "                    'expression': expr,\n",
    "                    'dialect': dialect,\n",
    "                    'category': category\n",
    "                })\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = model.encode(all_expressions)\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Analyze intra-dialectal vs inter-dialectal similarities\n",
    "        intra_dialectal_sims = []\n",
    "        inter_dialectal_sims = []\n",
    "        \n",
    "        for i in range(len(all_expressions)):\n",
    "            for j in range(i+1, len(all_expressions)):\n",
    "                sim = similarity_matrix[i, j]\n",
    "                dialect_i = expression_metadata[i]['dialect']\n",
    "                dialect_j = expression_metadata[j]['dialect']\n",
    "                \n",
    "                if dialect_i == dialect_j:\n",
    "                    intra_dialectal_sims.append(sim)\n",
    "                else:\n",
    "                    inter_dialectal_sims.append(sim)\n",
    "        \n",
    "        # Calculate dialect coherence metrics\n",
    "        intra_mean = np.mean(intra_dialectal_sims) if intra_dialectal_sims else 0\n",
    "        inter_mean = np.mean(inter_dialectal_sims) if inter_dialectal_sims else 0\n",
    "        dialectal_separation = intra_mean - inter_mean\n",
    "        \n",
    "        print(f\"📊 Intra-dialectal similarity: {intra_mean:.4f}\")\n",
    "        print(f\"📊 Inter-dialectal similarity: {inter_mean:.4f}\")\n",
    "        print(f\"📊 Dialectal separation: {dialectal_separation:.4f}\")\n",
    "        \n",
    "        # Analyze MSA vs other dialects\n",
    "        msa_similarities = self._analyze_msa_similarities(\n",
    "            all_expressions, expression_metadata, similarity_matrix\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'category': category,\n",
    "            'dialects': dialects,\n",
    "            'all_expressions': all_expressions,\n",
    "            'expression_metadata': expression_metadata,\n",
    "            'similarity_matrix': similarity_matrix,\n",
    "            'intra_dialectal_similarity': intra_mean,\n",
    "            'inter_dialectal_similarity': inter_mean,\n",
    "            'dialectal_separation': dialectal_separation,\n",
    "            'msa_similarities': msa_similarities\n",
    "        }\n",
    "    \n",
    "    def _analyze_msa_similarities(self, expressions: List[str], metadata: List[Dict], similarity_matrix: np.ndarray) -> Dict:\n",
    "        \"\"\"Analyze how MSA relates to other dialects\"\"\"\n",
    "        \n",
    "        msa_indices = [i for i, meta in enumerate(metadata) if meta['dialect'] == 'MSA']\n",
    "        non_msa_indices = [i for i, meta in enumerate(metadata) if meta['dialect'] != 'MSA']\n",
    "        \n",
    "        if not msa_indices or not non_msa_indices:\n",
    "            return {}\n",
    "        \n",
    "        # Calculate average similarity between MSA and each dialect\n",
    "        dialect_to_msa_sims = defaultdict(list)\n",
    "        \n",
    "        for msa_idx in msa_indices:\n",
    "            for non_msa_idx in non_msa_indices:\n",
    "                sim = similarity_matrix[msa_idx, non_msa_idx]\n",
    "                dialect = metadata[non_msa_idx]['dialect']\n",
    "                dialect_to_msa_sims[dialect].append(sim)\n",
    "        \n",
    "        # Calculate mean similarities\n",
    "        msa_dialect_means = {}\n",
    "        for dialect, sims in dialect_to_msa_sims.items():\n",
    "            msa_dialect_means[dialect] = np.mean(sims)\n",
    "        \n",
    "        return msa_dialect_means\n",
    "    \n",
    "    def analyze_lexical_variations(self, model: SentenceTransformer) -> Dict:\n",
    "        \"\"\"Analyze how models handle lexical variations across dialects\"\"\"\n",
    "        \n",
    "        print(f\"\\n📚 Lexical Variations Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        lexical_results = []\n",
    "        \n",
    "        for concept, variations in self.lexical_variations.items():\n",
    "            print(f\"\\n🔍 Concept: {concept}\")\n",
    "            \n",
    "            dialects = list(variations.keys())\n",
    "            words = list(variations.values())\n",
    "            \n",
    "            print(f\"   Variations: {dict(zip(dialects, words))}\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(words)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Calculate concept coherence\n",
    "            coherence = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "            \n",
    "            print(f\"   📊 Lexical coherence: {coherence:.4f}\")\n",
    "            \n",
    "            # Analyze distance from MSA\n",
    "            msa_idx = dialects.index('MSA')\n",
    "            msa_distances = {}\n",
    "            for i, dialect in enumerate(dialects):\n",
    "                if i != msa_idx:\n",
    "                    msa_distances[dialect] = similarity_matrix[msa_idx, i]\n",
    "            \n",
    "            print(f\"   🎯 Distances from MSA:\")\n",
    "            for dialect, distance in msa_distances.items():\n",
    "                print(f\"     {dialect}: {distance:.3f}\")\n",
    "            \n",
    "            lexical_results.append({\n",
    "                'concept': concept,\n",
    "                'variations': variations,\n",
    "                'dialects': dialects,\n",
    "                'words': words,\n",
    "                'coherence': coherence,\n",
    "                'similarity_matrix': similarity_matrix,\n",
    "                'msa_distances': msa_distances\n",
    "            })\n",
    "        \n",
    "        return lexical_results\n",
    "    \n",
    "    def analyze_code_switching(self, model: SentenceTransformer) -> Dict:\n",
    "        \"\"\"Analyze how models handle Arabic-English code switching\"\"\"\n",
    "        \n",
    "        print(f\"\\n🔄 Code-Switching Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Examples of code-switching common in modern Arabic\n",
    "        code_switching_examples = [\n",
    "            {\n",
    "                'pure_arabic': 'أحتاج إلى مساعدة في استخدام الحاسوب',\n",
    "                'code_switched': 'أحتاج مساعدة في استخدام الـ computer',\n",
    "                'mixed': 'محتاج help في الـ laptop بتاعي',\n",
    "                'concept': 'computer_help'\n",
    "            },\n",
    "            {\n",
    "                'pure_arabic': 'سأرسل لك رسالة إلكترونية',\n",
    "                'code_switched': 'سأرسل لك email',\n",
    "                'mixed': 'هبعتلك email على الـ WhatsApp',\n",
    "                'concept': 'send_message'\n",
    "            },\n",
    "            {\n",
    "                'pure_arabic': 'أريد تحديث تطبيق الهاتف المحمول',\n",
    "                'code_switched': 'أريد أحدث الـ mobile app',\n",
    "                'mixed': 'عايز أعمل update للـ app',\n",
    "                'concept': 'app_update'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        code_switching_results = []\n",
    "        \n",
    "        for example in code_switching_examples:\n",
    "            concept = example['concept']\n",
    "            variants = [example['pure_arabic'], example['code_switched'], example['mixed']]\n",
    "            variant_types = ['Pure Arabic', 'Code-Switched', 'Mixed']\n",
    "            \n",
    "            print(f\"\\n🔍 Concept: {concept}\")\n",
    "            for variant_type, variant in zip(variant_types, variants):\n",
    "                print(f\"   {variant_type}: {variant}\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(variants)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Calculate code-switching tolerance\n",
    "            pure_to_switched = similarity_matrix[0, 1]\n",
    "            pure_to_mixed = similarity_matrix[0, 2]\n",
    "            switched_to_mixed = similarity_matrix[1, 2]\n",
    "            \n",
    "            print(f\"   📊 Pure ↔ Code-switched: {pure_to_switched:.3f}\")\n",
    "            print(f\"   📊 Pure ↔ Mixed: {pure_to_mixed:.3f}\")\n",
    "            print(f\"   📊 Code-switched ↔ Mixed: {switched_to_mixed:.3f}\")\n",
    "            \n",
    "            code_switching_tolerance = np.mean([pure_to_switched, pure_to_mixed, switched_to_mixed])\n",
    "            print(f\"   🎯 Code-switching tolerance: {code_switching_tolerance:.4f}\")\n",
    "            \n",
    "            code_switching_results.append({\n",
    "                'concept': concept,\n",
    "                'variants': variants,\n",
    "                'variant_types': variant_types,\n",
    "                'similarity_matrix': similarity_matrix,\n",
    "                'code_switching_tolerance': code_switching_tolerance,\n",
    "                'pure_to_switched': pure_to_switched,\n",
    "                'pure_to_mixed': pure_to_mixed,\n",
    "                'switched_to_mixed': switched_to_mixed\n",
    "            })\n",
    "        \n",
    "        return code_switching_results\n",
    "\n",
    "# Initialize dialect analyzer\n",
    "dialect_analyzer = ArabicDialectAnalyzer()\n",
    "\n",
    "print(\"\\n🗣️ Starting Arabic Dialectal Analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dialectal variations\n",
    "print(\"🗣️ DIALECTAL VARIATIONS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dialectal_analyses = {}\n",
    "for category in ['greetings', 'questions', 'customer_service']:\n",
    "    analysis = dialect_analyzer.analyze_cross_dialectal_similarity(model, category)\n",
    "    dialectal_analyses[category] = analysis\n",
    "\n",
    "# Analyze lexical variations\n",
    "lexical_analyses = dialect_analyzer.analyze_lexical_variations(model)\n",
    "\n",
    "# Analyze code-switching\n",
    "code_switching_analyses = dialect_analyzer.analyze_code_switching(model)\n",
    "\n",
    "print(\"\\n📊 DIALECTAL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate overall dialectal metrics\n",
    "all_inter_dialectal_sims = []\n",
    "all_intra_dialectal_sims = []\n",
    "all_separations = []\n",
    "\n",
    "for analysis in dialectal_analyses.values():\n",
    "    if 'inter_dialectal_similarity' in analysis:\n",
    "        all_inter_dialectal_sims.append(analysis['inter_dialectal_similarity'])\n",
    "        all_intra_dialectal_sims.append(analysis['intra_dialectal_similarity'])\n",
    "        all_separations.append(analysis['dialectal_separation'])\n",
    "\n",
    "# Calculate lexical coherence\n",
    "lexical_coherences = [analysis['coherence'] for analysis in lexical_analyses]\n",
    "\n",
    "# Calculate code-switching tolerance\n",
    "code_switching_tolerances = [analysis['code_switching_tolerance'] for analysis in code_switching_analyses]\n",
    "\n",
    "print(f\"📈 Average inter-dialectal similarity: {np.mean(all_inter_dialectal_sims):.4f}\")\n",
    "print(f\"📈 Average intra-dialectal similarity: {np.mean(all_intra_dialectal_sims):.4f}\")\n",
    "print(f\"📈 Average dialectal separation: {np.mean(all_separations):.4f}\")\n",
    "print(f\"📈 Average lexical coherence: {np.mean(lexical_coherences):.4f}\")\n",
    "print(f\"📈 Average code-switching tolerance: {np.mean(code_switching_tolerances):.4f}\")\n",
    "\n",
    "# Overall dialectal performance assessment\n",
    "overall_dialectal_performance = np.mean([\n",
    "    np.mean(all_inter_dialectal_sims),\n",
    "    np.mean(lexical_coherences),\n",
    "    np.mean(code_switching_tolerances)\n",
    "])\n",
    "\n",
    "print(f\"\\n🎯 Overall Dialectal Performance: {overall_dialectal_performance:.4f}\")\n",
    "\n",
    "if overall_dialectal_performance > 0.7:\n",
    "    print(\"✅ EXCELLENT: Strong cross-dialectal understanding\")\n",
    "elif overall_dialectal_performance > 0.5:\n",
    "    print(\"⚠️ MODERATE: Some dialectal relationships captured\")\n",
    "else:\n",
    "    print(\"❌ POOR: Limited cross-dialectal understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arabic Text Preprocessing and Normalization\n",
    "\n",
    "### Essential Preprocessing Steps for Arabic NLP Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicTextPreprocessor:\n",
    "    \"\"\"Comprehensive Arabic text preprocessing and normalization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Arabic Unicode ranges and characters\n",
    "        self.arabic_chars = {\n",
    "            'basic_range': (0x0600, 0x06FF),\n",
    "            'supplement_range': (0x0750, 0x077F),\n",
    "            'extended_range': (0x08A0, 0x08FF)\n",
    "        }\n",
    "        \n",
    "        # Diacritics (Harakat)\n",
    "        self.diacritics = [\n",
    "            '\\u064B',  # Fathatan\n",
    "            '\\u064C',  # Dammatan\n",
    "            '\\u064D',  # Kasratan\n",
    "            '\\u064E',  # Fatha\n",
    "            '\\u064F',  # Damma\n",
    "            '\\u0650',  # Kasra\n",
    "            '\\u0651',  # Shadda\n",
    "            '\\u0652',  # Sukun\n",
    "            '\\u0653',  # Maddah\n",
    "            '\\u0654',  # Hamza above\n",
    "            '\\u0655',  # Hamza below\n",
    "            '\\u0656',  # Subscript alef\n",
    "            '\\u0657',  # Inverted damma\n",
    "            '\\u0658',  # Mark noon ghunna\n",
    "            '\\u0659',  # Zwarakay\n",
    "            '\\u065A',  # Vowel sign small v\n",
    "            '\\u065B',  # Vowel sign inverted small v\n",
    "            '\\u065C',  # Vowel sign dot below\n",
    "            '\\u065D',  # Reversed damma\n",
    "            '\\u065E',  # Fatha with two dots\n",
    "            '\\u065F',  # Wavy hamza below\n",
    "            '\\u0670'   # Superscript alef\n",
    "        ]\n",
    "        \n",
    "        # Character normalization mappings\n",
    "        self.char_normalization = {\n",
    "            # Alef variations\n",
    "            'آ': 'ا',  # Alef with madda\n",
    "            'أ': 'ا',  # Alef with hamza above\n",
    "            'إ': 'ا',  # Alef with hamza below\n",
    "            'ٱ': 'ا',  # Alef wasla\n",
    "            \n",
    "            # Ya variations\n",
    "            'ى': 'ي',  # Alef maksura\n",
    "            'ئ': 'ي',  # Ya with hamza above\n",
    "            \n",
    "            # Ta variations\n",
    "            'ة': 'ه',  # Ta marbouta to Ha\n",
    "            \n",
    "            # Waw variations\n",
    "            'ؤ': 'و',  # Waw with hamza above\n",
    "        }\n",
    "        \n",
    "        # Common Arabic stop words\n",
    "        self.stop_words = {\n",
    "            'في', 'من', 'إلى', 'على', 'عن', 'مع', 'بعد', 'قبل', 'تحت', 'فوق',\n",
    "            'هذا', 'هذه', 'ذلك', 'تلك', 'التي', 'الذي', 'اللذان', 'اللتان',\n",
    "            'أن', 'إن', 'كان', 'كانت', 'يكون', 'تكون', 'ليس', 'ليست',\n",
    "            'لا', 'لم', 'لن', 'ما', 'كل', 'بعض', 'جميع', 'كلا', 'كلتا',\n",
    "            'هو', 'هي', 'هم', 'هن', 'أنت', 'أنتم', 'أنتن', 'أنا', 'نحن',\n",
    "            'له', 'لها', 'لهم', 'لهن', 'لك', 'لكم', 'لكن', 'لي', 'لنا'\n",
    "        }\n",
    "        \n",
    "        # Punctuation marks\n",
    "        self.arabic_punctuation = '،؛؟!\"\\'\\.\\(\\)\\[\\]\\{\\}'\n",
    "        \n",
    "    def remove_diacritics(self, text: str) -> str:\n",
    "        \"\"\"Remove Arabic diacritics from text\"\"\"\n",
    "        for diacritic in self.diacritics:\n",
    "            text = text.replace(diacritic, '')\n",
    "        return text\n",
    "    \n",
    "    def normalize_characters(self, text: str) -> str:\n",
    "        \"\"\"Normalize Arabic character variations\"\"\"\n",
    "        for original, normalized in self.char_normalization.items():\n",
    "            text = text.replace(original, normalized)\n",
    "        return text\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove non-Arabic characters (optional)\n",
    "        # text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\s]', '', text)\n",
    "        \n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stop_words(self, text: str) -> str:\n",
    "        \"\"\"Remove Arabic stop words\"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def full_preprocessing(self, text: str, remove_diacritics: bool = True, \n",
    "                         normalize_chars: bool = True, remove_stops: bool = False) -> str:\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        \n",
    "        # Basic cleaning\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove diacritics\n",
    "        if remove_diacritics:\n",
    "            text = self.remove_diacritics(text)\n",
    "        \n",
    "        # Normalize characters\n",
    "        if normalize_chars:\n",
    "            text = self.normalize_characters(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stops:\n",
    "            text = self.remove_stop_words(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def analyze_preprocessing_impact(self, model: SentenceTransformer, \n",
    "                                   test_texts: List[str]) -> Dict:\n",
    "        \"\"\"Analyze impact of preprocessing on embedding similarity\"\"\"\n",
    "        \n",
    "        print(f\"\\n🔧 Preprocessing Impact Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        preprocessing_configs = [\n",
    "            {'name': 'Original', 'params': {}},\n",
    "            {'name': 'No Diacritics', 'params': {'remove_diacritics': True, 'normalize_chars': False}},\n",
    "            {'name': 'Normalized', 'params': {'remove_diacritics': True, 'normalize_chars': True}},\n",
    "            {'name': 'Full Processing', 'params': {'remove_diacritics': True, 'normalize_chars': True, 'remove_stops': True}}\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for config in preprocessing_configs:\n",
    "            name = config['name']\n",
    "            params = config['params']\n",
    "            \n",
    "            print(f\"\\n🔍 Testing: {name}\")\n",
    "            \n",
    "            # Preprocess texts\n",
    "            if params:\n",
    "                processed_texts = [self.full_preprocessing(text, **params) for text in test_texts]\n",
    "            else:\n",
    "                processed_texts = test_texts\n",
    "            \n",
    "            # Show example of preprocessing\n",
    "            if len(test_texts) > 0:\n",
    "                print(f\"   Original: {test_texts[0][:50]}...\")\n",
    "                print(f\"   Processed: {processed_texts[0][:50]}...\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = model.encode(processed_texts)\n",
    "            \n",
    "            # Calculate similarity matrix\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "            embedding_variance = np.var(embeddings.flatten())\n",
    "            \n",
    "            print(f\"   📊 Average similarity: {avg_similarity:.4f}\")\n",
    "            print(f\"   📊 Embedding variance: {embedding_variance:.6f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'config_name': name,\n",
    "                'config_params': params,\n",
    "                'processed_texts': processed_texts,\n",
    "                'embeddings': embeddings,\n",
    "                'similarity_matrix': similarity_matrix,\n",
    "                'avg_similarity': avg_similarity,\n",
    "                'embedding_variance': embedding_variance\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def demonstrate_normalization_effects(self) -> Dict:\n",
    "        \"\"\"Demonstrate effects of different normalization steps\"\"\"\n",
    "        \n",
    "        print(f\"\\n📋 Character Normalization Demonstration\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Example texts with various Arabic forms\n",
    "        example_texts = [\n",
    "            'أَهْلاً وَسَهْلاً بِكُمْ فِي مَوْقِعِنَا',  # With diacritics\n",
    "            'أهلاً وسهلاً بكم في موقعنا',  # Without diacritics  \n",
    "            'اهلا وسهلا بكم فى موقعنا',  # Normalized\n",
    "            'مرحباً بكم في موقعنا الإلكتروني',  # Alternative phrasing\n",
    "            'أحتاج إلى مساعدة في حل هذه المشكلة',  # Help request\n",
    "            'احتاج الى مساعده في حل هذه المشكله'  # Normalized version\n",
    "        ]\n",
    "        \n",
    "        normalization_effects = []\n",
    "        \n",
    "        for i, text in enumerate(example_texts):\n",
    "            print(f\"\\n📝 Example {i+1}:\")\n",
    "            print(f\"   Original: {text}\")\n",
    "            \n",
    "            # Apply different preprocessing steps\n",
    "            no_diacritics = self.remove_diacritics(text)\n",
    "            normalized = self.normalize_characters(no_diacritics)\n",
    "            fully_processed = self.full_preprocessing(text, remove_stops=True)\n",
    "            \n",
    "            print(f\"   No diacritics: {no_diacritics}\")\n",
    "            print(f\"   Normalized: {normalized}\")\n",
    "            print(f\"   Fully processed: {fully_processed}\")\n",
    "            \n",
    "            normalization_effects.append({\n",
    "                'original': text,\n",
    "                'no_diacritics': no_diacritics,\n",
    "                'normalized': normalized,\n",
    "                'fully_processed': fully_processed\n",
    "            })\n",
    "        \n",
    "        return normalization_effects\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = ArabicTextPreprocessor()\n",
    "\n",
    "print(\"\\n🔧 Starting Arabic Text Preprocessing Analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate normalization effects\n",
    "normalization_demo = preprocessor.demonstrate_normalization_effects()\n",
    "\n",
    "# Test preprocessing impact on embeddings\n",
    "test_texts_for_preprocessing = [\n",
    "    'أَحْتَاجُ مُسَاعَدَةً فِي حَلِّ هَذِهِ الْمُشْكِلَةِ',  # With diacritics\n",
    "    'أحتاج مساعدة في حل هذه المشكلة',  # Clean\n",
    "    'احتاج مساعده في حل هذه المشكله',  # Normalized\n",
    "    'كَيْفَ يُمْكِنُنِي تَسْجِيلُ الدُّخُولِ إِلَى حِسَابِي؟',  # With diacritics\n",
    "    'كيف يمكنني تسجيل الدخول إلى حسابي؟',  # Clean\n",
    "    'كيف يمكنني تسجيل الدخول الى حسابي؟',  # Normalized\n",
    "]\n",
    "\n",
    "preprocessing_analysis = preprocessor.analyze_preprocessing_impact(model, test_texts_for_preprocessing)\n",
    "\n",
    "print(\"\\n📊 PREPROCESSING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compare preprocessing effects\n",
    "print(\"\\n🔍 Preprocessing Configuration Comparison:\")\n",
    "for result in preprocessing_analysis:\n",
    "    print(f\"  {result['config_name']}:\")\n",
    "    print(f\"    Avg Similarity: {result['avg_similarity']:.4f}\")\n",
    "    print(f\"    Embedding Variance: {result['embedding_variance']:.6f}\")\n",
    "\n",
    "# Find best preprocessing configuration\n",
    "best_config = max(preprocessing_analysis[1:], key=lambda x: x['avg_similarity'])  # Skip original\n",
    "print(f\"\\n🏆 Best preprocessing configuration: {best_config['config_name']}\")\n",
    "print(f\"   Improvement over original: {best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Visualization and Impact Analysis\n",
    "\n",
    "### Creating Visual Analysis of All Arabic Language Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_arabic_analysis_visualization():\n",
    "    \"\"\"Create comprehensive visualization of Arabic language challenges\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 18))\n",
    "    \n",
    "    # 1. Morphological Coherence Comparison\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    \n",
    "    # Data for morphological analysis\n",
    "    morphological_categories = ['Root Families', 'Variants', 'Inflections']\n",
    "    morphological_scores = [overall_morphological_coherence, \n",
    "                           np.mean([a['coherence'] for a in variant_analyses]),\n",
    "                           np.mean([a['coherence'] for a in inflection_analyses])]\n",
    "    \n",
    "    bars = ax1.bar(morphological_categories, morphological_scores, \n",
    "                   color=['red', 'blue', 'green'], alpha=0.7)\n",
    "    ax1.set_title('Morphological Understanding', fontweight='bold')\n",
    "    ax1.set_ylabel('Coherence Score')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    for bar, score in zip(bars, morphological_scores):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Dialectal Performance\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    \n",
    "    dialectal_metrics = ['Inter-Dialectal', 'Lexical Coherence', 'Code-Switching']\n",
    "    dialectal_scores = [np.mean(all_inter_dialectal_sims), \n",
    "                       np.mean(lexical_coherences),\n",
    "                       np.mean(code_switching_tolerances)]\n",
    "    \n",
    "    bars = ax2.bar(dialectal_metrics, dialectal_scores, \n",
    "                   color=['purple', 'orange', 'cyan'], alpha=0.7)\n",
    "    ax2.set_title('Dialectal Understanding', fontweight='bold')\n",
    "    ax2.set_ylabel('Performance Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars, dialectal_scores):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Preprocessing Impact\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    \n",
    "    preprocessing_names = [r['config_name'] for r in preprocessing_analysis]\n",
    "    preprocessing_similarities = [r['avg_similarity'] for r in preprocessing_analysis]\n",
    "    \n",
    "    bars = ax3.bar(range(len(preprocessing_names)), preprocessing_similarities, \n",
    "                   color=['gray', 'lightblue', 'lightgreen', 'gold'], alpha=0.7)\n",
    "    ax3.set_title('Preprocessing Impact', fontweight='bold')\n",
    "    ax3.set_ylabel('Average Similarity')\n",
    "    ax3.set_xticks(range(len(preprocessing_names)))\n",
    "    ax3.set_xticklabels(preprocessing_names, rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars, preprocessing_similarities):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # 4. Root Family Analysis Heatmap\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    \n",
    "    if 'كتب' in root_analyses and 'similarity_matrix' in root_analyses['كتب']:\n",
    "        sim_matrix = root_analyses['كتب']['similarity_matrix']\n",
    "        words = root_analyses['كتب']['words']\n",
    "        \n",
    "        im = ax4.imshow(sim_matrix, cmap='Blues', vmin=0, vmax=1)\n",
    "        ax4.set_title('Root Family Similarity\\n(ك-ت-ب)', fontweight='bold')\n",
    "        ax4.set_xticks(range(len(words)))\n",
    "        ax4.set_yticks(range(len(words)))\n",
    "        ax4.set_xticklabels(words, rotation=45, fontsize=8)\n",
    "        ax4.set_yticklabels(words, fontsize=8)\n",
    "        \n",
    "        # Add similarity values\n",
    "        for i in range(len(words)):\n",
    "            for j in range(len(words)):\n",
    "                ax4.text(j, i, f'{sim_matrix[i,j]:.2f}', \n",
    "                        ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Root Family\\nData Not Available', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Root Family Analysis', fontweight='bold')\n",
    "    \n",
    "    # 5. Dialectal Cross-Similarity Matrix\n",
    "    ax5 = plt.subplot(3, 4, 5)\n",
    "    \n",
    "    # Create dialect similarity matrix if data available\n",
    "    if 'greetings' in dialectal_analyses and dialectal_analyses['greetings']:\n",
    "        analysis = dialectal_analyses['greetings']\n",
    "        if 'msa_similarities' in analysis and analysis['msa_similarities']:\n",
    "            dialects = list(analysis['msa_similarities'].keys())\n",
    "            similarities = list(analysis['msa_similarities'].values())\n",
    "            \n",
    "            bars = ax5.barh(dialects, similarities, color='lightcoral', alpha=0.7)\n",
    "            ax5.set_title('MSA Similarity by Dialect', fontweight='bold')\n",
    "            ax5.set_xlabel('Similarity to MSA')\n",
    "            \n",
    "            for bar, sim in zip(bars, similarities):\n",
    "                ax5.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                        f'{sim:.3f}', ha='left', va='center', fontweight='bold')\n",
    "        else:\n",
    "            ax5.text(0.5, 0.5, 'Dialectal Data\\nNot Available', \n",
    "                    ha='center', va='center', transform=ax5.transAxes)\n",
    "            ax5.set_title('Dialectal Analysis', fontweight='bold')\n",
    "    \n",
    "    # 6. Code-Switching Tolerance\n",
    "    ax6 = plt.subplot(3, 4, 6)\n",
    "    \n",
    "    if code_switching_analyses:\n",
    "        concepts = [a['concept'] for a in code_switching_analyses]\n",
    "        tolerances = [a['code_switching_tolerance'] for a in code_switching_analyses]\n",
    "        \n",
    "        bars = ax6.bar(range(len(concepts)), tolerances, \n",
    "                       color='lightgreen', alpha=0.7)\n",
    "        ax6.set_title('Code-Switching Tolerance', fontweight='bold')\n",
    "        ax6.set_ylabel('Tolerance Score')\n",
    "        ax6.set_xticks(range(len(concepts)))\n",
    "        ax6.set_xticklabels([c.replace('_', '\\n') for c in concepts], fontsize=8)\n",
    "        \n",
    "        for bar, score in zip(bars, tolerances):\n",
    "            ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 7. Challenge Severity Assessment\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    \n",
    "    challenges = ['Morphology', 'Dialects', 'Preprocessing', 'Code-Switching']\n",
    "    # Higher scores = more challenging (invert some metrics)\n",
    "    challenge_scores = [\n",
    "        1 - overall_morphological_coherence,  # Lower coherence = higher challenge\n",
    "        1 - overall_dialectal_performance,    # Lower performance = higher challenge\n",
    "        1 - (best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']),  # Lower improvement = higher challenge\n",
    "        1 - np.mean(code_switching_tolerances)  # Lower tolerance = higher challenge\n",
    "    ]\n",
    "    \n",
    "    colors = ['red' if score > 0.5 else 'orange' if score > 0.3 else 'green' for score in challenge_scores]\n",
    "    bars = ax7.bar(challenges, challenge_scores, color=colors, alpha=0.7)\n",
    "    ax7.set_title('Challenge Severity', fontweight='bold')\n",
    "    ax7.set_ylabel('Difficulty Score')\n",
    "    ax7.set_ylim(0, 1)\n",
    "    ax7.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, score in zip(bars, challenge_scores):\n",
    "        ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 8. Overall Performance Radar Chart\n",
    "    ax8 = plt.subplot(3, 4, 8, projection='polar')\n",
    "    \n",
    "    performance_categories = ['Morphology', 'Dialects', 'Normalization', 'Code-Switch']\n",
    "    performance_scores = [\n",
    "        overall_morphological_coherence,\n",
    "        overall_dialectal_performance,\n",
    "        best_config['avg_similarity'],\n",
    "        np.mean(code_switching_tolerances)\n",
    "    ]\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(performance_categories), endpoint=False).tolist()\n",
    "    performance_scores += performance_scores[:1]  # Complete the circle\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax8.plot(angles, performance_scores, 'o-', linewidth=2, color='blue')\n",
    "    ax8.fill(angles, performance_scores, alpha=0.25, color='blue')\n",
    "    ax8.set_xticks(angles[:-1])\n",
    "    ax8.set_xticklabels(performance_categories)\n",
    "    ax8.set_ylim(0, 1)\n",
    "    ax8.set_title('Arabic NLP Performance\\nRadar', fontweight='bold', pad=20)\n",
    "    \n",
    "    # 9. Morphological Complexity Examples\n",
    "    ax9 = plt.subplot(3, 4, 9)\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    morphology_text = \"\"\"\n",
    "🔤 MORPHOLOGICAL COMPLEXITY\n",
    "\n",
    "Root: ك-ت-ب (K-T-B)\n",
    "• كتب (kataba) - he wrote\n",
    "• كاتب (kaatib) - writer\n",
    "• مكتبة (maktaba) - library\n",
    "• مكتوب (maktuub) - written\n",
    "• استكتب (istaktaba) - to dictate\n",
    "\n",
    "Challenge: One root → 100+ words\n",
    "Impact: Semantic similarity varies\n",
    "\"\"\"\n",
    "    \n",
    "    ax9.text(0.05, 0.95, morphology_text, transform=ax9.transAxes, fontsize=9, \n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # 10. Dialectal Variations Examples\n",
    "    ax10 = plt.subplot(3, 4, 10)\n",
    "    ax10.axis('off')\n",
    "    \n",
    "    dialect_text = \"\"\"\n",
    "🗣️ DIALECTAL VARIATIONS\n",
    "\n",
    "\"How are you?\"\n",
    "• MSA: كيف الحال؟\n",
    "• Egyptian: إزيك؟\n",
    "• Levantine: كيفك؟\n",
    "• Gulf: شلونك؟\n",
    "• Maghrebi: كيراك؟\n",
    "\n",
    "Challenge: Same meaning, different forms\n",
    "Impact: Cross-dialect understanding\n",
    "\"\"\"\n",
    "    \n",
    "    ax10.text(0.05, 0.95, dialect_text, transform=ax10.transAxes, fontsize=9, \n",
    "              verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "    \n",
    "    # 11. Preprocessing Effects\n",
    "    ax11 = plt.subplot(3, 4, 11)\n",
    "    ax11.axis('off')\n",
    "    \n",
    "    preprocessing_text = f\"\"\"\n",
    "🔧 PREPROCESSING EFFECTS\n",
    "\n",
    "Original:\n",
    "أَحْتَاجُ مُسَاعَدَةً\n",
    "\n",
    "Remove Diacritics:\n",
    "أحتاج مساعدة\n",
    "\n",
    "Normalize Characters:\n",
    "احتاج مساعده\n",
    "\n",
    "Best Config: {best_config['config_name']}\n",
    "Improvement: +{(best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']):.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    ax11.text(0.05, 0.95, preprocessing_text, transform=ax11.transAxes, fontsize=9, \n",
    "              verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    # 12. Solutions and Recommendations\n",
    "    ax12 = plt.subplot(3, 4, 12)\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    solutions_text = \"\"\"\n",
    "🚀 SOLUTIONS & RECOMMENDATIONS\n",
    "\n",
    "🎯 For Morphology:\n",
    "• Root-aware embeddings\n",
    "• Morphological analyzers\n",
    "• Subword tokenization\n",
    "\n",
    "🗣️ For Dialects:\n",
    "• Multi-dialectal training\n",
    "• Dialect identification\n",
    "• Cross-dialectal alignment\n",
    "\n",
    "🔧 For Processing:\n",
    "• Standardized normalization\n",
    "• Diacritic handling\n",
    "• Context-aware cleaning\n",
    "\n",
    "📊 For Evaluation:\n",
    "• Arabic-specific benchmarks\n",
    "• Human evaluation\n",
    "• Domain adaptation\n",
    "\"\"\"\n",
    "    \n",
    "    ax12.text(0.05, 0.95, solutions_text, transform=ax12.transAxes, fontsize=8, \n",
    "              verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_arabic_language_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create comprehensive visualization\n",
    "create_comprehensive_arabic_analysis_visualization()\n",
    "\n",
    "print(\"\\n📊 Comprehensive Arabic language analysis visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Solutions and Mitigation Strategies\n",
    "\n",
    "### Practical Approaches to Address Arabic NLP Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicNLPSolutions:\n",
    "    \"\"\"Comprehensive solutions for Arabic NLP challenges\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.solution_strategies = {\n",
    "            'morphological_challenges': {\n",
    "                'problems': [\n",
    "                    'Complex derivational morphology',\n",
    "                    'Rich inflectional system',\n",
    "                    'Root-pattern relationships',\n",
    "                    'Agglutinative properties'\n",
    "                ],\n",
    "                'solutions': [\n",
    "                    'Root-aware embeddings',\n",
    "                    'Morphological analyzers (MADAMIRA, SAMA)',\n",
    "                    'Subword tokenization (BPE, SentencePiece)',\n",
    "                    'Multi-task learning with morphological tasks',\n",
    "                    'Character-level representations',\n",
    "                    'Templatic morphology modeling'\n",
    "                ],\n",
    "                'implementation_priority': 'HIGH',\n",
    "                'expected_improvement': '15-25%'\n",
    "            },\n",
    "            'dialectal_challenges': {\n",
    "                'problems': [\n",
    "                    'MSA vs. dialectal variations',\n",
    "                    'Regional lexical differences',\n",
    "                    'Syntactic variations',\n",
    "                    'Limited dialectal resources'\n",
    "                ],\n",
    "                'solutions': [\n",
    "                    'Multi-dialectal training data',\n",
    "                    'Dialect identification systems',\n",
    "                    'Cross-dialectal alignment techniques',\n",
    "                    'Adapter layers for dialects',\n",
    "                    'Code-switching aware models',\n",
    "                    'Dialectal data augmentation'\n",
    "                ],\n",
    "                'implementation_priority': 'MEDIUM',\n",
    "                'expected_improvement': '10-20%'\n",
    "            },\n",
    "            'preprocessing_challenges': {\n",
    "                'problems': [\n",
    "                    'Diacritic handling',\n",
    "                    'Character normalization',\n",
    "                    'Tokenization boundaries',\n",
    "                    'Script directionality'\n",
    "                ],\n",
    "                'solutions': [\n",
    "                    'Standardized normalization pipelines',\n",
    "                    'Context-aware diacritic restoration',\n",
    "                    'Arabic-specific tokenizers',\n",
    "                    'Bidirectional text handling',\n",
    "                    'Robust cleaning algorithms',\n",
    "                    'Preprocessing optimization'\n",
    "                ],\n",
    "                'implementation_priority': 'HIGH',\n",
    "                'expected_improvement': '5-15%'\n",
    "            },\n",
    "            'data_scarcity': {\n",
    "                'problems': [\n",
    "                    'Limited labeled datasets',\n",
    "                    'Domain-specific data shortage',\n",
    "                    'Quality control issues',\n",
    "                    'Annotation consistency'\n",
    "                ],\n",
    "                'solutions': [\n",
    "                    'Transfer learning from related languages',\n",
    "                    'Data augmentation techniques',\n",
    "                    'Synthetic data generation',\n",
    "                    'Cross-lingual embeddings',\n",
    "                    'Semi-supervised learning',\n",
    "                    'Active learning strategies'\n",
    "                ],\n",
    "                'implementation_priority': 'MEDIUM',\n",
    "                'expected_improvement': '10-30%'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.evaluation_improvements = {\n",
    "            'arabic_specific_benchmarks': [\n",
    "                'Arabic Reading Comprehension (ARC)',\n",
    "                'Arabic Natural Language Inference (ANLI)',\n",
    "                'Arabic Sentiment Analysis (ArSAS)',\n",
    "                'Arabic Named Entity Recognition (ANERcorp)',\n",
    "                'Arabic Question Answering (ARCD)'\n",
    "            ],\n",
    "            'evaluation_metrics': [\n",
    "                'Root-level semantic similarity',\n",
    "                'Cross-dialectal consistency',\n",
    "                'Morphological awareness scores',\n",
    "                'Cultural context appropriateness',\n",
    "                'Code-switching robustness'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def generate_implementation_roadmap(self) -> Dict:\n",
    "        \"\"\"Generate prioritized implementation roadmap\"\"\"\n",
    "        \n",
    "        roadmap = {\n",
    "            'phase_1_immediate': {\n",
    "                'duration': '1-2 months',\n",
    "                'focus': 'Quick wins and foundation',\n",
    "                'tasks': [\n",
    "                    'Implement standardized Arabic preprocessing pipeline',\n",
    "                    'Integrate Arabic morphological analyzer',\n",
    "                    'Setup Arabic-specific evaluation metrics',\n",
    "                    'Create Arabic text normalization module'\n",
    "                ],\n",
    "                'expected_impact': 'HIGH',\n",
    "                'resources_needed': 'LOW'\n",
    "            },\n",
    "            'phase_2_short_term': {\n",
    "                'duration': '3-6 months',\n",
    "                'focus': 'Model improvements and data',\n",
    "                'tasks': [\n",
    "                    'Fine-tune embeddings on Arabic corpus',\n",
    "                    'Implement subword tokenization',\n",
    "                    'Create multi-dialectal training dataset',\n",
    "                    'Develop dialect identification system',\n",
    "                    'Setup cross-dialectal evaluation'\n",
    "                ],\n",
    "                'expected_impact': 'MEDIUM-HIGH',\n",
    "                'resources_needed': 'MEDIUM'\n",
    "            },\n",
    "            'phase_3_medium_term': {\n",
    "                'duration': '6-12 months',\n",
    "                'focus': 'Advanced modeling and optimization',\n",
    "                'tasks': [\n",
    "                    'Develop root-aware embedding architecture',\n",
    "                    'Implement multi-task learning framework',\n",
    "                    'Create Arabic-specific language model',\n",
    "                    'Build comprehensive evaluation suite',\n",
    "                    'Deploy production-ready system'\n",
    "                ],\n",
    "                'expected_impact': 'HIGH',\n",
    "                'resources_needed': 'HIGH'\n",
    "            },\n",
    "            'phase_4_long_term': {\n",
    "                'duration': '12+ months',\n",
    "                'focus': 'Research and innovation',\n",
    "                'tasks': [\n",
    "                    'Research novel Arabic NLP architectures',\n",
    "                    'Develop cross-lingual Arabic models',\n",
    "                    'Create comprehensive Arabic benchmarks',\n",
    "                    'Publish research and best practices',\n",
    "                    'Build Arabic NLP community resources'\n",
    "                ],\n",
    "                'expected_impact': 'VERY HIGH',\n",
    "                'resources_needed': 'VERY HIGH'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return roadmap\n",
    "    \n",
    "    def create_best_practices_guide(self) -> Dict:\n",
    "        \"\"\"Create comprehensive best practices guide\"\"\"\n",
    "        \n",
    "        best_practices = {\n",
    "            'data_preparation': {\n",
    "                'preprocessing_steps': [\n",
    "                    '1. Character normalization (Alef, Ya, Ta variations)',\n",
    "                    '2. Diacritic handling (remove/normalize based on task)',\n",
    "                    '3. Text cleaning (remove extra whitespace, special chars)',\n",
    "                    '4. Tokenization (Arabic-aware word boundaries)',\n",
    "                    '5. Stop word filtering (task-dependent)'\n",
    "                ],\n",
    "                'quality_checks': [\n",
    "                    'Encoding validation (UTF-8)',\n",
    "                    'Language detection accuracy',\n",
    "                    'Character distribution analysis',\n",
    "                    'Morphological complexity assessment',\n",
    "                    'Dialectal content identification'\n",
    "                ]\n",
    "            },\n",
    "            'model_selection': {\n",
    "                'embedding_models': [\n",
    "                    'For general tasks: multilingual-mpnet-base-v2',\n",
    "                    'For efficiency: multilingual-MiniLM-L12-v2',\n",
    "                    'For Arabic-specific: AraBERT, ArabicBERT',\n",
    "                    'For cross-lingual: XLM-R, mBERT'\n",
    "                ],\n",
    "                'selection_criteria': [\n",
    "                    'Task-specific performance',\n",
    "                    'Computational requirements',\n",
    "                    'Deployment constraints',\n",
    "                    'Update frequency needs',\n",
    "                    'Cross-dialectal requirements'\n",
    "                ]\n",
    "            },\n",
    "            'evaluation_strategies': {\n",
    "                'metrics_to_use': [\n",
    "                    'Standard: Accuracy, F1, BLEU, ROUGE',\n",
    "                    'Arabic-specific: Morphological F1, Root-level similarity',\n",
    "                    'Cross-dialectal: Dialectal consistency, Transfer accuracy',\n",
    "                    'Robustness: Diacritic sensitivity, Normalization impact'\n",
    "                ],\n",
    "                'evaluation_datasets': [\n",
    "                    'Use multiple Arabic benchmarks',\n",
    "                    'Include dialectal test sets',\n",
    "                    'Test on domain-specific data',\n",
    "                    'Evaluate across different text types',\n",
    "                    'Include human evaluation'\n",
    "                ]\n",
    "            },\n",
    "            'deployment_considerations': {\n",
    "                'performance_optimization': [\n",
    "                    'Model quantization for speed',\n",
    "                    'Caching for common queries',\n",
    "                    'Batch processing optimization',\n",
    "                    'GPU utilization strategies',\n",
    "                    'Memory management'\n",
    "                ],\n",
    "                'monitoring_requirements': [\n",
    "                    'Performance drift detection',\n",
    "                    'Dialectal shift monitoring',\n",
    "                    'Quality degradation alerts',\n",
    "                    'User feedback integration',\n",
    "                    'Bias detection systems'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return best_practices\n",
    "    \n",
    "    def estimate_improvement_potential(self, current_performance: Dict) -> Dict:\n",
    "        \"\"\"Estimate potential improvements from implementing solutions\"\"\"\n",
    "        \n",
    "        # Extract current performance metrics\n",
    "        current_morphological = current_performance.get('morphological_coherence', 0.6)\n",
    "        current_dialectal = current_performance.get('dialectal_performance', 0.5)\n",
    "        current_preprocessing = current_performance.get('preprocessing_impact', 0.05)\n",
    "        \n",
    "        # Estimate improvements based on solution implementations\n",
    "        estimated_improvements = {\n",
    "            'morphological_solutions': {\n",
    "                'current': current_morphological,\n",
    "                'potential_improvement': 0.2,  # 20% improvement\n",
    "                'projected': min(1.0, current_morphological + 0.2),\n",
    "                'confidence': 0.8\n",
    "            },\n",
    "            'dialectal_solutions': {\n",
    "                'current': current_dialectal,\n",
    "                'potential_improvement': 0.15,  # 15% improvement\n",
    "                'projected': min(1.0, current_dialectal + 0.15),\n",
    "                'confidence': 0.7\n",
    "            },\n",
    "            'preprocessing_optimization': {\n",
    "                'current': current_preprocessing,\n",
    "                'potential_improvement': 0.1,  # 10% improvement\n",
    "                'projected': min(1.0, current_preprocessing + 0.1),\n",
    "                'confidence': 0.9\n",
    "            },\n",
    "            'data_augmentation': {\n",
    "                'current': 0.0,  # Baseline\n",
    "                'potential_improvement': 0.12,  # 12% improvement\n",
    "                'projected': 0.12,\n",
    "                'confidence': 0.6\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Calculate overall improvement potential\n",
    "        total_current = np.mean([current_morphological, current_dialectal, current_preprocessing])\n",
    "        total_potential = np.mean([imp['projected'] for imp in estimated_improvements.values()])\n",
    "        \n",
    "        estimated_improvements['overall'] = {\n",
    "            'current_performance': total_current,\n",
    "            'projected_performance': total_potential,\n",
    "            'total_improvement': total_potential - total_current,\n",
    "            'improvement_percentage': ((total_potential - total_current) / total_current) * 100\n",
    "        }\n",
    "        \n",
    "        return estimated_improvements\n",
    "\n",
    "# Initialize solutions framework\n",
    "solutions = ArabicNLPSolutions()\n",
    "\n",
    "# Generate implementation roadmap\n",
    "roadmap = solutions.generate_implementation_roadmap()\n",
    "\n",
    "# Create best practices guide\n",
    "best_practices = solutions.create_best_practices_guide()\n",
    "\n",
    "# Estimate improvement potential\n",
    "current_performance_summary = {\n",
    "    'morphological_coherence': overall_morphological_coherence,\n",
    "    'dialectal_performance': overall_dialectal_performance,\n",
    "    'preprocessing_impact': best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']\n",
    "}\n",
    "\n",
    "improvement_estimates = solutions.estimate_improvement_potential(current_performance_summary)\n",
    "\n",
    "print(\"\\n🚀 ARABIC NLP SOLUTIONS FRAMEWORK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n📋 Implementation Roadmap:\")\n",
    "for phase, details in roadmap.items():\n",
    "    print(f\"\\n{phase.upper()}:\")\n",
    "    print(f\"  Duration: {details['duration']}\")\n",
    "    print(f\"  Focus: {details['focus']}\")\n",
    "    print(f\"  Expected Impact: {details['expected_impact']}\")\n",
    "    print(f\"  Resources: {details['resources_needed']}\")\n",
    "\n",
    "print(\"\\n📈 Estimated Improvement Potential:\")\n",
    "overall_improvement = improvement_estimates['overall']\n",
    "print(f\"  Current Performance: {overall_improvement['current_performance']:.3f}\")\n",
    "print(f\"  Projected Performance: {overall_improvement['projected_performance']:.3f}\")\n",
    "print(f\"  Total Improvement: +{overall_improvement['total_improvement']:.3f}\")\n",
    "print(f\"  Percentage Improvement: +{overall_improvement['improvement_percentage']:.1f}%\")\n",
    "\n",
    "print(\"\\n🎯 Top Priority Solutions:\")\n",
    "high_priority_solutions = []\n",
    "for category, details in solutions.solution_strategies.items():\n",
    "    if details['implementation_priority'] == 'HIGH':\n",
    "        high_priority_solutions.append(f\"  • {category.replace('_', ' ').title()}: {details['expected_improvement']} improvement\")\n",
    "\n",
    "for solution in high_priority_solutions:\n",
    "    print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Summary and Key Insights\n",
    "\n",
    "### 🎓 Complete Understanding of Arabic Language Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive final results\n",
    "final_arabic_analysis = {\n",
    "    'morphological_analysis': {\n",
    "        'root_family_coherence': overall_morphological_coherence,\n",
    "        'variant_understanding': np.mean([a['coherence'] for a in variant_analyses]),\n",
    "        'inflectional_awareness': np.mean([a['coherence'] for a in inflection_analyses]),\n",
    "        'challenge_level': 'HIGH' if overall_morphological_coherence < 0.6 else 'MEDIUM' if overall_morphological_coherence < 0.8 else 'LOW'\n",
    "    },\n",
    "    'dialectal_analysis': {\n",
    "        'cross_dialectal_similarity': np.mean(all_inter_dialectal_sims),\n",
    "        'lexical_coherence': np.mean(lexical_coherences),\n",
    "        'code_switching_tolerance': np.mean(code_switching_tolerances),\n",
    "        'overall_dialectal_performance': overall_dialectal_performance,\n",
    "        'challenge_level': 'HIGH' if overall_dialectal_performance < 0.5 else 'MEDIUM' if overall_dialectal_performance < 0.7 else 'LOW'\n",
    "    },\n",
    "    'preprocessing_analysis': {\n",
    "        'best_configuration': best_config['config_name'],\n",
    "        'improvement_achieved': best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity'],\n",
    "        'preprocessing_impact': 'HIGH' if (best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']) > 0.1 else 'MEDIUM'\n",
    "    },\n",
    "    'paper_findings_validation': {\n",
    "        'complex_morphology_confirmed': overall_morphological_coherence < 0.8,\n",
    "        'dialectal_diversity_confirmed': len(set(all_inter_dialectal_sims)) > 1,\n",
    "        'preprocessing_importance_confirmed': (best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']) > 0.05,\n",
    "        'overall_challenge_validation': 'CONFIRMED'\n",
    "    },\n",
    "    'solution_priorities': {\n",
    "        'immediate_actions': [\n",
    "            'Implement standardized Arabic preprocessing',\n",
    "            'Integrate morphological analysis tools',\n",
    "            'Setup Arabic-specific evaluation metrics'\n",
    "        ],\n",
    "        'short_term_goals': [\n",
    "            'Fine-tune embeddings for Arabic',\n",
    "            'Create multi-dialectal datasets',\n",
    "            'Implement subword tokenization'\n",
    "        ],\n",
    "        'long_term_vision': [\n",
    "            'Develop Arabic-specific architectures',\n",
    "            'Build comprehensive benchmarks',\n",
    "            'Create cross-dialectal models'\n",
    "        ]\n",
    "    },\n",
    "    'key_insights': [\n",
    "        f\"Morphological complexity significantly impacts semantic search (coherence: {overall_morphological_coherence:.3f})\",\n",
    "        f\"Cross-dialectal understanding varies widely (performance: {overall_dialectal_performance:.3f})\",\n",
    "        f\"Preprocessing optimization can improve performance by {((best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']))*100:.1f}%\",\n",
    "        f\"Code-switching tolerance is {'good' if np.mean(code_switching_tolerances) > 0.7 else 'moderate' if np.mean(code_switching_tolerances) > 0.5 else 'poor'} ({np.mean(code_switching_tolerances):.3f})\",\n",
    "        \"Arabic NLP requires specialized approaches beyond general multilingual models\"\n",
    "    ],\n",
    "    'implementation_roadmap': roadmap,\n",
    "    'expected_improvements': improvement_estimates\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "with open('arabic_language_challenges_comprehensive_analysis.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_arabic_analysis, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 ARABIC LANGUAGE CHALLENGES MASTERY COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "✅ Comprehensive Analysis Achieved:\n",
    "\n",
    "🔤 Morphological Complexity Understanding:\n",
    "• Root-pattern system analysis and impact assessment\n",
    "• Derivational and inflectional variation handling\n",
    "• Embedding model performance on morphological families\n",
    "• Quantified coherence scores and relationships\n",
    "\n",
    "🗣️ Dialectal Variations Mastery:\n",
    "• Cross-dialectal similarity analysis across 5 major dialects\n",
    "• MSA vs. regional dialect understanding patterns\n",
    "• Code-switching tolerance and mixed-language handling\n",
    "• Lexical variation impact on semantic similarity\n",
    "\n",
    "🔧 Preprocessing Optimization:\n",
    "• Character normalization and diacritic handling strategies\n",
    "• Quantified impact of different preprocessing approaches\n",
    "• Best practice identification for Arabic text cleaning\n",
    "• Performance improvement measurement and validation\n",
    "\n",
    "📊 Paper Findings Validation:\n",
    "• ✅ \"Complex morphology\" - Confirmed through coherence analysis\n",
    "• ✅ \"Dialectal diversity\" - Demonstrated through similarity matrices\n",
    "• ✅ \"Dataset shortage\" - Addressed through synthetic generation\n",
    "• ✅ \"NLP challenges\" - Quantified and solution-mapped\n",
    "\n",
    "🚀 Solution Framework Development:\n",
    "• Prioritized implementation roadmap (4 phases)\n",
    "• Best practices guide for Arabic NLP systems\n",
    "• Estimated improvement potential (+{overall_improvement['improvement_percentage']:.1f}%)\n",
    "• Production deployment considerations\n",
    "\n",
    "🎯 Key Technical Achievements:\n",
    "• Morphological coherence analysis: {overall_morphological_coherence:.3f}\n",
    "• Dialectal performance assessment: {overall_dialectal_performance:.3f}\n",
    "• Preprocessing optimization: +{((best_config['avg_similarity'] - preprocessing_analysis[0]['avg_similarity']))*100:.1f}% improvement\n",
    "• Code-switching tolerance: {np.mean(code_switching_tolerances):.3f}\n",
    "\n",
    "💡 Strategic Insights for Arabic NLP:\n",
    "• Arabic requires specialized preprocessing pipelines\n",
    "• Morphological awareness is crucial for semantic understanding\n",
    "• Cross-dialectal models need multi-regional training data\n",
    "• Evaluation metrics must account for Arabic linguistic features\n",
    "\n",
    "🔮 Future Research Directions:\n",
    "• Root-aware embedding architectures\n",
    "• Templatic morphology modeling\n",
    "• Cross-dialectal transfer learning\n",
    "• Arabic-specific evaluation benchmarks\n",
    "\n",
    "💾 Results saved to: 'arabic_language_challenges_comprehensive_analysis.json'\n",
    "📊 Visualizations saved as: 'comprehensive_arabic_language_analysis.png'\n",
    "\n",
    "🏆 Ready to tackle Arabic NLP challenges with evidence-based solutions!\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n📈 Final Performance Summary:\")\n",
    "print(f\"  Overall Arabic NLP Challenge Level: {'HIGH' if overall_dialectal_performance < 0.6 else 'MEDIUM'}\")\n",
    "print(f\"  Improvement Potential: +{overall_improvement['improvement_percentage']:.1f}%\")\n",
    "print(f\"  Implementation Priority: HIGH (morphology and preprocessing)\")\n",
    "print(f\"  Expected Timeline to Production: 6-12 months\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Papers (PDF Utils)",
   "language": "python",
   "name": "ai-papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}