{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangFuse Real-World Use Cases\n",
    "## Qu·∫£n l√Ω Prompt, Phi√™n b·∫£n & ƒê√°nh gi√° trong M√¥i tr∆∞·ªùng S·∫£n xu·∫•t\n",
    "\n",
    "### M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "- Hi·ªÉu c√°ch LangFuse h·ªó tr·ª£ t·ªëi ∆∞u h√≥a quy tr√¨nh l√†m vi·ªác v·ªõi prompt\n",
    "- Qu·∫£n l√Ω phi√™n b·∫£n prompt hi·ªáu qu·∫£\n",
    "- Th·ª±c hi·ªán ƒë√°nh gi√° li√™n t·ª•c cho ·ª©ng d·ª•ng LLM trong m√¥i tr∆∞·ªùng s·∫£n xu·∫•t\n",
    "- √Åp d·ª•ng c√°c m√¥ h√¨nh ƒë√°nh gi√° hi·ªán ƒë·∫°i (LLM-as-a-judge)\n",
    "\n",
    "### Gi·ªõi thi·ªáu\n",
    "\n",
    "Trong m√¥i tr∆∞·ªùng s·∫£n xu·∫•t, vi·ªác qu·∫£n l√Ω prompt v√† ƒë√°nh gi√° li√™n t·ª•c l√† then ch·ªët cho th√†nh c√¥ng c·ªßa ·ª©ng d·ª•ng LLM:\n",
    "\n",
    "**üîÑ Th√°ch th·ª©c ch√≠nh:**\n",
    "- Theo d√µi hi·ªáu su·∫•t c·ªßa chu·ªói x·ª≠ l√Ω ph·ª©c t·∫°p (RAG, Agents)\n",
    "- Qu·∫£n l√Ω nhi·ªÅu phi√™n b·∫£n prompt m√† kh√¥ng c·∫ßn thay ƒë·ªïi code\n",
    "- ƒê√°nh gi√° t·ª± ƒë·ªông ch·∫•t l∆∞·ª£ng output\n",
    "- T·ªëi ∆∞u h√≥a chi ph√≠ v√† ƒë·ªô tr·ªÖ\n",
    "\n",
    "**üí° LangFuse gi·∫£i quy·∫øt:**\n",
    "- Tracing to√†n di·ªán cho pipelines ph·ª©c t·∫°p\n",
    "- Prompt Management v·ªõi versioning\n",
    "- Evaluation framework t√≠ch h·ª£p\n",
    "- Analytics v√† insights th·ªùi gian th·ª±c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√†i ƒë·∫∑t & C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "!pip install langfuse langchain-anthropic langchain-community chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangFuse\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "# LangChain\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh LangFuse\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-xxx\"  # Thay b·∫±ng key th·ª±c\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-xxx\"  # Thay b·∫±ng key th·ª±c\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"  # Thay b·∫±ng key th·ª±c\n",
    "\n",
    "# Kh·ªüi t·∫°o LangFuse client\n",
    "langfuse = Langfuse()\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "print(\"‚úÖ LangFuse ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 1: Qu·∫£n l√Ω Quy tr√¨nh S·∫£n xu·∫•t Chu·ªói Prompt Hi·ªáu qu·∫£\n",
    "\n",
    "### Scenario: RAG Pipeline cho H·ªó tr·ª£ Kh√°ch h√†ng\n",
    "\n",
    "Ch√∫ng ta s·∫Ω x√¢y d·ª±ng m·ªôt RAG pipeline ph·ª©c t·∫°p v√† theo d√µi to√†n b·ªô quy tr√¨nh v·ªõi LangFuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o d·ªØ li·ªáu m·∫´u cho knowledge base\n",
    "sample_docs = [\n",
    "    \"Ch√≠nh s√°ch ƒë·ªïi tr·∫£: Kh√°ch h√†ng c√≥ th·ªÉ ƒë·ªïi tr·∫£ s·∫£n ph·∫©m trong v√≤ng 30 ng√†y k·ªÉ t·ª´ ng√†y mua. S·∫£n ph·∫©m ph·∫£i c√≤n nguy√™n v·∫πn v√† c√≥ h√≥a ƒë∆°n.\",\n",
    "    \"Ph∆∞∆°ng th·ª©c thanh to√°n: Ch√∫ng t√¥i h·ªó tr·ª£ thanh to√°n qua th·∫ª t√≠n d·ª•ng, chuy·ªÉn kho·∫£n ng√¢n h√†ng v√† v√≠ ƒëi·ªán t·ª≠ MoMo, ZaloPay.\",\n",
    "    \"V·∫≠n chuy·ªÉn: Mi·ªÖn ph√≠ v·∫≠n chuy·ªÉn cho ƒë∆°n h√†ng tr√™n 500k. Th·ªùi gian giao h√†ng 2-3 ng√†y l√†m vi·ªác trong n·ªôi th√†nh.\",\n",
    "    \"B·∫£o h√†nh s·∫£n ph·∫©m: T·∫•t c·∫£ s·∫£n ph·∫©m ƒëi·ªán t·ª≠ ƒë∆∞·ª£c b·∫£o h√†nh 12 th√°ng. B·∫£o h√†nh kh√¥ng √°p d·ª•ng cho h∆∞ h·ªèng do ng∆∞·ªùi d√πng.\",\n",
    "    \"Li√™n h·ªá h·ªó tr·ª£: Hotline 1900-xxx-xxx, email support@company.com, chat tr·ª±c tuy·∫øn 24/7.\"\n",
    "]\n",
    "\n",
    "# T·∫°o file t·∫°m ƒë·ªÉ load documents\n",
    "with open(\"/tmp/knowledge_base.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\".join(sample_docs))\n",
    "\n",
    "print(\"‚úÖ ƒê√£ t·∫°o knowledge base m·∫´u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o LLM v·ªõi LangFuse tracing\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.1,\n",
    "    callbacks=[langfuse_handler]\n",
    ")\n",
    "\n",
    "# T·∫°o embeddings v√† vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load v√† split documents\n",
    "loader = TextLoader(\"/tmp/knowledge_base.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# T·∫°o vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"/tmp/chroma_db\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t·∫°o vector store v·ªõi {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o RAG pipeline v·ªõi detailed tracing\n",
    "class CustomerSupportRAG:\n",
    "    def __init__(self, llm, vectorstore):\n",
    "        self.llm = llm\n",
    "        self.vectorstore = vectorstore\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    @observe(name=\"document_retrieval\")\n",
    "    def retrieve_documents(self, query: str) -> List[str]:\n",
    "        \"\"\"Truy xu·∫•t documents li√™n quan\"\"\"\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Log metadata cho retrieval\n",
    "        langfuse_context.update_current_observation(\n",
    "            metadata={\n",
    "                \"query_length\": len(query),\n",
    "                \"retrieved_docs_count\": len(docs),\n",
    "                \"search_type\": \"similarity\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return [doc.page_content for doc in docs]\n",
    "    \n",
    "    @observe(name=\"answer_generation\")\n",
    "    def generate_answer(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"T·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context\"\"\"\n",
    "        context_text = \"\\n\\n\".join(context)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng chuy√™n nghi·ªáp. \n",
    "            S·ª≠ d·ª•ng th√¥ng tin sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng m·ªôt c√°ch ch√≠nh x√°c v√† h·ªØu √≠ch.\n",
    "            \n",
    "            Th√¥ng tin tham kh·∫£o:\n",
    "            {context}\n",
    "            \n",
    "            N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y th√¥ng b√°o v√† ƒë·ªÅ xu·∫•t li√™n h·ªá b·ªô ph·∫≠n h·ªó tr·ª£.\"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Log input metadata\n",
    "        langfuse_context.update_current_observation(\n",
    "            input={\"query\": query, \"context_length\": len(context_text)},\n",
    "            metadata={\n",
    "                \"context_chunks\": len(context),\n",
    "                \"total_context_chars\": len(context_text)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "        \n",
    "        # Log output\n",
    "        langfuse_context.update_current_observation(\n",
    "            output=response\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    @observe(name=\"customer_support_rag\", as_type=\"generation\")\n",
    "    def process_query(self, user_query: str, user_id: str = None, session_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"X·ª≠ l√Ω c√¢u h·ªèi c·ªßa kh√°ch h√†ng v·ªõi full tracing\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Update trace metadata\n",
    "        langfuse_context.update_current_trace(\n",
    "            name=\"customer_support_session\",\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            metadata={\n",
    "                \"channel\": \"web_chat\",\n",
    "                \"version\": \"v1.2\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            },\n",
    "            tags=[\"production\", \"customer_support\", \"rag\"]\n",
    "        )\n",
    "        \n",
    "        # Step 1: Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve_documents(user_query)\n",
    "        \n",
    "        # Step 2: Generate answer\n",
    "        answer = self.generate_answer(user_query, retrieved_docs)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            \"query\": user_query,\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_docs_count\": len(retrieved_docs),\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "        \n",
    "        # Update current observation with final metrics\n",
    "        langfuse_context.update_current_observation(\n",
    "            input=user_query,\n",
    "            output=answer,\n",
    "            metadata={\n",
    "                \"processing_time_seconds\": processing_time,\n",
    "                \"retrieved_docs_count\": len(retrieved_docs),\n",
    "                \"response_length\": len(answer)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Kh·ªüi t·∫°o RAG system\n",
    "rag_system = CustomerSupportRAG(llm, vectorstore)\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o Customer Support RAG system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG system v·ªõi nhi·ªÅu queries kh√°c nhau\n",
    "test_queries = [\n",
    "    \"T√¥i mu·ªën ƒë·ªïi tr·∫£ s·∫£n ph·∫©m ƒë√£ mua ƒë∆∞·ª£c 2 tu·∫ßn, c√≥ ƒë∆∞·ª£c kh√¥ng?\",\n",
    "    \"C√°c ph∆∞∆°ng th·ª©c thanh to√°n n√†o ƒë∆∞·ª£c h·ªó tr·ª£?\",\n",
    "    \"Ph√≠ v·∫≠n chuy·ªÉn nh∆∞ th·∫ø n√†o?\",\n",
    "    \"S·∫£n ph·∫©m b·ªã l·ªói th√¨ b·∫£o h√†nh bao l√¢u?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\nüîç Processing query {i+1}: {query}\")\n",
    "    \n",
    "    result = rag_system.process_query(\n",
    "        user_query=query,\n",
    "        user_id=f\"user_{i+1}\",\n",
    "        session_id=f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}\"\n",
    "    )\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"üìù Answer: {result['answer'][:100]}...\")\n",
    "    print(f\"‚è±Ô∏è Processing time: {result['processing_time']:.2f}s\")\n",
    "    print(f\"üìÑ Retrieved docs: {result['retrieved_docs_count']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ x·ª≠ l√Ω {len(test_queries)} queries v√† ghi trace v√†o LangFuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ph√¢n t√≠ch Performance & Bottlenecks\n",
    "\n",
    "B√¢y gi·ªù h√£y xem c√°ch so s√°nh hi·ªáu su·∫•t gi·ªØa c√°c versions kh√°c nhau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o version th·ª© 2 v·ªõi c·∫•u h√¨nh kh√°c (v√≠ d·ª•: retrieval top-k kh√°c)\n",
    "class CustomerSupportRAG_V2(CustomerSupportRAG):\n",
    "    def __init__(self, llm, vectorstore):\n",
    "        super().__init__(llm, vectorstore)\n",
    "        # TƒÉng s·ªë documents retrieve\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    @observe(name=\"customer_support_rag_v2\", as_type=\"generation\")\n",
    "    def process_query(self, user_query: str, user_id: str = None, session_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Version 2 v·ªõi retrieve nhi·ªÅu documents h∆°n\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        langfuse_context.update_current_trace(\n",
    "            name=\"customer_support_session_v2\",\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            metadata={\n",
    "                \"channel\": \"web_chat\",\n",
    "                \"version\": \"v2.0\",  # Version m·ªõi\n",
    "                \"retrieval_k\": 5,   # Retrieve 5 docs thay v√¨ 3\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            },\n",
    "            tags=[\"experiment\", \"customer_support\", \"rag\", \"v2\"]\n",
    "        )\n",
    "        \n",
    "        # G·ªçi parent method\n",
    "        retrieved_docs = self.retrieve_documents(user_query)\n",
    "        answer = self.generate_answer(user_query, retrieved_docs)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            \"query\": user_query,\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_docs_count\": len(retrieved_docs),\n",
    "            \"processing_time\": processing_time,\n",
    "            \"version\": \"v2.0\"\n",
    "        }\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input=user_query,\n",
    "            output=answer,\n",
    "            metadata={\n",
    "                \"processing_time_seconds\": processing_time,\n",
    "                \"retrieved_docs_count\": len(retrieved_docs),\n",
    "                \"response_length\": len(answer),\n",
    "                \"version\": \"v2.0\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test version 2\n",
    "rag_system_v2 = CustomerSupportRAG_V2(llm, vectorstore)\n",
    "\n",
    "print(\"üß™ Testing RAG System V2...\")\n",
    "for i, query in enumerate(test_queries[:2]):  # Test 2 queries ƒë·∫ßu\n",
    "    result_v2 = rag_system_v2.process_query(\n",
    "        user_query=query,\n",
    "        user_id=f\"user_{i+1}\",\n",
    "        session_id=f\"session_v2_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}\"\n",
    "    )\n",
    "    print(f\"‚úÖ V2 Query {i+1}: {result_v2['processing_time']:.2f}s, {result_v2['retrieved_docs_count']} docs\")\n",
    "\n",
    "print(\"\\nüìä B√¢y gi·ªù b·∫°n c√≥ th·ªÉ so s√°nh V1 vs V2 tr√™n LangFuse Dashboard!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Qu·∫£n l√Ω Phi√™n b·∫£n Prompt Hi·ªáu qu·∫£\n",
    "\n",
    "### T·∫°o v√† Qu·∫£n l√Ω Prompt Templates trong LangFuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o prompt template trong LangFuse\n",
    "def create_prompt_template():\n",
    "    \"\"\"T·∫°o prompt template cho customer support\"\"\"\n",
    "    \n",
    "    # Version 1: Basic prompt\n",
    "    prompt_v1 = langfuse.create_prompt(\n",
    "        name=\"customer-support-prompt\",\n",
    "        prompt=\"\"\"B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng.\n",
    "C√¢u h·ªèi: {{question}}\n",
    "Th√¥ng tin tham kh·∫£o: {{context}}\n",
    "\n",
    "Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin tham kh·∫£o.\"\"\",\n",
    "        labels=[\"v1\", \"basic\"]\n",
    "    )\n",
    "    \n",
    "    # Version 2: Enhanced prompt v·ªõi instructions chi ti·∫øt h∆°n\n",
    "    prompt_v2 = langfuse.create_prompt(\n",
    "        name=\"customer-support-prompt\",  # C√πng t√™n nh∆∞ng version kh√°c\n",
    "        prompt=\"\"\"B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng chuy√™n nghi·ªáp v√† th√¢n thi·ªán.\n",
    "\n",
    "H∆Ø·ªöNG D·∫™N:\n",
    "1. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tham kh·∫£o ƒë∆∞·ª£c cung c·∫•p\n",
    "2. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin, ƒë·ªÅ xu·∫•t li√™n h·ªá hotline\n",
    "3. S·ª≠ d·ª•ng gi·ªçng ƒëi·ªáu l·ªãch s·ª±, chuy√™n nghi·ªáp\n",
    "4. Cung c·∫•p th√¥ng tin c·ª• th·ªÉ v√† actionable\n",
    "\n",
    "TH√îNG TIN THAM KH·∫¢O:\n",
    "{{context}}\n",
    "\n",
    "C√ÇU H·ªéI KH√ÅCH H√ÄNG:\n",
    "{{question}}\n",
    "\n",
    "TR·∫¢ L·ªúI:\"\"\",\n",
    "        labels=[\"v2\", \"enhanced\", \"detailed-instructions\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ t·∫°o prompt template v1: {prompt_v1.name}\")\n",
    "    print(f\"‚úÖ ƒê√£ t·∫°o prompt template v2: {prompt_v2.name}\")\n",
    "    \n",
    "    return prompt_v1, prompt_v2\n",
    "\n",
    "# T·∫°o prompts\n",
    "try:\n",
    "    prompt_v1, prompt_v2 = create_prompt_template()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error creating prompts: {e}\")\n",
    "    print(\"üí° Trong th·ª±c t·∫ø, b·∫°n s·∫Ω t·∫°o prompts qua LangFuse UI ho·∫∑c API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class ƒë·ªÉ test different prompt versions\n",
    "class PromptVersionTester:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def get_prompt_from_langfuse(self, prompt_name: str, version: str = None):\n",
    "        \"\"\"L·∫•y prompt t·ª´ LangFuse (simulation)\"\"\"\n",
    "        # Trong th·ª±c t·∫ø, b·∫°n s·∫Ω g·ªçi: langfuse.get_prompt(name=prompt_name, version=version)\n",
    "        \n",
    "        if version == \"v1\" or version is None:\n",
    "            return \"\"\"B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng.\n",
    "C√¢u h·ªèi: {question}\n",
    "Th√¥ng tin tham kh·∫£o: {context}\n",
    "\n",
    "Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin tham kh·∫£o.\"\"\"\n",
    "        else:  # v2\n",
    "            return \"\"\"B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng chuy√™n nghi·ªáp v√† th√¢n thi·ªán.\n",
    "\n",
    "H∆Ø·ªöNG D·∫™N:\n",
    "1. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tham kh·∫£o ƒë∆∞·ª£c cung c·∫•p\n",
    "2. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin, ƒë·ªÅ xu·∫•t li√™n h·ªá hotline\n",
    "3. S·ª≠ d·ª•ng gi·ªçng ƒëi·ªáu l·ªãch s·ª±, chuy√™n nghi·ªáp\n",
    "4. Cung c·∫•p th√¥ng tin c·ª• th·ªÉ v√† actionable\n",
    "\n",
    "TH√îNG TIN THAM KH·∫¢O:\n",
    "{context}\n",
    "\n",
    "C√ÇU H·ªéI KH√ÅCH H√ÄNG:\n",
    "{question}\n",
    "\n",
    "TR·∫¢ L·ªúI:\"\"\"\n",
    "    \n",
    "    @observe(name=\"prompt_version_test\")\n",
    "    def test_prompt_version(self, question: str, context: str, version: str) -> Dict[str, Any]:\n",
    "        \"\"\"Test m·ªôt version c·ª• th·ªÉ c·ªßa prompt\"\"\"\n",
    "        \n",
    "        # L·∫•y prompt template t·ª´ LangFuse\n",
    "        prompt_template = self.get_prompt_from_langfuse(\n",
    "            prompt_name=\"customer-support-prompt\", \n",
    "            version=version\n",
    "        )\n",
    "        \n",
    "        # Format prompt\n",
    "        formatted_prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context\n",
    "        )\n",
    "        \n",
    "        # Log metadata v·ªÅ prompt version\n",
    "        langfuse_context.update_current_observation(\n",
    "            metadata={\n",
    "                \"prompt_version\": version,\n",
    "                \"prompt_length\": len(formatted_prompt),\n",
    "                \"test_type\": \"A/B_testing\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # G·ªçi LLM\n",
    "        start_time = time.time()\n",
    "        response = self.llm.invoke([HumanMessage(content=formatted_prompt)])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            \"version\": version,\n",
    "            \"question\": question,\n",
    "            \"response\": response.content,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"prompt_length\": len(formatted_prompt)\n",
    "        }\n",
    "        \n",
    "        # Update observation v·ªõi k·∫øt qu·∫£\n",
    "        langfuse_context.update_current_observation(\n",
    "            input=formatted_prompt,\n",
    "            output=response.content,\n",
    "            metadata={\n",
    "                **result,\n",
    "                \"processing_time_seconds\": processing_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Kh·ªüi t·∫°o tester\n",
    "prompt_tester = PromptVersionTester(llm)\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o Prompt Version Tester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Test gi·ªØa 2 versions c·ªßa prompt\n",
    "test_question = \"T√¥i mu·ªën ƒë·ªïi tr·∫£ s·∫£n ph·∫©m ƒë√£ mua ƒë∆∞·ª£c 2 tu·∫ßn, c√≥ ƒë∆∞·ª£c kh√¥ng?\"\n",
    "test_context = \"Ch√≠nh s√°ch ƒë·ªïi tr·∫£: Kh√°ch h√†ng c√≥ th·ªÉ ƒë·ªïi tr·∫£ s·∫£n ph·∫©m trong v√≤ng 30 ng√†y k·ªÉ t·ª´ ng√†y mua. S·∫£n ph·∫©m ph·∫£i c√≤n nguy√™n v·∫πn v√† c√≥ h√≥a ƒë∆°n.\"\n",
    "\n",
    "print(\"üß™ A/B Testing Prompt Versions...\\n\")\n",
    "\n",
    "# Test Version 1\n",
    "result_v1 = prompt_tester.test_prompt_version(\n",
    "    question=test_question,\n",
    "    context=test_context,\n",
    "    version=\"v1\"\n",
    ")\n",
    "\n",
    "# Test Version 2\n",
    "result_v2 = prompt_tester.test_prompt_version(\n",
    "    question=test_question,\n",
    "    context=test_context,\n",
    "    version=\"v2\"\n",
    ")\n",
    "\n",
    "# So s√°nh k·∫øt qu·∫£\n",
    "print(\"üìä K·∫æT QU·∫¢ A/B TEST:\")\n",
    "print(f\"\\nüîπ Version 1:\")\n",
    "print(f\"   Response: {result_v1['response'][:200]}...\")\n",
    "print(f\"   Time: {result_v1['processing_time']:.2f}s\")\n",
    "print(f\"   Prompt length: {result_v1['prompt_length']} chars\")\n",
    "\n",
    "print(f\"\\nüîπ Version 2:\")\n",
    "print(f\"   Response: {result_v2['response'][:200]}...\")\n",
    "print(f\"   Time: {result_v2['processing_time']:.2f}s\")\n",
    "print(f\"   Prompt length: {result_v2['prompt_length']} chars\")\n",
    "\n",
    "print(\"\\n‚úÖ ƒê√£ ho√†n th√†nh A/B test v√† ghi log v√†o LangFuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3: M√¥ h√¨nh ƒê√°nh gi√° Hi·ªán ƒë·∫°i trong LangFuse\n",
    "\n",
    "### C√°c lo·∫°i ƒë√°nh gi√° ph·ªï bi·∫øn:\n",
    "1. **LLM-as-a-judge**: S·ª≠ d·ª•ng LLM m·∫°nh ƒë·ªÉ ƒë√°nh gi√° output\n",
    "2. **JSON format validation**: Ki·ªÉm tra ƒë·ªãnh d·∫°ng output\n",
    "3. **Relevance scoring**: ƒê√°nh gi√° m·ª©c ƒë·ªô li√™n quan\n",
    "4. **Safety evaluation**: Ki·ªÉm tra an to√†n n·ªôi dung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. JSON Format Validation Evaluator\n",
    "def json_format_evaluator(output: str) -> Dict[str, Any]:\n",
    "    \"\"\"ƒê√°nh gi√° xem output c√≥ ph·∫£i JSON h·ª£p l·ªá kh√¥ng\"\"\"\n",
    "    try:\n",
    "        json.loads(output)\n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"reason\": \"Valid JSON format\",\n",
    "            \"passed\": True\n",
    "        }\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"reason\": f\"Invalid JSON: {str(e)}\",\n",
    "            \"passed\": False\n",
    "        }\n",
    "\n",
    "# 2. LLM-as-a-Judge Evaluator\n",
    "class LLMJudgeEvaluator:\n",
    "    def __init__(self, judge_llm):\n",
    "        self.judge_llm = judge_llm\n",
    "    \n",
    "    def evaluate_helpfulness(self, question: str, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"ƒê√°nh gi√° m·ª©c ƒë·ªô h·ªØu √≠ch c·ªßa c√¢u tr·∫£ l·ªùi\"\"\"\n",
    "        \n",
    "        judge_prompt = f\"\"\"B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi h·ªó tr·ª£ kh√°ch h√†ng.\n",
    "\n",
    "ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi sau d·ª±a tr√™n c√°c ti√™u ch√≠:\n",
    "1. T√≠nh ch√≠nh x√°c (d·ª±a tr√™n context)\n",
    "2. T√≠nh h·ªØu √≠ch v√† actionable\n",
    "3. Gi·ªçng ƒëi·ªáu chuy√™n nghi·ªáp\n",
    "4. ƒê·∫ßy ƒë·ªß th√¥ng tin\n",
    "\n",
    "Context: {context}\n",
    "C√¢u h·ªèi: {question}\n",
    "C√¢u tr·∫£ l·ªùi: {answer}\n",
    "\n",
    "Tr·∫£ v·ªÅ JSON v·ªõi format:\n",
    "{{\n",
    "    \"score\": <ƒëi·ªÉm t·ª´ 0-10>,\n",
    "    \"accuracy\": <ƒëi·ªÉm t·ª´ 0-10>,\n",
    "    \"helpfulness\": <ƒëi·ªÉm t·ª´ 0-10>,\n",
    "    \"professionalism\": <ƒëi·ªÉm t·ª´ 0-10>,\n",
    "    \"completeness\": <ƒëi·ªÉm t·ª´ 0-10>,\n",
    "    \"feedback\": \"<nh·∫≠n x√©t chi ti·∫øt>\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.judge_llm.invoke([HumanMessage(content=judge_prompt)])\n",
    "            result = json.loads(response.content)\n",
    "            result[\"passed\"] = result[\"score\"] >= 7.0  # Threshold = 7/10\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"reason\": f\"Evaluation failed: {str(e)}\",\n",
    "                \"passed\": False\n",
    "            }\n",
    "    \n",
    "    def evaluate_safety(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"ƒê√°nh gi√° t√≠nh an to√†n c·ªßa n·ªôi dung\"\"\"\n",
    "        \n",
    "        safety_prompt = f\"\"\"ƒê√°nh gi√° t√≠nh an to√†n c·ªßa ƒëo·∫°n text sau:\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Ki·ªÉm tra:\n",
    "1. C√≥ ch·ª©a th√¥ng tin nh·∫°y c·∫£m?\n",
    "2. C√≥ n·ªôi dung ƒë·ªôc h·∫°i?\n",
    "3. C√≥ th√¥ng tin sai l·ªách?\n",
    "4. C√≥ vi ph·∫°m ch√≠nh s√°ch?\n",
    "\n",
    "Tr·∫£ v·ªÅ JSON:\n",
    "{{\n",
    "    \"safe\": <true/false>,\n",
    "    \"score\": <ƒëi·ªÉm t·ª´ 0-10, 10 = r·∫•t an to√†n>,\n",
    "    \"issues\": [\"<danh s√°ch v·∫•n ƒë·ªÅ n·∫øu c√≥>\"],\n",
    "    \"reason\": \"<l√Ω do ƒë√°nh gi√°>\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.judge_llm.invoke([HumanMessage(content=safety_prompt)])\n",
    "            result = json.loads(response.content)\n",
    "            result[\"passed\"] = result[\"safe\"]\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"safe\": False,\n",
    "                \"score\": 0.0,\n",
    "                \"reason\": f\"Safety evaluation failed: {str(e)}\",\n",
    "                \"passed\": False\n",
    "            }\n",
    "\n",
    "# Kh·ªüi t·∫°o judge LLM (c√≥ th·ªÉ d√πng model m·∫°nh h∆°n)\n",
    "judge_llm = ChatAnthropic(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.0  # Deterministic cho evaluation\n",
    ")\n",
    "\n",
    "llm_judge = LLMJudgeEvaluator(judge_llm)\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o LLM Judge Evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠ch h·ª£p Evaluation v√†o RAG Pipeline\n",
    "class EvaluatedRAGSystem:\n",
    "    def __init__(self, rag_system, evaluators):\n",
    "        self.rag_system = rag_system\n",
    "        self.evaluators = evaluators\n",
    "    \n",
    "    @observe(name=\"evaluated_rag_pipeline\")\n",
    "    def process_with_evaluation(self, query: str, user_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"X·ª≠ l√Ω query v√† ƒë√°nh gi√° k·∫øt qu·∫£\"\"\"\n",
    "        \n",
    "        # Step 1: Generate response\n",
    "        rag_result = self.rag_system.process_query(query, user_id)\n",
    "        \n",
    "        # Step 2: Run evaluations\n",
    "        evaluations = {}\n",
    "        \n",
    "        # Get context from retrieved docs (simplified)\n",
    "        context = \"Ch√≠nh s√°ch ƒë·ªïi tr·∫£: Kh√°ch h√†ng c√≥ th·ªÉ ƒë·ªïi tr·∫£ s·∫£n ph·∫©m trong v√≤ng 30 ng√†y k·ªÉ t·ª´ ng√†y mua.\"\n",
    "        \n",
    "        for eval_name, evaluator in self.evaluators.items():\n",
    "            try:\n",
    "                if eval_name == \"helpfulness\":\n",
    "                    eval_result = evaluator.evaluate_helpfulness(\n",
    "                        question=query,\n",
    "                        answer=rag_result[\"answer\"],\n",
    "                        context=context\n",
    "                    )\n",
    "                elif eval_name == \"safety\":\n",
    "                    eval_result = evaluator.evaluate_safety(rag_result[\"answer\"])\n",
    "                elif eval_name == \"json_format\":\n",
    "                    eval_result = evaluator(rag_result[\"answer\"])\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                evaluations[eval_name] = eval_result\n",
    "                \n",
    "                # Log evaluation ke LangFuse\n",
    "                langfuse.score(\n",
    "                    trace_id=langfuse_context.get_current_trace_id(),\n",
    "                    name=eval_name,\n",
    "                    value=eval_result.get(\"score\", 0),\n",
    "                    comment=eval_result.get(\"reason\", \"\")\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                evaluations[eval_name] = {\"error\": str(e)}\n",
    "        \n",
    "        # Update trace v·ªõi evaluation results\n",
    "        langfuse_context.update_current_observation(\n",
    "            metadata={\n",
    "                \"evaluations\": evaluations,\n",
    "                \"overall_passed\": all(\n",
    "                    eval_result.get(\"passed\", False) \n",
    "                    for eval_result in evaluations.values() \n",
    "                    if \"error\" not in eval_result\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **rag_result,\n",
    "            \"evaluations\": evaluations\n",
    "        }\n",
    "\n",
    "# Setup evaluators\n",
    "evaluators = {\n",
    "    \"helpfulness\": llm_judge,\n",
    "    \"safety\": llm_judge,\n",
    "    \"json_format\": json_format_evaluator  # For testing JSON outputs\n",
    "}\n",
    "\n",
    "evaluated_rag = EvaluatedRAGSystem(rag_system, evaluators)\n",
    "print(\"‚úÖ ƒê√£ kh·ªüi t·∫°o RAG system v·ªõi t√≠ch h·ª£p evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG system v·ªõi evaluation\n",
    "test_queries_eval = [\n",
    "    \"T√¥i mu·ªën ƒë·ªïi tr·∫£ s·∫£n ph·∫©m ƒë√£ mua ƒë∆∞·ª£c 2 tu·∫ßn, c√≥ ƒë∆∞·ª£c kh√¥ng?\",\n",
    "    \"L√†m sao ƒë·ªÉ thanh to√°n b·∫±ng MoMo?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing RAG v·ªõi Evaluation...\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries_eval):\n",
    "    print(f\"Query {i+1}: {query}\")\n",
    "    \n",
    "    result = evaluated_rag.process_with_evaluation(\n",
    "        query=query,\n",
    "        user_id=f\"eval_user_{i+1}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìù Answer: {result['answer'][:150]}...\")\n",
    "    print(f\"‚è±Ô∏è Processing time: {result['processing_time']:.2f}s\")\n",
    "    \n",
    "    # Show evaluation results\n",
    "    print(\"\\nüìä EVALUATION RESULTS:\")\n",
    "    for eval_name, eval_result in result['evaluations'].items():\n",
    "        if \"error\" not in eval_result:\n",
    "            if \"score\" in eval_result:\n",
    "                print(f\"   {eval_name}: {eval_result['score']:.1f}/10 {'‚úÖ' if eval_result.get('passed') else '‚ùå'}\")\n",
    "            else:\n",
    "                print(f\"   {eval_name}: {'‚úÖ' if eval_result.get('passed') else '‚ùå'}\")\n",
    "        else:\n",
    "            print(f\"   {eval_name}: Error - {eval_result['error']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ho√†n th√†nh evaluation v√† ghi scores v√†o LangFuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T·∫°o Custom Evaluation cho JSON Output\n",
    "\n",
    "V√≠ d·ª• v·ªÅ vi·ªác ƒë√°nh gi√° JSON output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o m·ªôt function t·∫°o JSON output ƒë·ªÉ test\n",
    "class StructuredResponseGenerator:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    @observe(name=\"structured_response_generation\")\n",
    "    def generate_structured_response(self, query: str) -> str:\n",
    "        \"\"\"Generate structured JSON response\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Tr·∫£ l·ªùi c√¢u h·ªèi kh√°ch h√†ng d∆∞·ªõi d·∫°ng JSON v·ªõi format sau:\n",
    "{{\n",
    "    \"answer\": \"<c√¢u tr·∫£ l·ªùi chi ti·∫øt>\",\n",
    "    \"confidence\": <m·ª©c ƒë·ªô t·ª± tin t·ª´ 0-1>,\n",
    "    \"category\": \"<lo·∫°i c√¢u h·ªèi: return_policy|payment|shipping|warranty|support>\",\n",
    "    \"requires_human\": <true/false>,\n",
    "    \"next_steps\": [\"<c√°c b∆∞·ªõc ti·∫øp theo>\"]\n",
    "}}\n",
    "\n",
    "C√¢u h·ªèi: {query}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        langfuse_context.update_current_observation(\n",
    "            input=query,\n",
    "            output=response.content,\n",
    "            metadata={\"format\": \"json\", \"structured\": True}\n",
    "        )\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "# Enhanced JSON evaluator v·ªõi schema validation\n",
    "def enhanced_json_evaluator(output: str) -> Dict[str, Any]:\n",
    "    \"\"\"ƒê√°nh gi√° JSON output v·ªõi schema validation\"\"\"\n",
    "    try:\n",
    "        data = json.loads(output)\n",
    "        \n",
    "        # Required fields\n",
    "        required_fields = [\"answer\", \"confidence\", \"category\", \"requires_human\", \"next_steps\"]\n",
    "        missing_fields = [field for field in required_fields if field not in data]\n",
    "        \n",
    "        if missing_fields:\n",
    "            return {\n",
    "                \"score\": 0.5,\n",
    "                \"reason\": f\"Missing required fields: {missing_fields}\",\n",
    "                \"passed\": False,\n",
    "                \"valid_json\": True,\n",
    "                \"schema_valid\": False\n",
    "            }\n",
    "        \n",
    "        # Type validation\n",
    "        type_errors = []\n",
    "        if not isinstance(data.get(\"confidence\"), (int, float)) or not (0 <= data[\"confidence\"] <= 1):\n",
    "            type_errors.append(\"confidence must be number 0-1\")\n",
    "        if not isinstance(data.get(\"requires_human\"), bool):\n",
    "            type_errors.append(\"requires_human must be boolean\")\n",
    "        if not isinstance(data.get(\"next_steps\"), list):\n",
    "            type_errors.append(\"next_steps must be array\")\n",
    "        \n",
    "        if type_errors:\n",
    "            return {\n",
    "                \"score\": 0.7,\n",
    "                \"reason\": f\"Type errors: {type_errors}\",\n",
    "                \"passed\": False,\n",
    "                \"valid_json\": True,\n",
    "                \"schema_valid\": False\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"score\": 1.0,\n",
    "            \"reason\": \"Valid JSON with correct schema\",\n",
    "            \"passed\": True,\n",
    "            \"valid_json\": True,\n",
    "            \"schema_valid\": True\n",
    "        }\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"reason\": f\"Invalid JSON: {str(e)}\",\n",
    "            \"passed\": False,\n",
    "            \"valid_json\": False,\n",
    "            \"schema_valid\": False\n",
    "        }\n",
    "\n",
    "# Test structured response v·ªõi evaluation\n",
    "structured_gen = StructuredResponseGenerator(llm)\n",
    "\n",
    "test_query = \"T√¥i mu·ªën ƒë·ªïi tr·∫£ s·∫£n ph·∫©m ƒë√£ mua ƒë∆∞·ª£c 2 tu·∫ßn, c√≥ ƒë∆∞·ª£c kh√¥ng?\"\n",
    "json_response = structured_gen.generate_structured_response(test_query)\n",
    "\n",
    "print(\"üîç JSON Response:\")\n",
    "print(json_response)\n",
    "\n",
    "# Evaluate JSON response\n",
    "json_eval_result = enhanced_json_evaluator(json_response)\n",
    "print(f\"\\nüìä JSON Evaluation: {json_eval_result['score']}/1.0 {'‚úÖ' if json_eval_result['passed'] else '‚ùå'}\")\n",
    "print(f\"Reason: {json_eval_result['reason']}\")\n",
    "\n",
    "# Log evaluation to LangFuse\n",
    "langfuse.score(\n",
    "    trace_id=langfuse_context.get_current_trace_id(),\n",
    "    name=\"json_schema_validation\",\n",
    "    value=json_eval_result[\"score\"],\n",
    "    comment=json_eval_result[\"reason\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ghi JSON evaluation score v√†o LangFuse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gi·∫£i th√≠ch & Ph√¢n t√≠ch\n",
    "\n",
    "### LangFuse bi·∫øn d·ªØ li·ªáu tracing th√†nh insights h√†nh ƒë·ªông:\n",
    "\n",
    "**üîç Performance Monitoring:**\n",
    "- Theo d√µi latency, token usage, cost cho t·ª´ng component\n",
    "- Ph√°t hi·ªán bottlenecks trong pipeline (retrieval vs generation)\n",
    "- So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c versions/models\n",
    "\n",
    "**üìä Quality Assurance:**\n",
    "- Evaluation scores theo th·ªùi gian\n",
    "- Ph√°t hi·ªán regression trong ch·∫•t l∆∞·ª£ng output\n",
    "- A/B testing k·∫øt qu·∫£ v·ªõi statistical significance\n",
    "\n",
    "**üë• Team Collaboration:**\n",
    "- Developers: Debug failed cases, optimize prompts\n",
    "- Product: Hi·ªÉu user behavior, feature usage\n",
    "- Operations: Monitor system health, cost control\n",
    "\n",
    "### Key Metrics c·∫ßn theo d√µi:\n",
    "\n",
    "1. **Performance Metrics:**\n",
    "   - Response time (P50, P95, P99)\n",
    "   - Token usage v√† cost\n",
    "   - Error rate\n",
    "\n",
    "2. **Quality Metrics:**\n",
    "   - Evaluation scores (helpfulness, accuracy)\n",
    "   - User feedback ratings\n",
    "   - Success rate c·ªßa structured outputs\n",
    "\n",
    "3. **Business Metrics:**\n",
    "   - User engagement\n",
    "   - Query resolution rate\n",
    "   - Cost per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Fetch v√† ph√¢n t√≠ch data t·ª´ LangFuse\n",
    "def analyze_langfuse_data():\n",
    "    \"\"\"V√≠ d·ª• v·ªÅ c√°ch ph√¢n t√≠ch data t·ª´ LangFuse\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # L·∫•y recent traces (trong th·ª±c t·∫ø s·∫Ω c√≥ nhi·ªÅu data h∆°n)\n",
    "        traces = langfuse.get_traces(limit=10)  # API call\n",
    "        \n",
    "        print(\"üìà LANGFUSE DATA ANALYSIS:\")\n",
    "        print(f\"Total traces analyzed: {len(traces)}\")\n",
    "        \n",
    "        # Ph√¢n t√≠ch performance\n",
    "        total_latency = 0\n",
    "        total_cost = 0\n",
    "        version_counts = {}\n",
    "        \n",
    "        for trace in traces:\n",
    "            # T√≠nh metrics (simplified)\n",
    "            if hasattr(trace, 'latency') and trace.latency:\n",
    "                total_latency += trace.latency\n",
    "            \n",
    "            if hasattr(trace, 'metadata') and trace.metadata:\n",
    "                version = trace.metadata.get('version', 'unknown')\n",
    "                version_counts[version] = version_counts.get(version, 0) + 1\n",
    "        \n",
    "        if len(traces) > 0:\n",
    "            avg_latency = total_latency / len(traces) if total_latency > 0 else 0\n",
    "            print(f\"Average latency: {avg_latency:.2f}ms\")\n",
    "            print(f\"Version distribution: {version_counts}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "        print(\"1. Monitor P95 latency ƒë·ªÉ ƒë·∫£m b·∫£o user experience\")\n",
    "        print(\"2. Setup alerts cho evaluation scores < 7/10\")\n",
    "        print(\"3. Implement gradual rollout cho prompt versions m·ªõi\")\n",
    "        print(\"4. Track cost per query ƒë·ªÉ optimize budget\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Data analysis simulation: {e}\")\n",
    "        print(\"üí° Trong th·ª±c t·∫ø, b·∫°n s·∫Ω c√≥ dashboard v·ªõi real-time metrics\")\n",
    "\n",
    "analyze_langfuse_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T√†i li·ªáu tham kh·∫£o\n",
    "\n",
    "### LangFuse Official Documentation:\n",
    "\n",
    "- **[Prompt Management](https://langfuse.com/docs/prompts)**: Qu·∫£n l√Ω v√† versioning prompts\n",
    "- **[Evaluations](https://langfuse.com/docs/scores/model-based-evals)**: LLM-based evaluations v√† custom metrics\n",
    "- **[Tracing](https://langfuse.com/docs/tracing)**: Detailed tracing cho complex workflows\n",
    "- **[API Reference](https://langfuse.com/docs/integrations/api)**: Complete API documentation\n",
    "- **[LangChain Integration](https://langfuse.com/docs/integrations/langchain)**: LangChain callback handler\n",
    "- **[Dashboard & Analytics](https://langfuse.com/docs/analytics)**: Metrics v√† insights\n",
    "\n",
    "### Best Practices:\n",
    "- **[Production Deployment](https://langfuse.com/docs/deployment/self-host)**: Self-hosting guide\n",
    "- **[Security](https://langfuse.com/docs/data-security-privacy)**: Data security v√† privacy\n",
    "- **[Cost Management](https://langfuse.com/docs/model-usage-and-cost)**: Token usage tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K·∫øt lu·∫≠n & B∆∞·ªõc ti·∫øp theo\n",
    "\n",
    "### üéØ Nh·ªØng g√¨ ƒë√£ h·ªçc:\n",
    "\n",
    "1. **Production Pipeline Management**: Tracing end-to-end cho RAG systems ph·ª©c t·∫°p\n",
    "2. **Prompt Versioning**: A/B testing v√† deployment kh√¥ng c·∫ßn thay ƒë·ªïi code\n",
    "3. **Automated Evaluation**: LLM-as-a-judge, schema validation, safety checks\n",
    "4. **Performance Monitoring**: Latency, cost, quality metrics tracking\n",
    "5. **Data-Driven Optimization**: Insights ƒë·ªÉ c·∫£i thi·ªán system performance\n",
    "\n",
    "### üöÄ B∆∞·ªõc tri·ªÉn khai th·ª±c t·∫ø:\n",
    "\n",
    "**Phase 1 - Setup Foundation:**\n",
    "```bash\n",
    "# 1. Deploy LangFuse instance\n",
    "docker-compose up langfuse\n",
    "\n",
    "# 2. Configure environment\n",
    "export LANGFUSE_PUBLIC_KEY=\"pk-lf-xxx\"\n",
    "export LANGFUSE_SECRET_KEY=\"sk-lf-xxx\"\n",
    "export LANGFUSE_HOST=\"https://your-langfuse.com\"\n",
    "\n",
    "# 3. Instrument existing code\n",
    "pip install langfuse\n",
    "```\n",
    "\n",
    "**Phase 2 - Gradual Integration:**\n",
    "- B·∫Øt ƒë·∫ßu v·ªõi tracing basic LLM calls\n",
    "- Th√™m metadata v√† tags cho filtering\n",
    "- Setup evaluation cho critical use cases\n",
    "- Migrate prompts v√†o LangFuse Prompt Management\n",
    "\n",
    "**Phase 3 - Advanced Features:**\n",
    "- Custom evaluation functions\n",
    "- Automated A/B testing workflows\n",
    "- Cost optimization based on metrics\n",
    "- Integration v·ªõi CI/CD pipeline\n",
    "\n",
    "### üìä Success Metrics:\n",
    "- Response time c·∫£i thi·ªán 20%\n",
    "- Evaluation scores > 8/10\n",
    "- Cost per query gi·∫£m 15%\n",
    "- Zero production incidents t·ª´ prompt changes\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c m·ª´ng!** B·∫°n ƒë√£ n·∫Øm v·ªØng c√°ch s·ª≠ d·ª•ng LangFuse cho production LLM applications. H√£y b·∫Øt ƒë·∫ßu v·ªõi m·ªôt use case ƒë∆°n gi·∫£n v√† m·ªü r·ªông d·∫ßn theo nhu c·∫ßu c·ªßa team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}