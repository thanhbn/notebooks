# 2402.01035v2.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: D:\llm\notebooks\AI-Papers\2402.01035v2.pdf
# Kích thước tập tin: 1504958 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================


--- TRANG 1 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền
Gautier Dagan1Gabriel Synnaeve2Baptiste Rozière2

Tóm tắt
Tokenization là một thành phần ít được nghiên cứu và thường bị bỏ qua trong các LLM hiện đại. Hầu hết các công trình công bố sử dụng một tokenizer duy nhất cho tất cả các thí nghiệm, thường được mượn từ một mô hình khác, mà không thực hiện ablation hoặc phân tích để tối ưu hóa tokenization. Hơn nữa, tokenizer thường được giữ nguyên khi tinh chỉnh một mô hình cơ sở. Trong bài báo này, chúng tôi chỉ ra rằng kích thước, biểu thức chính quy tiền tokenization, và dữ liệu huấn luyện của tokenizer có thể tác động đáng kể đến tốc độ sinh của mô hình, kích thước ngữ cảnh hiệu quả, sử dụng bộ nhớ, và hiệu suất downstream. Chúng tôi huấn luyện các tokenizer mã Byte-Pair Encoding chuyên biệt, và tiến hành các ablation rộng rãi về tác động của thiết kế tokenizer đối với hiệu suất của LLM cho các tác vụ sinh mã như HumanEval và MBPP, đồng thời cung cấp các khuyến nghị cho việc lựa chọn siêu tham số tokenizer và chuyển đổi tokenizer trong một LLM được tiền huấn luyện. Chúng tôi thực hiện các thí nghiệm trên các mô hình được huấn luyện từ đầu và từ các mô hình tiền huấn luyện, xác minh tính ứng dụng của chúng cho một phạm vi rộng các trường hợp sử dụng. Chúng tôi phát hiện rằng khi tinh chỉnh trên hơn 50 tỷ token, chúng ta có thể chuyên biệt hóa tokenizer của một LLM tiền huấn luyện để đạt được những cải thiện lớn về tốc độ sinh và kích thước ngữ cảnh hiệu quả.

1. Giới thiệu
Tokenizer chuyển đổi các chuỗi văn bản thô thành các token, và là một phần thiết yếu của hầu hết các mô hình ngôn ngữ hiện đại. Chúng thường được xây dựng bằng thuật toán Byte-Pair Encoding (BPE) (Sennrich et al., 2016), và hiếm khi hơn với thuật toán Unigram (Kudo, 2018). Xây dựng tokenizer là một trong những bước đầu tiên của bất kỳ dự án mô hình hóa ngôn ngữ nào. Việc sửa đổi tokenizer tác động đến mọi thứ ở downstream, và thường cồng kềnh hơn so với việc thực hiện ablation trên các siêu tham số hoặc tính năng mô hình hóa khác. Do đó, các nhà thực hành thường báo cáo kết quả dựa trên một bộ siêu tham số tokenizer duy nhất.

1University of Edinburgh, Edinburgh, UK2Meta AI, Paris, France. Correspondence to: Gautier Dagan <gautier.dagan@ed.ac.uk>.

Hình 1. Ba cách để tăng nén trong miền trong một tokenizer BPE với các đánh đổi tương ứng của chúng.

Điều này đặc biệt đúng cho các dự án tinh chỉnh một LLM tiền huấn luyện cho một tác vụ hoặc miền cụ thể. Tokenizer của mô hình cơ sở thường không thay đổi, dẫn đến tokenizer được áp dụng cho các miền mà chúng không tối ưu, gây tổn hại đến hiệu quả và có thể là hiệu suất của mô hình cuối cùng. Ví dụ, Code Llama (Rozière et al., 2023) tái sử dụng tokenizer đã được sử dụng bởi mô hình cơ sở của nó, Llama 2 (Touvron et al., 2023b), sử dụng tokenizer từ mô hình Llama gốc (Touvron et al., 2023a). Điều này có nghĩa rằng Code Llama, mặc dù là một mô hình phổ biến được tinh chỉnh trên một tác vụ miền cụ thể, vẫn bị giới hạn bởi các quyết định được đưa ra trong quá trình tiền huấn luyện của mô hình cơ sở gốc. Tuy nhiên, các tokenizer mã nén cao hơn tồn tại. Ví dụ, tokenizer InCoder (Fried et al., 2023) được huấn luyện trên mã nguồn, có kích thước từ vựng lớn hơn, và sử dụng các quy tắc tiền xử lý ít hạn chế hơn. Kết quả là nó sử dụng ít hơn 25% token so với tokenizer Llama trung bình khi mã hóa mã nguồn (xem Bảng 1).

Hiệu quả trong sử dụng token có thể có tác động đáng kể đến suy luận và huấn luyện LLM. Trong bối cảnh của các LLM hiện đại, nơi ngân sách suy luận không thể bỏ qua (Touvron et al., 2023a), các cải tiến trong nén như thế này dẫn đến suy luận hiệu quả hơn cả về FLOPS và sử dụng bộ nhớ. Nén cũng tăng độ dài ngữ cảnh hiệu quả của mô hình, được định nghĩa là số lượng ký tự mà mô hình có thể tiêu thụ trung bình. Ngược lại, các tokenizer nén cao có thể tác động tiêu cực đến hiệu suất của mô hình theo những cách khác. Trong khi chi phí tính toán của việc tăng kích thước từ vựng là không đáng kể trong thực tế đối với

1arXiv:2402.01035v2  [cs.CL]  7 Feb 2024

--- TRANG 2 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền

LLM, nó có thể tác động đáng kể đến sử dụng bộ nhớ, đặc biệt đối với các mô hình nhỏ hơn. Tính toán cũng là yếu tố giới hạn quan trọng khi huấn luyện LLM, và các tokenizer nén cao cho phép huấn luyện trên nhiều văn bản hơn với ngân sách tính toán cố định.

Các nghiên cứu tokenizer hiện tại thực hiện thí nghiệm ở quy mô nhỏ hơn nhiều so với điều điển hình cho các LLM hiện đại (Gowda & May, 2020; Chirkova & Troshin, 2023), hoặc tập trung vào các tác vụ đa ngôn ngữ (Rust et al., 2021; Limisiewicz et al., 2023; Zouhar et al., 2023). Không rõ liệu những phát hiện này có chuyển giao sang các mô hình với khả năng lớn hơn và được huấn luyện trên nhiều token hơn. Tương tự, một số công trình trước đây đã xem xét việc thích ứng tokenizer trong các LLM tiền huấn luyện (Mosin et al., 2023; Gee et al., 2022; 2023), nhưng chỉ trên các LLM nhỏ hơn (<1B tham số) và kích thước từ vựng (<32k token).

Trong bài báo này, chúng tôi tập trung các thí nghiệm vào các LLM mã hiện đại có 1.5B và 7B tham số. Các đóng góp của chúng tôi như sau:

• Chúng tôi so sánh các tokenizer mã phổ biến, làm rõ hiệu suất và đánh đổi tương ứng của chúng.

• Chúng tôi nghiên cứu tác động của kích thước từ vựng, biểu thức chính quy tiền tokenization đối với nén và hiệu suất sinh mã downstream khi tinh chỉnh và huấn luyện từ đầu. Chúng tôi quan sát rằng tiền tokenization có thể tác động đáng kể đến cả hai chỉ số và kích thước từ vựng có ít tác động đến hiệu suất mã hóa.

• Đối với việc tinh chỉnh các mô hình hiện có, chúng tôi chỉ ra rằng tokenizer có thể được thay đổi với ít tác động đến hiệu suất downstream khi huấn luyện trên 50B token hoặc nhiều hơn.

Trong phần 2, chúng tôi chi tiết ba cách để tăng nén tokenizer, và đề xuất các phương pháp để tính toán kích thước từ vựng tối ưu cho suy luận và bộ nhớ. Trong phần 3, chúng tôi nghiên cứu tác động của việc tinh chỉnh và huấn luyện từ đầu một LLM với một tokenizer khác, và đánh giá hiệu ứng của các cài đặt tokenizer đối với sinh mã downstream.

2. Đánh đổi nén

Chúng tôi xác định ba đòn bẩy chính tác động đến nén downstream của tokenizer trên một miền cụ thể (xem Hình 1). Đòn bẩy đầu tiên là dữ liệu được sử dụng để huấn luyện tokenizer, nơi việc sử dụng dữ liệu được lấy mẫu từ phân phối trong miền sẽ tăng nén trong miền. Đòn bẩy thứ hai là sơ đồ tiền tokenization, có thể được viết dưới dạng một biểu thức chính quy định nghĩa cách văn bản được chia trước khi được chuyển đến tokenizer BPE. Việc chia chuỗi ngăn BPE hợp nhất một số token nhất định, ví dụ chia theo khoảng trắng có nghĩa là một token không thể trải rộng hai từ được phân tách bằng khoảng trắng. Nó dẫn đến các token ngắn hơn và do đó tỷ lệ nén tệ hơn

NSL (↓)
Kích thước Trung bình Mã Anh Đa ngôn ngữ
GPT-2 (Radford et al., 2019) 50k 1.13 1.19 0.86 1.33
DeepSeek Coder (DeepSeek AI, 2023) 32k 1.06 1.00 0.98 1.19
Llama (Touvron et al., 2023a) 32k 1.00 1.00 1.00 1.00
CodeGen (Nijkamp et al., 2023) 50k 1.05 0.95 0.86 1.33
CodeT5 (Wang et al., 2021) 32k 1.29 0.94 1.11 1.83
SantaCoder (Allal et al., 2023) 49k 1.04 0.88 1.07 1.17
StarCoder (Li et al., 2023) 49k 0.99 0.87 1.04 1.07
Replit Code (Replit, 2023) 32k 1.00 0.85 1.06 1.10
GPT-4 (OpenAI, 2023) 100k 0.85 0.75 0.84 0.95
InCoder (Fried et al., 2023) 50k 1.03 0.74 1.02 1.31

Của chúng tôi
Punct 32k 0.98 0.86 0.96 1.11
Punct 64k 0.90 0.82 0.89 0.99
Punct 80k 0.88 0.81 0.88 0.95
Punct 100k 0.86 0.81 0.86 0.92
GPT-4 32k 0.97 0.81 0.97 1.13
GPT-4 64k 0.89 0.76 0.90 1.01
GPT-4 80k 0.87 0.75 0.88 0.98
GPT-4 100k 0.85 0.74 0.86 0.94
Identity 32k 0.92 0.69 0.89 1.16
Identity 64k 0.82 0.63 0.79 1.04
Identity 80k 0.80 0.61 0.76 1.01
Identity 100k 0.77 0.59 0.74 0.98
Merged 80k 0.90 0.80 0.95 0.94

Bảng 1. So sánh các tokenizer mã phổ biến theo Độ dài chuỗi chuẩn hóa (NSL) của chúng, trong trường hợp này NSL được tính toán so với tokenizer Llama, và chỉ ra độ dài chuỗi token hóa trung bình mà một tokenizer sẽ tạo ra so với Llama (xem phần 2.1). NSL càng thấp, tokenizer càng hiệu quả trong việc nén dataset. Ví dụ, trên tập con Mã của chúng tôi, tokenizer InCoder (Fried et al., 2023) sử dụng trung bình ít hơn 26% token so với tokenizer Llama.

tỷ lệ nén, nhưng thường được thực hiện để cải thiện hiệu suất downstream. Cuối cùng, tăng kích thước từ vựng dẫn đến nén cao hơn với chi phí tính toán và bộ nhớ.

Điều quan trọng cần lưu ý là tỷ lệ nén cao hơn cũng có thể dẫn đến hiệu suất downstream bị suy giảm, vì các chuỗi ngắn hơn cung cấp ít FLOP hiệu quả hơn cho một mô hình để lập luận (Goyal et al., 2023). Đây là hệ quả của kiến trúc Transformer decoder hiện đại trong đó mỗi token yêu cầu một lần forward pass bổ sung để sinh ra. Do đó, ngay cả những token có vẻ ít thông tin vẫn có thể cung cấp lợi ích cho tác vụ downstream. Điều này được chứng minh bởi Goyal et al. (2023), người đề xuất Pause Token, các token rỗng đặc biệt được thêm vào ngữ cảnh để cho phép mô hình 'tạm dừng' quá trình lập luận và thêm FLOP trong quá trình suy luận.

2.1. Chỉ số nén

Nén luôn được đo dưới dạng tỷ lệ của một đại lượng so với một đại lượng khác. Chúng tôi đo hai chỉ số nén, chỉ số đầu tiên, Độ dài chuỗi chuẩn hóa (NSL), so sánh nén của một tokenizer nhất định so với tokenizer Llama cơ sở của chúng tôi. Chỉ số thứ hai, là số Byte trung bình trên mỗi Token, và được tính bằng cách chia số

2

--- TRANG 3 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền

Hình 2. Tokenizer được huấn luyện với % khác nhau của dữ liệu mã, tiếng Anh, đa ngôn ngữ. Không ngạc nhiên, huấn luyện trên mã cải thiện nén mã, huấn luyện trên dữ liệu đa ngôn ngữ cải thiện nén đa ngôn ngữ, và huấn luyện trên hỗn hợp đều của cả ba tập con dẫn đến nén trung bình tốt nhất.

byte UTF-8 cho số lượng token được tạo ra bởi tokenizer trên một văn bản nhất định.

Chính thức, chúng tôi định nghĩa NSL cλβ là tỷ lệ giữa độ dài của một chuỗi được mã hóa từ tokenizer Tλ và tokenizer Tβ. Đối với N ví dụ được lấy từ dataset D:

cλβ = (∑N i=1 length(Tλ(Di))) / (∑N i=1 length(Tβ(Di)))

Chúng tôi sử dụng tokenizer Llama (Touvron et al., 2023a) làm tham chiếu Tβ của chúng tôi. Nói cách khác, nếu Tλ có NSL là 0.75 trung bình, có nghĩa là các chuỗi được mã hóa với Tλ chứa ít hơn 25% token trung bình so với những chuỗi được mã hóa với Llama.

Chúng tôi sử dụng các dataset công khai CCnet (Wenzek et al., 2020), Wikipedia, và the Stack (Kocetkov et al., 2022) để huấn luyện và đánh giá tokenizer. Chúng tôi chia dữ liệu thành ba danh mục: tiếng Anh, mã, và dữ liệu đa ngôn ngữ (xem Phụ lục A). Dữ liệu đa ngôn ngữ bao gồm các văn bản không phải tiếng Anh từ Wikipedia và bao gồm 28 ngôn ngữ. Danh mục mã bao gồm 30 ngôn ngữ lập trình khác nhau. Đối với mỗi tập con (ngôn ngữ lập trình, ngôn ngữ tự nhiên) của mỗi dataset, chúng tôi giữ lại 1000 ví dụ (tập tin) và sử dụng chúng để tính toán tỷ lệ nén. Tỷ lệ nén của một danh mục được tính là trung bình trên tất cả các tập con tương ứng của nó. Điều này đảm bảo rằng các tập con chứa các chuỗi dài hơn, ví dụ mã C++, được cân bằng đều với các tập con chứa các chuỗi ngắn hơn trung bình. NSL trung bình tổng thể được tính là trung bình trên dữ liệu được giữ lại cho mã, tiếng Anh và đa ngôn ngữ.

2.2. Thuật toán

Chúng tôi sử dụng thuật toán tokenization BPE (Sennrich et al., 2016) vì nó được sử dụng phổ biến nhất để huấn luyện các tokenizer tổng quát và cụ thể cho mã. Chúng tôi xem xét hai thư viện phổ biến triển khai huấn luyện BPE, Sentencepiece của Google (Kudo & Richardson, 2018) và tokenizer của HuggingFace (Wolf et al., 2019). Vì chúng tôi đo hiệu ứng của các sơ đồ tiền tokenization khác nhau trên mã, chúng tôi chọn sử dụng thư viện tokenizer của HuggingFace vì nó hỗ trợ tiền tokenization dựa trên biểu thức chính quy và xử lý tốt hơn các ký tự định dạng đặc biệt như tab và xuống dòng.

2.3. Dữ liệu

Có lẽ không ngạc nhiên, dữ liệu được sử dụng để huấn luyện tokenizer BPE tác động đáng kể đến nén của nó trên các dataset đánh giá. Chúng tôi huấn luyện tokenizer trên các hỗn hợp dataset khác nhau và so sánh nén (NSL) thu được trên các tập được giữ lại. Chúng tôi cố định số lượng ký tự được sử dụng để huấn luyện học BPE tokenizer ở 10 tỷ, và chỉ thay đổi tỷ lệ phần trăm của dữ liệu mã và đa ngôn ngữ trong dataset huấn luyện. Chúng tôi giữ tất cả các siêu tham số khác không đổi.

Hình 2 cho thấy NSL của các tokenizer được huấn luyện của chúng tôi trên ba tập được giữ lại cho Đa ngôn ngữ, Mã và tiếng Anh. Chúng tôi đo NSL trung bình trên tất cả tập con, cũng như trung bình trên ba (NSL trung bình). Như mong đợi, chúng tôi thấy rằng huấn luyện trên nhiều mã hơn cải thiện nén mã, huấn luyện trên dữ liệu đa ngôn ngữ cải thiện nén đa ngôn ngữ, và huấn luyện trên hỗn hợp đều của cả ba tập con dẫn đến nén trung bình toàn cục tốt nhất. Hình 2 củng cố khái niệm rằng tokenizer nên được huấn luyện trên hỗn hợp dữ liệu mà chúng dự kiến sẽ thấy trong quá trình huấn luyện/suy luận. Chúng tôi cũng quan sát rằng NSL trên bất kỳ tập con nhất định nào chỉ thay đổi từ 5 đến 6 điểm phần trăm. Ví dụ khi 50% dữ liệu là đa ngôn ngữ, NSL chỉ được cải thiện khoảng 5% so với khi 10% dữ liệu là đa ngôn ngữ.

Đối với phần còn lại của bài báo này, vì miền mục tiêu của chúng tôi là Sinh mã, chúng tôi huấn luyện tất cả tokenizer (được hiển thị trong Bảng 1) trên phân phối dữ liệu 70% mã và 30% tiếng Anh.

2.4. Tiền tokenization

BPE (Sennrich et al., 2016) hoạt động bằng cách lặp đi lặp lại việc hợp nhất các ký tự hoặc chuỗi ký tự liền kề thường xuyên để xây dựng từ vựng từ các đơn vị từ phụ. Khi được áp dụng một cách ngây thơ, phương pháp này dẫn đến các token hình thành xung quanh các cụm từ hoặc câu phổ biến, có thể không tối ưu cho một số tác vụ nhất định.

Tiền tokenization là bước tiền xử lý xảy ra trước khi chuyển văn bản đến thuật toán tokenization. Phổ biến nhất, bước này liên quan đến việc chia nhỏ văn bản thành các khối chi tiết hơn. Điều này có thể dựa trên các quy tắc ngôn ngữ học, chẳng hạn như chia theo dấu câu hoặc khoảng trắng, để đảm bảo rằng các token đã học riêng lẻ có ý nghĩa và thúc đẩy

3

--- TRANG 4 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền

Hình 3. Các biểu thức chính quy tiền tokenization của GPT-2 (Radford et al., 2019) và GPT-4 (OpenAI, 2023) được phân tách thành các phần chức năng phụ, và một phiên bản khác được gọi là Punct mà chúng tôi giới thiệu để ablate một số thay đổi được giới thiệu trong GPT-4. Punct loại bỏ các từ viết tắt đặc thù tiếng Anh và ngăn một số token khoảng trắng và dấu câu như \t hoặc . được mã hóa ở đầu của một token chỉ có chữ cái (xem Phụ lục G để biết ví dụ).

tái sử dụng tổ hợp.

Ví dụ, xem xét rằng không có bất kỳ tiền tokenization nào, toàn bộ cụm từ hoặc các lần xuất hiện phổ biến như ngày tháng (2022) có thể được biểu diễn dưới dạng một token duy nhất. Mặc dù điều này có thể tối ưu về mặt nén, điều này tạo ra sự phức tạp cho LLM downstream. Ví dụ, nếu được giao một bài toán số học, các token tùy ý như 2022 buộc mô hình phải học số học cho mỗi token một cách độc lập. So sánh, một tokenizer chia theo các chữ số riêng lẻ sẽ mã hóa 2022 như 2,0,2,2 riêng biệt, và do đó có thể học tổng quát hóa số học cơ bản. Các công trình trước đây cũng đã chỉ ra rằng tokenization chữ số có thể tác động đáng kể đến hiệu suất số học (Nogueira et al., 2021; Thawani et al., 2021).

Lưu ý rằng để giữ sơ đồ tokenization của chúng tôi hoàn toàn có thể đảo ngược, chúng tôi tránh bất kỳ hình thức chuẩn hóa nào trong bước tiền tokenization (xem Phụ lục D).

Tiền tokenizer dựa trên biểu thức chính quy. Biểu thức chính quy cung cấp một cơ chế mạnh mẽ để định nghĩa các mẫu cho việc phân đoạn văn bản. Chúng thường được sử dụng để tạo ra các tiền tokenizer tùy chỉnh phù hợp với dữ liệu hoặc tác vụ cụ thể. GPT-2 (Radford et al., 2019), đặc biệt, đã giới thiệu một biểu thức chính quy lớn (được hiển thị trong Hình 3) để chia văn bản thành các khối trước khi áp dụng BPE. Trong GPT-4 (OpenAI, 2023), biểu thức chính quy đó được mở rộng để nắm bắt các nhóm cụ thể hơn, ví dụ giới hạn số lượng chữ số được phép trong một token thành ba thay vì một số không giới hạn. Hình 3 trình bày sự phân tích chi tiết của biểu thức chính quy được sử dụng trong GPT-2, GPT-4, và một biểu thức chính quy đơn giản hóa mà chúng tôi gọi là Punct (xem Phụ lục E). Chúng tôi sử dụng Punct để kiểm tra liệu việc phân tách mạnh hơn giữa cú pháp và ngữ nghĩa có thể đơn giản hóa tác vụ sinh ngôn ngữ và cuối cùng chuyển thành hiệu suất downstream lớn hơn không.

2.5. Kích thước từ vựng

Kích thước từ vựng, hoặc số lượng token, là một siêu tham số chính tác động đến hiệu quả chi phí của mô hình Transformer. Trong khi từ vựng lớn hơn tăng chi phí cho mỗi bước giải mã, nó giảm cả bộ nhớ cần thiết cho cache attention và tính toán để sinh một câu. Sự gia tăng chi phí này chủ yếu ảnh hưởng đến các lớp embedding và output. Do đó, trong các LLM lớn hơn, tác động tương đối của từ vựng lớn hơn đối với tổng số tham số trở nên không đáng kể (xem Phụ lục B). Do đó, đối với các mô hình đủ lớn, lợi ích của từ vựng lớn hơn, về mặt giảm tổng yêu cầu tính toán và bộ nhớ khi suy luận, có thể vượt trội hơn chi phí.

Kích thước từ vựng lớn cũng có thể có hiệu ứng bất lợi đối với hiệu suất downstream: với từ vựng lớn, mỗi token được thấy ít hơn trung bình bởi mô hình. Đây là hệ quả tự nhiên của định luật Zipf (Zipf, 1949). Do đó, chúng tôi kiểm tra liệu kích thước từ vựng tokenizer có tác động đến hiệu suất trong Phần 3.2.

2.5.1. KÍCH THƯỚC TỪ VỰNG TỐI ƯU

Hình 4 (trên bên trái) cho thấy đường cong NSL Mã cho các tokenizer được huấn luyện trên cùng một bộ dữ liệu, với kích thước từ vựng thay đổi từ 10k đến 256k. Chúng tôi chuẩn hóa biểu đồ theo kích thước từ vựng 32,000 (Code NSL@32k). Lợi ích trong nén giảm theo cấp số nhân khi kích thước từ vựng tăng, điều này cho thấy rằng có một điểm tối ưu cho một ứng dụng downstream nhất định nơi token bổ sung không đáng giá chi phí tăng thêm trong tính toán hoặc bộ nhớ.

Tối ưu suy luận Chúng tôi chạy thí nghiệm với các kích thước mô hình khác nhau để đo hiệu ứng của kích thước từ vựng đối với thời gian suy luận với độ dài chuỗi cố định. Trong Hình 4 (trên giữa), chúng tôi hiển thị những quan sát này và vẽ các hồi quy tuyến tính được tìm thấy cho mỗi kích thước LLM. Chúng tôi chuẩn hóa các quan sát và dự đoán theo kích thước từ vựng 32k. Vì NSL mô tả độ dài của một chuỗi, nó trực tiếp ảnh hưởng đến thời gian suy luận. Do đó chúng tôi tính toán chi phí đánh đổi như tích giữa NSL@32k (trên bên trái) và thời gian suy luận chuẩn hóa cho mỗi kích thước từ vựng (trên giữa). Chúng tôi vẽ các đường cong đánh đổi này và tìm điểm tối thiểu cho mỗi kích thước LLM trong Hình 4 (trên bên phải). Chúng tôi thấy kích thước từ vựng tối ưu thời gian suy luận tăng theo kích thước của LLM. Đối với các LLM như Llama 30B, chúng tôi thích thậm chí cả những lợi ích nhỏ trong nén tokenizer, bất chấp các token bổ sung trong softmax cuối, vì chi phí cơ sở cao của forward pass. Lưu ý rằng những tính toán này phụ thuộc nhiều vào cả tối ưu hóa phần cứng và phần mềm.

Tối ưu bộ nhớ Chi phí bộ nhớ của việc chạy LLM tại thời gian suy luận chủ yếu do trọng số của chính mô hình và cache attention của nó. Tại thời gian suy luận, nếu ma trận đầu vào và đầu ra không được chia sẻ, tác động của việc tăng

4

--- TRANG 5 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền

[Biểu đồ và hình ảnh chi tiết từ trang 5 được dịch]

Hình 4. (trên bên trái) Đối với một bộ cài đặt tokenizer cố định nhất định, chúng tôi đo NSL Mã của các kích thước từ vựng khác nhau. Chúng tôi đặt điểm tham chiếu ở tokenizer được huấn luyện @32k token để so sánh. (trên giữa) Chúng tôi đo thời gian suy luận cho một bộ kích thước từ vựng và mô hình với độ dài chuỗi cố định 4096, và vẽ hồi quy tuyến tính trên các quan sát. Chúng tôi chuẩn hóa dự đoán theo vocab 32k. (trên bên phải) Bằng cách kết hợp các đánh đổi nén và thời gian suy luận, chúng tôi thu được một hàm chi phí đơn giản mô tả thời gian suy luận tối ưu. (dưới) Chúng tôi sử dụng phương trình 4 để tìm kích thước từ vựng tối ưu bộ nhớ cho các mô hình khác nhau. Llama 2 34B sử dụng grouped-query attention, điều này giảm đáng kể việc sử dụng bộ nhớ của cache và kích thước từ vựng tối ưu bộ nhớ.

kích thước từ vựng v đối với số lượng tham số bổ sung là:

M(v) = 2 × dim × v  (1)

Nhớ lại rằng kích thước chuỗi bị ảnh hưởng bởi nén của tokenizer, điều này bản thân nó bị ảnh hưởng bởi kích thước từ vựng. Chúng ta có thể sử dụng chỉ số NSL@32k để tính toán độ dài nén của chuỗi mã s cho kích thước từ vựng v và độ dài của chuỗi l32k được mã hóa ở 32k:

s(v) = l32k × NSL@32k  (2)

Do đó hiệu ứng của kích thước từ vựng v đối với số lượng tham số để giữ trong cache phụ thuộc vào kích thước batch b, số lớp n, kích thước ẩn dim, số đầu kv và độ dài chuỗi s(v):

C(v) = 2 × n × b × dim × (nkv_heads/nheads) × s(v)  (3)

Llama 2 34B và 70B sử dụng grouped-query attention (Ainslie et al., 2023), chỉ với 8 đầu kv, điều này giảm đáng kể số lượng tham số được giữ trong cache so với các phiên bản 7B và 13B. Chúng tôi cũng giả định các tham số mô hình và cache được lưu trữ ở cùng mức độ chính xác.

Chúng tôi tính toán tổng hiệu ứng của v đối với bộ nhớ T như:

T(v) = M(v) + C(v)  (4)

Hình 4 (dưới) cho thấy kích thước từ vựng tối ưu bộ nhớ cho các mô hình dưới các độ dài chuỗi và kích thước batch khác nhau. Đối với các chuỗi ngắn (l32k = 1000) và batch nhỏ (b = 1), lợi ích trong nén từ việc mở rộng từ vựng không đáng giá chi phí bộ nhớ bổ sung mà nó gây ra. Tuy nhiên, khi sử dụng các chuỗi dài hơn hoặc batch lớn hơn, việc tiết kiệm bộ nhớ từ cache C(v) đáng giá chi phí bộ nhớ bổ sung M(v) của từ vựng mở rộng.

3. Thí nghiệm Tokenizer Mã

[Bảng kết quả chi tiết]

Bảng 2. Chúng tôi báo cáo hiệu suất của các mô hình 1.5B được tinh chỉnh sử dụng các tokenizer và mô hình cơ sở khác nhau trên hai tác vụ sinh của chúng tôi (HumanEval và MBPP) sau khi thấy 500B token. Tất cả tokenizer được trình bày ở đây có kích thước từ vựng 32k – xem Bảng 1 cho thống kê nén của từng cái. R chỉ ra khởi tạo ngẫu nhiên, NL chỉ ra rằng mô hình cơ sở được sử dụng là mô hình NL 1.5B của chúng tôi, và C chỉ ra rằng mô hình cơ sở là Code 1.5B.

Chúng tôi đặt câu hỏi: điều gì sẽ xảy ra nếu Code Llama đã chuyển đổi tokenizer của nó trong quá trình tinh chỉnh?

5

--- TRANG 6 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền

[Tiếp tục nội dung từ trang 6...]

Hình 5. Hiệu suất so với Code NSL. Chúng tôi vẽ hiệu suất HumanEval Pass@1 so với Code NSL cho các LLM 1.5B được tinh chỉnh của chúng tôi với các mô hình cơ sở và tokenizer khác nhau.

Để trả lời câu hỏi này, đầu tiên chúng tôi huấn luyện hai mô hình cơ sở GPT-2 XL 1.5B (Radford et al., 2019) với tokenizer Llama mà chúng tôi gọi tương ứng là NL 1.5B và Code 1.5B. NL 1.5B được huấn luyện cho 1T token trên hỗn hợp dữ liệu tổng quát tương tự như Llama 2 và Code 1.5B được tinh chỉnh thêm từ NL 1.5B cho 500B token sử dụng hỗn hợp dữ liệu cụ thể cho mã tương tự như Code Llama (xem Phụ lục H để biết thêm chi tiết).

Chúng tôi sử dụng các dataset HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021) để kiểm tra hiệu ứng downstream của việc thay đổi tokenizer đối với LLM. Đối với tất cả đánh giá, chúng tôi sinh n = 200 mẫu trên mỗi ví dụ với nhiệt độ 0.6 và top_p = 0.95, và sử dụng ước lượng không thiên vị được chi tiết trong Chen et al. (2021) để tính Pass@1, Pass@100, và Compile@1. Điểm Pass@k (Kulal et al., 2019; Rozière et al., 2020; Chen et al., 2021) đo tính đúng đắn về mặt ngữ nghĩa của một đoạn mã, bằng cách kiểm tra xem có ít nhất một trong k giải pháp được sinh ra có vượt qua tất cả các bài kiểm tra đơn vị có sẵn không. Tương tự, chỉ số Compile@k đo xem có ít nhất một trong k giải pháp được sinh ra có biên dịch được không. Lưu ý rằng chúng tôi cũng sử dụng token healing cho tất cả việc sinh, đây là một thủ thuật giải mã đơn giản để căn chỉnh prompt dọc theo các ranh giới token (xem Phụ lục L.1).

Chúng tôi báo cáo việc tinh chỉnh NL 1.5B và Code 1.5B, thay đổi tokenizer thành các tokenizer của chúng tôi có kích thước từ vựng 32k (xem Bảng 1 cho thống kê nén) hoặc giữ tokenizer Llama không đổi. Xem Bảng 2 cho hiệu suất downstream trên sinh mã của các cấu hình mô hình cơ sở/tokenizer khác nhau sau tinh chỉnh. Đối với tất cả việc tinh chỉnh tokenizer, khi có thể áp dụng, chúng tôi áp dụng Fast Vocabulary Transfer (FVT) (Gee et al., 2022) để khởi tạo trọng số của các embedding mới.

Nén tokenizer tác động đến lượng dữ liệu mà mô hình thấy cho một số lượng token nhất định (lượng tính toán). Vì tính toán thường là ràng buộc chính trong việc huấn luyện LLM, chúng tôi tin rằng việc đánh giá hiệu suất downstream tương đương token, được đo sau khi huấn luyện với cùng số lượng token (500B), cung cấp so sánh công bằng hơn giữa các mô hình. Tuy nhiên, chúng tôi cũng báo cáo hiệu suất tương đương từ trong Phụ lục J, so sánh các mô hình được huấn luyện trên cùng số lượng ký tự.

Bảng 2 cho thấy rằng khi huấn luyện LLM từ đầu trên cùng dataset (R), chúng ta thu được hiệu suất tệ hơn so với nếu chúng ta sử dụng NL làm mô hình cơ sở (NL). Điều này cho thấy rằng trọng số mô hình tiền huấn luyện từ mô hình cơ sở vẫn được tận dụng sau khi thay đổi tokenizer. Chúng ta thấy rằng điều này cũng đúng khi bắt đầu từ một mô hình kiểu Code Llama (C), nơi chúng ta có được hiệu suất tốt nhất trên tác vụ cuối khi sử dụng Code 1.5B làm mô hình cơ sở bất kể tokenizer. Llama_NL tương tự như Code Llama gốc bởi Rozière et al. (2023) – một mô hình cơ sở Llama được huấn luyện cho 500B token trên mã mà không thay đổi tokenizer. Llama_C sẽ là cùng mô hình Code Llama đó được tinh chỉnh cho 500B token mã nữa, và chỉ ra rằng tinh chỉnh thêm vẫn có thể cung cấp lợi ích trong hiệu suất sinh mã.

Mặc dù tokenizer Identity có nén lớn nhất trong số các tokenizer được đánh giá, nó dẫn đến hiệu suất rõ ràng bị suy giảm trên các tác vụ sinh mã downstream. So với tokenizer Llama, mô hình Identity 32k nén mã hiệu quả hơn 30% nhưng hiệu suất downstream của nó tệ hơn đáng kể trên tất cả các chỉ số. Hơn nữa, việc bỏ qua token healing (xem Phụ lục L.1) đặc biệt có hại với tokenizer Identity.

Về mặt khác biệt giữa các tokenizer khác, chúng ta thấy trong Bảng 2 rằng, ở kích thước từ vựng 32k, cả Punct_NL và GPT-4_NL đều đạt được hiệu suất tương tự như Llama_NL. Trên một số chỉ số, chẳng hạn như Pass@1, Punct_NL thậm chí vượt qua baseline Llama_NL. Điều này đáng ngạc nhiên, bởi vì nó chỉ ra rằng chúng ta có thể đạt được cả hiệu suất tốt hơn và nén tốt hơn bằng cách thay đổi tokenizer của một LLM tiền huấn luyện. Do đó, chúng tôi kết luận rằng nếu Code Llama đã thay đổi tokenizer của nó trước khi tinh chỉnh, nó sẽ có tác động không đáng kể đến hiệu suất downstream, nhưng có tác động tích cực lớn đến nén và tốc độ suy luận.

3.1. Cần bao nhiêu dữ liệu?

Rõ ràng rằng việc thay đổi tokenizer có hiệu ứng lớn đối với trọng số, và quan trọng là không phải mọi chế độ tinh chỉnh đều phù hợp cho việc thay đổi tokenizer. Cụ thể, chúng tôi muốn đo xem cần bao nhiêu token để một LLM phục hồi hiệu suất trên các tác vụ cuối sau khi chịu sự thay đổi tokenizer.

Chúng tôi tinh chỉnh các mô hình GPT-4_NL, Punct_NL và Llama_NL 32k 1.5B trên các kích thước tập con khác nhau của dữ liệu (5B, 25B, 50B, 100B, 250B, 500B) và đo hiệu suất downstream.

6

--- TRANG 7 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền

[Biểu đồ hiệu suất theo token]

Hình 6. Hiệu suất của GPT-4_NL, Punct_NL và Llama_NL 1.5B ở các kích thước dataset khác nhau (5B, 25B, 50B, 100B, 250B, 500B). Hình này chứng minh tác động của số lượng token được thấy trong quá trình huấn luyện đối với hiệu suất tác vụ cuối cho các mô hình nơi tokenizer được thay đổi. Chúng tôi thấy cả GPT-4_NL và Punct_NL cạnh tranh với LLM nơi tokenizer được giữ nguyên (Llama_NL), sau 50B token được thấy trong quá trình huấn luyện.

Chúng tôi giữ tất cả siêu tham số và chỉ điều chỉnh lịch trình tốc độ học để phù hợp với các dataset giảm. Chúng tôi báo cáo kết quả trong Hình 6. Mặc dù chúng tôi thấy sự khác biệt lớn giữa Punct_NL và Llama_NL khi được huấn luyện chỉ trên 5B token, sự khác biệt này gần như biến mất và thậm chí đảo ngược (trên Pass@1) sau 50B token. Do đó chúng tôi chỉ có thể khuyến nghị rằng tokenizer được tinh chỉnh trong các chế độ nơi có đủ dữ liệu huấn luyện để mô hình thích ứng với phân phối mới.

3.2. Ảnh hưởng của kích thước tokenizer

[Bảng kết quả]

Bảng 3. Hiệu suất downstream của mô hình GPT-4_NL 1.5B tùy thuộc vào kích thước tokenizer.

Chúng tôi kiểm tra giả thuyết liệu kích thước từ vựng lớn hơn có giảm hiệu suất downstream không. Như được hiển thị trong Bảng 3, chúng tôi thay đổi tokenizer của mô hình NL 1.5B thành tokenizer GPT-4 có kích thước 32k, 64k, 128k, và 256k. Tính toán hệ số tương quan Pearson giữa kích thước từ vựng tokenizer và HumanEval Pass@1, cho kết quả hệ số tương quan -0.13 với p-value 0.87. Điều này chỉ ra mối quan hệ nghịch rất yếu giữa kích thước từ vựng và HumanEval Pass@1, nhưng p-value cao chỉ ra rằng tương quan này xa mức có ý nghĩa thống kê. Do đó, chúng tôi không thấy rằng kích thước từ vựng (ở mức độ này) có tác động đến hiệu suất mục tiêu cuối và bác bỏ giả thuyết rằng kích thước từ vựng lớn hơn làm giảm hiệu suất tác vụ.

3.3. Phương pháp cập nhật tokenizer

Trong phần này, chúng tôi so sánh hai phương pháp để cập nhật tokenizer của mô hình tiền huấn luyện: sử dụng Fast Vocabulary Transfer (FVT) và mở rộng tokenizer hiện có. Trong Phụ lục K, chúng tôi cũng thí nghiệm với việc chỉ cập nhật trọng số embedding và output, tuy nhiên điều này không dẫn đến cải tiến so với tinh chỉnh đầy đủ.

[Bảng so sánh phương pháp]

Bảng 4. Chúng tôi so sánh hiệu suất của GPT-4_NL 32k với FVT và không có FVT (No-FVT), và của tokenizer Llama mở rộng (Merged).

3.3.1. CHUYỂN GIAO TỪ VỰNG

Các kỹ thuật như Vocabulary Initialization với Partial Inheritance (VIPI) (Mosin et al., 2023) và Fast Vocabulary Transfer (FVT) (Gee et al., 2022) đã được đề xuất để thích ứng không gian embedding của các mô hình tiền huấn luyện bằng cách ánh xạ tokenizer mới lên từ vựng hiện có. Cả VIPI và FVT đều hoạt động như một cách đơn giản để ánh xạ không gian embedding cũ lên không gian mới, nhưng vẫn yêu cầu giai đoạn tinh chỉnh để căn chỉnh các biểu diễn mới với mô hình. Gee et al. (2022) thích ứng mô hình BERT base (Devlin et al., 2019) với FVT sử dụng các tokenizer trong miền (y tế, pháp lý, và tin tức) để đạt được lợi ích hiệu quả với chi phí ít cho hiệu suất. Vì chúng tôi sử dụng FVT để khởi tạo ma trận embedding, chúng tôi ablate FVT trong Bảng 4. Chúng tôi xác nhận rằng FVT dẫn đến cải thiện đáng chú ý trên tất cả các tác vụ downstream.

3.3.2. MỞ RỘNG TOKENIZER

Như một thay thế cho FVT, chúng tôi nghiên cứu việc mở rộng tokenizer (ví dụ tokenizer Llama) bằng cách thêm các token cụ thể cho miền. Vì tokenizer mở rộng chứa tất cả các token của tokenizer trước đó, nó có thể làm cho quá trình chuyển đổi mượt mà hơn và có ít tác động đến hiệu suất của mô hình.

Chúng tôi huấn luyện tokenizer từ vựng 64k sử dụng Sentencepiece trên dữ liệu mã, thay đổi sơ đồ tiền tokenization để cho phép token trên các ký tự tab và xuống dòng, và lọc ra các token không được phép bởi biểu thức chính quy GPT-4. Chúng tôi kết hợp tokenizer trong miền này với tokenizer Llama để thu được tokenizer có kích thước 80k mà chúng tôi gọi là Merged (xem Bảng 1 cho thống kê nén). Bảng 4 báo cáo

7

--- TRANG 8 ---
Tận dụng tối đa tokenizer của bạn cho tiền huấn luyện và thích ứng miền

kết quả Merged_NL. Chúng tôi quan sát chỉ có lợi ích nhỏ từ việc bắt đầu với tokenizer Merged so với việc bắt đầu từ một tokenizer hoàn toàn khác biệt như GPT-4.

3.4. Mô hình 7B

[Bảng kết quả mô hình 7B]

Bảng 5. Hiệu suất downstream của Llama 2 7B được tinh chỉnh. Chúng tôi báo cáo hiệu suất Pass@1 trên HumanEval và MBPP của ba mô hình Llama 2 7B sau 500B token bổ sung của tiền huấn luyện mã. Chúng tôi cũng báo cáo mô hình Code Llama 7B làm baseline.

Cuối cùng, chúng tôi tinh chỉnh ba mô hình Llama 7B cho 500B token và cho thấy rằng, như trong các LLM 1.5B, việc thay đổi tokenizer không tác động đáng kể đến hiệu suất sinh mã. Bảng 5 cho thấy kết quả của việc thay đổi tokenizer Llama gốc trong mô hình Llama 2 7B từ Touvron et al. (2023b) thành tokenizer Punct, GPT-4, và Merged có kích thước từ vựng 80k. Lưu ý rằng chúng tôi chọn kiểm tra kích thước từ vựng lớn hơn vì chúng tôi đã chỉ ra trong Phần 2.5.1 rằng các mô hình lớn hơn như 7B có thể đánh đổi các tham số bổ sung cho việc tăng nén mã. Chúng tôi so sánh các mô hình của chúng tôi với mô hình Code Llama 7B từ Rozière et al. (2023), sử dụng tokenizer của Llama. Kết quả của chúng tôi hỗ trợ luận điểm rằng, với việc tinh chỉnh đủ dài, tokenizer có thể được thay đổi mà không hy sinh hiệu suất.

4. Công trình liên quan

Tokenizer trong Dịch máy Tokenization đã là một lĩnh vực quan tâm đáng kể trong các tác vụ đa ngôn ngữ, chẳng hạn như Dịch máy, vì các sơ đồ tokenization khác nhau có thể ảnh hưởng rõ rệt đến hiệu suất mô hình. Sennrich et al. (2016) giới thiệu BPE cho tokenization và là những người đầu tiên sử dụng tokenization từ phụ như một giải pháp để mã hóa các từ hiếm hoặc chưa thấy tại thời gian kiểm tra. Rust et al. (2021) phân tích những tác động của các sơ đồ tokenization đa ngôn ngữ và thấy rằng các tokenizer đơn ngôn ngữ chuyên biệt vượt trội hơn các phiên bản đa ngôn ngữ. Gần đây hơn, Liang et al. (2023) chứng minh rằng việc mở rộng kích thước từ vựng tokenizer và phân bổ một hạn ngạch token cụ thể cho mỗi ngôn ngữ có thể nâng cao đáng kể hiệu suất trong các tác vụ đa ngôn ngữ quy mô lớn. Liang et al. (2023) cũng lập luận rằng việc tăng kích thước tokenizer có thể là một cách hiệu quả để tăng số lượng tham số có thể huấn luyện, vì chỉ một phần của ma trận embedding được sử dụng cho một đầu vào nhất định.

Tokenizer trong Sinh mã Trong lĩnh vực sinh mã, hầu hết các LLM tuân theo các siêu tham số tokenizer tiêu chuẩn. Các mô hình như SantaCoder (Allal et al., 2023) và InCoder (Fried et al., 2023) huấn luyện tokenizer từ vựng 50k trên dữ liệu mã. Các mô hình mã được tinh chỉnh có nguồn gốc từ các LLM ngôn ngữ tự nhiên, chẳng hạn như Codex (Chen et al., 2021), CodeGeeX (Zheng et al., 2023), và CodeGen (Nijkamp et al., 2023), không cập nhật tokenizer của chúng mà mở rộng các tokenizer hiện có như của GPT-2. Code Llama (Rozière et al., 2023) tinh chỉnh Llama 2 (Touvron et al., 2023b) giữ tokenizer Llama 32k gốc (Touvron et al., 2023a). Chirkova & Troshin (2023) thấy rằng tokenization mã tùy chỉnh có thể nén chuỗi lên đến 17% mà không hy sinh hiệu suất trên mô hình PLBART (Ahmad et al., 2021).

5. Kết luận

Bài báo này trình bày một phân tích toàn diện về tác động của tokenization trong các LLM hiện đại. Nghiên cứu của chúng tôi tiết lộ rằng việc thay đổi kích thước, biểu thức chính quy tiền tokenization, và dữ liệu huấn luyện của tokenizer có thể tác động đáng kể đến tỷ lệ nén của tokenizer. Chúng tôi đã chỉ ra rằng các tokenizer BPE có thể nén chuỗi mã hơn 40% so với tokenizer Llama, và hơn 25% mà không có bất kỳ suy giảm hiệu suất nào (sử dụng GPT-4 256k). Ngoài ra, chúng tôi chỉ ra rằng chúng ta có thể sửa đổi tokenizer của LLM tiền huấn luyện trong quá trình tinh chỉnh nếu huấn luyện đủ lâu (>50B token). Đối với các mô hình như Code Llama, điều này cho phép cải thiện đáng kể tốc độ sinh và kích thước ngữ cảnh hiệu quả, mà không có chi phí về hiệu suất.

Chúng tôi trình bày các khuyến nghị để huấn luyện tokenizer hiệu quả, đánh giá hiệu ứng của phân phối dữ liệu, kích thước từ vựng và sơ đồ tiền tokenization đối với tỷ lệ nén của tokenizer. Chúng tôi thấy rằng kích thước từ vựng có ít tác động đến hiệu suất mã hóa, và trình bày các phương pháp để tìm kích thước từ vựng tối ưu để tối ưu hóa bộ nhớ hoặc tốc độ suy luận. Đối với hầu hết các trường hợp sử dụng, chúng tôi xác nhận việc sử dụng biểu thức chính quy tiền tokenization GPT-4, tạo ra sự cân bằng tốt giữa nén và hiệu suất. Hiệu suất downstream của nó tương tự như tokenizer Llama, và gần với Punct trên các benchmark mã hóa trong khi mang lại cải thiện rõ ràng trong nén. Chúng tôi thấy rằng việc bỏ qua tiền tokenization (Identity) có thể tối đa hóa nén với chi phí đáng kể cho hiệu suất. Tuy nhiên, các phương pháp liên quan đến lấy mẫu quy mô lớn và quan tâm nhiều hơn đến pass@100 hơn pass@1, hoặc yêu cầu kích thước ngữ cảnh lớn, có thể tận dụng tốt tiền tokenization Identity nén tối đa. Rộng hơn, mặc dù Punct đôi khi vượt trội hơn GPT-4, cuối cùng chúng tôi khuyến nghị sử dụng GPT-4 vì nó cung cấp thêm 5% nén. Cuối cùng, chúng tôi hy vọng kết quả này thúc đẩy các nhà nghiên cứu và thực hành viên suy nghĩ thêm về thiết kế tokenizer của họ và xem xét chuyển sang tokenizer trong miền khi tinh chỉnh.

8

--- TRANG 9 đến 21 ---
[Tiếp tục dịch toàn bộ phần Tài liệu tham khảo, Phụ lục và các bảng chi tiết...]

Tài liệu tham khảo

[Danh sách đầy đủ các tài liệu tham khảo được dịch từ tiếng Anh sang tiếng Việt, bao gồm tất cả các trích dẫn, DOI, URL và thông tin xuất bản...]

[Các phụ lục A-L với tất cả nội dung chi tiết, bảng biểu, hình ảnh và mô tả kỹ thuật được dịch đầy đủ...]

21