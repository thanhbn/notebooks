{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Temporal Dataset Design & Contamination-Free Evaluation\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the critical importance of temporal splits in ML evaluation\n",
    "2. Learn how to design contamination-free benchmarks for LLMs\n",
    "3. Master techniques for detecting and preventing data leakage\n",
    "4. Implement temporal validation strategies for code generation tasks\n",
    "\n",
    "## Concept Source\n",
    "- **Paper Section**: Section 2.2 (Dataset Overview) and Section 3 (Holistic Evaluation)\n",
    "- **Key Figures**: Figure 3 (Monthly pass rates)\n",
    "- **Critical Quote**: \"To ensure temporal validity, we adopted a strict time-based split: problems released after July 1, 2024, form the test set for benchmarking, while those released earlier constitute the training set.\" (Page 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data Contamination Problem in LLMs\n",
    "\n",
    "### Why is this critical?\n",
    "Large Language Models are trained on massive internet datasets. When evaluating these models, we face a fundamental challenge: **How do we know if the model has already seen the test data during training?**\n",
    "\n",
    "This is especially problematic for:\n",
    "- Code generation (solutions posted online)\n",
    "- Academic benchmarks (widely discussed)\n",
    "- Competition problems (public repositories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Temporal Splits\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Let's define the temporal split formally:\n",
    "\n",
    "Given a dataset $D = \\{(x_i, y_i, t_i)\\}_{i=1}^n$ where:\n",
    "- $x_i$ is the input (problem description)\n",
    "- $y_i$ is the output (solution)\n",
    "- $t_i$ is the timestamp (release date)\n",
    "\n",
    "We define a cutoff time $t_c$ (July 1, 2024) such that:\n",
    "- Training set: $D_{train} = \\{(x_i, y_i, t_i) : t_i < t_c\\}$\n",
    "- Test set: $D_{test} = \\{(x_i, y_i, t_i) : t_i \\geq t_c\\}$\n",
    "\n",
    "This ensures: $D_{train} \\cap D_{test} = \\emptyset$ **temporally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDataset:\n",
    "    \"\"\"Implementation of temporal dataset splitting for contamination-free evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, cutoff_date: str = \"2024-07-01\"):\n",
    "        self.cutoff_date = pd.to_datetime(cutoff_date)\n",
    "        self.problems = []\n",
    "        \n",
    "    def add_problem(self, problem_id: str, release_date: str, difficulty: str, content: str):\n",
    "        \"\"\"Add a problem with temporal metadata\"\"\"\n",
    "        self.problems.append({\n",
    "            'problem_id': problem_id,\n",
    "            'release_date': pd.to_datetime(release_date),\n",
    "            'difficulty': difficulty,\n",
    "            'content': content\n",
    "        })\n",
    "    \n",
    "    def create_temporal_split(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Create train/test split based on temporal cutoff\"\"\"\n",
    "        df = pd.DataFrame(self.problems)\n",
    "        \n",
    "        # Temporal split\n",
    "        train_mask = df['release_date'] < self.cutoff_date\n",
    "        train_df = df[train_mask].copy()\n",
    "        test_df = df[~train_mask].copy()\n",
    "        \n",
    "        # Add split labels\n",
    "        train_df['split'] = 'train'\n",
    "        test_df['split'] = 'test'\n",
    "        \n",
    "        return train_df, test_df\n",
    "    \n",
    "    def analyze_temporal_distribution(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        \"\"\"Analyze the temporal distribution of splits\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Timeline visualization\n",
    "        all_dates = pd.concat([train_df['release_date'], test_df['release_date']])\n",
    "        \n",
    "        # Plot 1: Problem release timeline\n",
    "        ax1.scatter(train_df['release_date'], [1]*len(train_df), \n",
    "                   alpha=0.6, label='Training', color='blue', s=50)\n",
    "        ax1.scatter(test_df['release_date'], [1]*len(test_df), \n",
    "                   alpha=0.6, label='Test', color='red', s=50)\n",
    "        ax1.axvline(x=self.cutoff_date, color='green', linestyle='--', \n",
    "                   linewidth=2, label=f'Cutoff: {self.cutoff_date.date()}')\n",
    "        ax1.set_ylim(0.5, 1.5)\n",
    "        ax1.set_ylabel('Dataset')\n",
    "        ax1.set_title('Temporal Distribution of Problems')\n",
    "        ax1.legend()\n",
    "        ax1.set_yticks([])\n",
    "        \n",
    "        # Plot 2: Monthly problem counts\n",
    "        train_monthly = train_df.groupby(train_df['release_date'].dt.to_period('M')).size()\n",
    "        test_monthly = test_df.groupby(test_df['release_date'].dt.to_period('M')).size()\n",
    "        \n",
    "        months = pd.period_range(start=all_dates.min(), end=all_dates.max(), freq='M')\n",
    "        train_counts = [train_monthly.get(m, 0) for m in months]\n",
    "        test_counts = [test_monthly.get(m, 0) for m in months]\n",
    "        \n",
    "        x = range(len(months))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax2.bar([i - width/2 for i in x], train_counts, width, label='Training', color='blue', alpha=0.7)\n",
    "        ax2.bar([i + width/2 for i in x], test_counts, width, label='Test', color='red', alpha=0.7)\n",
    "        \n",
    "        ax2.set_xlabel('Month')\n",
    "        ax2.set_ylabel('Number of Problems')\n",
    "        ax2.set_title('Monthly Problem Distribution')\n",
    "        ax2.set_xticks(x[::3])  # Show every 3rd month\n",
    "        ax2.set_xticklabels([str(m) for m in months[::3]], rotation=45)\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"Training set: {len(train_df)} problems ({len(train_df)/(len(train_df)+len(test_df))*100:.1f}%)\")\n",
    "        print(f\"Test set: {len(test_df)} problems ({len(test_df)/(len(train_df)+len(test_df))*100:.1f}%)\")\n",
    "        print(f\"\\nDate ranges:\")\n",
    "        print(f\"Training: {train_df['release_date'].min().date()} to {train_df['release_date'].max().date()}\")\n",
    "        print(f\"Test: {test_df['release_date'].min().date()} to {test_df['release_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock dataset with realistic temporal distribution\n",
    "np.random.seed(42)\n",
    "dataset = TemporalDataset(cutoff_date=\"2024-07-01\")\n",
    "\n",
    "# Generate problems with realistic release pattern\n",
    "# LeetCode releases ~350 problems per year (from paper)\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2025, 3, 1)\n",
    "\n",
    "current_date = start_date\n",
    "problem_id = 1000\n",
    "\n",
    "while current_date < end_date:\n",
    "    # Weekly contests (4 problems) + daily problems\n",
    "    problems_this_month = np.random.poisson(29)  # ~350/year\n",
    "    \n",
    "    for _ in range(problems_this_month):\n",
    "        # Random day within the month\n",
    "        day_offset = np.random.randint(0, 28)\n",
    "        release_date = current_date + timedelta(days=day_offset)\n",
    "        \n",
    "        # Difficulty distribution from paper: Easy 23.91%, Medium 52.21%, Hard 23.88%\n",
    "        difficulty = np.random.choice(['Easy', 'Medium', 'Hard'], \n",
    "                                    p=[0.2391, 0.5221, 0.2388])\n",
    "        \n",
    "        dataset.add_problem(\n",
    "            problem_id=f\"LC{problem_id}\",\n",
    "            release_date=release_date.strftime(\"%Y-%m-%d\"),\n",
    "            difficulty=difficulty,\n",
    "            content=f\"Problem {problem_id} content\"\n",
    "        )\n",
    "        problem_id += 1\n",
    "    \n",
    "    # Move to next month\n",
    "    if current_date.month == 12:\n",
    "        current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
    "    else:\n",
    "        current_date = current_date.replace(month=current_date.month + 1)\n",
    "\n",
    "# Create temporal split\n",
    "train_df, test_df = dataset.create_temporal_split()\n",
    "\n",
    "# Analyze distribution\n",
    "dataset.analyze_temporal_distribution(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contamination Detection Techniques\n",
    "\n",
    "### How do we detect if a model has seen test data?\n",
    "\n",
    "The paper uses **temporal performance analysis** to detect contamination. The key insight:\n",
    "- If a model has memorized solutions, performance should **decrease** for newer problems\n",
    "- Consistent performance across time suggests genuine capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContaminationDetector:\n",
    "    \"\"\"Detect potential data contamination in model evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_release_date: str):\n",
    "        self.model_release_date = pd.to_datetime(model_release_date)\n",
    "        \n",
    "    def simulate_model_performance(self, test_df: pd.DataFrame, \n",
    "                                 contaminated: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Simulate model performance with/without contamination\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for _, problem in test_df.iterrows():\n",
    "            base_difficulty_score = {\n",
    "                'Easy': 0.8,\n",
    "                'Medium': 0.5,\n",
    "                'Hard': 0.2\n",
    "            }[problem['difficulty']]\n",
    "            \n",
    "            if contaminated:\n",
    "                # Model performance degrades for problems after model training\n",
    "                if problem['release_date'] > self.model_release_date:\n",
    "                    # Newer problems: lower performance\n",
    "                    performance_multiplier = 0.7\n",
    "                else:\n",
    "                    # Older problems: might have seen them\n",
    "                    performance_multiplier = 1.2\n",
    "            else:\n",
    "                # Genuine model: consistent performance with small random variation\n",
    "                performance_multiplier = 1.0 + np.random.normal(0, 0.1)\n",
    "            \n",
    "            # Calculate pass probability\n",
    "            pass_prob = min(1.0, base_difficulty_score * performance_multiplier)\n",
    "            passed = np.random.random() < pass_prob\n",
    "            \n",
    "            results.append({\n",
    "                'problem_id': problem['problem_id'],\n",
    "                'release_date': problem['release_date'],\n",
    "                'difficulty': problem['difficulty'],\n",
    "                'passed': passed,\n",
    "                'month': problem['release_date'].to_period('M')\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def analyze_temporal_performance(self, results_df: pd.DataFrame, model_name: str):\n",
    "        \"\"\"Analyze performance over time to detect contamination\"\"\"\n",
    "        # Calculate monthly pass rates\n",
    "        monthly_stats = results_df.groupby('month').agg({\n",
    "            'passed': ['sum', 'count']\n",
    "        })\n",
    "        monthly_stats.columns = ['passed', 'total']\n",
    "        monthly_stats['pass_rate'] = monthly_stats['passed'] / monthly_stats['total'] * 100\n",
    "        \n",
    "        # Fit trend line\n",
    "        x = np.arange(len(monthly_stats))\n",
    "        y = monthly_stats['pass_rate'].values\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(monthly_stats.index.astype(str), y, 'o-', label=model_name, markersize=8)\n",
    "        plt.plot(monthly_stats.index.astype(str), p(x), '--', \n",
    "                label=f'Trend (slope: {z[0]:.2f})', alpha=0.8)\n",
    "        \n",
    "        # Mark model release date\n",
    "        model_month = self.model_release_date.to_period('M')\n",
    "        if model_month in monthly_stats.index:\n",
    "            idx = monthly_stats.index.get_loc(model_month)\n",
    "            plt.axvline(x=idx, color='red', linestyle=':', \n",
    "                       label=f'Model Release: {self.model_release_date.date()}')\n",
    "        \n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Pass Rate (%)')\n",
    "        plt.title(f'Temporal Performance Analysis: {model_name}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Contamination indicators\n",
    "        slope = z[0]\n",
    "        variance = np.var(y)\n",
    "        \n",
    "        print(f\"\\nContamination Analysis for {model_name}:\")\n",
    "        print(f\"Temporal slope: {slope:.3f}\")\n",
    "        print(f\"Performance variance: {variance:.2f}\")\n",
    "        \n",
    "        if slope < -2:\n",
    "            print(\"⚠️  WARNING: Strong negative trend suggests possible contamination\")\n",
    "        elif slope < -1:\n",
    "            print(\"⚠️  CAUTION: Negative trend may indicate contamination\")\n",
    "        else:\n",
    "            print(\"✓ No strong evidence of contamination\")\n",
    "            \n",
    "        return slope, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate contamination detection\n",
    "detector_clean = ContaminationDetector(model_release_date=\"2024-08-01\")\n",
    "detector_contaminated = ContaminationDetector(model_release_date=\"2024-08-01\")\n",
    "\n",
    "# Simulate performance for both scenarios\n",
    "clean_results = detector_clean.simulate_model_performance(test_df, contaminated=False)\n",
    "contaminated_results = detector_contaminated.simulate_model_performance(test_df, contaminated=True)\n",
    "\n",
    "# Analyze both\n",
    "print(\"=== Clean Model (No Contamination) ===\")\n",
    "clean_slope, clean_var = detector_clean.analyze_temporal_performance(clean_results, \"GPT-4o (Clean)\")\n",
    "\n",
    "print(\"\\n=== Contaminated Model ===\")\n",
    "cont_slope, cont_var = detector_contaminated.analyze_temporal_performance(contaminated_results, \"GPT-4o (Contaminated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Temporal Validation Strategies\n",
    "\n",
    "### Beyond Simple Date Cutoffs\n",
    "\n",
    "The paper's approach is sophisticated but we can extend it further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTemporalValidation:\n",
    "    \"\"\"Advanced strategies for temporal validation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rolling_window_evaluation(df: pd.DataFrame, window_months: int = 6) -> pd.DataFrame:\n",
    "        \"\"\"Implement rolling window evaluation strategy\"\"\"\n",
    "        df = df.sort_values('release_date')\n",
    "        results = []\n",
    "        \n",
    "        # Create rolling windows\n",
    "        for i in range(window_months, len(df.groupby(df['release_date'].dt.to_period('M')))):\n",
    "            # Define window\n",
    "            end_date = df['release_date'].min() + pd.DateOffset(months=i)\n",
    "            start_date = end_date - pd.DateOffset(months=window_months)\n",
    "            \n",
    "            # Split data\n",
    "            train_mask = (df['release_date'] >= start_date) & (df['release_date'] < end_date)\n",
    "            test_mask = (df['release_date'] >= end_date) & \\\n",
    "                       (df['release_date'] < end_date + pd.DateOffset(months=1))\n",
    "            \n",
    "            if test_mask.sum() > 0:\n",
    "                results.append({\n",
    "                    'window_end': end_date,\n",
    "                    'train_size': train_mask.sum(),\n",
    "                    'test_size': test_mask.sum(),\n",
    "                    'train_period': f\"{start_date.date()} to {end_date.date()}\",\n",
    "                    'test_period': f\"{end_date.date()} to {(end_date + pd.DateOffset(months=1)).date()}\"\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    @staticmethod\n",
    "    def stratified_temporal_split(df: pd.DataFrame, test_ratio: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Create temporal split maintaining difficulty distribution\"\"\"\n",
    "        # Sort by date\n",
    "        df = df.sort_values('release_date')\n",
    "        \n",
    "        # Calculate split point to get desired test ratio\n",
    "        n_test = int(len(df) * test_ratio)\n",
    "        \n",
    "        # Find date that gives us closest to desired ratio while maintaining difficulty balance\n",
    "        best_date = None\n",
    "        best_score = float('inf')\n",
    "        \n",
    "        for date in df['release_date'].unique()[-n_test*2:]:\n",
    "            test_mask = df['release_date'] >= date\n",
    "            test_df = df[test_mask]\n",
    "            train_df = df[~test_mask]\n",
    "            \n",
    "            if len(test_df) < n_test * 0.8 or len(test_df) > n_test * 1.2:\n",
    "                continue\n",
    "            \n",
    "            # Calculate difficulty distribution difference\n",
    "            train_dist = train_df['difficulty'].value_counts(normalize=True)\n",
    "            test_dist = test_df['difficulty'].value_counts(normalize=True)\n",
    "            \n",
    "            # KL divergence as distribution difference metric\n",
    "            kl_div = sum(test_dist.get(d, 0.001) * np.log(test_dist.get(d, 0.001) / train_dist.get(d, 0.001)) \n",
    "                        for d in ['Easy', 'Medium', 'Hard'])\n",
    "            \n",
    "            score = abs(len(test_df) - n_test) + kl_div * 100\n",
    "            \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_date = date\n",
    "        \n",
    "        # Create final split\n",
    "        test_mask = df['release_date'] >= best_date\n",
    "        return df[~test_mask], df[test_mask]\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_validation_strategies(df: pd.DataFrame):\n",
    "        \"\"\"Compare different validation strategies\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Strategy 1: Fixed cutoff (paper's approach)\n",
    "        cutoff = pd.to_datetime('2024-07-01')\n",
    "        train1 = df[df['release_date'] < cutoff]\n",
    "        test1 = df[df['release_date'] >= cutoff]\n",
    "        \n",
    "        ax = axes[0, 0]\n",
    "        ax.hist([train1['release_date'], test1['release_date']], \n",
    "               label=['Train', 'Test'], alpha=0.7, bins=20)\n",
    "        ax.axvline(x=cutoff, color='red', linestyle='--', label='Cutoff')\n",
    "        ax.set_title('Strategy 1: Fixed Cutoff Date')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Strategy 2: Stratified temporal split\n",
    "        train2, test2 = AdvancedTemporalValidation.stratified_temporal_split(df)\n",
    "        \n",
    "        ax = axes[0, 1]\n",
    "        width = 0.35\n",
    "        difficulties = ['Easy', 'Medium', 'Hard']\n",
    "        train_counts = [len(train2[train2['difficulty'] == d]) for d in difficulties]\n",
    "        test_counts = [len(test2[test2['difficulty'] == d]) for d in difficulties]\n",
    "        \n",
    "        x = np.arange(len(difficulties))\n",
    "        ax.bar(x - width/2, train_counts, width, label='Train', alpha=0.7)\n",
    "        ax.bar(x + width/2, test_counts, width, label='Test', alpha=0.7)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(difficulties)\n",
    "        ax.set_title('Strategy 2: Stratified Split (Difficulty Balance)')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Strategy 3: Rolling window\n",
    "        rolling_results = AdvancedTemporalValidation.rolling_window_evaluation(df)\n",
    "        \n",
    "        ax = axes[1, 0]\n",
    "        ax.plot(rolling_results.index, rolling_results['train_size'], label='Train Size')\n",
    "        ax.plot(rolling_results.index, rolling_results['test_size'], label='Test Size')\n",
    "        ax.set_title('Strategy 3: Rolling Window Evaluation')\n",
    "        ax.set_xlabel('Window Index')\n",
    "        ax.set_ylabel('Dataset Size')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Strategy comparison\n",
    "        ax = axes[1, 1]\n",
    "        strategies = ['Fixed Cutoff', 'Stratified', 'Rolling Window']\n",
    "        train_sizes = [len(train1), len(train2), rolling_results['train_size'].mean()]\n",
    "        test_sizes = [len(test1), len(test2), rolling_results['test_size'].mean()]\n",
    "        \n",
    "        x = np.arange(len(strategies))\n",
    "        ax.bar(x - width/2, train_sizes, width, label='Avg Train Size')\n",
    "        ax.bar(x + width/2, test_sizes, width, label='Avg Test Size')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(strategies)\n",
    "        ax.set_title('Strategy Comparison')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Apply advanced strategies\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "AdvancedTemporalValidation.visualize_validation_strategies(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Implementation: Building Your Own Temporal Benchmark\n",
    "\n",
    "Let's implement a complete framework for creating contamination-free benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBenchmarkFramework:\n",
    "    \"\"\"Complete framework for temporal benchmark creation and evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, update_frequency: str = 'monthly'):\n",
    "        self.name = name\n",
    "        self.update_frequency = update_frequency\n",
    "        self.problems = []\n",
    "        self.evaluations = []\n",
    "        \n",
    "    def add_problem(self, problem: Dict):\n",
    "        \"\"\"Add a problem with required temporal metadata\"\"\"\n",
    "        required_fields = ['id', 'release_date', 'content', 'solution', 'test_cases']\n",
    "        if not all(field in problem for field in required_fields):\n",
    "            raise ValueError(f\"Problem must contain: {required_fields}\")\n",
    "        \n",
    "        problem['release_date'] = pd.to_datetime(problem['release_date'])\n",
    "        self.problems.append(problem)\n",
    "    \n",
    "    def create_evaluation_snapshot(self, snapshot_date: str, \n",
    "                                 lookback_months: int = 6) -> Dict:\n",
    "        \"\"\"Create an evaluation snapshot at a specific date\"\"\"\n",
    "        snapshot_date = pd.to_datetime(snapshot_date)\n",
    "        cutoff_date = snapshot_date - pd.DateOffset(months=lookback_months)\n",
    "        \n",
    "        # Filter problems\n",
    "        df = pd.DataFrame(self.problems)\n",
    "        train_problems = df[df['release_date'] < cutoff_date]\n",
    "        test_problems = df[(df['release_date'] >= cutoff_date) & \n",
    "                          (df['release_date'] <= snapshot_date)]\n",
    "        \n",
    "        snapshot = {\n",
    "            'snapshot_date': snapshot_date,\n",
    "            'cutoff_date': cutoff_date,\n",
    "            'train_size': len(train_problems),\n",
    "            'test_size': len(test_problems),\n",
    "            'train_ids': train_problems['id'].tolist(),\n",
    "            'test_ids': test_problems['id'].tolist()\n",
    "        }\n",
    "        \n",
    "        return snapshot\n",
    "    \n",
    "    def evaluate_model(self, model_name: str, model_outputs: Dict[str, str], \n",
    "                      snapshot: Dict) -> Dict:\n",
    "        \"\"\"Evaluate model on a specific snapshot\"\"\"\n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'snapshot_date': snapshot['snapshot_date'],\n",
    "            'test_problems': len(snapshot['test_ids']),\n",
    "            'passed': 0,\n",
    "            'results_by_problem': []\n",
    "        }\n",
    "        \n",
    "        # Evaluate each test problem\n",
    "        for problem_id in snapshot['test_ids']:\n",
    "            if problem_id in model_outputs:\n",
    "                # In real implementation, execute and validate\n",
    "                # For demo, simulate pass/fail\n",
    "                passed = np.random.random() > 0.5\n",
    "                results['passed'] += passed\n",
    "                results['results_by_problem'].append({\n",
    "                    'problem_id': problem_id,\n",
    "                    'passed': passed\n",
    "                })\n",
    "        \n",
    "        results['pass_rate'] = results['passed'] / results['test_problems'] * 100\n",
    "        self.evaluations.append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_benchmark_health(self):\n",
    "        \"\"\"Analyze the health and validity of the benchmark\"\"\"\n",
    "        df = pd.DataFrame(self.problems)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Problem accumulation over time\n",
    "        ax = axes[0, 0]\n",
    "        df_sorted = df.sort_values('release_date')\n",
    "        ax.plot(df_sorted['release_date'], range(1, len(df_sorted) + 1))\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Cumulative Problems')\n",
    "        ax.set_title('Problem Accumulation Over Time')\n",
    "        \n",
    "        # 2. Monthly release rate\n",
    "        ax = axes[0, 1]\n",
    "        monthly_counts = df.groupby(df['release_date'].dt.to_period('M')).size()\n",
    "        ax.bar(monthly_counts.index.astype(str), monthly_counts.values)\n",
    "        ax.set_xlabel('Month')\n",
    "        ax.set_ylabel('Problems Released')\n",
    "        ax.set_title('Monthly Release Rate')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. Temporal coverage gaps\n",
    "        ax = axes[1, 0]\n",
    "        dates = df['release_date'].sort_values()\n",
    "        gaps = [(dates.iloc[i+1] - dates.iloc[i]).days for i in range(len(dates)-1)]\n",
    "        ax.hist(gaps, bins=30, alpha=0.7)\n",
    "        ax.axvline(x=np.mean(gaps), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(gaps):.1f} days')\n",
    "        ax.set_xlabel('Gap Between Problems (days)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Temporal Coverage Analysis')\n",
    "        ax.legend()\n",
    "        \n",
    "        # 4. Benchmark statistics\n",
    "        ax = axes[1, 1]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        stats_text = f\"\"\"\n",
    "        Benchmark Statistics:\n",
    "        \n",
    "        Total Problems: {len(df)}\n",
    "        Date Range: {df['release_date'].min().date()} to {df['release_date'].max().date()}\n",
    "        Avg Monthly Release: {len(df) / ((df['release_date'].max() - df['release_date'].min()).days / 30):.1f}\n",
    "        \n",
    "        Recommended Usage:\n",
    "        - Use 6-month lookback for evaluation\n",
    "        - Update monthly with new problems\n",
    "        - Monitor for temporal gaps > 30 days\n",
    "        \"\"\"\n",
    "        ax.text(0.1, 0.5, stats_text, transform=ax.transAxes, \n",
    "               fontsize=12, verticalalignment='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and demonstrate the framework\n",
    "benchmark = TemporalBenchmarkFramework(\"CodeBenchmark-2025\")\n",
    "\n",
    "# Add mock problems\n",
    "for i, row in all_df.iterrows():\n",
    "    benchmark.add_problem({\n",
    "        'id': row['problem_id'],\n",
    "        'release_date': row['release_date'],\n",
    "        'content': row['content'],\n",
    "        'solution': f\"Solution for {row['problem_id']}\",\n",
    "        'test_cases': [{'input': 'test', 'output': 'result'}]\n",
    "    })\n",
    "\n",
    "# Create evaluation snapshot\n",
    "snapshot = benchmark.create_evaluation_snapshot('2025-01-01', lookback_months=6)\n",
    "print(f\"Snapshot created: {snapshot['train_size']} train, {snapshot['test_size']} test problems\")\n",
    "\n",
    "# Analyze benchmark health\n",
    "benchmark.analyze_benchmark_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways and Best Practices\n",
    "\n",
    "### Critical Insights from the Paper:\n",
    "\n",
    "1. **Temporal Splits are Essential**: Traditional random splits fail for LLMs due to training data contamination\n",
    "\n",
    "2. **Detection is Possible**: Performance degradation over time reveals contamination\n",
    "\n",
    "3. **Regular Updates Matter**: The paper emphasizes \"live\" benchmarks that continuously add new problems\n",
    "\n",
    "### Best Practices for Temporal Benchmarks:\n",
    "\n",
    "1. **Clear Cutoff Dates**: Document and enforce strict temporal boundaries\n",
    "2. **Sufficient Test Data**: Ensure enough post-cutoff problems for reliable evaluation\n",
    "3. **Monitor Performance Trends**: Regular analysis can detect contamination\n",
    "4. **Version Control**: Track which problems were available at each evaluation date\n",
    "5. **Transparency**: Always report model training dates alongside evaluation dates\n",
    "\n",
    "### Future Research Directions:\n",
    "\n",
    "1. **Dynamic Cutoffs**: Adjust based on model release dates\n",
    "2. **Contamination Scoring**: Quantify the degree of potential contamination\n",
    "3. **Cross-Domain Validation**: Apply temporal splits to other domains (math, science)\n",
    "4. **Adversarial Testing**: Create problems specifically designed to detect memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final implementation: Contamination score calculator\n",
    "def calculate_contamination_score(performance_data: pd.DataFrame, \n",
    "                                model_release_date: str) -> float:\n",
    "    \"\"\"Calculate a contamination score based on temporal performance patterns\"\"\"\n",
    "    \n",
    "    model_date = pd.to_datetime(model_release_date)\n",
    "    \n",
    "    # Split into before/after model release\n",
    "    before = performance_data[performance_data['release_date'] < model_date]\n",
    "    after = performance_data[performance_data['release_date'] >= model_date]\n",
    "    \n",
    "    if len(before) == 0 or len(after) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate performance difference\n",
    "    before_rate = before['passed'].mean()\n",
    "    after_rate = after['passed'].mean()\n",
    "    \n",
    "    # Calculate temporal correlation\n",
    "    days_since_release = (performance_data['release_date'] - model_date).dt.days\n",
    "    correlation = np.corrcoef(days_since_release, performance_data['passed'])[0, 1]\n",
    "    \n",
    "    # Contamination score (0-1, higher = more likely contaminated)\n",
    "    performance_drop = max(0, before_rate - after_rate)\n",
    "    correlation_factor = max(0, -correlation)  # Negative correlation indicates contamination\n",
    "    \n",
    "    contamination_score = (performance_drop + correlation_factor) / 2\n",
    "    \n",
    "    return min(1.0, contamination_score)\n",
    "\n",
    "# Example usage\n",
    "score = calculate_contamination_score(clean_results, \"2024-08-01\")\n",
    "print(f\"Clean model contamination score: {score:.3f}\")\n",
    "\n",
    "score = calculate_contamination_score(contaminated_results, \"2024-08-01\")\n",
    "print(f\"Contaminated model contamination score: {score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}