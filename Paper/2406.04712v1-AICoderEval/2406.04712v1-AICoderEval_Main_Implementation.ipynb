{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AICoderEval: Improving AI Domain Code Generation - Main Implementation\n",
    "\n",
    "## 1. Giới thiệu Paper\n",
    "\n",
    "**Tên paper**: AICoderEval: Improving AI Domain Code Generation of Large Language Models\n",
    "\n",
    "**Tác giả**: Yinghui Xia (AutoAgents.ai), Yuyan Chen (Fudan University), Tianyu Shi (University of Toronto), Jun Wang (East China Normal University), Jinsong Yang (AutoAgents.ai)\n",
    "\n",
    "**Link**: https://arxiv.org/abs/2406.04712v1\n",
    "\n",
    "**Tóm tắt**: Paper giới thiệu AICoderEval - một benchmark dataset tập trung vào các task code generation trong lĩnh vực AI, sử dụng các thư viện phổ biến như HuggingFace, PyTorch, và TensorFlow. Paper cũng đề xuất CoderGen - một agent-based framework để cải thiện khả năng sinh code của LLMs cho các task cụ thể, và huấn luyện AICoder - một model mạnh hơn được fine-tune từ Llama-3.\n",
    "\n",
    "### Đóng góp chính:\n",
    "- **Benchmark Construction**: Xây dựng AICoderEval dataset với 492 task về AI\n",
    "- **Framework Design**: Thiết kế CoderGen framework để sinh training data chất lượng cao\n",
    "- **Model Evaluation**: Đánh giá nhiều LLMs và chứng minh hiệu quả của phương pháp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cài đặt môi trường và thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết\n",
    "!pip install -q langchain langchain-openai langchain-community\n",
    "!pip install -q langgraph\n",
    "!pip install -q deepeval\n",
    "!pip install -q transformers torch\n",
    "!pip install -q huggingface-hub\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import BaseMessage\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint import MemorySaver\n",
    "\n",
    "# DeepEval imports for evaluation\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load và khám phá AICoderEval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load AICoderEval dataset từ HuggingFace\n",
    "print(\"Loading AICoderEval dataset...\")\n",
    "dataset = load_dataset(\"vixuowis/AICoderEval\", split=\"train\")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "\n",
    "# Xem một số ví dụ\n",
    "print(\"\\nSample data:\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str) and len(value) > 200:\n",
    "        print(f\"{key}: {value[:200]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phân tích phân bố các category trong dataset\n",
    "categories = {\n",
    "    \"Natural Language Processing\": 383,\n",
    "    \"Computer Vision\": 50,\n",
    "    \"Tabular Data\": 18,\n",
    "    \"Audio and Speech\": 17,\n",
    "    \"Classification\": 12,\n",
    "    \"Multimodal\": 9,\n",
    "    \"Reinforcement Learning\": 3\n",
    "}\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart\n",
    "ax1.bar(categories.keys(), categories.values())\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Distribution of Tasks by Category')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%')\n",
    "ax2.set_title('Percentage Distribution of Tasks')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement CoderGen Framework với LangChain và LangGraph\n",
    "\n",
    "### Lý do sử dụng LangChain/LangGraph:\n",
    "- **LangChain**: Cung cấp abstraction tốt cho việc tương tác với LLMs, quản lý prompts, và xử lý output\n",
    "- **LangGraph**: Phù hợp để xây dựng agent-based framework với các state transitions như trong CoderGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import operator\n",
    "\n",
    "# Define state cho CoderGen agent\n",
    "class CoderGenState(TypedDict):\n",
    "    \"\"\"State definition for CoderGen agent\"\"\"\n",
    "    task_description: str\n",
    "    generated_code: str\n",
    "    error_traceback: Optional[str]\n",
    "    iteration_count: int\n",
    "    test_results: Dict[str, Any]\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    is_completed: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoderGenFramework:\n",
    "    \"\"\"CoderGen Framework implementation using LangChain and LangGraph\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo-1106\"):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0.6)\n",
    "        self.max_iterations = 5\n",
    "        self._setup_prompts()\n",
    "        self._build_graph()\n",
    "    \n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Setup prompts for different stages of code generation\"\"\"\n",
    "        \n",
    "        # Initial code generation prompt\n",
    "        self.code_gen_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"You are an expert AI developer specializing in HuggingFace, PyTorch, and TensorFlow.\n",
    "                Generate Python code that implements the given task using appropriate AI libraries.\n",
    "                Follow these guidelines:\n",
    "                1. Use proper imports and package installations\n",
    "                2. Include comprehensive error handling\n",
    "                3. Add test functions with 3 test cases (normal, edge case, correctness)\n",
    "                4. Follow Google Python Style Guide for documentation\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Task: {task_description}\n",
    "                \n",
    "                Generate complete Python code with:\n",
    "                - All necessary imports\n",
    "                - Main function implementation\n",
    "                - Test functions with 3 test cases\n",
    "                - Proper documentation\"\"\"\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Error analysis and fix prompt\n",
    "        self.error_fix_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"You are debugging Python code. Analyze the error and provide a fixed version.\n",
    "                Focus on:\n",
    "                1. Understanding the error traceback\n",
    "                2. Identifying the root cause\n",
    "                3. Providing a corrected implementation\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Original code:\n",
    "                {generated_code}\n",
    "                \n",
    "                Error traceback:\n",
    "                {error_traceback}\n",
    "                \n",
    "                Please fix the code and ensure it runs without errors.\"\"\"\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build the LangGraph workflow for CoderGen\"\"\"\n",
    "        workflow = StateGraph(CoderGenState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"generate_code\", self._generate_code)\n",
    "        workflow.add_node(\"execute_tests\", self._execute_tests)\n",
    "        workflow.add_node(\"analyze_errors\", self._analyze_errors)\n",
    "        workflow.add_node(\"regenerate_code\", self._regenerate_code)\n",
    "        \n",
    "        # Define edges\n",
    "        workflow.set_entry_point(\"generate_code\")\n",
    "        workflow.add_edge(\"generate_code\", \"execute_tests\")\n",
    "        \n",
    "        # Conditional edges based on test results\n",
    "        workflow.add_conditional_edges(\n",
    "            \"execute_tests\",\n",
    "            self._check_test_results,\n",
    "            {\n",
    "                \"success\": END,\n",
    "                \"failure\": \"analyze_errors\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"analyze_errors\", \"regenerate_code\")\n",
    "        workflow.add_edge(\"regenerate_code\", \"execute_tests\")\n",
    "        \n",
    "        # Compile the graph\n",
    "        memory = MemorySaver()\n",
    "        self.app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    def _generate_code(self, state: CoderGenState) -> CoderGenState:\n",
    "        \"\"\"Generate initial code based on task description\"\"\"\n",
    "        print(f\"\\n[Iteration {state['iteration_count']}] Generating code...\")\n",
    "        \n",
    "        messages = self.code_gen_prompt.format_messages(\n",
    "            task_description=state[\"task_description\"]\n",
    "        )\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            response = self.llm(messages)\n",
    "            print(f\"Tokens used: {cb.total_tokens}\")\n",
    "        \n",
    "        state[\"generated_code\"] = response.content\n",
    "        state[\"messages\"].append(response)\n",
    "        return state\n",
    "    \n",
    "    def _execute_tests(self, state: CoderGenState) -> CoderGenState:\n",
    "        \"\"\"Execute the generated code and run tests\"\"\"\n",
    "        print(\"\\nExecuting tests...\")\n",
    "        \n",
    "        # Mock execution for demonstration\n",
    "        # In real implementation, this would execute code in sandboxed environment\n",
    "        import random\n",
    "        \n",
    "        if state[\"iteration_count\"] < 2 and random.random() < 0.7:\n",
    "            # Simulate error\n",
    "            state[\"error_traceback\"] = \"\"\"Traceback (most recent call last):\n",
    "  File \"test.py\", line 15, in <module>\n",
    "    model = pipeline('text-classification')\n",
    "NameError: name 'pipeline' is not defined\"\"\"\n",
    "            state[\"test_results\"] = {\n",
    "                \"passed\": 0,\n",
    "                \"failed\": 3,\n",
    "                \"errors\": [state[\"error_traceback\"]]\n",
    "            }\n",
    "        else:\n",
    "            # Simulate success\n",
    "            state[\"error_traceback\"] = None\n",
    "            state[\"test_results\"] = {\n",
    "                \"passed\": 3,\n",
    "                \"failed\": 0,\n",
    "                \"errors\": []\n",
    "            }\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _check_test_results(self, state: CoderGenState) -> str:\n",
    "        \"\"\"Check test results and decide next action\"\"\"\n",
    "        if state[\"test_results\"][\"failed\"] == 0:\n",
    "            state[\"is_completed\"] = True\n",
    "            print(\"\\n✅ All tests passed!\")\n",
    "            return \"success\"\n",
    "        elif state[\"iteration_count\"] >= self.max_iterations:\n",
    "            print(\"\\n❌ Max iterations reached\")\n",
    "            return \"success\"  # Stop even if not all tests pass\n",
    "        else:\n",
    "            print(f\"\\n⚠️ {state['test_results']['failed']} tests failed\")\n",
    "            return \"failure\"\n",
    "    \n",
    "    def _analyze_errors(self, state: CoderGenState) -> CoderGenState:\n",
    "        \"\"\"Analyze errors and prepare for regeneration\"\"\"\n",
    "        print(\"\\nAnalyzing errors...\")\n",
    "        state[\"iteration_count\"] += 1\n",
    "        return state\n",
    "    \n",
    "    def _regenerate_code(self, state: CoderGenState) -> CoderGenState:\n",
    "        \"\"\"Regenerate code based on error analysis\"\"\"\n",
    "        print(f\"\\n[Iteration {state['iteration_count']}] Regenerating code with fixes...\")\n",
    "        \n",
    "        messages = self.error_fix_prompt.format_messages(\n",
    "            generated_code=state[\"generated_code\"],\n",
    "            error_traceback=state[\"error_traceback\"]\n",
    "        )\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            response = self.llm(messages)\n",
    "            print(f\"Tokens used: {cb.total_tokens}\")\n",
    "        \n",
    "        state[\"generated_code\"] = response.content\n",
    "        state[\"messages\"].append(response)\n",
    "        return state\n",
    "    \n",
    "    def generate(self, task_description: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main method to generate code for a given task\"\"\"\n",
    "        initial_state = {\n",
    "            \"task_description\": task_description,\n",
    "            \"generated_code\": \"\",\n",
    "            \"error_traceback\": None,\n",
    "            \"iteration_count\": 1,\n",
    "            \"test_results\": {},\n",
    "            \"messages\": [],\n",
    "            \"is_completed\": False\n",
    "        }\n",
    "        \n",
    "        config = {\"configurable\": {\"thread_id\": \"codergen-1\"}}\n",
    "        final_state = self.app.invoke(initial_state, config)\n",
    "        \n",
    "        return {\n",
    "            \"code\": final_state[\"generated_code\"],\n",
    "            \"iterations\": final_state[\"iteration_count\"],\n",
    "            \"success\": final_state[\"is_completed\"],\n",
    "            \"test_results\": final_state[\"test_results\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sử dụng CoderGen để sinh code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CoderGen\n",
    "codergen = CoderGenFramework(model_name=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Example task from paper\n",
    "task_description = \"\"\"\n",
    "Create a text classification function using HuggingFace transformers.\n",
    "The function should:\n",
    "1. Load a pre-trained sentiment analysis model\n",
    "2. Accept text input and return sentiment (positive/negative/neutral)\n",
    "3. Handle batch processing for multiple texts\n",
    "4. Include proper error handling\n",
    "\"\"\"\n",
    "\n",
    "# Generate code\n",
    "result = codergen.generate(task_description)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATED CODE:\")\n",
    "print(\"=\"*50)\n",
    "print(result[\"code\"])\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Iterations: {result['iterations']}\")\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Test Results: {result['test_results']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation với DeepEval\n",
    "\n",
    "### Lý do sử dụng DeepEval:\n",
    "- **Phù hợp với paper**: Paper đánh giá SR@All và SR@Any metrics\n",
    "- **DeepEval mapping**: \n",
    "  - SR@All → Tất cả test cases pass (DeepEval's test suite)\n",
    "  - SR@Any → Ít nhất 1 test case pass\n",
    "  - Code quality → AnswerRelevancy và Faithfulness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AICoderEvaluator:\n",
    "    \"\"\"Evaluator for AICoderEval using DeepEval metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "        self.faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n",
    "    \n",
    "    def evaluate_code_generation(self, task: str, generated_code: str, \n",
    "                                test_results: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate generated code using multiple metrics\"\"\"\n",
    "        \n",
    "        # Calculate SR@All and SR@Any\n",
    "        total_tests = test_results.get(\"passed\", 0) + test_results.get(\"failed\", 0)\n",
    "        sr_all = 1.0 if test_results.get(\"failed\", 0) == 0 else 0.0\n",
    "        sr_any = 1.0 if test_results.get(\"passed\", 0) > 0 else 0.0\n",
    "        \n",
    "        # Create test case for DeepEval\n",
    "        test_case = LLMTestCase(\n",
    "            input=task,\n",
    "            actual_output=generated_code,\n",
    "            expected_output=\"Working code implementation\",  # Simplified\n",
    "            context=[\"AI code generation\", \"HuggingFace\", \"PyTorch\"]\n",
    "        )\n",
    "        \n",
    "        # Evaluate with DeepEval metrics\n",
    "        relevancy_score = self.relevancy_metric.measure(test_case)\n",
    "        faithfulness_score = self.faithfulness_metric.measure(test_case)\n",
    "        \n",
    "        # Calculate code metrics\n",
    "        code_lines = len(generated_code.split('\\n'))\n",
    "        code_tokens = len(generated_code.split())\n",
    "        \n",
    "        return {\n",
    "            \"sr_all\": sr_all,\n",
    "            \"sr_any\": sr_any,\n",
    "            \"relevancy\": relevancy_score,\n",
    "            \"faithfulness\": faithfulness_score,\n",
    "            \"code_lines\": code_lines,\n",
    "            \"code_tokens\": code_tokens,\n",
    "            \"passed_tests\": test_results.get(\"passed\", 0),\n",
    "            \"total_tests\": total_tests\n",
    "        }\n",
    "    \n",
    "    def benchmark_models(self, models: List[str], tasks: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Benchmark multiple models on tasks\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for model in models:\n",
    "            print(f\"\\nEvaluating {model}...\")\n",
    "            codergen = CoderGenFramework(model_name=model)\n",
    "            \n",
    "            model_results = {\n",
    "                \"model\": model,\n",
    "                \"sr_all_scores\": [],\n",
    "                \"sr_any_scores\": [],\n",
    "                \"code_lines\": [],\n",
    "                \"code_tokens\": []\n",
    "            }\n",
    "            \n",
    "            for task in tasks:\n",
    "                try:\n",
    "                    result = codergen.generate(task)\n",
    "                    eval_scores = self.evaluate_code_generation(\n",
    "                        task, result[\"code\"], result[\"test_results\"]\n",
    "                    )\n",
    "                    \n",
    "                    model_results[\"sr_all_scores\"].append(eval_scores[\"sr_all\"])\n",
    "                    model_results[\"sr_any_scores\"].append(eval_scores[\"sr_any\"])\n",
    "                    model_results[\"code_lines\"].append(eval_scores[\"code_lines\"])\n",
    "                    model_results[\"code_tokens\"].append(eval_scores[\"code_tokens\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {model} on task: {e}\")\n",
    "                    model_results[\"sr_all_scores\"].append(0)\n",
    "                    model_results[\"sr_any_scores\"].append(0)\n",
    "                    model_results[\"code_lines\"].append(0)\n",
    "                    model_results[\"code_tokens\"].append(0)\n",
    "            \n",
    "            # Calculate averages\n",
    "            results.append({\n",
    "                \"Model\": model,\n",
    "                \"SR@All\": np.mean(model_results[\"sr_all_scores\"]) * 100,\n",
    "                \"SR@Any\": np.mean(model_results[\"sr_any_scores\"]) * 100,\n",
    "                \"Avg Code Lines\": np.mean(model_results[\"code_lines\"]),\n",
    "                \"Avg Code Tokens\": np.mean(model_results[\"code_tokens\"])\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation\n",
    "evaluator = AICoderEvaluator()\n",
    "\n",
    "# Sample tasks for evaluation\n",
    "sample_tasks = [\n",
    "    \"Create a text classification function using HuggingFace transformers\",\n",
    "    \"Implement image classification using PyTorch with pretrained ResNet\",\n",
    "    \"Build a speech recognition pipeline using HuggingFace models\"\n",
    "]\n",
    "\n",
    "# Models to evaluate (simplified for demo)\n",
    "models_to_test = [\"gpt-3.5-turbo-1106\"]\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = evaluator.benchmark_models(models_to_test, sample_tasks[:1])\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(benchmark_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization của kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate results from paper for visualization\n",
    "paper_results = pd.DataFrame({\n",
    "    'Model': ['GPT-3.5-turbo', 'llama-2-7b', 'llama-2-13b', 'llama-2-70b', \n",
    "              'codellama-7b', 'codellama-13b', 'codellama-34b', 'llama-3-8b'],\n",
    "    'SR@All_Original': [9.16, 1.23, 2.76, 6.32, 19.58, 20.46, 23.68, 30.49],\n",
    "    'SR@All_Agent': [13.03, 1.83, 3.98, 8.16, 23.86, 23.88, 25.78, 32.11],\n",
    "    'SR@Any_Original': [46.84, 26.02, 42.04, 65.89, 66.95, 67.22, 70.19, 85.80],\n",
    "    'SR@Any_Agent': [60.63, 33.41, 51.24, 78.68, 78.18, 75.67, 77.33, 86.82]\n",
    "})\n",
    "\n",
    "# Calculate improvements\n",
    "paper_results['SR@All_Improvement'] = (\n",
    "    (paper_results['SR@All_Agent'] - paper_results['SR@All_Original']) / \n",
    "    paper_results['SR@All_Original'] * 100\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. SR@All comparison\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(paper_results))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, paper_results['SR@All_Original'], width, label='Original', alpha=0.8)\n",
    "ax1.bar(x + width/2, paper_results['SR@All_Agent'], width, label='With Agent', alpha=0.8)\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('SR@All (%)')\n",
    "ax1.set_title('SR@All: Original vs With ReAct Agent')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(paper_results['Model'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. SR@Any comparison\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x - width/2, paper_results['SR@Any_Original'], width, label='Original', alpha=0.8)\n",
    "ax2.bar(x + width/2, paper_results['SR@Any_Agent'], width, label='With Agent', alpha=0.8)\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_ylabel('SR@Any (%)')\n",
    "ax2.set_title('SR@Any: Original vs With ReAct Agent')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(paper_results['Model'], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Improvement percentage\n",
    "ax3 = axes[1, 0]\n",
    "ax3.bar(paper_results['Model'], paper_results['SR@All_Improvement'], \n",
    "        color='green', alpha=0.7)\n",
    "ax3.set_xlabel('Model')\n",
    "ax3.set_ylabel('Improvement (%)')\n",
    "ax3.set_title('SR@All Improvement with ReAct Agent')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Code efficiency (lines vs tokens)\n",
    "code_metrics = pd.DataFrame({\n",
    "    'Model': ['GPT-3.5', 'llama-2-7b', 'llama-2-13b', 'llama-2-70b', \n",
    "              'codellama-7b', 'codellama-13b', 'codellama-34b', 'llama-3-8b'],\n",
    "    'Code_Lines': [8.6, 16.2, 18.5, 13.1, 21.5, 18.9, 18.4, 11.02],\n",
    "    'Code_Tokens': [62.9, 112.9, 116.3, 107.8, 128.3, 116.3, 114.4, 96.97]\n",
    "})\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(code_metrics['Code_Lines'], code_metrics['Code_Tokens'], \n",
    "                     s=100, alpha=0.6, c=range(len(code_metrics)), cmap='viridis')\n",
    "for i, model in enumerate(code_metrics['Model']):\n",
    "    ax4.annotate(model, (code_metrics['Code_Lines'][i], code_metrics['Code_Tokens'][i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax4.set_xlabel('Average Code Lines')\n",
    "ax4.set_ylabel('Average Code Tokens')\n",
    "ax4.set_title('Code Efficiency: Lines vs Tokens')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Template cho nghiên cứu cá nhân\n",
    "\n",
    "### Hướng dẫn sử dụng CoderGen cho nghiên cứu:\n",
    "\n",
    "1. **Chuẩn bị dataset của riêng bạn**:\n",
    "   - Thu thập task descriptions cho domain cụ thể\n",
    "   - Format theo cấu trúc AICoderEval\n",
    "\n",
    "2. **Customize CoderGen**:\n",
    "   - Thay đổi prompts cho domain của bạn\n",
    "   - Thêm specialized error handlers\n",
    "   - Tích hợp domain-specific validators\n",
    "\n",
    "3. **Fine-tune model**:\n",
    "   - Sử dụng generated data để fine-tune\n",
    "   - Experiment với different base models\n",
    "\n",
    "4. **Evaluation**:\n",
    "   - Định nghĩa metrics phù hợp với domain\n",
    "   - So sánh với baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for custom research\n",
    "class CustomCoderGen(CoderGenFramework):\n",
    "    \"\"\"Template for customizing CoderGen for your research\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, domain: str):\n",
    "        self.domain = domain\n",
    "        super().__init__(model_name)\n",
    "    \n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Override to customize prompts for your domain\"\"\"\n",
    "        super()._setup_prompts()\n",
    "        \n",
    "        # Add domain-specific instructions\n",
    "        self.code_gen_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                f\"\"\"You are an expert in {self.domain} development.\n",
    "                Generate code following best practices for {self.domain}.\n",
    "                [Add your domain-specific guidelines here]\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Task: {task_description}\\n\\nGenerate complete implementation.\"\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def add_domain_validator(self, code: str) -> bool:\n",
    "        \"\"\"Add custom validation logic for your domain\"\"\"\n",
    "        # Implement domain-specific validation\n",
    "        return True\n",
    "\n",
    "# Example usage\n",
    "# custom_gen = CustomCoderGen(\"gpt-3.5-turbo\", \"robotics\")\n",
    "# result = custom_gen.generate(\"Create a ROS2 node for obstacle detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Kết luận\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **AICoderEval Dataset**: Benchmark quan trọng cho AI code generation với 492 real-world tasks\n",
    "\n",
    "2. **CoderGen Framework**: \n",
    "   - Agent-based approach với iterative refinement\n",
    "   - Error analysis và automatic fixing\n",
    "   - Cải thiện đáng kể performance (trung bình 28.20% cho SR@All)\n",
    "\n",
    "3. **Implementation với LangChain/LangGraph**:\n",
    "   - LangChain: Quản lý prompts và LLM interactions hiệu quả\n",
    "   - LangGraph: Perfect cho agent workflows với state management\n",
    "   - DeepEval: Phù hợp để đánh giá code quality và test success rates\n",
    "\n",
    "4. **Future Work**:\n",
    "   - Mở rộng cho nhiều programming languages\n",
    "   - Tích hợp nhiều AI frameworks hơn\n",
    "   - Improve sandboxed execution environment\n",
    "   - Fine-tune models cho specific domains\n",
    "\n",
    "### Resources:\n",
    "- Paper: https://arxiv.org/abs/2406.04712v1\n",
    "- Dataset: https://huggingface.co/datasets/vixuowis/AICoderEval\n",
    "- This implementation: [Your GitHub repo]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}