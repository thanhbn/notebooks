{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Comparative Analysis Frameworks for PEFT Methods\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement a robust framework for evaluating and comparing different PEFT methods\n",
    "- Understand the metrics and benchmarks used to evaluate PEFT techniques\n",
    "- Analyze the trade-offs between parameter efficiency, performance, and computational cost\n",
    "- Develop methodologies for fair and systematic comparison across diverse applications\n",
    "\n",
    "## Paper Reference\n",
    "This notebook explores concepts from the paper \"Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications\" (arXiv:2404.13506v2).\n",
    "\n",
    "Specifically, we focus on Tables 1-3 and Section 4 which present comparative analyses of PEFT methods:\n",
    "\n",
    "> \"PEFT has emerged as a compelling approach for tailoring large pre-trained models to specific tasks while minimizing computational demands. Our review found that leveraging PEFT across diverse applications presents several key challenges that require careful consideration...\" (Section 4, Page 6)\n",
    "\n",
    "The paper provides a comprehensive comparative analysis across different PEFT techniques and applications, which we'll explore in this focused learning notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Comparative Analysis of PEFT Methods\n",
    "\n",
    "When evaluating Parameter-Efficient Fine-Tuning (PEFT) methods, it's crucial to have a structured framework for comparison. Different techniques can vary widely in their parameter efficiency, computational requirements, and performance across various tasks and domains.\n",
    "\n",
    "In this notebook, we'll develop and demonstrate a comprehensive framework for comparing PEFT methods, inspired by the analysis presented in the paper. We'll explore:\n",
    "\n",
    "1. Key metrics for evaluating PEFT methods\n",
    "2. Implementation of a comparative analysis framework\n",
    "3. Visualization techniques for comparing methods\n",
    "4. Application-specific comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch transformers datasets peft matplotlib numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    "    TaskType,\n",
    "    PeftType,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    BitFitConfig,\n",
    "    AdaLoraConfig\n",
    ")\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metrics for Evaluating PEFT Methods\n",
    "\n",
    "Based on the paper, we'll define a set of key metrics for evaluating PEFT methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Parameter Efficiency Metrics\n",
    "\n",
    "1. **Trainable Parameter Count**: The number of parameters that are updated during fine-tuning\n",
    "2. **Parameter Percentage**: The percentage of trainable parameters relative to the full model size\n",
    "3. **Parameter Reduction Ratio**: The ratio of parameters saved by using PEFT (1 - parameter percentage)\n",
    "\n",
    "### 2.2 Performance Metrics\n",
    "\n",
    "1. **Task-Specific Metrics**: Accuracy, F1 score, BLEU, etc., depending on the task\n",
    "2. **Performance Gap**: The difference in performance between PEFT and full fine-tuning\n",
    "3. **Performance Retention**: The percentage of full fine-tuning performance retained by PEFT\n",
    "\n",
    "### 2.3 Computational Efficiency Metrics\n",
    "\n",
    "1. **Training Time**: The time required to train the model\n",
    "2. **Memory Usage**: The peak memory consumption during training\n",
    "3. **Training Speed**: The number of samples processed per second\n",
    "\n",
    "Let's implement functions to compute these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_total_parameters(model):\n",
    "    \"\"\"Count the total number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def parameter_efficiency_metrics(model, full_model):\n",
    "    \"\"\"Compute parameter efficiency metrics\"\"\"\n",
    "    trainable_params = count_parameters(model)\n",
    "    total_params = count_total_parameters(model)\n",
    "    full_trainable_params = count_parameters(full_model)\n",
    "    full_total_params = count_total_parameters(full_model)\n",
    "    \n",
    "    param_percentage = trainable_params / full_trainable_params * 100\n",
    "    param_reduction = 1 - (trainable_params / full_trainable_params)\n",
    "    \n",
    "    return {\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"total_params\": total_params,\n",
    "        \"param_percentage\": param_percentage,\n",
    "        \"param_reduction\": param_reduction\n",
    "    }\n",
    "\n",
    "def performance_metrics(peft_results, full_results, metric_name=\"accuracy\"):\n",
    "    \"\"\"Compute performance metrics\"\"\"\n",
    "    peft_performance = peft_results[metric_name]\n",
    "    full_performance = full_results[metric_name]\n",
    "    \n",
    "    performance_gap = full_performance - peft_performance\n",
    "    performance_retention = peft_performance / full_performance * 100\n",
    "    \n",
    "    return {\n",
    "        \"peft_performance\": peft_performance,\n",
    "        \"full_performance\": full_performance,\n",
    "        \"performance_gap\": performance_gap,\n",
    "        \"performance_retention\": performance_retention\n",
    "    }\n",
    "\n",
    "def computational_efficiency_metrics(peft_time, full_time, peft_memory, full_memory, samples_per_epoch):\n",
    "    \"\"\"Compute computational efficiency metrics\"\"\"\n",
    "    training_time_reduction = 1 - (peft_time / full_time)\n",
    "    memory_usage_reduction = 1 - (peft_memory / full_memory)\n",
    "    peft_speed = samples_per_epoch / peft_time\n",
    "    full_speed = samples_per_epoch / full_time\n",
    "    speed_improvement = peft_speed / full_speed\n",
    "    \n",
    "    return {\n",
    "        \"peft_training_time\": peft_time,\n",
    "        \"full_training_time\": full_time,\n",
    "        \"training_time_reduction\": training_time_reduction,\n",
    "        \"peft_memory_usage\": peft_memory,\n",
    "        \"full_memory_usage\": full_memory,\n",
    "        \"memory_usage_reduction\": memory_usage_reduction,\n",
    "        \"peft_speed\": peft_speed,\n",
    "        \"full_speed\": full_speed,\n",
    "        \"speed_improvement\": speed_improvement\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing a Comparative Analysis Framework\n",
    "\n",
    "Now, let's implement a framework for systematically comparing different PEFT methods. We'll use the GLUE benchmark's SST-2 dataset for sentiment analysis as an example task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples, tokenizer, max_length=128):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(\n",
    "    lambda examples: preprocess_function(examples, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define PEFT Configurations\n",
    "\n",
    "Let's define configurations for different PEFT methods that we want to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peft_configs():\n",
    "    \"\"\"Define PEFT configurations for comparison\"\"\"\n",
    "    peft_configs = {\n",
    "        \"LoRA_r8\": LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"LoRA_r16\": LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"LoRA_r32\": LoraConfig(\n",
    "            r=32,\n",
    "            lora_alpha=64,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"PrefixTuning\": PrefixTuningConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            prefix_length=30,\n",
    "            num_virtual_tokens=20,\n",
    "        ),\n",
    "        \"PromptTuning\": PromptEncoderConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            num_virtual_tokens=20,\n",
    "            encoder_hidden_size=128\n",
    "        ),\n",
    "        \"BitFit\": BitFitConfig(\n",
    "            bias_term=\"all\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"AdaLoRA\": AdaLoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return peft_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training and Evaluation Function\n",
    "\n",
    "Now, let's implement a function to train and evaluate models with different PEFT configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_name, peft_configs, tokenized_datasets, metric_name=\"accuracy\", num_epochs=3):\n",
    "    \"\"\"Train and evaluate different PEFT configurations\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Train and evaluate full fine-tuning first\n",
    "    print(\"Training full fine-tuning model...\")\n",
    "    full_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    full_trainable_params = count_parameters(full_model)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Define trainer\n",
    "    trainer = Trainer(\n",
    "        model=full_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Train full model and measure time and memory\n",
    "    start_time = time.time()\n",
    "    full_memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    trainer.train()\n",
    "    full_memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    full_memory_usage = full_memory_after - full_memory_before\n",
    "    full_training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate full model\n",
    "    full_eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Store full model results\n",
    "    full_results = {\n",
    "        \"method\": \"Full Fine-tuning\",\n",
    "        \"trainable_params\": full_trainable_params,\n",
    "        \"param_percentage\": 100.0,\n",
    "        \"training_time\": full_training_time,\n",
    "        \"memory_usage\": full_memory_usage,\n",
    "        metric_name: full_eval_results[\"eval_accuracy\"]\n",
    "    }\n",
    "    \n",
    "    results.append(full_results)\n",
    "    \n",
    "    # Train and evaluate PEFT models\n",
    "    for method_name, peft_config in peft_configs.items():\n",
    "        print(f\"\\nTraining {method_name} model...\")\n",
    "        \n",
    "        # Create base model\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "        \n",
    "        # Create PEFT model\n",
    "        peft_model = get_peft_model(base_model, peft_config)\n",
    "        peft_trainable_params = count_parameters(peft_model)\n",
    "        param_percentage = peft_trainable_params / full_trainable_params * 100\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        peft_model.print_trainable_parameters()\n",
    "        \n",
    "        # Define trainer\n",
    "        trainer = Trainer(\n",
    "            model=peft_model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        # Train PEFT model and measure time and memory\n",
    "        start_time = time.time()\n",
    "        peft_memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "        trainer.train()\n",
    "        peft_memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "        peft_memory_usage = peft_memory_after - peft_memory_before\n",
    "        peft_training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate PEFT model\n",
    "        peft_eval_results = trainer.evaluate()\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        perf_metrics = performance_metrics(\n",
    "            {metric_name: peft_eval_results[\"eval_accuracy\"]},\n",
    "            {metric_name: full_results[metric_name]},\n",
    "            metric_name\n",
    "        )\n",
    "        \n",
    "        # Store PEFT model results\n",
    "        peft_results = {\n",
    "            \"method\": method_name,\n",
    "            \"trainable_params\": peft_trainable_params,\n",
    "            \"param_percentage\": param_percentage,\n",
    "            \"training_time\": peft_training_time,\n",
    "            \"memory_usage\": peft_memory_usage,\n",
    "            metric_name: peft_eval_results[\"eval_accuracy\"],\n",
    "            \"performance_gap\": perf_metrics[\"performance_gap\"],\n",
    "            \"performance_retention\": perf_metrics[\"performance_retention\"],\n",
    "            \"training_time_reduction\": 1 - (peft_training_time / full_training_time),\n",
    "            \"memory_reduction\": 1 - (peft_memory_usage / full_memory_usage)\n",
    "        }\n",
    "        \n",
    "        results.append(peft_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the Comparative Analysis\n",
    "\n",
    "Now, let's run our comparative analysis framework on the selected PEFT methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PEFT configurations\n",
    "peft_configs = get_peft_configs()\n",
    "\n",
    "# Run the comparative analysis\n",
    "results = train_and_evaluate(model_name, peft_configs, tokenized_datasets, \"accuracy\", num_epochs=3)\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Comparative Results\n",
    "\n",
    "Let's create visualizations to compare the performance of different PEFT methods based on our evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results table\n",
    "print(\"Comparative Analysis Results:\")\n",
    "display_cols = [\n",
    "    \"method\", \"trainable_params\", \"param_percentage\", \"accuracy\", \n",
    "    \"performance_retention\", \"training_time\", \"training_time_reduction\"\n",
    "]\n",
    "display_df = results_df[display_cols].copy()\n",
    "display_df[\"trainable_params\"] = display_df[\"trainable_params\"].apply(lambda x: f\"{x:,}\")\n",
    "display_df[\"param_percentage\"] = display_df[\"param_percentage\"].apply(lambda x: f\"{x:.2f}%\")\n",
    "display_df[\"accuracy\"] = display_df[\"accuracy\"].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df[\"performance_retention\"] = display_df[\"performance_retention\"].apply(lambda x: f\"{x:.2f}%\" if not pd.isna(x) else \"100.00%\")\n",
    "display_df[\"training_time\"] = display_df[\"training_time\"].apply(lambda x: f\"{x:.2f}s\")\n",
    "display_df[\"training_time_reduction\"] = display_df[\"training_time_reduction\"].apply(lambda x: f\"{x*100:.2f}%\" if not pd.isna(x) else \"0.00%\")\n",
    "\n",
    "# Print the table using tabulate\n",
    "print(tabulate(display_df, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter efficiency vs. performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "full_model_accuracy = results_df[results_df[\"method\"] == \"Full Fine-tuning\"][\"accuracy\"].values[0]\n",
    "\n",
    "# Filter out the full model for better visualization\n",
    "peft_results = results_df[results_df[\"method\"] != \"Full Fine-tuning\"].copy()\n",
    "\n",
    "# Create scatter plot\n",
    "sns.scatterplot(\n",
    "    data=peft_results, \n",
    "    x=\"param_percentage\", \n",
    "    y=\"accuracy\", \n",
    "    hue=\"method\", \n",
    "    size=\"training_time\",\n",
    "    sizes=(100, 500),\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add a horizontal line for full model accuracy\n",
    "plt.axhline(y=full_model_accuracy, color='r', linestyle='--', label=f\"Full Fine-tuning ({full_model_accuracy:.4f})\")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in peft_results.iterrows():\n",
    "    plt.annotate(\n",
    "        row[\"method\"], \n",
    "        (row[\"param_percentage\"], row[\"accuracy\"]),\n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10), \n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title(\"Parameter Efficiency vs. Performance\")\n",
    "plt.xlabel(\"Parameter Percentage (%)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(title=\"Method\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training time vs. performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=results_df, \n",
    "    x=\"training_time\", \n",
    "    y=\"accuracy\", \n",
    "    hue=\"method\", \n",
    "    size=\"param_percentage\",\n",
    "    sizes=(50, 500),\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.annotate(\n",
    "        row[\"method\"], \n",
    "        (row[\"training_time\"], row[\"accuracy\"]),\n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10), \n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title(\"Training Time vs. Performance\")\n",
    "plt.xlabel(\"Training Time (seconds)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(title=\"Method\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the efficiency-performance trade-off\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Create data for radar chart\n",
    "peft_methods = results_df[\"method\"].tolist()\n",
    "param_efficiency = 100 - results_df[\"param_percentage\"]\n",
    "normalized_accuracy = results_df[\"accuracy\"] / results_df[\"accuracy\"].max() * 100\n",
    "time_efficiency = (1 - (results_df[\"training_time\"] / results_df[\"training_time\"].max())) * 100\n",
    "\n",
    "# Plot parameter efficiency\n",
    "plt.subplot(1, 3, 1)\n",
    "bars = plt.bar(peft_methods, param_efficiency, color=sns.color_palette(\"viridis\", len(peft_methods)))\n",
    "plt.title(\"Parameter Efficiency (higher is better)\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Efficiency (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot normalized accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "bars = plt.bar(peft_methods, normalized_accuracy, color=sns.color_palette(\"viridis\", len(peft_methods)))\n",
    "plt.title(\"Normalized Accuracy (higher is better)\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Normalized Accuracy (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot time efficiency\n",
    "plt.subplot(1, 3, 3)\n",
    "bars = plt.bar(peft_methods, time_efficiency, color=sns.color_palette(\"viridis\", len(peft_methods)))\n",
    "plt.title(\"Time Efficiency (higher is better)\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Efficiency (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Across Different Tasks\n",
    "\n",
    "One of the key contributions of the paper is the analysis of PEFT methods across different application domains. Let's recreate and analyze some of the findings from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the paper on LoRA performance across domains\n",
    "domain_data = {\n",
    "    \"Application\": [\n",
    "        \"Commonsense Reasoning\",\n",
    "        \"Arithmetic Reasoning\",\n",
    "        \"Video Text Generation\",\n",
    "        \"Medical Imaging\",\n",
    "        \"Protein Models\",\n",
    "        \"Code Review\",\n",
    "        \"Speech Synthesis\"\n",
    "    ],\n",
    "    \"Backbone\": [\n",
    "        \"LLaMA-7B\",\n",
    "        \"LLaMA-7B\",\n",
    "        \"CLIP, LLaMA-7B\",\n",
    "        \"ResNet-50, ViT\",\n",
    "        \"ESM2\",\n",
    "        \"LLaMA-6.7B\",\n",
    "        \"WavLM, Whisper\"\n",
    "    ],\n",
    "    \"PEFT_Method\": [\n",
    "        \"LoRA\",\n",
    "        \"LoRA\",\n",
    "        \"LoRA + AGAdapter\",\n",
    "        \"LoRA + BitFit\",\n",
    "        \"LoRA + BitFit\",\n",
    "        \"Zero-init + LoRA\",\n",
    "        \"LoRA\"\n",
    "    ],\n",
    "    \"Param_Percentage\": [0.83, 0.83, 0.81, 0.81, 0.81, 0.8, 0.8],\n",
    "    \"Performance_Retention\": [95, 90, 97, 93, 99, 94, 98],\n",
    "    \"Primary_Benefit\": [\n",
    "        \"High accuracy with few parameters\",\n",
    "        \"Computational efficiency\",\n",
    "        \"Multi-modal adaptation\",\n",
    "        \"Reduced data requirements\",\n",
    "        \"Improved prediction accuracy\",\n",
    "        \"Fast fine-tuning\",\n",
    "        \"Enhanced fairness scores\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "domain_df = pd.DataFrame(domain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data table\n",
    "print(\"LoRA Performance Across Application Domains (Based on Paper Data):\")\n",
    "print(tabulate(domain_df, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance retention across domains\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(\n",
    "    domain_df[\"Application\"],\n",
    "    domain_df[\"Performance_Retention\"],\n",
    "    color=sns.color_palette(\"viridis\", len(domain_df))\n",
    ")\n",
    "plt.axhline(y=90, color='r', linestyle='--', label='90% Retention Threshold')\n",
    "plt.title(\"LoRA Performance Retention Across Application Domains\")\n",
    "plt.xlabel(\"Application Domain\")\n",
    "plt.ylabel(\"Performance Retention (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(80, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels on the bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{height:.0f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter percentage vs. performance retention\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(\n",
    "    domain_df[\"Param_Percentage\"],\n",
    "    domain_df[\"Performance_Retention\"],\n",
    "    s=100,\n",
    "    c=np.arange(len(domain_df)),\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in domain_df.iterrows():\n",
    "    plt.annotate(\n",
    "        row[\"Application\"], \n",
    "        (row[\"Param_Percentage\"], row[\"Performance_Retention\"]),\n",
    "        textcoords=\"offset points\", \n",
    "        xytext=(0, 10), \n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title(\"Parameter Efficiency vs. Performance Retention Across Domains\")\n",
    "plt.xlabel(\"Parameter Percentage (%)\")\n",
    "plt.ylabel(\"Performance Retention (%)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Different PEFT Methods on Commonsense Reasoning\n",
    "\n",
    "The paper provides a detailed comparison of different PEFT methods on commonsense reasoning tasks. Let's recreate and analyze this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Table 1 in the paper (LLaMA-7B model)\n",
    "commonsense_data_7b = {\n",
    "    \"Method\": [\"ChatGPT\", \"PrefT\", \"AdapterS\", \"AdapterP\", \"LoRA\", \"DoRA (half)\", \"DoRA\", \"LoReFT\"],\n",
    "    \"Params_Percentage\": [None, 0.110, 0.990, 3.540, 0.830, 0.430, 0.840, 0.031],\n",
    "    \"Average_Accuracy\": [77.0, 64.6, 70.8, 72.3, 74.7, 77.5, 78.1, 80.2]\n",
    "}\n",
    "\n",
    "# Data from Table 1 in the paper (LLaMA-13B model)\n",
    "commonsense_data_13b = {\n",
    "    \"Method\": [\"ChatGPT\", \"PrefT\", \"AdapterS\", \"AdapterP\", \"LoRA\", \"DoRA (half)\", \"DoRA\", \"LoReFT\"],\n",
    "    \"Params_Percentage\": [None, 0.030, 0.800, 2.890, 0.670, 0.350, 0.680, 0.025],\n",
    "    \"Average_Accuracy\": [77.0, 68.4, 79.5, 81.5, 80.5, 80.8, 81.5, 83.3]\n",
    "}\n",
    "\n",
    "commonsense_df_7b = pd.DataFrame(commonsense_data_7b)\n",
    "commonsense_df_13b = pd.DataFrame(commonsense_data_13b)\n",
    "\n",
    "# Add model column\n",
    "commonsense_df_7b[\"Model\"] = \"LLaMA-7B\"\n",
    "commonsense_df_13b[\"Model\"] = \"LLaMA-13B\"\n",
    "\n",
    "# Combine the data\n",
    "commonsense_df = pd.concat([commonsense_df_7b, commonsense_df_13b], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data table\n",
    "print(\"PEFT Methods Comparison on Commonsense Reasoning:\")\n",
    "display_df = commonsense_df.copy()\n",
    "display_df[\"Params_Percentage\"] = display_df[\"Params_Percentage\"].apply(lambda x: f\"{x:.3f}%\" if x is not None else \"N/A\")\n",
    "display_df[\"Average_Accuracy\"] = display_df[\"Average_Accuracy\"].apply(lambda x: f\"{x:.1f}%\")\n",
    "print(tabulate(display_df, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Filter out ChatGPT for parameter percentage comparison\n",
    "filtered_df = commonsense_df[commonsense_df[\"Method\"] != \"ChatGPT\"].copy()\n",
    "\n",
    "# Create two bar charts side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot LLaMA-7B\n",
    "df_7b = filtered_df[filtered_df[\"Model\"] == \"LLaMA-7B\"].sort_values(by=\"Average_Accuracy\")\n",
    "bars1 = ax1.bar(\n",
    "    df_7b[\"Method\"],\n",
    "    df_7b[\"Average_Accuracy\"],\n",
    "    color=sns.color_palette(\"viridis\", len(df_7b))\n",
    ")\n",
    "ax1.axhline(y=77.0, color='r', linestyle='--', label='ChatGPT (77.0%)')\n",
    "ax1.set_title(\"LLaMA-7B: Accuracy on Commonsense Reasoning\")\n",
    "ax1.set_xlabel(\"PEFT Method\")\n",
    "ax1.set_ylabel(\"Average Accuracy (%)\")\n",
    "ax1.set_ylim(60, 85)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# Add parameter percentage annotations\n",
    "for i, bar in enumerate(bars1):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width()/2.,\n",
    "        bar.get_height() + 0.5,\n",
    "        f'{df_7b[\"Params_Percentage\"].iloc[i]:.3f}%',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        rotation=0,\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# Plot LLaMA-13B\n",
    "df_13b = filtered_df[filtered_df[\"Model\"] == \"LLaMA-13B\"].sort_values(by=\"Average_Accuracy\")\n",
    "bars2 = ax2.bar(\n",
    "    df_13b[\"Method\"],\n",
    "    df_13b[\"Average_Accuracy\"],\n",
    "    color=sns.color_palette(\"viridis\", len(df_13b))\n",
    ")\n",
    "ax2.axhline(y=77.0, color='r', linestyle='--', label='ChatGPT (77.0%)')\n",
    "ax2.set_title(\"LLaMA-13B: Accuracy on Commonsense Reasoning\")\n",
    "ax2.set_xlabel(\"PEFT Method\")\n",
    "ax2.set_ylabel(\"Average Accuracy (%)\")\n",
    "ax2.set_ylim(60, 85)\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "# Add parameter percentage annotations\n",
    "for i, bar in enumerate(bars2):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width()/2.,\n",
    "        bar.get_height() + 0.5,\n",
    "        f'{df_13b[\"Params_Percentage\"].iloc[i]:.3f}%',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        rotation=0,\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of parameter percentage vs. accuracy\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Filter out ChatGPT\n",
    "filtered_df = commonsense_df[commonsense_df[\"Method\"] != \"ChatGPT\"].copy()\n",
    "\n",
    "# Plot LLaMA-7B\n",
    "plt.scatter(\n",
    "    filtered_df[filtered_df[\"Model\"] == \"LLaMA-7B\"][\"Params_Percentage\"],\n",
    "    filtered_df[filtered_df[\"Model\"] == \"LLaMA-7B\"][\"Average_Accuracy\"],\n",
    "    s=150,\n",
    "    marker='o',\n",
    "    label='LLaMA-7B',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Plot LLaMA-13B\n",
    "plt.scatter(\n",
    "    filtered_df[filtered_df[\"Model\"] == \"LLaMA-13B\"][\"Params_Percentage\"],\n",
    "    filtered_df[filtered_df[\"Model\"] == \"LLaMA-13B\"][\"Average_Accuracy\"],\n",
    "    s=150,\n",
    "    marker='s',\n",
    "    label='LLaMA-13B',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add horizontal line for ChatGPT\n",
    "plt.axhline(y=77.0, color='r', linestyle='--', label='ChatGPT (77.0%)')\n",
    "\n",
    "# Add method labels\n",
    "for model in [\"LLaMA-7B\", \"LLaMA-13B\"]:\n",
    "    model_df = filtered_df[filtered_df[\"Model\"] == model]\n",
    "    for i, row in model_df.iterrows():\n",
    "        plt.annotate(\n",
    "            row[\"Method\"],\n",
    "            (row[\"Params_Percentage\"], row[\"Average_Accuracy\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 10 if model == \"LLaMA-7B\" else -15),\n",
    "            ha='center',\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "plt.title(\"Parameter Efficiency vs. Accuracy on Commonsense Reasoning\")\n",
    "plt.xlabel(\"Parameter Percentage (%)\")\n",
    "plt.ylabel(\"Average Accuracy (%)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis of Results and Implications\n",
    "\n",
    "Based on our comparative analysis and the data from the paper, we can draw several key insights about PEFT methods and their evaluation frameworks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Parameter Efficiency vs. Performance Trade-off\n",
    "\n",
    "1. **Efficiency-Performance Spectrum**: Different PEFT methods occupy different positions on the efficiency-performance spectrum. Some methods (like LoReFT) achieve remarkable parameter efficiency with minimal performance degradation, while others prioritize performance at the cost of more parameters.\n",
    "\n",
    "2. **Non-linear Relationship**: The relationship between parameter percentage and performance is not linear. Some methods with very few parameters (like LoReFT with only 0.025-0.031% of parameters) can outperform methods with more parameters.\n",
    "\n",
    "3. **Diminishing Returns**: There appears to be a point of diminishing returns, where adding more trainable parameters yields minimal performance improvements.\n",
    "\n",
    "### 7.2 Application-Specific Considerations\n",
    "\n",
    "1. **Domain Dependency**: The performance of PEFT methods can vary significantly across different application domains. While LoRA shows consistent performance (90-99% retention) across domains, the optimal method may differ based on the specific application.\n",
    "\n",
    "2. **Model Size Effects**: The benefits of PEFT methods are often more pronounced with larger models. For instance, the performance gap between PEFT methods and full fine-tuning tends to be smaller with the LLaMA-13B model compared to the LLaMA-7B model.\n",
    "\n",
    "3. **Task Complexity**: More complex tasks (like arithmetic reasoning) may benefit from different PEFT approaches compared to simpler tasks.\n",
    "\n",
    "### 7.3 Evaluation Framework Considerations\n",
    "\n",
    "1. **Multi-dimensional Evaluation**: A comprehensive evaluation of PEFT methods should consider multiple dimensions, including parameter efficiency, performance, training time, memory usage, and generalization.\n",
    "\n",
    "2. **Standardized Benchmarks**: To ensure fair comparisons, it's important to use standardized benchmarks and consistent evaluation protocols across different methods.\n",
    "\n",
    "3. **Hyperparameter Sensitivity**: The performance of PEFT methods can be sensitive to hyperparameters. A fair comparison should account for this by optimizing hyperparameters for each method or using consistent hyperparameter settings.\n",
    "\n",
    "4. **Computational Efficiency Metrics**: Beyond parameter counts, metrics like training time, memory usage, and inference speed provide important insights into the practical efficiency of PEFT methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Best Practices\n",
    "\n",
    "Based on our analysis and the findings from the paper, we can distill some best practices for evaluating and comparing PEFT methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Best Practices for PEFT Evaluation\n",
    "\n",
    "1. **Comprehensive Metrics**: Include a diverse set of metrics covering parameter efficiency, performance, computational efficiency, and generalization.\n",
    "\n",
    "2. **Baseline Comparison**: Always compare PEFT methods against full fine-tuning and zero-shot performance as baselines.\n",
    "\n",
    "3. **Diverse Tasks**: Evaluate methods on a diverse set of tasks to assess their versatility across different application domains.\n",
    "\n",
    "4. **Hyperparameter Sensitivity Analysis**: Analyze the sensitivity of PEFT methods to their hyperparameters to ensure robust conclusions.\n",
    "\n",
    "5. **Resource Constraints**: Consider the target deployment environment and its resource constraints when evaluating PEFT methods.\n",
    "\n",
    "6. **Statistical Significance**: Run multiple trials with different random seeds to assess the statistical significance of performance differences.\n",
    "\n",
    "7. **Visualization Tools**: Use effective visualizations to communicate the trade-offs between different dimensions of PEFT methods.\n",
    "\n",
    "### 8.2 Future Directions\n",
    "\n",
    "The paper suggests several promising directions for future research on PEFT methods and their evaluation:\n",
    "\n",
    "1. **Task-Agnostic PEFT**: Developing PEFT methods that are universally applicable across different downstream tasks without the need for task-specific adaptation.\n",
    "\n",
    "2. **Privacy-Preserving PEFT**: Exploring techniques like federated learning or homomorphic encryption for privacy-preserving PEFT in sensitive domains like healthcare.\n",
    "\n",
    "3. **Limited Data Scenarios**: Enhancing the robustness of PEFT methods in scenarios with limited labeled data through techniques like active learning or curriculum learning.\n",
    "\n",
    "4. **Interpretability**: Improving the interpretability of fine-tuned models to understand how PEFT methods affect the model's decision-making process.\n",
    "\n",
    "5. **Automated PEFT Selection**: Developing methods to automatically select the most appropriate PEFT technique for a given task and model architecture.\n",
    "\n",
    "By following these best practices and exploring these future directions, researchers and practitioners can more effectively leverage PEFT methods to make deep learning more accessible, efficient, and adaptable across diverse applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Balne, C. C. S., Bhaduri, S., Roy, T., Jain, V., & Chadha, A. (2024). Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications. arXiv:2404.13506v2.\n",
    "\n",
    "2. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\n",
    "\n",
    "3. Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). ReFT: Representation finetuning for language models. arXiv preprint arXiv:2401.13622.\n",
    "\n",
    "4. Li, X. L., & Liang, P. (2021). Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.\n",
    "\n",
    "5. Zaken, E. B., Ravfogel, S., & Goldberg, Y. (2021). BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}