{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) v·ªõi Claude\n",
    "\n",
    "## üéØ M·ª•c ti√™u\n",
    "\n",
    "Trong notebook n√†y, ch√∫ng ta s·∫Ω x√¢y d·ª±ng m·ªôt **RAG system ho√†n ch·ªânh** t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi:\n",
    "\n",
    "1. **üìÅ Document Loading**: T·∫£i v√† x·ª≠ l√Ω documents\n",
    "2. **‚úÇÔ∏è Text Splitting**: Chia documents th√†nh chunks\n",
    "3. **üßÆ Embeddings**: Chuy·ªÉn ƒë·ªïi text th√†nh vectors\n",
    "4. **üóÑÔ∏è Vector Store**: L∆∞u tr·ªØ v√† index embeddings\n",
    "5. **üîç Retrieval**: T√¨m ki·∫øm relevant documents\n",
    "6. **ü§ñ Generation**: S·ª≠ d·ª•ng Claude ƒë·ªÉ generate answers\n",
    "7. **üîó End-to-end Pipeline**: K·∫øt h·ª£p t·∫•t c·∫£ th√†nh RAG system\n",
    "\n",
    "## üèóÔ∏è RAG Architecture Overview\n",
    "\n",
    "```\n",
    "üìÑ Documents ‚Üí Text Splitter ‚Üí üßÆ Embeddings ‚Üí üóÑÔ∏è Vector Store\n",
    "                                                      ‚Üì\n",
    "‚ùì User Query ‚Üí üßÆ Query Embedding ‚Üí üîç Similarity Search\n",
    "                                                      ‚Üì\n",
    "üìö Retrieved Context ‚Üí ü§ñ Claude (LLM) ‚Üí ‚ú® Final Answer\n",
    "```\n",
    "\n",
    "## üí° Why RAG?\n",
    "\n",
    "RAG gi·∫£i quy·∫øt nh·ªØng limitations c·ªßa LLMs:\n",
    "- **üìö Knowledge Cutoff**: LLMs trained v·ªõi data c≈©\n",
    "- **üîç Domain Specificity**: C·∫ßn knowledge v·ªÅ specific domains\n",
    "- **üìä Factual Accuracy**: Gi·∫£m hallucination b·∫±ng grounded information\n",
    "- **üîÑ Dynamic Updates**: Update knowledge m√† kh√¥ng c·∫ßn retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup v√† Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# Suppress warnings cho cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    DirectoryLoader\n",
    ")\n",
    "\n",
    "# Vector Stores v√† Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# LangChain Anthropic\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o models\n",
    "print(\"üöÄ Initializing models...\")\n",
    "\n",
    "# Claude LLM\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    temperature=0,  # Deterministic for factual responses\n",
    "    anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "# HuggingFace Embeddings (consistent v·ªõi previous notebooks)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Claude LLM initialized\")\n",
    "print(\"‚úÖ HuggingFace embeddings initialized\")\n",
    "\n",
    "# Test both models\n",
    "test_llm = llm.invoke(\"Hello Claude!\")\n",
    "test_embedding = embeddings.embed_query(\"test sentence\")\n",
    "\n",
    "print(f\"üß™ LLM test: {test_llm.content[:50]}...\")\n",
    "print(f\"üß™ Embedding test: dimension={len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chu·∫©n b·ªã Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o sample documents v·ªÅ LangChain v√† AI\n",
    "sample_documents = {\n",
    "    \"langchain_overview.txt\": \"\"\"\n",
    "LangChain Framework Overview\n",
    "\n",
    "LangChain l√† m·ªôt framework m·∫°nh m·∫Ω ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ph√°t tri·ªÉn c√°c ·ª©ng d·ª•ng s·ª≠ d·ª•ng Large Language Models (LLMs). \n",
    "Framework n√†y cung c·∫•p m·ªôt b·ªô c√¥ng c·ª• to√†n di·ªán ƒë·ªÉ x√¢y d·ª±ng, tri·ªÉn khai v√† qu·∫£n l√Ω c√°c ·ª©ng d·ª•ng AI ph·ª©c t·∫°p.\n",
    "\n",
    "C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa LangChain:\n",
    "\n",
    "1. LLMs v√† Prompts: LangChain h·ªó tr·ª£ nhi·ªÅu LLM providers nh∆∞ OpenAI, Anthropic, Cohere, v√† Hugging Face. \n",
    "Framework cung c·∫•p prompt templates v√† prompt management ƒë·ªÉ t·ªëi ∆∞u h√≥a interactions v·ªõi LLMs.\n",
    "\n",
    "2. Chains: Chains cho ph√©p k·∫øt n·ªëi nhi·ªÅu components l·∫°i v·ªõi nhau ƒë·ªÉ t·∫°o ra workflows ph·ª©c t·∫°p. \n",
    "V√≠ d·ª•: LLMChain, SequentialChain, v√† custom chains cho specific use cases.\n",
    "\n",
    "3. Data Connection: LangChain cung c·∫•p c√°c document loaders, text splitters, retrievers, v√† vector stores \n",
    "ƒë·ªÉ t√≠ch h·ª£p v·ªõi external data sources m·ªôt c√°ch hi·ªáu qu·∫£.\n",
    "\n",
    "4. Memory: Memory components gi√∫p maintain context gi·ªØa c√°c interactions, bao g·ªìm ConversationBufferMemory, \n",
    "ConversationSummaryMemory, v√† c√°c lo·∫°i memory kh√°c.\n",
    "\n",
    "5. Agents: Agents s·ª≠ d·ª•ng LLMs ƒë·ªÉ quy·∫øt ƒë·ªãnh actions v√† c√≥ th·ªÉ access tools ƒë·ªÉ th·ª±c hi·ªán complex tasks.\n",
    "\n",
    "LangChain ƒë∆∞·ª£c thi·∫øt k·∫ø v·ªõi t√≠nh modularity cao, cho ph√©p developers mix v√† match c√°c components \n",
    "ƒë·ªÉ t·∫°o ra solutions ph√π h·ª£p v·ªõi requirements c·ª• th·ªÉ.\n",
    "\"\"\",\n",
    "    \n",
    "    \"rag_systems.txt\": \"\"\"\n",
    "Retrieval Augmented Generation (RAG) Systems\n",
    "\n",
    "RAG l√† m·ªôt k·ªπ thu·∫≠t quan tr·ªçng trong AI hi·ªán ƒë·∫°i, k·∫øt h·ª£p kh·∫£ nƒÉng truy xu·∫•t th√¥ng tin (retrieval) \n",
    "v·ªõi kh·∫£ nƒÉng sinh vƒÉn b·∫£n (generation) c·ªßa LLMs ƒë·ªÉ t·∫°o ra responses ch√≠nh x√°c v√† c√≥ cƒÉn c·ª©.\n",
    "\n",
    "Ki·∫øn tr√∫c RAG System:\n",
    "\n",
    "1. Document Processing: Documents ƒë∆∞·ª£c load t·ª´ various sources (PDFs, websites, databases) \n",
    "v√† ƒë∆∞·ª£c preprocessing ƒë·ªÉ extract clean text content.\n",
    "\n",
    "2. Text Chunking: Large documents ƒë∆∞·ª£c chia th√†nh smaller chunks ƒë·ªÉ optimize retrieval performance \n",
    "v√† fit trong context windows c·ªßa LLMs. Chunk size v√† overlap ph·∫£i ƒë∆∞·ª£c tune carefully.\n",
    "\n",
    "3. Embedding Generation: Text chunks ƒë∆∞·ª£c convert th√†nh dense vector representations \n",
    "s·ª≠ d·ª•ng embedding models nh∆∞ OpenAI embeddings ho·∫∑c sentence transformers.\n",
    "\n",
    "4. Vector Storage: Embeddings ƒë∆∞·ª£c store trong specialized vector databases nh∆∞ Pinecone, \n",
    "Chroma, FAISS, ho·∫∑c Weaviate ƒë·ªÉ enable fast similarity search.\n",
    "\n",
    "5. Query Processing: User queries ƒë∆∞·ª£c embed s·ª≠ d·ª•ng same embedding model \n",
    "ƒë·ªÉ ensure consistency trong vector space.\n",
    "\n",
    "6. Retrieval: System th·ª±c hi·ªán similarity search ƒë·ªÉ find most relevant chunks \n",
    "based on cosine similarity ho·∫∑c other distance metrics.\n",
    "\n",
    "7. Context Construction: Retrieved chunks ƒë∆∞·ª£c combine th√†nh coherent context \n",
    "v·ªõi appropriate formatting v√† metadata.\n",
    "\n",
    "8. Response Generation: LLM generates final response based on retrieved context v√† original query, \n",
    "ensuring answers are grounded in actual information.\n",
    "\n",
    "RAG Benefits:\n",
    "- Improved factual accuracy\n",
    "- Access to up-to-date information\n",
    "- Domain-specific knowledge integration\n",
    "- Reduced hallucination\n",
    "- Transparency v√† traceability\n",
    "\"\"\",\n",
    "    \n",
    "    \"vector_databases.txt\": \"\"\"\n",
    "Vector Databases trong AI Applications\n",
    "\n",
    "Vector databases l√† specialized storage systems ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ efficiently store, index, \n",
    "v√† search high-dimensional vector data. Ch√∫ng ƒë√≥ng vai tr√≤ quan tr·ªçng trong modern AI applications.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "1. High-dimensional Storage: Vector databases optimize storage cho vectors v·ªõi hundreds ho·∫∑c \n",
    "thousands of dimensions, th∆∞·ªùng generated t·ª´ machine learning models.\n",
    "\n",
    "2. Similarity Search: Core functionality l√† t√¨m vectors similar nh·∫•t v·ªõi query vector \n",
    "s·ª≠ d·ª•ng metrics nh∆∞ cosine similarity, euclidean distance, ho·∫∑c dot product.\n",
    "\n",
    "3. Approximate Nearest Neighbor (ANN): ƒê·ªÉ achieve fast search tr√™n large datasets, \n",
    "vector databases s·ª≠ d·ª•ng ANN algorithms nh∆∞ HNSW, IVF, ho·∫∑c LSH.\n",
    "\n",
    "4. Metadata Filtering: Ngo√†i vector similarity, systems support filtering based on metadata \n",
    "ƒë·ªÉ narrow down search results.\n",
    "\n",
    "Popular Vector Databases:\n",
    "\n",
    "- Pinecone: Fully managed cloud service v·ªõi excellent performance v√† scalability\n",
    "- Chroma: Open-source, lightweight option t·ªët cho development v√† prototyping\n",
    "- FAISS: Facebook's library, excellent performance nh∆∞ng requires more setup\n",
    "- Weaviate: Open-source v·ªõi good GraphQL interface\n",
    "- Qdrant: Rust-based v·ªõi high performance\n",
    "\n",
    "Use Cases:\n",
    "- Semantic search\n",
    "- Recommendation systems\n",
    "- RAG systems\n",
    "- Image v√† audio similarity search\n",
    "- Anomaly detection\n",
    "\n",
    "Vector databases enable applications to understand semantic meaning rather than \n",
    "just keyword matching, leading to more intelligent v√† contextual results.\n",
    "\"\"\",\n",
    "    \n",
    "    \"embedding_models.txt\": \"\"\"\n",
    "Embedding Models cho Text Representation\n",
    "\n",
    "Embedding models convert text th√†nh dense vector representations m√† capture semantic meaning. \n",
    "Quality c·ªßa embeddings directly impacts performance c·ªßa downstream tasks nh∆∞ search v√† RAG.\n",
    "\n",
    "Types of Embedding Models:\n",
    "\n",
    "1. Sentence Transformers: Based on BERT architecture, ƒë∆∞·ª£c fine-tuned specifically \n",
    "cho sentence-level embeddings. Popular models include all-MiniLM-L6-v2, all-mpnet-base-v2.\n",
    "\n",
    "2. OpenAI Embeddings: Commercial service providing high-quality embeddings \n",
    "nh∆∞ text-embedding-ada-002. Excellent performance nh∆∞ng requires API calls.\n",
    "\n",
    "3. Instructor Models: Specialized embeddings m√† can be instructed cho specific tasks \n",
    "b·∫±ng c√°ch provide task descriptions.\n",
    "\n",
    "4. Domain-specific Models: Models ƒë∆∞·ª£c fine-tuned cho specific domains \n",
    "nh∆∞ legal, medical, ho·∫∑c scientific text.\n",
    "\n",
    "Key Considerations:\n",
    "\n",
    "Model Size vs Performance: Larger models generally provide better embeddings \n",
    "nh∆∞ng slower inference times v√† higher memory requirements.\n",
    "\n",
    "Language Support: Some models work better v·ªõi specific languages. \n",
    "Multilingual models can handle multiple languages nh∆∞ng might sacrifice some performance.\n",
    "\n",
    "Maximum Input Length: Models have limits on input text length, \n",
    "typically 512 tokens cho BERT-based models.\n",
    "\n",
    "Embedding Dimension: Higher dimensions can capture more information \n",
    "nh∆∞ng increase storage requirements v√† computation costs.\n",
    "\n",
    "Best Practices:\n",
    "- Use same embedding model cho both indexing v√† querying\n",
    "- Consider domain-specific models cho specialized content\n",
    "- Normalize embeddings cho consistent similarity calculations\n",
    "- Evaluate multiple models v·ªõi your specific use case\n",
    "- Monitor embedding quality v·ªõi human evaluation\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"üìö Created {len(sample_documents)} sample documents\")\n",
    "for filename in sample_documents.keys():\n",
    "    print(f\"   üìÑ {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o sample data directory v√† save documents\n",
    "data_dir = Path(\"rag_sample_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save documents to files\n",
    "for filename, content in sample_documents.items():\n",
    "    file_path = data_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content.strip())\n",
    "\n",
    "print(f\"‚úÖ Documents saved to {data_dir}\")\n",
    "print(f\"üìä Total files: {len(list(data_dir.glob('*.txt')))}\")\n",
    "\n",
    "# Verify files\n",
    "for file_path in data_dir.glob('*.txt'):\n",
    "    size = file_path.stat().st_size\n",
    "    print(f\"   üìÑ {file_path.name}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading v√† Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents s·ª≠ d·ª•ng DirectoryLoader\n",
    "print(\"üìÅ Loading documents...\")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    str(data_dir),\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "# Load all documents\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Inspect documents\n",
    "for i, doc in enumerate(documents):\n",
    "    filename = Path(doc.metadata['source']).name\n",
    "    content_length = len(doc.page_content)\n",
    "    word_count = len(doc.page_content.split())\n",
    "    \n",
    "    print(f\"üìÑ Document {i+1}: {filename}\")\n",
    "    print(f\"   üìè Length: {content_length:,} characters, {word_count:,} words\")\n",
    "    print(f\"   üìù Preview: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Splitting Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document characteristics ƒë·ªÉ determine optimal splitting strategy\n",
    "def analyze_documents(docs):\n",
    "    \"\"\"Analyze documents ƒë·ªÉ recommend splitting parameters\"\"\"\n",
    "    total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "    total_words = sum(len(doc.page_content.split()) for doc in docs)\n",
    "    avg_chars_per_doc = total_chars / len(docs)\n",
    "    avg_words_per_doc = total_words / len(docs)\n",
    "    \n",
    "    # Analyze paragraph structure\n",
    "    paragraphs = []\n",
    "    for doc in docs:\n",
    "        doc_paragraphs = [p.strip() for p in doc.page_content.split('\\n\\n') if p.strip()]\n",
    "        paragraphs.extend(doc_paragraphs)\n",
    "    \n",
    "    avg_paragraph_length = sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0\n",
    "    \n",
    "    print(\"üìä Document Analysis:\")\n",
    "    print(f\"   üìö Total documents: {len(docs)}\")\n",
    "    print(f\"   üìè Total characters: {total_chars:,}\")\n",
    "    print(f\"   üìù Total words: {total_words:,}\")\n",
    "    print(f\"   üìÑ Average chars per document: {avg_chars_per_doc:.0f}\")\n",
    "    print(f\"   üìñ Average words per document: {avg_words_per_doc:.0f}\")\n",
    "    print(f\"   üìã Total paragraphs: {len(paragraphs)}\")\n",
    "    print(f\"   üìê Average paragraph length: {avg_paragraph_length:.0f} characters\")\n",
    "    \n",
    "    # Recommend chunk size\n",
    "    if avg_paragraph_length > 800:\n",
    "        recommended_chunk_size = 1000\n",
    "        recommended_overlap = 100\n",
    "    elif avg_paragraph_length > 400:\n",
    "        recommended_chunk_size = 600\n",
    "        recommended_overlap = 60\n",
    "    else:\n",
    "        recommended_chunk_size = 400\n",
    "        recommended_overlap = 40\n",
    "    \n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    print(f\"   üéØ Chunk size: {recommended_chunk_size}\")\n",
    "    print(f\"   üîÑ Overlap: {recommended_overlap}\")\n",
    "    \n",
    "    return recommended_chunk_size, recommended_overlap\n",
    "\n",
    "chunk_size, overlap = analyze_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitting v·ªõi optimized parameters\n",
    "print(f\"\\n‚úÇÔ∏è Splitting documents v·ªõi chunk_size={chunk_size}, overlap={overlap}\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Created {len(splits)} chunks from {len(documents)} documents\")\n",
    "\n",
    "# Analyze splits\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in splits]\n",
    "print(f\"\\nüìä Chunk Statistics:\")\n",
    "print(f\"   üìè Average chunk size: {sum(chunk_sizes)/len(chunk_sizes):.0f} characters\")\n",
    "print(f\"   üìê Min chunk size: {min(chunk_sizes)} characters\")\n",
    "print(f\"   üìè Max chunk size: {max(chunk_sizes)} characters\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nüìã Sample Chunks:\")\n",
    "for i, chunk in enumerate(splits[:3]):\n",
    "    source_file = Path(chunk.metadata['source']).name\n",
    "    print(f\"\\nChunk {i+1} (from {source_file}):\")\n",
    "    print(f\"Length: {len(chunk.page_content)} chars\")\n",
    "    print(f\"Content: {chunk.page_content[:150]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings v√† Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings v√† vector store\n",
    "print(\"üßÆ Creating embeddings v√† building vector store...\")\n",
    "print(f\"üìä Processing {len(splits)} chunks...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Vector store created in {embedding_time:.2f} seconds\")\n",
    "print(f\"üìä Vector store info:\")\n",
    "print(f\"   üî¢ Total vectors: {vectorstore.index.ntotal}\")\n",
    "print(f\"   üìê Vector dimension: {vectorstore.index.d}\")\n",
    "print(f\"   ‚ö° Average embedding time: {embedding_time/len(splits):.3f}s per chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector store v·ªõi similarity search\n",
    "print(\"\\nüîç Testing vector store v·ªõi similarity search:\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"Types of vector databases\",\n",
    "    \"Embedding model selection\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    \n",
    "    # Similarity search\n",
    "    similar_docs = vectorstore.similarity_search(query, k=2)\n",
    "    \n",
    "    print(f\"üìö Found {len(similar_docs)} relevant chunks:\")\n",
    "    for i, doc in enumerate(similar_docs):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        content_preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"   {i+1}. {source}: {content_preview}...\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced search v·ªõi scores\n",
    "print(\"\\nüéØ Advanced similarity search v·ªõi scores:\")\n",
    "\n",
    "query = \"What are the benefits of RAG systems?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Search v·ªõi similarity scores\n",
    "docs_with_scores = vectorstore.similarity_search_with_score(query, k=4)\n",
    "\n",
    "print(f\"\\nüìä Results with similarity scores:\")\n",
    "for i, (doc, score) in enumerate(docs_with_scores):\n",
    "    source = Path(doc.metadata['source']).name\n",
    "    content_preview = doc.page_content[:120].replace('\\n', ' ')\n",
    "    \n",
    "    print(f\"\\n{i+1}. Score: {score:.4f} | Source: {source}\")\n",
    "    print(f\"   Content: {content_preview}...\")\n",
    "\n",
    "print(f\"\\nüí° Lower scores = higher similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retriever Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"üîç Retriever configured:\")\n",
    "print(f\"   Search type: similarity\")\n",
    "print(f\"   Retrieve top K: 4 chunks\")\n",
    "\n",
    "# Test retriever\n",
    "test_query = \"Explain vector databases and their use cases\"\n",
    "retrieved_docs = retriever.get_relevant_documents(test_query)\n",
    "\n",
    "print(f\"\\nüß™ Retriever test with query: '{test_query}'\")\n",
    "print(f\"üìö Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    source = Path(doc.metadata['source']).name\n",
    "    content_length = len(doc.page_content)\n",
    "    first_sentence = doc.page_content.split('.')[0] + '.'\n",
    "    \n",
    "    print(f\"\\nüìÑ Document {i+1}:\")\n",
    "    print(f\"   üìÅ Source: {source}\")\n",
    "    print(f\"   üìè Length: {content_length} characters\")\n",
    "    print(f\"   üìù First sentence: {first_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Chain with RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RetrievalQA chain\n",
    "print(\"üîó Creating RetrievalQA chain...\")\n",
    "\n",
    "# Custom prompt template for RAG\n",
    "rag_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Always provide your answer in Vietnamese and be specific about what information you're basing your answer on.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in Vietnamese:\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" all retrieved docs into prompt\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True,  # Return source docs for transparency\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RetrievalQA chain created successfully\")\n",
    "print(f\"   üß† LLM: Claude 3.5 Sonnet\")\n",
    "print(f\"   üîç Retriever: FAISS similarity search (k=4)\")\n",
    "print(f\"   üìù Chain type: stuff\")\n",
    "print(f\"   üìÑ Return sources: Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RetrievalQA chain\n",
    "def test_qa_chain(question: str, chain):\n",
    "    \"\"\"Test QA chain v√† display results\"\"\"\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = chain({\"query\": question})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nü§ñ Answer:\")\n",
    "    print(result['result'])\n",
    "    \n",
    "    print(f\"\\nüìö Source Documents ({len(result['source_documents'])}):");
    "    for i, doc in enumerate(result['source_documents']):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        content_preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"   {i+1}. {source}: {content_preview}...\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Response time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"LangChain l√† g√¨ v√† n√≥ c√≥ nh·ªØng th√†nh ph·∫ßn ch√≠nh n√†o?\",\n",
    "    \"RAG ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    result = test_qa_chain(question, qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom RAG Chain v·ªõi LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RAG chain s·ª≠ d·ª•ng LCEL cho more control\n",
    "print(\"üîß Building custom RAG chain v·ªõi LCEL...\")\n",
    "\n",
    "# Custom prompt template v·ªõi better formatting\n",
    "custom_rag_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"B·∫°n l√† m·ªôt AI assistant chuy√™n v·ªÅ LangChain v√† RAG systems. \n",
    "S·ª≠ d·ª•ng th√¥ng tin t·ª´ context d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt.\n",
    "N·∫øu th√¥ng tin kh√¥ng c√≥ trong context, h√£y n√≥i r√µ l√† b·∫°n kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë√≥.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER (in Vietnamese):\"\"\"\n",
    ")\n",
    "\n",
    "# Function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into context string\"\"\"\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        content = doc.page_content.strip()\n",
    "        formatted_docs.append(f\"Document {i+1} (from {source}):\\n{content}\")\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Custom RAG chain using LCEL\n",
    "custom_rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom LCEL RAG chain created\")\n",
    "print(\"   üîç Retriever ‚Üí Format Docs ‚Üí Prompt ‚Üí LLM ‚Üí Parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom RAG chain\n",
    "def test_custom_rag(question: str, chain, retriever):\n",
    "    \"\"\"Test custom RAG chain v·ªõi detailed output\"\"\"\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show retrieved documents first\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"\\nüîç Retrieved {len(retrieved_docs)} relevant documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        source = Path(doc.metadata['source']).name\n",
    "        print(f\"   {i+1}. {source} ({len(doc.page_content)} chars)\")\n",
    "    \n",
    "    # Generate answer\n",
    "    print(f\"\\nü§ñ Generating answer...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    answer = chain.invoke(question)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüí¨ Answer:\")\n",
    "    print(answer)\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Generation time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test custom chain\n",
    "custom_questions = [\n",
    "    \"So s√°nh c√°c lo·∫°i vector databases kh√°c nhau\",\n",
    "    \"Nh·ªØng y·∫øu t·ªë n√†o c·∫ßn consider khi ch·ªçn embedding model?\",\n",
    "    \"Agents trong LangChain ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?\"\n",
    "]\n",
    "\n",
    "for question in custom_questions[:2]:  # Test first 2 questions\n",
    "    answer = test_custom_rag(question, custom_rag_chain, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Enhanced RAG v·ªõi Multiple Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple retrievers v·ªõi different strategies\n",
    "print(\"üîç Creating multiple retrieval strategies...\")\n",
    "\n",
    "# 1. Similarity retriever (already created)\n",
    "similarity_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# 2. MMR (Maximum Marginal Relevance) retriever for diversity\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"fetch_k\": 6,  # Fetch more candidates for MMR\n",
    "        \"lambda_mult\": 0.7  # Balance relevance vs diversity\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Similarity with threshold\n",
    "threshold_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.8,  # Only return docs above threshold\n",
    "        \"k\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "retrievers = {\n",
    "    \"similarity\": similarity_retriever,\n",
    "    \"mmr\": mmr_retriever,\n",
    "    \"threshold\": threshold_retriever\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Created multiple retrieval strategies:\")\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"   üîç {name}: {retriever.search_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval strategies\n",
    "def compare_retrievers(question: str, retrievers_dict):\n",
    "    \"\"\"Compare different retrieval strategies\"\"\"\n",
    "    print(f\"\\nüîç Comparing retrievers for: '{question}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, retriever in retrievers_dict.items():\n",
    "        try:\n",
    "            docs = retriever.get_relevant_documents(question)\n",
    "            results[name] = docs\n",
    "            \n",
    "            print(f\"\\nüìö {name.upper()} Retriever ({len(docs)} docs):\")\n",
    "            for i, doc in enumerate(docs[:2]):  # Show first 2\n",
    "                source = Path(doc.metadata['source']).name\n",
    "                preview = doc.page_content[:80].replace('\\n', ' ')\n",
    "                print(f\"   {i+1}. {source}: {preview}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå {name.upper()} Retriever failed: {e}\")\n",
    "            results[name] = []\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test retriever comparison\n",
    "comparison_query = \"What are the main components of LangChain?\"\n",
    "retrieval_results = compare_retrievers(comparison_query, retrievers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RAG Evaluation v√† Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG evaluation framework\n",
    "def evaluate_rag_response(question: str, answer: str, source_docs: List[Document]):\n",
    "    \"\"\"Evaluate RAG response quality\"\"\"\n",
    "    evaluation = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"source_count\": len(source_docs),\n",
    "        \"answer_length\": len(answer),\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    # 1. Check if answer acknowledges sources\n",
    "    source_files = [Path(doc.metadata['source']).stem for doc in source_docs]\n",
    "    mentions_sources = any(file.lower() in answer.lower() for file in source_files)\n",
    "    evaluation[\"metrics\"][\"mentions_sources\"] = mentions_sources\n",
    "    \n",
    "    # 2. Check for uncertainty expressions\n",
    "    uncertainty_phrases = [\"kh√¥ng bi·∫øt\", \"kh√¥ng r√µ\", \"kh√¥ng c√≥ th√¥ng tin\", \"kh√¥ng ch·∫Øc ch·∫Øn\"]\n",
    "    expresses_uncertainty = any(phrase in answer.lower() for phrase in uncertainty_phrases)\n",
    "    evaluation[\"metrics\"][\"expresses_uncertainty_when_needed\"] = expresses_uncertainty\n",
    "    \n",
    "    # 3. Answer completeness (rough heuristic)\n",
    "    answer_completeness = \"complete\" if len(answer) > 200 else \"brief\" if len(answer) > 50 else \"very_brief\"\n",
    "    evaluation[\"metrics\"][\"answer_completeness\"] = answer_completeness\n",
    "    \n",
    "    # 4. Source diversity\n",
    "    unique_sources = len(set(Path(doc.metadata['source']).name for doc in source_docs))\n",
    "    source_diversity = unique_sources / len(source_docs) if source_docs else 0\n",
    "    evaluation[\"metrics\"][\"source_diversity\"] = source_diversity\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Evaluation test set\n",
    "eval_questions = [\n",
    "    \"LangChain c√≥ nh·ªØng th√†nh ph·∫ßn ch√≠nh n√†o?\",\n",
    "    \"∆Øu ƒëi·ªÉm c·ªßa RAG systems l√† g√¨?\",\n",
    "    \"L√†m th·∫ø n√†o ƒë·ªÉ choose embedding model?\",\n",
    "    \"TensorFlow c√≥ ph·∫£i l√† m·ªôt vector database kh√¥ng?\"  # Question not in docs\n",
    "]\n",
    "\n",
    "print(\"üìä Evaluating RAG system performance...\")\n",
    "evaluations = []\n",
    "\n",
    "for question in eval_questions:\n",
    "    print(f\"\\nüß™ Testing: {question}\")\n",
    "    \n",
    "    # Get answer using RetrievalQA\n",
    "    result = qa_chain({\"query\": question})\n",
    "    answer = result['result']\n",
    "    sources = result['source_documents']\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_result = evaluate_rag_response(question, answer, sources)\n",
    "    evaluations.append(eval_result)\n",
    "    \n",
    "    # Print evaluation\n",
    "    print(f\"   üìö Sources used: {eval_result['source_count']}\")\n",
    "    print(f\"   üìè Answer length: {eval_result['answer_length']} chars\")\n",
    "    print(f\"   üéØ Completeness: {eval_result['metrics']['answer_completeness']}\")\n",
    "    print(f\"   üìñ Source diversity: {eval_result['metrics']['source_diversity']:.2f}\")\n",
    "    print(f\"   ‚ùì Expresses uncertainty: {eval_result['metrics']['expresses_uncertainty_when_needed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate evaluation results\n",
    "print(\"\\nüìä RAG System Evaluation Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_questions = len(evaluations)\n",
    "avg_sources = sum(e['source_count'] for e in evaluations) / total_questions\n",
    "avg_answer_length = sum(e['answer_length'] for e in evaluations) / total_questions\n",
    "avg_source_diversity = sum(e['metrics']['source_diversity'] for e in evaluations) / total_questions\n",
    "\n",
    "completeness_dist = {}\n",
    "for eval_result in evaluations:\n",
    "    completeness = eval_result['metrics']['answer_completeness']\n",
    "    completeness_dist[completeness] = completeness_dist.get(completeness, 0) + 1\n",
    "\n",
    "uncertainty_count = sum(1 for e in evaluations if e['metrics']['expresses_uncertainty_when_needed'])\n",
    "\n",
    "print(f\"üìà Performance Metrics:\")\n",
    "print(f\"   üìö Average sources per answer: {avg_sources:.1f}\")\n",
    "print(f\"   üìè Average answer length: {avg_answer_length:.0f} characters\")\n",
    "print(f\"   üéØ Source diversity: {avg_source_diversity:.2f}\")\n",
    "print(f\"   ‚ùì Questions v·ªõi uncertainty: {uncertainty_count}/{total_questions}\")\n",
    "\n",
    "print(f\"\\nüìä Answer Completeness Distribution:\")\n",
    "for completeness, count in completeness_dist.items():\n",
    "    percentage = (count / total_questions) * 100\n",
    "    print(f\"   {completeness}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° System Assessment:\")\nif avg_sources >= 3 and avg_source_diversity > 0.7:\n",
    "    print(f\"   ‚úÖ Good source utilization\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è Could improve source utilization\")\n\nif uncertainty_count > 0:\n",
    "    print(f\"   ‚úÖ Appropriately expresses uncertainty\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è May not handle unknown questions well\")\n\nif avg_answer_length > 150:\n",
    "    print(f\"   ‚úÖ Provides detailed answers\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è Answers may be too brief\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production-Ready RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production RAG system with optimizations\n",
    "class ProductionRAGSystem:\n",
    "    \"\"\"Production-ready RAG system v·ªõi caching v√† error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, embeddings):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.cache = {}  # Simple in-memory cache\n",
    "        self.retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Production prompt\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"B·∫°n l√† m·ªôt AI assistant chuy√™n v·ªÅ LangChain, RAG systems, v√† AI technologies.\n",
    "            \n",
    "            INSTRUCTIONS:\n",
    "            1. S·ª≠ d·ª•ng ONLY th√¥ng tin t·ª´ context ƒë·ªÉ tr·∫£ l·ªùi\n",
    "            2. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong context, n√≥i r√µ ƒëi·ªÅu ƒë√≥\n",
    "            3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt\n",
    "            4. Cite sources khi c√≥ th·ªÉ\n",
    "            \n",
    "            CONTEXT:\n",
    "            {context}\n",
    "            \n",
    "            QUESTION: {question}\n",
    "            \n",
    "            ANSWER:\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Create chain\n",
    "        self.chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | self._format_docs,\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def _format_docs(self, docs):\n",
    "        \"\"\"Format retrieved documents\"\"\"\n",
    "        if not docs:\n",
    "            return \"No relevant documents found.\"\n",
    "        \n",
    "        formatted = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            source = Path(doc.metadata['source']).name\n",
    "            content = doc.page_content.strip()\n",
    "            formatted.append(f\"[Source {i+1}: {source}]\\n{content}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted)\n",
    "    \n",
    "    def query(self, question: str, use_cache: bool = True):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        # Check cache\n",
    "        if use_cache and question in self.cache:\n",
    "            print(\"üöÄ Cache hit!\")\n",
    "            return self.cache[question]\n",
    "        \n",
    "        try:\n",
    "            # Validate input\n",
    "            if not question or len(question.strip()) == 0:\n",
    "                return {\n",
    "                    \"answer\": \"Please provide a valid question.\",\n",
    "                    \"sources\": [],\n",
    "                    \"error\": \"Empty question\"\n",
    "                }\n",
    "            \n",
    "            # Retrieve relevant documents\n",
    "            start_time = time.time()\n",
    "            relevant_docs = self.retriever.get_relevant_documents(question)\n",
    "            retrieval_time = time.time() - start_time\n",
    "            \n",
    "            # Generate answer\n",
    "            generation_start = time.time()\n",
    "            answer = self.chain.invoke(question)\n",
    "            generation_time = time.time() - generation_start\n",
    "            \n",
    "            result = {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": [\n",
    "                    {\n",
    "                        \"filename\": Path(doc.metadata['source']).name,\n",
    "                        \"content_preview\": doc.page_content[:200] + \"...\"\n",
    "                    }\n",
    "                    for doc in relevant_docs\n",
    "                ],\n",
    "                \"metrics\": {\n",
    "                    \"retrieval_time\": retrieval_time,\n",
    "                    \"generation_time\": generation_time,\n",
    "                    \"total_time\": retrieval_time + generation_time,\n",
    "                    \"sources_count\": len(relevant_docs)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Cache result\n",
    "            if use_cache:\n",
    "                self.cache[question] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": \"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n.\",\n",
    "                \"sources\": [],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def add_documents(self, new_docs: List[Document]):\n",
    "        \"\"\"Add new documents to the knowledge base\"\"\"\n",
    "        try:\n",
    "            # Clear cache when adding new documents\n",
    "            self.cache.clear()\n",
    "            \n",
    "            # Add to vectorstore\n",
    "            self.vectorstore.add_documents(new_docs)\n",
    "            \n",
    "            return f\"Successfully added {len(new_docs)} documents\"\n",
    "        except Exception as e:\n",
    "            return f\"Error adding documents: {str(e)}\"\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get system statistics\"\"\"\n",
    "        return {\n",
    "            \"total_vectors\": self.vectorstore.index.ntotal,\n",
    "            \"vector_dimension\": self.vectorstore.index.d,\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"model_info\": {\n",
    "                \"llm\": \"Claude 3.5 Sonnet\",\n",
    "                \"embeddings\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize production system\n",
    "rag_system = ProductionRAGSystem(vectorstore, llm, embeddings)\n",
    "\n",
    "print(\"üè≠ Production RAG System initialized\")\n",
    "stats = rag_system.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production system\n",
    "def demo_production_system(system, questions):\n",
    "    \"\"\"Demo production RAG system\"\"\"\n",
    "    print(\"üöÄ Production RAG System Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nüìù Question {i}: {question}\")\n",
    "        \n",
    "        result = system.query(question)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"‚ùå Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"\\nüí¨ Answer:\")\n",
    "            print(result['answer'])\n",
    "            \n",
    "            print(f\"\\nüìä Metrics:\")\n",
    "            metrics = result['metrics']\n",
    "            print(f\"   ‚è±Ô∏è Total time: {metrics['total_time']:.2f}s\")\n",
    "            print(f\"   üîç Retrieval: {metrics['retrieval_time']:.2f}s\")\n",
    "            print(f\"   ü§ñ Generation: {metrics['generation_time']:.2f}s\")\n",
    "            print(f\"   üìö Sources: {metrics['sources_count']}\")\n",
    "            \n",
    "            print(f\"\\nüìÅ Sources used:\")\n",
    "            for j, source in enumerate(result['sources']):\n",
    "                print(f\"   {j+1}. {source['filename']}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Demo questions\n",
    "demo_questions = [\n",
    "    \"LangChain agents kh√°c v·ªõi chains nh∆∞ th·∫ø n√†o?\",\n",
    "    \"L√†m th·∫ø n√†o ƒë·ªÉ optimize RAG system performance?\",\n",
    "    \"\"  # Test empty question\n",
    "]\n",
    "\n",
    "demo_production_system(rag_system, demo_questions[:2])  # Skip empty question for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup v√† Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore for future use\n",
    "vectorstore_path = \"./rag_vectorstore\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"üíæ Vectorstore saved to {vectorstore_path}\")\n",
    "\n",
    "# Cleanup sample data\n",
    "import shutil\n",
    "try:\n",
    "    if data_dir.exists():\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"üßπ Cleaned up {data_dir}\")\nexcept Exception as e:\n",
    "    print(f\"Note: Could not cleanup {data_dir}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ RAG system setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "def rag_system_summary():\n",
    "    print(\"üéâ RAG with Claude - Complete Tutorial Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary = {\n",
    "        \"üèóÔ∏è Components Built\": [\n",
    "            \"Document loading v√† preprocessing system\",\n",
    "            \"Intelligent text splitting v·ªõi analysis\",\n",
    "            \"FAISS vector store v·ªõi HuggingFace embeddings\",\n",
    "            \"Multiple retrieval strategies (similarity, MMR, threshold)\",\n",
    "            \"RetrievalQA chain v·ªõi Claude 3.5 Sonnet\",\n",
    "            \"Custom LCEL RAG pipeline\",\n",
    "            \"Production-ready system v·ªõi caching\"\n",
    "        ],\n",
    "        \"üìä Key Features\": [\n",
    "            \"Source attribution v√† transparency\",\n",
    "            \"Error handling v√† graceful degradation\",\n",
    "            \"Performance monitoring v√† metrics\",\n",
    "            \"Caching for improved response times\",\n",
    "            \"Evaluation framework for quality assessment\",\n",
    "            \"Extensible architecture for new documents\"\n",
    "        ],\n",
    "        \"üí° Best Practices Implemented\": [\n",
    "            \"Chunk size optimization based on content analysis\",\n",
    "            \"Multiple retrieval strategies for robustness\",\n",
    "            \"Comprehensive prompt engineering\",\n",
    "            \"Production-ready error handling\",\n",
    "            \"Performance benchmarking v√† optimization\",\n",
    "            \"Quality evaluation framework\"\n",
    "        ],\n",
    "        \"üöÄ Production Considerations\": [\n",
    "            \"Vectorstore persistence for deployment\",\n",
    "            \"Caching strategy for performance\",\n",
    "            \"Error handling for reliability\",\n",
    "            \"Metrics collection for monitoring\",\n",
    "            \"Extensible design for new content\",\n",
    "            \"Source tracking for accountability\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for section, items in summary.items():\n",
    "        print(f\"\\n{section}:\")\n",
    "        for item in items:\n",
    "            print(f\"   ‚úÖ {item}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Next Steps:\")\n",
    "    next_steps = [\n",
    "        \"Deploy RAG system to production environment\",\n",
    "        \"Integrate v·ªõi web interface or API\",\n",
    "        \"Add more sophisticated evaluation metrics\",\n",
    "        \"Implement user feedback collection\",\n",
    "        \"Scale v·ªõi larger document collections\",\n",
    "        \"Add real-time document updates\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"   üöÄ {step}\")\n",
    "\n",
    "rag_system_summary()\n",
    "\n",
    "print(f\"\\nüèÜ You've successfully built a complete RAG system v·ªõi Claude!\")\n",
    "print(f\"üí™ Ready for production deployment v√† real-world applications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}