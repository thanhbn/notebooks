# 2407.15462v4.pdf
# Converted from PDF to TXT
# Source path: D:\llm\notebooks\AI-Papers\2407.15462v4.pdf
# File size: 1515870 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Retrieval with Learned Similarities
Bailu Dingâˆ—
badin@microsoft.com
Microsoft Research
Redmond, Washington, USAJiaqi Zhaiâˆ—
jiaqi@jiaqizhai.com
Meta
Bellevue, Washington, USA
Abstract
Retrieval plays a fundamental role in recommendation systems,
search, and natural language processing (NLP) by efficiently find-
ing relevant items from a large corpus given a query. Dot products
have been widely used as the similarity function in such tasks, en-
abled by Maximum Inner Product Search (MIPS) algorithms for effi-
cient retrieval. However, state-of-the-art retrieval algorithms have
migrated to learned similarities. These advanced approaches encom-
pass multiple query embeddings, complex neural networks, direct
item ID decoding via beam search, and hybrid solutions. Unfortu-
nately, we lack efficient solutions for retrieval in these state-of-the-
art setups. Our work addresses this gap by investigating efficient
retrieval techniques with expressive learned similarity functions.
We establish Mixture-of-Logits (MoL) as a universal approximator
of similarity functions, demonstrate that MoLâ€™s expressiveness can
be realized empirically to achieve superior performance on diverse
retrieval scenarios, and propose techniques to retrieve the approx-
imate top-ğ‘˜results using MoL with tight error bounds. Through
extensive experimentation, we show that MoL, enhanced by our
proposed mutual information-based load balancing loss, sets new
state-of-the-art results across heterogeneous scenarios, including
sequential retrieval models in recommendation systems and finetun-
ing language models for question answering; and our approximate
top-ğ‘˜algorithms outperform baselines by up to 66Ã—in latency while
achieving >.99recall rate compared to exact algorithms.1
CCS Concepts
â€¢Information systems â†’Similarity measures ;Top-k retrieval
in databases ;Learning to rank ;Probabilistic retrieval mod-
els;Question answering ;Recommender systems ;Personalization ;â€¢
Computing methodologies â†’Natural language processing .
Keywords
Nearest Neighbor Search, Learned Similarities, Top-K Retrieval,
Vector Databases, Recommendation Systems, Question Answering
ACM Reference Format:
Bailu Ding and Jiaqi Zhai. 2025. Retrieval with Learned Similarities. In
Proceedings of the ACM Web Conference 2025 (WWW â€™25), April 28-May 2,
2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3696410.3714822
âˆ—Equal contribution.
1Our code and model checkpoints are available at https://github.com/bailuding/rails.
This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Interna-
tional License.
WWW â€™25, Sydney, NSW, Australia
Â©2025 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-1274-6/25/04
https://doi.org/10.1145/3696410.37148221 Introduction
Retrieval requires efficient storing, indexing, and querying relevant
candidate items represented by high-dimensional vectors. Retrieval
is widely used as the initial preprocessing stage for internet appli-
cations such as recommendations, search, question answering, and
natural language processing that operate over corpus with up to
billions of items [ 5,10,16,28,33,35]. In many concrete use cases,
such as vector databases [ 26], the query- and the item- embeddings
are learned with deep neural networks in a dual-encoder setup,
and dot products are applied on top of such embeddings as the
similarity function for measuring relevance.
Despite the popularity of dot products and numerous work done
to improve their efficiency [ 9,25,37,51], state-of-the-art retrieval
algorithms have long moved to various learned similarity func-
tions. Their most basic versions preserve some dot product-related
structures, but turn either the query or the item into multiple em-
beddings, and rely on a max operator to combine those similar-
ity values [ 29,35]. As another example, Probabilistic Label Trees
(PLTs) [ 23] and Tree-based Deep Models (TDMs) [ 62,64] map
items to leaf nodes in a tree, and reduce retrieval to beam search
by making decisions sequentially using learned classifiers while
traversing trees from root to leaf. More recent work on generative
retrieval directly map the query to the item ids in sequence-to-
sequence or decoder-only setups [ 4,11,53,55,57]. Combinations
of these approaches have also been studied, with some performing
coarse-grained retrieval with generative approaches, followed by
re-ranking using dot products [ 15]. Finally, the similarity function
can be directly parameterized by carefully designed deep neural
networks that take various forms [21, 48, 58, 59].
Supporting efficient retrieval with these diverse learned simi-
larities is challenging. Learned similarity functions are generally
expensive to compute; with learned index structures, traversing a
binary tree with 4 million items requires running beam search for
20 non-parallelizable steps [ 62], while recommendation and NLP
deployments commonly need to handle billions of items [ 6,13,35]
with a latency budget of tens of milliseconds. When an arbitrary
deep neural network is employed, itâ€™s no longer clear how to per-
form top-ğ¾retrieval other than through brute-force [ 21] or heuris-
tics [ 59]. While graph-based methods can be used to prune the
search space [ 24,37,43,56], such methods tend to be much slower
compared with MIPS algorithms leveraging quantization at high
recall rates [ 1,19], and their performance can degrade when the
similarity function is not a distance metric [ 39]. What is worse,
these algorithms vary significantly in terms of their exact formu-
lations, and the lack of a universal interface makes it even more
difficult to design a general solution for efficient retrieval.
Taking a step back, our key insight is that learned similarity
approaches are but different ways to increase the expressiveness ofarXiv:2407.15462v4  [cs.IR]  25 Jan 2025

--- PAGE 2 ---
WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia Bailu Ding and Jiaqi Zhai
Notation Description
ğ‘(ğ‘„,|ğ‘„|) query (set of queries, number of queries)
ğ‘¥(ğ‘‹,|ğ‘‹|) item (set of items, number of items)
ğœ™(ğ‘,ğ‘¥) the learned similarity function, i.e., Mixture-of-Logits (MoL).
ğ‘ƒ(ğ‘ƒğ‘,ğ‘ƒğ‘¥)MoL usesğ‘ƒpairs of low-rank embeddings ("component-level embeddings") to represent ğ‘andğ‘¥. With the (batched)
outer product form of MoL, ğ‘ƒğ‘andğ‘ƒğ‘¥are the numbers of embeddings for ğ‘andğ‘¥, respectively; ğ‘ƒ=ğ‘ƒğ‘Ã—ğ‘ƒğ‘¥.
ğœ‹ğ‘(ğ‘,ğ‘¥)(ğœ‹ğ‘ğ‘,ğ‘ğ‘¥(ğ‘,ğ‘¥))weight for the ğ‘-th (orğ‘ğ‘-th byğ‘ğ‘¥-th with outer product) embedding set for (ğ‘,ğ‘¥).
ğ‘“(ğ‘)(ğ‘“ğ‘(ğ‘)) learned embedding for the query ( ğ‘-th component-level query embedding)
ğ‘”(ğ‘¥)(ğ‘”ğ‘(ğ‘¥)) learned embedding for the item ( ğ‘-th component-level item embedding)
ğ‘‘ğ‘ƒ dimensionality of low-rank (component-level) embeddings. ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘)âˆˆRğ‘‘ğ‘ƒ.
âŸ¨ğ‘“(ğ‘),ğ‘”(ğ‘¥)âŸ© the dot product similarity function: ğ‘”(ğ‘¥)ğ‘‡ğ‘“(ğ‘).âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©denotes the dot product for the ğ‘ğ‘¡â„embedding pair.
Table 1: Table of Notations.
the retrieval stage. Formally, for a query ğ‘and an item ğ‘¥, the expres-
siveness of the similarity function boils down to deriving alternative
parameterizations of ğ‘(ğ‘¥|ğ‘)matrices, with full rank matrices being
the most expressive among them. Dot products, on the other hand,
induces a low-rank bottleneck due to the dimensionality of the em-
bedding, i.e., lnğ‘(ğ‘¥|ğ‘)âˆâŸ¨ğ‘“(ğ‘),ğ‘”(ğ‘¥)âŸ©(ğ‘“(ğ‘),ğ‘”(ğ‘¥)âˆˆRğ‘‘). This can-
not be alleviated by simply increasing the embedding dimension ğ‘‘,
due to memory bandwidth being the main bottleneck in modern dot-
product based retrieval systems, such as vector databases [ 9,26,59],
and overfitting issues that come with larger embedding dimensions
due to the common need to co-train or finetune query- and item-
encoders from data [10, 15, 28, 35, 40, 41, 60].
This insight enables us to support efficient retrieval with ex-
pressive learned similarity functions by approximating them with
Mixture-of-Logits (MoL). To the best of our knowledge, this is the
first work that tackles the problem of efficient retrieval with univer-
sal learned similarities, while setting new state-of-the-art results
across heterogeneous scenarios. We first show that Mixture-of-Logits
is a universal approximator as it can express ğ‘(ğ‘¥|ğ‘)matrices of
arbitrary high rank, and hence approximate alllearned similarity
functions (Section 2.1). Our work lays theoretical foundations for
MoLâ€™s empirical impressive performance gains of 20%-30% across
Hit Rate@50-400 on web-scale corpus with hundreds of millions to
billions of items [ 6,59], and further enables MoL to be effectively
applied across diverse retrieval scenarios, from large-scale recom-
mendation systems to finetuning language models for question
answering (Section 2.2). We next propose techniques to retrieve
the approximate top- ğ¾results using MoL with a tight error bound
(Section 3). Our solution leverages the existing widely used APIs
of vector databases like top-K queries, thus benefiting from prior
work on efficient vector search like MIPS [ 19,25,26,51]. We empir-
ically compare our techniques with existing approaches, showing
that MoL sets new state-of-the-art results on recommendation re-
trieval and question answering tasks, and our approximate top-k
retrieval with learned similarities outperforms baselines by up to
66Ã—in latency, while achieving >.99recall rate of exact algorithms
(Section 4). Importantly, our approach with learned similarities effi-
ciently utilizes modern accelerators due to MoLâ€™s higher arithmetic
intensity [ 59], which results in MIPS-level inference latency and
throughput. Overall, our work provides strong theoretical and prac-
tical justifications to migrate away from the broadly adopted MIPS
solution in vector databases to Retriev al with Learned Similarities
(RAILS) on GPUs.2 Mixture of Logits
In this section, we describe Mixture of Logits (MoL), propose a load
balancing loss to improve conditional computations in MoL, prove
that MoL is expressive enough to represent any learned similarity
function, and demonstrate how to apply MoL to diverse retrieval
tasks. Table 1 summarizes the notations in this paper.
We first describe Mixture of Logits (MoL).
Mixture of Logits (MoL). MoL [ 59] assumes that the query ğ‘
and the item ğ‘¥are already mapped to ğ‘ƒpairs of low-rank embed-
dings (â€œcomponent-level embeddingsâ€), ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âˆˆRğ‘‘ğ‘ƒ, where
ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)are parameterized with some neural networks based
on query and item features, respectively, and ğ‘‘ğ‘ƒis the dimensional-
ity of the low-rank embeddings. MoL then calculates the similarity
between the query ğ‘and the item ğ‘¥by applying adaptive gating
weights,ğœ‹ğ‘(ğ‘,ğ‘¥)âˆˆ[ 0,1], to the inner products of these ğ‘ƒpairs
of low-rank embeddings, or âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©s. Note that prior work
assumes thatâˆ‘ï¸
ğ‘ğœ‹ğ‘(ğ‘,ğ‘¥)=1[6,59], but this does not affect our
analyses in this paper. Following [59]:
ğœ™(ğ‘,ğ‘¥)=ğ‘ƒâˆ‘ï¸‚
ğ‘=1ğœ‹ğ‘(ğ‘,ğ‘¥)âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ© (1)
To extend this to large-scale datasets and to enable hardware-
efficient implementations on accelerators like GPUs, Equation 1
was further modified by decomposing those ğ‘ƒdot products as
(batched) outer products of ğ‘ƒğ‘query-side and ğ‘ƒğ‘¥item-side embed-
dings, where ğ‘ƒğ‘Ã—ğ‘ƒğ‘¥=ğ‘ƒ, and applying l2-norm to the embeddings:
ğœ™(ğ‘,ğ‘¥)=ğ‘ƒğ‘âˆ‘ï¸‚
ğ‘ğ‘=1ğ‘ƒğ‘¥âˆ‘ï¸‚
ğ‘ğ‘¥=1ğœ‹ğ‘ğ‘,ğ‘ğ‘¥(ğ‘,ğ‘¥)âŸ¨ï¸„
ğ‘“ğ‘ğ‘(ğ‘)
||ğ‘“ğ‘ğ‘(ğ‘)||2,ğ‘”ğ‘ğ‘¥(ğ‘¥)
||ğ‘”ğ‘ğ‘¥(ğ‘¥)||2âŸ©ï¸„
(2)
We use Equation 1 and 2 interchangeably as the MoL form to ana-
lyze throughout the rest of this paper, given that the embedding
normalization for ğ‘“ğ‘ğ‘(ğ‘)s andğ‘”ğ‘ğ‘¥(ğ‘¥)s can be precomputed.
Mixture of Logits (MoL) with load balancing regularization loss.
We further observe ğœ‹ğ‘(ğ‘,ğ‘¥)defines conditional computation to be
performed over the ğ‘low-rank embedding pairs, or (ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥))s.
ğœ‹ğ‘(ğ‘,ğ‘¥)should hence satisfy two conditions:
â€¢Globally, the ğ‘low-rank embedding pairs, or (ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥))s,
should receive a similar number of training examples even when
ğ‘is large and ğœ‹ğ‘(ğ‘,ğ‘¥)is sparse, with load distributed evenly
across theğ‘pairs. One way to do this is to maximize the entropy
ğ»(ğ‘)over these embedding pairs.

--- PAGE 3 ---
Retrieval with Learned Similarities WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia
...
...Embedding f pq(q)Embedding f1(q)
PqÂ Ã— Px logits
(outer products)Ï€pq,px(q, x)
(query- & item-
dependent weights)Â·Ï†(q, x) (" Mixture-of-Logits")
Embedding g px(x)Embedding g1(x)Query
encoder
ItemÂ 
encoderQuery
features (q)
Item
features (x)
Figure 1: Mixture-of-logits (MoL) learned similarity.
â€¢The low-rank embedding pairs used to compute a particular
ğœ™(ğ‘,ğ‘¥)should be non-uniform and ideally sparse; e.g., itâ€™s desir-
able to avoid the degenerate solution where ğœ‹ğ‘(ğ‘,ğ‘¥)=1
ğ‘. One
way to do this is to minimize the conditional entropy ğ»(ğ‘|(ğ‘,ğ‘¥))
ofğ‘given (query, item) pairs.
Given these two desired conditions, we propose a mutual information-
based regularization loss for load balancing, defined as
Lğ‘€ğ¼=âˆ’ğ»(ğ‘)+ğ»(ğ‘|(ğ‘,ğ‘¥)) (3)
with the overall training loss as
âˆ’logexp(ğœ™(ğ‘,ğ‘¥))
exp(ğœ™(ğ‘,ğ‘¥))+âˆ‘ï¸
ğ‘¥â€²âˆˆXexp(ğœ™(ğ‘,ğ‘¥â€²))+ğ›¼Lğ‘€ğ¼ (4)
where the first part of Equation 4 is the sampled softmax loss used
in [59], and the second part adjusts the weight for the mutual
information-based load balancing loss with a hyperparameter ğ›¼.
2.1 Expressiveness of Mixture of Logits
Now we show that any high-rank matrix can be decomposed into a
mixture of logits based on low-rank matrices, i.e., MoL is a universal
approximator for all similarity functions. Without loss of generality,
we prove the following:
Theorem 1. MoL decomposition : Letğ´be a matrix of ğ‘›Ã—ğ‘š,
whereğ‘›â‰¤ğ‘š. There exists ğœ‹1,ğµ1,ğœ‹2,ğµ2,Â·Â·Â·,ğœ‹ğ‘,ğµğ‘such that|ğ´âˆ’âˆ‘ï¸ğ‘ƒ
ğ‘=1ğœ‹ğ‘â—¦ğµğ‘–|<ğœ–, whereğœ–is a small positive number. Here ğµğ‘–is a
matrix ofğ‘›Ã—ğ‘šwith rank equal to or less than ğ‘‘, andğœ‹1,ğœ‹2,Â·Â·Â·,ğœ‹ğ‘ƒ
areğ‘›Ã—ğ‘šmatrices that together define a probability distribution over
each(ğ‘–,ğ‘—)tuple, such thatâˆ‘ï¸ğ‘ƒ
ğ‘=1ğœ‹ğ‘(ğ‘–,ğ‘—)=1,0â‰¤ğœ‹ğ‘(ğ‘–,ğ‘—)â‰¤1for
any1â‰¤ğ‘–â‰¤ğ‘›,1â‰¤ğ‘—â‰¤ğ‘š,1â‰¤ğ‘â‰¤ğ‘ƒ.
We can think about ğ‘›as the number of queries and ğ‘šthe number
of items (or vice versa). First, the theorem trivially holds if the rank
ofğ´is less than or equal to ğ‘‘(ğ‘‘â‰¤ğ‘›):
Lemma 1. MoL decomposition when ğ‘…ğ‘ğ‘›ğ‘˜(ğ´)â‰¤ğ‘‘: Letğ´be
a matrix as defined in Theorem 1. If the rank of ğ´is less than or
equal toğ‘‘, then we have ğ´=ğœ‹â—¦ğ´, whereğœ‹(ğ‘–,ğ‘—)=1for any
1â‰¤ğ‘–â‰¤ğ‘›,1â‰¤ğ‘—â‰¤ğ‘š.
Then we prove for the case where the rank of ğ´is greater than
ğ‘‘. Without loss of generality, we prove the case where the matrix
has full rank, i.e., ğ‘…ğ‘ğ‘›ğ‘˜(ğ´)=ğ‘›:
Lemma 2. MoL decomposition when ğ‘…ğ‘ğ‘›ğ‘˜(ğ´)=ğ‘›: Letğ´be a
matrix as defined in Theorem 1. Then there exists ğœ‹,ğµ 1,ğµ2such that
|ğ´âˆ’(ğœ‹â—¦ğµ1+(1âˆ’ğœ‹)â—¦ğµ2)|<ğœ–, whereğ‘…ğ‘ğ‘›ğ‘˜(ğµ1)â‰¤ğ‘‘,ğ‘…ğ‘ğ‘›ğ‘˜(ğµ2)â‰¤ğ‘‘,
and0â‰¤ğœ‹(ğ‘–,ğ‘—)â‰¤1for1â‰¤ğ‘–â‰¤ğ‘›,1â‰¤ğ‘—â‰¤ğ‘š.Proof. Becauseğ´is a matrix of rank ğ‘›, it can be rewritten
asğ´=ğ‘ˆğ¼ğ‘›ğ‘‰, whereğ¼ğ‘›is an identity matrix with rank ğ‘›. Thus,
ğ´ğ‘–ğ‘—=âˆ‘ï¸ğ‘›
ğ‘˜=1ğ‘ˆğ‘–ğ‘˜ğ‘‰ğ‘˜ğ‘—,1â‰¤ğ‘–â‰¤ğ‘›,1â‰¤ğ‘—â‰¤ğ‘š. Letğ´â€²be a matrix of
ğ‘›Ã—ğ‘š, whereğ´â€²
ğ‘–ğ‘—=ğœ†ğ‘–ğ‘—Â·âˆ‘ï¸ğ‘‘
ğ‘˜=1ğ‘ˆğ‘–ğ‘˜ğ‘‰ğ‘˜ğ‘—for1â‰¤ğ‘–â‰¤ğ‘›,1â‰¤ğ‘—â‰¤ğ‘š.
Here,ğœ†ğ‘–ğ‘—=1+âˆ‘ï¸ğ‘›
ğ‘˜=ğ‘‘+1ğ‘ˆğ‘–ğ‘˜ğ‘‰ğ‘˜ğ‘—âˆ‘ï¸ğ‘‘
ğ‘˜=1ğ‘ˆğ‘–ğ‘˜ğ‘‰ğ‘˜ğ‘—ifâˆ‘ï¸ğ‘‘
ğ‘˜=1ğ‘ˆğ‘–ğ‘˜ğ‘‰ğ‘˜ğ‘—â‰ 0, otherwise ğœ†ğ‘–ğ‘—=
1+âˆ‘ï¸ğ‘›
ğ‘˜=ğ‘‘+1ğ‘ˆğ‘–ğ‘˜ğ‘‰ğ‘˜ğ‘—
ğœ–. Thus, we have|ğ´âˆ’ğ´â€²|â‰¤ğœ–.
Letğœ†ğ‘šğ‘–ğ‘›=minğœ†ğ‘–ğ‘—, andğœ†ğ‘šğ‘ğ‘¥=maxğœ†ğ‘–ğ‘—. Letğµ1=ğœ†ğ‘šğ‘–ğ‘›ğ‘ˆğ·ğ‘›,ğ‘‘ğ‘‰,
ğµ2=ğœ†ğ‘šğ‘ğ‘¥ğ‘ˆğ·ğ‘›,ğ‘‘ğ‘‰, whereğ·ğ‘›,ğ‘‘denotes anğ‘›-by-ğ‘›diagonal matrix
with the first ğ‘‘elements of the diagonal being 1s and the rest being
0s. We haveğ´â€²
ğ‘–ğ‘—=ğœ†ğ‘–ğ‘—âˆ‘ï¸ğ‘‘
ğ‘˜=1ğ‘ˆğ‘–ğ‘˜ğ‘‰ğ‘˜ğ‘—=ğœ‹(ğ‘–,ğ‘—)Â·ğµ1ğ‘–ğ‘—+(1âˆ’ğœ‹(ğ‘–,ğ‘—))Â·ğµ2ğ‘–ğ‘—,
whereğœ‹(ğ‘–,ğ‘—)=ğœ†ğ‘šğ‘ğ‘¥âˆ’ğœ†ğ‘–ğ‘—
ğœ†ğ‘šğ‘ğ‘¥âˆ’ğœ†ğ‘šğ‘–ğ‘›. Becauseğœ†ğ‘šğ‘–ğ‘›â‰¤ğœ†ğ‘–ğ‘—â‰¤ğœ†ğ‘šğ‘ğ‘¥, we have
0â‰¤ğœ‹(ğ‘–,ğ‘—)â‰¤1.
Thus, we have constructed ğµ1,ğµ2,ğœ‹such that|ğ´âˆ’(ğœ‹â—¦ğµ1+(1âˆ’
ğœ‹)â—¦ğµ2)|=|ğ´âˆ’ğ´â€²|â‰¤ğœ–. â–¡
Remark Here, we have shown that any high-rank matrix can be
expressed as a mixture of logits of two low-rank matrices. Note
that our decomposition is not intended to be used as a distillation
of the original high-rank matrix. It is likely prohibitively expensive
to populate the full matrix with a learned similarity function. In
addition, our proof also does not indicate that having two mixture
components is sufficient to train the embeddings and the learned
similarity function. It is well-known that overparameterization is
often necessary to enable efficient and performant training.
2.2 Applying MoL to Heterogeneous Use Cases
We now discuss how to apply MoL to retrieval tasks in different
domains. Parameterization of the low-rank, component-level em-
beddings, or ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âˆˆRğ‘‘ğ‘ƒ, plays an important role in realiz-
ing MoLâ€™s theoretical expressiveness in practice, as suggested by
prior work [ 6]. We discuss two scenarios on the opposite end of the
spectrum, one with a large number of heterogeneous features â€“ re-
trieval in large-scale recommendation systems, followed by another
with a single homogeneous feature â€“ finetuning language models for
question answering and related NLP use cases, shown in Figure 2.
Retrieval in Large-scale Recommendation Systems. Recommen-
dation systems are characterized by the large number of heteroge-
neous features they use [ 10,52,60]. This naturally enables some of
those features to be utilized on the query- (user-) or on the item-side.
For instance, embeddings can be constructed based on cluster ids on
both the query-side and the item-side [ 6]. For common benchmark

--- PAGE 4 ---
WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia Bailu Ding and Jiaqi Zhai
X1 X2 ... XPX SP1 SP2 SP3 ... SPNItem Encoder: Finetuned Language Model (e.g., T5)Parameterized Pooling: (D,) -> (max_seq_len, PX)f1(x) f2(x) ... fPX(x)
Sequential Encoder
(RNNs/T ransformers)
Î¦1 Î¦2 Î¦3 ... Î¦Ng1(q) g2(q) ... gPQ(q)
Query MLPSide information, e.g.,
user embeddings
Rich, h eterogeneous features (Recommendations) Single, homogeneous feature (Language Models )
Figure 2: Illustration of how to apply Mixture-of-logits (MoL) learned similarity to various retrieval scenarios, with a language
model finetuning use case (characterized by a single homogeneous feature) shown on the left, and a recommendation use case
(characterized by a large number of heterogeneous features) shown on the right. More details can be found in Appendix A.2.
datasets, User ID-based one-hot embeddings [ 30] represent another
possibleğ‘”ğ‘(ğ‘)to use, which we evaluate in Section 4.
Finetuning Language Models for Question Answering. In contrast,
language models are characterized by their use of homogeneous
semantic features, such as wordpieces and sentencepieces [ 31]. We
observe that MoL can be similarly adopted for those use cases. To
obtain theğ‘ƒğ‘‹item embeddings for MoL, we expand the tokenizerâ€™s
vocabulary with ğ‘ƒğ‘‹special aggregation tokens ğ‘‹1,...,ğ‘‹ğ‘ƒğ‘‹, and
append those ğ‘ƒğ‘‹tokens at the beginning of every tokenized se-
quence,ğ‘†ğ‘ƒ1,...,ğ‘†ğ‘ƒğ‘, as illustrated in Figure 21. Theseğ‘ƒğ‘‹special
tokens play similar roles as the CLS token in BERT [ 12], and dur-
ing finetuning of the language model, are co-trained to aggregate
different aspects of information as inputs for MoL. Additionally, we
can design a learned pooling function to adapt pooling policy at an
example-level (â€œParameterized Poolingâ€) to improve model quality,
which we discuss further in Appendix A.2.
3 Retrieval Algorithms
In this section, we describe the problem of retrieving the top ğ¾items
with MoL as well as exact and approximate retrieval algorithms.
Formally, we define the top ğ¾retrieval problem as follows:
Definition 1. Topğ¾with MoL : Letğ‘be a query and ğ‘‹be a
set of items, where both the query ğ‘and each item ğ‘¥âˆˆğ‘‹are asso-
ciated withğ‘ƒembeddings. Together we have ğ‘ƒpairs of embeddings,
(ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)),1â‰¤ğ‘â‰¤ğ‘ƒ. Letğœ™(ğ‘,ğ‘¥)=âˆ‘ï¸ğ‘ƒ
ğ‘=1ğœ‹ğ‘(ğ‘,ğ‘¥)âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©
be the similarity score of ğ‘,ğ‘¥, whereğ‘¥âˆˆğ‘‹. The topğ¾query with
MoL returns the ğ¾items fromğ‘‹with the highest ğœ™(ğ‘,ğ‘¥)s.
For approximate top ğ¾retrieval with MoL, we define the gap of
the approximate and exact top ğ¾results as follows:
Definition 2. Gap of approximate top ğ¾:Letğ‘be a query and
ğ‘‹ğ¾be the set of exact top ğ¾items for the query ğ‘from a set of items
ğ‘‹. Letğ‘‹âˆ—be the set of approximate top ğ¾results, where ğ‘‹âˆ—âŠ†ğ‘‹. Let
ğ‘†=min{ğœ™(ğ‘,ğ‘¥),ğ‘¥âˆˆğ‘‹âˆ—}andğ‘†â€²=max{ğœ™(ğ‘,ğ‘¥),ğ‘¥âˆˆğ‘‹ğ¾\ğ‘‹âˆ—}. We
callğ‘†Î”=ğ‘†â€²âˆ’ğ‘†thegapof the topğ¾withğ‘‹âˆ—.
3.1 Exact algorithm
The brute-force algorithm to retrieve the exact top ğ¾with MoL is to
evaluateğœ™(ğ‘,ğ‘¥)for each query ğ‘and itemğ‘¥. This algorithm can be
1Note that many question answering scenarios [ 11,28,41,53,57] utilize bidirectional
language models for retrieval, like BERT [ 12] or T5 [ 44]; for recent unidirectional
language models, we can add ğ‘‹1,...,ğ‘‹ğ‘ƒğ‘‹to the end of the input sequence instead.prohibitively expensive if the number of items is large. Instead, we
describe a more efficient two-pass algorithm to retrieve the exact
topğ¾items as shown in Algorithm 1 (example in Appendix B).
Algorithm 1 Exact topğ¾algorithm.
Input: queryğ‘, a set of items ğ‘‹,ğ‘“ğ‘(Â·),ğ‘”ğ‘(Â·)for constructing the
component-level embeddings ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)
Output: exact topğ¾items
1:ğºâ†âˆ…
2:forğ‘âˆˆğ‘ƒdo
3:ğ‘‹ğ‘â†{ğ‘”ğ‘(ğ‘¥),ğ‘¥âˆˆğ‘‹} âŠ²Can be preprocessed.
4:ğºâ†ğºâˆªğ‘‡ğ‘œğ‘ğ¾ğ·ğ‘œğ‘¡ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ (ğ‘“ğ‘(ğ‘),ğ‘‹ğ‘)âŠ²Retrieve top ğ¾items
for each pair of embeddings.
5:ğ‘†ğ‘šğ‘–ğ‘›â†âˆ
6:forğ‘¥âˆˆğºdo
7:ğ‘ â†ğ‘€ğ‘œğ¿(ğ‘,ğ‘¥)
8: ifğ‘ <ğ‘†ğ‘šğ‘–ğ‘›thenğ‘†ğ‘šğ‘–ğ‘›â†ğ‘ 
9:ğºâ€²â†âˆ…
10:forğ‘âˆˆğ‘ƒdo
11:ğºâ€²â†ğºâ€²âˆªğ‘…ğ‘ğ‘›ğ‘”ğ‘’ğ·ğ‘œğ‘¡ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ (ğ‘“ğ‘(ğ‘),ğ‘†ğ‘šğ‘–ğ‘›,ğ‘‹ğ‘)âŠ²Retrieve all
itemsğ‘¥âˆˆğ‘‹ğ‘ƒwithâŸ¨ğ‘“ğ‘(ğ‘),ğ‘¥âŸ©â‰¥ğ‘†ğ‘šğ‘–ğ‘›.
12:returnğµğ‘Ÿğ‘¢ğ‘¡ğ‘’ğ¹ğ‘œğ‘Ÿğ‘ğ‘’ğ‘‡ğ‘œğ‘ğ¾ğ‘€ğ‘œğ¿ (ğ‘,ğºâ€²) âŠ²Retrieve the top ğ¾items
fromğºâ€²with MoL.
We start by retrieving the top ğ¾items with the highest dot
product scores for each group of embeddings as the initial candidate
setğº(line 1-4). Then we evaluate the MoL scores of the items in ğº
and find the minimal MoL score ğ‘†ğ‘šğ‘–ğ‘›(line 5-8). Next we retrieve all
items within a distance of ğ‘†ğ‘šğ‘–ğ‘›with the query ğ‘as the candidate
setğºâ€²(line 9-11). Finally, we evaluate the MoL scores of the items
inğºâ€², and return the top ğ¾items with the highest scores (line 12).
We argue that Algorithm 1 retrieves the exact top ğ¾items with
MoL. Letğ‘‹ğ¾be the set of the exact top ğ¾items andğ‘‹â€²be the result
of Algorithm 1. Let ğ‘¥âˆˆğ‘‹ğ¾andğœ™(ğ‘,ğ‘¥)be the MoL score ofğ‘¥and
ğ‘. Sinceğ‘¥has the highest top ğ¾score with MoL,ğœ™(ğ‘,ğ‘¥)â‰¥ğ‘†ğ‘šğ‘–ğ‘›.
Since the MoL score is a weighted score over the dot product scores,
we have max{âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©,1â‰¤ğ‘â‰¤ğ‘ƒ}â‰¥ğœ™(ğ‘,ğ‘¥)â‰¥ğ‘†ğ‘šğ‘–ğ‘›. Since
Algorithm 1 retrieves all the items with a dot product higher than
or equal toğ‘†ğ‘šğ‘–ğ‘›ofğ‘for each embedding ğ‘ğ‘(line 9-11), we have
ğ‘¥âˆˆğºâ€². Thus,ğ‘¥âˆˆğ‘‹â€². So we have shown that ğ‘‹ğ¾=ğ‘‹â€².
3.2 Approximate algorithms
In the exact algorithm shown in Algorithm 1, we need to retrieve
all the items with a dot product higher than or equal to a threshold.

--- PAGE 5 ---
Retrieval with Learned Similarities WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia
Algorithm 2 Approximate top- ğ‘˜algorithms.
Input: a queryğ‘, a set of items ğ‘‹
Output: approximate top ğ¾items
1:function ApproxTopK (ğ‘,ğ‘‹,ğ¾,ğ¾â€²)
2:ğºâ†ğ‘‡ğ‘œğ‘ğ¾ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’(ğ‘,ğ‘‹,ğ¾â€²)âŠ²Retrieve the top ğ¾â€²candidates.
3: returnğµğ‘Ÿğ‘¢ğ‘¡ğ‘’ğ¹ğ‘œğ‘Ÿğ‘ğ‘’ğ‘‡ğ‘œğ‘ğ¾ğ‘€ğ‘œğ¿ (ğ‘,ğº,ğ¾) âŠ²Retrieve the top ğ¾
items fromğºwith MoL.
Input: a queryğ‘, a set of items ğ‘‹,ğ‘“ğ‘(Â·),ğ‘”ğ‘(Â·)for constructing the ğ‘ƒ
component-level embedding pairs ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)
Output: union of top ğ¾items overğ‘ƒembedding pairs by dot product
4:function TopKPerEmbedding (ğ‘,ğ‘‹,ğ¾ )
5:ğºâ†âˆ…
6: forğ‘âˆˆğ‘ƒdo
7:ğ‘‹ğ‘â†{ğ‘”ğ‘(ğ‘¥),ğ‘¥âˆˆğ‘‹} âŠ²Can be preprocessed.
8:ğºâ†ğºâˆªğ‘‡ğ‘œğ‘ğ¾ğ·ğ‘œğ‘¡ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ (ğ‘“ğ‘(ğ‘),ğ‘‹ğ‘,ğ¾)âŠ²Retrieve the top
ğ¾items by dot product for the ğ‘-th embedding pair.
9: returnğºâŠ²Dedupâ€™ed set of the top ğ¾item for each of the ğ‘ƒqueries
Input: a queryğ‘, a set of items ğ‘‹,ğ‘“ğ‘(Â·),ğ‘”ğ‘(Â·)for constructing the
component-level embeddings ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)
Output: topğ¾items with averaged dot product,âˆ‘ï¸
ğ‘âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©/ğ‘ƒ
10:function TopKAvg (ğ‘,ğ‘‹,ğ¾ )
11:ğ‘â€²â†âˆ‘ï¸ğ‘ƒ
ğ‘=1ğ‘“ğ‘(ğ‘)
12:ğ‘‹â€²â†{âˆ‘ï¸ğ‘ƒ
ğ‘=1ğ‘”ğ‘(ğ‘¥)/ğ‘ƒ,ğ‘¥âˆˆğ‘‹} âŠ²Can be preprocessed.
13: returnğ‘‡ğ‘œğ‘ğ¾ğ·ğ‘œğ‘¡ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ (ğ‘â€²,ğ‘‹â€²,ğ¾)
When the threshold is a loose filter of the item set, which may
happen when the dot products are skewed, ğºâ€²can be large, and the
evaluation of MoL over a large number of candidates can be expen-
sive. Here, we describe two heuristics to approximately retrieve the
topğ¾items and analyze their gap against the exact algorithm.
In both heuristics, we perform a two-stage retrieval as shown
in Algorithm 2. In the first stage, we retrieve a set of ğ¾â€²candidate
items that are potentially high in MoL score by using dot products
(line 2). Note that ğ¾â€²can be larger than ğ¾, e.g., due to oversampling.
In the second stage, we evaluate the MoL scores of the candidate
items and return the top ğ¾items (line 3).
Here, we describe two heuristics to retrieve the candidate items:
Topğ¾per embedding. Given a query ğ‘and a set of items ğ‘‹, for
each of theğ‘embedding pairs, retrieve top ğ¾itemsğ‘‹ğ¾,ğ‘based on
dot product (âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©). Return the union across ğ‘ƒqueries.
The topğ¾per embedding heuristic returns the union of the top ğ¾
items for each embedding pair (group) by dot product. We analyze
the gap of this approach as follows:
Theorem 2. Upper bound of the gap of top ğ¾per embed-
ding: Letğ‘‹ğ¾,ğ‘be the topğ¾items of the embedding set ğ‘andğ‘†=
max{âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©,ğ‘¥âˆˆğ‘‹ğ¾+1,ğ‘\ğ‘‹ğ¾,ğ‘,âˆ€ğ‘}. Letğ‘†ğ¾be theğ¾ğ‘¡â„largest
MoL score of the items in âˆªğ‘ğ‘‹ğ¾,ğ‘, then we have ğ‘†Î”â‰¤ğ‘†âˆ’ğ‘†ğ¾.
Remark This gap (error bound) bounds the maximal difference
between the ğ¾ğ‘¡â„largest MoL score from the set of items retrieved by
the heuristic and the actual ğ¾ğ‘¡â„largest MoL score. In addition, any
retrieved item ğ‘¥withğœ™(ğ‘,ğ‘¥)â‰¥ğ‘†belongs to the actual top ğ¾items.
Note that there exists an MoL such thatğ‘†Î”=ğ‘†âˆ’ğ‘†ğ¾, i.e., when
ğœ‹ğ‘(ğ‘,ğ‘¥)=1forğ‘¥,ğ‘=arg maxğ‘¥,ğ‘{âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©,ğ‘¥âˆˆğ‘‹ğ¾+1,ğ‘\
ğ‘‹ğ¾,ğ‘,âˆ€ğ‘}. Thus, the upper bound of ğ‘†Î”is tight. In practice, we can
calculate a looser bound with the items retrieved by per embedding
topğ¾, i.e., fromğ‘‹ğ¾,ğ‘. We provide an example in Appendix B.Topğ¾average. Given a query ğ‘and a set of items ğ‘‹, return
the topğ¾items with the highest average dot product defined asâˆ‘ï¸
ğ‘âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©/ğ‘ƒ.
Note that the top ğ¾average heuristic returns the exact top ğ¾
items when the gating weight distribution in MoL,ğœ‹, is uniform.
This heuristic is interesting for two reasons. First, the items re-
trieved by this heuristic are likely to be the top ğ¾items of MoL
when the weight distribution is more balanced. This complements
the heuristic that retrieves top ğ¾per embedding. Second, in the
setup where the set of embedding pairs is constructed as the outer
product of the embeddings of a query and those of an item (Equa-
tion 2), the average dot product can be efficiently preprocessed
and materialized for the items, and the computation of the top ğ¾
average is then agnostic to the number of embedding pairs.
Formally, let ğ‘ƒ=ğ‘ƒğ‘Ã—ğ‘ƒğ‘¥be the number of embedding pairs,
whereğ‘ƒğ‘is the number of embeddings of a query ğ‘andğ‘ƒğ‘¥is that
of an itemğ‘¥. The average dot product can be computed as
1
ğ‘ƒÂ·ğ‘ƒâˆ‘ï¸‚
ğ‘=1âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©=1
ğ‘ƒÂ·ğ‘ƒğ‘âˆ‘ï¸‚
ğ‘ğ‘=1ğ‘ƒğ‘¥âˆ‘ï¸‚
ğ‘ğ‘¥=1âŸ¨ğ‘“ğ‘ğ‘(ğ‘),ğ‘”ğ‘ğ‘¥(ğ‘¥)âŸ© (5)
=1
ğ‘ƒÂ·âŸ¨ï¸„ğ‘ƒğ‘âˆ‘ï¸‚
ğ‘ğ‘=1ğ‘“ğ‘ğ‘(ğ‘),ğ‘ƒğ‘¥âˆ‘ï¸‚
ğ‘ğ‘¥=1ğ‘”ğ‘ğ‘¥(ğ‘¥)âŸ©ï¸„
(6)
Thus, we can preprocess the embeddings of the items and the
query, so the number of embeddings accessed is 1per item for a
given query, regardless of the overall number of component-level
embeddings used by MoL, i.e.,ğ‘ƒ.
Finally, we can combine the candidates retrieved from top ğ¾per
embedding group and the top ğ¾average as the following:
Combined top ğ¾.Given a query ğ‘, a set of items ğ‘‹, andğ¾, return
the union of the items from the top ğ¾per embedding group across
theğ‘ƒgroups and the top ğ¾items from the top ğ¾average.
Theorem 3. Upper bound of the gap of combined top ğ¾.Let
ğ‘‹ğ¾,ğ‘be the topğ¾items of the embedding set ğ‘andğ‘†ğ¾as defined
in Theorem 2. Let ğ‘‹â€²
ğ¾be the topğ¾items from top ğ¾average. Let
ğ‘†â€²=max{âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©,ğ‘¥âˆˆğ‘‹\(ğ‘‹ğ¾,ğ‘âˆªğ‘‹â€²
ğ¾),âˆ€ğ‘}. Then the gap
ğ‘†Î”satisfiesğ‘†Î”â‰¤ğ‘†â€²âˆ’ğ‘†ğ¾.
Remark Similar to Theorem 2, the upper bound of this gap is
tight. In practice, we can configure the ğ¾to be different for the two
heuristics, i.e., ğ¾1andğ¾2. For example, when the weight distribu-
tionğœ‹is more balanced, ğ¾2can be configured to be larger as the
topğ¾average approach approximates MoL well while being more
computationally efficient.
4 Evaluation
In this section, we evaluate the performance of MoL based learned
similarity with the proposed load balancing loss, and the efficiency
of retrieval algorithms discussed in Section 3. Our code and model
checkpoints are available at https://github.com/bailuding/rails.
4.1 Workloads
We benchmark MoL with the proposed load balancing loss Lğ‘€ğ¼, on
top of state-of-the-art baselines in recommendation systems and
question answering. We describe workloads used below.

--- PAGE 6 ---
WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia Bailu Ding and Jiaqi Zhai
Workload|ğ‘„||ğ‘‹||ğ‘ƒğ‘||ğ‘ƒğ‘¥|ğ‘‘ğ‘ƒ
ML-1M 6,040 3,649 8 4 64
ML-20M 138,493 24,186 8 4 128
Books 694,897 674,044 8 8 32
NQ320K 307,373 109,739 4 4 768
Table 2: Workload statistics.
Recommendation Systems. We consider three widely used
datasets, the 1M and 20M subsets of MovieLens [ 20], and the largest
Books subset of Amazon Reviews [ 38]. Sequential retrieval mod-
els have been shown to achieve state-of-the-art results on these
datasets [ 22,27,60]. In these settings, sequential encoders, like
RNNs or Transformers, are used to map user representations at
timeğ‘¡â€“ e.g., in a commonly used setting shown in Figure 2, the
list of items in user history up until time ğ‘¡,Î¦0,...,Î¦ğ‘¡â€“ toRğ‘‘, and
the model is trained to autoregressively predict the next item ğ‘¥ğ‘¡+1.
We hence compare MoL with the proposed regularization loss on
top of two popular backbones used for sequential retrieval models,
SASRec [ 27] and HSTU [ 60], against cosine similarity baselines. We
utilize user id-based embeddings discussed in Section 2.2 and MLPs
to parameterize the ğ‘ƒğ‘„query-side and the ğ‘ƒğ‘‹item-side features.
Question Answering (QA). Natural Questions (NQ) [ 32] is com-
monly used to evaluate state-of-the-art neural retrieval models,
including dense retrieval [ 28,41] and generative retrieval [ 11,53,
55,57] approaches in recent years. The most commonly used ver-
sion [ 53,55,57], which we reuse in our work, is often referred to
as NQ320k. NQ320k consists of 320k query-items pairs, where the
items are from Wikipedia pages and the queries are natural lan-
guage questions. We utilize special aggregation tokens discussed in
Section 2.2 to parameterize embeddings in MoL, and compare MoL
with popular sparse retrieval methods [ 42,47], dense retrieval meth-
ods [ 28,40,41], and generative retrieval methods [ 4,11,53,55,57].
Consistent with recent work [ 53,57,63], we use the pre-trained
query generation model from DocT5Query [ 42] to generate syn-
thetic (query, item) pairs for data augmentation.
Table 2 summarizes the statistics of these four workloads.
4.2 Quality of MoL-based Learned Similarity
Metrics. We use Recall (Hit Rate) as the main metric. We report
Hit Rate@{1, 10, 100} and Mean Reciprocal Rank (MRR) on NQ320K,
following [ 53,57], and Hit Rate@{1, 10, 50, 200} on ML-1M ,ML-20M ,
andBooks , following [59, 60].
Hyperparameter Settings. We set the weight ğ›¼for the proposed
load balancing loss Lğ‘€ğ¼to0.001for all experiments. We reuse
baseline settings for most other hyperparameters, including learn-
ing rate, number of examples used for in-batch negative sampling,
etc., with detailed discussions in Appendix A. For the NQ320K
dataset, we reuse SEAL [ 4] and NCI [ 57] results reported by [ 57],
and results for other models as reported by [ 53]. The Sentence-
T5 [40], GENRE [ 11], DSI [ 55], SEAL [ 4], DSI+QG [ 63], NCI [ 57],
and GenRet [ 53] rows are all finetuned from T5-base, consistent
with MoL, to ensure a fair comparison. All other results are imple-
mented in PyTorch, and are trained with 1x/2x 48GB GPUs for the
recommendation datasets and 4x 80GB GPUs for the QA datasets.MethodHR@KMRRK=1 K=10 K=50 K=200
ML-1M dataset
SASRec [27] .0610 .2818 .5470 .7540 .1352
SASRec + MoL .0697 .3036 .5617 .7667 .1441
HSTU [60] .0750 .3332 .5956 .7824 .1579
HSTU + MoL .0884 .3465 .6022 .7935 .1712
HSTU + MoL abl.Lğ‘€ğ¼ .0847 .3417 .6011 .7942 .1662
ML-20M dataset
SASRec [27] .0653 .2883 .5484 .7658 .1375
SASRec + MoL .0778 .3102 .5682 .7779 .1535
HSTU [60] .0962 .3557 .6146 .8080 .1800
HSTU + MoL .1010 .3698 .6260 .8132 .1881
HSTU + MoL abl.Lğ‘€ğ¼ .0994 .3670 .6241 .8128 .1866
Books dataset
SASRec [27] .0058 .0306 .0754 .1431 .0153
SASRec + MoL .0095 .0429 .0915 .1635 .0212
HSTU [60] .0101 .0469 .1066 .1876 .0233
HSTU + MoL .0156 .0631 .1308 .2173 .0324
HSTU + MoL abl.Lğ‘€ğ¼ .0153 .0625 .1286 .2172 .0321
Table 3: Evaluation of performance for sequential retrieval
models on MovieLens and Amazon Reviews.
MethodHR@KMRRK=1 K=10 K=100
Sparse retrieval
BM25 [47] .297 .603 .821 .402
DocT5Query [42] .380 .693 .861 .489
Dense retrieval
DPR [28] .502 .777 .909 .599
Sentence-T5 [40] .536 .830 .938 .641
GTR-Base [41] .560 .844 .937 .662
Generative retrieval
GENRE [11] .552 .673 .754 .599
DSI [55] .552 .674 .780 .596
SEAL [4] .570 .800 .914 .655
DSI+QG [63] .631 .807 .880 .695
NCI [57] .659 .852 .924 .731
GenRet [53] .681 .888 .952 .759
Learned similarities
MoL .685 .919 .970 .773
MoL abl.Lğ‘€ğ¼ .673 .919 .968 .767
Table 4: Evaluation of performance for QA retrieval models
finetuned from language models on Natural Questions.
Results. Across the six recommendation scenarios utilizing dif-
ferent sequential encoder backbones, Mixture-of-Logits (MoL rows)
consistently outperform the state-of-the-art dense retrieval base-
lines (dot products) by an average of 29.1% in HR@1, 16.3% in
HR@10, and 18.1% in MRR (Table 3). On the widely used Natural
Questions QA dataset, MoL outperforms all recent generative re-
trieval approaches as well as strong dense- and sparse- retrieval
baselines (Table 4). These results validate that learned similari-
ties, in particular MoL, are not only theoretically expressive but
also practically learnable , improving retrieval quality across het-
erogeneous scenarios, including sequential retrieval models for
Recommendations and finetuning LMs for Question Answering.

--- PAGE 7 ---
Retrieval with Learned Similarities WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia
Method HR@1 HR@5 HR@10 HR@50 HR@100 Latency / ms
ML-20MBruteForce 1.00 1.00 1.00 1.00 1.00 2.73 Â±0.01
TopKPerEmbd 5 0.69 0.67 0.63 0.46 0.40 1.61 Â±0.05
TopKPerEmbd 10 0.95 0.88 0.82 0.65 0.57 1.65 Â±0.06
TopKPerEmbd 50 1.00 1.00 0.99 0.95 0.92 1.64 Â±0.05
TopKPerEmbd 100 1.00 1.00 1.00 0.99 0.98 2.31Â±0.02
TopKAvg 200 1.00 1.00 1.00 0.99 0.97 1.19Â±0.04
TopKAvg 500 1.00 1.00 1.00 1.00 1.00 1.22Â±0.05
CombTopK 5_200 1.00 1.00 1.00 1.00 1.00 1.86Â±0.06
BooksBruteForce 1.00 1.00 1.00 1.00 1.00 128.36 Â±0.30
TopKPerEmbd 5 1.00 0.92 0.90 0.61 0.48 20.47 Â±0.07
TopKPerEmbd 50 1.00 1.00 1.00 0.98 0.93 21.60 Â±0.08
TopKPerEmbd 100 1.00 1.00 1.00 0.99 0.97 23.32Â±0.08
TopKPerEmbd 200 1.00 1.00 1.00 1.00 1.00 26.55Â±0.12
TopKAvg 200 0.97 0.92 0.91 0.76 0.67 1.13 Â±0.04
TopKAvg 500 0.97 0.98 0.97 0.86 0.81 1.17 Â±0.04
TopKAvg 1000 0.99 0.98 1.00 0.92 0.88 1.12 Â±0.05
TopKAvg 2000 1.00 0.99 1.00 0.95 0.92 1.20 Â±0.02
TopKAvg 4000 1.00 1.00 1.00 0.96 0.95 2.05 Â±0.01
TopKAvg 8000 1.00 1.00 1.00 0.97 0.97 3.79 Â±0.01
CombTopK 5_200 1.00 1.00 1.00 0.96 0.95 20.75 Â±0.07
CombTopK 50_500 1.00 1.00 1.00 0.99 0.96 22.12Â±0.07
CombTopK 100_1000 1.00 1.00 1.00 0.99 0.98 24.02Â±0.13
CombTopK 200_2000 1.00 1.00 1.00 1.00 1.00 28.01Â±0.11
NQ320KBruteForce 1.00 1.00 1.00 1.00 1.00 37.74 Â±.47
TopKPerEmbd 5 1.00 1.00 1.00 0.96 1.00 4.71Â±0.08
TopKPerEmbd 10 1.00 1.00 1.00 0.98 1.00 4.83Â±0.08
TopKPerEmbd 50 1.00 1.00 1.00 1.00 1.00 6.31Â±0.09
TopKAvg 100 1.00 1.00 1.00 1.00 1.00 0.57Â±0.05
CombTopK 5_100 1.00 1.00 1.00 1.00 1.00 5.28Â±0.08
Table 5: Evaluation of top ğ¾retrieval performance, with hit rate (HR) normalized by the brute-force top ğ¾method and latency
measured over a batch of queries (where the batch size is 32). (Relative) hit rate higher than .99 is marked in bold.
Ablation Studies. We conduct ablation studies for the proposed
mutual information-based load balancing loss relative to the best
performing method for each dataset (â€œabl. Lğ‘€ğ¼â€ rows). Results
show that our proposed Lğ‘€ğ¼loss improves HR@1 by 2.4%, HR@10
by 0.8% and MRR by 1.4% across the four datasets. In particular, our
proposedLğ‘€ğ¼loss enables MoL to outperform the best generative
retrieval approach on NQ320K, GenRet [53], across all metrics.
4.3 Topğ¾retrieval performance
We evaluate the following methods for top ğ¾retrieval performance:
â€¢Brute-force top ğ¾(BruteForce ): Evaluate the MoL scores for all
items and return the top ğ¾items. This is the ground truth in our
topğ¾evaluation2.
â€¢Per embedding top ğ¾(TopKPerEmbd (ğ‘)): This algorithm is de-
scribed in Section 3.2. ğ‘is the number of candidate items re-
trieved from each embedding set, where ğ‘Ã—ğ‘ƒâ‰¥ğ¾.
â€¢Average top ğ¾(TopKAvg (ğ‘)): This algorithm is described in Sec-
tion 3.2.ğ‘is the number of the candidate items retrieved by
average dot products, where ğ‘â‰¥ğ¾.
â€¢Combined top ğ¾from per embedding top ğ¾and average top ğ¾
(CombTopKğ‘1_ğ‘2): This is described in Section 3.2. ğ‘1is the
2We omit the baseline with the two-pass exact algorithm (Section 3.1) because the
range-based item retrieval can still be expensive when the range threshold is loose.
Empirically, the brute-force top ğ¾is more efficient on our datasets. We leave the
efficient implementation of the two-pass exact algorithm as future work.number of candidate items retrieved from per embedding top ğ¾
andğ‘2is the number of candidate items retrieved from average
topğ¾, whereğ‘1Ã—ğ‘ƒ+ğ‘2â‰¥ğ¾.
For each dataset, we evaluate top ğ¾retrieval methods based
on the best performing model configurations reported in Table 3
and Table 4. Table 5 shows the hit rate (HR) and latency of all
the methods. The hit rate is normalized by the ground truth, i.e.,
the hit rate achieved with brute-force top ğ¾. We measure latency
by evaluating a batch of 32 retrieval queries, in order to achieve
high accelerator utilization; this is consistent with prior work on
GPU/TPU-based retrieval algorithms [ 9,26,59]. We omit ML-1M
as its size is small (Table 2). We set batch size to 32for all datasets.
We perform evaluation on a single RTX 6000 Ada GPU. We report
latency averaged over 20 warm runs.
We observe that our approximate heuristics achieve high HR
with overfetching. For example, TopKAvg 500 achieves >.99in rel-
ative HR across the board for ML-20M , and TopKAvg 100 achieves
>.99in relative HR across the board for NQ320K . In addition, the
combined top ğ¾algorithm can outperform both TopKPerEmbd and
TopKAvg of the corresponding configurations, sometimes signifi-
cantly, e.g., CombTopK 5_200 vs. TopKPerEmbd 5 and TopKAvg 200
onBooks . This indicates that the set of candidate items retrieved by
each individual approximate algorithm indeed complements each
other when the weight distributions, ğœ‹ğ‘(ğ‘,ğ‘¥)s, vary in MoL.

--- PAGE 8 ---
WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia Bailu Ding and Jiaqi Zhai
In terms of efficiency, we observe that our approximate heuristics
are significantly lower in latency than the exact baselines, especially
as the number of items in the dataset becomes large. For example,
compared to BruteForce ,TopKAvg achieves >.99relative HR@100
with a speedup of 66Ã—in latency for NQ320K . While the algorithm
latency grows with the size of the dataset in the brute-force base-
line, it grows much slower with the approximate methods. For
example, the algorithm latency increases by 47.0Ã—from ML-20M
toBooks inBruteForce , while the growth rate is 10.1Ã—and1.0Ã—for
TopKPerEmbd 100 and TopKAvg 500, respectively. Thus, we expect
the speedup of the approximate methods to become even more
pronounced with larger datasets.
We also notice that TopKAvg tends to be more efficient than
TopKPerEmbd at comparable HRs, e.g., TopKAvg 4000 is 10.5Ã—faster
than TopKPerEmbd 50 on Books in terms of latency. First, the compu-
tation in TopKAvg is agnostic to the number of component-level em-
beddings,ğ‘ƒ, because of the preprocessing described in Section 3.2.
Second, the branching and deduplication steps in TopKPerEmbd
cannot leverage the parallel processing capabilities of GPUs effec-
tively. Additionally, the latency of the combined top ğ¾algorithm is
lower than the sum of its individual componentsâ€™ latencies; e.g., on
ML-20M ,CombTopK 5_200 has a latency 1.5Ã—lower than the sum of
the individual latencies of TopKPerEmbd 5 and TopKAvg 200. This is
done by consolidating processing shared by the two components.
Overall, empirically TopKAvg strikes a good balance between
high HR and low latency, and the combined top ğ¾algorithm can
be used if the target HR is extremely stringent.
5 Related work
Similarity Functions in Retrieval. Most information retrieval mod-
els in recommendation systems and natural language process-
ing (e.g., question answering) follow a classical two-stage para-
digm [ 10,28], where up to billions of items [ 6,13,35,59] are first
filtered down to hundreds in the retrieval stage, followed by another
stage (e.g., ranking in recommendation systems or generation in
RAG [ 33]) that produces the final results. Earlier work on large-
scale neural retrieval models primarily utilize dual-encoder (dense
retrieval, etc.) setups, with dot products as the similarity func-
tion [ 10,28,40,41]. Researchers quickly realized that dot products
limited retrieval stageâ€™s performance, and explored various learned
similarity-based approaches. Prominent variants include maximum
similarity based on multiple embeddings [ 29,35,48], specialized
neural networks, often leveraging Hadamard products [ 6,21,54,56],
and representing item ids as token sequences (â€œlearned index struc-
turesâ€), either implicitly defined during tree traversal [ 23,62,64] or
explicitly in the â€œgenerative retrievalâ€ setups [ 4,11,53,55,57,63]. It
has been shown, however, that learned neural distances often fail to
outperform dot products, e.g., Hadamard MLPs in recommendation
systems [ 46] and DSI for QA scenarios in NLP [ 53]. Learned index
structures further introduce stability and latency challenges as both
NLP and recommendation systems need to support billion-scale
realtime updated set of items [ 6,13,59]. Despite these challenges,
significant gains (17% gains at Hit Rate@100 [ 59] to 24% gains at Hit
Rate@400 [ 6]) with learned similarities have been reported in re-
cent years; these can be attributed to careful construction of learned
similarity functions [ 48,59], implicit diversification done as part of
beam search [ 15], explicit incorporation of side-information usingspecial neural architectures [ 6,59], and hardware-aware similarity
function and inference algorithm design on GPUs [6, 9, 43, 59].
Load Balancing for Conditional Computations in Neural Networks.
Conditional computations have been widely utilized in deep learn-
ing models [ 2,8,49]. Regularization losses have been proposed
based on the observation that an ideal policy should evenly utilize
all compute units in aggregate while being sparse at an individual
example level [ 2]. Mixture-of-experts, a common way to imple-
ment conditional computations, has been widely used in language
and vision domains [ 8,49] where mutual information-based reg-
ularization losses between experts and tasks [ 8] and experts and
tokens [50] have been shown to help with various architectures.
Efficient Nearest Neighbor Search (NNS). NNS has been a popular
research topic due to their critical role in large-scale retrieval and
vector databases. Most studies focus on the dot product case, also
known as Maximum Inner Product Search (MIPS). Various tech-
niques were proposed and analyzed, including tree structures [ 3,45],
locality sensitive hashing [ 17,51], production quantization [ 18,25],
data partitioning [ 34,61], graph-based methods [ 24,37], and so on.
The general case for NNS utilizing learned similarities remains less
studied; for learned index structures, techniques to construct trees
have been proposed to ensure beam search result in globally optimal
top-ğ¾results [ 64]. Algorithms based on implicit [ 24,37,43,56] or
explicit graphs [ 56] have been proposed to obtain a tractable candi-
date set in multi-stage retrieval setups; however, such approachesâ€™
performance can degrade when the similarity function is not a
metric, and constructing appropriate graph indices for non-metric
similarity functions can remain challenging even for the inner prod-
uct case [ 39]. Due to GPUs and other accelerators having orders of
magnitude higher arithmetic intensity vs CPUs, traditional quan-
tization techniques [ 18,51] no longer fully utilize the compute;
accelerator-specific nearest neighbor algorithms that benefit from
increased compute have been proposed recently [6, 9, 43, 59].
6 Conclusion
We have analyzed techniques for efficient retrieval with expressive
learned similarities in this work. We begin by showing Mixture-of-
Logits ( MoL) is a universal approximator of all similarity functions,
and further empirically learnable â€“ MoL with our proposed load bal-
ancing loss consistently outperforms dot products (dense retrieval),
sparse retrieval, and generative retrieval approaches across Rec-
ommendation Systems and Question Answering scenarios, setting
new state-of-the-art across common, heterogeneous benchmark
datasets. We next propose exact and approximate algorithms to
enable efficient retrieval using learned similarity functions, and
show their correctness and error bounds. Our approximate top ğ¾
algorithms can reach >.99of Hit Rate relative to exact algorithms,
while achieving up to 66Ã—reduction in end-to-end latency and with
minimal indexing overheads. We expect the speedups to be further
amplified with larger-scale datasets and GPU kernel optimizations.
Given MoLâ€™s impressive empirical performance gains of 20%-30%
across Hit Rate@50-400 over hundreds of millions to billions of
items [ 6,59] and broad applicability across heterogeneous scenar-
ios, our work provides strong theoretical and practical justifications
for migrating web-scale vector databases away from dense retrieval
and MIPS to Retrieva l with Learned Similarities (RAILS) on GPUs.

--- PAGE 9 ---
Retrieval with Learned Similarities WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia
References
[1][n. d.]. ANN Benchmarks. https://ann-benchmarks.com/. Accessed: 2024-08-06.
[2]Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup.
2016. Conditional Computation in Neural Networks for faster models.
arXiv:1511.06297 [cs.LG] https://arxiv.org/abs/1511.06297
[3]Jon Louis Bentley. 1975. Multidimensional binary search trees used for associative
searching. Commun. ACM 18, 9 (sep 1975), 509â€“517. https://doi.org/10.1145/
361002.361007
[4]Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Se-
bastian Riedel, and Fabio Petroni. 2022. Autoregressive Search En-
gines: Generating Substrings as Document Identifiers. In Advances in Neu-
ral Information Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal,
D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc.,
31668â€“31683. https://proceedings.neurips.cc/paper_files/paper/2022/file/
cd88d62a2063fdaf7ce6f9068fb15dcd-Paper-Conference.pdf
[5]Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-
ford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman
Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer,
Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,
Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving
Language Models by Retrieving from Trillions of Tokens. In International Con-
ference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,
USA (Proceedings of Machine Learning Research, Vol. 162) , Kamalika Chaudhuri,
Stefanie Jegelka, Le Song, Csaba SzepesvÃ¡ri, Gang Niu, and Sivan Sabato (Eds.).
PMLR, 2206â€“2240. https://proceedings.mlr.press/v162/borgeaud22a.html
[6]Fedor Borisyuk, Qingquan Song, Mingzhou Zhou, Ganesh Parameswaran, Madhu
Arun, Siva Popuri, Tugrul Bingol, Zhuotao Pei, Kuang-Hsuan Lee, Lu Zheng,
Qizhan Shao, Ali Naqvi, Sen Zhou, and Aman Gupta. 2024. LiNR: Model Based
Neural Retrieval on GPUs at LinkedIn. In Proceedings of the 33rd ACM International
Conference on Information and Knowledge Management (Boise, ID, USA) (CIKM
â€™24). Association for Computing Machinery, New York, NY, USA, 4366â€“4373.
https://doi.org/10.1145/3627673.3680091
[7]Haw-Shiuan Chang, Ruei-Yao Sun, Kathryn Ricci, and Andrew McCallum. 2023.
Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling. In Proceed-
ings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki
(Eds.). Association for Computational Linguistics, Toronto, Canada, 821â€“854.
https://doi.org/10.18653/v1/2023.acl-long.48
[8]Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao,
Erik G. Learned-Miller, and Chuang Gan. 2023. Mod-Squad: Designing Mixtures
of Experts As Modular Multi-Task Learners. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) . 11828â€“11837.
[9]Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and
Sanjiv Kumar. 2022. TPU-KNN: K Nearest Neighbor Search at Peak FLOP/s. In
Advances in Neural Information Processing Systems .
[10] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks
for YouTube Recommendations. In Proceedings of the 10th ACM Conference on
Recommender Systems (RecSys â€™16) . 191â€“198.
[11] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Au-
toregressive Entity Retrieval. In 9th International Conference on Learning Rep-
resentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net.
https://openreview.net/forum?id=5k8F6UU39V
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill
Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-
tional Linguistics, 4171â€“4186. https://doi.org/10.18653/v1/n19-1423
[13] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma,
Charles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A System for
Recommending 3+ Billion Items to 200+ Million Users in Real-Time. In Proceedings
of the 2018 World Wide Web Conference (WWW â€™18) . 1775â€“1784.
[14] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2017. Sigmoid-Weighted Linear
Units for Neural Network Function Approximation in Reinforcement Learning.
CoRR abs/1702.03118 (2017). arXiv:1702.03118 http://arxiv.org/abs/1702.03118
[15] Weihao Gao, Xiangjun Fan, Chong Wang, Jiankai Sun, Kai Jia, Wenzi Xiao, Ruofan
Ding, Xingyan Bin, Hui Yang, and Xiaobing Liu. 2021. Learning An End-to-End
Structure for Retrieval in Large-Scale Recommendations. In Proceedings of the
30th ACM International Conference on Information and Knowledge Management
(CIKM â€™21) . 524â€“533.
[16] Daniel Gillick, Alessandro Presta, and Gaurav Singh Tomar. 2018. End-to-End
Retrieval in Continuous Space. arXiv:1811.08008 [cs.IR]
[17] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999. Similarity Search in
High Dimensions via Hashing. In Proceedings of the 25th International Conference
on Very Large Data Bases (VLDB â€™99) . Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 518â€“529.[18] Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and David Simcha. 2016. Quan-
tization based Fast Inner Product Search. In Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics, AISTATS 2016 , Vol. 51. 482â€“490.
[19] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and
Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector
quantization. In Proceedings of the 37th International Conference on Machine
Learning (ICMLâ€™20) . JMLR.org, Article 364, 10 pages.
[20] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History
and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (dec 2015), 19 pages.
https://doi.org/10.1145/2827872
[21] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International
Conference on World Wide Web (Perth, Australia) (WWW â€™17) . 173â€“182.
[22] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2016. Session-based Recommendations with Recurrent Neural Networks. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto
Rico, May 2-4, 2016, Conference Track Proceedings , Yoshua Bengio and Yann LeCun
(Eds.). http://arxiv.org/abs/1511.06939
[23] Kalina Jasinska, Krzysztof Dembczynski, Robert Busa-Fekete, Karlson
Pfannschmidt, Timo Klerx, and Eyke Hullermeier. 2016. Extreme F-measure Max-
imization using Sparse Probability Estimates. In Proceedings of The 33rd Interna-
tional Conference on Machine Learning (Proceedings of Machine Learning Research,
Vol. 48) , Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York,
New York, USA, 1435â€“1444. https://proceedings.mlr.press/v48/jasinska16.html
[24] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar
Krishnawamy, and Rohan Kadekodi. 2019. DiskANN: Fast Accurate Billion-point
Nearest Neighbor Search on a Single Node. In Advances in Neural Information Pro-
cessing Systems , H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'AlchÃ©-Buc, E. Fox,
and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.
cc/paper_files/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf
[25] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2011. Product Quantization
for Nearest Neighbor Search. IEEE Trans. Pattern Anal. Mach. Intell. 33, 1 (jan
2011), 117â€“128. https://doi.org/10.1109/TPAMI.2010.57
[26] J. Johnson, M. Douze, and H. Jegou. 2021. Billion-Scale Similarity Search with
GPUs. IEEE Transactions on Big Data 7, 03 (Jul 2021), 535â€“547.
[27] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 International Conference on Data Mining (ICDM) . 197â€“206.
[28] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , Bonnie Webber, Trevor Cohn,
Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,
6769â€“6781. https://doi.org/10.18653/v1/2020.emnlp-main.550
[29] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage
Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information
Retrieval (Virtual Event, China) (SIGIR â€™20) . Association for Computing Machinery,
New York, NY, USA, 39â€“48. https://doi.org/10.1145/3397271.3401075
[30] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization
Techniques for Recommender Systems. Computer 42, 8 (2009), 30â€“37. https:
//doi.org/10.1109/MC.2009.263
[31] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language
independent subword tokenizer and detokenizer for Neural Text Processing. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations , Eduardo Blanco and Wei Lu (Eds.). Association
for Computational Linguistics, Brussels, Belgium, 66â€“71. https://doi.org/10.
18653/v1/D18-2012
[32] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,
Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.
Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A
Benchmark for Question Answering Research. Transactions of the Association for
Computational Linguistics 7 (2019), 452â€“466. https://doi.org/10.1162/tacl_a_00276
[33] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel,
Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Proceedings of the 34th International Conference
on Neural Information Processing Systems (, Vancouver, BC, Canada,) (NIPS â€™20) .
Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.
[34] Chen Li, E. Chang, H. Garcia-Molina, and G. Wiederhold. 2002. Clustering for
approximate similarity search in high-dimensional spaces. IEEE Transactions on
Knowledge and Data Engineering 14, 4 (2002), 792â€“808.
[35] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,
Guoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-Interest
Network with Dynamic Routing for Recommendation at Tmall. In Proceedings of
the 28th ACM International Conference on Information and Knowledge Management
(CIKM â€™19) . 2615â€“2623.

--- PAGE 10 ---
WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia Bailu Ding and Jiaqi Zhai
[36] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.
InInternational Conference on Learning Representations . https://openreview.net/
forum?id=Bkg6RiCqY7
[37] Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate
Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.
IEEE Trans. Pattern Anal. Mach. Intell. 42, 4 (apr 2020), 824â€“836. https://doi.org/
10.1109/TPAMI.2018.2889473
[38] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel.
2015. Image-Based Recommendations on Styles and Substitutes. In Proceedings
of the 38th International ACM SIGIR Conference on Research and Development in
Information Retrieval (Santiago, Chile) (SIGIR â€™15) . Association for Computing
Machinery, New York, NY, USA, 43â€“52. https://doi.org/10.1145/2766462.2767755
[39] Stanislav Morozov and Artem Babenko. 2018. Non-metric Similarity Graphs for
Maximum Inner Product Search. In Advances in Neural Information Processing
Systems , S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/
paper_files/paper/2018/file/229754d7799160502a143a72f6789927-Paper.pdf
[40] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel
Cer, and Yinfei Yang. 2022. Sentence-T5: Scalable Sentence Encoders from Pre-
trained Text-to-Text Models. In Findings of the Association for Computational
Linguistics: ACL 2022 , Smaranda Muresan, Preslav Nakov, and Aline Villavicencio
(Eds.). Association for Computational Linguistics, Dublin, Ireland, 1864â€“1874.
https://doi.org/10.18653/v1/2022.findings-acl.146
[41] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma,
Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large
Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa
Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,
Abu Dhabi, United Arab Emirates, 9844â€“9855. https://doi.org/10.18653/v1/2022.
emnlp-main.669
[42] Rodrigo Nogueira and Jimmy Lin. 2019. From doc2query to docttttt-
query. https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_
docTTTTTquery-v2.pdf
[43] Hiroyuki Ootomo, Akira Naruse, Corey Nolet, Ray Wang, Tamas Feher, and Yong
Wang. 2024. CAGRA: Highly Parallel Graph Construction and Approximate
Nearest Neighbor Search for GPUs.
[44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring
the Limits of Transfer Learning with a Unified Text-to-Text Transformer.
arXiv:1910.10683 [cs.LG] https://arxiv.org/abs/1910.10683
[45] Parikshit Ram and Alexander G. Gray. 2012. Maximum Inner-Product Search Us-
ing Cone Trees. In Proceedings of the 18th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD â€™12) . 931â€“939.
[46] Steffen Rendle, Walid Krichene, Li Zhang, and John Anderson. 2020. Neural
Collaborative Filtering vs. Matrix Factorization Revisited. In Fourteenth ACM
Conference on Recommender Systems (RecSysâ€™20) . 240â€“248.
[47] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Frame-
work: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (April 2009), 333â€“389.
https://doi.org/10.1561/1500000019
[48] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei
Zaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late
Interaction. In Proceedings of the 2022 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies ,
Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz
(Eds.). Association for Computational Linguistics, Seattle, United States, 3715â€“
3734. https://doi.org/10.18653/v1/2022.naacl-main.272
[49] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks: The
Sparsely-Gated Mixture-of-Experts Layer. In International Conference on Learning
Representations . https://openreview.net/forum?id=B1ckMDqlg
[50] Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and
Chuang Gan. 2023. ModuleFormer: Modularity Emerges from Mixture-of-Experts.
arXiv:2306.04640 [cs.CL] https://arxiv.org/abs/2306.04640
[51] Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for Sublinear
Time Maximum Inner Product Search (MIPS). In Advances in Neural Information
Processing Systems , Vol. 27.
[52] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via
Self-Attentive Neural Networks. In Proceedings of the 28th ACM International
Conference on Information and Knowledge Management (Beijing, China) (CIKM
â€™19). Association for Computing Machinery, New York, NY, USA, 1161â€“1170.
https://doi.org/10.1145/3357384.3357925
[53] Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu,
Pengjie Ren, Zhumin Chen, Dawei Yin, Maarten Rijke, and Zhaochun
Ren. 2023. Learning to Tokenize for Generative Retrieval. In Advances
in Neural Information Processing Systems , A. Oh, T. Naumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates,Inc., 46345â€“46361. https://proceedings.neurips.cc/paper_files/paper/2023/file/
91228b942a4528cdae031c1b68b127e8-Paper-Conference.pdf
[54] Shulong Tan, Zhixin Zhou, Zhaozhuo Xu, and Ping Li. 2020. Fast Item Ranking
under Neural Network based Measures. In Proceedings of the 13th International
Conference on Web Search and Data Mining (Houston, TX, USA) (WSDM â€™20) .
Association for Computing Machinery, New York, NY, USA, 591â€“599. https:
//doi.org/10.1145/3336191.3371830
[55] Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta,
Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and
Donald Metzler. 2022. Transformer Memory as a Differentiable Search Index. In
Advances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal,
Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?
id=Vu-B0clPfq
[56] Yiwei Wang, Bryan Hooi, Yozen Liu, Tong Zhao, Zhichun Guo, and Neil Shah.
2022. Flashlight: Scalable Link Prediction With Effective Decoders. In Proceedings
of the First Learning on Graphs Conference (Proceedings of Machine Learning
Research, Vol. 198) , Bastian Rieck and Razvan Pascanu (Eds.). PMLR, 14:1â€“14:17.
https://proceedings.mlr.press/v198/wang22a.html
[57] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen,
Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Sun,
Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A Neural Corpus Indexer for Doc-
ument Retrieval. In Advances in Neural Information Processing Systems , S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran
Associates, Inc., 25600â€“25614. https://proceedings.neurips.cc/paper_files/paper/
2022/file/a46156bd3579c3b268108ea6aca71d13-Paper-Conference.pdf
[58] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018.
Breaking the Softmax Bottleneck: A High-Rank RNN Language Model. In Inter-
national Conference on Learning Representations (ICLRâ€™18) .
[59] Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, and Xing
Liu. 2023. Revisiting Neural Retrieval on Accelerators. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach,
CA, USA) (KDD â€™23) . Association for Computing Machinery, New York, NY, USA,
5520â€“5531. https://doi.org/10.1145/3580305.3599897
[60] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao,
Zhaojie Gong, Fangda Gu, Jiayuan He, Yinghai Lu, and Yu Shi. 2024. Actions
Speak Louder than Words: Trillion-Parameter Sequential Transducers for Gen-
erative Recommendations. In Proceedings of the 41st International Conference
on Machine Learning (Proceedings of Machine Learning Research, Vol. 235) , Rus-
lan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver,
Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR, 58484â€“58509. https:
//proceedings.mlr.press/v235/zhai24a.html
[61] Jiaqi Zhai, Yin Lou, and Johannes Gehrke. 2011. ATLAS: A Probabilistic Algorithm
for High Dimensional Similarity Search. In Proceedings of the 2011 ACM SIGMOD
International Conference on Management of Data (SIGMOD â€™11) . 997â€“1008.
[62] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.
2018. Learning Tree-Based Deep Model for Recommender Systems. In Proceedings
of the 24th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (London, United Kingdom) (KDD â€™18) . 1079â€“1088.
[63] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuc-
con, and Daxin Jiang. 2023. Bridging the Gap Between Indexing and Retrieval
for Differentiable Search Index with Query Generation. arXiv:2206.10128 [cs.IR]
https://arxiv.org/abs/2206.10128
[64] Jingwei Zhuo, Ziru Xu, Wei Dai, Han Zhu, Han Li, Jian Xu, and Kun Gai. 2020.
Learning optimal tree models under beam search. In Proceedings of the 37th
International Conference on Machine Learning (ICMLâ€™20) . JMLR.org, Article 1080,
10 pages.
A Experiment Setups
A.1 Reproducibility
The implementations and hyperparameter settings for reproduc-
ing our experiment results can be found at https://github.com/
bailuding/rails. We discuss specific details below.
A.2 Parameterization of low-rank
(â€œcomponent-levelâ€) embeddings
In this section, we elaborate on the embedding parameterization
methods for MoL that we discussed in Section 2.2.
A.2.1 Recommendation Systems. Prior work have shown that care-
ful parameterization of low-rank (â€œcomponent-levelâ€) embeddings,
orğ‘“ğ‘(ğ‘)andğ‘”ğ‘(ğ‘¥)s for 1â‰¤ğ‘â‰¤ğ‘ƒ, can significantly improve MoLâ€™s

--- PAGE 11 ---
Retrieval with Learned Similarities WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia
Q1 Q2 ... QPQ SP1 SP2 SP3 ... SPNQuery Encoder: Finetuned Language Model (e.g., T5)Parameterized Pooling: (D,) -> (max_seq_len, PQ)g1(q) g2(q) ... gPQ(q)
X1 X2 ... XPX SP1 SP2 SP3 ... SPNItem Encoder: Finetuned Language Model (e.g., T5)Parameterized Pooling: (D,) -> (max_seq_len, PX)f1(x) f2(x) ... fPX(x)Sequential Encoder
(RNNs/T ransformers)
Î¦1 Î¦2 Î¦3 ... Î¦Ng1(q) g2(q) ... gPQ(q)
f1(x) f2(x) ... fPX(x)
Item embeddingsItem MLPQuery MLPSide information, e.g.,
user embeddings
Rich, h eterogeneous features (Recommendations) Single, homogeneous feature (Language Models )
Figure 3: Illustration of how to parameterize the embeddings to adapt Mixture-of-logits (MoL) learned similarity to various re-
trieval scenarios, with a language model (LM) finetuning use case in question answering (characterized by a single homogeneous
feature) shown on the left, and a recommendation systems use case (characterized by a large number of heterogeneous features)
shown on the right. For the Question Answering example on the left, ğ‘†ğ‘ƒ1,...,ğ‘†ğ‘ƒğ‘represents the original SentencePiece [ 31]
tokens that are inputs to the pre-trained language model LM, e.g., T5 [ 44].ğ‘„1,ğ‘„2,...,ğ‘„ğ‘ƒğ‘„andğ‘‹1,ğ‘‹2,...,ğ‘‹ğ‘ƒğ‘‹represent the
special aggregation tokens we add to the LM tokenizer for pooling information across the sequence. The â€œParameterized
Poolingâ€ component uses a ğ·-dimensional embedding as input to parameterize, at an example-level , how to weight each of the
(max_seq_len) encoder outputs for the ğ‘ƒğ‘„/ğ‘ƒğ‘‹MoL component-level embeddings.
performance [ 6]. In the context of large-scale recommendation sys-
tems, cluster information based on interests of cohorts of members
and topics of posts by themselves can lead to 10% recall gain at
ğ¾=400[6]. However, we cannot easily access similar information
in the publicly available MovieLens [ 20] and Amazon Reviews [ 38]
datasets. We therefore follow implementation provided by [ 59] and
additionally optionally utilizes a User ID keyed one-hot embedding
as one query-side low-rank (â€œcomponent-levelâ€) embeddings ğ‘“ğ‘(ğ‘),
which is a widely used technique in recommendation systems [ 30]
that we discussed in Section 2.2. All other component-level embed-
dings,ğ‘“ğ‘(ğ‘)s andğ‘”ğ‘(ğ‘¥)s, are obtained by applying a multi-layer
perceptron (MLP) on top of query-side/item-side representations
in standard sequential recommendation setups [ 22,27]. The overall
setup is illustrated on the right hand side of Figure 3.
A.2.2 Question Answering (QA). Unlike Recommendation Systems,
retrieval models used in question answering generally take the full
semantic representation(s) of the query and/or the document as
input, and are finetuned on top of pre-trained language models
with homogeneous inputs, or wordpiece / sentencepiece tokens.
Our MoL embedding construction consists of two components,
special aggregation tokens and parameterized pooling. We present
embedding construction on the query side first.Special Aggregation Tokens. Given both queries and documents
are represented as token sequences (e.g., SentencePieces [ 31] in
T5 [44]), we propose to add special tokens that can be used to
aggregate different aspects of information as part of the overall
self-attention based language model. Specifically, on the query
side, let the tokenized sequence be ğ‘†ğ‘ƒ1,ğ‘†ğ‘ƒ2,...,ğ‘†ğ‘ƒğ‘. During
finetuning of the pretrained language model, we create ğ‘ƒğ‘„spe-
cial tokens, ğ‘„1,...,ğ‘„ğ‘ƒğ‘„, and add them to the vocabulary of the
query tokenizer. We also append those exact same ğ‘ƒğ‘„tokens
beforeğ‘†ğ‘ƒ1,ğ‘†ğ‘ƒ2,...,ğ‘†ğ‘ƒğ‘, so that the ğ‘ƒğ‘„special tokens can be
used to aggregate information across the query input using early-
fusion mechanisms. Note that many question answering scenar-
ios [11,28,41,53,57] utilize bidirectional language models for re-
trieval, like BERT [ 12] or T5 [ 44]; for recent unidirectional language
models, we can add the special aggregation tokens ğ‘‹1,...,ğ‘‹ğ‘ƒğ‘‹and
ğ‘„1,...,ğ‘„ğ‘ƒğ‘„to the end of the input sequence instead. Our con-
struction can also be viewed as a way to extend the CLS token in
BERT [ 7,12] to cover multiple aspects of information, in a way that
encourages diversity via the Lğ‘€ğ¼load balancing loss discussed in
Section 2.

--- PAGE 12 ---
WWW â€™25, April 28-May 2, 2025, Sydney, NSW, Australia Bailu Ding and Jiaqi Zhai
itemâŸ¨ğ‘“1,ğ‘”1âŸ©âŸ¨ğ‘“2,ğ‘”2âŸ©ğœ‹1ğœ‹2ğœ™
a 1 1 0.5 0.5 1.0
b 0.8 0 0.5 0.5 0.4
c 0 0.8 0.5 0.5 0.4
d 0.7 0 1 0 0.7
e 0.2 0.2 0.5 0.5 0.2
Table 6: Example illustrating how exact- and approximate-
top-ğ‘˜algorithms work. We consider a fixed query ğ‘, and
provide inner products âŸ¨ğ‘“ğ‘(ğ‘¥),ğ‘”ğ‘(ğ‘)âŸ©s, gating weights ğœ‹ğ‘s,
and learned similarity scores ğœ™s for that query.
Parameterized Pooling. We next add a pooling layer after the
language model to encourage learning of aggregation mechanisms
separate from language semantics. For each position 1â‰¤ğ‘â‰¤ğ‘ƒğ‘„,
this pooling layer defines a probability distribution over different
positions in language modelâ€™s outputs, or (0,...,ğ‘šğ‘ğ‘¥ _ğ‘ ğ‘’ğ‘_ğ‘™ğ‘’ğ‘›âˆ’1).
We further parameterize the pooling layer, using the ğ·-dimensional
embedding at the first position after encoders. This enables us to
define a pooling policy, at an example-level, how to weight each
of theğ‘šğ‘ğ‘¥_ğ‘ ğ‘’ğ‘_ğ‘™ğ‘’ğ‘›LM encoder outputs to arrive at the ğ‘ƒğ‘„MoL
embeddings.
The embedding construction on the item-side is identical. We
illustrate the overall finetuning setup we use for question answering
on the left hand side of Figure 3.
A.3 Parameterization of ğœ‹ğ‘(ğ‘,ğ‘¥)matrices
We follow the implementation provided in the original MoL pa-
per [ 59], which parameterizes ğœ‹ğ‘(ğ‘,ğ‘¥)as a two-layer multi-layer
perceptron (MLP) with SiLU [ 14] non-linearity. For recommenda-
tion datasets ( ML-1M ,ML-20M ,Books ), the inputs to this MLP con-
sist of user-side features, item-side features, and the ğ‘ƒdot products
âŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©s between the low-rank embeddings. For question
answering datasets (NQ320K), we only use the last part â€“ the ğ‘ƒdot
productsâŸ¨ğ‘“ğ‘(ğ‘),ğ‘”ğ‘(ğ‘¥)âŸ©s between the low-rank embeddings â€“ as
inputs to this MLP.
A.4 Hyperparameter settings
A.4.1 Recommendation Systems. We use an identical number of
sampled negatives for dot product baselines (cosine similarity, â€œSAS-
Recâ€, â€œHSTUâ€ rows in Table 3) and Mixture-of-Logits (â€œSASRec +
MoLâ€, â€œHSTU + MoLâ€ rows in Table 3) to ensure a fair compari-
son, which is 128for ML-1M and ML-20M and 512for Amazon
Books following prior work. For â€œ+ MoLâ€ rows, we additionally
grid searched|ğ‘ƒğ‘¥|in{2,4,8,16},ğ‘‘ğ‘ƒin{32,64,128}, whether to
enable user-id based learned embeddings, and the dropout rate toapply to user-id based embeddings in {0.2,0.5,0.8}for the smaller
MovieLens datasets. We followed initial hyperparameters provided
by the authors [ 59] for all other parameters. The models are trained
using PyTorch over 1 NVIDIA RTX 6000 Ada GPU for the smaller
ML-1M andML-20M datasets and 2 NVIDIA RTX 6000 Ada GPUs
for the larger Books datasets.
A.4.2 Question Answering (QA). We train the model with AdamW
optimizer [ 36], and grid searched learning rate in {2e-4, 5e-4, 8e-4}
due to the introduction of the parameterized pooling component
(Appendix A.2). We apply linear scheduling with warm-up over a
fixed 10% of the training epochs. We train the model on 4 NVIDIA
H100 80GB GPUs with a local batch size of 512. Note that due
to the computational requirements of this dataset, prior work are
frequently trained on 8 GPUs [ 28,57] or more, e.g., 32 GPUs in
GENRE [ 11] and 256 TPUs in DSI [ 55]. We perform in-batch nega-
tive sampling, consistent with baselines [ 28,40]. For MoL hyperpa-
rameters, we grid searched ğ‘ƒğ‘„andğ‘ƒğ‘‹in {(2, 2), (4, 4), (8, 8), (16, 16)},
keptğ‘‘ğ‘ƒidentical to the embedding dimension of the pretrained lan-
guage model ( 768), and selected the best hyperparameters utilizing
a validation set.
B Examples for exact and approximate top ğ¾
retrieval algorithms
We provide examples for our exact- and approximate- retrieval
algorithms discussed in Section 3 to facilitate understanding.
Retrieval with Exact Algorithm (Section 3.1). Table 6 shows an
example with 4items and 2pairs (groups) of embeddings ( ğ‘ƒ=2).
Assume the goal is to retrieve the top ğ¾=2items. In the first
stage, we retrieve the top 2items for each embedding set, i.e., item
ğ‘,ğ‘,ğ‘ are retrieved. For each of the retrieved items, we calculate
the MoL scores based on their gating weights, i.e., 1.0,0.4,0.4for
ğ‘,ğ‘, andğ‘, respectively. Here, ğ‘†ğ‘šğ‘–ğ‘›is set to 0.4. In the second stage,
we retrieve all items with âŸ¨ğ‘“,ğ‘”âŸ©â‰¥0.4for each embedding set, i.e.,
itemğ‘‘, and then calculate their corresponding MoL score, i.e., 0.7.
The algorithm returns ğ‘andğ‘‘as the top 2items.
Retrieval with Approximate Algorithm (Theorem 2). Consider
again the example shown in Table 6. Assume we want to retrieve
the top 2items. With top ğ¾over component-level embeddings,
itemğ‘,ğ‘,ğ‘ are retrieved, and ğ‘†ğ‘˜=0.4. Sinceğ‘†is calculated as
max{0.7,0.2}, the upper bound of the gap is 0.3. Here, the gap is
exact, i.e., the actual second largest MoL score with item ğ‘‘is0.3
higher than the second largest MoL score from the set of retrieved
items{ğ‘,ğ‘,ğ‘}. If we bound the gap with the retrieved items only,
i.e.,ğ‘,ğ‘,ğ‘ , then we will get a looser bound of 0.8âˆ’0.4=0.4.
