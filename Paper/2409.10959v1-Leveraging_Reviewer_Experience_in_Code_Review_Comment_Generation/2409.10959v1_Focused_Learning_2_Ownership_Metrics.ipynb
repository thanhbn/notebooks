{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Reviewer Ownership Metrics (ACO/RSO) Implementation\n",
    "\n",
    "## Learning Objective\n",
    "Master the calculation and application of Authoring Code Ownership (ACO) and Review-Specific Ownership (RSO) metrics, understanding how to measure developer experience at multiple granularities (repository, subsystem, package).\n",
    "\n",
    "## Paper Reference\n",
    "- **Section 3.1**: Reviewer Experience Heuristics (Pages 6-7)\n",
    "- **Equation (1)**: ACO(D,G) = α(D,G) / C(G)\n",
    "- **Equation (2)**: RSO(D,G) = r(D,G) / ρ(G)\n",
    "- **Algorithm 1 & 2**: ACO and RSO Implementation\n",
    "\n",
    "## Why Ownership Metrics are Complex\n",
    "1. **Temporal Dynamics**: Metrics must be calculated at specific timestamps\n",
    "2. **Multi-granularity**: Repository, subsystem, and package levels capture different expertise\n",
    "3. **Large-scale Computation**: Processing millions of commits and PRs efficiently\n",
    "4. **Data Quality Issues**: Missing data, bot accounts, rebasing affects accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Foundation: Understanding Code Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import defaultdict, Counter\n",
    "import networkx as nx\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Structures for Repository History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Commit:\n",
    "    \"\"\"Represents a single commit in the repository\"\"\"\n",
    "    commit_id: str\n",
    "    author_id: str\n",
    "    timestamp: datetime\n",
    "    files_changed: List[str]\n",
    "    is_merge: bool = False\n",
    "    \n",
    "    def get_subsystem(self, file_path: str) -> str:\n",
    "        \"\"\"Extract subsystem (top-level directory) from file path\"\"\"\n",
    "        parts = file_path.strip('/').split('/')\n",
    "        return parts[0] if parts else 'root'\n",
    "    \n",
    "    def get_package(self, file_path: str) -> str:\n",
    "        \"\"\"Extract package (immediate folder) from file path\"\"\"\n",
    "        parts = file_path.strip('/').split('/')\n",
    "        if len(parts) >= 2:\n",
    "            return '/'.join(parts[:2])\n",
    "        return parts[0] if parts else 'root'\n",
    "\n",
    "@dataclass\n",
    "class PullRequest:\n",
    "    \"\"\"Represents a pull request/code review\"\"\"\n",
    "    pr_id: str\n",
    "    reviewers: List[str]  # List of reviewer IDs who commented\n",
    "    timestamp: datetime\n",
    "    files_changed: List[str]\n",
    "    status: str = \"closed\"  # closed, merged, open\n",
    "\n",
    "@dataclass\n",
    "class ReviewComment:\n",
    "    \"\"\"Represents a single review comment\"\"\"\n",
    "    comment_id: str\n",
    "    reviewer_id: str\n",
    "    pr_id: str\n",
    "    timestamp: datetime\n",
    "    file_path: str\n",
    "    repository: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Repository History Generator (Mock Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepositoryHistoryGenerator:\n",
    "    \"\"\"Generate realistic repository history for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.developers = self._generate_developers()\n",
    "        self.file_structure = self._generate_file_structure()\n",
    "        \n",
    "    def _generate_developers(self, n=20):\n",
    "        \"\"\"Generate developer profiles with different activity levels\"\"\"\n",
    "        developers = []\n",
    "        \n",
    "        # Core maintainers (high activity)\n",
    "        for i in range(3):\n",
    "            developers.append({\n",
    "                'id': f'maintainer_{i}',\n",
    "                'type': 'maintainer',\n",
    "                'commit_prob': 0.3,\n",
    "                'review_prob': 0.4\n",
    "            })\n",
    "        \n",
    "        # Regular contributors\n",
    "        for i in range(7):\n",
    "            developers.append({\n",
    "                'id': f'contributor_{i}',\n",
    "                'type': 'contributor',\n",
    "                'commit_prob': 0.15,\n",
    "                'review_prob': 0.2\n",
    "            })\n",
    "        \n",
    "        # Occasional contributors\n",
    "        for i in range(10):\n",
    "            developers.append({\n",
    "                'id': f'occasional_{i}',\n",
    "                'type': 'occasional',\n",
    "                'commit_prob': 0.05,\n",
    "                'review_prob': 0.1\n",
    "            })\n",
    "        \n",
    "        return developers\n",
    "    \n",
    "    def _generate_file_structure(self):\n",
    "        \"\"\"Generate realistic file structure\"\"\"\n",
    "        structure = {\n",
    "            'src': [\n",
    "                'src/core/engine.py',\n",
    "                'src/core/utils.py',\n",
    "                'src/core/config.py',\n",
    "                'src/api/routes.py',\n",
    "                'src/api/middleware.py',\n",
    "                'src/api/auth.py',\n",
    "                'src/models/user.py',\n",
    "                'src/models/product.py',\n",
    "                'src/models/order.py'\n",
    "            ],\n",
    "            'tests': [\n",
    "                'tests/unit/test_engine.py',\n",
    "                'tests/unit/test_utils.py',\n",
    "                'tests/integration/test_api.py',\n",
    "                'tests/integration/test_models.py'\n",
    "            ],\n",
    "            'docs': [\n",
    "                'docs/api.md',\n",
    "                'docs/setup.md',\n",
    "                'docs/contributing.md'\n",
    "            ],\n",
    "            'config': [\n",
    "                'config/production.yml',\n",
    "                'config/development.yml',\n",
    "                'config/testing.yml'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Flatten to list\n",
    "        all_files = []\n",
    "        for category, files in structure.items():\n",
    "            all_files.extend(files)\n",
    "        return all_files\n",
    "    \n",
    "    def generate_commits(self, n_commits=1000, time_span_days=365):\n",
    "        \"\"\"Generate commit history\"\"\"\n",
    "        commits = []\n",
    "        start_date = datetime.now() - timedelta(days=time_span_days)\n",
    "        \n",
    "        for i in range(n_commits):\n",
    "            # Select developer based on activity probability\n",
    "            dev_probs = [d['commit_prob'] for d in self.developers]\n",
    "            dev_probs = np.array(dev_probs) / sum(dev_probs)\n",
    "            developer = np.random.choice(self.developers, p=dev_probs)\n",
    "            \n",
    "            # Generate timestamp\n",
    "            days_offset = np.random.uniform(0, time_span_days)\n",
    "            timestamp = start_date + timedelta(days=days_offset)\n",
    "            \n",
    "            # Select files changed (developers tend to work on specific areas)\n",
    "            n_files = np.random.poisson(2) + 1  # At least 1 file\n",
    "            if developer['type'] == 'maintainer':\n",
    "                # Maintainers work across the codebase\n",
    "                files = np.random.choice(self.file_structure, min(n_files, 5), replace=False)\n",
    "            else:\n",
    "                # Others tend to focus on specific subsystems\n",
    "                subsystem = np.random.choice(['src', 'tests', 'docs', 'config'])\n",
    "                subsystem_files = [f for f in self.file_structure if f.startswith(subsystem)]\n",
    "                files = np.random.choice(subsystem_files, \n",
    "                                       min(n_files, len(subsystem_files)), \n",
    "                                       replace=False)\n",
    "            \n",
    "            commit = Commit(\n",
    "                commit_id=f\"commit_{i:04d}\",\n",
    "                author_id=developer['id'],\n",
    "                timestamp=timestamp,\n",
    "                files_changed=list(files),\n",
    "                is_merge=np.random.random() < 0.1  # 10% merge commits\n",
    "            )\n",
    "            commits.append(commit)\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        commits.sort(key=lambda x: x.timestamp)\n",
    "        return commits\n",
    "    \n",
    "    def generate_pull_requests(self, n_prs=300, time_span_days=365):\n",
    "        \"\"\"Generate pull request history\"\"\"\n",
    "        prs = []\n",
    "        start_date = datetime.now() - timedelta(days=time_span_days)\n",
    "        \n",
    "        for i in range(n_prs):\n",
    "            # Generate timestamp\n",
    "            days_offset = np.random.uniform(0, time_span_days)\n",
    "            timestamp = start_date + timedelta(days=days_offset)\n",
    "            \n",
    "            # Select reviewers (usually 1-3)\n",
    "            n_reviewers = np.random.poisson(1.5) + 1\n",
    "            review_probs = [d['review_prob'] for d in self.developers]\n",
    "            review_probs = np.array(review_probs) / sum(review_probs)\n",
    "            reviewers = np.random.choice(self.developers, \n",
    "                                       min(n_reviewers, 3), \n",
    "                                       replace=False,\n",
    "                                       p=review_probs)\n",
    "            reviewer_ids = [r['id'] for r in reviewers]\n",
    "            \n",
    "            # Select files changed\n",
    "            n_files = np.random.poisson(3) + 1\n",
    "            files = np.random.choice(self.file_structure, min(n_files, 10), replace=False)\n",
    "            \n",
    "            pr = PullRequest(\n",
    "                pr_id=f\"pr_{i:04d}\",\n",
    "                reviewers=reviewer_ids,\n",
    "                timestamp=timestamp,\n",
    "                files_changed=list(files),\n",
    "                status=\"closed\"\n",
    "            )\n",
    "            prs.append(pr)\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        prs.sort(key=lambda x: x.timestamp)\n",
    "        return prs\n",
    "\n",
    "# Generate mock repository history\n",
    "generator = RepositoryHistoryGenerator()\n",
    "commits = generator.generate_commits(1000, 365)\n",
    "pull_requests = generator.generate_pull_requests(300, 365)\n",
    "\n",
    "print(f\"Generated {len(commits)} commits and {len(pull_requests)} pull requests\")\n",
    "print(f\"Time span: {commits[0].timestamp.date()} to {commits[-1].timestamp.date()}\")\n",
    "print(f\"\\nDevelopers: {len(generator.developers)}\")\n",
    "print(f\"Files: {len(generator.file_structure)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing ACO and RSO Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnershipCalculator:\n",
    "    \"\"\"Calculate ACO and RSO metrics following Algorithm 1 & 2 from the paper\"\"\"\n",
    "    \n",
    "    def __init__(self, commits: List[Commit], pull_requests: List[PullRequest]):\n",
    "        self.commits = commits\n",
    "        self.pull_requests = pull_requests\n",
    "        self._preprocess_data()\n",
    "        \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"Preprocess data for efficient calculation\"\"\"\n",
    "        # Remove merge commits as per paper\n",
    "        self.commits = [c for c in self.commits if not c.is_merge]\n",
    "        \n",
    "        # Index commits by granularity for faster lookup\n",
    "        self.commits_by_repo = defaultdict(list)\n",
    "        self.commits_by_subsystem = defaultdict(list)\n",
    "        self.commits_by_package = defaultdict(list)\n",
    "        \n",
    "        for commit in self.commits:\n",
    "            # Repository level\n",
    "            self.commits_by_repo['repository'].append(commit)\n",
    "            \n",
    "            # Subsystem and package level\n",
    "            for file_path in commit.files_changed:\n",
    "                subsystem = commit.get_subsystem(file_path)\n",
    "                package = commit.get_package(file_path)\n",
    "                \n",
    "                self.commits_by_subsystem[subsystem].append(commit)\n",
    "                self.commits_by_package[package].append(commit)\n",
    "        \n",
    "        # Similarly for PRs\n",
    "        self.prs_by_repo = defaultdict(list)\n",
    "        self.prs_by_subsystem = defaultdict(list)\n",
    "        self.prs_by_package = defaultdict(list)\n",
    "        \n",
    "        for pr in self.pull_requests:\n",
    "            if pr.status == \"closed\":  # Only closed PRs as per paper\n",
    "                self.prs_by_repo['repository'].append(pr)\n",
    "                \n",
    "                for file_path in pr.files_changed:\n",
    "                    subsystem = Commit(None, None, None, []).get_subsystem(file_path)\n",
    "                    package = Commit(None, None, None, []).get_package(file_path)\n",
    "                    \n",
    "                    self.prs_by_subsystem[subsystem].append(pr)\n",
    "                    self.prs_by_package[package].append(pr)\n",
    "    \n",
    "    def calculate_aco(self, developer_id: str, granularity: str, \n",
    "                     target: str, review_timestamp: datetime) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Authoring Code Ownership (ACO) - Algorithm 1\n",
    "        ACO(D,G) = α(D,G) / C(G)\n",
    "        \"\"\"\n",
    "        if granularity == \"repository\":\n",
    "            commits_at_g = self.commits_by_repo[target]\n",
    "        elif granularity == \"subsystem\":\n",
    "            commits_at_g = self.commits_by_subsystem[target]\n",
    "        else:  # package\n",
    "            commits_at_g = self.commits_by_package[target]\n",
    "        \n",
    "        # Filter commits before review timestamp\n",
    "        prior_commits = [c for c in commits_at_g if c.timestamp < review_timestamp]\n",
    "        \n",
    "        if not prior_commits:\n",
    "            return 0.0\n",
    "        \n",
    "        # Count developer's commits\n",
    "        developer_commits = sum(1 for c in prior_commits if c.author_id == developer_id)\n",
    "        total_commits = len(prior_commits)\n",
    "        \n",
    "        return developer_commits / total_commits\n",
    "    \n",
    "    def calculate_rso(self, developer_id: str, granularity: str,\n",
    "                     target: str, review_timestamp: datetime) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Review-Specific Ownership (RSO) - Algorithm 2\n",
    "        RSO(D,G) = r(D,G) / ρ(G)\n",
    "        \"\"\"\n",
    "        if granularity == \"repository\":\n",
    "            prs_at_g = self.prs_by_repo[target]\n",
    "        elif granularity == \"subsystem\":\n",
    "            prs_at_g = self.prs_by_subsystem[target]\n",
    "        else:  # package\n",
    "            prs_at_g = self.prs_by_package[target]\n",
    "        \n",
    "        # Filter PRs before review timestamp\n",
    "        prior_prs = [pr for pr in prs_at_g if pr.timestamp < review_timestamp]\n",
    "        \n",
    "        if not prior_prs:\n",
    "            return 0.0\n",
    "        \n",
    "        # Count PRs reviewed by developer\n",
    "        developer_reviews = sum(1 for pr in prior_prs if developer_id in pr.reviewers)\n",
    "        total_prs = len(prior_prs)\n",
    "        \n",
    "        return developer_reviews / total_prs\n",
    "    \n",
    "    def calculate_all_metrics(self, developer_id: str, file_path: str, \n",
    "                            review_timestamp: datetime) -> Dict[str, float]:\n",
    "        \"\"\"Calculate all ownership metrics for a developer at a given time\"\"\"\n",
    "        # Determine granularity targets\n",
    "        subsystem = Commit(None, None, None, []).get_subsystem(file_path)\n",
    "        package = Commit(None, None, None, []).get_package(file_path)\n",
    "        \n",
    "        metrics = {\n",
    "            'aco_repo': self.calculate_aco(developer_id, \"repository\", \"repository\", review_timestamp),\n",
    "            'aco_sys': self.calculate_aco(developer_id, \"subsystem\", subsystem, review_timestamp),\n",
    "            'aco_pkg': self.calculate_aco(developer_id, \"package\", package, review_timestamp),\n",
    "            'rso_repo': self.calculate_rso(developer_id, \"repository\", \"repository\", review_timestamp),\n",
    "            'rso_sys': self.calculate_rso(developer_id, \"subsystem\", subsystem, review_timestamp),\n",
    "            'rso_pkg': self.calculate_rso(developer_id, \"package\", package, review_timestamp)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Create calculator and test\n",
    "calculator = OwnershipCalculator(commits, pull_requests)\n",
    "\n",
    "# Test calculation for a specific developer\n",
    "test_timestamp = datetime.now()\n",
    "test_file = \"src/core/engine.py\"\n",
    "test_developer = \"maintainer_0\"\n",
    "\n",
    "metrics = calculator.calculate_all_metrics(test_developer, test_file, test_timestamp)\n",
    "\n",
    "print(f\"Ownership Metrics for {test_developer} at {test_file}:\")\n",
    "print(f\"\\nAuthoring Code Ownership (ACO):\")\n",
    "print(f\"  Repository: {metrics['aco_repo']:.3f}\")\n",
    "print(f\"  Subsystem:  {metrics['aco_sys']:.3f}\")\n",
    "print(f\"  Package:    {metrics['aco_pkg']:.3f}\")\n",
    "print(f\"\\nReview-Specific Ownership (RSO):\")\n",
    "print(f\"  Repository: {metrics['rso_repo']:.3f}\")\n",
    "print(f\"  Subsystem:  {metrics['rso_sys']:.3f}\")\n",
    "print(f\"  Package:    {metrics['rso_pkg']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Ownership Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ownership_distributions(calculator: OwnershipCalculator, developers: List[Dict]):\n",
    "    \"\"\"Analyze and visualize ownership distributions across developers\"\"\"\n",
    "    \n",
    "    # Calculate metrics for all developers\n",
    "    all_metrics = []\n",
    "    timestamp = datetime.now()\n",
    "    \n",
    "    for dev in developers:\n",
    "        # Calculate for a common file\n",
    "        metrics = calculator.calculate_all_metrics(\n",
    "            dev['id'], \n",
    "            \"src/core/engine.py\", \n",
    "            timestamp\n",
    "        )\n",
    "        metrics['developer_id'] = dev['id']\n",
    "        metrics['developer_type'] = dev['type']\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(all_metrics)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot ACO distributions\n",
    "    for idx, gran in enumerate(['repo', 'sys', 'pkg']):\n",
    "        ax = axes[0, idx]\n",
    "        \n",
    "        # Group by developer type\n",
    "        for dev_type in ['maintainer', 'contributor', 'occasional']:\n",
    "            data = df_metrics[df_metrics['developer_type'] == dev_type][f'aco_{gran}']\n",
    "            ax.hist(data, alpha=0.6, label=dev_type, bins=20)\n",
    "        \n",
    "        ax.set_title(f'ACO Distribution - {gran.capitalize()} Level')\n",
    "        ax.set_xlabel('ACO Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot RSO distributions\n",
    "    for idx, gran in enumerate(['repo', 'sys', 'pkg']):\n",
    "        ax = axes[1, idx]\n",
    "        \n",
    "        # Group by developer type\n",
    "        for dev_type in ['maintainer', 'contributor', 'occasional']:\n",
    "            data = df_metrics[df_metrics['developer_type'] == dev_type][f'rso_{gran}']\n",
    "            ax.hist(data, alpha=0.6, label=dev_type, bins=20)\n",
    "        \n",
    "        ax.set_title(f'RSO Distribution - {gran.capitalize()} Level')\n",
    "        ax.set_xlabel('RSO Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = ['aco_repo', 'aco_sys', 'aco_pkg', 'rso_repo', 'rso_sys', 'rso_pkg']\n",
    "    correlation_matrix = df_metrics[numeric_cols].corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, ax=ax)\n",
    "    ax.set_title('Correlation between Ownership Metrics', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_metrics\n",
    "\n",
    "# Analyze ownership distributions\n",
    "df_metrics = analyze_ownership_distributions(calculator, generator.developers)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics by Developer Type:\")\n",
    "summary = df_metrics.groupby('developer_type')[['aco_repo', 'rso_repo']].agg(['mean', 'std'])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Dynamics of Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_ownership(calculator: OwnershipCalculator, \n",
    "                             developer_id: str,\n",
    "                             file_path: str,\n",
    "                             time_points: int = 12):\n",
    "    \"\"\"Analyze how ownership changes over time\"\"\"\n",
    "    \n",
    "    # Get time range from commits\n",
    "    min_time = min(c.timestamp for c in calculator.commits)\n",
    "    max_time = max(c.timestamp for c in calculator.commits)\n",
    "    \n",
    "    # Create time points\n",
    "    time_delta = (max_time - min_time) / time_points\n",
    "    timestamps = [min_time + time_delta * i for i in range(1, time_points + 1)]\n",
    "    \n",
    "    # Calculate metrics at each time point\n",
    "    temporal_metrics = []\n",
    "    for ts in timestamps:\n",
    "        metrics = calculator.calculate_all_metrics(developer_id, file_path, ts)\n",
    "        metrics['timestamp'] = ts\n",
    "        temporal_metrics.append(metrics)\n",
    "    \n",
    "    df_temporal = pd.DataFrame(temporal_metrics)\n",
    "    \n",
    "    # Visualize temporal evolution\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    # Plot ACO evolution\n",
    "    ax1.plot(df_temporal['timestamp'], df_temporal['aco_repo'], \n",
    "             'b-', label='Repository', linewidth=2)\n",
    "    ax1.plot(df_temporal['timestamp'], df_temporal['aco_sys'], \n",
    "             'g--', label='Subsystem', linewidth=2)\n",
    "    ax1.plot(df_temporal['timestamp'], df_temporal['aco_pkg'], \n",
    "             'r:', label='Package', linewidth=2)\n",
    "    ax1.set_ylabel('ACO Value')\n",
    "    ax1.set_title(f'Temporal Evolution of ACO for {developer_id}', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, max(df_temporal[['aco_repo', 'aco_sys', 'aco_pkg']].max()) * 1.1)\n",
    "    \n",
    "    # Plot RSO evolution\n",
    "    ax2.plot(df_temporal['timestamp'], df_temporal['rso_repo'], \n",
    "             'b-', label='Repository', linewidth=2)\n",
    "    ax2.plot(df_temporal['timestamp'], df_temporal['rso_sys'], \n",
    "             'g--', label='Subsystem', linewidth=2)\n",
    "    ax2.plot(df_temporal['timestamp'], df_temporal['rso_pkg'], \n",
    "             'r:', label='Package', linewidth=2)\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('RSO Value')\n",
    "    ax2.set_title(f'Temporal Evolution of RSO for {developer_id}', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, max(df_temporal[['rso_repo', 'rso_sys', 'rso_pkg']].max()) * 1.1)\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_temporal\n",
    "\n",
    "# Analyze temporal dynamics for a maintainer\n",
    "df_temporal = analyze_temporal_ownership(calculator, \"maintainer_0\", \"src/core/engine.py\")\n",
    "\n",
    "# Calculate growth rates\n",
    "print(\"\\nOwnership Growth Analysis:\")\n",
    "for metric in ['aco_repo', 'aco_sys', 'aco_pkg', 'rso_repo', 'rso_sys', 'rso_pkg']:\n",
    "    initial = df_temporal[metric].iloc[0]\n",
    "    final = df_temporal[metric].iloc[-1]\n",
    "    growth = (final - initial) / (initial + 1e-6) * 100  # Avoid division by zero\n",
    "    print(f\"{metric}: {initial:.3f} → {final:.3f} (Growth: {growth:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Granularity Analysis: Repository vs Subsystem vs Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_granularity_effects(calculator: OwnershipCalculator, developers: List[Dict]):\n",
    "    \"\"\"Analyze how granularity affects ownership metrics\"\"\"\n",
    "    \n",
    "    # Calculate metrics for multiple files at different levels\n",
    "    test_files = [\n",
    "        \"src/core/engine.py\",\n",
    "        \"src/api/routes.py\",\n",
    "        \"tests/unit/test_engine.py\",\n",
    "        \"docs/api.md\"\n",
    "    ]\n",
    "    \n",
    "    timestamp = datetime.now()\n",
    "    results = []\n",
    "    \n",
    "    for dev in developers[:10]:  # Top 10 developers\n",
    "        for file_path in test_files:\n",
    "            metrics = calculator.calculate_all_metrics(dev['id'], file_path, timestamp)\n",
    "            \n",
    "            result = {\n",
    "                'developer': dev['id'],\n",
    "                'dev_type': dev['type'],\n",
    "                'file': file_path,\n",
    "                'subsystem': Commit(None, None, None, []).get_subsystem(file_path),\n",
    "                'package': Commit(None, None, None, []).get_package(file_path),\n",
    "                **metrics\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    df_gran = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualization 1: Ownership increase by granularity\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Calculate average increase factors\n",
    "    df_gran['aco_sys_factor'] = df_gran['aco_sys'] / (df_gran['aco_repo'] + 1e-6)\n",
    "    df_gran['aco_pkg_factor'] = df_gran['aco_pkg'] / (df_gran['aco_repo'] + 1e-6)\n",
    "    df_gran['rso_sys_factor'] = df_gran['rso_sys'] / (df_gran['rso_repo'] + 1e-6)\n",
    "    df_gran['rso_pkg_factor'] = df_gran['rso_pkg'] / (df_gran['rso_repo'] + 1e-6)\n",
    "    \n",
    "    # Box plot of increase factors\n",
    "    factor_data = [\n",
    "        df_gran['aco_sys_factor'].dropna(),\n",
    "        df_gran['aco_pkg_factor'].dropna(),\n",
    "        df_gran['rso_sys_factor'].dropna(),\n",
    "        df_gran['rso_pkg_factor'].dropna()\n",
    "    ]\n",
    "    \n",
    "    positions = [1, 2, 4, 5]\n",
    "    colors = ['lightblue', 'darkblue', 'lightgreen', 'darkgreen']\n",
    "    \n",
    "    bp = ax1.boxplot(factor_data, positions=positions, widths=0.6, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax1.set_xticks([1.5, 4.5])\n",
    "    ax1.set_xticklabels(['ACO', 'RSO'])\n",
    "    ax1.set_ylabel('Ownership Increase Factor')\n",
    "    ax1.set_title('Ownership Increase from Repository to Finer Granularities')\n",
    "    ax1.axhline(y=1, color='red', linestyle='--', alpha=0.5)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightblue', label='Subsystem/Repo'),\n",
    "        Patch(facecolor='darkblue', label='Package/Repo')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements)\n",
    "    \n",
    "    # Visualization 2: Scatter plot of ACO vs RSO at different granularities\n",
    "    for idx, (gran, marker) in enumerate([('repo', 'o'), ('sys', 's'), ('pkg', '^')]):\n",
    "        ax2.scatter(df_gran[f'aco_{gran}'], df_gran[f'rso_{gran}'], \n",
    "                   label=f'{gran.capitalize()} level',\n",
    "                   alpha=0.6, s=100, marker=marker)\n",
    "    \n",
    "    ax2.set_xlabel('ACO (Authoring Code Ownership)')\n",
    "    ax2.set_ylabel('RSO (Review-Specific Ownership)')\n",
    "    ax2.set_title('ACO vs RSO Relationship at Different Granularities')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    max_val = max(ax2.get_xlim()[1], ax2.get_ylim()[1])\n",
    "    ax2.plot([0, max_val], [0, max_val], 'k--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\nGranularity Effect Statistics:\")\n",
    "    print(\"\\nAverage Ownership Increase Factors:\")\n",
    "    print(f\"ACO Subsystem/Repository: {df_gran['aco_sys_factor'].mean():.2f}x\")\n",
    "    print(f\"ACO Package/Repository: {df_gran['aco_pkg_factor'].mean():.2f}x\")\n",
    "    print(f\"RSO Subsystem/Repository: {df_gran['rso_sys_factor'].mean():.2f}x\")\n",
    "    print(f\"RSO Package/Repository: {df_gran['rso_pkg_factor'].mean():.2f}x\")\n",
    "    \n",
    "    return df_gran\n",
    "\n",
    "# Analyze granularity effects\n",
    "df_granularity = analyze_granularity_effects(calculator, generator.developers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Efficient Implementation for Large-Scale Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedOwnershipCalculator:\n",
    "    \"\"\"Optimized implementation for processing millions of commits/PRs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.commit_cache = {}\n",
    "        self.pr_cache = {}\n",
    "        self.metric_cache = {}\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "    def _get_cache_key(self, developer_id: str, granularity: str, \n",
    "                      target: str, timestamp: datetime) -> str:\n",
    "        \"\"\"Generate cache key for metrics\"\"\"\n",
    "        # Round timestamp to nearest hour for better cache hits\n",
    "        rounded_ts = timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "        return f\"{developer_id}:{granularity}:{target}:{rounded_ts.isoformat()}\"\n",
    "    \n",
    "    def batch_calculate_metrics(self, review_comments: List[ReviewComment],\n",
    "                              commits: List[Commit],\n",
    "                              pull_requests: List[PullRequest]) -> pd.DataFrame:\n",
    "        \"\"\"Efficiently calculate metrics for a batch of review comments\"\"\"\n",
    "        \n",
    "        # Build indices for fast lookup\n",
    "        print(\"Building indices...\")\n",
    "        commit_index = self._build_commit_index(commits)\n",
    "        pr_index = self._build_pr_index(pull_requests)\n",
    "        \n",
    "        results = []\n",
    "        total = len(review_comments)\n",
    "        \n",
    "        print(f\"Processing {total} review comments...\")\n",
    "        for idx, comment in enumerate(review_comments):\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Progress: {idx}/{total} ({idx/total*100:.1f}%)\")\n",
    "            \n",
    "            # Check cache first\n",
    "            cache_key = self._get_cache_key(\n",
    "                comment.reviewer_id,\n",
    "                \"all\",\n",
    "                comment.file_path,\n",
    "                comment.timestamp\n",
    "            )\n",
    "            \n",
    "            if cache_key in self.metric_cache:\n",
    "                metrics = self.metric_cache[cache_key]\n",
    "                self.cache_hits += 1\n",
    "            else:\n",
    "                # Calculate metrics\n",
    "                metrics = self._calculate_metrics_fast(\n",
    "                    comment.reviewer_id,\n",
    "                    comment.file_path,\n",
    "                    comment.timestamp,\n",
    "                    commit_index,\n",
    "                    pr_index\n",
    "                )\n",
    "                self.metric_cache[cache_key] = metrics\n",
    "                self.cache_misses += 1\n",
    "            \n",
    "            result = {\n",
    "                'comment_id': comment.comment_id,\n",
    "                'reviewer_id': comment.reviewer_id,\n",
    "                'timestamp': comment.timestamp,\n",
    "                'file_path': comment.file_path,\n",
    "                **metrics\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        print(f\"\\nCache performance: {self.cache_hits} hits, {self.cache_misses} misses\")\n",
    "        print(f\"Cache hit rate: {self.cache_hits/(self.cache_hits+self.cache_misses)*100:.1f}%\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _build_commit_index(self, commits: List[Commit]) -> Dict:\n",
    "        \"\"\"Build efficient index for commits\"\"\"\n",
    "        index = {\n",
    "            'by_author': defaultdict(list),\n",
    "            'by_subsystem': defaultdict(list),\n",
    "            'by_package': defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        for commit in commits:\n",
    "            if not commit.is_merge:\n",
    "                index['by_author'][commit.author_id].append(commit)\n",
    "                \n",
    "                for file_path in commit.files_changed:\n",
    "                    subsystem = commit.get_subsystem(file_path)\n",
    "                    package = commit.get_package(file_path)\n",
    "                    index['by_subsystem'][subsystem].append(commit)\n",
    "                    index['by_package'][package].append(commit)\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def _build_pr_index(self, pull_requests: List[PullRequest]) -> Dict:\n",
    "        \"\"\"Build efficient index for pull requests\"\"\"\n",
    "        index = {\n",
    "            'by_reviewer': defaultdict(list),\n",
    "            'by_subsystem': defaultdict(list),\n",
    "            'by_package': defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        for pr in pull_requests:\n",
    "            if pr.status == \"closed\":\n",
    "                for reviewer_id in pr.reviewers:\n",
    "                    index['by_reviewer'][reviewer_id].append(pr)\n",
    "                \n",
    "                for file_path in pr.files_changed:\n",
    "                    subsystem = Commit(None, None, None, []).get_subsystem(file_path)\n",
    "                    package = Commit(None, None, None, []).get_package(file_path)\n",
    "                    index['by_subsystem'][subsystem].append(pr)\n",
    "                    index['by_package'][package].append(pr)\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def _calculate_metrics_fast(self, developer_id: str, file_path: str,\n",
    "                              timestamp: datetime, commit_index: Dict,\n",
    "                              pr_index: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Fast calculation using indices\"\"\"\n",
    "        subsystem = Commit(None, None, None, []).get_subsystem(file_path)\n",
    "        package = Commit(None, None, None, []).get_package(file_path)\n",
    "        \n",
    "        # Calculate ACO metrics\n",
    "        aco_repo = self._calculate_aco_fast(\n",
    "            developer_id, \n",
    "            [c for author_commits in commit_index['by_author'].values() \n",
    "             for c in author_commits],\n",
    "            timestamp\n",
    "        )\n",
    "        \n",
    "        aco_sys = self._calculate_aco_fast(\n",
    "            developer_id,\n",
    "            commit_index['by_subsystem'][subsystem],\n",
    "            timestamp\n",
    "        )\n",
    "        \n",
    "        aco_pkg = self._calculate_aco_fast(\n",
    "            developer_id,\n",
    "            commit_index['by_package'][package],\n",
    "            timestamp\n",
    "        )\n",
    "        \n",
    "        # Calculate RSO metrics\n",
    "        rso_repo = self._calculate_rso_fast(\n",
    "            developer_id,\n",
    "            [pr for reviewer_prs in pr_index['by_reviewer'].values() \n",
    "             for pr in reviewer_prs],\n",
    "            timestamp\n",
    "        )\n",
    "        \n",
    "        rso_sys = self._calculate_rso_fast(\n",
    "            developer_id,\n",
    "            pr_index['by_subsystem'][subsystem],\n",
    "            timestamp\n",
    "        )\n",
    "        \n",
    "        rso_pkg = self._calculate_rso_fast(\n",
    "            developer_id,\n",
    "            pr_index['by_package'][package],\n",
    "            timestamp\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'aco_repo': aco_repo,\n",
    "            'aco_sys': aco_sys,\n",
    "            'aco_pkg': aco_pkg,\n",
    "            'rso_repo': rso_repo,\n",
    "            'rso_sys': rso_sys,\n",
    "            'rso_pkg': rso_pkg\n",
    "        }\n",
    "    \n",
    "    def _calculate_aco_fast(self, developer_id: str, commits: List[Commit],\n",
    "                           timestamp: datetime) -> float:\n",
    "        \"\"\"Fast ACO calculation\"\"\"\n",
    "        prior_commits = [c for c in commits if c.timestamp < timestamp]\n",
    "        if not prior_commits:\n",
    "            return 0.0\n",
    "        \n",
    "        developer_commits = sum(1 for c in prior_commits if c.author_id == developer_id)\n",
    "        return developer_commits / len(prior_commits)\n",
    "    \n",
    "    def _calculate_rso_fast(self, developer_id: str, prs: List[PullRequest],\n",
    "                           timestamp: datetime) -> float:\n",
    "        \"\"\"Fast RSO calculation\"\"\"\n",
    "        prior_prs = [pr for pr in prs if pr.timestamp < timestamp]\n",
    "        if not prior_prs:\n",
    "            return 0.0\n",
    "        \n",
    "        developer_reviews = sum(1 for pr in prior_prs if developer_id in pr.reviewers)\n",
    "        return developer_reviews / len(prior_prs)\n",
    "\n",
    "# Demonstrate optimized calculation\n",
    "print(\"Generating mock review comments...\")\n",
    "mock_comments = []\n",
    "for i in range(500):  # Simulate 500 review comments\n",
    "    reviewer = np.random.choice(generator.developers)\n",
    "    file_path = np.random.choice(generator.file_structure)\n",
    "    \n",
    "    comment = ReviewComment(\n",
    "        comment_id=f\"comment_{i:04d}\",\n",
    "        reviewer_id=reviewer['id'],\n",
    "        pr_id=f\"pr_{np.random.randint(0, 300):04d}\",\n",
    "        timestamp=datetime.now() - timedelta(days=np.random.randint(0, 365)),\n",
    "        file_path=file_path,\n",
    "        repository=\"mock_repo\",\n",
    "        content=\"Mock review comment\"\n",
    "    )\n",
    "    mock_comments.append(comment)\n",
    "\n",
    "# Calculate metrics\n",
    "opt_calculator = OptimizedOwnershipCalculator()\n",
    "df_results = opt_calculator.batch_calculate_metrics(mock_comments, commits, pull_requests)\n",
    "\n",
    "print(f\"\\nProcessed {len(df_results)} comments\")\n",
    "print(\"\\nSample results:\")\n",
    "print(df_results.head())\n",
    "\n",
    "# Performance statistics\n",
    "print(\"\\nOwnership Statistics:\")\n",
    "print(df_results[['aco_repo', 'aco_sys', 'aco_pkg', 'rso_repo', 'rso_sys', 'rso_pkg']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Challenges and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnershipChallenges:\n",
    "    \"\"\"Common challenges when calculating ownership metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_bot_accounts(commits: List[Commit], pull_requests: List[PullRequest]):\n",
    "        \"\"\"Filter out bot accounts from ownership calculations\"\"\"\n",
    "        print(\"Challenge: Bot Account Detection\")\n",
    "        \n",
    "        # Common bot patterns\n",
    "        bot_patterns = [\n",
    "            lambda x: x.endswith('[bot]'),\n",
    "            lambda x: x.endswith('-bot'),\n",
    "            lambda x: 'dependabot' in x.lower(),\n",
    "            lambda x: 'renovate' in x.lower(),\n",
    "            lambda x: 'github-actions' in x.lower()\n",
    "        ]\n",
    "        \n",
    "        def is_bot(user_id: str) -> bool:\n",
    "            return any(pattern(user_id) for pattern in bot_patterns)\n",
    "        \n",
    "        # Filter commits\n",
    "        human_commits = [c for c in commits if not is_bot(c.author_id)]\n",
    "        bot_commits = [c for c in commits if is_bot(c.author_id)]\n",
    "        \n",
    "        print(f\"Filtered {len(bot_commits)} bot commits out of {len(commits)}\")\n",
    "        print(f\"Remaining human commits: {len(human_commits)}\")\n",
    "        \n",
    "        return human_commits\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_file_renames(file_history: Dict[str, List[str]]):\n",
    "        \"\"\"Track file renames to maintain accurate ownership\"\"\"\n",
    "        print(\"\\nChallenge: File Rename Tracking\")\n",
    "        \n",
    "        # Build rename graph\n",
    "        rename_graph = nx.DiGraph()\n",
    "        \n",
    "        for old_path, new_paths in file_history.items():\n",
    "            for new_path in new_paths:\n",
    "                rename_graph.add_edge(old_path, new_path)\n",
    "        \n",
    "        # Find connected components (files that are the same through renames)\n",
    "        file_groups = list(nx.weakly_connected_components(rename_graph.to_undirected()))\n",
    "        \n",
    "        print(f\"Found {len(file_groups)} unique files after resolving renames\")\n",
    "        \n",
    "        # Create mapping\n",
    "        file_mapping = {}\n",
    "        for group in file_groups:\n",
    "            canonical_name = sorted(group)[0]  # Use first name alphabetically\n",
    "            for file_name in group:\n",
    "                file_mapping[file_name] = canonical_name\n",
    "        \n",
    "        return file_mapping\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_large_scale_data():\n",
    "        \"\"\"Strategies for handling millions of commits/PRs\"\"\"\n",
    "        print(\"\\nChallenge: Large-Scale Data Processing\")\n",
    "        print(\"Solutions:\")\n",
    "        print(\"1. Use incremental processing with checkpoints\")\n",
    "        print(\"2. Implement parallel processing for independent calculations\")\n",
    "        print(\"3. Use database indexing for fast lookups\")\n",
    "        print(\"4. Implement time-based partitioning\")\n",
    "        print(\"5. Cache frequently accessed metrics\")\n",
    "        \n",
    "        # Example: Time-based partitioning\n",
    "        class TimePartitionedCalculator:\n",
    "            def __init__(self, partition_days=30):\n",
    "                self.partition_days = partition_days\n",
    "                self.partitions = {}\n",
    "            \n",
    "            def add_to_partition(self, item, timestamp):\n",
    "                partition_key = timestamp.date() // timedelta(days=self.partition_days)\n",
    "                if partition_key not in self.partitions:\n",
    "                    self.partitions[partition_key] = []\n",
    "                self.partitions[partition_key].append(item)\n",
    "        \n",
    "        return TimePartitionedCalculator\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_data_quality_issues():\n",
    "        \"\"\"Visualize common data quality problems\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Issue 1: Missing author information\n",
    "        ax1 = axes[0, 0]\n",
    "        missing_rates = np.random.beta(2, 20, 12)  # Monthly missing rates\n",
    "        months = pd.date_range('2023-01', periods=12, freq='MS')\n",
    "        ax1.plot(months, missing_rates * 100, 'r-', marker='o')\n",
    "        ax1.set_title('Missing Author Information Over Time')\n",
    "        ax1.set_ylabel('Missing Rate (%)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Issue 2: Bot activity spikes\n",
    "        ax2 = axes[0, 1]\n",
    "        human_commits = np.random.poisson(100, 52)  # Weekly\n",
    "        bot_commits = np.random.poisson(20, 52)\n",
    "        bot_commits[10:15] = np.random.poisson(100, 5)  # Spike\n",
    "        \n",
    "        weeks = range(52)\n",
    "        ax2.bar(weeks, human_commits, label='Human', alpha=0.7)\n",
    "        ax2.bar(weeks, bot_commits, bottom=human_commits, label='Bot', alpha=0.7)\n",
    "        ax2.set_title('Weekly Commit Activity (Human vs Bot)')\n",
    "        ax2.set_xlabel('Week')\n",
    "        ax2.set_ylabel('Number of Commits')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Issue 3: File rename frequency\n",
    "        ax3 = axes[1, 0]\n",
    "        subsystems = ['src', 'tests', 'docs', 'config', 'scripts']\n",
    "        rename_counts = [45, 23, 12, 5, 8]\n",
    "        ax3.bar(subsystems, rename_counts, color='orange')\n",
    "        ax3.set_title('File Renames by Subsystem')\n",
    "        ax3.set_ylabel('Number of Renames')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Issue 4: Reviewer coverage gaps\n",
    "        ax4 = axes[1, 1]\n",
    "        # Create heatmap data\n",
    "        developers = [f'Dev{i}' for i in range(10)]\n",
    "        subsystems = ['src/core', 'src/api', 'tests', 'docs']\n",
    "        coverage = np.random.random((len(developers), len(subsystems)))\n",
    "        coverage[5:8, 2:] = 0  # Coverage gaps\n",
    "        \n",
    "        im = ax4.imshow(coverage, cmap='YlOrRd', aspect='auto')\n",
    "        ax4.set_xticks(range(len(subsystems)))\n",
    "        ax4.set_xticklabels(subsystems, rotation=45)\n",
    "        ax4.set_yticks(range(len(developers)))\n",
    "        ax4.set_yticklabels(developers)\n",
    "        ax4.set_title('Review Coverage Heatmap')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax4)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate challenges\n",
    "challenges = OwnershipChallenges()\n",
    "\n",
    "# Handle bot accounts\n",
    "filtered_commits = challenges.handle_bot_accounts(commits, pull_requests)\n",
    "\n",
    "# Handle file renames\n",
    "mock_renames = {\n",
    "    'src/old_module.py': ['src/core/module.py'],\n",
    "    'src/core/module.py': ['src/core/engine.py'],\n",
    "    'tests/test_old.py': ['tests/unit/test_engine.py']\n",
    "}\n",
    "file_mapping = challenges.handle_file_renames(mock_renames)\n",
    "\n",
    "# Show large-scale strategies\n",
    "challenges.handle_large_scale_data()\n",
    "\n",
    "# Visualize data quality issues\n",
    "challenges.visualize_data_quality_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration with ELF and Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_elf_integration(df_metrics: pd.DataFrame):\n",
    "    \"\"\"Show how ownership metrics integrate with ELF\"\"\"\n",
    "    \n",
    "    # Calculate ELF weights for each metric combination\n",
    "    strategies = ['aco', 'rso', 'avg', 'max']\n",
    "    granularities = ['repo', 'sys', 'pkg']\n",
    "    \n",
    "    # Add ELF weights to dataframe\n",
    "    for strategy in strategies:\n",
    "        for gran in granularities:\n",
    "            if strategy == 'aco':\n",
    "                df_metrics[f'weight_{strategy}_{gran}'] = np.exp(1 + df_metrics[f'aco_{gran}'])\n",
    "            elif strategy == 'rso':\n",
    "                df_metrics[f'weight_{strategy}_{gran}'] = np.exp(1 + df_metrics[f'rso_{gran}'])\n",
    "            elif strategy == 'avg':\n",
    "                df_metrics[f'weight_{strategy}_{gran}'] = np.exp(1 + \n",
    "                    (df_metrics[f'aco_{gran}'] + df_metrics[f'rso_{gran}']) / 2)\n",
    "            else:  # max\n",
    "                df_metrics[f'weight_{strategy}_{gran}'] = np.exp(1 + \n",
    "                    np.maximum(df_metrics[f'aco_{gran}'], df_metrics[f'rso_{gran}']))\n",
    "    \n",
    "    # Visualize weight distributions\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    \n",
    "    for i, gran in enumerate(granularities):\n",
    "        for j, strategy in enumerate(strategies):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Plot weight distribution\n",
    "            weights = df_metrics[f'weight_{strategy}_{gran}']\n",
    "            ax.hist(weights, bins=30, alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_w = weights.mean()\n",
    "            std_w = weights.std()\n",
    "            ax.axvline(mean_w, color='red', linestyle='--', label=f'μ={mean_w:.2f}')\n",
    "            \n",
    "            ax.set_title(f'{strategy.upper()} - {gran.capitalize()} Level')\n",
    "            ax.set_xlabel('ELF Weight')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show impact on training\n",
    "    print(\"\\nELF Weight Impact Analysis:\")\n",
    "    print(\"\\nWeight Ratios (High Experience / Low Experience):\")\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        for gran in granularities:\n",
    "            weight_col = f'weight_{strategy}_{gran}'\n",
    "            high_exp = df_metrics[df_metrics['developer_type'] == 'maintainer'][weight_col].mean()\n",
    "            low_exp = df_metrics[df_metrics['developer_type'] == 'occasional'][weight_col].mean()\n",
    "            ratio = high_exp / low_exp if low_exp > 0 else np.inf\n",
    "            \n",
    "            print(f\"{strategy}_{gran}: {ratio:.2f}x\")\n",
    "    \n",
    "    return df_metrics\n",
    "\n",
    "# Integrate with ELF\n",
    "df_with_weights = demonstrate_elf_integration(df_metrics)\n",
    "\n",
    "# Best configuration analysis\n",
    "print(\"\\nBest ELF Configurations (highest weight differentiation):\")\n",
    "weight_cols = [col for col in df_with_weights.columns if col.startswith('weight_')]\n",
    "weight_stds = df_with_weights[weight_cols].std().sort_values(ascending=False)\n",
    "print(weight_stds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### Core Concepts Mastered\n",
    "1. **ACO Formula**: α(D,G) / C(G) - Proportion of commits by developer\n",
    "2. **RSO Formula**: r(D,G) / ρ(G) - Proportion of PRs reviewed\n",
    "3. **Granularity Levels**: Repository → Subsystem → Package (increasing specialization)\n",
    "4. **Temporal Aspects**: Metrics calculated at review timestamp\n",
    "\n",
    "### Implementation Insights\n",
    "1. **Preprocessing is Critical**: Index data by granularity for efficiency\n",
    "2. **Cache Aggressively**: Ownership values change slowly\n",
    "3. **Handle Edge Cases**: Bot accounts, file renames, missing data\n",
    "4. **Batch Processing**: Essential for large-scale datasets\n",
    "\n",
    "### Key Findings\n",
    "1. **Ownership Increases at Finer Granularities**: ~1.5-2x at package level\n",
    "2. **ACO < RSO**: Developers review more broadly than they code\n",
    "3. **High Correlation**: But not perfect - capturing different aspects\n",
    "4. **Temporal Stability**: Ownership evolves gradually over months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final implementation template\n",
    "class ProductionOwnershipCalculator:\n",
    "    \"\"\"Production-ready ownership calculator template\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.cache_size = config.get('cache_size', 10000)\n",
    "        self.batch_size = config.get('batch_size', 1000)\n",
    "        self.parallel_workers = config.get('parallel_workers', 4)\n",
    "        self.bot_patterns = config.get('bot_patterns', [])\n",
    "        \n",
    "    def process_repository(self, repo_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process entire repository to calculate ownership metrics\n",
    "        \n",
    "        Steps:\n",
    "        1. Extract commit history\n",
    "        2. Extract PR/review history  \n",
    "        3. Filter bot accounts\n",
    "        4. Handle file renames\n",
    "        5. Calculate metrics in batches\n",
    "        6. Save results\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def update_metrics_incremental(self, new_commits, new_prs):\n",
    "        \"\"\"Incrementally update metrics with new data\"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Ownership Metrics Implementation Complete!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Apply to your repository using PyGithub/PyDriller\")\n",
    "print(\"2. Experiment with different granularity levels\")\n",
    "print(\"3. Analyze ownership patterns in your codebase\")\n",
    "print(\"4. Integrate with ELF for model training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}