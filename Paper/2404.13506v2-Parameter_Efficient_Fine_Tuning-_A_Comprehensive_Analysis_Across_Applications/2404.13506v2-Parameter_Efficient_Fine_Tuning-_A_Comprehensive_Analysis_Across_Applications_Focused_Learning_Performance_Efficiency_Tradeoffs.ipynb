{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Learning: Performance-Efficiency Tradeoffs in PEFT Methods\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the fundamental tradeoffs between performance and efficiency in PEFT methods\n",
    "- Explore how to measure and quantify these tradeoffs systematically\n",
    "- Implement experiments to evaluate different aspects of the efficiency-performance spectrum\n",
    "- Develop a framework for selecting optimal PEFT configurations based on specific requirements\n",
    "\n",
    "## Paper Reference\n",
    "This notebook explores concepts from the paper \"Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications\" (arXiv:2404.13506v2).\n",
    "\n",
    "Specifically, we focus on Section 4 which discusses the evaluation considerations and tradeoffs in PEFT methods:\n",
    "\n",
    "> \"PEFT has emerged as a compelling approach for tailoring large pre-trained models to specific tasks while minimizing computational demands. Our review found that leveraging PEFT across diverse applications presents several key challenges that require careful consideration, as practitioners consider applying PEFT for their applications:\" (Section 4, Page 6)\n",
    "\n",
    "The paper highlights the key tradeoff: \"A) Balancing Efficiency and Performance: A core challenge lies in striking a delicate balance between reducing trainable parameters and maintaining robust performance.\" (Section 4, Page 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Performance-Efficiency Tradeoffs\n",
    "\n",
    "Parameter-Efficient Fine-Tuning (PEFT) methods aim to reduce the number of trainable parameters while maintaining performance comparable to full fine-tuning. However, there is an inherent tradeoff between efficiency and performance that must be carefully navigated.\n",
    "\n",
    "In this notebook, we'll explore the multi-dimensional nature of this tradeoff, considering not just parameter counts but also:\n",
    "- Computational efficiency (training time, memory usage)\n",
    "- Task performance (accuracy, F1 score, etc.)\n",
    "- Generalization capabilities\n",
    "- Data efficiency requirements\n",
    "- Inference speed and resource needs\n",
    "\n",
    "We'll implement experiments to quantify these tradeoffs and develop a framework for selecting the optimal PEFT configuration based on specific requirements and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install torch transformers datasets peft matplotlib numpy pandas seaborn scikit-learn memory_profiler psutil tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from memory_profiler import profile\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    "    TaskType,\n",
    "    PeftType,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    BitFitConfig,\n",
    "    AdaLoraConfig\n",
    ")\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Efficiency-Performance Dimensions\n",
    "\n",
    "Before we can effectively evaluate the tradeoffs, we need to define the key dimensions that characterize the efficiency-performance spectrum of PEFT methods. Based on the paper, these include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key dimensions for evaluating efficiency-performance tradeoffs\n",
    "efficiency_performance_dimensions = {\n",
    "    \"Parameter Efficiency\": {\n",
    "        \"metrics\": [\n",
    "            \"Trainable parameter count\",\n",
    "            \"Parameter percentage (relative to full model)\",\n",
    "            \"Parameter reduction ratio\"\n",
    "        ],\n",
    "        \"importance\": \"Primary measure of PEFT efficiency; lower is generally better\",\n",
    "        \"measurement\": \"Count parameters that require gradients\"\n",
    "    },\n",
    "    \"Computational Efficiency\": {\n",
    "        \"metrics\": [\n",
    "            \"Training time (epochs/hour)\",\n",
    "            \"Memory usage (peak GPU/RAM)\",\n",
    "            \"FLOPs per training step\"\n",
    "        ],\n",
    "        \"importance\": \"Critical for resource-constrained environments\",\n",
    "        \"measurement\": \"Time training runs, monitor memory usage, count operations\"\n",
    "    },\n",
    "    \"Task Performance\": {\n",
    "        \"metrics\": [\n",
    "            \"Accuracy\",\n",
    "            \"F1 score\",\n",
    "            \"Domain-specific metrics (BLEU, ROUGE, etc.)\",\n",
    "            \"Performance gap vs. full fine-tuning\"\n",
    "        ],\n",
    "        \"importance\": \"The core measure of model effectiveness on the target task\",\n",
    "        \"measurement\": \"Evaluate on validation/test sets\"\n",
    "    },\n",
    "    \"Generalization\": {\n",
    "        \"metrics\": [\n",
    "            \"Performance on out-of-distribution data\",\n",
    "            \"Transfer learning capability\",\n",
    "            \"Robustness to perturbations\"\n",
    "        ],\n",
    "        \"importance\": \"Measures how well the model generalizes beyond training data\",\n",
    "        \"measurement\": \"Test on varied datasets, evaluate with perturbations\"\n",
    "    },\n",
    "    \"Data Efficiency\": {\n",
    "        \"metrics\": [\n",
    "            \"Performance vs. training data size\",\n",
    "            \"Few-shot capabilities\",\n",
    "            \"Sample efficiency curves\"\n",
    "        ],\n",
    "        \"importance\": \"Critical in domains with limited data availability\",\n",
    "        \"measurement\": \"Train with varying dataset sizes, measure performance\"\n",
    "    },\n",
    "    \"Inference Efficiency\": {\n",
    "        \"metrics\": [\n",
    "            \"Inference time (samples/second)\",\n",
    "            \"Inference memory usage\",\n",
    "            \"Deployment size\"\n",
    "        ],\n",
    "        \"importance\": \"Important for production deployment scenarios\",\n",
    "        \"measurement\": \"Time inference runs, monitor resource usage\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the dimensions and their metrics\n",
    "print(\"Efficiency-Performance Dimensions for PEFT Evaluation\")\n",
    "print(\"===================================================\")\n",
    "for dimension, details in efficiency_performance_dimensions.items():\n",
    "    print(f\"\\n{dimension}\")\n",
    "    print(\"-\" * len(dimension))\n",
    "    print(f\"Importance: {details['importance']}\")\n",
    "    print(\"Metrics:\")\n",
    "    for metric in details['metrics']:\n",
    "        print(f\"  - {metric}\")\n",
    "    print(f\"Measurement: {details['measurement']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Measurement Functions\n",
    "\n",
    "Now, let's implement functions to measure these efficiency and performance metrics for different PEFT methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_parameter_efficiency(model, full_model=None):\n",
    "    \"\"\"Measure parameter efficiency metrics\"\"\"\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    if full_model is not None:\n",
    "        full_trainable_params = sum(p.numel() for p in full_model.parameters() if p.requires_grad)\n",
    "        param_percentage = (trainable_params / full_trainable_params) * 100\n",
    "        param_reduction = 1 - (trainable_params / full_trainable_params)\n",
    "    else:\n",
    "        full_trainable_params = total_params\n",
    "        param_percentage = (trainable_params / total_params) * 100\n",
    "        param_reduction = 1 - (trainable_params / total_params)\n",
    "    \n",
    "    return {\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"total_params\": total_params,\n",
    "        \"param_percentage\": param_percentage,\n",
    "        \"param_reduction\": param_reduction\n",
    "    }\n",
    "\n",
    "def measure_computational_efficiency(training_func, model, dataset, batch_size=8, num_epochs=1):\n",
    "    \"\"\"Measure computational efficiency metrics during training\"\"\"\n",
    "    # Start time and memory tracking\n",
    "    start_time = time.time()\n",
    "    start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "    \n",
    "    # Run the training function\n",
    "    training_func(model, dataset, batch_size, num_epochs)\n",
    "    \n",
    "    # End time and memory tracking\n",
    "    end_time = time.time()\n",
    "    end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        end_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
    "        peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB\n",
    "    else:\n",
    "        end_gpu_memory = 0\n",
    "        peak_gpu_memory = 0\n",
    "    \n",
    "    # Calculate metrics\n",
    "    training_time = end_time - start_time\n",
    "    memory_usage = end_memory - start_memory\n",
    "    gpu_memory_usage = end_gpu_memory - start_gpu_memory\n",
    "    \n",
    "    # Calculate samples per second\n",
    "    total_samples = len(dataset) * num_epochs\n",
    "    samples_per_second = total_samples / training_time\n",
    "    \n",
    "    return {\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"training_time_per_epoch\": training_time / num_epochs,\n",
    "        \"samples_per_second\": samples_per_second,\n",
    "        \"memory_usage_mb\": memory_usage,\n",
    "        \"peak_gpu_memory_mb\": peak_gpu_memory,\n",
    "        \"gpu_memory_usage_mb\": gpu_memory_usage\n",
    "    }\n",
    "\n",
    "def measure_task_performance(model, eval_dataset, metric_func):\n",
    "    \"\"\"Measure task performance metrics\"\"\"\n",
    "    # Run evaluation\n",
    "    results = metric_func(model, eval_dataset)\n",
    "    \n",
    "    # If the results are a single value, wrap it in a dict\n",
    "    if not isinstance(results, dict):\n",
    "        results = {\"performance\": results}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def measure_inference_efficiency(model, dataset, batch_size=1):\n",
    "    \"\"\"Measure inference efficiency metrics\"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Create a dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Start timing and memory tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Process batch\n",
    "            if isinstance(batch, dict):\n",
    "                outputs = model(**{k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)})\n",
    "            else:\n",
    "                outputs = model(batch[0].to(device))\n",
    "    \n",
    "    # End timing and memory tracking\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB\n",
    "    else:\n",
    "        peak_gpu_memory = 0\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inference_time = end_time - start_time\n",
    "    samples_per_second = len(dataset) / inference_time\n",
    "    \n",
    "    return {\n",
    "        \"inference_time_seconds\": inference_time,\n",
    "        \"samples_per_second\": samples_per_second,\n",
    "        \"peak_gpu_memory_mb\": peak_gpu_memory\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment Setup: Testing Tradeoffs on a Classification Task\n",
    "\n",
    "Let's set up an experiment to evaluate the efficiency-performance tradeoffs of different PEFT methods on a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset for our experiment (SST-2 sentiment classification)\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples, tokenizer, max_length=128):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(\n",
    "    lambda examples: preprocess_function(examples, tokenizer),\n",
    "    batched=True\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Define a training function for our experiments\n",
    "def train_model(model, train_dataset, batch_size=16, num_epochs=3):\n",
    "    \"\"\"Training function for efficiency measurement\"\"\"\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=len(train_dataset) // batch_size // 4,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Define trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Define an evaluation function\n",
    "def evaluate_model(model, eval_dataset):\n",
    "    \"\"\"Evaluation function for performance measurement\"\"\"\n",
    "    # Define evaluation arguments\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=\"./eval_results\",\n",
    "        per_device_eval_batch_size=32,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Define evaluator\n",
    "    evaluator = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = evaluator.evaluate()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PEFT Methods Configuration\n",
    "\n",
    "Now, let's define the PEFT methods we want to compare and their configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peft_configurations():\n",
    "    \"\"\"Define PEFT configurations to compare\"\"\"\n",
    "    peft_configs = {\n",
    "        \"LoRA-r4\": LoraConfig(\n",
    "            r=4,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"LoRA-r8\": LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"LoRA-r16\": LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"LoRA-r32\": LoraConfig(\n",
    "            r=32,\n",
    "            lora_alpha=64,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"PrefixTuning-len16\": PrefixTuningConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            prefix_length=16,\n",
    "            num_virtual_tokens=16\n",
    "        ),\n",
    "        \"PrefixTuning-len32\": PrefixTuningConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            prefix_length=32,\n",
    "            num_virtual_tokens=32\n",
    "        ),\n",
    "        \"PromptTuning-len16\": PromptEncoderConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            num_virtual_tokens=16,\n",
    "            encoder_hidden_size=128\n",
    "        ),\n",
    "        \"PromptTuning-len32\": PromptEncoderConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            num_virtual_tokens=32,\n",
    "            encoder_hidden_size=128\n",
    "        ),\n",
    "        \"BitFit\": BitFitConfig(\n",
    "            bias_term=\"all\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"AdaLoRA\": AdaLoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        ),\n",
    "        \"LoRA-AllLayers\": LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\", \"output.dense\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return peft_configs\n",
    "\n",
    "# Get PEFT configurations\n",
    "peft_configs = get_peft_configurations()\n",
    "\n",
    "# Print PEFT configuration details\n",
    "print(\"PEFT Configurations for Comparison\")\n",
    "print(\"=================================\")\n",
    "for method, config in peft_configs.items():\n",
    "    print(f\"\\n{method}\")\n",
    "    print(\"-\" * len(method))\n",
    "    print(f\"Configuration Type: {type(config).__name__}\")\n",
    "    \n",
    "    # Print attributes based on configuration type\n",
    "    if isinstance(config, LoraConfig) or isinstance(config, AdaLoraConfig):\n",
    "        print(f\"Rank (r): {config.r}\")\n",
    "        print(f\"Alpha: {config.lora_alpha}\")\n",
    "        print(f\"Target Modules: {config.target_modules}\")\n",
    "        print(f\"Dropout: {config.lora_dropout}\")\n",
    "    elif isinstance(config, PrefixTuningConfig):\n",
    "        print(f\"Prefix Length: {config.prefix_length}\")\n",
    "        print(f\"Virtual Tokens: {config.num_virtual_tokens}\")\n",
    "    elif isinstance(config, PromptEncoderConfig):\n",
    "        print(f\"Virtual Tokens: {config.num_virtual_tokens}\")\n",
    "        print(f\"Encoder Hidden Size: {config.encoder_hidden_size}\")\n",
    "    elif isinstance(config, BitFitConfig):\n",
    "        print(f\"Bias Term: {config.bias_term}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conducting the Tradeoff Experiments\n",
    "\n",
    "Now, let's conduct our experiments to measure the efficiency-performance tradeoffs for each PEFT method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tradeoff_experiments(batch_size=16, num_epochs=1, subset_size=1000):\n",
    "    \"\"\"Run experiments to measure efficiency-performance tradeoffs\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create subset of data for faster experimentation\n",
    "    train_subset = tokenized_datasets[\"train\"].select(range(subset_size))\n",
    "    eval_subset = tokenized_datasets[\"validation\"].select(range(subset_size // 2))\n",
    "    \n",
    "    # First, measure full fine-tuning as a baseline\n",
    "    print(\"\\nMeasuring Full Fine-tuning (Baseline)...\")\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    \n",
    "    # Measure parameter efficiency\n",
    "    param_metrics = measure_parameter_efficiency(base_model)\n",
    "    \n",
    "    # Measure computational efficiency\n",
    "    comp_metrics = measure_computational_efficiency(train_model, base_model, train_subset, batch_size, num_epochs)\n",
    "    \n",
    "    # Measure task performance\n",
    "    perf_metrics = measure_task_performance(base_model, eval_subset, evaluate_model)\n",
    "    \n",
    "    # Measure inference efficiency\n",
    "    inf_metrics = measure_inference_efficiency(base_model, eval_subset)\n",
    "    \n",
    "    # Combine all metrics\n",
    "    full_metrics = {\n",
    "        \"method\": \"Full Fine-tuning\",\n",
    "        **param_metrics,\n",
    "        **comp_metrics,\n",
    "        **perf_metrics,\n",
    "        **inf_metrics\n",
    "    }\n",
    "    \n",
    "    results.append(full_metrics)\n",
    "    \n",
    "    # Now measure each PEFT method\n",
    "    for method_name, peft_config in peft_configs.items():\n",
    "        print(f\"\\nMeasuring {method_name}...\")\n",
    "        \n",
    "        # Create base model\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "        \n",
    "        # Create PEFT model\n",
    "        peft_model = get_peft_model(base_model, peft_config)\n",
    "        \n",
    "        # Measure parameter efficiency\n",
    "        param_metrics = measure_parameter_efficiency(peft_model, base_model)\n",
    "        \n",
    "        # Measure computational efficiency\n",
    "        comp_metrics = measure_computational_efficiency(train_model, peft_model, train_subset, batch_size, num_epochs)\n",
    "        \n",
    "        # Measure task performance\n",
    "        perf_metrics = measure_task_performance(peft_model, eval_subset, evaluate_model)\n",
    "        \n",
    "        # Measure inference efficiency\n",
    "        inf_metrics = measure_inference_efficiency(peft_model, eval_subset)\n",
    "        \n",
    "        # Combine all metrics\n",
    "        peft_metrics = {\n",
    "            \"method\": method_name,\n",
    "            **param_metrics,\n",
    "            **comp_metrics,\n",
    "            **perf_metrics,\n",
    "            **inf_metrics\n",
    "        }\n",
    "        \n",
    "        results.append(peft_metrics)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiments with a small subset and fewer epochs for demonstration\n",
    "experiment_results = run_tradeoff_experiments(batch_size=8, num_epochs=1, subset_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "\n",
    "# Display key metrics\n",
    "display_cols = [\n",
    "    \"method\", \"param_percentage\", \"trainable_params\", \n",
    "    \"training_time_seconds\", \"eval_accuracy\", \n",
    "    \"peak_gpu_memory_mb\", \"samples_per_second\"\n",
    "]\n",
    "\n",
    "# Display formatted results\n",
    "display_df = results_df[display_cols].copy()\n",
    "display_df[\"trainable_params\"] = display_df[\"trainable_params\"].apply(lambda x: f\"{x:,}\")\n",
    "display_df[\"param_percentage\"] = display_df[\"param_percentage\"].apply(lambda x: f\"{x:.4f}%\")\n",
    "display_df[\"training_time_seconds\"] = display_df[\"training_time_seconds\"].apply(lambda x: f\"{x:.2f}s\")\n",
    "display_df[\"eval_accuracy\"] = display_df[\"eval_accuracy\"].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df[\"peak_gpu_memory_mb\"] = display_df[\"peak_gpu_memory_mb\"].apply(lambda x: f\"{x:.1f} MB\")\n",
    "display_df[\"samples_per_second\"] = display_df[\"samples_per_second\"].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "print(\"\\nEfficiency-Performance Tradeoff Results\")\n",
    "print(\"=======================================\\n\")\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Efficiency-Performance Tradeoffs\n",
    "\n",
    "Let's create visualizations to better understand the tradeoffs between different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for our visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# 1. Parameter Efficiency vs. Performance\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    results_df[\"param_percentage\"],\n",
    "    results_df[\"eval_accuracy\"],\n",
    "    s=results_df[\"training_time_seconds\"] * 5,  # Size represents training time\n",
    "    alpha=0.7,\n",
    "    c=np.arange(len(results_df)),  # Color by index\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.annotate(\n",
    "        row[\"method\"],\n",
    "        (row[\"param_percentage\"], row[\"eval_accuracy\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"Parameter Efficiency vs. Performance Tradeoff\")\n",
    "plt.xlabel(\"Parameter Percentage (%)\")\n",
    "plt.ylabel(\"Evaluation Accuracy\")\n",
    "plt.xscale(\"log\")  # Log scale for better visualization of parameter percentages\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add a legend for bubble size (training time)\n",
    "sizes = [5, 10, 20]\n",
    "labels = [\"Fast\", \"Medium\", \"Slow\"]\n",
    "for size, label in zip(sizes, labels):\n",
    "    plt.scatter([], [], s=size*5, alpha=0.7, color='gray', label=label)\n",
    "plt.legend(title=\"Training Speed\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Training Time vs. Performance\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    results_df[\"training_time_seconds\"],\n",
    "    results_df[\"eval_accuracy\"],\n",
    "    s=results_df[\"param_percentage\"] * 10,  # Size represents parameter percentage\n",
    "    alpha=0.7,\n",
    "    c=np.arange(len(results_df)),  # Color by index\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.annotate(\n",
    "        row[\"method\"],\n",
    "        (row[\"training_time_seconds\"], row[\"eval_accuracy\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"Training Time vs. Performance Tradeoff\")\n",
    "plt.xlabel(\"Training Time (seconds)\")\n",
    "plt.ylabel(\"Evaluation Accuracy\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add a legend for bubble size (parameter percentage)\n",
    "sizes = [0.1, 1, 10, 100]\n",
    "labels = [\"0.1%\", \"1%\", \"10%\", \"100%\"]\n",
    "for size, label in zip(sizes, labels):\n",
    "    plt.scatter([], [], s=size*10, alpha=0.7, color='gray', label=label)\n",
    "plt.legend(title=\"Parameter %\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Memory Usage vs. Performance\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    results_df[\"peak_gpu_memory_mb\"],\n",
    "    results_df[\"eval_accuracy\"],\n",
    "    s=results_df[\"param_percentage\"] * 10,  # Size represents parameter percentage\n",
    "    alpha=0.7,\n",
    "    c=np.arange(len(results_df)),  # Color by index\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.annotate(\n",
    "        row[\"method\"],\n",
    "        (row[\"peak_gpu_memory_mb\"], row[\"eval_accuracy\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(\"Memory Usage vs. Performance Tradeoff\")\n",
    "plt.xlabel(\"Peak GPU Memory Usage (MB)\")\n",
    "plt.ylabel(\"Evaluation Accuracy\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add a legend for bubble size (parameter percentage)\n",
    "sizes = [0.1, 1, 10, 100]\n",
    "labels = [\"0.1%\", \"1%\", \"10%\", \"100%\"]\n",
    "for size, label in zip(sizes, labels):\n",
    "    plt.scatter([], [], s=size*10, alpha=0.7, color='gray', label=label)\n",
    "plt.legend(title=\"Parameter %\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Multi-dimensional comparison - Radar chart\n",
    "# Normalize the metrics for comparison\n",
    "metrics_to_normalize = [\n",
    "    \"param_percentage\",\n",
    "    \"training_time_seconds\",\n",
    "    \"eval_accuracy\",\n",
    "    \"peak_gpu_memory_mb\",\n",
    "    \"samples_per_second\"\n",
    "]\n",
    "\n",
    "normalized_df = results_df.copy()\n",
    "\n",
    "# Invert metrics where lower is better\n",
    "for metric in [\"param_percentage\", \"training_time_seconds\", \"peak_gpu_memory_mb\"]:\n",
    "    max_val = normalized_df[metric].max()\n",
    "    normalized_df[f\"{metric}_inv\"] = (max_val - normalized_df[metric]) / max_val\n",
    "\n",
    "# Normalize metrics where higher is better\n",
    "for metric in [\"eval_accuracy\", \"samples_per_second\"]:\n",
    "    min_val = normalized_df[metric].min()\n",
    "    max_val = normalized_df[metric].max()\n",
    "    normalized_df[f\"{metric}_norm\"] = (normalized_df[metric] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Select methods to include in the radar chart (to avoid overcrowding)\n",
    "methods_to_include = [\"Full Fine-tuning\", \"LoRA-r8\", \"PrefixTuning-len32\", \"BitFit\", \"AdaLoRA\"]\n",
    "radar_df = normalized_df[normalized_df[\"method\"].isin(methods_to_include)]\n",
    "\n",
    "# Create radar chart\n",
    "categories = [\n",
    "    \"Parameter Efficiency\",\n",
    "    \"Training Speed\",\n",
    "    \"Performance\",\n",
    "    \"Memory Efficiency\",\n",
    "    \"Inference Speed\"\n",
    "]\n",
    "\n",
    "# Number of categories\n",
    "N = len(categories)\n",
    "\n",
    "# Create angle for each category\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Close the loop\n",
    "\n",
    "# Create radar chart figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Draw one axis per variable and add labels\n",
    "plt.xticks(angles[:-1], categories, size=12)\n",
    "\n",
    "# Draw ylabels\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.5\", \"0.75\"], color=\"grey\", size=10)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plot data\n",
    "for i, method in enumerate(methods_to_include):\n",
    "    method_data = radar_df[radar_df[\"method\"] == method]\n",
    "    \n",
    "    # Get values for each dimension\n",
    "    values = [\n",
    "        method_data[\"param_percentage_inv\"].values[0],\n",
    "        method_data[\"training_time_seconds_inv\"].values[0],\n",
    "        method_data[\"eval_accuracy_norm\"].values[0],\n",
    "        method_data[\"peak_gpu_memory_mb_inv\"].values[0],\n",
    "        method_data[\"samples_per_second_norm\"].values[0]\n",
    "    ]\n",
    "    \n",
    "    # Close the loop\n",
    "    values += values[:1]\n",
    "    \n",
    "    # Plot values\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=method)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "plt.title(\"Multi-dimensional Comparison of PEFT Methods\", size=15, y=1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Efficiency Experiment\n",
    "\n",
    "One important aspect mentioned in the paper is data efficiency. Let's conduct an experiment to evaluate how different PEFT methods perform with varying amounts of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_data_efficiency_experiment(methods=[\"Full Fine-tuning\", \"LoRA-r8\", \"BitFit\", \"PrefixTuning-len32\"]):\n",
    "    \"\"\"Experiment to measure performance with varying dataset sizes\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Define dataset sizes to test\n",
    "    dataset_sizes = [100, 250, 500, 1000]\n",
    "    \n",
    "    for method_name in methods:\n",
    "        for dataset_size in dataset_sizes:\n",
    "            print(f\"\\nTesting {method_name} with {dataset_size} training examples...\")\n",
    "            \n",
    "            # Create training subset\n",
    "            train_subset = tokenized_datasets[\"train\"].select(range(dataset_size))\n",
    "            eval_subset = tokenized_datasets[\"validation\"].select(range(200))  # Fixed eval set size\n",
    "            \n",
    "            # Create base model\n",
    "            base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "            \n",
    "            if method_name == \"Full Fine-tuning\":\n",
    "                model = base_model\n",
    "            else:\n",
    "                # Get PEFT configuration\n",
    "                peft_config = peft_configs[method_name]\n",
    "                \n",
    "                # Create PEFT model\n",
    "                model = get_peft_model(base_model, peft_config)\n",
    "            \n",
    "            # Train the model\n",
    "            train_model(model, train_subset, batch_size=8, num_epochs=3)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            eval_results = evaluate_model(model, eval_subset)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"method\": method_name,\n",
    "                \"dataset_size\": dataset_size,\n",
    "                \"accuracy\": eval_results[\"eval_accuracy\"],\n",
    "                \"loss\": eval_results[\"eval_loss\"]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the data efficiency experiment with a subset of methods\n",
    "data_efficiency_results = run_data_efficiency_experiment([\"Full Fine-tuning\", \"LoRA-r8\", \"BitFit\", \"PrefixTuning-len32\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data efficiency results to DataFrame\n",
    "data_eff_df = pd.DataFrame(data_efficiency_results)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nData Efficiency Results\")\n",
    "print(\"======================\")\n",
    "print(data_eff_df.to_string(index=False))\n",
    "\n",
    "# Create a line plot to visualize data efficiency\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Group by method and dataset size\n",
    "for method in data_eff_df[\"method\"].unique():\n",
    "    method_data = data_eff_df[data_eff_df[\"method\"] == method]\n",
    "    plt.plot(\n",
    "        method_data[\"dataset_size\"],\n",
    "        method_data[\"accuracy\"],\n",
    "        marker='o',\n",
    "        linewidth=2,\n",
    "        label=method\n",
    "    )\n",
    "\n",
    "plt.title(\"Data Efficiency: Performance vs. Training Data Size\")\n",
    "plt.xlabel(\"Number of Training Examples\")\n",
    "plt.ylabel(\"Evaluation Accuracy\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Rank vs. Performance Experiment for LoRA\n",
    "\n",
    "The paper mentions that the choice of hyperparameters like rank in LoRA can significantly impact the efficiency-performance tradeoff. Let's conduct an experiment to explore this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lora_rank_experiment():\n",
    "    \"\"\"Experiment to measure how LoRA rank affects performance and efficiency\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Define LoRA ranks to test\n",
    "    ranks = [1, 2, 4, 8, 16, 32, 64]\n",
    "    \n",
    "    # Create training and evaluation subsets\n",
    "    train_subset = tokenized_datasets[\"train\"].select(range(500))\n",
    "    eval_subset = tokenized_datasets[\"validation\"].select(range(200))\n",
    "    \n",
    "    for rank in ranks:\n",
    "        print(f\"\\nTesting LoRA with rank {rank}...\")\n",
    "        \n",
    "        # Create base model\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "        \n",
    "        # Create LoRA configuration\n",
    "        lora_config = LoraConfig(\n",
    "            r=rank,\n",
    "            lora_alpha=rank * 2,  # Scale alpha with rank\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS\n",
    "        )\n",
    "        \n",
    "        # Create LoRA model\n",
    "        lora_model = get_peft_model(base_model, lora_config)\n",
    "        \n",
    "        # Measure parameter efficiency\n",
    "        param_metrics = measure_parameter_efficiency(lora_model, base_model)\n",
    "        \n",
    "        # Train the model\n",
    "        train_model(lora_model, train_subset, batch_size=8, num_epochs=3)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_results = evaluate_model(lora_model, eval_subset)\n",
    "        \n",
    "        # Measure inference efficiency\n",
    "        inf_metrics = measure_inference_efficiency(lora_model, eval_subset)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"trainable_params\": param_metrics[\"trainable_params\"],\n",
    "            \"param_percentage\": param_metrics[\"param_percentage\"],\n",
    "            \"accuracy\": eval_results[\"eval_accuracy\"],\n",
    "            \"loss\": eval_results[\"eval_loss\"],\n",
    "            \"inference_time\": inf_metrics[\"inference_time_seconds\"],\n",
    "            \"samples_per_second\": inf_metrics[\"samples_per_second\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the LoRA rank experiment\n",
    "lora_rank_results = run_lora_rank_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LoRA rank results to DataFrame\n",
    "lora_rank_df = pd.DataFrame(lora_rank_results)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nLoRA Rank Experiment Results\")\n",
    "print(\"============================\")\n",
    "display_df = lora_rank_df.copy()\n",
    "display_df[\"trainable_params\"] = display_df[\"trainable_params\"].apply(lambda x: f\"{x:,}\")\n",
    "display_df[\"param_percentage\"] = display_df[\"param_percentage\"].apply(lambda x: f\"{x:.4f}%\")\n",
    "display_df[\"accuracy\"] = display_df[\"accuracy\"].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df[\"inference_time\"] = display_df[\"inference_time\"].apply(lambda x: f\"{x:.4f}s\")\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot rank vs. accuracy\n",
    "ax1.plot(lora_rank_df[\"rank\"], lora_rank_df[\"accuracy\"], marker='o', linewidth=2)\n",
    "ax1.set_title(\"LoRA Rank vs. Accuracy\")\n",
    "ax1.set_xlabel(\"LoRA Rank (r)\")\n",
    "ax1.set_ylabel(\"Evaluation Accuracy\")\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot parameter percentage vs. accuracy\n",
    "ax2.scatter(\n",
    "    lora_rank_df[\"param_percentage\"],\n",
    "    lora_rank_df[\"accuracy\"],\n",
    "    s=lora_rank_df[\"rank\"] * 5,\n",
    "    alpha=0.7\n",
    ")\n",
    "# Add labels for each point\n",
    "for i, row in lora_rank_df.iterrows():\n",
    "    ax2.annotate(\n",
    "        f\"r={row['rank']}\",\n",
    "        (row[\"param_percentage\"], row[\"accuracy\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "ax2.set_title(\"Parameter Percentage vs. Accuracy\")\n",
    "ax2.set_xlabel(\"Parameter Percentage (%)\")\n",
    "ax2.set_ylabel(\"Evaluation Accuracy\")\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating a PEFT Selection Framework\n",
    "\n",
    "Based on our experiments and the findings from the paper, let's create a framework to help select the most appropriate PEFT method based on specific requirements and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peft_method_selector(importance_weights, available_methods=None):\n",
    "    \"\"\"Select the most appropriate PEFT method based on importance weights\"\"\"\n",
    "    # Default available methods if none provided\n",
    "    if available_methods is None:\n",
    "        available_methods = [\n",
    "            \"Full Fine-tuning\", \"LoRA\", \"AdaLoRA\", \"BitFit\", \n",
    "            \"PrefixTuning\", \"PromptTuning\"\n",
    "        ]\n",
    "    \n",
    "    # Define method characteristics based on our experiments and the paper\n",
    "    method_profiles = {\n",
    "        \"Full Fine-tuning\": {\n",
    "            \"parameter_efficiency\": 0.1,  # Low efficiency (uses 100% of parameters)\n",
    "            \"computational_efficiency\": 0.2,  # Low efficiency (slow training, high memory)\n",
    "            \"performance\": 1.0,  # High performance (baseline)\n",
    "            \"generalization\": 0.9,  # High generalization\n",
    "            \"data_efficiency\": 0.3,  # Low data efficiency (needs more data)\n",
    "            \"inference_efficiency\": 0.7  # Moderate inference efficiency\n",
    "        },\n",
    "        \"LoRA\": {\n",
    "            \"parameter_efficiency\": 0.9,  # Very high efficiency (<1% of parameters)\n",
    "            \"computational_efficiency\": 0.8,  # High efficiency (faster training, lower memory)\n",
    "            \"performance\": 0.95,  # Very good performance (close to full fine-tuning)\n",
    "            \"generalization\": 0.8,  # Good generalization\n",
    "            \"data_efficiency\": 0.7,  # Good data efficiency\n",
    "            \"inference_efficiency\": 0.8  # Good inference efficiency\n",
    "        },\n",
    "        \"AdaLoRA\": {\n",
    "            \"parameter_efficiency\": 0.85,  # High efficiency\n",
    "            \"computational_efficiency\": 0.7,  # Good efficiency\n",
    "            \"performance\": 0.97,  # Excellent performance\n",
    "            \"generalization\": 0.85,  # Very good generalization\n",
    "            \"data_efficiency\": 0.75,  # Good data efficiency\n",
    "            \"inference_efficiency\": 0.75  # Good inference efficiency\n",
    "        },\n",
    "        \"BitFit\": {\n",
    "            \"parameter_efficiency\": 0.95,  # Extremely high efficiency (<0.1% of parameters)\n",
    "            \"computational_efficiency\": 0.9,  # Very high efficiency\n",
    "            \"performance\": 0.85,  # Good performance (some drop from full fine-tuning)\n",
    "            \"generalization\": 0.75,  # Decent generalization\n",
    "            \"data_efficiency\": 0.8,  # Very good data efficiency\n",
    "            \"inference_efficiency\": 0.9  # Very good inference efficiency\n",
    "        },\n",
    "        \"PrefixTuning\": {\n",
    "            \"parameter_efficiency\": 0.9,  # Very high efficiency\n",
    "            \"computational_efficiency\": 0.8,  # High efficiency\n",
    "            \"performance\": 0.85,  # Good performance\n",
    "            \"generalization\": 0.8,  # Good generalization\n",
    "            \"data_efficiency\": 0.7,  # Good data efficiency\n",
    "            \"inference_efficiency\": 0.7  # Good inference efficiency\n",
    "        },\n",
    "        \"PromptTuning\": {\n",
    "            \"parameter_efficiency\": 0.95,  # Extremely high efficiency\n",
    "            \"computational_efficiency\": 0.85,  # Very high efficiency\n",
    "            \"performance\": 0.8,  # Moderate performance\n",
    "            \"generalization\": 0.7,  # Moderate generalization\n",
    "            \"data_efficiency\": 0.8,  # Very good data efficiency\n",
    "            \"inference_efficiency\": 0.8  # Good inference efficiency\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Filter available methods\n",
    "    filtered_methods = {k: v for k, v in method_profiles.items() if k in available_methods}\n",
    "    \n",
    "    # Calculate weighted scores\n",
    "    method_scores = {}\n",
    "    for method, profile in filtered_methods.items():\n",
    "        weighted_score = sum(profile[dim] * weight for dim, weight in importance_weights.items())\n",
    "        method_scores[method] = weighted_score\n",
    "    \n",
    "    # Sort methods by score\n",
    "    sorted_methods = sorted(method_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_methods\n",
    "\n",
    "# Example usage: Performance is most important, followed by parameter efficiency\n",
    "example_weights = {\n",
    "    \"parameter_efficiency\": 0.3,\n",
    "    \"computational_efficiency\": 0.2,\n",
    "    \"performance\": 0.3,\n",
    "    \"generalization\": 0.1,\n",
    "    \"data_efficiency\": 0.05,\n",
    "    \"inference_efficiency\": 0.05\n",
    "}\n",
    "\n",
    "recommended_methods = peft_method_selector(example_weights)\n",
    "\n",
    "# Display recommendations\n",
    "print(\"PEFT Method Recommendations\")\n",
    "print(\"==========================\")\n",
    "print(f\"Importance Weights: {example_weights}\")\n",
    "print(\"\\nRecommended Methods (in order):\")\n",
    "for method, score in recommended_methods:\n",
    "    print(f\"{method}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive PEFT Method Selector\n",
    "\n",
    "Let's create an interactive tool to help users select the most appropriate PEFT method based on their specific requirements and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_peft_selector():\n",
    "    \"\"\"Interactive tool to help select the most appropriate PEFT method\"\"\"\n",
    "    print(\"PEFT Method Selector\")\n",
    "    print(\"===================\")\n",
    "    print(\"\\nRate the importance of each dimension on a scale of 0-10:\")\n",
    "    \n",
    "    # Collect importance weights\n",
    "    dimensions = [\n",
    "        \"parameter_efficiency\",\n",
    "        \"computational_efficiency\",\n",
    "        \"performance\",\n",
    "        \"generalization\",\n",
    "        \"data_efficiency\",\n",
    "        \"inference_efficiency\"\n",
    "    ]\n",
    "    \n",
    "    weights = {}\n",
    "    for dim in dimensions:\n",
    "        # In a real interactive environment, you would use input() here\n",
    "        # For this notebook, we'll simulate with predefined values\n",
    "        weights[dim] = 5  # Default value\n",
    "    \n",
    "    # Example ratings for different scenarios\n",
    "    scenarios = {\n",
    "        \"Performance Critical\": {\n",
    "            \"parameter_efficiency\": 3,\n",
    "            \"computational_efficiency\": 2,\n",
    "            \"performance\": 10,\n",
    "            \"generalization\": 7,\n",
    "            \"data_efficiency\": 3,\n",
    "            \"inference_efficiency\": 5\n",
    "        },\n",
    "        \"Resource Constrained\": {\n",
    "            \"parameter_efficiency\": 10,\n",
    "            \"computational_efficiency\": 9,\n",
    "            \"performance\": 5,\n",
    "            \"generalization\": 3,\n",
    "            \"data_efficiency\": 7,\n",
    "            \"inference_efficiency\": 8\n",
    "        },\n",
    "        \"Limited Data\": {\n",
    "            \"parameter_efficiency\": 7,\n",
    "            \"computational_efficiency\": 5,\n",
    "            \"performance\": 6,\n",
    "            \"generalization\": 8,\n",
    "            \"data_efficiency\": 10,\n",
    "            \"inference_efficiency\": 4\n",
    "        },\n",
    "        \"Deployment Focused\": {\n",
    "            \"parameter_efficiency\": 8,\n",
    "            \"computational_efficiency\": 6,\n",
    "            \"performance\": 7,\n",
    "            \"generalization\": 5,\n",
    "            \"data_efficiency\": 3,\n",
    "            \"inference_efficiency\": 10\n",
    "        },\n",
    "        \"Balanced\": {\n",
    "            \"parameter_efficiency\": 6,\n",
    "            \"computational_efficiency\": 6,\n",
    "            \"performance\": 7,\n",
    "            \"generalization\": 6,\n",
    "            \"data_efficiency\": 5,\n",
    "            \"inference_efficiency\": 5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Display recommendations for each scenario\n",
    "    for scenario, scenario_weights in scenarios.items():\n",
    "        # Normalize weights\n",
    "        total = sum(scenario_weights.values())\n",
    "        normalized_weights = {k: v/total for k, v in scenario_weights.items()}\n",
    "        \n",
    "        # Get recommendations\n",
    "        recommendations = peft_method_selector(normalized_weights)\n",
    "        \n",
    "        print(f\"\\n{scenario} Scenario\")\n",
    "        print(\"-\" * (len(scenario) + 9))\n",
    "        print(\"Importance Weights:\")\n",
    "        for dim, weight in normalized_weights.items():\n",
    "            print(f\"  {dim}: {weight:.2f}\")\n",
    "        print(\"\\nRecommended Methods:\")\n",
    "        for i, (method, score) in enumerate(recommendations[:3], 1):\n",
    "            print(f\"  {i}. {method} (score: {score:.4f})\")\n",
    "        print(f\"\\nSpecific Configuration Advice for {recommendations[0][0]}:\")\n",
    "        \n",
    "        # Provide specific configuration advice based on top recommendation\n",
    "        top_method = recommendations[0][0]\n",
    "        if top_method == \"LoRA\" or top_method == \"AdaLoRA\":\n",
    "            if normalized_weights[\"performance\"] > 0.3:\n",
    "                print(\"  - Use higher rank (r=16 or r=32) for better performance\")\n",
    "            else:\n",
    "                print(\"  - Use lower rank (r=4 or r=8) for better efficiency\")\n",
    "            print(\"  - Target attention layers (query, key, value) for most domains\")\n",
    "            print(\"  - Consider adding dense layers for more complex tasks\")\n",
    "        elif top_method == \"PrefixTuning\":\n",
    "            if normalized_weights[\"performance\"] > 0.3:\n",
    "                print(\"  - Use longer prefix length (32-64) for better performance\")\n",
    "            else:\n",
    "                print(\"  - Use shorter prefix length (8-16) for better efficiency\")\n",
    "            print(\"  - Consider combined with BitFit for better performance\")\n",
    "        elif top_method == \"BitFit\":\n",
    "            print(\"  - Works well with larger models where biases capture more information\")\n",
    "            print(\"  - Consider combining with LoRA for better performance-efficiency tradeoff\")\n",
    "        elif top_method == \"Full Fine-tuning\":\n",
    "            print(\"  - Use gradient accumulation for larger batch sizes with limited memory\")\n",
    "            print(\"  - Consider 8-bit or 4-bit quantization to reduce memory requirements\")\n",
    "            print(\"  - Explore using LoRA with high rank as an alternative\")\n",
    "\n",
    "# Run the interactive selector\n",
    "interactive_peft_selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion: Key Insights and Best Practices\n",
    "\n",
    "Based on our experiments and the findings from the paper, we can distill several key insights and best practices for navigating the performance-efficiency tradeoffs in PEFT methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from Our Experiments\n",
    "\n",
    "1. **No One-Size-Fits-All Solution**: Different PEFT methods excel in different dimensions. The optimal choice depends on specific requirements and constraints.\n",
    "\n",
    "2. **Parameter Efficiency ≠ Performance Sacrifice**: Some methods (like LoRA with appropriate rank) can achieve performance very close to full fine-tuning while using <1% of parameters.\n",
    "\n",
    "3. **Data Efficiency Varies**: PEFT methods show different behavior with limited data. Some methods (like BitFit) excel in low-data regimes, making them suitable for domains with limited labeled data.\n",
    "\n",
    "4. **Hyperparameter Sensitivity**: The performance of PEFT methods can be significantly influenced by hyperparameter choices. For example, LoRA's rank parameter creates a direct tradeoff between efficiency and performance.\n",
    "\n",
    "5. **Multi-dimensional Tradeoffs**: The efficiency-performance tradeoff is multi-dimensional, involving parameters, computation, memory, performance, and generalization. Optimizing for one dimension often affects others.\n",
    "\n",
    "### Best Practices for PEFT Method Selection\n",
    "\n",
    "1. **Clearly Define Requirements**: Before selecting a PEFT method, clearly define your requirements across all dimensions: parameter efficiency, computational efficiency, performance, generalization, data efficiency, and inference efficiency.\n",
    "\n",
    "2. **Prioritize Dimensions**: Determine which dimensions are most important for your specific application. This will guide the selection of the most appropriate PEFT method.\n",
    "\n",
    "3. **Consider Application Domain**: Different domains may benefit from different PEFT methods. For instance, NLP tasks often work well with LoRA and BitFit, while vision tasks might benefit from adapter approaches.\n",
    "\n",
    "4. **Test Multiple Methods**: Given the significant variation in performance across methods, it's beneficial to experiment with multiple PEFT approaches for your specific task.\n",
    "\n",
    "5. **Optimize Hyperparameters**: Once you've selected a method, carefully tune its hyperparameters to find the optimal balance between efficiency and performance for your specific requirements.\n",
    "\n",
    "6. **Consider Combining Methods**: Some of the best results come from combining multiple PEFT methods (e.g., BitFit+LoRA), leveraging the strengths of each approach.\n",
    "\n",
    "7. **Benchmark Systematically**: Use a systematic benchmarking approach that considers all relevant dimensions, not just parameter count or accuracy.\n",
    "\n",
    "By following these insights and best practices, practitioners can effectively navigate the complex tradeoffs involved in PEFT methods and select the approach that best meets their specific requirements and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Balne, C. C. S., Bhaduri, S., Roy, T., Jain, V., & Chadha, A. (2024). Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications. arXiv:2404.13506v2.\n",
    "\n",
    "2. Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). ReFT: Representation finetuning for language models. arXiv preprint arXiv:2401.13622.\n",
    "\n",
    "3. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\n",
    "\n",
    "4. Li, X. L., & Liang, P. (2021). Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.\n",
    "\n",
    "5. Zaken, E. B., Ravfogel, S., & Goldberg, Y. (2021). BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199.\n",
    "\n",
    "6. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., & Smola, A. (2022). Differentiable prompt makes pre-trained language models better few-shot learners. arXiv preprint arXiv:2108.13161."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}