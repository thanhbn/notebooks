{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIPO Focused Learning: Iterative Preference Learning (IPL)\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "Notebook nÃ y táº­p trung vÃ o **Iterative Preference Learning (IPL)** - phÆ°Æ¡ng phÃ¡p self-rewarding Ä‘á»™c Ä‘Ã¡o cá»§a FIPO:\n",
    "\n",
    "1. Hiá»ƒu cÆ¡ cháº¿ self-rewarding vÃ  self-improvement\n",
    "2. Implement Algorithm 1 tá»« paper\n",
    "3. PhÃ¢n tÃ­ch quÃ¡ trÃ¬nh iterative refinement\n",
    "4. So sÃ¡nh IPL-DPO vs IPL-IPO\n",
    "\n",
    "## ðŸ“š Paper References\n",
    "\n",
    "- **Section 2.4**: Iterative Preference Learning strategy\n",
    "- **Algorithm 1**: Self-rewarding IPL Algorithm (Appendix C)\n",
    "- **Equations 12-14**: IPL mathematical formulation\n",
    "- **Table 7**: IPL iteration analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Self-Rewarding Systems\n",
    "\n",
    "### 1.1 The Concept of Self-Improvement\n",
    "\n",
    "IPL cho phÃ©p model tá»± Ä‘Ã¡nh giÃ¡ vÃ  cáº£i thiá»‡n cháº¥t lÆ°á»£ng cá»§a mÃ¬nh qua nhiá»u iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ipl_concept():\n",
    "    \"\"\"Visualize the IPL self-rewarding concept\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left: Traditional vs IPL training\n",
    "    methods = ['Traditional\\nPreference Learning', 'Iterative\\nPreference Learning']\n",
    "    capabilities = [1.0, 1.45]  # IPL shows more improvement\n",
    "    colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    bars = ax1.bar(methods, capabilities, color=colors, alpha=0.7, width=0.6)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax1.text(0, 0.5, \"Static Data\\nNo Self-Improvement\", ha='center', fontsize=10)\n",
    "    ax1.text(1, 0.7, \"Dynamic Updates\\nSelf-Rewarding\\nIterative Refinement\", \n",
    "            ha='center', fontsize=10)\n",
    "    \n",
    "    ax1.set_ylabel('Relative Performance', fontsize=12)\n",
    "    ax1.set_title('Traditional vs IPL Training', fontsize=14, weight='bold')\n",
    "    ax1.set_ylim(0, 1.6)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Right: IPL process flow\n",
    "    G = nx.DiGraph()\n",
    "    nodes = [\n",
    "        (\"Start\", {\"pos\": (0, 2), \"color\": \"lightblue\"}),\n",
    "        (\"Generate\\nNew Prompt\", {\"pos\": (1, 2), \"color\": \"lightgreen\"}),\n",
    "        (\"Self-Judge\\nQuality\", {\"pos\": (2, 2), \"color\": \"lightyellow\"}),\n",
    "        (\"Better?\", {\"pos\": (3, 2), \"color\": \"lightcoral\"}),\n",
    "        (\"Update\\nDataset\", {\"pos\": (4, 3), \"color\": \"lightgreen\"}),\n",
    "        (\"Keep\\nOriginal\", {\"pos\": (4, 1), \"color\": \"lightgray\"}),\n",
    "        (\"Train\\nModel\", {\"pos\": (5, 2), \"color\": \"lightblue\"}),\n",
    "        (\"Next\\nIteration\", {\"pos\": (6, 2), \"color\": \"lightblue\"})\n",
    "    ]\n",
    "    \n",
    "    for node, attrs in nodes:\n",
    "        G.add_node(node, **attrs)\n",
    "    \n",
    "    edges = [\n",
    "        (\"Start\", \"Generate\\nNew Prompt\"),\n",
    "        (\"Generate\\nNew Prompt\", \"Self-Judge\\nQuality\"),\n",
    "        (\"Self-Judge\\nQuality\", \"Better?\"),\n",
    "        (\"Better?\", \"Update\\nDataset\"),\n",
    "        (\"Better?\", \"Keep\\nOriginal\"),\n",
    "        (\"Update\\nDataset\", \"Train\\nModel\"),\n",
    "        (\"Keep\\nOriginal\", \"Train\\nModel\"),\n",
    "        (\"Train\\nModel\", \"Next\\nIteration\"),\n",
    "        (\"Next\\nIteration\", \"Generate\\nNew Prompt\")\n",
    "    ]\n",
    "    \n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    colors = [G.nodes[node]['color'] for node in G.nodes()]\n",
    "    \n",
    "    nx.draw(G, pos, ax=ax2, with_labels=True, node_color=colors, \n",
    "            node_size=2000, font_size=9, font_weight='bold',\n",
    "            arrows=True, arrowsize=20, edge_color='gray')\n",
    "    \n",
    "    ax2.set_title('IPL Self-Rewarding Process Flow', fontsize=14, weight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ipl_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IPL Algorithm Implementation\n",
    "\n",
    "### 2.1 Core IPL Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IPLDataPoint:\n",
    "    \"\"\"Data point for IPL training\"\"\"\n",
    "    naive_prompt: str\n",
    "    naive_response: str\n",
    "    ground_truth: str\n",
    "    current_prompt: str = field(default=\"\")\n",
    "    current_response: str = field(default=\"\")\n",
    "    iteration_history: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.current_prompt:\n",
    "            self.current_prompt = self.naive_prompt\n",
    "        if not self.current_response:\n",
    "            self.current_response = self.naive_response\n",
    "\n",
    "class IPLOptimizer:\n",
    "    \"\"\"Iterative Preference Learning Optimizer (Algorithm 1)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_method: str = \"IPO\",  # or \"DPO\"\n",
    "        beta: float = 0.01,\n",
    "        total_iterations: int = 3\n",
    "    ):\n",
    "        self.base_method = base_method\n",
    "        self.beta = beta\n",
    "        self.total_iterations = total_iterations\n",
    "        self.current_iteration = 0\n",
    "        self.update_history = []\n",
    "        \n",
    "    def generate_new_prompt(self, data_point: IPLDataPoint) -> str:\n",
    "        \"\"\"Generate new optimized prompt (Line 8 in Algorithm 1)\"\"\"\n",
    "        \n",
    "        # Simulate prompt optimization\n",
    "        # In reality, this would use the trained model Mo\n",
    "        current = data_point.current_prompt\n",
    "        \n",
    "        # Add progressive improvements\n",
    "        improvements = [\n",
    "            \"Please carefully \",\n",
    "            \"Step by step, \",\n",
    "            \"With detailed explanation, \",\n",
    "            \"Systematically and accurately \"\n",
    "        ]\n",
    "        \n",
    "        if self.current_iteration < len(improvements):\n",
    "            new_prompt = improvements[self.current_iteration] + current.lower()\n",
    "        else:\n",
    "            new_prompt = current + \" (Enhanced)\"\n",
    "            \n",
    "        return new_prompt\n",
    "    \n",
    "    def judge_prompts(\n",
    "        self,\n",
    "        prompt1: str,\n",
    "        prompt2: str,\n",
    "        ground_truth: str\n",
    "    ) -> bool:\n",
    "        \"\"\"Judge if prompt2 is better than prompt1 (Line 9)\"\"\"\n",
    "        \n",
    "        # Simulate discrimination\n",
    "        # In reality, this uses the trained discriminator\n",
    "        \n",
    "        # Simple heuristics for demonstration\n",
    "        score1 = len(prompt1.split()) + prompt1.count(\"step\")\n",
    "        score2 = len(prompt2.split()) + prompt2.count(\"step\")\n",
    "        \n",
    "        # Add some randomness but bias towards improvement\n",
    "        if np.random.random() < 0.7:  # 70% chance of correct judgment\n",
    "            return score2 > score1\n",
    "        else:\n",
    "            return np.random.random() < 0.3  # 30% chance of accepting anyway\n",
    "    \n",
    "    def generate_new_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response for new prompt (Line 10)\"\"\"\n",
    "        \n",
    "        # Simulate response generation\n",
    "        if \"step\" in prompt.lower():\n",
    "            return \"[Improved response with step-by-step reasoning]\"\n",
    "        elif \"careful\" in prompt.lower():\n",
    "            return \"[Carefully considered response]\"\n",
    "        else:\n",
    "            return \"[Standard response]\"\n",
    "    \n",
    "    def run_iteration(self, dataset: List[IPLDataPoint]) -> Tuple[List[IPLDataPoint], Dict]:\n",
    "        \"\"\"Run one IPL iteration\"\"\"\n",
    "        \n",
    "        self.current_iteration += 1\n",
    "        iteration_stats = {\n",
    "            \"iteration\": self.current_iteration,\n",
    "            \"total_samples\": len(dataset),\n",
    "            \"updated_samples\": 0,\n",
    "            \"acceptance_rate\": 0.0\n",
    "        }\n",
    "        \n",
    "        # Skip first iteration (warmup)\n",
    "        if self.current_iteration == 1:\n",
    "            iteration_stats[\"status\"] = \"warmup\"\n",
    "            return dataset, iteration_stats\n",
    "        \n",
    "        # Process each data point\n",
    "        updated_dataset = []\n",
    "        \n",
    "        for data_point in tqdm(dataset, desc=f\"IPL Iteration {self.current_iteration}\"):\n",
    "            # Generate new prompt (Line 8)\n",
    "            new_prompt = self.generate_new_prompt(data_point)\n",
    "            \n",
    "            # Judge if better (Line 9)\n",
    "            is_better = self.judge_prompts(\n",
    "                data_point.current_prompt,\n",
    "                new_prompt,\n",
    "                data_point.ground_truth\n",
    "            )\n",
    "            \n",
    "            if is_better:\n",
    "                # Generate new response (Line 10)\n",
    "                new_response = self.generate_new_response(new_prompt)\n",
    "                \n",
    "                # Update data point (Lines 11-12)\n",
    "                data_point.current_prompt = new_prompt\n",
    "                data_point.current_response = new_response\n",
    "                iteration_stats[\"updated_samples\"] += 1\n",
    "                \n",
    "                # Record history\n",
    "                data_point.iteration_history.append({\n",
    "                    \"iteration\": self.current_iteration,\n",
    "                    \"prompt\": new_prompt,\n",
    "                    \"response\": new_response,\n",
    "                    \"accepted\": True\n",
    "                })\n",
    "            else:\n",
    "                data_point.iteration_history.append({\n",
    "                    \"iteration\": self.current_iteration,\n",
    "                    \"prompt\": new_prompt,\n",
    "                    \"accepted\": False\n",
    "                })\n",
    "            \n",
    "            updated_dataset.append(data_point)\n",
    "        \n",
    "        iteration_stats[\"acceptance_rate\"] = iteration_stats[\"updated_samples\"] / len(dataset)\n",
    "        self.update_history.append(iteration_stats)\n",
    "        \n",
    "        return updated_dataset, iteration_stats\n",
    "\n",
    "# Create sample dataset\n",
    "sample_dataset = [\n",
    "    IPLDataPoint(\n",
    "        naive_prompt=\"Calculate the average\",\n",
    "        naive_response=\"The average is 42\",\n",
    "        ground_truth=\"The average is 44.25\"\n",
    "    ),\n",
    "    IPLDataPoint(\n",
    "        naive_prompt=\"What is machine learning?\",\n",
    "        naive_response=\"ML is AI\",\n",
    "        ground_truth=\"Machine learning is a subset of AI that learns from data\"\n",
    "    )\n",
    "] * 5  # Replicate for demonstration\n",
    "\n",
    "# Run IPL\n",
    "ipl_optimizer = IPLOptimizer(base_method=\"IPO\", total_iterations=3)\n",
    "\n",
    "print(\"Running IPL optimization...\\n\")\n",
    "for i in range(ipl_optimizer.total_iterations):\n",
    "    sample_dataset, stats = ipl_optimizer.run_iteration(sample_dataset)\n",
    "    print(f\"Iteration {stats['iteration']}: \", end=\"\")\n",
    "    if 'status' in stats:\n",
    "        print(f\"{stats['status']}\")\n",
    "    else:\n",
    "        print(f\"Updated {stats['updated_samples']}/{stats['total_samples']} samples \"\n",
    "              f\"({stats['acceptance_rate']:.1%} acceptance rate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing IPL Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ipl_progress(optimizer: IPLOptimizer, dataset: List[IPLDataPoint]):\n",
    "    \"\"\"Visualize IPL training progress\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Acceptance rate over iterations\n",
    "    ax = axes[0, 0]\n",
    "    iterations = [h['iteration'] for h in optimizer.update_history if 'acceptance_rate' in h]\n",
    "    acceptance_rates = [h['acceptance_rate'] for h in optimizer.update_history if 'acceptance_rate' in h]\n",
    "    \n",
    "    if iterations:\n",
    "        ax.plot(iterations, acceptance_rates, 'o-', linewidth=2, markersize=8, color='green')\n",
    "        ax.set_xlabel('Iteration', fontsize=12)\n",
    "        ax.set_ylabel('Acceptance Rate', fontsize=12)\n",
    "        ax.set_title('Self-Rewarding Acceptance Rate', fontsize=14, weight='bold')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Sample evolution\n",
    "    ax = axes[0, 1]\n",
    "    sample = dataset[0]  # Track first sample\n",
    "    \n",
    "    evolution_data = [\n",
    "        {\"iteration\": 0, \"prompt_length\": len(sample.naive_prompt.split())}\n",
    "    ]\n",
    "    \n",
    "    for hist in sample.iteration_history:\n",
    "        if hist['accepted']:\n",
    "            evolution_data.append({\n",
    "                \"iteration\": hist['iteration'],\n",
    "                \"prompt_length\": len(hist['prompt'].split())\n",
    "            })\n",
    "    \n",
    "    if len(evolution_data) > 1:\n",
    "        df = pd.DataFrame(evolution_data)\n",
    "        ax.plot(df['iteration'], df['prompt_length'], 's-', linewidth=2, markersize=8, color='blue')\n",
    "        ax.set_xlabel('Iteration', fontsize=12)\n",
    "        ax.set_ylabel('Prompt Length (words)', fontsize=12)\n",
    "        ax.set_title('Prompt Evolution Example', fontsize=14, weight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Update distribution\n",
    "    ax = axes[1, 0]\n",
    "    update_counts = [0] * (optimizer.total_iterations + 1)\n",
    "    \n",
    "    for dp in dataset:\n",
    "        updates = sum(1 for h in dp.iteration_history if h.get('accepted', False))\n",
    "        if updates < len(update_counts):\n",
    "            update_counts[updates] += 1\n",
    "    \n",
    "    ax.bar(range(len(update_counts)), update_counts, color='orange', alpha=0.7)\n",
    "    ax.set_xlabel('Number of Updates', fontsize=12)\n",
    "    ax.set_ylabel('Sample Count', fontsize=12)\n",
    "    ax.set_title('Distribution of Updates per Sample', fontsize=14, weight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Performance simulation\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Simulate performance improvement\n",
    "    base_performance = 47.79  # From paper\n",
    "    ipl_improvements = [0, 0.51, 3.02, 4.34]  # Approximate from paper\n",
    "    \n",
    "    iterations_plot = list(range(len(ipl_improvements)))\n",
    "    performances = [base_performance + imp for imp in ipl_improvements]\n",
    "    \n",
    "    ax.plot(iterations_plot, performances, 'o-', linewidth=3, markersize=10, color='purple')\n",
    "    ax.axhline(base_performance, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    \n",
    "    # Add improvement annotations\n",
    "    for i, (iter_num, perf, imp) in enumerate(zip(iterations_plot[1:], performances[1:], ipl_improvements[1:])):\n",
    "        ax.annotate(f'+{imp:.1f}%', xy=(iter_num, perf), xytext=(iter_num, perf+0.5),\n",
    "                   ha='center', fontsize=9, color='darkgreen', weight='bold')\n",
    "    \n",
    "    ax.set_xlabel('IPL Iteration', fontsize=12)\n",
    "    ax.set_ylabel('Performance (%)', fontsize=12)\n",
    "    ax.set_title('IPL Performance Improvement (from paper)', fontsize=14, weight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_ipl_progress(ipl_optimizer, sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IPL Loss Functions\n",
    "\n",
    "### 3.1 IPL-DPO vs IPL-IPO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPLLoss(nn.Module):\n",
    "    \"\"\"IPL Loss implementation (Equation 12)\"\"\"\n",
    "    \n",
    "    def __init__(self, base_loss: str = \"IPO\", beta: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_logps: torch.Tensor,\n",
    "        rejected_logps: torch.Tensor,\n",
    "        ref_chosen_logps: torch.Tensor,\n",
    "        ref_rejected_logps: torch.Tensor,\n",
    "        updated_mask: torch.Tensor  # Indicates which samples were updated\n",
    "    ) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Compute IPL loss with dynamic updates\"\"\"\n",
    "        \n",
    "        # Compute log ratios\n",
    "        chosen_logratios = chosen_logps - ref_chosen_logps\n",
    "        rejected_logratios = rejected_logps - ref_rejected_logps\n",
    "        delta = chosen_logratios - rejected_logratios\n",
    "        \n",
    "        if self.base_loss == \"DPO\":\n",
    "            # IPL-DPO loss\n",
    "            losses = -torch.nn.functional.logsigmoid(self.beta * delta)\n",
    "        else:\n",
    "            # IPL-IPO loss\n",
    "            target = 1 / (2 * self.beta)\n",
    "            losses = (delta - target) ** 2\n",
    "        \n",
    "        # Apply update weighting\n",
    "        # Give more weight to updated samples\n",
    "        weights = torch.where(updated_mask, 1.5, 1.0)\n",
    "        weighted_loss = (losses * weights).mean()\n",
    "        \n",
    "        # Compute metrics\n",
    "        with torch.no_grad():\n",
    "            accuracy = (delta > 0).float().mean()\n",
    "            updated_accuracy = (delta[updated_mask] > 0).float().mean() if updated_mask.any() else 0\n",
    "            \n",
    "        return weighted_loss, {\n",
    "            'loss': weighted_loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'updated_accuracy': updated_accuracy.item(),\n",
    "            'delta_mean': delta.mean().item(),\n",
    "            'update_rate': updated_mask.float().mean().item()\n",
    "        }\n",
    "\n",
    "# Test IPL losses\n",
    "def test_ipl_losses():\n",
    "    \"\"\"Compare IPL-DPO and IPL-IPO losses\"\"\"\n",
    "    \n",
    "    batch_size = 8\n",
    "    \n",
    "    # Simulate logprobs\n",
    "    chosen_logps = torch.randn(batch_size) - 1\n",
    "    rejected_logps = torch.randn(batch_size) - 2\n",
    "    ref_chosen_logps = torch.randn(batch_size) - 1.5\n",
    "    ref_rejected_logps = torch.randn(batch_size) - 2.5\n",
    "    \n",
    "    # Simulate that 30% of samples were updated\n",
    "    updated_mask = torch.rand(batch_size) < 0.3\n",
    "    \n",
    "    # Test both losses\n",
    "    results = {}\n",
    "    \n",
    "    for loss_type in [\"DPO\", \"IPO\"]:\n",
    "        ipl_loss = IPLLoss(base_loss=loss_type, beta=0.01)\n",
    "        loss, metrics = ipl_loss(\n",
    "            chosen_logps, rejected_logps,\n",
    "            ref_chosen_logps, ref_rejected_logps,\n",
    "            updated_mask\n",
    "        )\n",
    "        results[f\"IPL-{loss_type}\"] = metrics\n",
    "    \n",
    "    # Display results\n",
    "    df = pd.DataFrame(results).T\n",
    "    print(\"IPL Loss Comparison:\")\n",
    "    print(df.round(4))\n",
    "    \n",
    "    return df\n",
    "\n",
    "ipl_results = test_ipl_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 IPL Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ipl_training():\n",
    "    \"\"\"Simulate IPL training dynamics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Training parameters\n",
    "    num_steps = 200\n",
    "    num_iterations = 3\n",
    "    steps_per_iteration = num_steps // num_iterations\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'IPL-DPO': {'loss': [], 'accuracy': [], 'delta': [], 'update_rate': []},\n",
    "        'IPL-IPO': {'loss': [], 'accuracy': [], 'delta': [], 'update_rate': []}\n",
    "    }\n",
    "    \n",
    "    # Simulate training\n",
    "    for step in range(num_steps):\n",
    "        iteration = step // steps_per_iteration + 1\n",
    "        \n",
    "        # Simulate metrics with iteration-based improvements\n",
    "        base_loss = 2.5 * np.exp(-0.02 * step)\n",
    "        base_acc = 0.5 + 0.4 * (1 - np.exp(-0.03 * step))\n",
    "        base_delta = -1 + 2 * (1 - np.exp(-0.025 * step))\n",
    "        \n",
    "        # Update rate increases then stabilizes each iteration\n",
    "        if iteration > 1:\n",
    "            update_rate = 0.02 + 0.02 * np.sin(0.1 * (step - iteration * steps_per_iteration))\n",
    "        else:\n",
    "            update_rate = 0\n",
    "        \n",
    "        # IPL-DPO (more volatile)\n",
    "        metrics['IPL-DPO']['loss'].append(base_loss + 0.1 * np.random.randn())\n",
    "        metrics['IPL-DPO']['accuracy'].append(base_acc + 0.05 * np.random.randn())\n",
    "        metrics['IPL-DPO']['delta'].append(base_delta + 0.2 * np.random.randn())\n",
    "        metrics['IPL-DPO']['update_rate'].append(update_rate)\n",
    "        \n",
    "        # IPL-IPO (more stable)\n",
    "        metrics['IPL-IPO']['loss'].append(base_loss * 0.8 + 0.05 * np.random.randn())\n",
    "        metrics['IPL-IPO']['accuracy'].append(base_acc + 0.02 + 0.03 * np.random.randn())\n",
    "        metrics['IPL-IPO']['delta'].append(base_delta + 0.5 + 0.1 * np.random.randn())\n",
    "        metrics['IPL-IPO']['update_rate'].append(update_rate * 1.2)\n",
    "    \n",
    "    # Plot results\n",
    "    steps = np.arange(num_steps)\n",
    "    \n",
    "    # Loss curves\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(steps, metrics['IPL-DPO']['loss'], label='IPL-DPO', alpha=0.8)\n",
    "    ax.plot(steps, metrics['IPL-IPO']['loss'], label='IPL-IPO', alpha=0.8)\n",
    "    \n",
    "    # Add iteration markers\n",
    "    for i in range(1, num_iterations):\n",
    "        ax.axvline(i * steps_per_iteration, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.text(i * steps_per_iteration, ax.get_ylim()[1] * 0.9, f'Iter {i+1}',\n",
    "               ha='center', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('IPL Loss Curves')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(steps, metrics['IPL-DPO']['accuracy'], label='IPL-DPO', alpha=0.8)\n",
    "    ax.plot(steps, metrics['IPL-IPO']['accuracy'], label='IPL-IPO', alpha=0.8)\n",
    "    for i in range(1, num_iterations):\n",
    "        ax.axvline(i * steps_per_iteration, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Preference Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Delta evolution\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(steps, metrics['IPL-DPO']['delta'], label='IPL-DPO', alpha=0.8)\n",
    "    ax.plot(steps, metrics['IPL-IPO']['delta'], label='IPL-IPO', alpha=0.8)\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(1/(2*0.01), color='green', linestyle='--', alpha=0.5, label='IPO Target')\n",
    "    for i in range(1, num_iterations):\n",
    "        ax.axvline(i * steps_per_iteration, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Delta (Î”)')\n",
    "    ax.set_title('Delta Evolution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Update rate\n",
    "    ax = axes[1, 0]\n",
    "    ax.fill_between(steps, 0, metrics['IPL-DPO']['update_rate'], \n",
    "                   alpha=0.5, label='IPL-DPO')\n",
    "    ax.fill_between(steps, 0, metrics['IPL-IPO']['update_rate'], \n",
    "                   alpha=0.5, label='IPL-IPO')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Update Rate')\n",
    "    ax.set_title('Self-Rewarding Update Rate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance comparison (from paper Table 3)\n",
    "    ax = axes[1, 1]\n",
    "    methods = ['Naive', 'DPO-70B', 'IPL-DPO-70B\\n(e3)', 'IPO-70B', 'IPL-IPO-70B\\n(e3)']\n",
    "    scores = [47.79, 49.07, 48.10, 50.94, 52.13]\n",
    "    colors = ['gray', 'lightblue', 'blue', 'lightgreen', 'green']\n",
    "    \n",
    "    bars = ax.bar(methods, scores, color=colors, alpha=0.7)\n",
    "    ax.axhline(47.79, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add improvement labels\n",
    "    for i, (method, score) in enumerate(zip(methods[1:], scores[1:]), 1):\n",
    "        improvement = score - 47.79\n",
    "        ax.text(i, score + 0.5, f'+{improvement:.2f}%', ha='center', fontsize=9)\n",
    "    \n",
    "    ax.set_ylabel('Weighted Average (%)')\n",
    "    ax.set_title('Final Performance Comparison (Table 3)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Selection rate analysis (from paper)\n",
    "    ax = axes[1, 2]\n",
    "    iterations = ['e1 (warmup)', 'e2', 'e3']\n",
    "    selection_rates = [0, 1.25, 2.40]\n",
    "    \n",
    "    ax.plot(iterations, selection_rates, 'o-', linewidth=3, markersize=10, color='purple')\n",
    "    ax.fill_between(range(len(iterations)), 0, selection_rates, alpha=0.3, color='purple')\n",
    "    \n",
    "    ax.set_xlabel('IPL Iteration')\n",
    "    ax.set_ylabel('Selection Rate (%)')\n",
    "    ax.set_title('Prompt Update Selection Rate (Table 7)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "simulate_ipl_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IPL Components Deep Dive\n",
    "\n",
    "### 4.1 Discrimination Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPLDiscriminator:\n",
    "    \"\"\"Discriminator for IPL prompt quality judgment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.discrimination_template = \"\"\"\n",
    "You are an expert of prompt discrimination.\n",
    "\n",
    "```\n",
    "Raw Prompt:\n",
    "{raw_prompt}\n",
    "```\n",
    "\n",
    "```\n",
    "New Prompt A:\n",
    "{prompt_a}\n",
    "```\n",
    "\n",
    "```\n",
    "New Prompt B:\n",
    "{prompt_b}\n",
    "```\n",
    "\n",
    "```\n",
    "Golden Response:\n",
    "{ground_truth}\n",
    "```\n",
    "\n",
    "New Prompt A and New Prompt B are optimized from Raw Prompt. \n",
    "Please judge which prompt is more loyal to the factual information of Raw Prompt, \n",
    "and is more desirable for an AI to generate the Golden Response. \n",
    "Only answer with A or B.\n",
    "\"\"\"\n",
    "    \n",
    "    def create_discrimination_example(\n",
    "        self,\n",
    "        raw_prompt: str,\n",
    "        current_prompt: str,\n",
    "        new_prompt: str,\n",
    "        ground_truth: str\n",
    "    ) -> str:\n",
    "        \"\"\"Create discrimination task\"\"\"\n",
    "        \n",
    "        return self.discrimination_template.format(\n",
    "            raw_prompt=raw_prompt,\n",
    "            prompt_a=current_prompt,\n",
    "            prompt_b=new_prompt,\n",
    "            ground_truth=ground_truth\n",
    "        )\n",
    "    \n",
    "    def analyze_prompt_quality(self, prompt: str) -> Dict[str, float]:\n",
    "        \"\"\"Analyze prompt quality features\"\"\"\n",
    "        \n",
    "        features = {\n",
    "            \"length\": len(prompt.split()),\n",
    "            \"specificity\": len([w for w in prompt.split() if len(w) > 6]) / max(len(prompt.split()), 1),\n",
    "            \"structure\": prompt.count(\".\") + prompt.count(\":\") + prompt.count(\",\"),\n",
    "            \"clarity_words\": sum(1 for word in [\"step\", \"first\", \"then\", \"finally\", \"ensure\"] \n",
    "                                if word in prompt.lower()),\n",
    "            \"instruction_strength\": sum(1 for word in [\"must\", \"should\", \"need\", \"required\"] \n",
    "                                      if word in prompt.lower())\n",
    "        }\n",
    "        \n",
    "        # Compute quality score\n",
    "        features[\"quality_score\"] = (\n",
    "            features[\"specificity\"] * 20 +\n",
    "            features[\"clarity_words\"] * 10 +\n",
    "            features[\"instruction_strength\"] * 5 +\n",
    "            min(features[\"length\"] / 10, 5)\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Demonstrate discrimination\n",
    "discriminator = IPLDiscriminator()\n",
    "\n",
    "# Example prompts\n",
    "examples = [\n",
    "    {\n",
    "        \"raw\": \"Calculate the average\",\n",
    "        \"current\": \"Find the average value\",\n",
    "        \"new\": \"Step 1: Add all numbers. Step 2: Divide by count to get average.\",\n",
    "        \"ground_truth\": \"The average is 44.25\"\n",
    "    },\n",
    "    {\n",
    "        \"raw\": \"Explain photosynthesis\",\n",
    "        \"current\": \"Tell me about photosynthesis\",\n",
    "        \"new\": \"Explain photosynthesis: First, describe light-dependent reactions. Then, explain the Calvin cycle. Finally, summarize the overall process.\",\n",
    "        \"ground_truth\": \"Photosynthesis converts light energy into chemical energy...\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Analyze examples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, (example, ax) in enumerate(zip(examples, axes)):\n",
    "    # Analyze both prompts\n",
    "    current_features = discriminator.analyze_prompt_quality(example[\"current\"])\n",
    "    new_features = discriminator.analyze_prompt_quality(example[\"new\"])\n",
    "    \n",
    "    # Compare features\n",
    "    feature_names = list(current_features.keys())\n",
    "    current_values = list(current_features.values())\n",
    "    new_values = list(new_features.values())\n",
    "    \n",
    "    x = np.arange(len(feature_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, current_values, width, label='Current', color='lightcoral', alpha=0.7)\n",
    "    bars2 = ax.bar(x + width/2, new_values, width, label='New', color='lightgreen', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Example {idx+1}: Prompt Quality Analysis')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add winner annotation\n",
    "    winner = \"New\" if new_features[\"quality_score\"] > current_features[\"quality_score\"] else \"Current\"\n",
    "    ax.text(0.5, 0.95, f\"Winner: {winner}\", transform=ax.transAxes, \n",
    "           ha='center', fontsize=12, weight='bold',\n",
    "           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show discrimination prompt example\n",
    "print(\"\\nDiscrimination Prompt Example:\")\n",
    "print(\"=\" * 60)\n",
    "disc_example = discriminator.create_discrimination_example(\n",
    "    examples[0][\"raw\"], examples[0][\"current\"], \n",
    "    examples[0][\"new\"], examples[0][\"ground_truth\"]\n",
    ")\n",
    "print(disc_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 IPL Data Augmentation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ipl_data_augmentation():\n",
    "    \"\"\"Visualize how IPL augments training data\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Left: Data composition\n",
    "    original_size = 30000  # Original POP dataset\n",
    "    instruction_data = 15000  # Additional instruction following\n",
    "    discrimination_data = 15000  # Additional discrimination\n",
    "    \n",
    "    sizes = [original_size, instruction_data, discrimination_data]\n",
    "    labels = ['Original POP\\n(30k)', 'Instruction\\nFollowing (15k)', 'Discrimination\\n(15k)']\n",
    "    colors = ['lightblue', 'lightgreen', 'lightyellow']\n",
    "    explode = (0.05, 0.05, 0.05)\n",
    "    \n",
    "    ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', \n",
    "           startangle=90, explode=explode, shadow=True)\n",
    "    ax1.set_title('IPL Training Data Composition', fontsize=14, weight='bold')\n",
    "    \n",
    "    # Right: Data flow\n",
    "    ax2.text(0.5, 0.9, \"IPL Data Flow\", ha='center', fontsize=16, weight='bold',\n",
    "            transform=ax2.transAxes)\n",
    "    \n",
    "    # Draw flow diagram\n",
    "    stages = [\n",
    "        {\"y\": 0.7, \"text\": \"Original 30k POP Data\", \"color\": \"lightblue\"},\n",
    "        {\"y\": 0.5, \"text\": \"â†“ Reuse for IPL\", \"color\": \"white\"},\n",
    "        {\"y\": 0.3, \"text\": \"15k Instruction + 15k Discrimination\", \"color\": \"lightgreen\"},\n",
    "        {\"y\": 0.1, \"text\": \"Total: 60k Training Examples\", \"color\": \"gold\"}\n",
    "    ]\n",
    "    \n",
    "    for stage in stages:\n",
    "        ax2.text(0.5, stage[\"y\"], stage[\"text\"], ha='center', fontsize=12,\n",
    "                transform=ax2.transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=stage[\"color\"], alpha=0.7))\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show data examples\n",
    "    print(\"\\nIPL Data Examples:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"1. Original Preference Data:\")\n",
    "    print(\"   - Naive prompt: 'Calculate the sum'\")\n",
    "    print(\"   - Chosen: 'To calculate the sum, add all numbers together'\")\n",
    "    print(\"   - Rejected: 'Find the sum'\")\n",
    "    \n",
    "    print(\"\\n2. Instruction Following Data (reused):\")\n",
    "    print(\"   - Input: 'Calculate the sum'\")\n",
    "    print(\"   - Output: 'The sum is 150'\")\n",
    "    \n",
    "    print(\"\\n3. Discrimination Data (reused):\")\n",
    "    print(\"   - Task: Judge between two prompts\")\n",
    "    print(\"   - Answer: A or B\")\n",
    "\n",
    "visualize_ipl_data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical IPL Implementation\n",
    "\n",
    "### 5.1 Complete IPL Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPLTrainingPipeline:\n",
    "    \"\"\"Complete IPL training pipeline\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Tulu2-70B\",\n",
    "        base_method: str = \"IPO\",\n",
    "        num_iterations: int = 3,\n",
    "        beta: float = 0.01\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.base_method = base_method\n",
    "        self.num_iterations = num_iterations\n",
    "        self.beta = beta\n",
    "        \n",
    "        # Components\n",
    "        self.optimizer = IPLOptimizer(base_method, beta, num_iterations)\n",
    "        self.discriminator = IPLDiscriminator()\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            \"iteration\": [],\n",
    "            \"performance\": [],\n",
    "            \"selection_rate\": [],\n",
    "            \"discrimination_accuracy\": []\n",
    "        }\n",
    "    \n",
    "    def run_training(\n",
    "        self,\n",
    "        initial_dataset: List[IPLDataPoint],\n",
    "        validation_set: List[IPLDataPoint]\n",
    "    ):\n",
    "        \"\"\"Run complete IPL training\"\"\"\n",
    "        \n",
    "        print(f\"Starting IPL-{self.base_method} training on {self.model_name}\")\n",
    "        print(f\"Dataset size: {len(initial_dataset)}\")\n",
    "        print(f\"Iterations: {self.num_iterations}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        current_dataset = initial_dataset.copy()\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            print(f\"\\nIteration {iteration + 1}/{self.num_iterations}\")\n",
    "            \n",
    "            # Run IPL iteration\n",
    "            current_dataset, stats = self.optimizer.run_iteration(current_dataset)\n",
    "            \n",
    "            # Simulate performance evaluation\n",
    "            performance = self._evaluate_performance(current_dataset, validation_set)\n",
    "            \n",
    "            # Record history\n",
    "            self.training_history[\"iteration\"].append(iteration + 1)\n",
    "            self.training_history[\"performance\"].append(performance)\n",
    "            self.training_history[\"selection_rate\"].append(\n",
    "                stats.get(\"acceptance_rate\", 0) * 100\n",
    "            )\n",
    "            self.training_history[\"discrimination_accuracy\"].append(\n",
    "                100 if iteration == 0 else 95 + np.random.randn() * 2\n",
    "            )\n",
    "            \n",
    "            print(f\"Performance: {performance:.2f}%\")\n",
    "            print(f\"Selection rate: {stats.get('acceptance_rate', 0):.1%}\")\n",
    "        \n",
    "        return current_dataset, self.training_history\n",
    "    \n",
    "    def _evaluate_performance(self, dataset, validation_set):\n",
    "        \"\"\"Simulate performance evaluation\"\"\"\n",
    "        \n",
    "        # Base performance from paper\n",
    "        base_performances = {\n",
    "            \"IPO\": [47.79, 48.30, 50.81, 52.13],\n",
    "            \"DPO\": [47.79, 48.12, 48.23, 48.10]\n",
    "        }\n",
    "        \n",
    "        iteration = self.optimizer.current_iteration\n",
    "        if iteration < len(base_performances[self.base_method]):\n",
    "            return base_performances[self.base_method][iteration]\n",
    "        else:\n",
    "            # Extrapolate with diminishing returns\n",
    "            last_perf = base_performances[self.base_method][-1]\n",
    "            return last_perf + np.random.randn() * 0.5\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize training results\"\"\"\n",
    "        \n",
    "        history = self.training_history\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Performance curve\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(history[\"iteration\"], history[\"performance\"], \n",
    "               'o-', linewidth=3, markersize=10, color='green')\n",
    "        ax.axhline(history[\"performance\"][0], color='red', \n",
    "                  linestyle='--', alpha=0.5, label='Baseline')\n",
    "        \n",
    "        for i, (iter_num, perf) in enumerate(zip(history[\"iteration\"][1:], \n",
    "                                                 history[\"performance\"][1:]), 1):\n",
    "            improvement = perf - history[\"performance\"][0]\n",
    "            ax.annotate(f'+{improvement:.2f}%', \n",
    "                       xy=(iter_num, perf), \n",
    "                       xytext=(iter_num, perf + 0.5),\n",
    "                       ha='center', fontsize=9, color='darkgreen')\n",
    "        \n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Performance (%)')\n",
    "        ax.set_title(f'IPL-{self.base_method} Performance')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Selection rate\n",
    "        ax = axes[0, 1]\n",
    "        ax.bar(history[\"iteration\"], history[\"selection_rate\"], \n",
    "              color='orange', alpha=0.7)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Selection Rate (%)')\n",
    "        ax.set_title('Prompt Update Selection Rate')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Discrimination accuracy\n",
    "        ax = axes[1, 0]\n",
    "        ax.plot(history[\"iteration\"], history[\"discrimination_accuracy\"],\n",
    "               's-', linewidth=2, markersize=8, color='purple')\n",
    "        ax.axhline(100, color='green', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Discrimination Accuracy (%)')\n",
    "        ax.set_title('Self-Judgment Accuracy')\n",
    "        ax.set_ylim(90, 105)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Summary statistics\n",
    "        ax = axes[1, 1]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "IPL-{self.base_method} Training Summary\n",
    "{'='*40}\n",
    "Model: {self.model_name}\n",
    "Iterations: {self.num_iterations}\n",
    "Beta: {self.beta}\n",
    "\n",
    "Final Performance: {history['performance'][-1]:.2f}%\n",
    "Total Improvement: +{history['performance'][-1] - history['performance'][0]:.2f}%\n",
    "Average Selection Rate: {np.mean(history['selection_rate'][1:]):.1f}%\n",
    "\n",
    "Key Insights:\n",
    "â€¢ IPL-IPO shows steady improvement\n",
    "â€¢ Conservative selection (2.4% final)\n",
    "â€¢ High discrimination accuracy (>95%)\n",
    "\"\"\"\n",
    "        \n",
    "        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,\n",
    "               fontsize=11, verticalalignment='top',\n",
    "               fontfamily='monospace',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run IPL training simulation\n",
    "print(\"IPL Training Simulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create datasets\n",
    "train_data = [IPLDataPoint(\n",
    "    naive_prompt=f\"Task {i}\",\n",
    "    naive_response=f\"Response {i}\",\n",
    "    ground_truth=f\"Ground truth {i}\"\n",
    ") for i in range(100)]\n",
    "\n",
    "val_data = train_data[:20]  # Use subset for validation\n",
    "\n",
    "# Train IPL-IPO\n",
    "pipeline = IPLTrainingPipeline(base_method=\"IPO\")\n",
    "final_dataset, history = pipeline.run_training(train_data, val_data)\n",
    "\n",
    "# Visualize results\n",
    "pipeline.visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Key Takeaways\n",
    "\n",
    "### Core IPL Concepts:\n",
    "\n",
    "1. **Self-Rewarding Mechanism**:\n",
    "   - Model generates new prompts and judges their quality\n",
    "   - Conservative updates (only 2.4% acceptance rate)\n",
    "   - Iterative improvement through self-evaluation\n",
    "\n",
    "2. **Algorithm Components**:\n",
    "   - Warmup in first iteration\n",
    "   - Dynamic prompt generation (xn+)\n",
    "   - Self-discrimination for quality judgment\n",
    "   - Conditional dataset updates\n",
    "\n",
    "3. **IPL Advantages**:\n",
    "   - IPL-IPO achieves best results (52.13%)\n",
    "   - Steady improvement across iterations\n",
    "   - No external feedback required\n",
    "   - Maintains high discrimination accuracy\n",
    "\n",
    "4. **Implementation Details**:\n",
    "   - Requires additional discrimination & instruction data\n",
    "   - Total 60k training examples (30k original + 30k augmented)\n",
    "   - 3 iterations optimal (e1=warmup, e2-e3=updates)\n",
    "   - Beta=0.01 for stable training\n",
    "\n",
    "### Practical Tips:\n",
    "\n",
    "- Start with IPO as base method (more stable than DPO)\n",
    "- Monitor selection rate (should be conservative)\n",
    "- Ensure discrimination accuracy stays high (>95%)\n",
    "- Use warmup iteration to establish baseline\n",
    "\n",
    "IPL represents a significant advancement in self-improving AI systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}