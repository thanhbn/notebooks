{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitters trong LangChain\n",
    "\n",
    "## Tại sao cần chia văn bản?\n",
    "\n",
    "Khi làm việc với LLMs và RAG systems, chúng ta thường gặp phải những thách thức sau:\n",
    "\n",
    "### 1. **Giới hạn Context Window**\n",
    "- Hầu hết LLMs có giới hạn về số tokens có thể xử lý trong một lần\n",
    "- Ví dụ: GPT-3.5 (~4K tokens), GPT-4 (~8K-32K tokens), Claude-3 (~200K tokens)\n",
    "- Documents dài có thể vượt quá giới hạn này\n",
    "\n",
    "### 2. **Hiệu quả Retrieval**\n",
    "- Khi search trong vector database, chunks nhỏ hơn thường cho kết quả chính xác hơn\n",
    "- Chunks lớn có thể chứa nhiều topics, làm giảm precision\n",
    "\n",
    "### 3. **Cost Optimization**\n",
    "- Chỉ retrieve và process những phần relevant\n",
    "- Giảm số tokens gửi lên LLM\n",
    "\n",
    "### 4. **Semantic Coherence**\n",
    "- Mỗi chunk nên chứa một ý tưởng hoặc concept hoàn chỉnh\n",
    "- Tránh cắt ngang giữa câu hoặc paragraph\n",
    "\n",
    "Text Splitters giúp giải quyết những vấn đề này bằng cách chia documents thành những chunks có kích thước phù hợp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup và chuẩn bị dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    SpacyTextSplitter,\n",
    "    NLTKTextSplitter\n",
    ")\n",
    "from langchain.schema import Document\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo sample text cho demonstration\n",
    "sample_text = \"\"\"Giới thiệu về RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "RAG là một kỹ thuật kết hợp khả năng truy xuất thông tin (retrieval) với khả năng sinh văn bản (generation) của các Large Language Models. Phương pháp này giúp cải thiện đáng kể chất lượng và độ chính xác của các câu trả lời được tạo ra.\n",
    "\n",
    "Kiến trúc RAG System\n",
    "\n",
    "Một hệ thống RAG điển hình bao gồm các thành phần chính sau:\n",
    "\n",
    "1. Document Loading và Preprocessing\n",
    "Bước đầu tiên là tải và xử lý các documents từ nhiều nguồn khác nhau như PDF, websites, databases. Quá trình này bao gồm việc làm sạch dữ liệu, loại bỏ các ký tự đặc biệt, và chuẩn hóa format.\n",
    "\n",
    "2. Text Splitting\n",
    "Documents được chia thành các chunks nhỏ hơn để dễ dàng xử lý và tìm kiếm. Việc chia này phải đảm bảo tính toàn vẹn về mặt ngữ nghĩa và tránh cắt ngang giữa các ý tưởng quan trọng.\n",
    "\n",
    "3. Embedding Generation\n",
    "Mỗi chunk text được chuyển đổi thành vector embeddings sử dụng các mô hình như OpenAI Embeddings, Sentence-BERT, hoặc các mô hình embedding khác. Embeddings này capture được semantic meaning của text.\n",
    "\n",
    "4. Vector Storage\n",
    "Các embeddings được lưu trữ trong vector databases như Pinecone, Chroma, FAISS, hoặc Weaviate. Các databases này được tối ưu hóa cho việc tìm kiếm similarity.\n",
    "\n",
    "5. Query Processing\n",
    "Khi user đưa ra một câu hỏi, query cũng được convert thành embedding vector sử dụng cùng model đã dùng để embed documents.\n",
    "\n",
    "6. Similarity Search\n",
    "Vector database thực hiện similarity search để tìm ra các chunks có embedding gần nhất với query embedding. Thường sử dụng cosine similarity hoặc euclidean distance.\n",
    "\n",
    "7. Context Construction\n",
    "Các chunks relevant nhất được kết hợp lại thành context. Số lượng chunks được chọn phụ thuộc vào context window của LLM và độ phức tạp của câu hỏi.\n",
    "\n",
    "8. Response Generation\n",
    "Cuối cùng, LLM sử dụng context đã được retrieve cùng với original query để generate ra câu trả lời chính xác và có căn cứ.\n",
    "\n",
    "Ưu điểm của RAG\n",
    "\n",
    "RAG mang lại nhiều lợi ích quan trọng:\n",
    "- Cải thiện độ chính xác: Câu trả lời được base trên thông tin thực tế từ documents\n",
    "- Giảm hallucination: LLM ít có khả năng tạo ra thông tin sai lệch\n",
    "- Cập nhật thông tin: Có thể easily update knowledge base mà không cần retrain model\n",
    "- Traceability: Có thể trace back câu trả lời đến nguồn gốc thông tin\n",
    "- Cost-effective: Không cần fine-tune LLM với domain-specific data\n",
    "\n",
    "Thách thức và Giải pháp\n",
    "\n",
    "Tuy nhiên, RAG cũng đối mặt với một số thách thức:\n",
    "- Chất lượng retrieval: Cần optimize embedding models và search algorithms\n",
    "- Chunk size optimization: Balance giữa semantic coherence và retrieval precision\n",
    "- Latency: Multiple steps có thể tăng response time\n",
    "- Context management: Quản lý context window limits của LLMs\n",
    "\n",
    "Để giải quyết các thách thức này, các practitioners thường áp dụng các techniques như re-ranking, query expansion, và hybrid search approaches.\n",
    "\"\"\"\n",
    "\n",
    "# Tạo Document object\n",
    "sample_doc = Document(\n",
    "    page_content=sample_text,\n",
    "    metadata={\"source\": \"rag_tutorial\", \"author\": \"LangChain Tutorial\"}\n",
    ")\n",
    "\n",
    "print(f\"Sample text length: {len(sample_text)} characters\")\n",
    "print(f\"Word count: {len(sample_text.split())} words\")\n",
    "print(f\"Line count: {len(sample_text.split(chr(10)))} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CharacterTextSplitter - Splitter cơ bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharacterTextSplitter cơ bản\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Chia theo paragraph\n",
    "    chunk_size=500,   # Kích thước mỗi chunk (characters)\n",
    "    chunk_overlap=50, # Overlap giữa các chunks\n",
    "    length_function=len,  # Function để đo độ dài\n",
    "    is_separator_regex=False  # Separator là string literal, không phải regex\n",
    ")\n",
    "\n",
    "# Split document\n",
    "char_chunks = char_splitter.split_documents([sample_doc])\n",
    "\n",
    "print(f\"CharacterTextSplitter results:\")\n",
    "print(f\"Number of chunks: {len(char_chunks)}\")\n",
    "print(f\"\\nChunk sizes:\")\n",
    "for i, chunk in enumerate(char_chunks):\n",
    "    print(f\"Chunk {i+1}: {len(chunk.page_content)} characters\")\n",
    "\n",
    "print(f\"\\n=== First 3 chunks ===\")\n",
    "for i, chunk in enumerate(char_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(chunk.page_content[:200] + \"...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thử với separator khác\n",
    "char_splitter_newline = CharacterTextSplitter(\n",
    "    separator=\"\\n\",     # Chia theo line breaks\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "char_chunks_newline = char_splitter_newline.split_documents([sample_doc])\n",
    "\n",
    "print(f\"\\nCharacterTextSplitter với separator='\\\\n':\")\n",
    "print(f\"Number of chunks: {len(char_chunks_newline)}\")\n",
    "\n",
    "# So sánh kích thước chunks\n",
    "sizes_paragraph = [len(chunk.page_content) for chunk in char_chunks]\n",
    "sizes_newline = [len(chunk.page_content) for chunk in char_chunks_newline]\n",
    "\n",
    "print(f\"\\nChunk size comparison:\")\n",
    "print(f\"Paragraph separator - Average: {np.mean(sizes_paragraph):.1f}, Std: {np.std(sizes_paragraph):.1f}\")\n",
    "print(f\"Newline separator - Average: {np.mean(sizes_newline):.1f}, Std: {np.std(sizes_newline):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RecursiveCharacterTextSplitter - Splitter thông minh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter - default settings\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    # Default separators: [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    separators=None  # Sử dụng default separators\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_documents([sample_doc])\n",
    "\n",
    "print(f\"RecursiveCharacterTextSplitter results:\")\n",
    "print(f\"Number of chunks: {len(recursive_chunks)}\")\n",
    "print(f\"\\nChunk sizes:\")\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i+1}: {len(chunk.page_content)} characters\")\n",
    "\n",
    "print(f\"\\n=== First 3 chunks ===\")\n",
    "for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(chunk.page_content)\n",
    "    print(f\"Length: {len(chunk.page_content)} chars\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom separators cho Vietnamese text\n",
    "vietnamese_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=40,\n",
    "    separators=[\n",
    "        \"\\n\\n\",  # Paragraph breaks\n",
    "        \"\\n\",    # Line breaks\n",
    "        \". \",    # Sentence endings\n",
    "        \", \",    # Clause separators\n",
    "        \" \",     # Word boundaries\n",
    "        \"\"       # Character level (last resort)\n",
    "    ]\n",
    ")\n",
    "\n",
    "vietnamese_chunks = vietnamese_splitter.split_documents([sample_doc])\n",
    "\n",
    "print(f\"Vietnamese-optimized splitter:\")\n",
    "print(f\"Number of chunks: {len(vietnamese_chunks)}\")\n",
    "\n",
    "# Analyze where splits occur\n",
    "print(f\"\\n=== Split analysis ===\")\n",
    "for i, chunk in enumerate(vietnamese_chunks[:4]):\n",
    "    # Check how chunk starts and ends\n",
    "    start_char = chunk.page_content[0] if chunk.page_content else ''\n",
    "    end_chars = chunk.page_content[-5:] if len(chunk.page_content) >= 5 else chunk.page_content\n",
    "    \n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  Length: {len(chunk.page_content)} chars\")\n",
    "    print(f\"  Starts with: '{start_char}'\")\n",
    "    print(f\"  Ends with: '{end_chars}'\")\n",
    "    print(f\"  Preview: {chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tham số chunk_size và chunk_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thử nghiệm với các chunk_size khác nhau\n",
    "chunk_sizes = [200, 400, 600, 800]\n",
    "overlap_ratio = 0.1  # 10% overlap\n",
    "\n",
    "results = {}\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    overlap = int(size * overlap_ratio)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents([sample_doc])\n",
    "    \n",
    "    results[size] = {\n",
    "        'num_chunks': len(chunks),\n",
    "        'avg_size': np.mean([len(c.page_content) for c in chunks]),\n",
    "        'size_std': np.std([len(c.page_content) for c in chunks]),\n",
    "        'overlap_used': overlap\n",
    "    }\n",
    "\n",
    "print(\"Chunk size analysis:\")\n",
    "print(f\"{'Size':<6} {'Overlap':<7} {'Chunks':<6} {'Avg Size':<9} {'Std Dev':<8}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for size, stats in results.items():\n",
    "    print(f\"{size:<6} {stats['overlap_used']:<7} {stats['num_chunks']:<6} \"\n",
    "          f\"{stats['avg_size']:<9.1f} {stats['size_std']:<8.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thử nghiệm với các overlap ratios khác nhau\n",
    "overlap_ratios = [0, 0.05, 0.1, 0.2, 0.3]\n",
    "fixed_chunk_size = 500\n",
    "\n",
    "overlap_results = {}\n",
    "\n",
    "for ratio in overlap_ratios:\n",
    "    overlap = int(fixed_chunk_size * ratio)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=fixed_chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents([sample_doc])\n",
    "    \n",
    "    # Calculate actual overlap\n",
    "    actual_overlaps = []\n",
    "    for i in range(len(chunks) - 1):\n",
    "        current_chunk = chunks[i].page_content\n",
    "        next_chunk = chunks[i + 1].page_content\n",
    "        \n",
    "        # Find overlap (simplified approach)\n",
    "        overlap_chars = 0\n",
    "        for j in range(1, min(len(current_chunk), len(next_chunk)) + 1):\n",
    "            if current_chunk[-j:] == next_chunk[:j]:\n",
    "                overlap_chars = j\n",
    "        actual_overlaps.append(overlap_chars)\n",
    "    \n",
    "    overlap_results[ratio] = {\n",
    "        'num_chunks': len(chunks),\n",
    "        'overlap_setting': overlap,\n",
    "        'avg_actual_overlap': np.mean(actual_overlaps) if actual_overlaps else 0\n",
    "    }\n",
    "\n",
    "print(f\"\\nOverlap analysis (chunk_size={fixed_chunk_size}):\")\n",
    "print(f\"{'Ratio':<6} {'Setting':<8} {'Chunks':<6} {'Actual Avg':<12}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for ratio, stats in overlap_results.items():\n",
    "    print(f\"{ratio:<6} {stats['overlap_setting']:<8} {stats['num_chunks']:<6} \"\n",
    "          f\"{stats['avg_actual_overlap']:<12.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing chunk overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo visualization để hiểu overlap\n",
    "def visualize_chunks_overlap(chunks, max_chunks=5):\n",
    "    \"\"\"Visualize how chunks overlap\"\"\"\n",
    "    print(\"=== CHUNK OVERLAP VISUALIZATION ===\")\n",
    "    \n",
    "    for i in range(min(len(chunks) - 1, max_chunks - 1)):\n",
    "        current = chunks[i].page_content\n",
    "        next_chunk = chunks[i + 1].page_content\n",
    "        \n",
    "        print(f\"\\nChunk {i+1} -> Chunk {i+2}:\")\n",
    "        print(f\"Chunk {i+1} ending: ...{current[-100:]}\")\n",
    "        print(f\"Chunk {i+2} starting: {next_chunk[:100]}...\")\n",
    "        \n",
    "        # Find potential overlap\n",
    "        overlap_found = False\n",
    "        for length in range(50, 5, -5):  # Check overlap từ 50 chars xuống 5\n",
    "            if current[-length:] in next_chunk[:length*2]:\n",
    "                overlap_text = current[-length:]\n",
    "                print(f\"OVERLAP DETECTED ({length} chars): '{overlap_text[:50]}...'\")\n",
    "                overlap_found = True\n",
    "                break\n",
    "        \n",
    "        if not overlap_found:\n",
    "            print(\"No significant overlap detected\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Test overlap visualization\n",
    "test_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "test_chunks = test_splitter.split_documents([sample_doc])\n",
    "visualize_chunks_overlap(test_chunks, max_chunks=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. So sánh CharacterTextSplitter vs RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sánh trực tiếp hai loại splitter\n",
    "comparison_chunk_size = 400\n",
    "comparison_overlap = 50\n",
    "\n",
    "# CharacterTextSplitter\n",
    "char_splitter_comp = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=comparison_chunk_size,\n",
    "    chunk_overlap=comparison_overlap\n",
    ")\n",
    "\n",
    "# RecursiveCharacterTextSplitter\n",
    "recursive_splitter_comp = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=comparison_chunk_size,\n",
    "    chunk_overlap=comparison_overlap\n",
    ")\n",
    "\n",
    "char_chunks_comp = char_splitter_comp.split_documents([sample_doc])\n",
    "recursive_chunks_comp = recursive_splitter_comp.split_documents([sample_doc])\n",
    "\n",
    "print(\"=== COMPARISON: CharacterTextSplitter vs RecursiveCharacterTextSplitter ===\")\n",
    "print(f\"Settings: chunk_size={comparison_chunk_size}, chunk_overlap={comparison_overlap}\")\n",
    "print(f\"\\nCharacterTextSplitter:\")\n",
    "print(f\"  - Number of chunks: {len(char_chunks_comp)}\")\n",
    "print(f\"  - Average chunk size: {np.mean([len(c.page_content) for c in char_chunks_comp]):.1f}\")\n",
    "print(f\"  - Size std deviation: {np.std([len(c.page_content) for c in char_chunks_comp]):.1f}\")\n",
    "\n",
    "print(f\"\\nRecursiveCharacterTextSplitter:\")\n",
    "print(f\"  - Number of chunks: {len(recursive_chunks_comp)}\")\n",
    "print(f\"  - Average chunk size: {np.mean([len(c.page_content) for c in recursive_chunks_comp]):.1f}\")\n",
    "print(f\"  - Size std deviation: {np.std([len(c.page_content) for c in recursive_chunks_comp]):.1f}\")\n",
    "\n",
    "# Show example chunks\n",
    "print(f\"\\n=== CHUNK EXAMPLES ===\")\n",
    "print(f\"\\nCharacterTextSplitter - Chunk 1:\")\n",
    "print(char_chunks_comp[0].page_content[:300] + \"...\")\n",
    "\n",
    "print(f\"\\nRecursiveCharacterTextSplitter - Chunk 1:\")\n",
    "print(recursive_chunks_comp[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze semantic coherence\n",
    "def analyze_chunk_endings(chunks, splitter_name):\n",
    "    \"\"\"Analyze how chunks end - complete sentences vs cut-off\"\"\"\n",
    "    complete_sentences = 0\n",
    "    cut_off_sentences = 0\n",
    "    \n",
    "    print(f\"\\n=== {splitter_name} - Ending Analysis ===\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks[:5]):  # Analyze first 5 chunks\n",
    "        content = chunk.page_content.strip()\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        last_char = content[-1]\n",
    "        last_sentence = content.split('.')[-1].strip()\n",
    "        \n",
    "        if last_char in '.!?':\n",
    "            complete_sentences += 1\n",
    "            ending_type = \"Complete sentence\"\n",
    "        else:\n",
    "            cut_off_sentences += 1\n",
    "            ending_type = \"Cut-off\"\n",
    "        \n",
    "        print(f\"Chunk {i+1}: {ending_type}\")\n",
    "        print(f\"  Last 80 chars: ...{content[-80:]}\")\n",
    "        print(f\"  Ends with: '{last_char}'\")\n",
    "        print()\n",
    "    \n",
    "    return complete_sentences, cut_off_sentences\n",
    "\n",
    "# Analyze both splitters\n",
    "char_complete, char_cutoff = analyze_chunk_endings(char_chunks_comp, \"CharacterTextSplitter\")\n",
    "recursive_complete, recursive_cutoff = analyze_chunk_endings(recursive_chunks_comp, \"RecursiveCharacterTextSplitter\")\n",
    "\n",
    "print(f\"\\n=== SEMANTIC COHERENCE SUMMARY ===\")\n",
    "print(f\"CharacterTextSplitter:\")\n",
    "print(f\"  - Complete sentences: {char_complete}\")\n",
    "print(f\"  - Cut-off sentences: {char_cutoff}\")\n",
    "print(f\"  - Coherence score: {char_complete/(char_complete+char_cutoff)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nRecursiveCharacterTextSplitter:\")\n",
    "print(f\"  - Complete sentences: {recursive_complete}\")\n",
    "print(f\"  - Cut-off sentences: {recursive_cutoff}\")\n",
    "print(f\"  - Coherence score: {recursive_complete/(recursive_complete+recursive_cutoff)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TokenTextSplitter - Split theo tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TokenTextSplitter sử dụng tiktoken (OpenAI tokenizer)\n",
    "try:\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        encoding_name=\"cl100k_base\",  # GPT-4 encoding\n",
    "        chunk_size=200,  # 200 tokens per chunk\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    \n",
    "    token_chunks = token_splitter.split_documents([sample_doc])\n",
    "    \n",
    "    print(f\"TokenTextSplitter results:\")\n",
    "    print(f\"Number of chunks: {len(token_chunks)}\")\n",
    "    \n",
    "    # Analyze token counts\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    print(f\"\\nToken analysis:\")\n",
    "    for i, chunk in enumerate(token_chunks[:3]):\n",
    "        tokens = encoding.encode(chunk.page_content)\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(f\"  Characters: {len(chunk.page_content)}\")\n",
    "        print(f\"  Tokens: {len(tokens)}\")\n",
    "        print(f\"  Chars/Token ratio: {len(chunk.page_content)/len(tokens):.2f}\")\n",
    "        print(f\"  Preview: {chunk.page_content[:100]}...\")\n",
    "        print()\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"tiktoken not installed. Install with: pip install tiktoken\")\n",
    "    \n",
    "    # Alternative: count \"words\" as proxy for tokens\n",
    "    print(\"Using word-based approximation instead:\")\n",
    "    \n",
    "    def count_words(text):\n",
    "        return len(text.split())\n",
    "    \n",
    "    word_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=150,  # Approximate 200 tokens ≈ 150 words\n",
    "        chunk_overlap=15,\n",
    "        length_function=count_words  # Use word count instead of character count\n",
    "    )\n",
    "    \n",
    "    word_chunks = word_splitter.split_documents([sample_doc])\n",
    "    \n",
    "    print(f\"Word-based splitting:\")\n",
    "    print(f\"Number of chunks: {len(word_chunks)}\")\n",
    "    for i, chunk in enumerate(word_chunks[:3]):\n",
    "        print(f\"Chunk {i+1}: {count_words(chunk.page_content)} words, {len(chunk.page_content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices và Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function để test và recommend optimal settings\n",
    "def recommend_chunk_settings(document, target_chunks=None, max_chunk_size=1000):\n",
    "    \"\"\"Recommend optimal chunk settings based on document characteristics\"\"\"\n",
    "    doc_length = len(document.page_content)\n",
    "    word_count = len(document.page_content.split())\n",
    "    \n",
    "    print(f\"=== DOCUMENT ANALYSIS ===\")\n",
    "    print(f\"Document length: {doc_length:,} characters\")\n",
    "    print(f\"Word count: {word_count:,} words\")\n",
    "    print(f\"Average word length: {doc_length/word_count:.1f} chars/word\")\n",
    "    \n",
    "    # Analyze document structure\n",
    "    paragraphs = document.page_content.split('\\n\\n')\n",
    "    sentences = document.page_content.split('. ')\n",
    "    \n",
    "    avg_paragraph_length = np.mean([len(p) for p in paragraphs if p.strip()])\n",
    "    avg_sentence_length = np.mean([len(s) for s in sentences if s.strip()])\n",
    "    \n",
    "    print(f\"\\n=== STRUCTURE ANALYSIS ===\")\n",
    "    print(f\"Paragraphs: {len(paragraphs)}\")\n",
    "    print(f\"Average paragraph length: {avg_paragraph_length:.0f} chars\")\n",
    "    print(f\"Sentences: {len(sentences)}\")\n",
    "    print(f\"Average sentence length: {avg_sentence_length:.0f} chars\")\n",
    "    \n",
    "    # Make recommendations\n",
    "    print(f\"\\n=== RECOMMENDATIONS ===\")\n",
    "    \n",
    "    if target_chunks:\n",
    "        recommended_size = doc_length // target_chunks\n",
    "        print(f\"For {target_chunks} chunks: chunk_size ≈ {recommended_size}\")\n",
    "    else:\n",
    "        # Base recommendations on document structure\n",
    "        if avg_paragraph_length > 800:\n",
    "            recommended_size = int(avg_paragraph_length * 0.8)\n",
    "            print(f\"Large paragraphs detected. Recommended chunk_size: {recommended_size}\")\n",
    "        elif avg_paragraph_length < 200:\n",
    "            recommended_size = int(avg_paragraph_length * 3)\n",
    "            print(f\"Small paragraphs detected. Recommended chunk_size: {recommended_size}\")\n",
    "        else:\n",
    "            recommended_size = int(avg_paragraph_length * 1.5)\n",
    "            print(f\"Medium paragraphs. Recommended chunk_size: {recommended_size}\")\n",
    "    \n",
    "    # Overlap recommendation\n",
    "    recommended_overlap = max(20, int(recommended_size * 0.1))\n",
    "    print(f\"Recommended overlap: {recommended_overlap} (10% of chunk_size)\")\n",
    "    \n",
    "    # Splitter recommendation\n",
    "    if avg_sentence_length > 100:\n",
    "        print(f\"Long sentences detected. Recommend RecursiveCharacterTextSplitter\")\n",
    "        splitter_rec = \"RecursiveCharacterTextSplitter\"\n",
    "    else:\n",
    "        print(f\"Well-structured text. CharacterTextSplitter might work well\")\n",
    "        splitter_rec = \"CharacterTextSplitter\"\n",
    "    \n",
    "    return {\n",
    "        'chunk_size': min(recommended_size, max_chunk_size),\n",
    "        'chunk_overlap': recommended_overlap,\n",
    "        'splitter_type': splitter_rec\n",
    "    }\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = recommend_chunk_settings(sample_doc, target_chunks=8)\n",
    "\n",
    "# Test recommended settings\n",
    "print(f\"\\n=== TESTING RECOMMENDATIONS ===\")\n",
    "test_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=recommendations['chunk_size'],\n",
    "    chunk_overlap=recommendations['chunk_overlap']\n",
    ")\n",
    "\n",
    "test_chunks = test_splitter.split_documents([sample_doc])\n",
    "print(f\"Result: {len(test_chunks)} chunks created\")\n",
    "print(f\"Average chunk size: {np.mean([len(c.page_content) for c in test_chunks]):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Use Case Specific Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Code documentation\n",
    "code_doc_text = \"\"\"# API Documentation\n",
    "\n",
    "## Authentication\n",
    "All API requests must include an API key in the header:\n",
    "```\n",
    "Authorization: Bearer YOUR_API_KEY\n",
    "```\n",
    "\n",
    "## Endpoints\n",
    "\n",
    "### GET /users\n",
    "Retrieve a list of all users.\n",
    "\n",
    "Parameters:\n",
    "- limit (optional): Number of users to return (default: 50)\n",
    "- offset (optional): Number of users to skip (default: 0)\n",
    "\n",
    "Response:\n",
    "```json\n",
    "{\n",
    "  \"users\": [\n",
    "    {\n",
    "      \"id\": 1,\n",
    "      \"name\": \"John Doe\",\n",
    "      \"email\": \"john@example.com\"\n",
    "    }\n",
    "  ],\n",
    "  \"total\": 100\n",
    "}\n",
    "```\n",
    "\n",
    "### POST /users\n",
    "Create a new user.\n",
    "\n",
    "Request body:\n",
    "```json\n",
    "{\n",
    "  \"name\": \"Jane Doe\",\n",
    "  \"email\": \"jane@example.com\",\n",
    "  \"password\": \"secure_password\"\n",
    "}\n",
    "```\n",
    "\n",
    "Response:\n",
    "```json\n",
    "{\n",
    "  \"id\": 2,\n",
    "  \"name\": \"Jane Doe\",\n",
    "  \"email\": \"jane@example.com\",\n",
    "  \"created_at\": \"2024-01-15T10:30:00Z\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "code_doc = Document(page_content=code_doc_text, metadata={\"type\": \"api_docs\"})\n",
    "\n",
    "# Splitter cho code documentation - preserve code blocks\n",
    "code_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\n",
    "        \"\\n## \",    # Section headers\n",
    "        \"\\n### \",   # Subsection headers\n",
    "        \"\\n\\n\",     # Paragraphs\n",
    "        \"\\n\",       # Lines\n",
    "        \" \",        # Words\n",
    "        \"\",         # Characters\n",
    "    ]\n",
    ")\n",
    "\n",
    "code_chunks = code_splitter.split_documents([code_doc])\n",
    "\n",
    "print(f\"Code documentation splitting:\")\n",
    "print(f\"Chunks created: {len(code_chunks)}\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(code_chunks[0].page_content)\n",
    "print(f\"\\nSecond chunk:\")\n",
    "print(code_chunks[1].page_content if len(code_chunks) > 1 else \"No second chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Legal documents\n",
    "legal_text = \"\"\"ĐIỀU KHOẢN SỬ DỤNG DỊCH VỤ\n",
    "\n",
    "Điều 1. Định nghĩa\n",
    "1.1. \"Dịch vụ\" có nghĩa là tất cả các dịch vụ được cung cấp thông qua nền tảng của chúng tôi.\n",
    "1.2. \"Người dùng\" là cá nhân hoặc tổ chức sử dụng Dịch vụ.\n",
    "1.3. \"Tài khoản\" là tài khoản được tạo bởi Người dùng để truy cập Dịch vụ.\n",
    "\n",
    "Điều 2. Quyền và nghĩa vụ của Người dùng\n",
    "2.1. Người dùng có quyền:\n",
    "a) Sử dụng Dịch vụ theo đúng mục đích được thiết kế;\n",
    "b) Được bảo vệ thông tin cá nhân theo quy định pháp luật;\n",
    "c) Khiếu nại khi có vi phạm từ phía nhà cung cấp dịch vụ.\n",
    "\n",
    "2.2. Người dùng có nghĩa vụ:\n",
    "a) Cung cấp thông tin chính xác khi đăng ký;\n",
    "b) Bảo mật thông tin tài khoản;\n",
    "c) Không sử dụng Dịch vụ cho mục đích bất hợp pháp;\n",
    "d) Thanh toán đầy đủ các khoản phí theo quy định.\n",
    "\n",
    "Điều 3. Trách nhiệm của nhà cung cấp dịch vụ\n",
    "3.1. Cung cấp Dịch vụ ổn định và chất lượng.\n",
    "3.2. Bảo vệ thông tin cá nhân của Người dùng.\n",
    "3.3. Hỗ trợ Người dùng khi gặp sự cố kỹ thuật.\n",
    "\"\"\"\n",
    "\n",
    "legal_doc = Document(page_content=legal_text, metadata={\"type\": \"legal_terms\"})\n",
    "\n",
    "# Splitter cho văn bản pháp lý - preserve article structure\n",
    "legal_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\n",
    "        \"\\n\\nĐiều \",  # Articles\n",
    "        \"\\n\\n\",       # Paragraphs\n",
    "        \"\\n\",         # Lines\n",
    "        \"; \",         # Clauses\n",
    "        \", \",         # Sub-clauses\n",
    "        \" \",          # Words\n",
    "        \"\",           # Characters\n",
    "    ]\n",
    ")\n",
    "\n",
    "legal_chunks = legal_splitter.split_documents([legal_doc])\n",
    "\n",
    "print(f\"\\nLegal document splitting:\")\n",
    "print(f\"Chunks created: {len(legal_chunks)}\")\n",
    "\n",
    "for i, chunk in enumerate(legal_chunks[:3]):\n",
    "    print(f\"\\n--- Legal Chunk {i+1} ---\")\n",
    "    print(chunk.page_content)\n",
    "    print(f\"Length: {len(chunk.page_content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance và Memory Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "# Tạo large document để test performance\n",
    "large_text = sample_text * 10  # 10x larger\n",
    "large_doc = Document(page_content=large_text, metadata={\"type\": \"large_doc\"})\n",
    "\n",
    "print(f\"Large document stats:\")\n",
    "print(f\"- Characters: {len(large_text):,}\")\n",
    "print(f\"- Words: {len(large_text.split()):,}\")\n",
    "print(f\"- Memory size: {sys.getsizeof(large_text):,} bytes\")\n",
    "\n",
    "# Test different splitters performance\n",
    "splitters_to_test = {\n",
    "    \"CharacterTextSplitter\": CharacterTextSplitter(\n",
    "        separator=\"\\n\\n\", chunk_size=500, chunk_overlap=50\n",
    "    ),\n",
    "    \"RecursiveCharacterTextSplitter\": RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=50\n",
    "    )\n",
    "}\n",
    "\n",
    "performance_results = {}\n",
    "\n",
    "for name, splitter in splitters_to_test.items():\n",
    "    start_time = time.time()\n",
    "    chunks = splitter.split_documents([large_doc])\n",
    "    end_time = time.time()\n",
    "    \n",
    "    performance_results[name] = {\n",
    "        'time': end_time - start_time,\n",
    "        'chunks': len(chunks),\n",
    "        'avg_chunk_size': np.mean([len(c.page_content) for c in chunks])\n",
    "    }\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "for name, results in performance_results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  - Time: {results['time']:.4f} seconds\")\n",
    "    print(f\"  - Chunks: {results['chunks']}\")\n",
    "    print(f\"  - Avg chunk size: {results['avg_chunk_size']:.0f} chars\")\n",
    "    print(f\"  - Chunks per second: {results['chunks']/results['time']:.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Common Pitfalls và Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pitfall 1: Chunk size quá nhỏ\n",
    "print(\"=== PITFALL 1: Chunk size quá nhỏ ===\")\n",
    "tiny_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,  # Quá nhỏ!\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "tiny_chunks = tiny_splitter.split_documents([sample_doc])\n",
    "print(f\"Chunks with tiny size: {len(tiny_chunks)}\")\n",
    "print(f\"First tiny chunk: '{tiny_chunks[0].page_content}'\")\n",
    "print(f\"Problem: Chunks không có đủ context để meaningful retrieval\")\n",
    "\n",
    "# Common pitfall 2: Overlap quá lớn\n",
    "print(f\"\\n=== PITFALL 2: Overlap quá lớn ===\")\n",
    "huge_overlap_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=150  # 75% overlap!\n",
    ")\n",
    "\n",
    "huge_overlap_chunks = huge_overlap_splitter.split_documents([sample_doc])\n",
    "print(f\"Chunks with huge overlap: {len(huge_overlap_chunks)}\")\n",
    "print(f\"Problem: Quá nhiều duplicate content, tăng cost và noise\")\n",
    "\n",
    "# Common pitfall 3: Ignoring document structure\n",
    "print(f\"\\n=== PITFALL 3: Ignoring document structure ===\")\n",
    "bad_separator_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",  # Split by spaces - very bad!\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "bad_chunks = bad_separator_splitter.split_documents([sample_doc])\n",
    "print(f\"Chunks with bad separator: {len(bad_chunks)}\")\n",
    "print(f\"First bad chunk: '{bad_chunks[0].page_content}'\")\n",
    "print(f\"Problem: Chunks không respect ngữ nghĩa và structure\")\n",
    "\n",
    "# Solution examples\n",
    "print(f\"\\n=== SOLUTIONS ===\")\n",
    "print(f\"1. Optimal chunk size: 200-800 characters cho general text\")\n",
    "print(f\"2. Overlap ratio: 10-20% of chunk size\")\n",
    "print(f\"3. Use RecursiveCharacterTextSplitter với appropriate separators\")\n",
    "print(f\"4. Test với actual data để tune parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng kết\n",
    "\n",
    "### **Tại sao cần Text Splitting?**\n",
    "1. **Context window limits** của LLMs\n",
    "2. **Retrieval efficiency** - chunks nhỏ hơn = precision cao hơn\n",
    "3. **Cost optimization** - chỉ process relevant parts\n",
    "4. **Semantic coherence** - mỗi chunk một ý tưởng\n",
    "\n",
    "### **CharacterTextSplitter vs RecursiveCharacterTextSplitter**\n",
    "\n",
    "| Aspect | CharacterTextSplitter | RecursiveCharacterTextSplitter |\n",
    "|--------|----------------------|--------------------------------|\n",
    "| **Flexibility** | Single separator | Multiple separators, hierarchical |\n",
    "| **Semantic Preservation** | Có thể cắt ngang câu | Cố gắng preserve semantic boundaries |\n",
    "| **Performance** | Nhanh hơn | Chậm hơn một chút |\n",
    "| **Use Cases** | Well-structured documents | General purpose, mixed content |\n",
    "| **Recommendation** | Khi biết rõ document structure | Default choice cho most cases |\n",
    "\n",
    "### **Key Parameters**\n",
    "- **`chunk_size`**: 200-800 chars cho general text, 100-300 tokens cho LLM processing\n",
    "- **`chunk_overlap`**: 10-20% của chunk_size để maintain context\n",
    "- **`separators`**: Hierarchical từ paragraph → sentence → word → character\n",
    "\n",
    "### **Best Practices**\n",
    "1. **Test với actual data** để tìm optimal settings\n",
    "2. **Analyze document structure** trước khi chọn splitter\n",
    "3. **Monitor chunk quality** - check endings, semantic coherence\n",
    "4. **Consider domain-specific requirements** (code, legal, scientific texts)\n",
    "5. **Balance chunk size với retrieval precision**\n",
    "\n",
    "### **Next Steps**\n",
    "- **Embeddings**: Convert chunks thành vectors\n",
    "- **Vector Stores**: Store và search embeddings\n",
    "- **Retrieval**: Find relevant chunks for queries\n",
    "- **Generation**: Use retrieved context với LLMs\n",
    "\n",
    "Text Splitting là foundation quan trọng cho successful RAG implementations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}