{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIPO Focused Learning: Preference Learning Models (DPO/IPO)\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "Notebook nÃ y táº­p trung vÃ o viá»‡c hiá»ƒu sÃ¢u vá» **Preference Learning** - má»™t trong nhá»¯ng Ä‘Ã³ng gÃ³p quan trá»ng nháº¥t cá»§a FIPO. ChÃºng ta sáº½:\n",
    "\n",
    "1. Hiá»ƒu cÆ¡ cháº¿ hoáº¡t Ä‘á»™ng cá»§a Direct Preference Optimization (DPO)\n",
    "2. So sÃ¡nh DPO vá»›i Identity Preference Optimization (IPO)\n",
    "3. Implement cÃ¡c loss functions vÃ  training loops\n",
    "4. Visualize quÃ¡ trÃ¬nh há»c preference\n",
    "\n",
    "## ðŸ“š Paper References\n",
    "\n",
    "- **Section 2.4**: Fine-tuning Strategies (Equations 9-11)\n",
    "- **Figure 3**: Strategic Fine-tuning approaches\n",
    "- **Table 3**: Comparison of different fine-tuning strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Foundation\n",
    "\n",
    "### 1.1 Why Preference Learning?\n",
    "\n",
    "Thay vÃ¬ chá»‰ há»c tá»« \"good examples\" (SFT), preference learning há»c tá»« cáº£ \"good\" vÃ  \"bad\" examples Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n vá» cháº¥t lÆ°á»£ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, List\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreferenceExample:\n",
    "    \"\"\"Represents a preference learning example\"\"\"\n",
    "    prompt: str\n",
    "    chosen: str  # xo+ in paper\n",
    "    rejected: str  # xo- in paper\n",
    "    \n",
    "# Example preference pairs from FIPO\n",
    "examples = [\n",
    "    PreferenceExample(\n",
    "        prompt=\"Calculate the average of [12, 34, 56, 75]\",\n",
    "        chosen=\"To find the average: 1) Add all numbers: 12+34+56+75=177, 2) Divide by count: 177/4=44.25\",\n",
    "        rejected=\"Find the average of the given numbers\"\n",
    "    ),\n",
    "    PreferenceExample(\n",
    "        prompt=\"What is the capital of France?\",\n",
    "        chosen=\"Identify the capital city of France. Provide a direct answer: The capital of France is Paris\",\n",
    "        rejected=\"Tell me about France's capital\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Preference Learning Examples:\")\n",
    "for i, ex in enumerate(examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Prompt: {ex.prompt}\")\n",
    "    print(f\"âœ… Chosen: {ex.chosen}\")\n",
    "    print(f\"âŒ Rejected: {ex.rejected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mathematical Foundation\n",
    "\n",
    "FIPO sá»­ dá»¥ng preference learning Ä‘á»ƒ há»c hÃ m reward áº©n tá»« human preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceLearningVisualizer:\n",
    "    \"\"\"Visualize preference learning concepts\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_preference_distribution():\n",
    "        \"\"\"Visualize how preference learning works\"\"\"\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Left: Quality distribution\n",
    "        quality_scores = np.linspace(0, 10, 100)\n",
    "        chosen_dist = np.exp(-(quality_scores - 8)**2 / 2) / np.sqrt(2 * np.pi)\n",
    "        rejected_dist = np.exp(-(quality_scores - 3)**2 / 2) / np.sqrt(2 * np.pi)\n",
    "        \n",
    "        ax1.fill_between(quality_scores, chosen_dist, alpha=0.5, label='Chosen (xo+)', color='green')\n",
    "        ax1.fill_between(quality_scores, rejected_dist, alpha=0.5, label='Rejected (xo-)', color='red')\n",
    "        ax1.set_xlabel('Quality Score')\n",
    "        ax1.set_ylabel('Probability Density')\n",
    "        ax1.set_title('Quality Distribution of Prompts')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Right: Preference probability\n",
    "        delta = np.linspace(-5, 5, 100)\n",
    "        beta = 0.1\n",
    "        preference_prob = 1 / (1 + np.exp(-beta * delta))\n",
    "        \n",
    "        ax2.plot(delta, preference_prob, linewidth=3, label=f'Î²={beta}')\n",
    "        ax2.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax2.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax2.set_xlabel('Î” = log p(chosen) - log p(rejected)')\n",
    "        ax2.set_ylabel('P(chosen > rejected)')\n",
    "        ax2.set_title('Preference Probability Function')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualizer = PreferenceLearningVisualizer()\n",
    "visualizer.visualize_preference_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Direct Preference Optimization (DPO)\n",
    "\n",
    "### 2.1 DPO Loss Function\n",
    "\n",
    "DPO directly optimizes for human preferences without explicit reward modeling:\n",
    "\n",
    "$$L_{DPO}(M_o) = -E_{(x_n, \\hat{y}_n, y_n, x_o^+, x_o^-) \\sim D}[\\log \\sigma(\\beta \\cdot \\Delta)]$$\n",
    "\n",
    "where $\\Delta = \\log \\frac{M_o(x_o^+|x_r, \\hat{y}_r, y_r)}{M_{ref}(x_o^+|x_r, \\hat{y}_r, y_r)} - \\log \\frac{M_o(x_o^-|x_r, \\hat{y}_r, y_r)}{M_{ref}(x_o^-|x_r, \\hat{y}_r, y_r)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOLoss(nn.Module):\n",
    "    \"\"\"Direct Preference Optimization Loss (Equation 9)\"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_logps: torch.Tensor,     # log p(chosen|prompt)\n",
    "        rejected_logps: torch.Tensor,   # log p(rejected|prompt)\n",
    "        reference_chosen_logps: torch.Tensor,   # log p_ref(chosen|prompt)\n",
    "        reference_rejected_logps: torch.Tensor  # log p_ref(rejected|prompt)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute DPO loss\"\"\"\n",
    "        \n",
    "        # Compute log ratios\n",
    "        chosen_logratios = chosen_logps - reference_chosen_logps\n",
    "        rejected_logratios = rejected_logps - reference_rejected_logps\n",
    "        \n",
    "        # Compute delta\n",
    "        delta = chosen_logratios - rejected_logratios\n",
    "        \n",
    "        # DPO loss\n",
    "        loss = -F.logsigmoid(self.beta * delta).mean()\n",
    "        \n",
    "        # Compute metrics for logging\n",
    "        with torch.no_grad():\n",
    "            accuracy = (delta > 0).float().mean()\n",
    "            \n",
    "        return loss, {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'delta_mean': delta.mean().item(),\n",
    "            'delta_std': delta.std().item()\n",
    "        }\n",
    "\n",
    "# Test DPO loss\n",
    "dpo_loss = DPOLoss(beta=0.01)\n",
    "\n",
    "# Simulate log probabilities\n",
    "batch_size = 4\n",
    "chosen_logps = torch.randn(batch_size) - 1  # Slightly negative\n",
    "rejected_logps = torch.randn(batch_size) - 2  # More negative\n",
    "ref_chosen_logps = torch.randn(batch_size) - 1.5\n",
    "ref_rejected_logps = torch.randn(batch_size) - 2.5\n",
    "\n",
    "loss, metrics = dpo_loss(chosen_logps, rejected_logps, ref_chosen_logps, ref_rejected_logps)\n",
    "\n",
    "print(\"DPO Loss Computation:\")\n",
    "print(f\"Loss: {metrics['loss']:.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
    "print(f\"Delta mean: {metrics['delta_mean']:.4f}\")\n",
    "print(f\"Delta std: {metrics['delta_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing DPO Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dpo_loss_landscape():\n",
    "    \"\"\"Visualize how DPO loss changes with different parameters\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Loss vs Delta for different beta values\n",
    "    ax = axes[0, 0]\n",
    "    deltas = np.linspace(-5, 5, 100)\n",
    "    betas = [0.01, 0.1, 0.5, 1.0]\n",
    "    \n",
    "    for beta in betas:\n",
    "        losses = -np.log(1 / (1 + np.exp(-beta * deltas)))\n",
    "        ax.plot(deltas, losses, label=f'Î²={beta}', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Î” (log ratio difference)')\n",
    "    ax.set_ylabel('DPO Loss')\n",
    "    ax.set_title('DPO Loss vs Delta for Different Î²')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Gradient magnitude\n",
    "    ax = axes[0, 1]\n",
    "    for beta in betas:\n",
    "        gradients = beta * np.exp(-beta * deltas) / (1 + np.exp(-beta * deltas))\n",
    "        ax.plot(deltas, gradients, label=f'Î²={beta}', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Î” (log ratio difference)')\n",
    "    ax.set_ylabel('Gradient Magnitude')\n",
    "    ax.set_title('DPO Gradient vs Delta')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Training dynamics simulation\n",
    "    ax = axes[1, 0]\n",
    "    steps = 100\n",
    "    delta_trajectory = np.zeros(steps)\n",
    "    delta_trajectory[0] = -2  # Start with rejected > chosen\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    beta = 0.1\n",
    "    \n",
    "    for t in range(1, steps):\n",
    "        gradient = beta * np.exp(-beta * delta_trajectory[t-1]) / (1 + np.exp(-beta * delta_trajectory[t-1]))\n",
    "        delta_trajectory[t] = delta_trajectory[t-1] + learning_rate * gradient\n",
    "    \n",
    "    ax.plot(delta_trajectory, linewidth=2, color='purple')\n",
    "    ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Î” Value')\n",
    "    ax.set_title('DPO Training Dynamics')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Preference accuracy vs delta\n",
    "    ax = axes[1, 1]\n",
    "    accuracy = 1 / (1 + np.exp(-deltas))\n",
    "    ax.plot(deltas, accuracy, linewidth=3, color='green')\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.fill_between(deltas[deltas > 0], 0.5, accuracy[deltas > 0], alpha=0.3, color='green')\n",
    "    ax.set_xlabel('Î” (log ratio difference)')\n",
    "    ax.set_ylabel('P(chosen > rejected)')\n",
    "    ax.set_title('Preference Accuracy')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_dpo_loss_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identity Preference Optimization (IPO)\n",
    "\n",
    "### 3.1 IPO Loss Function\n",
    "\n",
    "IPO is a regularized version of DPO with squared loss:\n",
    "\n",
    "$$L_{IPO}(M_o) = -E_{(x_n, \\hat{y}_n, y_n, x_o^+, x_o^-) \\sim D}[(\\Delta - \\frac{1}{2\\beta})^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPOLoss(nn.Module):\n",
    "    \"\"\"Identity Preference Optimization Loss (Equation 10)\"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_logps: torch.Tensor,\n",
    "        rejected_logps: torch.Tensor,\n",
    "        reference_chosen_logps: torch.Tensor,\n",
    "        reference_rejected_logps: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Compute IPO loss\"\"\"\n",
    "        \n",
    "        # Compute log ratios\n",
    "        chosen_logratios = chosen_logps - reference_chosen_logps\n",
    "        rejected_logratios = rejected_logps - reference_rejected_logps\n",
    "        \n",
    "        # Compute delta\n",
    "        delta = chosen_logratios - rejected_logratios\n",
    "        \n",
    "        # IPO loss - squared loss for regularization\n",
    "        target = 1 / (2 * self.beta)\n",
    "        loss = ((delta - target) ** 2).mean()\n",
    "        \n",
    "        # Compute metrics\n",
    "        with torch.no_grad():\n",
    "            accuracy = (delta > 0).float().mean()\n",
    "            \n",
    "        return loss, {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'delta_mean': delta.mean().item(),\n",
    "            'delta_std': delta.std().item(),\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "# Compare DPO and IPO\n",
    "ipo_loss = IPOLoss(beta=0.01)\n",
    "\n",
    "# Use same data\n",
    "ipo_result, ipo_metrics = ipo_loss(chosen_logps, rejected_logps, ref_chosen_logps, ref_rejected_logps)\n",
    "dpo_result, dpo_metrics = dpo_loss(chosen_logps, rejected_logps, ref_chosen_logps, ref_rejected_logps)\n",
    "\n",
    "print(\"Loss Comparison:\")\n",
    "print(f\"DPO Loss: {dpo_metrics['loss']:.4f}\")\n",
    "print(f\"IPO Loss: {ipo_metrics['loss']:.4f}\")\n",
    "print(f\"IPO Target: {ipo_metrics['target']:.4f}\")\n",
    "print(f\"\\nBoth achieve {ipo_metrics['accuracy']:.2%} preference accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DPO vs IPO Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dpo_ipo():\n",
    "    \"\"\"Compare DPO and IPO loss functions\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    deltas = np.linspace(-3, 3, 100)\n",
    "    beta = 0.1\n",
    "    \n",
    "    # Row 1: Loss functions\n",
    "    ax = axes[0, 0]\n",
    "    dpo_losses = -np.log(1 / (1 + np.exp(-beta * deltas)))\n",
    "    ipo_losses = (deltas - 1/(2*beta))**2\n",
    "    \n",
    "    ax.plot(deltas, dpo_losses, label='DPO', linewidth=2, color='blue')\n",
    "    ax.plot(deltas, ipo_losses, label='IPO', linewidth=2, color='orange')\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Î”')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Loss Functions')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradients\n",
    "    ax = axes[0, 1]\n",
    "    dpo_grads = beta * np.exp(-beta * deltas) / (1 + np.exp(-beta * deltas))\n",
    "    ipo_grads = 2 * (deltas - 1/(2*beta))\n",
    "    \n",
    "    ax.plot(deltas, dpo_grads, label='DPO', linewidth=2, color='blue')\n",
    "    ax.plot(deltas, ipo_grads, label='IPO', linewidth=2, color='orange')\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Î”')\n",
    "    ax.set_ylabel('Gradient')\n",
    "    ax.set_title('Gradient Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient magnitude\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(deltas, np.abs(dpo_grads), label='DPO', linewidth=2, color='blue')\n",
    "    ax.plot(deltas, np.abs(ipo_grads), label='IPO', linewidth=2, color='orange')\n",
    "    ax.set_xlabel('Î”')\n",
    "    ax.set_ylabel('|Gradient|')\n",
    "    ax.set_title('Gradient Magnitude')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 2: Training dynamics\n",
    "    ax = axes[1, 0]\n",
    "    steps = 200\n",
    "    lr = 0.05\n",
    "    \n",
    "    # DPO trajectory\n",
    "    delta_dpo = np.zeros(steps)\n",
    "    delta_dpo[0] = -2\n",
    "    for t in range(1, steps):\n",
    "        grad = beta * np.exp(-beta * delta_dpo[t-1]) / (1 + np.exp(-beta * delta_dpo[t-1]))\n",
    "        delta_dpo[t] = delta_dpo[t-1] + lr * grad\n",
    "    \n",
    "    # IPO trajectory\n",
    "    delta_ipo = np.zeros(steps)\n",
    "    delta_ipo[0] = -2\n",
    "    for t in range(1, steps):\n",
    "        grad = -2 * (delta_ipo[t-1] - 1/(2*beta))\n",
    "        delta_ipo[t] = delta_ipo[t-1] + lr * grad\n",
    "    \n",
    "    ax.plot(delta_dpo, label='DPO', linewidth=2, color='blue')\n",
    "    ax.plot(delta_ipo, label='IPO', linewidth=2, color='orange')\n",
    "    ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(1/(2*beta), color='orange', linestyle='--', alpha=0.5, label='IPO target')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Î”')\n",
    "    ax.set_title('Training Trajectories')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss over time\n",
    "    ax = axes[1, 1]\n",
    "    dpo_loss_traj = -np.log(1 / (1 + np.exp(-beta * delta_dpo)))\n",
    "    ipo_loss_traj = (delta_ipo - 1/(2*beta))**2\n",
    "    \n",
    "    ax.semilogy(dpo_loss_traj, label='DPO', linewidth=2, color='blue')\n",
    "    ax.semilogy(ipo_loss_traj, label='IPO', linewidth=2, color='orange')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Loss (log scale)')\n",
    "    ax.set_title('Loss Convergence')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Beta sensitivity\n",
    "    ax = axes[1, 2]\n",
    "    betas = np.logspace(-3, 0, 50)\n",
    "    delta_test = 1.0\n",
    "    \n",
    "    dpo_losses_beta = [-np.log(1 / (1 + np.exp(-b * delta_test))) for b in betas]\n",
    "    ipo_losses_beta = [(delta_test - 1/(2*b))**2 for b in betas]\n",
    "    \n",
    "    ax.loglog(betas, dpo_losses_beta, label='DPO', linewidth=2, color='blue')\n",
    "    ax.loglog(betas, ipo_losses_beta, label='IPO', linewidth=2, color='orange')\n",
    "    ax.set_xlabel('Î²')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Î² Sensitivity (Î”=1.0)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_dpo_ipo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Implementation\n",
    "\n",
    "### 4.1 Mini Preference Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePromptOptimizer(nn.Module):\n",
    "    \"\"\"Simplified prompt optimizer for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        embeds = self.embedding(input_ids)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        logits = self.output(lstm_out)\n",
    "        return logits\n",
    "    \n",
    "    def get_logprobs(self, input_ids: torch.Tensor, target_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get log probabilities for target sequences\"\"\"\n",
    "        logits = self.forward(input_ids)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Gather log probs for target tokens\n",
    "        target_log_probs = log_probs.gather(2, target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Sum over sequence length\n",
    "        return target_log_probs.sum(dim=1)\n",
    "\n",
    "# Create model and reference model\n",
    "model = SimplePromptOptimizer()\n",
    "ref_model = SimplePromptOptimizer()\n",
    "ref_model.load_state_dict(model.state_dict())  # Same initialization\n",
    "ref_model.eval()  # Freeze reference model\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop with DPO/IPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceTrainer:\n",
    "    \"\"\"Trainer for preference learning\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, ref_model: nn.Module, loss_type: str = \"DPO\", beta: float = 0.01):\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        if loss_type == \"DPO\":\n",
    "            self.loss_fn = DPOLoss(beta)\n",
    "        else:\n",
    "            self.loss_fn = IPOLoss(beta)\n",
    "            \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        self.history = {'loss': [], 'accuracy': [], 'delta_mean': []}\n",
    "    \n",
    "    def create_dummy_batch(self, batch_size: int = 8):\n",
    "        \"\"\"Create dummy preference data\"\"\"\n",
    "        seq_len = 10\n",
    "        vocab_size = 1000\n",
    "        \n",
    "        # Random sequences\n",
    "        prompts = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        chosen = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        rejected = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        \n",
    "        return prompts, chosen, rejected\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Get batch\n",
    "        prompts, chosen, rejected = self.create_dummy_batch()\n",
    "        \n",
    "        # Get log probabilities\n",
    "        with torch.no_grad():\n",
    "            ref_chosen_logps = self.ref_model.get_logprobs(prompts, chosen)\n",
    "            ref_rejected_logps = self.ref_model.get_logprobs(prompts, rejected)\n",
    "        \n",
    "        chosen_logps = self.model.get_logprobs(prompts, chosen)\n",
    "        rejected_logps = self.model.get_logprobs(prompts, rejected)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, metrics = self.loss_fn(chosen_logps, rejected_logps, ref_chosen_logps, ref_rejected_logps)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Record metrics\n",
    "        for key in ['loss', 'accuracy', 'delta_mean']:\n",
    "            self.history[key].append(metrics[key])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def train(self, num_steps: int = 100):\n",
    "        \"\"\"Training loop\"\"\"\n",
    "        pbar = range(num_steps)\n",
    "        \n",
    "        for step in pbar:\n",
    "            metrics = self.train_step()\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}: Loss={metrics['loss']:.4f}, Acc={metrics['accuracy']:.2%}, Î”={metrics['delta_mean']:.3f}\")\n",
    "    \n",
    "    def plot_training_curves(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        axes[0].plot(self.history['loss'], linewidth=2)\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title(f'{self.loss_type} Loss')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].plot(self.history['accuracy'], linewidth=2, color='green')\n",
    "        axes[1].set_xlabel('Step')\n",
    "        axes[1].set_ylabel('Preference Accuracy')\n",
    "        axes[1].set_title('Training Accuracy')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[2].plot(self.history['delta_mean'], linewidth=2, color='purple')\n",
    "        axes[2].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[2].set_xlabel('Step')\n",
    "        axes[2].set_ylabel('Î” (mean)')\n",
    "        axes[2].set_title('Average Delta')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Train with DPO\n",
    "print(\"Training with DPO...\")\n",
    "dpo_trainer = PreferenceTrainer(model, ref_model, loss_type=\"DPO\", beta=0.1)\n",
    "dpo_trainer.train(num_steps=50)\n",
    "dpo_trainer.plot_training_curves()\n",
    "\n",
    "# Reset model and train with IPO\n",
    "model2 = SimplePromptOptimizer()\n",
    "print(\"\\nTraining with IPO...\")\n",
    "ipo_trainer = PreferenceTrainer(model2, ref_model, loss_type=\"IPO\", beta=0.1)\n",
    "ipo_trainer.train(num_steps=50)\n",
    "ipo_trainer.plot_training_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Insights from FIPO's Preference Learning\n",
    "\n",
    "### 5.1 Why IPO Outperforms DPO in FIPO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fipo_results():\n",
    "    \"\"\"Analyze FIPO's experimental results from Table 3\"\"\"\n",
    "    \n",
    "    # Results from paper\n",
    "    results = pd.DataFrame({\n",
    "        'Method': ['Naive', 'SFT-70B', 'DPO-70B', 'IPO-70B', 'IPL-DPO-70B', 'IPL-IPO-70B'],\n",
    "        'GSM8K': [24.77, 21.43, 27.74, 25.00, 25.13, 26.67],\n",
    "        'BBH': [36.21, 32.92, 35.56, 39.21, 35.25, 39.60],\n",
    "        'PiQA': [73.35, 74.39, 74.17, 76.84, 74.95, 77.11],\n",
    "        'CosmosQA': [51.17, 49.97, 54.93, 56.01, 50.46, 56.71],\n",
    "        'MMLU': [51.22, 51.55, 52.73, 54.29, 52.12, 56.02],\n",
    "        'Weighted_Avg': [47.79, 46.96, 49.07, 50.94, 48.10, 52.13]\n",
    "    })\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar plot comparison\n",
    "    methods = results['Method']\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x, results['Weighted_Avg'], width, label='Weighted Average', color='skyblue')\n",
    "    ax1.axhline(results.loc[0, 'Weighted_Avg'], color='red', linestyle='--', alpha=0.5, label='Naive baseline')\n",
    "    \n",
    "    ax1.set_xlabel('Method')\n",
    "    ax1.set_ylabel('Performance (%)')\n",
    "    ax1.set_title('FIPO Fine-tuning Strategies Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(methods, rotation=45)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Heatmap of improvements\n",
    "    improvements = results.iloc[:, 1:-1].values - results.loc[0, results.columns[1:-1]].values\n",
    "    \n",
    "    im = ax2.imshow(improvements.T, cmap='RdYlGn', aspect='auto')\n",
    "    ax2.set_xticks(np.arange(len(methods)))\n",
    "    ax2.set_yticks(np.arange(len(results.columns[1:-1])))\n",
    "    ax2.set_xticklabels(methods, rotation=45)\n",
    "    ax2.set_yticklabels(results.columns[1:-1])\n",
    "    ax2.set_title('Improvement over Naive Baseline')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(len(results.columns[1:-1])):\n",
    "            text = ax2.text(i, j, f'{improvements[i, j]:.1f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Key insights\n",
    "    print(\"\\nðŸ” Key Insights from FIPO Results:\")\n",
    "    print(\"1. IPO consistently outperforms DPO across benchmarks\")\n",
    "    print(\"2. IPL-IPO achieves best overall performance (52.13%)\")\n",
    "    print(\"3. SFT alone performs worse than naive baseline\")\n",
    "    print(\"4. Preference learning is crucial for prompt optimization\")\n",
    "    print(\"\\nðŸ“Š Performance Rankings:\")\n",
    "    print(results[['Method', 'Weighted_Avg']].sort_values('Weighted_Avg', ascending=False))\n",
    "\n",
    "analyze_fipo_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Understanding Beta Parameter Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_parameter_study():\n",
    "    \"\"\"Study the impact of beta parameter on preference learning\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    betas = [0.001, 0.01, 0.1, 1.0]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(betas)))\n",
    "    \n",
    "    # Effect on optimal delta\n",
    "    ax = axes[0, 0]\n",
    "    optimal_deltas_ipo = [1/(2*beta) for beta in betas]\n",
    "    ax.bar(range(len(betas)), optimal_deltas_ipo, color=colors)\n",
    "    ax.set_xticks(range(len(betas)))\n",
    "    ax.set_xticklabels([f'Î²={b}' for b in betas])\n",
    "    ax.set_ylabel('Optimal Î” (IPO)')\n",
    "    ax.set_title('IPO Target Delta vs Beta')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Learning dynamics\n",
    "    ax = axes[0, 1]\n",
    "    for i, beta in enumerate(betas):\n",
    "        steps = 100\n",
    "        delta = np.zeros(steps)\n",
    "        delta[0] = -1\n",
    "        lr = 0.1\n",
    "        \n",
    "        for t in range(1, steps):\n",
    "            # IPO gradient\n",
    "            grad = -2 * (delta[t-1] - 1/(2*beta))\n",
    "            delta[t] = delta[t-1] + lr * grad\n",
    "        \n",
    "        ax.plot(delta, label=f'Î²={beta}', color=colors[i], linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Î”')\n",
    "    ax.set_title('IPO Convergence for Different Î²')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient scale\n",
    "    ax = axes[1, 0]\n",
    "    delta_range = np.linspace(-2, 2, 100)\n",
    "    \n",
    "    for i, beta in enumerate(betas):\n",
    "        # DPO gradient magnitude\n",
    "        dpo_grads = beta * np.exp(-beta * delta_range) / (1 + np.exp(-beta * delta_range))\n",
    "        ax.plot(delta_range, dpo_grads, label=f'Î²={beta}', color=colors[i], linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Î”')\n",
    "    ax.set_ylabel('DPO Gradient')\n",
    "    ax.set_title('DPO Gradient Scale vs Beta')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Practical recommendations\n",
    "    ax = axes[1, 1]\n",
    "    ax.text(0.5, 0.8, \"FIPO Beta Selection Guidelines\", \n",
    "            horizontalalignment='center', fontsize=16, weight='bold', transform=ax.transAxes)\n",
    "    \n",
    "    guidelines = [\n",
    "        \"Î² = 0.01 (FIPO default):\",\n",
    "        \"  â€¢ Stable training\",\n",
    "        \"  â€¢ Good for large models\",\n",
    "        \"  â€¢ IPO target Î” = 50\",\n",
    "        \"\",\n",
    "        \"Smaller Î² (0.001):\",\n",
    "        \"  â€¢ Stronger preferences\",\n",
    "        \"  â€¢ Risk of overfitting\",\n",
    "        \"\",\n",
    "        \"Larger Î² (0.1-1.0):\",\n",
    "        \"  â€¢ Weaker preferences\",\n",
    "        \"  â€¢ More exploration\"\n",
    "    ]\n",
    "    \n",
    "    y_pos = 0.6\n",
    "    for line in guidelines:\n",
    "        ax.text(0.1, y_pos, line, transform=ax.transAxes, fontsize=12)\n",
    "        y_pos -= 0.06\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "beta_parameter_study()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Key Takeaways\n",
    "\n",
    "### Core Concepts Learned:\n",
    "\n",
    "1. **Preference Learning Fundamentals**:\n",
    "   - Learn from both positive (chosen) and negative (rejected) examples\n",
    "   - Model implicit reward function through pairwise comparisons\n",
    "   - More sample-efficient than standard supervised learning\n",
    "\n",
    "2. **DPO (Direct Preference Optimization)**:\n",
    "   - Uses logistic loss: $-\\log \\sigma(\\beta \\cdot \\Delta)$\n",
    "   - Directly optimizes for preferences without explicit reward model\n",
    "   - Gradient vanishes as model becomes confident\n",
    "\n",
    "3. **IPO (Identity Preference Optimization)**:\n",
    "   - Uses squared loss: $(\\Delta - \\frac{1}{2\\beta})^2$\n",
    "   - Regularized version provides more stable training\n",
    "   - Targets specific delta value rather than maximizing\n",
    "\n",
    "4. **FIPO's Success Factors**:\n",
    "   - IPO > DPO due to regularization benefits\n",
    "   - Beta = 0.01 provides good balance\n",
    "   - Preference learning crucial for prompt optimization\n",
    "\n",
    "### Practical Implementation Tips:\n",
    "\n",
    "- Always maintain frozen reference model\n",
    "- Monitor delta values during training\n",
    "- Use IPO for more stable convergence\n",
    "- Careful beta tuning based on model size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}