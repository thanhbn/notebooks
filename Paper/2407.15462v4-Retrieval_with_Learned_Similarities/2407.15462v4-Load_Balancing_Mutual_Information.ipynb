{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Load Balancing Loss & Mutual Information trong MoL\n",
    "\n",
    "## üéØ M·ª•c ti√™u H·ªçc t·∫≠p\n",
    "\n",
    "Hi·ªÉu s√¢u v·ªÅ:\n",
    "1. **Load Balancing Problem** trong Mixture-of-Experts systems\n",
    "2. **Mutual Information-based Loss** ƒë·ªÉ c·∫£i thi·ªán component utilization\n",
    "3. **Component Collapse** v√† c√°ch ngƒÉn ch·∫∑n\n",
    "4. **Conditional Computation** trong MoL\n",
    "5. **Mathematical Foundations** c·ªßa Information Theory trong ML\n",
    "\n",
    "## üìñ Tr√≠ch xu·∫•t t·ª´ Paper\n",
    "\n",
    "### Section 2.2 - Load Balancing Loss:\n",
    "\n",
    "> *\"We propose techniques to retrieve the approximate top-k results using MoL with tight error bounds... enhanced by our proposed mutual information-based load balancing loss\"*\n",
    "\n",
    "> *\"Our approximate top-k algorithms outperform baselines by up to 66√ó in latency while achieving >.99 recall rate compared to exact algorithms.\"*\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Component Collapse**: Khi m·ªôt v√†i components dominate, others become unused\n",
    "**Load Balancing**: Ensure equal utilization across components\n",
    "**Mutual Information**: Measure dependence between gating decisions and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Ph·∫ßn 1: Understanding Component Collapse\n",
    "\n",
    "### üìä V·∫•n ƒë·ªÅ:\n",
    "\n",
    "Trong Mixture-of-Experts systems, **component collapse** x·∫£y ra khi:\n",
    "- M·ªôt v√†i components ƒë∆∞·ª£c s·ª≠ d·ª•ng heavily\n",
    "- Majority components √≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng ho·∫∑c unused\n",
    "- Model capacity b·ªã waste, performance gi·∫£m\n",
    "\n",
    "### üéØ Gi·∫£i ph√°p:\n",
    "**Load Balancing Loss** khuy·∫øn kh√≠ch:\n",
    "- Uniform distribution c·ªßa gating weights\n",
    "- Equal utilization across components\n",
    "- Better model capacity utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_component_collapse():\n",
    "    \"\"\"\n",
    "    Demonstrate component collapse phenomenon\n",
    "    \"\"\"\n",
    "    print(\"üö® Component Collapse Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    num_components = 8\n",
    "    num_samples = 1000\n",
    "    \n",
    "    # Simulate different scenarios\n",
    "    scenarios = {\n",
    "        'uniform': np.random.dirichlet(np.ones(num_components), num_samples),\n",
    "        'slightly_biased': np.random.dirichlet(np.array([2, 2, 2, 1, 1, 1, 1, 1]), num_samples),\n",
    "        'heavily_biased': np.random.dirichlet(np.array([10, 5, 1, 1, 1, 1, 1, 1]), num_samples),\n",
    "        'collapsed': np.random.dirichlet(np.array([100, 1, 1, 1, 1, 1, 1, 1]), num_samples)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for scenario_name, weights in scenarios.items():\n",
    "        # Compute utilization statistics\n",
    "        mean_weights = np.mean(weights, axis=0)\n",
    "        std_weights = np.std(weights, axis=0)\n",
    "        \n",
    "        # Effective number of components (Perplexity)\n",
    "        entropy = -np.sum(mean_weights * np.log(mean_weights + 1e-8))\n",
    "        perplexity = np.exp(entropy)\n",
    "        \n",
    "        # Gini coefficient (inequality measure)\n",
    "        sorted_weights = np.sort(mean_weights)\n",
    "        n = len(sorted_weights)\n",
    "        index = np.arange(1, n + 1)\n",
    "        gini = (2 * np.sum(index * sorted_weights)) / (n * np.sum(sorted_weights)) - (n + 1) / n\n",
    "        \n",
    "        results[scenario_name] = {\n",
    "            'mean_weights': mean_weights,\n",
    "            'std_weights': std_weights,\n",
    "            'perplexity': perplexity,\n",
    "            'gini': gini,\n",
    "            'entropy': entropy\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä {scenario_name.upper()}:\")\n",
    "        print(f\"   Effective components: {perplexity:.2f} / {num_components}\")\n",
    "        print(f\"   Gini coefficient: {gini:.3f} (0=equal, 1=collapsed)\")\n",
    "        print(f\"   Entropy: {entropy:.3f} (max={np.log(num_components):.3f})\")\n",
    "        print(f\"   Weight distribution: {mean_weights}\")\n",
    "    \n",
    "    return results, scenarios\n",
    "\n",
    "collapse_results, collapse_scenarios = demonstrate_component_collapse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Ph·∫ßn 2: Information Theory Foundations\n",
    "\n",
    "### üìñ L√Ω thuy·∫øt:\n",
    "\n",
    "**Entropy** (Shannon Entropy):\n",
    "$$H(X) = -\\sum_{i} p(x_i) \\log p(x_i)$$\n",
    "\n",
    "**Mutual Information**:\n",
    "$$I(X; Y) = H(X) - H(X|Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$$\n",
    "\n",
    "**KL Divergence**:\n",
    "$$D_{KL}(P||Q) = \\sum_{i} p_i \\log \\frac{p_i}{q_i}$$\n",
    "\n",
    "### üéØ Application to MoL:\n",
    "- **Z**: Latent representations (queries/items)\n",
    "- **G**: Gating decisions (component assignments)\n",
    "- **Goal**: Minimize I(Z; G) ‚Üí components independent of input patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationTheoryUtils:\n",
    "    \"\"\"\n",
    "    Utility functions for Information Theory calculations\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy(probabilities: np.ndarray, base: float = 2) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy\n",
    "        \"\"\"\n",
    "        p = probabilities + 1e-12  # Avoid log(0)\n",
    "        return -np.sum(p * np.log(p) / np.log(base))\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutual_information_discrete(X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate mutual information for discrete variables\n",
    "        \"\"\"\n",
    "        return mutual_info_score(X, Y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence(P: np.ndarray, Q: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate KL divergence D_KL(P||Q)\n",
    "        \"\"\"\n",
    "        P = P + 1e-12\n",
    "        Q = Q + 1e-12\n",
    "        return np.sum(P * np.log(P / Q))\n",
    "    \n",
    "    @staticmethod\n",
    "    def perplexity(probabilities: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate perplexity (effective number of components)\n",
    "        \"\"\"\n",
    "        entropy = InformationTheoryUtils.entropy(probabilities, base=np.e)\n",
    "        return np.exp(entropy)\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_mutual_information_continuous(X: torch.Tensor, Y: torch.Tensor, \n",
    "                                             bins: int = 20) -> float:\n",
    "        \"\"\"\n",
    "        Estimate mutual information for continuous variables using binning\n",
    "        \"\"\"\n",
    "        # Convert to numpy\n",
    "        X_np = X.detach().cpu().numpy().flatten()\n",
    "        Y_np = Y.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        # Discretize using histograms\n",
    "        X_discrete = np.digitize(X_np, np.histogram(X_np, bins=bins)[1][:-1])\n",
    "        Y_discrete = np.digitize(Y_np, np.histogram(Y_np, bins=bins)[1][:-1])\n",
    "        \n",
    "        return mutual_info_score(X_discrete, Y_discrete)\n",
    "\n",
    "# Demonstrate information theory concepts\n",
    "print(\"üìä Information Theory Demonstrations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "utils = InformationTheoryUtils()\n",
    "\n",
    "# Entropy examples\n",
    "distributions = {\n",
    "    'uniform': np.ones(8) / 8,\n",
    "    'peaked': np.array([0.7, 0.1, 0.05, 0.05, 0.03, 0.03, 0.02, 0.02]),\n",
    "    'bimodal': np.array([0.4, 0.4, 0.05, 0.05, 0.05, 0.05, 0, 0]),\n",
    "    'delta': np.array([1.0, 0, 0, 0, 0, 0, 0, 0])\n",
    "}\n",
    "\n",
    "print(\"\\nüî¢ Entropy Analysis:\")\n",
    "for name, dist in distributions.items():\n",
    "    entropy = utils.entropy(dist)\n",
    "    perplexity = utils.perplexity(dist)\n",
    "    print(f\"   {name:8s}: H = {entropy:.3f}, Perplexity = {perplexity:.2f}\")\n",
    "\n",
    "# Mutual Information demonstration\n",
    "print(\"\\nüîó Mutual Information Examples:\")\n",
    "\n",
    "# Independent variables\n",
    "X_indep = np.random.randint(0, 4, 1000)\n",
    "Y_indep = np.random.randint(0, 4, 1000)\n",
    "mi_indep = utils.mutual_information_discrete(X_indep, Y_indep)\n",
    "print(f\"   Independent variables: I(X;Y) = {mi_indep:.3f}\")\n",
    "\n",
    "# Dependent variables\n",
    "X_dep = np.random.randint(0, 4, 1000)\n",
    "Y_dep = (X_dep + np.random.randint(0, 2, 1000)) % 4  # Partial dependence\n",
    "mi_dep = utils.mutual_information_discrete(X_dep, Y_dep)\n",
    "print(f\"   Dependent variables: I(X;Y) = {mi_dep:.3f}\")\n",
    "\n",
    "# Perfectly correlated\n",
    "X_corr = np.random.randint(0, 4, 1000)\n",
    "Y_corr = X_corr.copy()  # Perfect correlation\n",
    "mi_corr = utils.mutual_information_discrete(X_corr, Y_corr)\n",
    "print(f\"   Perfectly correlated: I(X;Y) = {mi_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Ph·∫ßn 3: Load Balancing Loss Implementation\n",
    "\n",
    "### üéØ M·ª•c ti√™u:\n",
    "Thi·∫øt k·∫ø loss function ƒë·ªÉ:\n",
    "1. **Encourage uniform component usage**\n",
    "2. **Minimize mutual information** between inputs v√† gating decisions\n",
    "3. **Prevent component collapse**\n",
    "4. **Maintain model expressiveness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLoadBalancingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Load Balancing Loss with multiple components:\n",
    "    1. Uniform Distribution Loss (KL divergence)\n",
    "    2. Entropy Maximization Loss\n",
    "    3. Mutual Information Minimization Loss\n",
    "    4. Variance Minimization Loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_components: int,\n",
    "                 lambda_uniform: float = 0.01,\n",
    "                 lambda_entropy: float = 0.01,\n",
    "                 lambda_mi: float = 0.005,\n",
    "                 lambda_variance: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.num_components = num_components\n",
    "        self.lambda_uniform = lambda_uniform\n",
    "        self.lambda_entropy = lambda_entropy\n",
    "        self.lambda_mi = lambda_mi\n",
    "        self.lambda_variance = lambda_variance\n",
    "        \n",
    "        # Target uniform distribution\n",
    "        self.register_buffer('uniform_target', \n",
    "                           torch.ones(num_components) / num_components)\n",
    "    \n",
    "    def forward(self, \n",
    "               gating_weights: torch.Tensor,\n",
    "               input_features: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute comprehensive load balancing loss\n",
    "        \n",
    "        Args:\n",
    "            gating_weights: [batch_size, num_components] - gating decisions\n",
    "            input_features: [batch_size, feature_dim] - input representations\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of loss components\n",
    "        \"\"\"\n",
    "        batch_size = gating_weights.size(0)\n",
    "        \n",
    "        # Reshape if needed\n",
    "        if gating_weights.dim() > 2:\n",
    "            gating_weights = gating_weights.view(batch_size, -1)\n",
    "        \n",
    "        # Component usage statistics\n",
    "        component_usage = gating_weights.mean(dim=0)  # [num_components]\n",
    "        \n",
    "        losses = {}\n",
    "        \n",
    "        # 1. Uniform Distribution Loss (KL Divergence)\n",
    "        uniform_loss = F.kl_div(\n",
    "            (component_usage + 1e-8).log(),\n",
    "            self.uniform_target,\n",
    "            reduction='sum'\n",
    "        )\n",
    "        losses['uniform'] = self.lambda_uniform * uniform_loss\n",
    "        \n",
    "        # 2. Entropy Maximization Loss\n",
    "        entropy = -torch.sum(component_usage * torch.log(component_usage + 1e-8))\n",
    "        max_entropy = math.log(self.num_components)  # Maximum possible entropy\n",
    "        entropy_loss = max_entropy - entropy  # Minimize negative entropy\n",
    "        losses['entropy'] = self.lambda_entropy * entropy_loss\n",
    "        \n",
    "        # 3. Variance Minimization Loss (encourage equal usage)\n",
    "        variance_loss = torch.var(component_usage)\n",
    "        losses['variance'] = self.lambda_variance * variance_loss\n",
    "        \n",
    "        # 4. Mutual Information Minimization (if input features available)\n",
    "        if input_features is not None:\n",
    "            mi_loss = self._approximate_mutual_information_loss(\n",
    "                input_features, gating_weights\n",
    "            )\n",
    "            losses['mutual_info'] = self.lambda_mi * mi_loss\n",
    "        else:\n",
    "            losses['mutual_info'] = torch.tensor(0.0, device=gating_weights.device)\n",
    "        \n",
    "        # Total loss\n",
    "        losses['total'] = sum(losses.values())\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def _approximate_mutual_information_loss(self, \n",
    "                                           inputs: torch.Tensor, \n",
    "                                           gating_weights: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Approximate mutual information loss using neural estimation\n",
    "        \n",
    "        This is a simplified version - in practice, you might use:\n",
    "        - MINE (Mutual Information Neural Estimation)\n",
    "        - InfoNCE\n",
    "        - Histogram-based estimation\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        # Simple approximation: encourage independence\n",
    "        # by minimizing correlation between input patterns and gating decisions\n",
    "        \n",
    "        # Compute pairwise similarities in input space\n",
    "        input_norm = F.normalize(inputs, dim=1)\n",
    "        input_sim = torch.mm(input_norm, input_norm.t())  # [batch, batch]\n",
    "        \n",
    "        # Compute pairwise similarities in gating space\n",
    "        gating_norm = F.normalize(gating_weights, dim=1)\n",
    "        gating_sim = torch.mm(gating_norm, gating_norm.t())  # [batch, batch]\n",
    "        \n",
    "        # Minimize correlation between input and gating similarities\n",
    "        # This encourages gating decisions to be independent of input patterns\n",
    "        correlation = torch.mean(input_sim * gating_sim)\n",
    "        \n",
    "        return correlation\n",
    "\n",
    "# Demonstration\n",
    "print(\"‚öñÔ∏è Advanced Load Balancing Loss Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "num_components = 8\n",
    "batch_size = 100\n",
    "feature_dim = 64\n",
    "\n",
    "# Create load balancing loss\n",
    "lb_loss = AdvancedLoadBalancingLoss(num_components)\n",
    "\n",
    "# Test different gating weight scenarios\n",
    "scenarios = {\n",
    "    'uniform': torch.ones(batch_size, num_components) / num_components,\n",
    "    'biased': F.softmax(torch.tensor([3., 2., 1., 1., 0.5, 0.5, 0.2, 0.2]).unsqueeze(0).expand(batch_size, -1), dim=1),\n",
    "    'collapsed': F.softmax(torch.tensor([10., 1., 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]).unsqueeze(0).expand(batch_size, -1), dim=1)\n",
    "}\n",
    "\n",
    "input_features = torch.randn(batch_size, feature_dim)\n",
    "\n",
    "print(\"\\nüìä Load Balancing Loss Analysis:\")\n",
    "for scenario_name, weights in scenarios.items():\n",
    "    with torch.no_grad():\n",
    "        losses = lb_loss(weights, input_features)\n",
    "        \n",
    "        component_usage = weights.mean(dim=0)\n",
    "        entropy = -torch.sum(component_usage * torch.log(component_usage + 1e-8))\n",
    "        \n",
    "        print(f\"\\n   {scenario_name.upper()}:\")\n",
    "        print(f\"     Total Loss: {losses['total'].item():.4f}\")\n",
    "        print(f\"     Uniform Loss: {losses['uniform'].item():.4f}\")\n",
    "        print(f\"     Entropy Loss: {losses['entropy'].item():.4f}\")\n",
    "        print(f\"     Variance Loss: {losses['variance'].item():.4f}\")\n",
    "        print(f\"     MI Loss: {losses['mutual_info'].item():.4f}\")\n",
    "        print(f\"     Component Entropy: {entropy.item():.3f} / {math.log(num_components):.3f}\")\n",
    "        print(f\"     Usage: {component_usage.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Ph·∫ßn 4: Training v·ªõi Load Balancing\n",
    "\n",
    "### üéØ Experiment Design:\n",
    "So s√°nh training v·ªõi v√† kh√¥ng c√≥ load balancing loss ƒë·ªÉ th·∫•y impact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMoLWithLoadBalancing(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified MoL model with integrated load balancing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, num_components: int = 6, component_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.num_components = num_components\n",
    "        self.component_dim = component_dim\n",
    "        \n",
    "        # Component embeddings\n",
    "        self.query_embeddings = nn.ModuleList([\n",
    "            nn.Linear(input_dim, component_dim) for _ in range(num_components)\n",
    "        ])\n",
    "        self.item_embeddings = nn.ModuleList([\n",
    "            nn.Linear(input_dim, component_dim) for _ in range(num_components)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_components),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Load balancing\n",
    "        self.load_balancer = AdvancedLoadBalancingLoss(num_components)\n",
    "    \n",
    "    def forward(self, queries: torch.Tensor, items: torch.Tensor, \n",
    "               return_gating_info: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Forward pass with optional gating information\n",
    "        \"\"\"\n",
    "        batch_q, batch_i = queries.size(0), items.size(0)\n",
    "        \n",
    "        similarities = torch.zeros(batch_q, batch_i, device=queries.device)\n",
    "        all_gating_weights = []\n",
    "        all_input_features = []\n",
    "        \n",
    "        # Compute component embeddings\n",
    "        q_components = [F.normalize(emb(queries), dim=-1) for emb in self.query_embeddings]\n",
    "        i_components = [F.normalize(emb(items), dim=-1) for emb in self.item_embeddings]\n",
    "        \n",
    "        # Compute similarities for all pairs\n",
    "        for i in range(batch_q):\n",
    "            for j in range(batch_i):\n",
    "                # Combined features for gating\n",
    "                combined_features = torch.cat([queries[i], items[j]], dim=0)\n",
    "                gating_weights = self.gating_network(combined_features.unsqueeze(0)).squeeze(0)\n",
    "                \n",
    "                if return_gating_info:\n",
    "                    all_gating_weights.append(gating_weights)\n",
    "                    all_input_features.append(combined_features)\n",
    "                \n",
    "                # Compute weighted similarity\n",
    "                similarity = 0.0\n",
    "                for p in range(self.num_components):\n",
    "                    component_sim = torch.dot(q_components[p][i], i_components[p][j])\n",
    "                    similarity += gating_weights[p] * component_sim\n",
    "                \n",
    "                similarities[i, j] = similarity\n",
    "        \n",
    "        result = {'similarities': similarities}\n",
    "        \n",
    "        if return_gating_info and all_gating_weights:\n",
    "            result['gating_weights'] = torch.stack(all_gating_weights)\n",
    "            result['input_features'] = torch.stack(all_input_features)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def compute_load_balancing_loss(self, gating_weights: torch.Tensor, \n",
    "                                  input_features: torch.Tensor) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute load balancing loss\n",
    "        \"\"\"\n",
    "        return self.load_balancer(gating_weights, input_features)\n",
    "\n",
    "def train_with_load_balancing_comparison(num_epochs: int = 100):\n",
    "    \"\"\"\n",
    "    Compare training with and without load balancing\n",
    "    \"\"\"\n",
    "    print(\"\\nüß™ Load Balancing Training Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    num_queries, num_items = 30, 50\n",
    "    input_dim = 32\n",
    "    \n",
    "    queries = torch.randn(num_queries, input_dim)\n",
    "    items = torch.randn(num_items, input_dim)\n",
    "    \n",
    "    # Create relevance labels (simplified)\n",
    "    labels = torch.sigmoid(torch.mm(queries, items.t()) + 0.5 * torch.randn(num_queries, num_items))\n",
    "    \n",
    "    # Two models: with and without load balancing\n",
    "    model_with_lb = SimpleMoLWithLoadBalancing(input_dim, num_components=6)\n",
    "    model_without_lb = SimpleMoLWithLoadBalancing(input_dim, num_components=6)\n",
    "    \n",
    "    # Optimizers\n",
    "    opt_with_lb = torch.optim.Adam(model_with_lb.parameters(), lr=0.01)\n",
    "    opt_without_lb = torch.optim.Adam(model_without_lb.parameters(), lr=0.01)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'with_lb': {'main_loss': [], 'lb_loss': [], 'entropy': [], 'gini': []},\n",
    "        'without_lb': {'main_loss': [], 'lb_loss': [], 'entropy': [], 'gini': []}\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train model WITH load balancing\n",
    "        opt_with_lb.zero_grad()\n",
    "        \n",
    "        output_with_lb = model_with_lb(queries, items, return_gating_info=True)\n",
    "        similarities_with_lb = output_with_lb['similarities']\n",
    "        \n",
    "        main_loss_with_lb = F.mse_loss(similarities_with_lb, labels)\n",
    "        \n",
    "        if 'gating_weights' in output_with_lb:\n",
    "            lb_losses = model_with_lb.compute_load_balancing_loss(\n",
    "                output_with_lb['gating_weights'], \n",
    "                output_with_lb['input_features']\n",
    "            )\n",
    "            total_loss_with_lb = main_loss_with_lb + lb_losses['total']\n",
    "        else:\n",
    "            lb_losses = {'total': torch.tensor(0.0)}\n",
    "            total_loss_with_lb = main_loss_with_lb\n",
    "        \n",
    "        total_loss_with_lb.backward()\n",
    "        opt_with_lb.step()\n",
    "        \n",
    "        # Train model WITHOUT load balancing\n",
    "        opt_without_lb.zero_grad()\n",
    "        \n",
    "        output_without_lb = model_without_lb(queries, items, return_gating_info=True)\n",
    "        similarities_without_lb = output_without_lb['similarities']\n",
    "        \n",
    "        main_loss_without_lb = F.mse_loss(similarities_without_lb, labels)\n",
    "        main_loss_without_lb.backward()\n",
    "        opt_without_lb.step()\n",
    "        \n",
    "        # Compute statistics\n",
    "        with torch.no_grad():\n",
    "            for model_name, output in [('with_lb', output_with_lb), ('without_lb', output_without_lb)]:\n",
    "                if 'gating_weights' in output:\n",
    "                    gating_weights = output['gating_weights']\n",
    "                    component_usage = gating_weights.mean(dim=0)\n",
    "                    \n",
    "                    # Compute entropy\n",
    "                    entropy = -torch.sum(component_usage * torch.log(component_usage + 1e-8))\n",
    "                    \n",
    "                    # Compute Gini coefficient\n",
    "                    sorted_weights = torch.sort(component_usage)[0]\n",
    "                    n = len(sorted_weights)\n",
    "                    index = torch.arange(1, n + 1, dtype=torch.float32)\n",
    "                    gini = (2 * torch.sum(index * sorted_weights)) / (n * torch.sum(sorted_weights)) - (n + 1) / n\n",
    "                    \n",
    "                    history[model_name]['entropy'].append(entropy.item())\n",
    "                    history[model_name]['gini'].append(gini.item())\n",
    "                else:\n",
    "                    history[model_name]['entropy'].append(0)\n",
    "                    history[model_name]['gini'].append(1.0)\n",
    "        \n",
    "        # Record losses\n",
    "        history['with_lb']['main_loss'].append(main_loss_with_lb.item())\n",
    "        history['with_lb']['lb_loss'].append(lb_losses['total'].item())\n",
    "        history['without_lb']['main_loss'].append(main_loss_without_lb.item())\n",
    "        history['without_lb']['lb_loss'].append(0.0)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: WITH LB - Main: {main_loss_with_lb.item():.4f}, LB: {lb_losses['total'].item():.4f}\")\n",
    "            print(f\"           WITHOUT LB - Main: {main_loss_without_lb.item():.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run training comparison\n",
    "training_history = train_with_load_balancing_comparison(num_epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Ph·∫ßn 5: Visualization v√† Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# 1. Component Collapse Scenarios\n",
    "scenario_names = list(collapse_results.keys())\n",
    "perplexities = [collapse_results[name]['perplexity'] for name in scenario_names]\n",
    "ginis = [collapse_results[name]['gini'] for name in scenario_names]\n",
    "\n",
    "axes[0, 0].bar(scenario_names, perplexities, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].axhline(y=8, color='red', linestyle='--', alpha=0.7, label='Max Perplexity')\n",
    "axes[0, 0].set_title('Component Collapse: Effective Components')\n",
    "axes[0, 0].set_ylabel('Perplexity (Effective Components)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Gini Coefficient\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "bars = axes[0, 1].bar(scenario_names, ginis, alpha=0.7, color=colors)\n",
    "axes[0, 1].set_title('Component Collapse: Inequality (Gini)')\n",
    "axes[0, 1].set_ylabel('Gini Coefficient')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add text annotations\n",
    "for bar, gini in zip(bars, ginis):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{gini:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Weight Distributions\n",
    "for i, (name, weights) in enumerate(collapse_scenarios.items()):\n",
    "    if i < 4:  # Show first 4 scenarios\n",
    "        mean_weights = np.mean(weights, axis=0)\n",
    "        axes[0, 2].plot(mean_weights, 'o-', label=name, alpha=0.8, linewidth=2)\n",
    "\n",
    "axes[0, 2].set_title('Component Weight Distributions')\n",
    "axes[0, 2].set_xlabel('Component Index')\n",
    "axes[0, 2].set_ylabel('Average Weight')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Loss Comparison\n",
    "epochs = range(len(training_history['with_lb']['main_loss']))\n",
    "axes[1, 0].plot(epochs, training_history['with_lb']['main_loss'], 'b-', label='With Load Balancing', linewidth=2)\n",
    "axes[1, 0].plot(epochs, training_history['without_lb']['main_loss'], 'r-', label='Without Load Balancing', linewidth=2)\n",
    "axes[1, 0].set_title('Training: Main Loss Comparison')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Main Loss (MSE)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Load Balancing Loss\n",
    "axes[1, 1].plot(epochs, training_history['with_lb']['lb_loss'], 'g-', linewidth=2)\n",
    "axes[1, 1].set_title('Load Balancing Loss Over Time')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Load Balancing Loss')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Entropy Evolution\n",
    "max_entropy = math.log(6)  # For 6 components\n",
    "axes[1, 2].plot(epochs, training_history['with_lb']['entropy'], 'b-', label='With LB', linewidth=2)\n",
    "axes[1, 2].plot(epochs, training_history['without_lb']['entropy'], 'r-', label='Without LB', linewidth=2)\n",
    "axes[1, 2].axhline(y=max_entropy, color='green', linestyle='--', alpha=0.7, label='Max Entropy')\n",
    "axes[1, 2].set_title('Component Usage Entropy')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Entropy')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Gini Evolution\n",
    "axes[2, 0].plot(epochs, training_history['with_lb']['gini'], 'b-', label='With LB', linewidth=2)\n",
    "axes[2, 0].plot(epochs, training_history['without_lb']['gini'], 'r-', label='Without LB', linewidth=2)\n",
    "axes[2, 0].set_title('Component Usage Inequality (Gini)')\n",
    "axes[2, 0].set_xlabel('Epoch')\n",
    "axes[2, 0].set_ylabel('Gini Coefficient')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Information Theory Concepts Visualization\n",
    "entropy_values = []\n",
    "perplexity_values = []\n",
    "prob_values = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "for p in prob_values:\n",
    "    # Binary distribution\n",
    "    dist = np.array([p, 1-p])\n",
    "    entropy = utils.entropy(dist, base=2)\n",
    "    perplexity = utils.perplexity(dist)\n",
    "    entropy_values.append(entropy)\n",
    "    perplexity_values.append(perplexity)\n",
    "\n",
    "axes[2, 1].plot(prob_values, entropy_values, 'go-', label='Entropy', linewidth=2)\n",
    "axes[2, 1].set_title('Binary Distribution: Entropy vs Probability')\n",
    "axes[2, 1].set_xlabel('P(X=1)')\n",
    "axes[2, 1].set_ylabel('Entropy (bits)')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Perplexity vs Probability\n",
    "axes[2, 2].plot(prob_values, perplexity_values, 'mo-', linewidth=2)\n",
    "axes[2, 2].set_title('Binary Distribution: Perplexity vs Probability')\n",
    "axes[2, 2].set_xlabel('P(X=1)')\n",
    "axes[2, 2].set_ylabel('Perplexity')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Comprehensive visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Insights v√† Practical Guidelines\n",
    "\n",
    "### üîç Quan s√°t t·ª´ Experiments:\n",
    "\n",
    "1. **Component Collapse Impact**:\n",
    "   - Uniform distribution: Perplexity = 8/8 (full utilization)\n",
    "   - Collapsed distribution: Perplexity << 8 (wasted capacity)\n",
    "   - Gini coefficient tƒÉng ‚Üí inequality tƒÉng\n",
    "\n",
    "2. **Load Balancing Benefits**:\n",
    "   - Higher entropy ‚Üí better component utilization\n",
    "   - Lower Gini coefficient ‚Üí more equal distribution\n",
    "   - Potentially better generalization\n",
    "\n",
    "3. **Training Dynamics**:\n",
    "   - Load balancing loss initially high, decreases over time\n",
    "   - Main task performance kh√¥ng b·ªã hurt significantly\n",
    "   - Component usage becomes more balanced\n",
    "\n",
    "### üìñ Theoretical Insights:\n",
    "\n",
    "**Information Theory Perspective**:\n",
    "- **High Entropy** ‚Üí Uniform component usage ‚Üí Better capacity utilization\n",
    "- **Low Mutual Information** ‚Üí Gating decisions independent of input patterns\n",
    "- **Perplexity** ‚Üí Effective number of components being used\n",
    "\n",
    "**Mathematical Foundation**:\n",
    "```\n",
    "Optimal Load Balancing:\n",
    "- Maximize H(G) (entropy of gating decisions)\n",
    "- Minimize I(Z; G) (mutual information with inputs)\n",
    "- Minimize Var(usage) (variance in component usage)\n",
    "```\n",
    "\n",
    "### üöÄ Practical Implementation Guidelines:\n",
    "\n",
    "1. **Loss Function Design**:\n",
    "   ```python\n",
    "   total_loss = main_loss + Œª‚ÇÅ * uniform_loss + Œª‚ÇÇ * entropy_loss + Œª‚ÇÉ * mi_loss\n",
    "   ```\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Start with small Œª values (0.001 - 0.01)\n",
    "   - Monitor component usage statistics\n",
    "   - Adjust based on Gini coefficient and perplexity\n",
    "\n",
    "3. **Monitoring Metrics**:\n",
    "   - **Perplexity**: Should be close to num_components\n",
    "   - **Gini Coefficient**: Should be close to 0\n",
    "   - **Entropy**: Should be close to log(num_components)\n",
    "\n",
    "4. **When to Use Load Balancing**:\n",
    "   - Large number of components (P > 4)\n",
    "   - Complex datasets with diverse patterns\n",
    "   - When component collapse is observed\n",
    "   - In production systems where efficiency matters\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls:\n",
    "\n",
    "1. **Over-regularization**: Too strong load balancing ‚Üí worse main task performance\n",
    "2. **Under-regularization**: Component collapse ‚Üí wasted model capacity\n",
    "3. **Ignoring MI term**: Components may still correlate with input patterns\n",
    "4. **Fixed Œª values**: May need adaptive scheduling during training\n",
    "\n",
    "### üéØ Advanced Techniques:\n",
    "\n",
    "1. **Adaptive Load Balancing**: Adjust Œª based on current component usage\n",
    "2. **Curriculum Learning**: Gradually increase load balancing strength\n",
    "3. **Temperature Annealing**: Start with high temperature, decrease over time\n",
    "4. **Component-specific Penalties**: Different penalties for different components\n",
    "\n",
    "### üìö Research Directions:\n",
    "\n",
    "1. **Better MI Estimation**: MINE, InfoNCE, contrastive methods\n",
    "2. **Dynamic Component Count**: Adaptive number of components\n",
    "3. **Hierarchical Load Balancing**: Multi-level component organization\n",
    "4. **Hardware-aware Load Balancing**: Consider GPU memory and compute constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}